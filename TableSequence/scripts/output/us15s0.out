Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:19<06:15, 19.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:38<05:46, 19.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:56<05:18, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:14<04:52, 18.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:37<04:59, 19.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:56<04:36, 19.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:17<04:20, 20.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:39<04:07, 20.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:58<03:43, 20.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [03:17<03:16, 19.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:36<02:55, 19.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:56<02:39, 19.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [04:14<02:13, 19.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:32<01:52, 18.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:50<01:32, 18.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [05:12<01:18, 19.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [05:36<01:02, 20.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:54<00:40, 20.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [06:17<00:21, 21.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [06:37<00:00, 20.72s/it]Generating: 100%|██████████| 20/20 [06:37<00:00, 19.88s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : military branch . Context : Later in the year , the command under the command of Gen. Thomas L. B. Nesbitt was established to continue the campaign against the Japanese , led by Chief of Staff Sir Richard R. Nesbitt , on Okinawa . Head Entity : Thomas L. B. Nesbitt , Tail Entity : commanding general .\n']
['Relation : military branch . Context : Later in the year , the command under the command of Gen. Thomas L. B. Nesbitt was established to continue the campaign against the Japanese , led by Chief of Staff Sir Richard R. Nesbitt , on Okinawa . Head Entity : Thomas L. B. Nesbitt , Tail Entity : commanding general .\n', 'Relation : military branch . Context : General Sir Herbert Wilfrid Wilford , ( 9 January 1789 - 7 February 1897 ) was a Royal Canadian Air Force officer who commanded the First Air Force . Head Entity : Herbert Wilfrid Wilfrid , Tail Entity : Royal Canadian Air Force .\n']
['Relation : military branch . Context : Later in the year , the command under the command of Gen. Thomas L. B. Nesbitt was established to continue the campaign against the Japanese , led by Chief of Staff Sir Richard R. Nesbitt , on Okinawa . Head Entity : Thomas L. B. Nesbitt , Tail Entity : commanding general .\n', 'Relation : military branch . Context : General Sir Herbert Wilfrid Wilford , ( 9 January 1789 - 7 February 1897 ) was a Royal Canadian Air Force officer who commanded the First Air Force . Head Entity : Herbert Wilfrid Wilfrid , Tail Entity : Royal Canadian Air Force .\n', 'Relation : military branch . Context : This was the first Russian warplane to be operated in combat with the Warsaw Pact . Head Entity : Soviet Warplane , Tail Entity : Warsaw Pact .\n']
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 297, 'raw': 384}
{'target': 600, 'success': 325, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 371, 'raw': 480}
{'target': 600, 'success': 398, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 455, 'raw': 576}
{'target': 600, 'success': 480, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : military branch .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 15, 'raw': 32}
{'target': 600, 'success': 33, 'raw': 64}
{'target': 600, 'success': 54, 'raw': 96}
{'target': 600, 'success': 73, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 117, 'raw': 192}
{'target': 600, 'success': 136, 'raw': 224}
{'target': 600, 'success': 157, 'raw': 256}
{'target': 600, 'success': 176, 'raw': 288}
{'target': 600, 'success': 194, 'raw': 320}
{'target': 600, 'success': 216, 'raw': 352}
{'target': 600, 'success': 237, 'raw': 384}
{'target': 600, 'success': 257, 'raw': 416}
{'target': 600, 'success': 276, 'raw': 448}
{'target': 600, 'success': 295, 'raw': 480}
{'target': 600, 'success': 311, 'raw': 512}
{'target': 600, 'success': 333, 'raw': 544}
{'target': 600, 'success': 353, 'raw': 576}
{'target': 600, 'success': 372, 'raw': 608}
{'target': 600, 'success': 392, 'raw': 640}
{'target': 600, 'success': 417, 'raw': 672}
{'target': 600, 'success': 437, 'raw': 704}
{'target': 600, 'success': 459, 'raw': 736}
{'target': 600, 'success': 478, 'raw': 768}
{'target': 600, 'success': 495, 'raw': 800}
{'target': 600, 'success': 511, 'raw': 832}
{'target': 600, 'success': 532, 'raw': 864}
{'target': 600, 'success': 549, 'raw': 896}
{'target': 600, 'success': 564, 'raw': 928}
{'target': 600, 'success': 582, 'raw': 960}
{'target': 600, 'success': 602, 'raw': 992}
{'prompt': 'Relation : voice type .', 'success_rate': 0.6068548387096774, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'The Simpsons Comic Comedy\', \'voice type\', \'\', \'After this show , " The Simpsons : The Last Simpsons " was remade as " The Simpsons Comic Comedy " and as " Simpsons - themed video games on Xbox 360 , PlayStation 3 , and Xbox 360 .\')'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 163, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 340, 'raw': 448}
{'target': 600, 'success': 365, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 433, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 572, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 621, 'raw': 832}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7463942307692307, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : field of work . Context : Following his leadership in the 1972 elections under the leadership of Alastair Campbell , he represented the electorate of New South Wales from 1993 until the 2002 election under the leadership of Greg Clark . Head Entity : Greg Clark , Tail Entity : Australian electoral district .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 177, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 264, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 414, 'raw': 544}
{'target': 600, 'success': 439, 'raw': 576}
{'target': 600, 'success': 463, 'raw': 608}
{'target': 600, 'success': 488, 'raw': 640}
{'target': 600, 'success': 510, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 622, 'raw': 832}
{'prompt': 'Relation : field of work .', 'success_rate': 0.7475961538461539, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : language of work or name . Context : Later in the year ( 1143–1230 ) he married daughter of Louis of Saxony and had two sons married to this writer , the King of England , Thomas II , and the Marquess of Suffolk . Head Entity : Louis of Saxony , Tail Entity : language of work or name .\n']
['Relation : language of work or name . Context : Later in the year ( 1143–1230 ) he married daughter of Louis of Saxony and had two sons married to this writer , the King of England , Thomas II , and the Marquess of Suffolk . Head Entity : Louis of Saxony , Tail Entity : language of work or name .\n', 'Relation : language of work or name . Context : Eileen Waugh ( born 14 March 1950 ) is an American writer - songwriter , songwriter , and producer . Head Entity : Eileen Waugh , Tail Entity : English .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 125, 'raw': 192}
{'target': 600, 'success': 144, 'raw': 224}
{'target': 600, 'success': 167, 'raw': 256}
{'target': 600, 'success': 192, 'raw': 288}
{'target': 600, 'success': 216, 'raw': 320}
{'target': 600, 'success': 240, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 286, 'raw': 416}
{'target': 600, 'success': 302, 'raw': 448}
{'target': 600, 'success': 330, 'raw': 480}
{'target': 600, 'success': 356, 'raw': 512}
{'target': 600, 'success': 378, 'raw': 544}
{'target': 600, 'success': 400, 'raw': 576}
{'target': 600, 'success': 422, 'raw': 608}
{'target': 600, 'success': 442, 'raw': 640}
{'target': 600, 'success': 462, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 504, 'raw': 736}
{'target': 600, 'success': 531, 'raw': 768}
{'target': 600, 'success': 554, 'raw': 800}
{'target': 600, 'success': 578, 'raw': 832}
{'target': 600, 'success': 600, 'raw': 864}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.6944444444444444, 'errors': {'', "('Gérard Côteau', 'language of work or name', '', 'Gérard Côteau ( 8 February 1913 – 11 August 1986 ) was an Italian composer and arranger .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amain - sur - Mardin , and found the church deserted and no sign of the invading French forces . Head Entity : Amain - sur - Mardin , Tail Entity : France .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 295, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 394, 'raw': 512}
{'target': 600, 'success': 421, 'raw': 544}
{'target': 600, 'success': 447, 'raw': 576}
{'target': 600, 'success': 470, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 519, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 571, 'raw': 736}
{'target': 600, 'success': 597, 'raw': 768}
{'target': 600, 'success': 619, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.77375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8536931818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', '(\'MiniB1\', \'manufacturer\', \'\', \'A new model , a " MiniB1 " , was introduced in 2008 .\')'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 265, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 315, 'raw': 416}
{'target': 600, 'success': 345, 'raw': 448}
{'target': 600, 'success': 368, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 440, 'raw': 576}
{'target': 600, 'success': 468, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 520, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 593, 'raw': 768}
{'target': 600, 'success': 618, 'raw': 800}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7725, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 16, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 65, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 132, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 207, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 255, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 304, 'raw': 416}
{'target': 600, 'success': 326, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 367, 'raw': 512}
{'target': 600, 'success': 389, 'raw': 544}
{'target': 600, 'success': 411, 'raw': 576}
{'target': 600, 'success': 429, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 501, 'raw': 704}
{'target': 600, 'success': 527, 'raw': 736}
{'target': 600, 'success': 552, 'raw': 768}
{'target': 600, 'success': 572, 'raw': 800}
{'target': 600, 'success': 594, 'raw': 832}
{'target': 600, 'success': 620, 'raw': 864}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.7175925925925926, 'errors': {'', '(\'" The Oprah Winfrey Show\', \'nominated for\', \'\', \'When in 2009 , she appeared as the host of " The Oprah Winfrey Show " on " Variety " , and she did host a number of other shows , including a number of awards shows for children \\\'s programming , including " The Oprah Winfrey Show " , and a number of movies .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 311, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 497, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8233695652173914, 'errors': {'', "('American Airlines', 'operating system', '', 'The company was founded in 1969 by American Airlines engineer Neil Armstrong , who had retired from service in 1967 after retiring from the USAF as early as 1958 .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : original broadcaster . Context : Later in the year , the band formed New River Band with producer Brian Lauter . Head Entity : New River Band , Tail Entity : New River Times .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8410326086956522, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : Later in the year , the court of the king , the sovereign of Morocco , abolished the " sovereign " designation in favour of a civil magistrate , King Mohamed , to determine how much royal patronage they might allow in each department . Head Entity : Mohamed , Tail Entity : ruler .\n']
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 64, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 106, 'raw': 160}
{'target': 600, 'success': 129, 'raw': 192}
{'target': 600, 'success': 148, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 191, 'raw': 288}
{'target': 600, 'success': 210, 'raw': 320}
{'target': 600, 'success': 229, 'raw': 352}
{'target': 600, 'success': 253, 'raw': 384}
{'target': 600, 'success': 275, 'raw': 416}
{'target': 600, 'success': 295, 'raw': 448}
{'target': 600, 'success': 319, 'raw': 480}
{'target': 600, 'success': 343, 'raw': 512}
{'target': 600, 'success': 365, 'raw': 544}
{'target': 600, 'success': 390, 'raw': 576}
{'target': 600, 'success': 410, 'raw': 608}
{'target': 600, 'success': 429, 'raw': 640}
{'target': 600, 'success': 449, 'raw': 672}
{'target': 600, 'success': 465, 'raw': 704}
{'target': 600, 'success': 484, 'raw': 736}
{'target': 600, 'success': 506, 'raw': 768}
{'target': 600, 'success': 529, 'raw': 800}
{'target': 600, 'success': 550, 'raw': 832}
{'target': 600, 'success': 574, 'raw': 864}
{'target': 600, 'success': 591, 'raw': 896}
{'target': 600, 'success': 609, 'raw': 928}
{'prompt': 'Relation : position held .', 'success_rate': 0.65625, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 19, 'raw': 32}
{'target': 600, 'success': 34, 'raw': 64}
{'target': 600, 'success': 55, 'raw': 96}
{'target': 600, 'success': 70, 'raw': 128}
{'target': 600, 'success': 92, 'raw': 160}
{'target': 600, 'success': 113, 'raw': 192}
{'target': 600, 'success': 133, 'raw': 224}
{'target': 600, 'success': 150, 'raw': 256}
{'target': 600, 'success': 171, 'raw': 288}
{'target': 600, 'success': 193, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 241, 'raw': 416}
{'target': 600, 'success': 256, 'raw': 448}
{'target': 600, 'success': 272, 'raw': 480}
{'target': 600, 'success': 292, 'raw': 512}
{'target': 600, 'success': 309, 'raw': 544}
{'target': 600, 'success': 326, 'raw': 576}
{'target': 600, 'success': 345, 'raw': 608}
{'target': 600, 'success': 368, 'raw': 640}
{'target': 600, 'success': 383, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 426, 'raw': 736}
{'target': 600, 'success': 441, 'raw': 768}
{'target': 600, 'success': 455, 'raw': 800}
{'target': 600, 'success': 470, 'raw': 832}
{'target': 600, 'success': 491, 'raw': 864}
{'target': 600, 'success': 510, 'raw': 896}
{'target': 600, 'success': 531, 'raw': 928}
{'target': 600, 'success': 551, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 587, 'raw': 1024}
{'target': 600, 'success': 603, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5710227272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 463, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 546, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 185, 'raw': 256}
{'target': 600, 'success': 202, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 246, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 292, 'raw': 416}
{'target': 600, 'success': 314, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 383, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 453, 'raw': 640}
{'target': 600, 'success': 475, 'raw': 672}
{'target': 600, 'success': 497, 'raw': 704}
{'target': 600, 'success': 524, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 571, 'raw': 800}
{'target': 600, 'success': 591, 'raw': 832}
{'target': 600, 'success': 614, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.7106481481481481, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 391, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 512, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.796875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 17789
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17889, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:16, 16.10s/it]Extractor Estimating: 2it [00:17,  7.34s/it]Extractor Estimating: 3it [00:17,  4.29s/it]Extractor Estimating: 4it [00:18,  2.84s/it]Extractor Estimating: 5it [00:19,  2.29s/it]Extractor Estimating: 6it [00:20,  1.73s/it]Extractor Estimating: 7it [00:21,  1.37s/it]Extractor Estimating: 8it [00:25,  2.37s/it]Extractor Estimating: 9it [00:26,  1.82s/it]Extractor Estimating: 10it [00:27,  1.49s/it]Extractor Estimating: 11it [00:27,  1.24s/it]Extractor Estimating: 12it [00:28,  1.05s/it]Extractor Estimating: 13it [00:29,  1.05it/s]Extractor Estimating: 14it [00:29,  1.12it/s]Extractor Estimating: 15it [00:30,  1.24it/s]Extractor Estimating: 16it [00:31,  1.32it/s]Extractor Estimating: 17it [00:31,  1.41it/s]Extractor Estimating: 18it [00:32,  1.39it/s]Extractor Estimating: 19it [00:33,  1.41it/s]Extractor Estimating: 20it [00:33,  1.45it/s]Extractor Estimating: 21it [00:34,  1.46it/s]Extractor Estimating: 22it [00:35,  1.44it/s]Extractor Estimating: 23it [00:35,  1.49it/s]Extractor Estimating: 24it [00:36,  1.44it/s]Extractor Estimating: 25it [00:37,  1.48it/s]Extractor Estimating: 26it [00:37,  1.50it/s]Extractor Estimating: 27it [00:38,  1.45it/s]Extractor Estimating: 28it [00:39,  1.43it/s]Extractor Estimating: 29it [00:39,  1.47it/s]Extractor Estimating: 30it [00:40,  1.47it/s]Extractor Estimating: 31it [00:41,  1.46it/s]Extractor Estimating: 32it [00:41,  1.53it/s]Extractor Estimating: 33it [00:42,  1.54it/s]Extractor Estimating: 34it [00:43,  1.49it/s]Extractor Estimating: 35it [00:43,  1.51it/s]Extractor Estimating: 36it [00:44,  1.49it/s]Extractor Estimating: 37it [00:45,  1.51it/s]Extractor Estimating: 38it [00:45,  1.49it/s]Extractor Estimating: 39it [00:46,  1.54it/s]Extractor Estimating: 40it [00:47,  1.54it/s]Extractor Estimating: 41it [00:47,  1.55it/s]Extractor Estimating: 42it [00:48,  1.48it/s]Extractor Estimating: 43it [00:49,  1.50it/s]Extractor Estimating: 44it [00:49,  1.51it/s]Extractor Estimating: 45it [00:50,  1.47it/s]Extractor Estimating: 46it [00:51,  1.50it/s]Extractor Estimating: 47it [00:51,  1.50it/s]Extractor Estimating: 48it [00:52,  1.52it/s]Extractor Estimating: 49it [00:53,  1.53it/s]Extractor Estimating: 50it [00:53,  1.53it/s]Extractor Estimating: 51it [00:54,  1.58it/s]Extractor Estimating: 52it [00:54,  1.58it/s]Extractor Estimating: 53it [00:55,  1.59it/s]Extractor Estimating: 54it [00:56,  1.62it/s]Extractor Estimating: 55it [00:56,  1.61it/s]Extractor Estimating: 56it [00:57,  1.60it/s]Extractor Estimating: 57it [00:58,  1.66it/s]Extractor Estimating: 58it [00:58,  1.61it/s]Extractor Estimating: 59it [00:59,  1.62it/s]Extractor Estimating: 60it [00:59,  1.57it/s]Extractor Estimating: 61it [01:00,  1.57it/s]Extractor Estimating: 62it [01:01,  1.57it/s]Extractor Estimating: 63it [01:01,  1.60it/s]Extractor Estimating: 64it [01:02,  1.58it/s]Extractor Estimating: 65it [01:03,  1.55it/s]Extractor Estimating: 66it [01:03,  1.58it/s]Extractor Estimating: 67it [01:04,  1.60it/s]Extractor Estimating: 68it [01:05,  1.50it/s]Extractor Estimating: 69it [01:05,  1.53it/s]Extractor Estimating: 70it [01:06,  1.58it/s]Extractor Estimating: 71it [01:06,  1.60it/s]Extractor Estimating: 72it [01:07,  1.54it/s]Extractor Estimating: 73it [01:08,  1.60it/s]Extractor Estimating: 74it [01:09,  1.46it/s]Extractor Estimating: 75it [01:09,  1.47it/s]Extractor Estimating: 76it [01:10,  1.45it/s]Extractor Estimating: 77it [01:11,  1.48it/s]Extractor Estimating: 78it [01:11,  1.53it/s]Extractor Estimating: 79it [01:12,  1.53it/s]Extractor Estimating: 80it [01:13,  1.50it/s]Extractor Estimating: 81it [01:13,  1.48it/s]Extractor Estimating: 82it [01:14,  1.52it/s]Extractor Estimating: 83it [01:14,  1.51it/s]Extractor Estimating: 84it [01:15,  1.53it/s]Extractor Estimating: 85it [01:16,  1.48it/s]Extractor Estimating: 86it [01:17,  1.41it/s]Extractor Estimating: 87it [01:17,  1.43it/s]Extractor Estimating: 88it [01:18,  1.45it/s]Extractor Estimating: 89it [01:19,  1.44it/s]Extractor Estimating: 90it [01:19,  1.49it/s]Extractor Estimating: 91it [01:20,  1.51it/s]Extractor Estimating: 92it [01:21,  1.49it/s]Extractor Estimating: 93it [01:21,  1.52it/s]Extractor Estimating: 94it [01:23,  1.14it/s]Extractor Estimating: 95it [01:23,  1.22it/s]Extractor Estimating: 96it [01:24,  1.33it/s]Extractor Estimating: 97it [01:25,  1.34it/s]Extractor Estimating: 98it [01:25,  1.43it/s]Extractor Estimating: 99it [01:26,  1.47it/s]Extractor Estimating: 100it [01:27,  1.50it/s]Extractor Estimating: 101it [01:27,  1.53it/s]Extractor Estimating: 102it [01:28,  1.54it/s]Extractor Estimating: 103it [01:28,  1.55it/s]Extractor Estimating: 104it [01:29,  1.54it/s]Extractor Estimating: 105it [01:30,  1.51it/s]Extractor Estimating: 106it [01:31,  1.47it/s]Extractor Estimating: 107it [01:31,  1.46it/s]Extractor Estimating: 108it [01:32,  1.51it/s]Extractor Estimating: 109it [01:32,  1.51it/s]Extractor Estimating: 110it [01:33,  1.56it/s]Extractor Estimating: 111it [01:34,  1.59it/s]Extractor Estimating: 112it [01:34,  1.58it/s]Extractor Estimating: 113it [01:35,  1.62it/s]Extractor Estimating: 114it [01:35,  1.64it/s]Extractor Estimating: 115it [01:36,  1.56it/s]Extractor Estimating: 116it [01:37,  1.49it/s]Extractor Estimating: 117it [01:38,  1.51it/s]Extractor Estimating: 118it [01:38,  1.54it/s]Extractor Estimating: 119it [01:39,  1.59it/s]Extractor Estimating: 120it [01:39,  1.57it/s]Extractor Estimating: 121it [01:40,  1.54it/s]Extractor Estimating: 122it [01:41,  1.46it/s]Extractor Estimating: 123it [01:42,  1.44it/s]Extractor Estimating: 124it [01:42,  1.50it/s]Extractor Estimating: 125it [01:43,  1.54it/s]Extractor Estimating: 126it [01:43,  1.59it/s]Extractor Estimating: 127it [01:44,  1.60it/s]Extractor Estimating: 128it [01:45,  1.63it/s]Extractor Estimating: 129it [01:45,  1.64it/s]Extractor Estimating: 130it [01:46,  1.66it/s]Extractor Estimating: 131it [01:46,  1.64it/s]Extractor Estimating: 132it [01:47,  1.64it/s]Extractor Estimating: 133it [01:48,  1.65it/s]Extractor Estimating: 134it [01:48,  1.67it/s]Extractor Estimating: 135it [01:49,  1.68it/s]Extractor Estimating: 136it [01:49,  1.64it/s]Extractor Estimating: 137it [01:50,  1.66it/s]Extractor Estimating: 138it [01:51,  1.55it/s]Extractor Estimating: 139it [01:51,  1.63it/s]Extractor Estimating: 140it [01:52,  1.68it/s]Extractor Estimating: 141it [01:52,  1.67it/s]Extractor Estimating: 142it [01:53,  1.66it/s]Extractor Estimating: 143it [01:54,  1.65it/s]Extractor Estimating: 144it [01:54,  1.62it/s]Extractor Estimating: 145it [01:55,  1.67it/s]Extractor Estimating: 146it [01:55,  1.67it/s]Extractor Estimating: 147it [01:56,  1.67it/s]Extractor Estimating: 148it [01:57,  1.63it/s]Extractor Estimating: 149it [01:57,  1.62it/s]Extractor Estimating: 150it [01:58,  1.66it/s]Extractor Estimating: 151it [01:59,  1.60it/s]Extractor Estimating: 152it [01:59,  1.59it/s]Extractor Estimating: 153it [02:00,  1.59it/s]Extractor Estimating: 154it [02:01,  1.52it/s]Extractor Estimating: 155it [02:01,  1.50it/s]Extractor Estimating: 156it [02:02,  1.54it/s]Extractor Estimating: 157it [02:03,  1.54it/s]Extractor Estimating: 158it [02:03,  1.55it/s]Extractor Estimating: 159it [02:04,  1.55it/s]Extractor Estimating: 160it [02:04,  1.54it/s]Extractor Estimating: 161it [02:05,  1.51it/s]Extractor Estimating: 162it [02:06,  1.54it/s]Extractor Estimating: 163it [02:06,  1.56it/s]Extractor Estimating: 164it [02:07,  1.52it/s]Extractor Estimating: 165it [02:08,  1.42it/s]Extractor Estimating: 166it [02:09,  1.48it/s]Extractor Estimating: 167it [02:09,  1.53it/s]Extractor Estimating: 168it [02:10,  1.54it/s]Extractor Estimating: 169it [02:10,  1.55it/s]Extractor Estimating: 170it [02:11,  1.57it/s]Extractor Estimating: 171it [02:12,  1.54it/s]Extractor Estimating: 172it [02:12,  1.55it/s]Extractor Estimating: 173it [02:13,  1.53it/s]Extractor Estimating: 174it [02:14,  1.48it/s]Extractor Estimating: 175it [02:14,  1.52it/s]Extractor Estimating: 176it [02:15,  1.57it/s]Extractor Estimating: 177it [02:16,  1.57it/s]Extractor Estimating: 178it [02:16,  1.55it/s]Extractor Estimating: 179it [02:17,  1.53it/s]Extractor Estimating: 180it [02:18,  1.50it/s]Extractor Estimating: 181it [02:18,  1.52it/s]Extractor Estimating: 182it [02:19,  1.55it/s]Extractor Estimating: 183it [02:20,  1.51it/s]Extractor Estimating: 184it [02:20,  1.56it/s]Extractor Estimating: 185it [02:21,  1.56it/s]Extractor Estimating: 186it [02:21,  1.61it/s]Extractor Estimating: 187it [02:22,  1.55it/s]Extractor Estimating: 188it [02:23,  1.50it/s]Extractor Estimating: 189it [02:23,  1.53it/s]Extractor Estimating: 190it [02:24,  1.50it/s]Extractor Estimating: 191it [02:25,  1.54it/s]Extractor Estimating: 192it [02:25,  1.54it/s]Extractor Estimating: 193it [02:26,  1.51it/s]Extractor Estimating: 194it [02:27,  1.49it/s]Extractor Estimating: 195it [02:27,  1.53it/s]Extractor Estimating: 196it [02:28,  1.55it/s]Extractor Estimating: 197it [02:29,  1.47it/s]Extractor Estimating: 198it [02:29,  1.48it/s]Extractor Estimating: 199it [02:30,  1.52it/s]Extractor Estimating: 200it [02:31,  1.51it/s]Extractor Estimating: 201it [02:31,  1.48it/s]Extractor Estimating: 202it [02:32,  1.47it/s]Extractor Estimating: 203it [02:33,  1.54it/s]Extractor Estimating: 204it [02:33,  1.56it/s]Extractor Estimating: 205it [02:34,  1.53it/s]Extractor Estimating: 206it [02:35,  1.47it/s]Extractor Estimating: 207it [02:35,  1.47it/s]Extractor Estimating: 208it [02:36,  1.49it/s]Extractor Estimating: 209it [02:37,  1.51it/s]Extractor Estimating: 210it [02:37,  1.48it/s]Extractor Estimating: 211it [02:38,  1.53it/s]Extractor Estimating: 212it [02:39,  1.50it/s]Extractor Estimating: 213it [02:39,  1.51it/s]Extractor Estimating: 214it [02:40,  1.56it/s]Extractor Estimating: 215it [02:41,  1.53it/s]Extractor Estimating: 216it [02:41,  1.52it/s]Extractor Estimating: 217it [02:42,  1.56it/s]Extractor Estimating: 218it [02:43,  1.54it/s]Extractor Estimating: 219it [02:43,  1.57it/s]Extractor Estimating: 220it [02:44,  1.63it/s]Extractor Estimating: 221it [02:44,  1.61it/s]Extractor Estimating: 222it [02:45,  1.54it/s]Extractor Estimating: 223it [02:46,  1.56it/s]Extractor Estimating: 224it [02:46,  1.48it/s]Extractor Estimating: 225it [02:47,  1.45it/s]Extractor Estimating: 226it [02:48,  1.50it/s]Extractor Estimating: 227it [02:48,  1.56it/s]Extractor Estimating: 228it [02:49,  1.52it/s]Extractor Estimating: 229it [02:50,  1.49it/s]Extractor Estimating: 230it [02:51,  1.45it/s]Extractor Estimating: 231it [02:51,  1.47it/s]Extractor Estimating: 232it [02:52,  1.52it/s]Extractor Estimating: 233it [02:52,  1.50it/s]Extractor Estimating: 234it [02:53,  1.46it/s]Extractor Estimating: 235it [02:54,  1.50it/s]Extractor Estimating: 236it [02:55,  1.49it/s]Extractor Estimating: 237it [02:55,  1.49it/s]Extractor Estimating: 238it [02:56,  1.48it/s]Extractor Estimating: 239it [02:57,  1.50it/s]Extractor Estimating: 240it [02:57,  1.55it/s]Extractor Estimating: 241it [02:58,  1.55it/s]Extractor Estimating: 242it [02:58,  1.52it/s]Extractor Estimating: 243it [02:59,  1.49it/s]Extractor Estimating: 244it [03:00,  1.46it/s]Extractor Estimating: 245it [03:00,  1.53it/s]Extractor Estimating: 246it [03:01,  1.51it/s]Extractor Estimating: 247it [03:02,  1.46it/s]Extractor Estimating: 248it [03:03,  1.44it/s]Extractor Estimating: 249it [03:03,  1.33it/s]Extractor Estimating: 250it [03:04,  1.36it/s]Extractor Estimating: 251it [03:05,  1.43it/s]Extractor Estimating: 252it [03:05,  1.43it/s]Extractor Estimating: 253it [03:06,  1.43it/s]Extractor Estimating: 254it [03:07,  1.46it/s]Extractor Estimating: 255it [03:08,  1.44it/s]Extractor Estimating: 256it [03:08,  1.45it/s]Extractor Estimating: 257it [03:09,  1.50it/s]Extractor Estimating: 258it [03:09,  1.56it/s]Extractor Estimating: 259it [03:10,  1.57it/s]Extractor Estimating: 260it [03:11,  1.58it/s]Extractor Estimating: 261it [03:11,  1.57it/s]Extractor Estimating: 262it [03:12,  1.59it/s]Extractor Estimating: 263it [03:13,  1.53it/s]Extractor Estimating: 264it [03:13,  1.56it/s]Extractor Estimating: 265it [03:14,  1.53it/s]Extractor Estimating: 266it [03:15,  1.58it/s]Extractor Estimating: 267it [03:15,  1.56it/s]Extractor Estimating: 268it [03:16,  1.58it/s]Extractor Estimating: 269it [03:16,  1.58it/s]Extractor Estimating: 270it [03:17,  1.59it/s]Extractor Estimating: 271it [03:18,  1.57it/s]Extractor Estimating: 272it [03:18,  1.56it/s]Extractor Estimating: 273it [03:19,  1.58it/s]Extractor Estimating: 274it [03:20,  1.57it/s]Extractor Estimating: 275it [03:20,  1.54it/s]Extractor Estimating: 276it [03:21,  1.51it/s]Extractor Estimating: 277it [03:22,  1.46it/s]Extractor Estimating: 278it [03:22,  1.48it/s]Extractor Estimating: 279it [03:23,  1.48it/s]Extractor Estimating: 280it [03:24,  1.45it/s]Extractor Estimating: 281it [03:24,  1.48it/s]Extractor Estimating: 282it [03:25,  1.44it/s]Extractor Estimating: 283it [03:26,  1.44it/s]Extractor Estimating: 284it [03:27,  1.32it/s]Extractor Estimating: 285it [03:27,  1.37it/s]Extractor Estimating: 286it [03:28,  1.41it/s]Extractor Estimating: 287it [03:29,  1.44it/s]Extractor Estimating: 288it [03:29,  1.41it/s]Extractor Estimating: 289it [03:30,  1.42it/s]Extractor Estimating: 290it [03:31,  1.38it/s]Extractor Estimating: 291it [03:32,  1.39it/s]Extractor Estimating: 292it [03:32,  1.42it/s]Extractor Estimating: 293it [03:33,  1.44it/s]Extractor Estimating: 294it [03:34,  1.48it/s]Extractor Estimating: 295it [03:34,  1.48it/s]Extractor Estimating: 296it [03:35,  1.49it/s]Extractor Estimating: 297it [03:36,  1.47it/s]Extractor Estimating: 298it [03:36,  1.46it/s]Extractor Estimating: 299it [03:37,  1.47it/s]Extractor Estimating: 300it [03:38,  1.50it/s]Extractor Estimating: 301it [03:38,  1.54it/s]Extractor Estimating: 302it [03:39,  1.54it/s]Extractor Estimating: 303it [03:40,  1.58it/s]Extractor Estimating: 304it [03:40,  1.53it/s]Extractor Estimating: 305it [03:41,  1.61it/s]Extractor Estimating: 306it [03:41,  1.61it/s]Extractor Estimating: 307it [03:42,  1.61it/s]Extractor Estimating: 308it [03:43,  1.54it/s]Extractor Estimating: 309it [03:43,  1.56it/s]Extractor Estimating: 310it [03:44,  1.59it/s]Extractor Estimating: 311it [03:45,  1.60it/s]Extractor Estimating: 312it [03:45,  1.57it/s]Extractor Estimating: 313it [03:46,  1.62it/s]Extractor Estimating: 314it [03:47,  1.54it/s]Extractor Estimating: 315it [03:47,  1.54it/s]Extractor Estimating: 316it [03:48,  1.60it/s]Extractor Estimating: 317it [03:48,  1.50it/s]Extractor Estimating: 318it [03:49,  1.51it/s]Extractor Estimating: 319it [03:50,  1.53it/s]Extractor Estimating: 320it [03:50,  1.51it/s]Extractor Estimating: 321it [03:51,  1.54it/s]Extractor Estimating: 322it [03:52,  1.62it/s]Extractor Estimating: 323it [03:52,  1.55it/s]Extractor Estimating: 324it [03:53,  1.53it/s]Extractor Estimating: 325it [03:54,  1.54it/s]Extractor Estimating: 326it [03:54,  1.56it/s]Extractor Estimating: 327it [03:55,  1.55it/s]Extractor Estimating: 328it [03:56,  1.53it/s]Extractor Estimating: 329it [03:56,  1.58it/s]Extractor Estimating: 330it [03:57,  1.53it/s]Extractor Estimating: 331it [03:58,  1.55it/s]Extractor Estimating: 332it [03:58,  1.44it/s]Extractor Estimating: 333it [03:59,  1.50it/s]Extractor Estimating: 334it [04:00,  1.48it/s]Extractor Estimating: 335it [04:00,  1.51it/s]Extractor Estimating: 336it [04:01,  1.49it/s]Extractor Estimating: 337it [04:02,  1.52it/s]Extractor Estimating: 338it [04:02,  1.52it/s]Extractor Estimating: 339it [04:03,  1.52it/s]Extractor Estimating: 340it [04:04,  1.52it/s]Extractor Estimating: 341it [04:04,  1.57it/s]Extractor Estimating: 342it [04:05,  1.59it/s]Extractor Estimating: 343it [04:05,  1.55it/s]Extractor Estimating: 344it [04:06,  1.52it/s]Extractor Estimating: 345it [04:07,  1.47it/s]Extractor Estimating: 346it [04:08,  1.48it/s]Extractor Estimating: 347it [04:08,  1.48it/s]Extractor Estimating: 348it [04:09,  1.45it/s]Extractor Estimating: 349it [04:10,  1.46it/s]Extractor Estimating: 350it [04:10,  1.46it/s]Extractor Estimating: 351it [04:11,  1.50it/s]Extractor Estimating: 352it [04:12,  1.50it/s]Extractor Estimating: 353it [04:12,  1.51it/s]Extractor Estimating: 354it [04:13,  1.53it/s]Extractor Estimating: 355it [04:13,  1.57it/s]Extractor Estimating: 356it [04:14,  1.58it/s]Extractor Estimating: 357it [04:15,  1.60it/s]Extractor Estimating: 358it [04:15,  1.56it/s]Extractor Estimating: 359it [04:16,  1.59it/s]Extractor Estimating: 360it [04:17,  1.55it/s]Extractor Estimating: 361it [04:17,  1.54it/s]Extractor Estimating: 362it [04:18,  1.57it/s]Extractor Estimating: 363it [04:19,  1.56it/s]Extractor Estimating: 364it [04:19,  1.53it/s]Extractor Estimating: 365it [04:20,  1.50it/s]Extractor Estimating: 366it [04:21,  1.53it/s]Extractor Estimating: 367it [04:21,  1.58it/s]Extractor Estimating: 368it [04:22,  1.59it/s]Extractor Estimating: 369it [04:22,  1.61it/s]Extractor Estimating: 370it [04:23,  1.62it/s]Extractor Estimating: 371it [04:24,  1.61it/s]Extractor Estimating: 372it [04:24,  1.55it/s]Extractor Estimating: 373it [04:25,  1.51it/s]Extractor Estimating: 374it [04:26,  1.54it/s]Extractor Estimating: 375it [04:26,  1.51it/s]Extractor Estimating: 376it [04:27,  1.50it/s]Extractor Estimating: 377it [04:28,  1.47it/s]Extractor Estimating: 378it [04:28,  1.53it/s]Extractor Estimating: 379it [04:29,  1.56it/s]Extractor Estimating: 380it [04:30,  1.57it/s]Extractor Estimating: 381it [04:30,  1.58it/s]Extractor Estimating: 382it [04:31,  1.59it/s]Extractor Estimating: 383it [04:31,  1.56it/s]Extractor Estimating: 384it [04:32,  1.55it/s]Extractor Estimating: 385it [04:33,  1.57it/s]Extractor Estimating: 386it [04:33,  1.55it/s]Extractor Estimating: 387it [04:34,  1.56it/s]Extractor Estimating: 388it [04:35,  1.54it/s]Extractor Estimating: 389it [04:35,  1.59it/s]Extractor Estimating: 390it [04:36,  1.60it/s]Extractor Estimating: 391it [04:37,  1.60it/s]Extractor Estimating: 392it [04:37,  1.54it/s]Extractor Estimating: 393it [04:38,  1.51it/s]Extractor Estimating: 394it [04:39,  1.50it/s]Extractor Estimating: 395it [04:39,  1.53it/s]Extractor Estimating: 396it [04:40,  1.54it/s]Extractor Estimating: 397it [04:40,  1.61it/s]Extractor Estimating: 398it [04:41,  1.62it/s]Extractor Estimating: 399it [04:42,  1.56it/s]Extractor Estimating: 400it [04:42,  1.57it/s]Extractor Estimating: 401it [04:43,  1.61it/s]Extractor Estimating: 402it [04:44,  1.63it/s]Extractor Estimating: 403it [04:44,  1.63it/s]Extractor Estimating: 404it [04:45,  1.64it/s]Extractor Estimating: 405it [04:45,  1.68it/s]Extractor Estimating: 406it [04:46,  1.64it/s]Extractor Estimating: 407it [04:47,  1.64it/s]Extractor Estimating: 408it [04:47,  1.49it/s]Extractor Estimating: 409it [04:48,  1.55it/s]Extractor Estimating: 410it [04:49,  1.60it/s]Extractor Estimating: 411it [04:49,  1.64it/s]Extractor Estimating: 412it [04:50,  1.65it/s]Extractor Estimating: 413it [04:50,  1.66it/s]Extractor Estimating: 414it [04:51,  1.61it/s]Extractor Estimating: 415it [04:52,  1.61it/s]Extractor Estimating: 416it [04:52,  1.61it/s]Extractor Estimating: 417it [04:53,  1.59it/s]Extractor Estimating: 418it [04:53,  1.63it/s]Extractor Estimating: 419it [04:54,  1.63it/s]Extractor Estimating: 420it [04:55,  1.67it/s]Extractor Estimating: 421it [04:55,  1.64it/s]Extractor Estimating: 422it [04:56,  1.64it/s]Extractor Estimating: 423it [04:56,  1.69it/s]Extractor Estimating: 424it [04:57,  1.61it/s]Extractor Estimating: 425it [04:58,  1.61it/s]Extractor Estimating: 426it [04:58,  1.53it/s]Extractor Estimating: 427it [04:59,  1.54it/s]Extractor Estimating: 428it [05:00,  1.55it/s]Extractor Estimating: 429it [05:00,  1.53it/s]Extractor Estimating: 430it [05:01,  1.53it/s]Extractor Estimating: 431it [05:02,  1.52it/s]Extractor Estimating: 432it [05:02,  1.50it/s]Extractor Estimating: 433it [05:03,  1.55it/s]Extractor Estimating: 434it [05:04,  1.51it/s]Extractor Estimating: 435it [05:04,  1.54it/s]Extractor Estimating: 436it [05:05,  1.53it/s]Extractor Estimating: 437it [05:06,  1.54it/s]Extractor Estimating: 438it [05:06,  1.50it/s]Extractor Estimating: 439it [05:07,  1.53it/s]Extractor Estimating: 440it [05:08,  1.53it/s]Extractor Estimating: 441it [05:08,  1.51it/s]Extractor Estimating: 442it [05:09,  1.51it/s]Extractor Estimating: 443it [05:10,  1.50it/s]Extractor Estimating: 444it [05:10,  1.52it/s]Extractor Estimating: 445it [05:11,  1.46it/s]Extractor Estimating: 446it [05:12,  1.43it/s]Extractor Estimating: 447it [05:12,  1.48it/s]Extractor Estimating: 448it [05:13,  1.50it/s]Extractor Estimating: 449it [05:14,  1.50it/s]Extractor Estimating: 450it [05:14,  1.45it/s]Extractor Estimating: 451it [05:15,  1.42it/s]Extractor Estimating: 452it [05:16,  1.43it/s]Extractor Estimating: 453it [05:17,  1.43it/s]Extractor Estimating: 454it [05:17,  1.47it/s]Extractor Estimating: 455it [05:18,  1.45it/s]Extractor Estimating: 456it [05:19,  1.45it/s]Extractor Estimating: 457it [05:19,  1.48it/s]Extractor Estimating: 458it [05:20,  1.50it/s]Extractor Estimating: 459it [05:21,  1.49it/s]Extractor Estimating: 460it [05:21,  1.47it/s]Extractor Estimating: 461it [05:22,  1.43it/s]Extractor Estimating: 462it [05:23,  1.42it/s]Extractor Estimating: 463it [05:23,  1.42it/s]Extractor Estimating: 464it [05:24,  1.43it/s]Extractor Estimating: 465it [05:25,  1.44it/s]Extractor Estimating: 466it [05:25,  1.47it/s]Extractor Estimating: 467it [05:26,  1.53it/s]Extractor Estimating: 468it [05:27,  1.49it/s]Extractor Estimating: 469it [05:27,  1.48it/s]Extractor Estimating: 470it [05:28,  1.45it/s]Extractor Estimating: 471it [05:29,  1.54it/s]Extractor Estimating: 472it [05:29,  1.54it/s]Extractor Estimating: 473it [05:30,  1.53it/s]Extractor Estimating: 474it [05:31,  1.50it/s]Extractor Estimating: 475it [05:31,  1.49it/s]Extractor Estimating: 476it [05:32,  1.50it/s]Extractor Estimating: 477it [05:33,  1.52it/s]Extractor Estimating: 478it [05:33,  1.47it/s]Extractor Estimating: 479it [05:34,  1.45it/s]Extractor Estimating: 480it [05:35,  1.44it/s]Extractor Estimating: 481it [05:35,  1.51it/s]Extractor Estimating: 482it [05:36,  1.39it/s]Extractor Estimating: 483it [05:37,  1.43it/s]Extractor Estimating: 484it [05:38,  1.46it/s]Extractor Estimating: 485it [05:38,  1.45it/s]Extractor Estimating: 486it [05:39,  1.48it/s]Extractor Estimating: 487it [05:40,  1.52it/s]Extractor Estimating: 488it [05:40,  1.58it/s]Extractor Estimating: 489it [05:41,  1.57it/s]Extractor Estimating: 490it [05:41,  1.52it/s]Extractor Estimating: 491it [05:42,  1.57it/s]Extractor Estimating: 492it [05:43,  1.56it/s]Extractor Estimating: 493it [05:43,  1.54it/s]Extractor Estimating: 494it [05:44,  1.52it/s]Extractor Estimating: 495it [05:45,  1.48it/s]Extractor Estimating: 496it [05:46,  1.40it/s]Extractor Estimating: 497it [05:46,  1.48it/s]Extractor Estimating: 498it [05:47,  1.48it/s]Extractor Estimating: 499it [05:47,  1.50it/s]Extractor Estimating: 500it [05:48,  1.48it/s]Extractor Estimating: 500it [05:48,  1.43it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9971 mean pseudo reward: 0.967948963862939
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 31062
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 31162, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=31162, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.369, loss:3694.1027
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.045, loss:2405.3355
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.057, loss:1852.9970
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.071, loss:1769.2922
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.066, loss:1626.3995
>> valid entity prec:0.6700, rec:0.6335, f1:0.6512
>> valid relation prec:0.6121, rec:0.1273, f1:0.2107
>> valid relation with NER prec:0.6121, rec:0.1273, f1:0.2107
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.414, loss:1544.0997
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.061, loss:1422.9486
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.061, loss:1345.9940
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.059, loss:1232.0709
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.070, loss:1192.4177
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6643, rec:0.6300, f1:0.6467
>> valid relation prec:0.5371, rec:0.0869, f1:0.1496
>> valid relation with NER prec:0.5371, rec:0.0869, f1:0.1496
g_step 1100, step 268, avg_time 2.412, loss:1176.5191
g_step 1200, step 368, avg_time 1.056, loss:1145.7313
g_step 1300, step 52, avg_time 1.068, loss:1080.6582
g_step 1400, step 152, avg_time 1.064, loss:1054.1963
g_step 1500, step 252, avg_time 1.073, loss:1027.4110
>> valid entity prec:0.6968, rec:0.6645, f1:0.6803
>> valid relation prec:0.4142, rec:0.1367, f1:0.2055
>> valid relation with NER prec:0.4142, rec:0.1367, f1:0.2055
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.408, loss:1021.9820
g_step 1700, step 36, avg_time 1.049, loss:969.7662
g_step 1800, step 136, avg_time 1.053, loss:921.5477
g_step 1900, step 236, avg_time 1.066, loss:954.7619
g_step 2000, step 336, avg_time 1.058, loss:961.1689
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.7112, rec:0.6444, f1:0.6762
>> valid relation prec:0.3274, rec:0.1098, f1:0.1645
>> valid relation with NER prec:0.3274, rec:0.1098, f1:0.1645
g_step 2100, step 20, avg_time 2.413, loss:948.0251
g_step 2200, step 120, avg_time 1.063, loss:880.7704
g_step 2300, step 220, avg_time 1.055, loss:868.5122
g_step 2400, step 320, avg_time 1.066, loss:898.2193
g_step 2500, step 4, avg_time 1.055, loss:923.5969
>> valid entity prec:0.6388, rec:0.6860, f1:0.6616
>> valid relation prec:0.3432, rec:0.1327, f1:0.1914
>> valid relation with NER prec:0.3432, rec:0.1327, f1:0.1914
g_step 2600, step 104, avg_time 2.413, loss:806.3714
g_step 2700, step 204, avg_time 1.067, loss:824.2780
g_step 2800, step 304, avg_time 1.072, loss:878.9370
g_step 2900, step 404, avg_time 1.055, loss:853.7111
g_step 3000, step 88, avg_time 1.047, loss:768.5500
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6100, rec:0.7042, f1:0.6538
>> valid relation prec:0.3287, rec:0.1704, f1:0.2245
>> valid relation with NER prec:0.3287, rec:0.1704, f1:0.2245
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 188, avg_time 2.417, loss:779.4861
g_step 3200, step 288, avg_time 1.053, loss:800.5850
g_step 3300, step 388, avg_time 1.068, loss:843.3626
g_step 3400, step 72, avg_time 1.066, loss:767.9388
g_step 3500, step 172, avg_time 1.068, loss:775.8298
>> valid entity prec:0.7078, rec:0.6573, f1:0.6816
>> valid relation prec:0.3414, rec:0.1724, f1:0.2291
>> valid relation with NER prec:0.3414, rec:0.1724, f1:0.2291
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 272, avg_time 2.411, loss:765.3040
g_step 3700, step 372, avg_time 1.060, loss:767.6915
g_step 3800, step 56, avg_time 1.054, loss:768.2540
g_step 3900, step 156, avg_time 1.068, loss:689.8833
g_step 4000, step 256, avg_time 1.063, loss:747.6256
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6386, rec:0.6247, f1:0.6315
>> valid relation prec:0.2872, rec:0.0884, f1:0.1351
>> valid relation with NER prec:0.2872, rec:0.0884, f1:0.1351
g_step 4100, step 356, avg_time 2.414, loss:722.1464
g_step 4200, step 40, avg_time 1.061, loss:729.5755
g_step 4300, step 140, avg_time 1.067, loss:689.6274
g_step 4400, step 240, avg_time 1.067, loss:676.8950
g_step 4500, step 340, avg_time 1.056, loss:691.3729
>> valid entity prec:0.6865, rec:0.6373, f1:0.6610
>> valid relation prec:0.3599, rec:0.1473, f1:0.2090
>> valid relation with NER prec:0.3599, rec:0.1473, f1:0.2090
g_step 4600, step 24, avg_time 2.411, loss:713.7678
g_step 4700, step 124, avg_time 1.059, loss:642.3009
g_step 4800, step 224, avg_time 1.066, loss:650.3959
g_step 4900, step 324, avg_time 1.063, loss:682.2231
g_step 5000, step 8, avg_time 1.053, loss:678.3121
learning rate was adjusted to 0.0008
>> valid entity prec:0.6252, rec:0.6614, f1:0.6428
>> valid relation prec:0.3460, rec:0.1350, f1:0.1942
>> valid relation with NER prec:0.3460, rec:0.1350, f1:0.1942
g_step 5100, step 108, avg_time 2.430, loss:624.9358
g_step 5200, step 208, avg_time 1.062, loss:615.4071
g_step 5300, step 308, avg_time 1.053, loss:635.0700
g_step 5400, step 408, avg_time 1.064, loss:673.5012
g_step 5500, step 92, avg_time 1.055, loss:600.6155
>> valid entity prec:0.6387, rec:0.6477, f1:0.6432
>> valid relation prec:0.2949, rec:0.1404, f1:0.1902
>> valid relation with NER prec:0.2949, rec:0.1404, f1:0.1902
g_step 5600, step 192, avg_time 2.417, loss:601.3028
g_step 5700, step 292, avg_time 1.062, loss:620.0989
g_step 5800, step 392, avg_time 1.060, loss:616.7365
g_step 5900, step 76, avg_time 1.051, loss:591.5065
g_step 6000, step 176, avg_time 1.066, loss:584.4195
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6599, rec:0.6442, f1:0.6519
>> valid relation prec:0.3050, rec:0.1701, f1:0.2184
>> valid relation with NER prec:0.3050, rec:0.1701, f1:0.2184
g_step 6100, step 276, avg_time 2.418, loss:598.4549
g_step 6200, step 376, avg_time 1.055, loss:581.0813
g_step 6300, step 60, avg_time 1.055, loss:556.5168
g_step 6400, step 160, avg_time 1.052, loss:568.0459
g_step 6500, step 260, avg_time 1.060, loss:570.5224
>> valid entity prec:0.6451, rec:0.6450, f1:0.6451
>> valid relation prec:0.2826, rec:0.1393, f1:0.1866
>> valid relation with NER prec:0.2826, rec:0.1393, f1:0.1866
g_step 6600, step 360, avg_time 2.424, loss:588.1844
g_step 6700, step 44, avg_time 1.057, loss:548.2078
g_step 6800, step 144, avg_time 1.061, loss:536.7154
g_step 6900, step 244, avg_time 1.061, loss:547.3176
g_step 7000, step 344, avg_time 1.057, loss:551.8910
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6597, rec:0.6199, f1:0.6392
>> valid relation prec:0.2923, rec:0.1521, f1:0.2001
>> valid relation with NER prec:0.2923, rec:0.1521, f1:0.2001
g_step 7100, step 28, avg_time 2.414, loss:544.5320
g_step 7200, step 128, avg_time 1.057, loss:515.7626
g_step 7300, step 228, avg_time 1.063, loss:528.4863
g_step 7400, step 328, avg_time 1.057, loss:531.5068
g_step 7500, step 12, avg_time 1.062, loss:515.5968
>> valid entity prec:0.6471, rec:0.6262, f1:0.6365
>> valid relation prec:0.2760, rec:0.1478, f1:0.1926
>> valid relation with NER prec:0.2760, rec:0.1478, f1:0.1926
g_step 7600, step 112, avg_time 2.403, loss:499.5512
g_step 7700, step 212, avg_time 1.071, loss:505.0763
g_step 7800, step 312, avg_time 1.050, loss:519.6622
g_step 7900, step 412, avg_time 1.073, loss:505.6425
g_step 8000, step 96, avg_time 1.058, loss:465.9984
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6624, rec:0.6429, f1:0.6525
>> valid relation prec:0.2690, rec:0.1447, f1:0.1882
>> valid relation with NER prec:0.2690, rec:0.1447, f1:0.1882
g_step 8100, step 196, avg_time 2.412, loss:477.8041
g_step 8200, step 296, avg_time 1.059, loss:497.1804
g_step 8300, step 396, avg_time 1.056, loss:499.1060
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:06:04 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:06:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-06-04_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:06:05 - WARNING - datasets.builder -   Using custom data configuration default-4b53c63f08967018
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-4b53c63f08967018/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:06:06,221 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:06:06,222 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:06:06,222 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:06:06,223 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:06:06,231 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:06,236 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:06,236 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:06,236 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:06,236 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:06,236 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:06:06,236 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:06:06,364 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:06:09,447 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:06:09,452 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-4b53c63f08967018/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 14:06:09 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14e0729ab200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.83ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.68ba/s] 27%|██▋       | 3/11 [00:00<00:02,  3.17ba/s] 36%|███▋      | 4/11 [00:01<00:01,  3.59ba/s] 45%|████▌     | 5/11 [00:01<00:01,  3.86ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.04ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.17ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.27ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.34ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.38ba/s]100%|██████████| 11/11 [00:02<00:00,  4.38ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.95ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.23ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.32ba/s]100%|██████████| 4/4 [00:00<00:00,  5.38ba/s]100%|██████████| 4/4 [00:00<00:00,  4.88ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  7.69ba/s] 18%|█▊        | 2/11 [00:00<00:01,  8.74ba/s] 27%|██▋       | 3/11 [00:00<00:00,  9.27ba/s] 36%|███▋      | 4/11 [00:00<00:00,  9.51ba/s] 45%|████▌     | 5/11 [00:00<00:00,  9.66ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.83ba/s] 82%|████████▏ | 9/11 [00:00<00:00,  9.93ba/s]100%|██████████| 11/11 [00:01<00:00, 12.16ba/s]100%|██████████| 11/11 [00:01<00:00, 10.61ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.73ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.80ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.15ba/s]100%|██████████| 4/4 [00:00<00:00, 10.35ba/s]
[INFO|trainer.py:414] 2023-08-28 14:06:14,667 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:06:14,682 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:06:14,682 >>   Num examples = 10029
[INFO|trainer.py:1149] 2023-08-28 14:06:14,682 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:06:14,682 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:06:14,682 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:06:14,682 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:06:14,682 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:56,  3.32it/s]  0%|          | 2/785 [00:00<03:49,  3.41it/s]  0%|          | 3/785 [00:00<03:47,  3.44it/s]  1%|          | 4/785 [00:01<03:47,  3.44it/s]  1%|          | 5/785 [00:01<03:45,  3.45it/s]  1%|          | 6/785 [00:01<03:45,  3.46it/s]  1%|          | 7/785 [00:02<03:44,  3.47it/s]  1%|          | 8/785 [00:02<03:43,  3.47it/s]  1%|          | 9/785 [00:02<03:43,  3.47it/s]  1%|▏         | 10/785 [00:02<03:43,  3.47it/s]  1%|▏         | 11/785 [00:03<03:42,  3.47it/s]  2%|▏         | 12/785 [00:03<03:42,  3.47it/s]  2%|▏         | 13/785 [00:03<03:42,  3.47it/s]  2%|▏         | 14/785 [00:04<03:42,  3.47it/s]  2%|▏         | 15/785 [00:04<03:41,  3.47it/s]  2%|▏         | 16/785 [00:04<03:41,  3.47it/s]  2%|▏         | 17/785 [00:04<03:41,  3.47it/s]  2%|▏         | 18/785 [00:05<03:40,  3.47it/s]  2%|▏         | 19/785 [00:05<03:40,  3.47it/s]  3%|▎         | 20/785 [00:05<03:40,  3.48it/s]  3%|▎         | 21/785 [00:06<03:39,  3.48it/s]  3%|▎         | 22/785 [00:06<03:39,  3.48it/s]  3%|▎         | 23/785 [00:06<03:39,  3.47it/s]  3%|▎         | 24/785 [00:06<03:38,  3.48it/s]  3%|▎         | 25/785 [00:07<03:38,  3.48it/s]  3%|▎         | 26/785 [00:07<03:38,  3.48it/s]  3%|▎         | 27/785 [00:07<03:38,  3.48it/s]  4%|▎         | 28/785 [00:08<03:37,  3.47it/s]  4%|▎         | 29/785 [00:08<03:37,  3.47it/s]  4%|▍         | 30/785 [00:08<03:37,  3.47it/s]  4%|▍         | 31/785 [00:08<03:37,  3.47it/s]  4%|▍         | 32/785 [00:09<03:36,  3.47it/s]  4%|▍         | 33/785 [00:09<03:36,  3.47it/s]  4%|▍         | 34/785 [00:09<03:36,  3.47it/s]  4%|▍         | 35/785 [00:10<03:36,  3.47it/s]  5%|▍         | 36/785 [00:10<03:35,  3.47it/s]  5%|▍         | 37/785 [00:10<03:35,  3.47it/s]  5%|▍         | 38/785 [00:10<03:35,  3.47it/s]  5%|▍         | 39/785 [00:11<03:35,  3.47it/s]  5%|▌         | 40/785 [00:11<03:34,  3.47it/s]  5%|▌         | 41/785 [00:11<03:34,  3.47it/s]  5%|▌         | 42/785 [00:12<03:33,  3.47it/s]  5%|▌         | 43/785 [00:12<03:33,  3.47it/s]  6%|▌         | 44/785 [00:12<03:33,  3.47it/s]  6%|▌         | 45/785 [00:12<03:33,  3.47it/s]  6%|▌         | 46/785 [00:13<03:33,  3.46it/s]  6%|▌         | 47/785 [00:13<03:33,  3.46it/s]  6%|▌         | 48/785 [00:13<03:32,  3.46it/s]  6%|▌         | 49/785 [00:14<03:32,  3.47it/s]  6%|▋         | 50/785 [00:14<03:32,  3.46it/s]  6%|▋         | 51/785 [00:14<03:32,  3.46it/s]  7%|▋         | 52/785 [00:14<03:31,  3.46it/s]  7%|▋         | 53/785 [00:15<03:31,  3.46it/s]  7%|▋         | 54/785 [00:15<03:30,  3.47it/s]  7%|▋         | 55/785 [00:15<03:30,  3.47it/s]  7%|▋         | 56/785 [00:16<03:30,  3.47it/s]  7%|▋         | 57/785 [00:16<03:29,  3.47it/s]  7%|▋         | 58/785 [00:16<03:29,  3.47it/s]  8%|▊         | 59/785 [00:17<03:29,  3.47it/s]  8%|▊         | 60/785 [00:17<03:28,  3.47it/s]  8%|▊         | 61/785 [00:17<03:28,  3.47it/s]  8%|▊         | 62/785 [00:17<03:28,  3.47it/s]  8%|▊         | 63/785 [00:18<03:28,  3.47it/s]  8%|▊         | 64/785 [00:18<03:27,  3.47it/s]  8%|▊         | 65/785 [00:18<03:27,  3.47it/s]  8%|▊         | 66/785 [00:19<03:27,  3.47it/s]  9%|▊         | 67/785 [00:19<03:26,  3.47it/s]  9%|▊         | 68/785 [00:19<03:26,  3.47it/s]  9%|▉         | 69/785 [00:19<03:26,  3.47it/s]  9%|▉         | 70/785 [00:20<03:26,  3.47it/s]  9%|▉         | 71/785 [00:20<03:25,  3.47it/s]  9%|▉         | 72/785 [00:20<03:25,  3.47it/s]  9%|▉         | 73/785 [00:21<03:25,  3.47it/s]  9%|▉         | 74/785 [00:21<03:24,  3.47it/s] 10%|▉         | 75/785 [00:21<03:24,  3.47it/s] 10%|▉         | 76/785 [00:21<03:24,  3.47it/s] 10%|▉         | 77/785 [00:22<03:24,  3.47it/s] 10%|▉         | 78/785 [00:22<03:23,  3.47it/s] 10%|█         | 79/785 [00:22<03:23,  3.47it/s] 10%|█         | 80/785 [00:23<03:23,  3.47it/s] 10%|█         | 81/785 [00:23<03:22,  3.47it/s] 10%|█         | 82/785 [00:23<03:22,  3.47it/s] 11%|█         | 83/785 [00:23<03:22,  3.47it/s] 11%|█         | 84/785 [00:24<03:22,  3.47it/s] 11%|█         | 85/785 [00:24<03:21,  3.47it/s] 11%|█         | 86/785 [00:24<03:21,  3.47it/s] 11%|█         | 87/785 [00:25<03:21,  3.46it/s] 11%|█         | 88/785 [00:25<03:21,  3.47it/s] 11%|█▏        | 89/785 [00:25<03:20,  3.47it/s] 11%|█▏        | 90/785 [00:25<03:20,  3.46it/s] 12%|█▏        | 91/785 [00:26<03:20,  3.46it/s] 12%|█▏        | 92/785 [00:26<03:19,  3.47it/s] 12%|█▏        | 93/785 [00:26<03:19,  3.47it/s] 12%|█▏        | 94/785 [00:27<03:19,  3.46it/s] 12%|█▏        | 95/785 [00:27<03:19,  3.47it/s] 12%|█▏        | 96/785 [00:27<03:18,  3.47it/s] 12%|█▏        | 97/785 [00:27<03:18,  3.47it/s] 12%|█▏        | 98/785 [00:28<03:18,  3.46it/s] 13%|█▎        | 99/785 [00:28<03:18,  3.46it/s] 13%|█▎        | 100/785 [00:28<03:17,  3.47it/s] 13%|█▎        | 101/785 [00:29<03:17,  3.47it/s] 13%|█▎        | 102/785 [00:29<03:17,  3.46it/s] 13%|█▎        | 103/785 [00:29<03:16,  3.47it/s] 13%|█▎        | 104/785 [00:29<03:16,  3.47it/s] 13%|█▎        | 105/785 [00:30<03:16,  3.46it/s] 14%|█▎        | 106/785 [00:30<03:16,  3.46it/s] 14%|█▎        | 107/785 [00:30<03:15,  3.46it/s] 14%|█▍        | 108/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 109/785 [00:31<03:15,  3.46it/s] 14%|█▍        | 110/785 [00:31<03:14,  3.46it/s] 14%|█▍        | 111/785 [00:32<03:14,  3.46it/s] 14%|█▍        | 112/785 [00:32<03:14,  3.46it/s] 14%|█▍        | 113/785 [00:32<03:13,  3.46it/s] 15%|█▍        | 114/785 [00:32<03:13,  3.46it/s] 15%|█▍        | 115/785 [00:33<03:13,  3.46it/s] 15%|█▍        | 116/785 [00:33<03:13,  3.46it/s] 15%|█▍        | 117/785 [00:33<03:12,  3.47it/s] 15%|█▌        | 118/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 119/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 120/785 [00:34<03:12,  3.46it/s] 15%|█▌        | 121/785 [00:34<03:11,  3.46it/s] 16%|█▌        | 122/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 123/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 124/785 [00:35<03:11,  3.46it/s] 16%|█▌        | 125/785 [00:36<03:10,  3.46it/s] 16%|█▌        | 126/785 [00:36<03:10,  3.46it/s] 16%|█▌        | 127/785 [00:36<03:10,  3.46it/s] 16%|█▋        | 128/785 [00:36<03:09,  3.46it/s] 16%|█▋        | 129/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 130/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 131/785 [00:37<03:09,  3.46it/s] 17%|█▋        | 132/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 133/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 134/785 [00:38<03:08,  3.46it/s] 17%|█▋        | 135/785 [00:38<03:07,  3.46it/s] 17%|█▋        | 136/785 [00:39<03:07,  3.46it/s] 17%|█▋        | 137/785 [00:39<03:07,  3.46it/s] 18%|█▊        | 138/785 [00:39<03:06,  3.46it/s] 18%|█▊        | 139/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 140/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 141/785 [00:40<03:06,  3.46it/s] 18%|█▊        | 142/785 [00:40<03:05,  3.46it/s] 18%|█▊        | 143/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 144/785 [00:41<03:05,  3.46it/s] 18%|█▊        | 145/785 [00:41<03:04,  3.46it/s] 19%|█▊        | 146/785 [00:42<03:04,  3.46it/s] 19%|█▊        | 147/785 [00:42<03:04,  3.46it/s] 19%|█▉        | 148/785 [00:42<03:04,  3.46it/s] 19%|█▉        | 149/785 [00:42<03:03,  3.46it/s] 19%|█▉        | 150/785 [00:43<03:03,  3.46it/s] 19%|█▉        | 151/785 [00:43<03:03,  3.46it/s] 19%|█▉        | 152/785 [00:43<03:02,  3.46it/s] 19%|█▉        | 153/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 154/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 155/785 [00:44<03:02,  3.46it/s] 20%|█▉        | 156/785 [00:45<03:01,  3.46it/s] 20%|██        | 157/785 [00:45<02:47,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 14:06:59,916 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:06:59,916 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:06:59,916 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.82it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.63it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.97it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.30it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.74it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.42it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.21it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.91it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.85it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.74it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.79it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.85it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.95it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.94it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.84it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.81it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.65it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.74it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.78it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.61it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.76it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.83it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.83it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.85it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.87it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.71it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.78it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.78it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.70it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.76it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.73it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.78it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.90it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.78it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.79it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.76it/s][A
 43%|████▎     | 188/438 [00:03<00:05, 46.73it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.74it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.69it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.73it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.78it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.88it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.85it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.76it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.77it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.74it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 45.08it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 45.70it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.03it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 46.23it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.41it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.57it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.61it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.71it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.64it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.55it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.69it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.63it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.66it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.76it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.67it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.70it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.67it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.65it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.62it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.68it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.69it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.72it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.73it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.82it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.67it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.68it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.66it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.63it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.64it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.74it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.74it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.68it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.80it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.72it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.65it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.71it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.58it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.68it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.64it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.65it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.64it/s][A                                                 
                                                 [A 20%|██        | 157/785 [00:54<02:47,  3.75it/s]
100%|██████████| 438/438 [00:09<00:00, 46.64it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:07:09,319 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-28 14:07:09,342 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:07:12,344 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:07:12,370 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:07:12,402 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:03<58:41,  5.62s/it] 20%|██        | 159/785 [01:03<41:55,  4.02s/it] 20%|██        | 160/785 [01:03<30:12,  2.90s/it] 21%|██        | 161/785 [01:04<22:00,  2.12s/it] 21%|██        | 162/785 [01:04<16:16,  1.57s/it] 21%|██        | 163/785 [01:04<12:16,  1.18s/it] 21%|██        | 164/785 [01:05<09:28,  1.09it/s] 21%|██        | 165/785 [01:05<07:30,  1.37it/s] 21%|██        | 166/785 [01:05<06:08,  1.68it/s] 21%|██▏       | 167/785 [01:05<05:11,  1.99it/s] 21%|██▏       | 168/785 [01:06<04:31,  2.28it/s] 22%|██▏       | 169/785 [01:06<04:02,  2.54it/s] 22%|██▏       | 170/785 [01:06<03:42,  2.76it/s] 22%|██▏       | 171/785 [01:07<03:29,  2.94it/s] 22%|██▏       | 172/785 [01:07<03:19,  3.08it/s] 22%|██▏       | 173/785 [01:07<03:12,  3.18it/s] 22%|██▏       | 174/785 [01:07<03:07,  3.27it/s] 22%|██▏       | 175/785 [01:08<03:03,  3.32it/s] 22%|██▏       | 176/785 [01:08<03:01,  3.36it/s] 23%|██▎       | 177/785 [01:08<02:59,  3.39it/s] 23%|██▎       | 178/785 [01:09<02:57,  3.42it/s] 23%|██▎       | 179/785 [01:09<02:56,  3.43it/s] 23%|██▎       | 180/785 [01:09<02:55,  3.44it/s] 23%|██▎       | 181/785 [01:09<02:55,  3.45it/s] 23%|██▎       | 182/785 [01:10<02:54,  3.46it/s] 23%|██▎       | 183/785 [01:10<02:54,  3.46it/s] 23%|██▎       | 184/785 [01:10<02:53,  3.46it/s] 24%|██▎       | 185/785 [01:11<02:53,  3.46it/s] 24%|██▎       | 186/785 [01:11<02:52,  3.46it/s] 24%|██▍       | 187/785 [01:11<02:52,  3.46it/s] 24%|██▍       | 188/785 [01:11<02:52,  3.47it/s] 24%|██▍       | 189/785 [01:12<02:51,  3.47it/s] 24%|██▍       | 190/785 [01:12<02:51,  3.46it/s] 24%|██▍       | 191/785 [01:12<02:51,  3.46it/s] 24%|██▍       | 192/785 [01:13<02:51,  3.46it/s] 25%|██▍       | 193/785 [01:13<02:50,  3.46it/s] 25%|██▍       | 194/785 [01:13<02:50,  3.46it/s] 25%|██▍       | 195/785 [01:14<02:50,  3.46it/s] 25%|██▍       | 196/785 [01:14<02:50,  3.46it/s] 25%|██▌       | 197/785 [01:14<02:49,  3.46it/s] 25%|██▌       | 198/785 [01:14<02:49,  3.46it/s] 25%|██▌       | 199/785 [01:15<02:49,  3.46it/s] 25%|██▌       | 200/785 [01:15<02:48,  3.47it/s] 26%|██▌       | 201/785 [01:15<02:48,  3.46it/s] 26%|██▌       | 202/785 [01:16<02:48,  3.46it/s] 26%|██▌       | 203/785 [01:16<02:48,  3.46it/s] 26%|██▌       | 204/785 [01:16<02:47,  3.46it/s] 26%|██▌       | 205/785 [01:16<02:47,  3.46it/s] 26%|██▌       | 206/785 [01:17<02:47,  3.46it/s] 26%|██▋       | 207/785 [01:17<02:47,  3.46it/s] 26%|██▋       | 208/785 [01:17<02:46,  3.46it/s] 27%|██▋       | 209/785 [01:18<02:46,  3.46it/s] 27%|██▋       | 210/785 [01:18<02:46,  3.46it/s] 27%|██▋       | 211/785 [01:18<02:45,  3.46it/s] 27%|██▋       | 212/785 [01:18<02:45,  3.46it/s] 27%|██▋       | 213/785 [01:19<02:45,  3.45it/s] 27%|██▋       | 214/785 [01:19<02:45,  3.45it/s] 27%|██▋       | 215/785 [01:19<02:44,  3.46it/s] 28%|██▊       | 216/785 [01:20<02:44,  3.46it/s] 28%|██▊       | 217/785 [01:20<02:44,  3.46it/s] 28%|██▊       | 218/785 [01:20<02:43,  3.46it/s] 28%|██▊       | 219/785 [01:20<02:43,  3.46it/s] 28%|██▊       | 220/785 [01:21<02:43,  3.46it/s] 28%|██▊       | 221/785 [01:21<02:42,  3.46it/s] 28%|██▊       | 222/785 [01:21<02:42,  3.46it/s] 28%|██▊       | 223/785 [01:22<02:42,  3.46it/s] 29%|██▊       | 224/785 [01:22<02:42,  3.45it/s] 29%|██▊       | 225/785 [01:22<02:42,  3.45it/s] 29%|██▉       | 226/785 [01:22<02:41,  3.46it/s] 29%|██▉       | 227/785 [01:23<02:41,  3.46it/s] 29%|██▉       | 228/785 [01:23<02:41,  3.46it/s] 29%|██▉       | 229/785 [01:23<02:40,  3.45it/s] 29%|██▉       | 230/785 [01:24<02:40,  3.46it/s] 29%|██▉       | 231/785 [01:24<02:40,  3.46it/s] 30%|██▉       | 232/785 [01:24<02:39,  3.46it/s] 30%|██▉       | 233/785 [01:24<02:39,  3.46it/s] 30%|██▉       | 234/785 [01:25<02:39,  3.46it/s] 30%|██▉       | 235/785 [01:25<02:40,  3.43it/s] 30%|███       | 236/785 [01:25<02:39,  3.44it/s] 30%|███       | 237/785 [01:26<02:39,  3.44it/s] 30%|███       | 238/785 [01:26<02:38,  3.45it/s] 30%|███       | 239/785 [01:26<02:38,  3.45it/s] 31%|███       | 240/785 [01:27<02:37,  3.45it/s] 31%|███       | 241/785 [01:27<02:37,  3.46it/s] 31%|███       | 242/785 [01:27<02:36,  3.46it/s] 31%|███       | 243/785 [01:27<02:36,  3.46it/s] 31%|███       | 244/785 [01:28<02:36,  3.46it/s] 31%|███       | 245/785 [01:28<02:36,  3.46it/s] 31%|███▏      | 246/785 [01:28<02:36,  3.45it/s] 31%|███▏      | 247/785 [01:29<02:35,  3.45it/s] 32%|███▏      | 248/785 [01:29<02:35,  3.46it/s] 32%|███▏      | 249/785 [01:29<02:35,  3.46it/s] 32%|███▏      | 250/785 [01:29<02:34,  3.46it/s] 32%|███▏      | 251/785 [01:30<02:34,  3.46it/s] 32%|███▏      | 252/785 [01:30<02:34,  3.45it/s] 32%|███▏      | 253/785 [01:30<02:33,  3.46it/s] 32%|███▏      | 254/785 [01:31<02:33,  3.45it/s] 32%|███▏      | 255/785 [01:31<02:33,  3.46it/s] 33%|███▎      | 256/785 [01:31<02:33,  3.46it/s] 33%|███▎      | 257/785 [01:31<02:33,  3.45it/s] 33%|███▎      | 258/785 [01:32<02:32,  3.45it/s] 33%|███▎      | 259/785 [01:32<02:32,  3.45it/s] 33%|███▎      | 260/785 [01:32<02:31,  3.46it/s] 33%|███▎      | 261/785 [01:33<02:31,  3.46it/s] 33%|███▎      | 262/785 [01:33<02:31,  3.46it/s] 34%|███▎      | 263/785 [01:33<02:30,  3.46it/s] 34%|███▎      | 264/785 [01:33<02:31,  3.45it/s] 34%|███▍      | 265/785 [01:34<02:30,  3.45it/s] 34%|███▍      | 266/785 [01:34<02:30,  3.45it/s] 34%|███▍      | 267/785 [01:34<02:29,  3.46it/s] 34%|███▍      | 268/785 [01:35<02:30,  3.45it/s] 34%|███▍      | 269/785 [01:35<02:29,  3.45it/s] 34%|███▍      | 270/785 [01:35<02:29,  3.45it/s] 35%|███▍      | 271/785 [01:35<02:28,  3.46it/s] 35%|███▍      | 272/785 [01:36<02:28,  3.46it/s] 35%|███▍      | 273/785 [01:36<02:28,  3.46it/s] 35%|███▍      | 274/785 [01:36<02:27,  3.46it/s] 35%|███▌      | 275/785 [01:37<02:27,  3.46it/s] 35%|███▌      | 276/785 [01:37<02:27,  3.46it/s] 35%|███▌      | 277/785 [01:37<02:26,  3.46it/s] 35%|███▌      | 278/785 [01:38<02:26,  3.46it/s] 36%|███▌      | 279/785 [01:38<02:26,  3.44it/s] 36%|███▌      | 280/785 [01:38<02:26,  3.45it/s] 36%|███▌      | 281/785 [01:38<02:26,  3.45it/s] 36%|███▌      | 282/785 [01:39<02:25,  3.45it/s] 36%|███▌      | 283/785 [01:39<02:25,  3.45it/s] 36%|███▌      | 284/785 [01:39<02:24,  3.46it/s] 36%|███▋      | 285/785 [01:40<02:24,  3.46it/s] 36%|███▋      | 286/785 [01:40<02:24,  3.46it/s] 37%|███▋      | 287/785 [01:40<02:24,  3.46it/s] 37%|███▋      | 288/785 [01:40<02:23,  3.46it/s] 37%|███▋      | 289/785 [01:41<02:23,  3.46it/s] 37%|███▋      | 290/785 [01:41<02:23,  3.45it/s] 37%|███▋      | 291/785 [01:41<02:23,  3.45it/s] 37%|███▋      | 292/785 [01:42<02:22,  3.46it/s] 37%|███▋      | 293/785 [01:42<02:22,  3.46it/s] 37%|███▋      | 294/785 [01:42<02:22,  3.46it/s] 38%|███▊      | 295/785 [01:42<02:21,  3.46it/s] 38%|███▊      | 296/785 [01:43<02:21,  3.46it/s] 38%|███▊      | 297/785 [01:43<02:21,  3.46it/s] 38%|███▊      | 298/785 [01:43<02:20,  3.46it/s] 38%|███▊      | 299/785 [01:44<02:20,  3.45it/s] 38%|███▊      | 300/785 [01:44<02:20,  3.45it/s] 38%|███▊      | 301/785 [01:44<02:20,  3.46it/s] 38%|███▊      | 302/785 [01:44<02:19,  3.46it/s] 39%|███▊      | 303/785 [01:45<02:19,  3.46it/s] 39%|███▊      | 304/785 [01:45<02:19,  3.46it/s] 39%|███▉      | 305/785 [01:45<02:18,  3.46it/s] 39%|███▉      | 306/785 [01:46<02:19,  3.44it/s] 39%|███▉      | 307/785 [01:46<02:18,  3.45it/s] 39%|███▉      | 308/785 [01:46<02:18,  3.45it/s] 39%|███▉      | 309/785 [01:46<02:17,  3.46it/s] 39%|███▉      | 310/785 [01:47<02:17,  3.45it/s] 40%|███▉      | 311/785 [01:47<02:17,  3.45it/s] 40%|███▉      | 312/785 [01:47<02:16,  3.45it/s] 40%|███▉      | 313/785 [01:48<02:16,  3.46it/s] 40%|████      | 314/785 [01:48<02:05,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 14:08:03,057 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:08:03,057 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:08:03,057 >>   Batch size = 8
{'eval_loss': 0.9714815616607666, 'eval_runtime': 9.3753, 'eval_samples_per_second': 373.001, 'eval_steps_per_second': 46.718, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.64it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.54it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.89it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.11it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.64it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.41it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.10it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.73it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.77it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.75it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.72it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.84it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.82it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.74it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.71it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.62it/s][A
 20%|██        | 88/438 [00:01<00:08, 43.25it/s][A
 21%|██        | 93/438 [00:01<00:07, 44.62it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.36it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.86it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.14it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.45it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.62it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.51it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.53it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.20it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.28it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.45it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.55it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.63it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.82it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.88it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.69it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.61it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.45it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.46it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.53it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.53it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.56it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.78it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.83it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.74it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.73it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.53it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.56it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.59it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.54it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.62it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.75it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.77it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.72it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.68it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.54it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.55it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.54it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.50it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.63it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.69it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.66it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.65it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.62it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.54it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.62it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.56it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.56it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.64it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.69it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.63it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.77it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.73it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.57it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.56it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.56it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.44it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.60it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.71it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.68it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.75it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.69it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.62it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.54it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.64it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.63it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.53it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.65it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.68it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.77it/s][A                                                 
                                                 [A 40%|████      | 314/785 [01:57<02:05,  3.74it/s]
100%|██████████| 438/438 [00:09<00:00, 46.77it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:08:12,488 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-28 14:08:12,505 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:08:14,595 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:08:14,606 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:08:14,611 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:04<40:22,  5.15s/it] 40%|████      | 316/785 [02:05<28:53,  3.70s/it] 40%|████      | 317/785 [02:05<20:51,  2.67s/it] 41%|████      | 318/785 [02:05<15:14,  1.96s/it] 41%|████      | 319/785 [02:06<11:19,  1.46s/it] 41%|████      | 320/785 [02:06<08:34,  1.11s/it] 41%|████      | 321/785 [02:06<06:39,  1.16it/s] 41%|████      | 322/785 [02:06<05:19,  1.45it/s] 41%|████      | 323/785 [02:07<04:23,  1.76it/s] 41%|████▏     | 324/785 [02:07<03:43,  2.06it/s] 41%|████▏     | 325/785 [02:07<03:16,  2.35it/s] 42%|████▏     | 326/785 [02:08<02:56,  2.60it/s] 42%|████▏     | 327/785 [02:08<02:43,  2.79it/s] 42%|████▏     | 328/785 [02:08<02:34,  2.97it/s] 42%|████▏     | 329/785 [02:08<02:27,  3.10it/s] 42%|████▏     | 330/785 [02:09<02:22,  3.20it/s] 42%|████▏     | 331/785 [02:09<02:18,  3.27it/s] 42%|████▏     | 332/785 [02:09<02:16,  3.33it/s] 42%|████▏     | 333/785 [02:10<02:14,  3.37it/s] 43%|████▎     | 334/785 [02:10<02:12,  3.39it/s] 43%|████▎     | 335/785 [02:10<02:11,  3.42it/s] 43%|████▎     | 336/785 [02:11<02:10,  3.43it/s] 43%|████▎     | 337/785 [02:11<02:10,  3.44it/s] 43%|████▎     | 338/785 [02:11<02:10,  3.43it/s] 43%|████▎     | 339/785 [02:11<02:09,  3.44it/s] 43%|████▎     | 340/785 [02:12<02:09,  3.45it/s] 43%|████▎     | 341/785 [02:12<02:08,  3.45it/s] 44%|████▎     | 342/785 [02:12<02:08,  3.45it/s] 44%|████▎     | 343/785 [02:13<02:07,  3.46it/s] 44%|████▍     | 344/785 [02:13<02:07,  3.46it/s] 44%|████▍     | 345/785 [02:13<02:07,  3.46it/s] 44%|████▍     | 346/785 [02:13<02:06,  3.46it/s] 44%|████▍     | 347/785 [02:14<02:06,  3.46it/s] 44%|████▍     | 348/785 [02:14<02:06,  3.46it/s] 44%|████▍     | 349/785 [02:14<02:06,  3.45it/s] 45%|████▍     | 350/785 [02:15<02:05,  3.45it/s] 45%|████▍     | 351/785 [02:15<02:05,  3.46it/s] 45%|████▍     | 352/785 [02:15<02:05,  3.46it/s] 45%|████▍     | 353/785 [02:15<02:04,  3.46it/s] 45%|████▌     | 354/785 [02:16<02:04,  3.46it/s] 45%|████▌     | 355/785 [02:16<02:04,  3.46it/s] 45%|████▌     | 356/785 [02:16<02:03,  3.46it/s] 45%|████▌     | 357/785 [02:17<02:03,  3.46it/s] 46%|████▌     | 358/785 [02:17<02:03,  3.46it/s] 46%|████▌     | 359/785 [02:17<02:03,  3.46it/s] 46%|████▌     | 360/785 [02:17<02:03,  3.45it/s] 46%|████▌     | 361/785 [02:18<02:02,  3.45it/s] 46%|████▌     | 362/785 [02:18<02:02,  3.45it/s] 46%|████▌     | 363/785 [02:18<02:02,  3.46it/s] 46%|████▋     | 364/785 [02:19<02:01,  3.46it/s] 46%|████▋     | 365/785 [02:19<02:01,  3.46it/s] 47%|████▋     | 366/785 [02:19<02:01,  3.46it/s] 47%|████▋     | 367/785 [02:19<02:00,  3.46it/s] 47%|████▋     | 368/785 [02:20<02:00,  3.46it/s] 47%|████▋     | 369/785 [02:20<02:00,  3.46it/s] 47%|████▋     | 370/785 [02:20<02:00,  3.46it/s] 47%|████▋     | 371/785 [02:21<02:00,  3.44it/s] 47%|████▋     | 372/785 [02:21<01:59,  3.45it/s] 48%|████▊     | 373/785 [02:21<01:59,  3.45it/s] 48%|████▊     | 374/785 [02:21<01:59,  3.45it/s] 48%|████▊     | 375/785 [02:22<01:58,  3.45it/s] 48%|████▊     | 376/785 [02:22<01:58,  3.46it/s] 48%|████▊     | 377/785 [02:22<01:58,  3.46it/s] 48%|████▊     | 378/785 [02:23<01:57,  3.46it/s] 48%|████▊     | 379/785 [02:23<01:57,  3.46it/s] 48%|████▊     | 380/785 [02:23<01:57,  3.46it/s] 49%|████▊     | 381/785 [02:24<01:56,  3.46it/s] 49%|████▊     | 382/785 [02:24<01:57,  3.44it/s] 49%|████▉     | 383/785 [02:24<01:56,  3.45it/s] 49%|████▉     | 384/785 [02:24<01:56,  3.45it/s] 49%|████▉     | 385/785 [02:25<01:55,  3.45it/s] 49%|████▉     | 386/785 [02:25<01:55,  3.45it/s] 49%|████▉     | 387/785 [02:25<01:55,  3.46it/s] 49%|████▉     | 388/785 [02:26<01:54,  3.46it/s] 50%|████▉     | 389/785 [02:26<01:54,  3.46it/s] 50%|████▉     | 390/785 [02:26<01:54,  3.46it/s] 50%|████▉     | 391/785 [02:26<01:53,  3.46it/s] 50%|████▉     | 392/785 [02:27<01:53,  3.45it/s] 50%|█████     | 393/785 [02:27<01:53,  3.44it/s] 50%|█████     | 394/785 [02:27<01:53,  3.45it/s] 50%|█████     | 395/785 [02:28<01:53,  3.45it/s] 50%|█████     | 396/785 [02:28<01:52,  3.45it/s] 51%|█████     | 397/785 [02:28<01:52,  3.45it/s] 51%|█████     | 398/785 [02:28<01:51,  3.46it/s] 51%|█████     | 399/785 [02:29<01:51,  3.46it/s] 51%|█████     | 400/785 [02:29<01:51,  3.46it/s] 51%|█████     | 401/785 [02:29<01:51,  3.46it/s] 51%|█████     | 402/785 [02:30<01:50,  3.46it/s] 51%|█████▏    | 403/785 [02:30<01:50,  3.45it/s] 51%|█████▏    | 404/785 [02:30<01:50,  3.45it/s] 52%|█████▏    | 405/785 [02:30<01:50,  3.45it/s] 52%|█████▏    | 406/785 [02:31<01:49,  3.46it/s] 52%|█████▏    | 407/785 [02:31<01:49,  3.46it/s] 52%|█████▏    | 408/785 [02:31<01:49,  3.46it/s] 52%|█████▏    | 409/785 [02:32<01:49,  3.44it/s] 52%|█████▏    | 410/785 [02:32<01:48,  3.45it/s] 52%|█████▏    | 411/785 [02:32<01:48,  3.45it/s] 52%|█████▏    | 412/785 [02:33<01:48,  3.45it/s] 53%|█████▎    | 413/785 [02:33<01:47,  3.45it/s] 53%|█████▎    | 414/785 [02:33<01:47,  3.45it/s] 53%|█████▎    | 415/785 [02:33<01:47,  3.46it/s] 53%|█████▎    | 416/785 [02:34<01:46,  3.46it/s] 53%|█████▎    | 417/785 [02:34<01:46,  3.46it/s] 53%|█████▎    | 418/785 [02:34<01:46,  3.46it/s] 53%|█████▎    | 419/785 [02:35<01:45,  3.46it/s] 54%|█████▎    | 420/785 [02:35<01:45,  3.46it/s] 54%|█████▎    | 421/785 [02:35<01:45,  3.46it/s] 54%|█████▍    | 422/785 [02:35<01:45,  3.46it/s] 54%|█████▍    | 423/785 [02:36<01:44,  3.46it/s] 54%|█████▍    | 424/785 [02:36<01:44,  3.46it/s] 54%|█████▍    | 425/785 [02:36<01:44,  3.46it/s] 54%|█████▍    | 426/785 [02:37<01:43,  3.46it/s] 54%|█████▍    | 427/785 [02:37<01:43,  3.44it/s] 55%|█████▍    | 428/785 [02:37<01:43,  3.45it/s] 55%|█████▍    | 429/785 [02:37<01:43,  3.45it/s] 55%|█████▍    | 430/785 [02:38<01:42,  3.45it/s] 55%|█████▍    | 431/785 [02:38<01:42,  3.46it/s] 55%|█████▌    | 432/785 [02:38<01:42,  3.45it/s] 55%|█████▌    | 433/785 [02:39<01:41,  3.46it/s] 55%|█████▌    | 434/785 [02:39<01:41,  3.46it/s] 55%|█████▌    | 435/785 [02:39<01:41,  3.46it/s] 56%|█████▌    | 436/785 [02:39<01:40,  3.46it/s] 56%|█████▌    | 437/785 [02:40<01:40,  3.46it/s] 56%|█████▌    | 438/785 [02:40<01:40,  3.46it/s] 56%|█████▌    | 439/785 [02:40<01:40,  3.46it/s] 56%|█████▌    | 440/785 [02:41<01:39,  3.46it/s] 56%|█████▌    | 441/785 [02:41<01:39,  3.46it/s] 56%|█████▋    | 442/785 [02:41<01:39,  3.46it/s] 56%|█████▋    | 443/785 [02:41<01:38,  3.46it/s] 57%|█████▋    | 444/785 [02:42<01:38,  3.46it/s] 57%|█████▋    | 445/785 [02:42<01:38,  3.45it/s] 57%|█████▋    | 446/785 [02:42<01:38,  3.45it/s] 57%|█████▋    | 447/785 [02:43<01:37,  3.45it/s] 57%|█████▋    | 448/785 [02:43<01:37,  3.45it/s] 57%|█████▋    | 449/785 [02:43<01:37,  3.45it/s] 57%|█████▋    | 450/785 [02:43<01:37,  3.45it/s] 57%|█████▋    | 451/785 [02:44<01:36,  3.45it/s] 58%|█████▊    | 452/785 [02:44<01:36,  3.45it/s] 58%|█████▊    | 453/785 [02:44<01:36,  3.46it/s] 58%|█████▊    | 454/785 [02:45<01:35,  3.46it/s] 58%|█████▊    | 455/785 [02:45<01:35,  3.46it/s] 58%|█████▊    | 456/785 [02:45<01:35,  3.46it/s] 58%|█████▊    | 457/785 [02:46<01:34,  3.46it/s] 58%|█████▊    | 458/785 [02:46<01:34,  3.46it/s] 58%|█████▊    | 459/785 [02:46<01:34,  3.46it/s] 59%|█████▊    | 460/785 [02:46<01:34,  3.46it/s] 59%|█████▊    | 461/785 [02:47<01:33,  3.46it/s] 59%|█████▉    | 462/785 [02:47<01:33,  3.45it/s] 59%|█████▉    | 463/785 [02:47<01:33,  3.45it/s] 59%|█████▉    | 464/785 [02:48<01:32,  3.45it/s] 59%|█████▉    | 465/785 [02:48<01:32,  3.45it/s] 59%|█████▉    | 466/785 [02:48<01:32,  3.45it/s] 59%|█████▉    | 467/785 [02:48<01:32,  3.43it/s] 60%|█████▉    | 468/785 [02:49<01:32,  3.44it/s] 60%|█████▉    | 469/785 [02:49<01:31,  3.45it/s] 60%|█████▉    | 470/785 [02:49<01:31,  3.45it/s] 60%|██████    | 471/785 [02:50<01:24,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 14:09:04,695 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:09:04,695 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:09:04,695 >>   Batch size = 8
{'eval_loss': 0.9560226202011108, 'eval_runtime': 9.4036, 'eval_samples_per_second': 371.879, 'eval_steps_per_second': 46.578, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.41it/s][A
  3%|▎         | 12/438 [00:00<00:09, 46.77it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.28it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.96it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.92it/s][A
  7%|▋         | 32/438 [00:00<00:08, 46.91it/s][A
  8%|▊         | 37/438 [00:00<00:08, 46.90it/s][A
 10%|▉         | 42/438 [00:00<00:08, 46.72it/s][A
 11%|█         | 47/438 [00:00<00:08, 46.46it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 46.78it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 46.62it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 46.61it/s][A
 15%|█▌        | 67/438 [00:01<00:07, 46.71it/s][A
 16%|█▋        | 72/438 [00:01<00:07, 46.61it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 46.57it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 46.71it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 46.73it/s][A
 21%|██        | 92/438 [00:01<00:07, 46.68it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 46.66it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 46.59it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 46.58it/s][A
 26%|██▌       | 112/438 [00:02<00:06, 46.68it/s][A
 27%|██▋       | 117/438 [00:02<00:06, 46.63it/s][A
 28%|██▊       | 122/438 [00:02<00:06, 46.59it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 46.61it/s][A
 30%|███       | 132/438 [00:02<00:06, 46.65it/s][A
 31%|███▏      | 137/438 [00:02<00:06, 46.66it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 46.70it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 46.69it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 46.62it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 46.69it/s][A
 37%|███▋      | 162/438 [00:03<00:05, 46.72it/s][A
 38%|███▊      | 167/438 [00:03<00:05, 46.60it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 46.68it/s][A
 40%|████      | 177/438 [00:03<00:05, 46.58it/s][A
 42%|████▏     | 182/438 [00:03<00:05, 46.67it/s][A
 43%|████▎     | 187/438 [00:03<00:05, 46.70it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 46.69it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 46.68it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 46.65it/s][A
 47%|████▋     | 207/438 [00:04<00:04, 46.69it/s][A
 48%|████▊     | 212/438 [00:04<00:04, 46.69it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 46.71it/s][A
 51%|█████     | 222/438 [00:04<00:04, 46.68it/s][A
 52%|█████▏    | 227/438 [00:04<00:04, 46.56it/s][A
 53%|█████▎    | 232/438 [00:04<00:04, 46.59it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 46.65it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 46.63it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 46.60it/s][A
 58%|█████▊    | 252/438 [00:05<00:03, 46.67it/s][A
 59%|█████▊    | 257/438 [00:05<00:03, 46.67it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 46.75it/s][A
 61%|██████    | 267/438 [00:05<00:03, 46.74it/s][A
 62%|██████▏   | 272/438 [00:05<00:03, 46.68it/s][A
 63%|██████▎   | 277/438 [00:05<00:03, 46.59it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 46.62it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 46.66it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 46.66it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 46.66it/s][A
 69%|██████▉   | 302/438 [00:06<00:02, 46.68it/s][A
 70%|███████   | 307/438 [00:06<00:02, 46.70it/s][A
 71%|███████   | 312/438 [00:06<00:02, 46.72it/s][A
 72%|███████▏  | 317/438 [00:06<00:02, 46.77it/s][A
 74%|███████▎  | 322/438 [00:06<00:02, 46.70it/s][A
 75%|███████▍  | 327/438 [00:06<00:02, 46.60it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 46.66it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 46.68it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 46.71it/s][A
 79%|███████▉  | 347/438 [00:07<00:01, 46.72it/s][A
 80%|████████  | 352/438 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 46.75it/s][A
 83%|████████▎ | 362/438 [00:07<00:01, 46.79it/s][A
 84%|████████▍ | 367/438 [00:07<00:01, 46.67it/s][A
 85%|████████▍ | 372/438 [00:07<00:01, 46.56it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 46.61it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 46.58it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 46.62it/s][A
 89%|████████▉ | 392/438 [00:08<00:00, 46.72it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 46.65it/s][A
 92%|█████████▏| 402/438 [00:08<00:00, 46.66it/s][A
 93%|█████████▎| 407/438 [00:08<00:00, 46.74it/s][A
 94%|█████████▍| 412/438 [00:08<00:00, 46.72it/s][A
 95%|█████████▌| 417/438 [00:08<00:00, 46.63it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 46.51it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 46.66it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 46.60it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 46.67it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [02:59<01:24,  3.74it/s]
100%|██████████| 438/438 [00:09<00:00, 46.67it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:09:14,121 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-28 14:09:14,154 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:09:16,847 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:09:16,875 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:09:16,884 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:07<29:03,  5.57s/it] 60%|██████    | 473/785 [03:08<20:44,  3.99s/it] 60%|██████    | 474/785 [03:08<14:54,  2.88s/it] 61%|██████    | 475/785 [03:08<10:51,  2.10s/it] 61%|██████    | 476/785 [03:09<08:01,  1.56s/it] 61%|██████    | 477/785 [03:09<06:02,  1.18s/it] 61%|██████    | 478/785 [03:09<04:39,  1.10it/s] 61%|██████    | 479/785 [03:09<03:41,  1.38it/s] 61%|██████    | 480/785 [03:10<03:00,  1.69it/s] 61%|██████▏   | 481/785 [03:10<02:32,  1.99it/s] 61%|██████▏   | 482/785 [03:10<02:12,  2.28it/s] 62%|██████▏   | 483/785 [03:11<01:58,  2.54it/s] 62%|██████▏   | 484/785 [03:11<01:49,  2.75it/s] 62%|██████▏   | 485/785 [03:11<01:42,  2.93it/s] 62%|██████▏   | 486/785 [03:12<01:37,  3.07it/s] 62%|██████▏   | 487/785 [03:12<01:33,  3.18it/s] 62%|██████▏   | 488/785 [03:12<01:31,  3.26it/s] 62%|██████▏   | 489/785 [03:12<01:29,  3.32it/s] 62%|██████▏   | 490/785 [03:13<01:27,  3.36it/s] 63%|██████▎   | 491/785 [03:13<01:26,  3.39it/s] 63%|██████▎   | 492/785 [03:13<01:25,  3.41it/s] 63%|██████▎   | 493/785 [03:14<01:25,  3.43it/s] 63%|██████▎   | 494/785 [03:14<01:24,  3.44it/s] 63%|██████▎   | 495/785 [03:14<01:24,  3.43it/s] 63%|██████▎   | 496/785 [03:14<01:23,  3.44it/s] 63%|██████▎   | 497/785 [03:15<01:23,  3.45it/s] 63%|██████▎   | 498/785 [03:15<01:23,  3.45it/s] 64%|██████▎   | 499/785 [03:15<01:22,  3.45it/s] 64%|██████▎   | 500/785 [03:16<01:22,  3.45it/s]                                                  64%|██████▎   | 500/785 [03:16<01:22,  3.45it/s] 64%|██████▍   | 501/785 [03:16<01:22,  3.45it/s] 64%|██████▍   | 502/785 [03:16<01:21,  3.45it/s] 64%|██████▍   | 503/785 [03:16<01:21,  3.46it/s] 64%|██████▍   | 504/785 [03:17<01:21,  3.46it/s] 64%|██████▍   | 505/785 [03:17<01:21,  3.46it/s] 64%|██████▍   | 506/785 [03:17<01:20,  3.45it/s] 65%|██████▍   | 507/785 [03:18<01:20,  3.45it/s] 65%|██████▍   | 508/785 [03:18<01:20,  3.45it/s] 65%|██████▍   | 509/785 [03:18<01:19,  3.45it/s] 65%|██████▍   | 510/785 [03:18<01:19,  3.46it/s] 65%|██████▌   | 511/785 [03:19<01:19,  3.46it/s] 65%|██████▌   | 512/785 [03:19<01:18,  3.46it/s] 65%|██████▌   | 513/785 [03:19<01:18,  3.46it/s] 65%|██████▌   | 514/785 [03:20<01:18,  3.46it/s] 66%|██████▌   | 515/785 [03:20<01:18,  3.46it/s] 66%|██████▌   | 516/785 [03:20<01:17,  3.46it/s] 66%|██████▌   | 517/785 [03:20<01:17,  3.45it/s] 66%|██████▌   | 518/785 [03:21<01:17,  3.45it/s] 66%|██████▌   | 519/785 [03:21<01:16,  3.46it/s] 66%|██████▌   | 520/785 [03:21<01:16,  3.46it/s] 66%|██████▋   | 521/785 [03:22<01:16,  3.46it/s] 66%|██████▋   | 522/785 [03:22<01:16,  3.46it/s] 67%|██████▋   | 523/785 [03:22<01:15,  3.46it/s] 67%|██████▋   | 524/785 [03:23<01:15,  3.46it/s] 67%|██████▋   | 525/785 [03:23<01:15,  3.46it/s] 67%|██████▋   | 526/785 [03:23<01:14,  3.46it/s] 67%|██████▋   | 527/785 [03:23<01:14,  3.46it/s] 67%|██████▋   | 528/785 [03:24<01:14,  3.45it/s] 67%|██████▋   | 529/785 [03:24<01:14,  3.45it/s] 68%|██████▊   | 530/785 [03:24<01:13,  3.45it/s] 68%|██████▊   | 531/785 [03:25<01:13,  3.46it/s] 68%|██████▊   | 532/785 [03:25<01:13,  3.45it/s] 68%|██████▊   | 533/785 [03:25<01:12,  3.45it/s] 68%|██████▊   | 534/785 [03:25<01:12,  3.46it/s] 68%|██████▊   | 535/785 [03:26<01:12,  3.46it/s] 68%|██████▊   | 536/785 [03:26<01:12,  3.46it/s] 68%|██████▊   | 537/785 [03:26<01:11,  3.46it/s] 69%|██████▊   | 538/785 [03:27<01:11,  3.46it/s] 69%|██████▊   | 539/785 [03:27<01:11,  3.44it/s] 69%|██████▉   | 540/785 [03:27<01:11,  3.45it/s] 69%|██████▉   | 541/785 [03:27<01:10,  3.45it/s] 69%|██████▉   | 542/785 [03:28<01:10,  3.45it/s] 69%|██████▉   | 543/785 [03:28<01:10,  3.44it/s] 69%|██████▉   | 544/785 [03:28<01:09,  3.45it/s] 69%|██████▉   | 545/785 [03:29<01:09,  3.45it/s] 70%|██████▉   | 546/785 [03:29<01:09,  3.45it/s] 70%|██████▉   | 547/785 [03:29<01:08,  3.45it/s] 70%|██████▉   | 548/785 [03:29<01:08,  3.46it/s] 70%|██████▉   | 549/785 [03:30<01:08,  3.46it/s] 70%|███████   | 550/785 [03:30<01:07,  3.46it/s] 70%|███████   | 551/785 [03:30<01:07,  3.46it/s] 70%|███████   | 552/785 [03:31<01:07,  3.46it/s] 70%|███████   | 553/785 [03:31<01:07,  3.45it/s] 71%|███████   | 554/785 [03:31<01:06,  3.46it/s] 71%|███████   | 555/785 [03:31<01:06,  3.46it/s] 71%|███████   | 556/785 [03:32<01:06,  3.45it/s] 71%|███████   | 557/785 [03:32<01:06,  3.45it/s] 71%|███████   | 558/785 [03:32<01:05,  3.46it/s] 71%|███████   | 559/785 [03:33<01:05,  3.46it/s] 71%|███████▏  | 560/785 [03:33<01:05,  3.46it/s] 71%|███████▏  | 561/785 [03:33<01:05,  3.44it/s] 72%|███████▏  | 562/785 [03:34<01:04,  3.45it/s] 72%|███████▏  | 563/785 [03:34<01:04,  3.45it/s] 72%|███████▏  | 564/785 [03:34<01:04,  3.45it/s] 72%|███████▏  | 565/785 [03:34<01:03,  3.45it/s] 72%|███████▏  | 566/785 [03:35<01:03,  3.46it/s] 72%|███████▏  | 567/785 [03:35<01:03,  3.45it/s] 72%|███████▏  | 568/785 [03:35<01:02,  3.46it/s] 72%|███████▏  | 569/785 [03:36<01:02,  3.46it/s] 73%|███████▎  | 570/785 [03:36<01:02,  3.45it/s] 73%|███████▎  | 571/785 [03:36<01:01,  3.46it/s] 73%|███████▎  | 572/785 [03:36<01:01,  3.45it/s] 73%|███████▎  | 573/785 [03:37<01:01,  3.45it/s] 73%|███████▎  | 574/785 [03:37<01:01,  3.45it/s] 73%|███████▎  | 575/785 [03:37<01:00,  3.45it/s] 73%|███████▎  | 576/785 [03:38<01:00,  3.46it/s] 74%|███████▎  | 577/785 [03:38<01:00,  3.46it/s] 74%|███████▎  | 578/785 [03:38<00:59,  3.45it/s] 74%|███████▍  | 579/785 [03:38<00:59,  3.45it/s] 74%|███████▍  | 580/785 [03:39<00:59,  3.45it/s] 74%|███████▍  | 581/785 [03:39<00:59,  3.46it/s] 74%|███████▍  | 582/785 [03:39<00:58,  3.45it/s] 74%|███████▍  | 583/785 [03:40<00:58,  3.44it/s] 74%|███████▍  | 584/785 [03:40<00:58,  3.45it/s] 75%|███████▍  | 585/785 [03:40<00:57,  3.45it/s] 75%|███████▍  | 586/785 [03:40<00:57,  3.45it/s] 75%|███████▍  | 587/785 [03:41<00:57,  3.45it/s] 75%|███████▍  | 588/785 [03:41<00:57,  3.45it/s] 75%|███████▌  | 589/785 [03:41<00:56,  3.46it/s] 75%|███████▌  | 590/785 [03:42<00:56,  3.45it/s] 75%|███████▌  | 591/785 [03:42<00:56,  3.45it/s] 75%|███████▌  | 592/785 [03:42<00:55,  3.45it/s] 76%|███████▌  | 593/785 [03:42<00:55,  3.46it/s] 76%|███████▌  | 594/785 [03:43<00:55,  3.45it/s] 76%|███████▌  | 595/785 [03:43<00:54,  3.46it/s] 76%|███████▌  | 596/785 [03:43<00:54,  3.45it/s] 76%|███████▌  | 597/785 [03:44<00:54,  3.45it/s] 76%|███████▌  | 598/785 [03:44<00:54,  3.45it/s] 76%|███████▋  | 599/785 [03:44<00:53,  3.46it/s] 76%|███████▋  | 600/785 [03:45<00:53,  3.46it/s] 77%|███████▋  | 601/785 [03:45<00:53,  3.46it/s] 77%|███████▋  | 602/785 [03:45<00:52,  3.46it/s] 77%|███████▋  | 603/785 [03:45<00:52,  3.46it/s] 77%|███████▋  | 604/785 [03:46<00:52,  3.46it/s] 77%|███████▋  | 605/785 [03:46<00:52,  3.46it/s] 77%|███████▋  | 606/785 [03:46<00:51,  3.45it/s] 77%|███████▋  | 607/785 [03:47<00:51,  3.45it/s] 77%|███████▋  | 608/785 [03:47<00:51,  3.46it/s] 78%|███████▊  | 609/785 [03:47<00:50,  3.45it/s] 78%|███████▊  | 610/785 [03:47<00:50,  3.46it/s] 78%|███████▊  | 611/785 [03:48<00:50,  3.46it/s] 78%|███████▊  | 612/785 [03:48<00:50,  3.46it/s] 78%|███████▊  | 613/785 [03:48<00:49,  3.46it/s] 78%|███████▊  | 614/785 [03:49<00:50,  3.40it/s] 78%|███████▊  | 615/785 [03:49<00:49,  3.42it/s] 78%|███████▊  | 616/785 [03:49<00:49,  3.43it/s] 79%|███████▊  | 617/785 [03:49<00:48,  3.44it/s] 79%|███████▊  | 618/785 [03:50<00:48,  3.44it/s] 79%|███████▉  | 619/785 [03:50<00:49,  3.34it/s] 79%|███████▉  | 620/785 [03:50<00:49,  3.36it/s] 79%|███████▉  | 621/785 [03:51<00:48,  3.39it/s] 79%|███████▉  | 622/785 [03:51<00:47,  3.41it/s] 79%|███████▉  | 623/785 [03:51<00:47,  3.43it/s] 79%|███████▉  | 624/785 [03:52<00:46,  3.43it/s] 80%|███████▉  | 625/785 [03:52<00:46,  3.44it/s] 80%|███████▉  | 626/785 [03:52<00:46,  3.44it/s] 80%|███████▉  | 627/785 [03:52<00:45,  3.45it/s] 80%|████████  | 628/785 [03:53<00:42,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 14:10:07,775 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:10:07,775 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:10:07,775 >>   Batch size = 8
{'eval_loss': 0.9580727219581604, 'eval_runtime': 9.3911, 'eval_samples_per_second': 372.374, 'eval_steps_per_second': 46.64, 'epoch': 3.0}
{'loss': 0.8241, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.71it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.59it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.88it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.14it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.76it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.45it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.13it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.82it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.62it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.69it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.72it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.79it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.82it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.72it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.82it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.83it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.62it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.50it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.52it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.66it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.67it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.71it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.83it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.78it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.67it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.73it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.60it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.39it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.53it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.58it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.64it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.71it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.61it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.70it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.69it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.61it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.64it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.68it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.49it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.66it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.74it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.70it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.67it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.65it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.69it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.69it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.66it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.65it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.61it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.61it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.74it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.44it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.51it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.61it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.57it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.65it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.63it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.55it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.62it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.68it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.63it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.69it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.68it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.62it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.67it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.69it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.60it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.59it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.64it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.66it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.68it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.71it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.63it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.64it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.71it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.66it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.54it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.71it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.64it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.69it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.66it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.64it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.64it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.66it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.61it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.65it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.66it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:02<00:42,  3.73it/s]
100%|██████████| 438/438 [00:09<00:00, 46.66it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:10:17,174 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-28 14:10:17,203 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:10:19,756 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:10:19,784 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:10:19,794 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:10<13:55,  5.35s/it] 80%|████████  | 630/785 [04:10<09:54,  3.84s/it] 80%|████████  | 631/785 [04:10<07:06,  2.77s/it] 81%|████████  | 632/785 [04:11<05:10,  2.03s/it] 81%|████████  | 633/785 [04:11<03:48,  1.51s/it] 81%|████████  | 634/785 [04:11<02:52,  1.14s/it] 81%|████████  | 635/785 [04:12<02:12,  1.13it/s] 81%|████████  | 636/785 [04:12<01:45,  1.42it/s] 81%|████████  | 637/785 [04:12<01:25,  1.72it/s] 81%|████████▏ | 638/785 [04:12<01:12,  2.03it/s] 81%|████████▏ | 639/785 [04:13<01:03,  2.32it/s] 82%|████████▏ | 640/785 [04:13<00:56,  2.57it/s] 82%|████████▏ | 641/785 [04:13<00:51,  2.78it/s] 82%|████████▏ | 642/785 [04:14<00:48,  2.96it/s] 82%|████████▏ | 643/785 [04:14<00:45,  3.09it/s] 82%|████████▏ | 644/785 [04:14<00:44,  3.20it/s] 82%|████████▏ | 645/785 [04:14<00:42,  3.27it/s] 82%|████████▏ | 646/785 [04:15<00:41,  3.33it/s] 82%|████████▏ | 647/785 [04:15<00:40,  3.37it/s] 83%|████████▎ | 648/785 [04:15<00:40,  3.39it/s] 83%|████████▎ | 649/785 [04:16<00:39,  3.42it/s] 83%|████████▎ | 650/785 [04:16<00:39,  3.43it/s] 83%|████████▎ | 651/785 [04:16<00:38,  3.44it/s] 83%|████████▎ | 652/785 [04:16<00:38,  3.43it/s] 83%|████████▎ | 653/785 [04:17<00:38,  3.44it/s] 83%|████████▎ | 654/785 [04:17<00:38,  3.44it/s] 83%|████████▎ | 655/785 [04:17<00:37,  3.45it/s] 84%|████████▎ | 656/785 [04:18<00:37,  3.46it/s] 84%|████████▎ | 657/785 [04:18<00:37,  3.46it/s] 84%|████████▍ | 658/785 [04:18<00:36,  3.46it/s] 84%|████████▍ | 659/785 [04:18<00:36,  3.46it/s] 84%|████████▍ | 660/785 [04:19<00:36,  3.46it/s] 84%|████████▍ | 661/785 [04:19<00:35,  3.46it/s] 84%|████████▍ | 662/785 [04:19<00:35,  3.46it/s] 84%|████████▍ | 663/785 [04:20<00:35,  3.45it/s] 85%|████████▍ | 664/785 [04:20<00:35,  3.45it/s] 85%|████████▍ | 665/785 [04:20<00:34,  3.45it/s] 85%|████████▍ | 666/785 [04:21<00:34,  3.46it/s] 85%|████████▍ | 667/785 [04:21<00:34,  3.46it/s] 85%|████████▌ | 668/785 [04:21<00:33,  3.46it/s] 85%|████████▌ | 669/785 [04:21<00:33,  3.46it/s] 85%|████████▌ | 670/785 [04:22<00:33,  3.46it/s] 85%|████████▌ | 671/785 [04:22<00:32,  3.46it/s] 86%|████████▌ | 672/785 [04:22<00:32,  3.46it/s] 86%|████████▌ | 673/785 [04:23<00:32,  3.46it/s] 86%|████████▌ | 674/785 [04:23<00:32,  3.45it/s] 86%|████████▌ | 675/785 [04:23<00:31,  3.45it/s] 86%|████████▌ | 676/785 [04:23<00:31,  3.46it/s] 86%|████████▌ | 677/785 [04:24<00:31,  3.46it/s] 86%|████████▋ | 678/785 [04:24<00:30,  3.46it/s] 86%|████████▋ | 679/785 [04:24<00:30,  3.46it/s] 87%|████████▋ | 680/785 [04:25<00:30,  3.46it/s] 87%|████████▋ | 681/785 [04:25<00:30,  3.46it/s] 87%|████████▋ | 682/785 [04:25<00:29,  3.46it/s] 87%|████████▋ | 683/785 [04:25<00:29,  3.45it/s] 87%|████████▋ | 684/785 [04:26<00:29,  3.45it/s] 87%|████████▋ | 685/785 [04:26<00:29,  3.44it/s] 87%|████████▋ | 686/785 [04:26<00:28,  3.45it/s] 88%|████████▊ | 687/785 [04:27<00:28,  3.45it/s] 88%|████████▊ | 688/785 [04:27<00:28,  3.45it/s] 88%|████████▊ | 689/785 [04:27<00:27,  3.45it/s] 88%|████████▊ | 690/785 [04:27<00:27,  3.45it/s] 88%|████████▊ | 691/785 [04:28<00:27,  3.46it/s] 88%|████████▊ | 692/785 [04:28<00:26,  3.46it/s] 88%|████████▊ | 693/785 [04:28<00:26,  3.46it/s] 88%|████████▊ | 694/785 [04:29<00:26,  3.46it/s] 89%|████████▊ | 695/785 [04:29<00:26,  3.46it/s] 89%|████████▊ | 696/785 [04:29<00:25,  3.45it/s] 89%|████████▉ | 697/785 [04:29<00:25,  3.45it/s] 89%|████████▉ | 698/785 [04:30<00:25,  3.46it/s] 89%|████████▉ | 699/785 [04:30<00:24,  3.46it/s] 89%|████████▉ | 700/785 [04:30<00:24,  3.46it/s] 89%|████████▉ | 701/785 [04:31<00:24,  3.46it/s] 89%|████████▉ | 702/785 [04:31<00:23,  3.46it/s] 90%|████████▉ | 703/785 [04:31<00:23,  3.46it/s] 90%|████████▉ | 704/785 [04:32<00:23,  3.46it/s] 90%|████████▉ | 705/785 [04:32<00:23,  3.46it/s] 90%|████████▉ | 706/785 [04:32<00:22,  3.46it/s] 90%|█████████ | 707/785 [04:32<00:22,  3.46it/s] 90%|█████████ | 708/785 [04:33<00:22,  3.46it/s] 90%|█████████ | 709/785 [04:33<00:21,  3.46it/s] 90%|█████████ | 710/785 [04:33<00:21,  3.46it/s] 91%|█████████ | 711/785 [04:34<00:21,  3.46it/s] 91%|█████████ | 712/785 [04:34<00:21,  3.46it/s] 91%|█████████ | 713/785 [04:34<00:20,  3.46it/s] 91%|█████████ | 714/785 [04:34<00:20,  3.46it/s] 91%|█████████ | 715/785 [04:35<00:20,  3.45it/s] 91%|█████████ | 716/785 [04:35<00:19,  3.45it/s] 91%|█████████▏| 717/785 [04:35<00:19,  3.45it/s] 91%|█████████▏| 718/785 [04:36<00:19,  3.45it/s] 92%|█████████▏| 719/785 [04:36<00:19,  3.45it/s] 92%|█████████▏| 720/785 [04:36<00:18,  3.46it/s] 92%|█████████▏| 721/785 [04:36<00:18,  3.46it/s] 92%|█████████▏| 722/785 [04:37<00:18,  3.46it/s] 92%|█████████▏| 723/785 [04:37<00:17,  3.46it/s] 92%|█████████▏| 724/785 [04:37<00:17,  3.46it/s] 92%|█████████▏| 725/785 [04:38<00:17,  3.46it/s] 92%|█████████▏| 726/785 [04:38<00:17,  3.45it/s] 93%|█████████▎| 727/785 [04:38<00:16,  3.45it/s] 93%|█████████▎| 728/785 [04:38<00:16,  3.45it/s] 93%|█████████▎| 729/785 [04:39<00:16,  3.45it/s] 93%|█████████▎| 730/785 [04:39<00:15,  3.45it/s] 93%|█████████▎| 731/785 [04:39<00:15,  3.45it/s] 93%|█████████▎| 732/785 [04:40<00:15,  3.45it/s] 93%|█████████▎| 733/785 [04:40<00:15,  3.46it/s] 94%|█████████▎| 734/785 [04:40<00:14,  3.46it/s] 94%|█████████▎| 735/785 [04:40<00:14,  3.46it/s] 94%|█████████▍| 736/785 [04:41<00:14,  3.46it/s] 94%|█████████▍| 737/785 [04:41<00:13,  3.45it/s] 94%|█████████▍| 738/785 [04:41<00:13,  3.45it/s] 94%|█████████▍| 739/785 [04:42<00:13,  3.45it/s] 94%|█████████▍| 740/785 [04:42<00:13,  3.46it/s] 94%|█████████▍| 741/785 [04:42<00:12,  3.46it/s] 95%|█████████▍| 742/785 [04:42<00:12,  3.46it/s] 95%|█████████▍| 743/785 [04:43<00:12,  3.46it/s] 95%|█████████▍| 744/785 [04:43<00:11,  3.46it/s] 95%|█████████▍| 745/785 [04:43<00:11,  3.46it/s] 95%|█████████▌| 746/785 [04:44<00:11,  3.45it/s] 95%|█████████▌| 747/785 [04:44<00:11,  3.45it/s] 95%|█████████▌| 748/785 [04:44<00:10,  3.44it/s] 95%|█████████▌| 749/785 [04:45<00:10,  3.45it/s] 96%|█████████▌| 750/785 [04:45<00:10,  3.45it/s] 96%|█████████▌| 751/785 [04:45<00:09,  3.45it/s] 96%|█████████▌| 752/785 [04:45<00:09,  3.46it/s] 96%|█████████▌| 753/785 [04:46<00:09,  3.46it/s] 96%|█████████▌| 754/785 [04:46<00:08,  3.46it/s] 96%|█████████▌| 755/785 [04:46<00:08,  3.46it/s] 96%|█████████▋| 756/785 [04:47<00:08,  3.46it/s] 96%|█████████▋| 757/785 [04:47<00:08,  3.46it/s] 97%|█████████▋| 758/785 [04:47<00:07,  3.46it/s] 97%|█████████▋| 759/785 [04:47<00:07,  3.45it/s] 97%|█████████▋| 760/785 [04:48<00:07,  3.45it/s] 97%|█████████▋| 761/785 [04:48<00:06,  3.45it/s] 97%|█████████▋| 762/785 [04:48<00:06,  3.45it/s] 97%|█████████▋| 763/785 [04:49<00:06,  3.43it/s] 97%|█████████▋| 764/785 [04:49<00:06,  3.44it/s] 97%|█████████▋| 765/785 [04:49<00:05,  3.45it/s] 98%|█████████▊| 766/785 [04:49<00:05,  3.45it/s] 98%|█████████▊| 767/785 [04:50<00:05,  3.45it/s] 98%|█████████▊| 768/785 [04:50<00:05,  3.38it/s] 98%|█████████▊| 769/785 [04:50<00:04,  3.40it/s] 98%|█████████▊| 770/785 [04:51<00:04,  3.40it/s] 98%|█████████▊| 771/785 [04:51<00:04,  3.42it/s] 98%|█████████▊| 772/785 [04:51<00:03,  3.43it/s] 98%|█████████▊| 773/785 [04:52<00:03,  3.44it/s] 99%|█████████▊| 774/785 [04:52<00:03,  3.44it/s] 99%|█████████▊| 775/785 [04:52<00:02,  3.45it/s] 99%|█████████▉| 776/785 [04:52<00:02,  3.45it/s] 99%|█████████▉| 777/785 [04:53<00:02,  3.45it/s] 99%|█████████▉| 778/785 [04:53<00:02,  3.45it/s] 99%|█████████▉| 779/785 [04:53<00:01,  3.46it/s] 99%|█████████▉| 780/785 [04:54<00:01,  3.45it/s] 99%|█████████▉| 781/785 [04:54<00:01,  3.44it/s]100%|█████████▉| 782/785 [04:54<00:00,  3.45it/s]100%|█████████▉| 783/785 [04:54<00:00,  3.45it/s]100%|█████████▉| 784/785 [04:55<00:00,  3.45it/s]100%|██████████| 785/785 [04:55<00:00,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 14:11:10,090 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:11:10,091 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:11:10,091 >>   Batch size = 8
{'eval_loss': 0.9621087312698364, 'eval_runtime': 9.3799, 'eval_samples_per_second': 372.817, 'eval_steps_per_second': 46.695, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.29it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.68it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.79it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.10it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.65it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.36it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.12it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.72it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.61it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.66it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.73it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.72it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.79it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.73it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.69it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.66it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.52it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.47it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.52it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.61it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.62it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.75it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.67it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.65it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.61it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.52it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.49it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.57it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.58it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.66it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.70it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.68it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.67it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.68it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.56it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.51it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.45it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.50it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.58it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.66it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.54it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.64it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.68it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.59it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.54it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.57it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.60it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.71it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.66it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.61it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.55it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.55it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.59it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.58it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.51it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.55it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.63it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.64it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.57it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.56it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.57it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.58it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.59it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.61it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.58it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.57it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.62it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.58it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.61it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.60it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.57it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.60it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.57it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.56it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.62it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.66it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.68it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.69it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.57it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.56it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.58it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.57it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.64it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.57it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.53it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.66it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.62it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:04<00:00,  3.74it/s]
100%|██████████| 438/438 [00:09<00:00, 46.62it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:11:19,510 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-28 14:11:19,530 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:11:22,099 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:11:22,117 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:11:22,126 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:11:27,149 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:11:27,153 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314 (score: 0.9560226202011108).
                                                 100%|██████████| 785/785 [05:14<00:00,  3.74it/s]100%|██████████| 785/785 [05:14<00:00,  2.50it/s]
[INFO|trainer.py:1894] 2023-08-28 14:11:29,031 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 14:11:29,045 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:11:31,231 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:11:31,252 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:11:31,262 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:11:31,438 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:31,438 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:31,438 >>   train_loss               =     0.8092
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:31,438 >>   train_runtime            = 0:05:14.34
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:31,438 >>   train_samples            =      10029
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:31,438 >>   train_samples_per_second =    159.522
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:31,438 >>   train_steps_per_second   =      2.497
{'eval_loss': 0.9651848673820496, 'eval_runtime': 9.3895, 'eval_samples_per_second': 372.439, 'eval_steps_per_second': 46.648, 'epoch': 5.0}
{'train_runtime': 314.345, 'train_samples_per_second': 159.522, 'train_steps_per_second': 2.497, 'train_loss': 0.8091919528450936, 'epoch': 5.0}
08/28/2023 14:11:31 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:11:31,478 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:11:31,478 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:11:31,478 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 58.62it/s]  3%|▎         | 12/438 [00:00<00:08, 51.28it/s]  4%|▍         | 18/438 [00:00<00:08, 49.33it/s]  5%|▌         | 23/438 [00:00<00:08, 48.55it/s]  6%|▋         | 28/438 [00:00<00:08, 48.02it/s]  8%|▊         | 33/438 [00:00<00:08, 47.78it/s]  9%|▊         | 38/438 [00:00<00:08, 47.54it/s] 10%|▉         | 43/438 [00:00<00:08, 47.32it/s] 11%|█         | 48/438 [00:00<00:08, 46.82it/s] 12%|█▏        | 53/438 [00:01<00:08, 46.79it/s] 13%|█▎        | 58/438 [00:01<00:08, 46.80it/s] 14%|█▍        | 63/438 [00:01<00:07, 46.91it/s] 16%|█▌        | 68/438 [00:01<00:07, 46.94it/s] 17%|█▋        | 73/438 [00:01<00:07, 46.99it/s] 18%|█▊        | 78/438 [00:01<00:07, 46.95it/s] 19%|█▉        | 83/438 [00:01<00:07, 46.98it/s] 20%|██        | 88/438 [00:01<00:07, 46.80it/s] 21%|██        | 93/438 [00:01<00:07, 46.71it/s] 22%|██▏       | 98/438 [00:02<00:07, 46.60it/s] 24%|██▎       | 103/438 [00:02<00:07, 46.66it/s] 25%|██▍       | 108/438 [00:02<00:07, 46.57it/s] 26%|██▌       | 113/438 [00:02<00:06, 46.76it/s] 27%|██▋       | 118/438 [00:02<00:06, 46.92it/s] 28%|██▊       | 123/438 [00:02<00:06, 46.94it/s] 29%|██▉       | 128/438 [00:02<00:06, 46.94it/s] 30%|███       | 133/438 [00:02<00:06, 46.82it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.73it/s] 33%|███▎      | 143/438 [00:03<00:06, 46.63it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.61it/s] 35%|███▍      | 153/438 [00:03<00:06, 46.59it/s] 36%|███▌      | 158/438 [00:03<00:05, 46.68it/s] 37%|███▋      | 163/438 [00:03<00:05, 46.78it/s] 38%|███▊      | 168/438 [00:03<00:05, 46.91it/s] 39%|███▉      | 173/438 [00:03<00:05, 46.95it/s] 41%|████      | 178/438 [00:03<00:05, 46.91it/s] 42%|████▏     | 183/438 [00:03<00:05, 46.70it/s] 43%|████▎     | 188/438 [00:03<00:05, 46.70it/s] 44%|████▍     | 193/438 [00:04<00:05, 46.68it/s] 45%|████▌     | 198/438 [00:04<00:05, 46.67it/s] 46%|████▋     | 203/438 [00:04<00:05, 46.57it/s] 47%|████▋     | 208/438 [00:04<00:04, 46.68it/s] 49%|████▊     | 213/438 [00:04<00:04, 46.73it/s] 50%|████▉     | 218/438 [00:04<00:04, 46.86it/s] 51%|█████     | 223/438 [00:04<00:04, 46.69it/s] 52%|█████▏    | 228/438 [00:04<00:04, 46.84it/s] 53%|█████▎    | 233/438 [00:04<00:04, 46.66it/s] 54%|█████▍    | 238/438 [00:05<00:04, 46.67it/s] 55%|█████▌    | 243/438 [00:05<00:04, 46.67it/s] 57%|█████▋    | 248/438 [00:05<00:04, 46.69it/s] 58%|█████▊    | 253/438 [00:05<00:03, 46.66it/s] 59%|█████▉    | 258/438 [00:05<00:03, 46.74it/s] 60%|██████    | 263/438 [00:05<00:03, 46.82it/s] 61%|██████    | 268/438 [00:05<00:03, 46.84it/s] 62%|██████▏   | 273/438 [00:05<00:03, 46.83it/s] 63%|██████▎   | 278/438 [00:05<00:03, 46.69it/s] 65%|██████▍   | 283/438 [00:06<00:03, 46.63it/s] 66%|██████▌   | 288/438 [00:06<00:03, 46.64it/s] 67%|██████▋   | 293/438 [00:06<00:03, 46.65it/s] 68%|██████▊   | 298/438 [00:06<00:02, 46.70it/s] 69%|██████▉   | 303/438 [00:06<00:02, 46.83it/s] 70%|███████   | 308/438 [00:06<00:02, 46.76it/s] 71%|███████▏  | 313/438 [00:06<00:02, 46.82it/s] 73%|███████▎  | 318/438 [00:06<00:02, 46.81it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.75it/s] 75%|███████▍  | 328/438 [00:06<00:02, 46.70it/s] 76%|███████▌  | 333/438 [00:07<00:02, 46.68it/s] 77%|███████▋  | 338/438 [00:07<00:02, 46.61it/s] 78%|███████▊  | 343/438 [00:07<00:02, 46.73it/s] 79%|███████▉  | 348/438 [00:07<00:01, 46.65it/s] 81%|████████  | 353/438 [00:07<00:01, 46.78it/s] 82%|████████▏ | 358/438 [00:07<00:01, 46.78it/s] 83%|████████▎ | 363/438 [00:07<00:01, 46.83it/s] 84%|████████▍ | 368/438 [00:07<00:01, 46.71it/s] 85%|████████▌ | 373/438 [00:07<00:01, 46.73it/s] 86%|████████▋ | 378/438 [00:08<00:01, 46.61it/s] 87%|████████▋ | 383/438 [00:08<00:01, 46.62it/s] 89%|████████▊ | 388/438 [00:08<00:01, 46.68it/s] 90%|████████▉ | 393/438 [00:08<00:00, 46.71it/s] 91%|█████████ | 398/438 [00:08<00:00, 46.74it/s] 92%|█████████▏| 403/438 [00:08<00:00, 46.79it/s] 93%|█████████▎| 408/438 [00:08<00:00, 46.70it/s] 94%|█████████▍| 413/438 [00:08<00:00, 46.76it/s] 95%|█████████▌| 418/438 [00:08<00:00, 46.66it/s] 97%|█████████▋| 423/438 [00:09<00:00, 46.68it/s] 98%|█████████▊| 428/438 [00:09<00:00, 46.58it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.60it/s]100%|██████████| 438/438 [00:09<00:00, 46.76it/s]100%|██████████| 438/438 [00:09<00:00, 46.89it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:11:40,843 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:40,843 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:40,843 >>   eval_loss               =      0.956
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:40,843 >>   eval_runtime            = 0:00:09.36
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:40,843 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:40,843 >>   eval_samples_per_second =     373.41
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:40,843 >>   eval_steps_per_second   =      46.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:11:40,843 >>   perplexity              =     2.6013
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:46,580 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:46,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:46,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:46,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:46,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:11:47,296 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:11:47,297 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:11:47,558 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:11:48,604 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:11:48,604 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:49,961 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:49,963 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:49,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:49,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:11:49,964 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:11:50,688 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:11:50,689 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:11:50,949 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:11:51,115 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:11:51,115 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 2it [00:01,  1.69it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:06,  1.55it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.58it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:13,  1.53it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:15,  1.58it/s]Extractor Predicting: 26it [00:16,  1.64it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:18,  1.65it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:19,  1.64it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:23,  1.55it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:25,  1.53it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:27,  1.56it/s]Extractor Predicting: 45it [00:28,  1.56it/s]Extractor Predicting: 46it [00:28,  1.60it/s]Extractor Predicting: 47it [00:29,  1.58it/s]Extractor Predicting: 48it [00:30,  1.45it/s]Extractor Predicting: 49it [00:31,  1.45it/s]Extractor Predicting: 50it [00:31,  1.45it/s]Extractor Predicting: 51it [00:32,  1.46it/s]Extractor Predicting: 52it [00:33,  1.46it/s]Extractor Predicting: 53it [00:33,  1.48it/s]Extractor Predicting: 54it [00:34,  1.49it/s]Extractor Predicting: 55it [00:35,  1.51it/s]Extractor Predicting: 56it [00:35,  1.52it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:38,  1.50it/s]Extractor Predicting: 61it [00:39,  1.48it/s]Extractor Predicting: 62it [00:39,  1.46it/s]Extractor Predicting: 63it [00:40,  1.45it/s]Extractor Predicting: 64it [00:41,  1.46it/s]Extractor Predicting: 65it [00:41,  1.45it/s]Extractor Predicting: 66it [00:42,  1.43it/s]Extractor Predicting: 67it [00:43,  1.42it/s]Extractor Predicting: 68it [00:44,  1.42it/s]Extractor Predicting: 69it [00:44,  1.42it/s]Extractor Predicting: 70it [00:45,  1.42it/s]Extractor Predicting: 71it [00:46,  1.41it/s]Extractor Predicting: 72it [00:46,  1.41it/s]Extractor Predicting: 73it [00:47,  1.39it/s]Extractor Predicting: 74it [00:48,  1.40it/s]Extractor Predicting: 75it [00:49,  1.39it/s]Extractor Predicting: 76it [00:49,  1.42it/s]Extractor Predicting: 77it [00:50,  1.42it/s]Extractor Predicting: 78it [00:51,  1.41it/s]Extractor Predicting: 79it [00:51,  1.43it/s]Extractor Predicting: 80it [00:52,  1.41it/s]Extractor Predicting: 81it [00:53,  1.38it/s]Extractor Predicting: 82it [00:54,  1.39it/s]Extractor Predicting: 83it [00:54,  1.43it/s]Extractor Predicting: 84it [00:55,  1.44it/s]Extractor Predicting: 85it [00:56,  1.45it/s]Extractor Predicting: 86it [00:56,  1.45it/s]Extractor Predicting: 87it [00:57,  1.41it/s]Extractor Predicting: 88it [00:58,  1.42it/s]Extractor Predicting: 89it [00:58,  1.45it/s]Extractor Predicting: 90it [00:59,  1.47it/s]Extractor Predicting: 91it [01:00,  1.49it/s]Extractor Predicting: 92it [01:00,  1.46it/s]Extractor Predicting: 93it [01:01,  1.49it/s]Extractor Predicting: 94it [01:02,  1.50it/s]Extractor Predicting: 95it [01:02,  1.49it/s]Extractor Predicting: 96it [01:03,  1.48it/s]Extractor Predicting: 97it [01:04,  1.49it/s]Extractor Predicting: 98it [01:04,  1.47it/s]Extractor Predicting: 99it [01:05,  1.49it/s]Extractor Predicting: 100it [01:06,  1.50it/s]Extractor Predicting: 101it [01:06,  1.51it/s]Extractor Predicting: 102it [01:07,  1.48it/s]Extractor Predicting: 103it [01:08,  1.44it/s]Extractor Predicting: 104it [01:08,  1.46it/s]Extractor Predicting: 105it [01:09,  1.47it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:10,  1.44it/s]Extractor Predicting: 108it [01:11,  1.46it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:13,  1.43it/s]Extractor Predicting: 111it [01:13,  1.43it/s]Extractor Predicting: 112it [01:14,  1.44it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:15,  1.45it/s]Extractor Predicting: 115it [01:16,  1.45it/s]Extractor Predicting: 116it [01:17,  1.45it/s]Extractor Predicting: 117it [01:17,  1.48it/s]Extractor Predicting: 118it [01:18,  1.51it/s]Extractor Predicting: 119it [01:19,  1.55it/s]Extractor Predicting: 120it [01:19,  1.57it/s]Extractor Predicting: 121it [01:20,  1.55it/s]Extractor Predicting: 122it [01:20,  1.59it/s]Extractor Predicting: 123it [01:21,  1.60it/s]Extractor Predicting: 124it [01:22,  1.58it/s]Extractor Predicting: 125it [01:22,  1.60it/s]Extractor Predicting: 126it [01:23,  1.61it/s]Extractor Predicting: 127it [01:24,  1.65it/s]Extractor Predicting: 128it [01:24,  1.64it/s]Extractor Predicting: 129it [01:25,  1.66it/s]Extractor Predicting: 130it [01:25,  1.61it/s]Extractor Predicting: 131it [01:26,  1.61it/s]Extractor Predicting: 132it [01:27,  1.67it/s]Extractor Predicting: 133it [01:27,  1.54it/s]Extractor Predicting: 134it [01:28,  1.51it/s]Extractor Predicting: 135it [01:29,  1.56it/s]Extractor Predicting: 136it [01:29,  1.61it/s]Extractor Predicting: 137it [01:30,  1.64it/s]Extractor Predicting: 138it [01:30,  1.63it/s]Extractor Predicting: 139it [01:31,  1.64it/s]Extractor Predicting: 140it [01:32,  1.69it/s]Extractor Predicting: 141it [01:32,  1.71it/s]Extractor Predicting: 142it [01:33,  1.74it/s]Extractor Predicting: 143it [01:33,  1.70it/s]Extractor Predicting: 144it [01:34,  1.67it/s]Extractor Predicting: 145it [01:34,  1.87it/s]Extractor Predicting: 145it [01:34,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:34,942 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:34,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:34,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:34,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:34,947 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:13:35,706 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:13:35,707 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:13:36,299 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:13:37,335 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:13:37,335 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:40,131 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:40,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:40,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:40,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:13:40,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:13:40,783 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:13:40,784 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:13:41,354 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:13:41,519 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:13:41,520 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6891666666666667,
  "recall": 0.23648841864455247,
  "score": 0.35213966361507343,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.51it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.50it/s]Extractor Predicting: 20it [00:13,  1.50it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.47it/s]Extractor Predicting: 23it [00:15,  1.49it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.51it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.47it/s]Extractor Predicting: 30it [00:19,  1.44it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:21,  1.45it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:22,  1.49it/s]Extractor Predicting: 35it [00:23,  1.50it/s]Extractor Predicting: 36it [00:23,  1.51it/s]Extractor Predicting: 37it [00:24,  1.57it/s]Extractor Predicting: 38it [00:25,  1.56it/s]Extractor Predicting: 39it [00:25,  1.55it/s]Extractor Predicting: 40it [00:26,  1.56it/s]Extractor Predicting: 41it [00:27,  1.56it/s]Extractor Predicting: 42it [00:27,  1.43it/s]Extractor Predicting: 43it [00:28,  1.46it/s]Extractor Predicting: 44it [00:29,  1.48it/s]Extractor Predicting: 45it [00:29,  1.48it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:31,  1.52it/s]Extractor Predicting: 48it [00:31,  1.52it/s]Extractor Predicting: 49it [00:32,  1.52it/s]Extractor Predicting: 50it [00:33,  1.51it/s]Extractor Predicting: 51it [00:33,  1.52it/s]Extractor Predicting: 52it [00:34,  1.52it/s]Extractor Predicting: 53it [00:35,  1.53it/s]Extractor Predicting: 54it [00:35,  1.54it/s]Extractor Predicting: 55it [00:36,  1.50it/s]Extractor Predicting: 56it [00:37,  1.52it/s]Extractor Predicting: 57it [00:37,  1.55it/s]Extractor Predicting: 58it [00:38,  1.57it/s]Extractor Predicting: 59it [00:38,  1.57it/s]Extractor Predicting: 60it [00:39,  1.57it/s]Extractor Predicting: 61it [00:40,  1.55it/s]Extractor Predicting: 62it [00:40,  1.56it/s]Extractor Predicting: 63it [00:41,  1.54it/s]Extractor Predicting: 64it [00:42,  1.54it/s]Extractor Predicting: 65it [00:42,  1.52it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:44,  1.53it/s]Extractor Predicting: 68it [00:44,  1.56it/s]Extractor Predicting: 69it [00:45,  1.57it/s]Extractor Predicting: 70it [00:46,  1.56it/s]Extractor Predicting: 71it [00:46,  1.55it/s]Extractor Predicting: 72it [00:47,  1.56it/s]Extractor Predicting: 73it [00:48,  1.51it/s]Extractor Predicting: 74it [00:48,  1.52it/s]Extractor Predicting: 75it [00:49,  1.53it/s]Extractor Predicting: 76it [00:50,  1.55it/s]Extractor Predicting: 77it [00:50,  1.50it/s]Extractor Predicting: 78it [00:51,  1.50it/s]Extractor Predicting: 79it [00:52,  1.51it/s]Extractor Predicting: 80it [00:52,  1.50it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:54,  1.50it/s]Extractor Predicting: 83it [00:54,  1.50it/s]Extractor Predicting: 84it [00:55,  1.50it/s]Extractor Predicting: 85it [00:55,  1.52it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:57,  1.51it/s]Extractor Predicting: 88it [00:58,  1.51it/s]Extractor Predicting: 89it [00:58,  1.51it/s]Extractor Predicting: 90it [00:59,  1.53it/s]Extractor Predicting: 91it [00:59,  1.52it/s]Extractor Predicting: 92it [01:00,  1.51it/s]Extractor Predicting: 93it [01:01,  1.49it/s]Extractor Predicting: 94it [01:01,  1.51it/s]Extractor Predicting: 95it [01:02,  1.50it/s]Extractor Predicting: 96it [01:03,  1.49it/s]Extractor Predicting: 97it [01:03,  1.51it/s]Extractor Predicting: 98it [01:04,  1.52it/s]Extractor Predicting: 99it [01:05,  1.53it/s]Extractor Predicting: 100it [01:05,  1.53it/s]Extractor Predicting: 101it [01:06,  1.55it/s]Extractor Predicting: 102it [01:07,  1.55it/s]Extractor Predicting: 103it [01:07,  1.54it/s]Extractor Predicting: 104it [01:08,  1.50it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:09,  1.49it/s]Extractor Predicting: 107it [01:10,  1.49it/s]Extractor Predicting: 108it [01:11,  1.50it/s]Extractor Predicting: 109it [01:11,  1.48it/s]Extractor Predicting: 110it [01:12,  1.50it/s]Extractor Predicting: 111it [01:13,  1.51it/s]Extractor Predicting: 112it [01:13,  1.48it/s]Extractor Predicting: 113it [01:14,  1.47it/s]Extractor Predicting: 114it [01:15,  1.46it/s]Extractor Predicting: 115it [01:16,  1.43it/s]Extractor Predicting: 116it [01:16,  1.43it/s]Extractor Predicting: 117it [01:17,  1.44it/s]Extractor Predicting: 118it [01:18,  1.45it/s]Extractor Predicting: 119it [01:18,  1.46it/s]Extractor Predicting: 120it [01:19,  1.48it/s]Extractor Predicting: 121it [01:20,  1.51it/s]Extractor Predicting: 122it [01:20,  1.52it/s]Extractor Predicting: 123it [01:21,  1.53it/s]Extractor Predicting: 124it [01:22,  1.52it/s]Extractor Predicting: 125it [01:22,  1.49it/s]Extractor Predicting: 126it [01:23,  1.47it/s]Extractor Predicting: 127it [01:24,  1.47it/s]Extractor Predicting: 128it [01:24,  1.48it/s]Extractor Predicting: 129it [01:25,  1.50it/s]Extractor Predicting: 130it [01:26,  1.50it/s]Extractor Predicting: 131it [01:26,  1.51it/s]Extractor Predicting: 132it [01:27,  1.54it/s]Extractor Predicting: 133it [01:28,  1.51it/s]Extractor Predicting: 134it [01:28,  1.50it/s]Extractor Predicting: 135it [01:29,  1.34it/s]Extractor Predicting: 136it [01:30,  1.38it/s]Extractor Predicting: 137it [01:31,  1.40it/s]Extractor Predicting: 138it [01:31,  1.46it/s]Extractor Predicting: 139it [01:32,  1.47it/s]Extractor Predicting: 140it [01:33,  1.45it/s]Extractor Predicting: 141it [01:33,  1.47it/s]Extractor Predicting: 142it [01:34,  1.49it/s]Extractor Predicting: 143it [01:35,  1.49it/s]Extractor Predicting: 144it [01:35,  1.48it/s]Extractor Predicting: 145it [01:36,  1.50it/s]Extractor Predicting: 146it [01:37,  1.50it/s]Extractor Predicting: 147it [01:37,  1.54it/s]Extractor Predicting: 148it [01:38,  1.52it/s]Extractor Predicting: 149it [01:38,  1.53it/s]Extractor Predicting: 150it [01:39,  1.54it/s]Extractor Predicting: 151it [01:40,  1.56it/s]Extractor Predicting: 152it [01:40,  1.58it/s]Extractor Predicting: 153it [01:41,  1.57it/s]Extractor Predicting: 154it [01:42,  1.58it/s]Extractor Predicting: 155it [01:42,  1.57it/s]Extractor Predicting: 156it [01:43,  1.57it/s]Extractor Predicting: 157it [01:44,  1.54it/s]Extractor Predicting: 158it [01:44,  1.50it/s]Extractor Predicting: 159it [01:45,  1.52it/s]Extractor Predicting: 160it [01:45,  1.56it/s]Extractor Predicting: 161it [01:46,  1.55it/s]Extractor Predicting: 162it [01:47,  1.56it/s]Extractor Predicting: 163it [01:47,  1.57it/s]Extractor Predicting: 164it [01:48,  1.57it/s]Extractor Predicting: 165it [01:49,  1.55it/s]Extractor Predicting: 166it [01:49,  1.54it/s]Extractor Predicting: 167it [01:50,  1.54it/s]Extractor Predicting: 168it [01:51,  1.55it/s]Extractor Predicting: 169it [01:51,  1.58it/s]Extractor Predicting: 170it [01:52,  1.59it/s]Extractor Predicting: 171it [01:53,  1.55it/s]Extractor Predicting: 172it [01:53,  1.53it/s]Extractor Predicting: 173it [01:54,  1.53it/s]Extractor Predicting: 174it [01:54,  1.56it/s]Extractor Predicting: 175it [01:55,  1.58it/s]Extractor Predicting: 176it [01:56,  1.59it/s]Extractor Predicting: 177it [01:56,  1.53it/s]Extractor Predicting: 178it [01:57,  1.56it/s]Extractor Predicting: 179it [01:58,  1.55it/s]Extractor Predicting: 180it [01:58,  1.54it/s]Extractor Predicting: 181it [01:59,  1.55it/s]Extractor Predicting: 182it [02:00,  1.53it/s]Extractor Predicting: 183it [02:00,  1.54it/s]Extractor Predicting: 184it [02:01,  1.55it/s]Extractor Predicting: 185it [02:02,  1.57it/s]Extractor Predicting: 186it [02:02,  1.59it/s]Extractor Predicting: 187it [02:03,  1.63it/s]Extractor Predicting: 188it [02:03,  1.60it/s]Extractor Predicting: 189it [02:04,  1.56it/s]Extractor Predicting: 190it [02:05,  1.55it/s]Extractor Predicting: 191it [02:05,  1.52it/s]Extractor Predicting: 192it [02:06,  1.55it/s]Extractor Predicting: 193it [02:07,  1.55it/s]Extractor Predicting: 194it [02:07,  1.57it/s]Extractor Predicting: 195it [02:08,  1.58it/s]Extractor Predicting: 196it [02:09,  1.58it/s]Extractor Predicting: 197it [02:09,  1.54it/s]Extractor Predicting: 198it [02:10,  1.55it/s]Extractor Predicting: 199it [02:11,  1.53it/s]Extractor Predicting: 200it [02:11,  1.53it/s]Extractor Predicting: 201it [02:12,  1.54it/s]Extractor Predicting: 202it [02:13,  1.53it/s]Extractor Predicting: 203it [02:13,  1.53it/s]Extractor Predicting: 204it [02:14,  1.50it/s]Extractor Predicting: 205it [02:15,  1.47it/s]Extractor Predicting: 206it [02:15,  1.51it/s]Extractor Predicting: 207it [02:16,  1.51it/s]Extractor Predicting: 208it [02:16,  1.54it/s]Extractor Predicting: 209it [02:17,  1.56it/s]Extractor Predicting: 210it [02:18,  1.54it/s]Extractor Predicting: 211it [02:18,  1.53it/s]Extractor Predicting: 212it [02:19,  1.52it/s]Extractor Predicting: 213it [02:20,  1.51it/s]Extractor Predicting: 214it [02:20,  1.50it/s]Extractor Predicting: 215it [02:21,  1.50it/s]Extractor Predicting: 216it [02:22,  1.50it/s]Extractor Predicting: 217it [02:22,  1.49it/s]Extractor Predicting: 218it [02:23,  1.49it/s]Extractor Predicting: 219it [02:24,  1.49it/s]Extractor Predicting: 220it [02:24,  1.51it/s]Extractor Predicting: 221it [02:25,  1.55it/s]Extractor Predicting: 222it [02:26,  1.49it/s]Extractor Predicting: 223it [02:26,  1.47it/s]Extractor Predicting: 224it [02:27,  1.51it/s]Extractor Predicting: 225it [02:28,  1.52it/s]Extractor Predicting: 226it [02:28,  1.50it/s]Extractor Predicting: 227it [02:29,  1.51it/s]Extractor Predicting: 228it [02:30,  1.52it/s]Extractor Predicting: 229it [02:30,  1.55it/s]Extractor Predicting: 230it [02:31,  1.50it/s]Extractor Predicting: 231it [02:32,  1.52it/s]Extractor Predicting: 232it [02:32,  1.54it/s]Extractor Predicting: 233it [02:33,  1.35it/s]Extractor Predicting: 234it [02:34,  1.38it/s]Extractor Predicting: 235it [02:35,  1.43it/s]Extractor Predicting: 236it [02:35,  1.46it/s]Extractor Predicting: 237it [02:36,  1.45it/s]Extractor Predicting: 238it [02:37,  1.47it/s]Extractor Predicting: 239it [02:37,  1.48it/s]Extractor Predicting: 240it [02:38,  1.48it/s]Extractor Predicting: 241it [02:39,  1.49it/s]Extractor Predicting: 242it [02:39,  1.51it/s]Extractor Predicting: 243it [02:40,  1.49it/s]Extractor Predicting: 244it [02:41,  1.51it/s]Extractor Predicting: 245it [02:41,  1.50it/s]Extractor Predicting: 246it [02:42,  1.52it/s]Extractor Predicting: 247it [02:43,  1.51it/s]Extractor Predicting: 248it [02:43,  1.51it/s]Extractor Predicting: 249it [02:44,  1.52it/s]Extractor Predicting: 250it [02:45,  1.51it/s]Extractor Predicting: 251it [02:45,  1.52it/s]Extractor Predicting: 252it [02:46,  1.50it/s]Extractor Predicting: 253it [02:47,  1.52it/s]Extractor Predicting: 254it [02:47,  1.52it/s]Extractor Predicting: 255it [02:48,  1.53it/s]Extractor Predicting: 256it [02:49,  1.52it/s]Extractor Predicting: 257it [02:49,  1.55it/s]Extractor Predicting: 258it [02:50,  1.55it/s]Extractor Predicting: 259it [02:50,  1.57it/s]Extractor Predicting: 260it [02:51,  1.55it/s]Extractor Predicting: 261it [02:52,  1.56it/s]Extractor Predicting: 262it [02:52,  1.55it/s]Extractor Predicting: 263it [02:53,  1.55it/s]Extractor Predicting: 264it [02:54,  1.51it/s]Extractor Predicting: 265it [02:54,  1.51it/s]Extractor Predicting: 266it [02:55,  1.51it/s]Extractor Predicting: 267it [02:56,  1.53it/s]Extractor Predicting: 268it [02:56,  1.55it/s]Extractor Predicting: 269it [02:57,  1.55it/s]Extractor Predicting: 270it [02:58,  1.54it/s]Extractor Predicting: 271it [02:58,  1.55it/s]Extractor Predicting: 272it [02:59,  1.59it/s]Extractor Predicting: 273it [02:59,  1.59it/s]Extractor Predicting: 274it [03:00,  1.60it/s]Extractor Predicting: 275it [03:01,  1.57it/s]Extractor Predicting: 276it [03:01,  1.54it/s]Extractor Predicting: 277it [03:02,  1.57it/s]Extractor Predicting: 278it [03:03,  1.54it/s]Extractor Predicting: 279it [03:03,  1.55it/s]Extractor Predicting: 280it [03:04,  1.53it/s]Extractor Predicting: 281it [03:05,  1.54it/s]Extractor Predicting: 282it [03:05,  1.52it/s]Extractor Predicting: 283it [03:06,  1.52it/s]Extractor Predicting: 284it [03:07,  1.53it/s]Extractor Predicting: 285it [03:07,  1.52it/s]Extractor Predicting: 286it [03:08,  1.50it/s]Extractor Predicting: 287it [03:09,  1.52it/s]Extractor Predicting: 288it [03:09,  1.53it/s]Extractor Predicting: 289it [03:10,  1.53it/s]Extractor Predicting: 290it [03:11,  1.52it/s]Extractor Predicting: 291it [03:11,  1.52it/s]Extractor Predicting: 292it [03:12,  1.53it/s]Extractor Predicting: 293it [03:13,  1.53it/s]Extractor Predicting: 294it [03:13,  1.52it/s]Extractor Predicting: 295it [03:14,  1.55it/s]Extractor Predicting: 296it [03:14,  1.55it/s]Extractor Predicting: 297it [03:15,  1.51it/s]Extractor Predicting: 298it [03:16,  1.54it/s]Extractor Predicting: 299it [03:16,  1.54it/s]Extractor Predicting: 300it [03:17,  1.52it/s]Extractor Predicting: 301it [03:18,  1.58it/s]Extractor Predicting: 302it [03:18,  1.56it/s]Extractor Predicting: 303it [03:19,  1.59it/s]Extractor Predicting: 304it [03:20,  1.57it/s]Extractor Predicting: 305it [03:20,  1.55it/s]Extractor Predicting: 306it [03:21,  1.55it/s]Extractor Predicting: 307it [03:22,  1.53it/s]Extractor Predicting: 308it [03:22,  1.52it/s]Extractor Predicting: 309it [03:23,  1.53it/s]Extractor Predicting: 310it [03:24,  1.51it/s]Extractor Predicting: 311it [03:24,  1.50it/s]Extractor Predicting: 312it [03:25,  1.47it/s]Extractor Predicting: 313it [03:26,  1.48it/s]Extractor Predicting: 314it [03:26,  1.48it/s]Extractor Predicting: 315it [03:27,  1.52it/s]Extractor Predicting: 316it [03:28,  1.53it/s]Extractor Predicting: 317it [03:28,  1.51it/s]Extractor Predicting: 318it [03:29,  1.53it/s]Extractor Predicting: 319it [03:30,  1.52it/s]Extractor Predicting: 320it [03:30,  1.51it/s]Extractor Predicting: 321it [03:31,  1.51it/s]Extractor Predicting: 322it [03:32,  1.50it/s]Extractor Predicting: 323it [03:32,  1.55it/s]Extractor Predicting: 324it [03:33,  1.52it/s]Extractor Predicting: 325it [03:33,  1.53it/s]Extractor Predicting: 326it [03:34,  1.53it/s]Extractor Predicting: 327it [03:35,  1.55it/s]Extractor Predicting: 328it [03:35,  1.56it/s]Extractor Predicting: 329it [03:36,  1.53it/s]Extractor Predicting: 330it [03:37,  1.51it/s]Extractor Predicting: 331it [03:37,  1.50it/s]Extractor Predicting: 332it [03:38,  1.50it/s]Extractor Predicting: 333it [03:39,  1.51it/s]Extractor Predicting: 334it [03:39,  1.51it/s]Extractor Predicting: 335it [03:40,  1.51it/s]Extractor Predicting: 336it [03:41,  1.55it/s]Extractor Predicting: 337it [03:41,  1.56it/s]Extractor Predicting: 338it [03:42,  1.55it/s]Extractor Predicting: 339it [03:43,  1.53it/s]Extractor Predicting: 340it [03:43,  1.53it/s]Extractor Predicting: 341it [03:44,  1.56it/s]Extractor Predicting: 342it [03:45,  1.52it/s]Extractor Predicting: 343it [03:45,  1.54it/s]Extractor Predicting: 344it [03:46,  1.53it/s]Extractor Predicting: 345it [03:47,  1.55it/s]Extractor Predicting: 346it [03:47,  1.38it/s]Extractor Predicting: 347it [03:48,  1.42it/s]Extractor Predicting: 348it [03:49,  1.46it/s]Extractor Predicting: 349it [03:49,  1.46it/s]Extractor Predicting: 350it [03:50,  1.48it/s]Extractor Predicting: 351it [03:51,  1.47it/s]Extractor Predicting: 352it [03:51,  1.46it/s]Extractor Predicting: 353it [03:52,  1.47it/s]Extractor Predicting: 354it [03:53,  1.48it/s]Extractor Predicting: 355it [03:53,  1.50it/s]Extractor Predicting: 356it [03:54,  1.51it/s]Extractor Predicting: 357it [03:55,  1.54it/s]Extractor Predicting: 358it [03:55,  1.55it/s]Extractor Predicting: 359it [03:56,  1.54it/s]Extractor Predicting: 360it [03:57,  1.52it/s]Extractor Predicting: 361it [03:57,  1.54it/s]Extractor Predicting: 362it [03:58,  1.54it/s]Extractor Predicting: 363it [03:59,  1.56it/s]Extractor Predicting: 364it [03:59,  1.56it/s]Extractor Predicting: 365it [04:00,  1.55it/s]Extractor Predicting: 366it [04:01,  1.54it/s]Extractor Predicting: 367it [04:01,  1.53it/s]Extractor Predicting: 368it [04:02,  1.56it/s]Extractor Predicting: 369it [04:02,  1.56it/s]Extractor Predicting: 370it [04:03,  1.58it/s]Extractor Predicting: 371it [04:04,  1.57it/s]Extractor Predicting: 372it [04:04,  1.57it/s]Extractor Predicting: 373it [04:05,  1.56it/s]Extractor Predicting: 374it [04:06,  1.55it/s]Extractor Predicting: 375it [04:06,  1.56it/s]Extractor Predicting: 376it [04:07,  1.55it/s]Extractor Predicting: 377it [04:08,  1.56it/s]Extractor Predicting: 378it [04:08,  1.58it/s]Extractor Predicting: 379it [04:09,  1.52it/s]Extractor Predicting: 380it [04:10,  1.53it/s]Extractor Predicting: 381it [04:10,  1.57it/s]Extractor Predicting: 382it [04:11,  1.57it/s]Extractor Predicting: 383it [04:11,  1.51it/s]Extractor Predicting: 384it [04:12,  1.50it/s]Extractor Predicting: 385it [04:13,  1.52it/s]Extractor Predicting: 386it [04:13,  1.57it/s]Extractor Predicting: 387it [04:14,  1.58it/s]Extractor Predicting: 388it [04:15,  1.57it/s]Extractor Predicting: 389it [04:15,  1.55it/s]Extractor Predicting: 390it [04:16,  1.56it/s]Extractor Predicting: 391it [04:17,  1.54it/s]Extractor Predicting: 392it [04:17,  1.56it/s]Extractor Predicting: 393it [04:18,  1.58it/s]Extractor Predicting: 394it [04:18,  1.61it/s]Extractor Predicting: 395it [04:19,  1.57it/s]Extractor Predicting: 396it [04:20,  1.54it/s]Extractor Predicting: 397it [04:20,  1.56it/s]Extractor Predicting: 398it [04:21,  1.57it/s]Extractor Predicting: 399it [04:22,  1.60it/s]Extractor Predicting: 400it [04:22,  1.63it/s]Extractor Predicting: 401it [04:23,  1.60it/s]Extractor Predicting: 402it [04:24,  1.61it/s]Extractor Predicting: 403it [04:24,  1.59it/s]Extractor Predicting: 404it [04:25,  1.58it/s]Extractor Predicting: 405it [04:25,  1.58it/s]Extractor Predicting: 406it [04:26,  1.54it/s]Extractor Predicting: 407it [04:27,  1.54it/s]Extractor Predicting: 408it [04:27,  1.56it/s]Extractor Predicting: 409it [04:28,  1.58it/s]Extractor Predicting: 409it [04:28,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:19,163 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:19,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:19,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:19,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:19,171 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:18:19,815 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:18:19,816 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:18:20,401 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:18:21,472 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:18:21,473 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:24,389 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:24,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:24,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:24,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:18:24,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:18:25,034 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:18:25,035 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:18:25,626 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:18:25,791 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:18:25,791 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4213068904144419,
  "recall": 0.16885763782737184,
  "score": 0.24108831660119306,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.41it/s]Extractor Predicting: 4it [00:02,  1.45it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.52it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 13it [00:08,  1.47it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:18:35,292 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:18:35,292 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:18:35,295 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:18:35,295 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:18:35,300 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:18:38,601 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:18:38,605 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:18:38,616 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:18:38,617 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:18:38,628 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:18:38,630 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:18:38,630 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:18:38,631 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:18:38,631 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:18:38,631 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:18:38,631 >> loading file outputs/wrapper/fewrel/unseen_15_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2676056338028169,
  "recall": 0.027656477438136828,
  "score": 0.05013192612137204,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:18:38,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:39,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:40,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:41,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:41,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:42,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:43,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:44,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:45,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:45,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:46,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:47,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:48,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:48,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:49,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:50,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:51,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:51,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:52,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:53,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:53,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<05:00, 15.82s/it][WARNING|generation_utils.py:914] 2023-08-28 14:18:54,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:55,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:56,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:56,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:57,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:58,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:59,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:18:59,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:00,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:01,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:02,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:02,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:03,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:04,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:04,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:05,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:06,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:07,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:07,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:08,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:09,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:09,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:10,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:32<04:52, 16.26s/it][WARNING|generation_utils.py:914] 2023-08-28 14:19:11,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:12,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:12,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:13,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:14,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:15,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:16,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:16,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:17,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:18,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:19,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:19,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:20,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:21,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:22,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:22,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:23,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:24,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:25,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:26,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:26,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:27,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:49<04:43, 16.67s/it][WARNING|generation_utils.py:914] 2023-08-28 14:19:28,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:29,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:29,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:30,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:31,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:31,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:32,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:33,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:34,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:34,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:35,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:36,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:36,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:37,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:38,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:38,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:39,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:40,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:41,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:41,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:42,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:43,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:05<04:19, 16.23s/it][WARNING|generation_utils.py:914] 2023-08-28 14:19:43,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:44,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:45,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:46,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:47,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:47,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:48,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:49,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:50,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:50,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:51,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:52,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:52,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:53,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:54,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:54,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:55,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:56,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:57,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:57,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:58,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:19:59,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:00,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:00,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:01,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:02,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:02,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:03,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:04,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:25<04:28, 17.90s/it][WARNING|generation_utils.py:914] 2023-08-28 14:20:04,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:05,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:06,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:06,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:07,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:08,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:08,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:09,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:10,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:11,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:11,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:12,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:13,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:13,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:14,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:15,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:16,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:16,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:17,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:18,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:19,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:20,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:21,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:21,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:43<04:08, 17.76s/it][WARNING|generation_utils.py:914] 2023-08-28 14:20:22,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:23,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:23,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:24,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:25,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:26,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:27,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:27,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:28,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:29,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:30,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:30,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:31,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:32,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:33,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:33,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:34,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:35,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:35,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:36,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:37,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:38,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:38,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:39,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:01<03:52, 17.90s/it][WARNING|generation_utils.py:914] 2023-08-28 14:20:40,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:41,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:42,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:42,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:43,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:44,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:45,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:45,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:46,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:47,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:48,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:49,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:50,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:51,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:52,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:53,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:54,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:54,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:55,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:56,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:57,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:58,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:59,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:20:59,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:00,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:01,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:02,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:24<03:52, 19.35s/it][WARNING|generation_utils.py:914] 2023-08-28 14:21:02,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:03,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:04,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:05,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:05,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:06,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:07,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:08,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:08,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:09,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:10,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:10,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:11,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:12,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:13,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:14,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:14,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:15,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:16,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:17,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:18,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:18,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:19,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:20,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:21,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:43<03:31, 19.26s/it][WARNING|generation_utils.py:914] 2023-08-28 14:21:22,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:22,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:23,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:24,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:24,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:25,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:26,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:27,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:28,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:29,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:29,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:30,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:31,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:32,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:32,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:33,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:34,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:35,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:35,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:36,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:37,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:38,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [03:00<03:05, 18.54s/it][WARNING|generation_utils.py:914] 2023-08-28 14:21:38,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:39,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:40,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:41,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:41,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:42,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:43,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:43,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:44,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:45,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:46,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:46,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:47,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:48,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:48,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:49,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:50,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:50,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:51,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:52,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:52,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:53,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:54,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:16<02:40, 17.82s/it][WARNING|generation_utils.py:914] 2023-08-28 14:21:55,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:55,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:56,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:57,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:57,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:58,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:21:59,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:00,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:00,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:01,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:02,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:03,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:03,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:04,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:05,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:06,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:07,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:07,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:08,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:09,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:09,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:10,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:11,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:12,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:34<02:22, 17.83s/it][WARNING|generation_utils.py:914] 2023-08-28 14:22:12,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:14,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:14,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:15,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:16,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:17,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:17,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:18,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:18,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:19,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:20,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:20,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:21,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:22,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:23,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:23,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:24,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:25,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:25,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:26,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:27,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:27,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:49<01:59, 17.09s/it][WARNING|generation_utils.py:914] 2023-08-28 14:22:28,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:29,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:29,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:30,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:31,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:31,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:32,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:33,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:33,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:34,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:35,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:36,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:36,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:37,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:38,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:39,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:39,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:40,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:41,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:41,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:42,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:43,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:05<01:40, 16.68s/it][WARNING|generation_utils.py:914] 2023-08-28 14:22:44,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:44,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:45,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:46,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:46,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:47,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:48,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:49,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:49,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:50,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:51,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:52,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:52,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:53,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:54,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:54,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:55,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:56,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:56,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:57,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:58,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:59,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:22:59,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:00,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:22<01:24, 16.82s/it][WARNING|generation_utils.py:914] 2023-08-28 14:23:01,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:01,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:02,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:03,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:04,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:04,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:05,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:06,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:06,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:07,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:08,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:09,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:09,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:10,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:11,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:12,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:12,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:13,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:14,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:15,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:15,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:16,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:17,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:17,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:39<01:07, 16.92s/it][WARNING|generation_utils.py:914] 2023-08-28 14:23:18,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:18,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:19,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:20,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:20,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:21,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:22,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:22,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:23,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:24,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:24,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:25,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:25,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:26,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:27,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:27,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:28,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:29,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:30,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:30,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:31,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:32,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:32,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:54<00:49, 16.34s/it][WARNING|generation_utils.py:914] 2023-08-28 14:23:33,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:34,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:34,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:35,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:36,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:36,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:37,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:38,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:39,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:39,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:40,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:41,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:41,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:42,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:43,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:44,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:44,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:45,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:46,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:47,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:47,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:48,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:10<00:32, 16.20s/it][WARNING|generation_utils.py:914] 2023-08-28 14:23:49,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:50,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:50,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:51,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:52,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:53,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:54,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:55,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:55,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:56,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:57,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:58,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:59,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:23:59,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:00,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:01,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:02,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:03,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:03,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:04,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:05,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:06,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:07,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:07,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:08,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:30<00:17, 17.36s/it][WARNING|generation_utils.py:914] 2023-08-28 14:24:09,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:10,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:10,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:11,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:12,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:13,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:14,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:14,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:15,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:16,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:17,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:17,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:18,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:19,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:20,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:20,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:21,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:22,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:23,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:23,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:24,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:24:25,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:47<00:00, 17.16s/it]Generating: 100%|██████████| 20/20 [05:47<00:00, 17.36s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:33,045 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:33,048 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:33,048 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:33,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:33,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:24:33,658 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:24:33,660 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:24:34,227 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:24:35,295 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:24:35,295 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:38,182 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:38,184 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:38,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:38,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:24:38,185 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:24:38,822 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:24:38,827 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:24:39,397 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:24:39,549 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:24:39,549 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.8943452380952381, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 503, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 560, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : military branch .', 'success_rate': 0.8328804347826086, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : place served by transport hub . Context : Later in the year , the railway built the Puyallup railway station located at the end of the town . Head Entity : Puyallup railway station , Tail Entity : Puyallup town .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 554, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.8622159090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8892045454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 35, 'raw': 64}
{'target': 600, 'success': 62, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 99, 'raw': 160}
{'target': 600, 'success': 114, 'raw': 192}
{'target': 600, 'success': 133, 'raw': 224}
{'target': 600, 'success': 156, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 193, 'raw': 320}
{'target': 600, 'success': 217, 'raw': 352}
{'target': 600, 'success': 235, 'raw': 384}
{'target': 600, 'success': 258, 'raw': 416}
{'target': 600, 'success': 280, 'raw': 448}
{'target': 600, 'success': 302, 'raw': 480}
{'target': 600, 'success': 325, 'raw': 512}
{'target': 600, 'success': 344, 'raw': 544}
{'target': 600, 'success': 368, 'raw': 576}
{'target': 600, 'success': 389, 'raw': 608}
{'target': 600, 'success': 409, 'raw': 640}
{'target': 600, 'success': 430, 'raw': 672}
{'target': 600, 'success': 451, 'raw': 704}
{'target': 600, 'success': 476, 'raw': 736}
{'target': 600, 'success': 494, 'raw': 768}
{'target': 600, 'success': 517, 'raw': 800}
{'target': 600, 'success': 538, 'raw': 832}
{'target': 600, 'success': 561, 'raw': 864}
{'target': 600, 'success': 587, 'raw': 896}
{'target': 600, 'success': 606, 'raw': 928}
{'prompt': 'Relation : voice type .', 'success_rate': 0.6530172413793104, 'errors': {'', '(\'Pårästelles\', \'voice type\', \'\', \'In 1995 he became the lead single producer in two Swedish albums " Pårästelles " and " Teilum " .\')', '(\'How Little Loved\', \'voice type\', \' \', \'" How Little Loved " is recorded in the studio in Waka Flocka Flame \\\'s   .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 220, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 271, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 378, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 559, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.796875, 'errors': {'', "('PGA Tour', 'competition class', '', 'He reached the second round of the PGA Tour in 2005 and played a combined 13 rounds of the tournament .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 567, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8059895833333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
["Relation : language of work or name . Context : Later in the year ( 1143–1230 ) he married Anne of Denmark at Stadion , who was married to Anne of Denmark 's second husband , Thomas II , and remarried in 1232 to Catherine Sibast , Prince of Hesse . Head Entity : Thomas II , Tail Entity : Danish .\n"]
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 116, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 231, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 295, 'raw': 416}
{'target': 600, 'success': 318, 'raw': 448}
{'target': 600, 'success': 344, 'raw': 480}
{'target': 600, 'success': 363, 'raw': 512}
{'target': 600, 'success': 387, 'raw': 544}
{'target': 600, 'success': 412, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 460, 'raw': 640}
{'target': 600, 'success': 481, 'raw': 672}
{'target': 600, 'success': 507, 'raw': 704}
{'target': 600, 'success': 532, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 576, 'raw': 800}
{'target': 600, 'success': 597, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.7118055555555556, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 183, 'raw': 256}
{'target': 600, 'success': 209, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 302, 'raw': 416}
{'target': 600, 'success': 325, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 374, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 429, 'raw': 576}
{'target': 600, 'success': 456, 'raw': 608}
{'target': 600, 'success': 482, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 557, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 609, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.76125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 230, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 522, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 573, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8165760869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 327, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 428, 'raw': 544}
{'target': 600, 'success': 449, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 502, 'raw': 640}
{'target': 600, 'success': 526, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 579, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.7825520833333334, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 613, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8707386363636364, 'errors': {'', "('Windows XP', 'operating system', '', 'After Windows XP versions 10.0 and 2010 , Microsoft distributed .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 524, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : Later in life he joined the University of New Brunswick , where he received teaching degrees at the University of Oxford , Oxford and also at the University of Western Ontario , where he was professor of chemistry . Head Entity : Professor of Chemistry , Tail Entity : University of Oxford .\n']
['Relation : position held . Context : Later in life he joined the University of New Brunswick , where he received teaching degrees at the University of Oxford , Oxford and also at the University of Western Ontario , where he was professor of chemistry . Head Entity : Professor of Chemistry , Tail Entity : University of Oxford .\n', 'Relation : position held . Context : After the death of Emperor Hélène of Hesse , the Second Marquess of Düsseldorf was created as the crown prince of Switzerland . Head Entity : Emperor Hélène of Hesse , Tail Entity : Crown prince .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 336, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 564, 'raw': 704}
{'target': 600, 'success': 589, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 232, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 495, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8179347826086957, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : record label .', 'success_rate': 0.8735795454545454, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', '(\'I Was Never There\', \'record label\', \'\', \'While working on the song " I Was Never There " at the same time as B.\')'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 323, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 396, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 498, 'raw': 640}
{'target': 600, 'success': 522, 'raw': 672}
{'target': 600, 'success': 543, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 586, 'raw': 768}
{'target': 600, 'success': 606, 'raw': 800}
{'prompt': 'Relation : religion .', 'success_rate': 0.7575, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : said to be the same as . Context : Later in the year ( October 1887 ) , a young French journalist named Louis B. de la Rocher was married to a student in the department , and her husband was a prominent literary critic . Head Entity : Louise B. de la Rocher , Tail Entity : Louis B. de la Rocher .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 15278
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15378, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.27it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.47it/s]Extractor Estimating: 4it [00:02,  1.49it/s]Extractor Estimating: 5it [00:03,  1.55it/s]Extractor Estimating: 6it [00:03,  1.61it/s]Extractor Estimating: 7it [00:04,  1.58it/s]Extractor Estimating: 8it [00:05,  1.55it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.53it/s]Extractor Estimating: 11it [00:07,  1.57it/s]Extractor Estimating: 12it [00:07,  1.53it/s]Extractor Estimating: 13it [00:08,  1.58it/s]Extractor Estimating: 14it [00:09,  1.56it/s]Extractor Estimating: 15it [00:09,  1.51it/s]Extractor Estimating: 16it [00:10,  1.54it/s]Extractor Estimating: 17it [00:11,  1.57it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:12,  1.61it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:13,  1.57it/s]Extractor Estimating: 22it [00:14,  1.59it/s]Extractor Estimating: 23it [00:14,  1.56it/s]Extractor Estimating: 24it [00:15,  1.55it/s]Extractor Estimating: 25it [00:16,  1.58it/s]Extractor Estimating: 26it [00:16,  1.60it/s]Extractor Estimating: 27it [00:17,  1.53it/s]Extractor Estimating: 28it [00:18,  1.55it/s]Extractor Estimating: 29it [00:18,  1.55it/s]Extractor Estimating: 30it [00:19,  1.53it/s]Extractor Estimating: 31it [00:20,  1.48it/s]Extractor Estimating: 32it [00:20,  1.52it/s]Extractor Estimating: 33it [00:21,  1.56it/s]Extractor Estimating: 34it [00:21,  1.54it/s]Extractor Estimating: 35it [00:22,  1.55it/s]Extractor Estimating: 36it [00:23,  1.54it/s]Extractor Estimating: 37it [00:23,  1.53it/s]Extractor Estimating: 38it [00:24,  1.55it/s]Extractor Estimating: 39it [00:25,  1.57it/s]Extractor Estimating: 40it [00:25,  1.61it/s]Extractor Estimating: 41it [00:26,  1.65it/s]Extractor Estimating: 42it [00:27,  1.54it/s]Extractor Estimating: 43it [00:27,  1.55it/s]Extractor Estimating: 44it [00:28,  1.57it/s]Extractor Estimating: 45it [00:28,  1.57it/s]Extractor Estimating: 46it [00:29,  1.60it/s]Extractor Estimating: 47it [00:30,  1.60it/s]Extractor Estimating: 48it [00:30,  1.60it/s]Extractor Estimating: 49it [00:31,  1.59it/s]Extractor Estimating: 50it [00:32,  1.56it/s]Extractor Estimating: 51it [00:32,  1.58it/s]Extractor Estimating: 52it [00:33,  1.59it/s]Extractor Estimating: 53it [00:33,  1.63it/s]Extractor Estimating: 54it [00:34,  1.65it/s]Extractor Estimating: 55it [00:35,  1.68it/s]Extractor Estimating: 56it [00:35,  1.70it/s]Extractor Estimating: 57it [00:36,  1.69it/s]Extractor Estimating: 58it [00:36,  1.71it/s]Extractor Estimating: 59it [00:37,  1.69it/s]Extractor Estimating: 60it [00:38,  1.64it/s]Extractor Estimating: 61it [00:38,  1.67it/s]Extractor Estimating: 62it [00:39,  1.67it/s]Extractor Estimating: 63it [00:39,  1.71it/s]Extractor Estimating: 64it [00:40,  1.70it/s]Extractor Estimating: 65it [00:40,  1.68it/s]Extractor Estimating: 66it [00:41,  1.69it/s]Extractor Estimating: 67it [00:42,  1.72it/s]Extractor Estimating: 68it [00:42,  1.77it/s]Extractor Estimating: 69it [00:43,  1.78it/s]Extractor Estimating: 70it [00:43,  1.81it/s]Extractor Estimating: 71it [00:44,  1.73it/s]Extractor Estimating: 72it [00:44,  1.74it/s]Extractor Estimating: 73it [00:45,  1.76it/s]Extractor Estimating: 74it [00:46,  1.68it/s]Extractor Estimating: 75it [00:46,  1.73it/s]Extractor Estimating: 76it [00:47,  1.66it/s]Extractor Estimating: 77it [00:48,  1.63it/s]Extractor Estimating: 78it [00:48,  1.58it/s]Extractor Estimating: 79it [00:49,  1.53it/s]Extractor Estimating: 80it [00:50,  1.54it/s]Extractor Estimating: 81it [00:50,  1.54it/s]Extractor Estimating: 82it [00:51,  1.58it/s]Extractor Estimating: 83it [00:51,  1.56it/s]Extractor Estimating: 84it [00:52,  1.58it/s]Extractor Estimating: 85it [00:53,  1.58it/s]Extractor Estimating: 86it [00:53,  1.57it/s]Extractor Estimating: 87it [00:54,  1.61it/s]Extractor Estimating: 88it [00:55,  1.57it/s]Extractor Estimating: 89it [00:55,  1.56it/s]Extractor Estimating: 90it [00:56,  1.55it/s]Extractor Estimating: 91it [00:57,  1.50it/s]Extractor Estimating: 92it [00:57,  1.55it/s]Extractor Estimating: 93it [00:58,  1.55it/s]Extractor Estimating: 94it [00:59,  1.54it/s]Extractor Estimating: 95it [00:59,  1.54it/s]Extractor Estimating: 96it [01:00,  1.50it/s]Extractor Estimating: 97it [01:00,  1.55it/s]Extractor Estimating: 98it [01:01,  1.55it/s]Extractor Estimating: 99it [01:02,  1.51it/s]Extractor Estimating: 100it [01:02,  1.50it/s]Extractor Estimating: 101it [01:03,  1.51it/s]Extractor Estimating: 102it [01:04,  1.55it/s]Extractor Estimating: 103it [01:04,  1.56it/s]Extractor Estimating: 104it [01:05,  1.54it/s]Extractor Estimating: 105it [01:06,  1.58it/s]Extractor Estimating: 106it [01:06,  1.59it/s]Extractor Estimating: 107it [01:07,  1.59it/s]Extractor Estimating: 108it [01:07,  1.62it/s]Extractor Estimating: 109it [01:08,  1.64it/s]Extractor Estimating: 110it [01:09,  1.62it/s]Extractor Estimating: 111it [01:09,  1.65it/s]Extractor Estimating: 112it [01:10,  1.62it/s]Extractor Estimating: 113it [01:11,  1.56it/s]Extractor Estimating: 114it [01:11,  1.54it/s]Extractor Estimating: 115it [01:12,  1.56it/s]Extractor Estimating: 116it [01:13,  1.54it/s]Extractor Estimating: 117it [01:13,  1.59it/s]Extractor Estimating: 118it [01:14,  1.61it/s]Extractor Estimating: 119it [01:14,  1.57it/s]Extractor Estimating: 120it [01:15,  1.56it/s]Extractor Estimating: 121it [01:16,  1.61it/s]Extractor Estimating: 122it [01:16,  1.58it/s]Extractor Estimating: 123it [01:17,  1.61it/s]Extractor Estimating: 124it [01:18,  1.61it/s]Extractor Estimating: 125it [01:18,  1.51it/s]Extractor Estimating: 126it [01:19,  1.59it/s]Extractor Estimating: 127it [01:19,  1.65it/s]Extractor Estimating: 128it [01:20,  1.66it/s]Extractor Estimating: 129it [01:21,  1.69it/s]Extractor Estimating: 130it [01:21,  1.65it/s]Extractor Estimating: 131it [01:22,  1.68it/s]Extractor Estimating: 132it [01:22,  1.75it/s]Extractor Estimating: 133it [01:23,  1.73it/s]Extractor Estimating: 134it [01:24,  1.70it/s]Extractor Estimating: 135it [01:24,  1.72it/s]Extractor Estimating: 136it [01:25,  1.67it/s]Extractor Estimating: 137it [01:26,  1.50it/s]Extractor Estimating: 138it [01:26,  1.57it/s]Extractor Estimating: 139it [01:27,  1.60it/s]Extractor Estimating: 140it [01:27,  1.61it/s]Extractor Estimating: 141it [01:28,  1.67it/s]Extractor Estimating: 142it [01:28,  1.68it/s]Extractor Estimating: 143it [01:29,  1.68it/s]Extractor Estimating: 144it [01:30,  1.70it/s]Extractor Estimating: 145it [01:30,  1.70it/s]Extractor Estimating: 146it [01:31,  1.74it/s]Extractor Estimating: 147it [01:31,  1.71it/s]Extractor Estimating: 148it [01:32,  1.67it/s]Extractor Estimating: 149it [01:33,  1.72it/s]Extractor Estimating: 150it [01:33,  1.79it/s]Extractor Estimating: 151it [01:34,  1.80it/s]Extractor Estimating: 152it [01:34,  1.68it/s]Extractor Estimating: 153it [01:35,  1.63it/s]Extractor Estimating: 154it [01:36,  1.57it/s]Extractor Estimating: 155it [01:36,  1.59it/s]Extractor Estimating: 156it [01:37,  1.58it/s]Extractor Estimating: 157it [01:37,  1.57it/s]Extractor Estimating: 158it [01:38,  1.56it/s]Extractor Estimating: 159it [01:39,  1.60it/s]Extractor Estimating: 160it [01:39,  1.59it/s]Extractor Estimating: 161it [01:40,  1.61it/s]Extractor Estimating: 162it [01:41,  1.62it/s]Extractor Estimating: 163it [01:41,  1.58it/s]Extractor Estimating: 164it [01:42,  1.63it/s]Extractor Estimating: 165it [01:42,  1.63it/s]Extractor Estimating: 166it [01:43,  1.62it/s]Extractor Estimating: 167it [01:44,  1.64it/s]Extractor Estimating: 168it [01:44,  1.64it/s]Extractor Estimating: 169it [01:45,  1.63it/s]Extractor Estimating: 170it [01:46,  1.61it/s]Extractor Estimating: 171it [01:46,  1.61it/s]Extractor Estimating: 172it [01:47,  1.68it/s]Extractor Estimating: 173it [01:47,  1.75it/s]Extractor Estimating: 174it [01:48,  1.72it/s]Extractor Estimating: 175it [01:48,  1.69it/s]Extractor Estimating: 176it [01:49,  1.65it/s]Extractor Estimating: 177it [01:50,  1.57it/s]Extractor Estimating: 178it [01:50,  1.58it/s]Extractor Estimating: 179it [01:51,  1.62it/s]Extractor Estimating: 180it [01:52,  1.58it/s]Extractor Estimating: 181it [01:52,  1.60it/s]Extractor Estimating: 182it [01:53,  1.60it/s]Extractor Estimating: 183it [01:54,  1.48it/s]Extractor Estimating: 184it [01:54,  1.55it/s]Extractor Estimating: 185it [01:55,  1.50it/s]Extractor Estimating: 186it [01:56,  1.50it/s]Extractor Estimating: 187it [01:56,  1.50it/s]Extractor Estimating: 188it [01:57,  1.50it/s]Extractor Estimating: 189it [01:58,  1.49it/s]Extractor Estimating: 190it [01:58,  1.51it/s]Extractor Estimating: 191it [01:59,  1.56it/s]Extractor Estimating: 192it [02:00,  1.50it/s]Extractor Estimating: 193it [02:00,  1.47it/s]Extractor Estimating: 194it [02:01,  1.50it/s]Extractor Estimating: 195it [02:02,  1.49it/s]Extractor Estimating: 196it [02:02,  1.52it/s]Extractor Estimating: 197it [02:03,  1.57it/s]Extractor Estimating: 198it [02:03,  1.65it/s]Extractor Estimating: 199it [02:04,  1.64it/s]Extractor Estimating: 200it [02:05,  1.61it/s]Extractor Estimating: 201it [02:05,  1.58it/s]Extractor Estimating: 202it [02:06,  1.59it/s]Extractor Estimating: 203it [02:07,  1.57it/s]Extractor Estimating: 204it [02:07,  1.61it/s]Extractor Estimating: 205it [02:08,  1.59it/s]Extractor Estimating: 206it [02:08,  1.54it/s]Extractor Estimating: 207it [02:09,  1.52it/s]Extractor Estimating: 208it [02:10,  1.59it/s]Extractor Estimating: 209it [02:10,  1.55it/s]Extractor Estimating: 210it [02:11,  1.58it/s]Extractor Estimating: 211it [02:12,  1.56it/s]Extractor Estimating: 212it [02:12,  1.55it/s]Extractor Estimating: 213it [02:13,  1.56it/s]Extractor Estimating: 214it [02:14,  1.58it/s]Extractor Estimating: 215it [02:14,  1.56it/s]Extractor Estimating: 216it [02:15,  1.62it/s]Extractor Estimating: 217it [02:15,  1.58it/s]Extractor Estimating: 218it [02:16,  1.57it/s]Extractor Estimating: 219it [02:17,  1.60it/s]Extractor Estimating: 220it [02:17,  1.54it/s]Extractor Estimating: 221it [02:18,  1.52it/s]Extractor Estimating: 222it [02:19,  1.52it/s]Extractor Estimating: 223it [02:19,  1.52it/s]Extractor Estimating: 224it [02:20,  1.52it/s]Extractor Estimating: 225it [02:21,  1.55it/s]Extractor Estimating: 226it [02:21,  1.58it/s]Extractor Estimating: 227it [02:22,  1.62it/s]Extractor Estimating: 228it [02:23,  1.57it/s]Extractor Estimating: 229it [02:23,  1.62it/s]Extractor Estimating: 230it [02:24,  1.60it/s]Extractor Estimating: 231it [02:24,  1.59it/s]Extractor Estimating: 232it [02:25,  1.59it/s]Extractor Estimating: 233it [02:26,  1.62it/s]Extractor Estimating: 234it [02:26,  1.64it/s]Extractor Estimating: 235it [02:27,  1.67it/s]Extractor Estimating: 236it [02:27,  1.66it/s]Extractor Estimating: 237it [02:28,  1.67it/s]Extractor Estimating: 238it [02:29,  1.68it/s]Extractor Estimating: 239it [02:29,  1.67it/s]Extractor Estimating: 240it [02:30,  1.65it/s]Extractor Estimating: 241it [02:30,  1.65it/s]Extractor Estimating: 242it [02:31,  1.62it/s]Extractor Estimating: 243it [02:32,  1.62it/s]Extractor Estimating: 244it [02:32,  1.63it/s]Extractor Estimating: 245it [02:33,  1.60it/s]Extractor Estimating: 246it [02:33,  1.66it/s]Extractor Estimating: 247it [02:34,  1.58it/s]Extractor Estimating: 248it [02:35,  1.46it/s]Extractor Estimating: 249it [02:36,  1.48it/s]Extractor Estimating: 250it [02:37,  1.35it/s]Extractor Estimating: 251it [02:37,  1.42it/s]Extractor Estimating: 252it [02:38,  1.45it/s]Extractor Estimating: 253it [02:38,  1.49it/s]Extractor Estimating: 254it [02:39,  1.56it/s]Extractor Estimating: 255it [02:40,  1.56it/s]Extractor Estimating: 256it [02:40,  1.58it/s]Extractor Estimating: 257it [02:41,  1.59it/s]Extractor Estimating: 258it [02:42,  1.60it/s]Extractor Estimating: 259it [02:42,  1.61it/s]Extractor Estimating: 260it [02:43,  1.62it/s]Extractor Estimating: 261it [02:43,  1.70it/s]Extractor Estimating: 262it [02:44,  1.67it/s]Extractor Estimating: 263it [02:44,  1.67it/s]Extractor Estimating: 264it [02:45,  1.69it/s]Extractor Estimating: 265it [02:46,  1.68it/s]Extractor Estimating: 266it [02:46,  1.73it/s]Extractor Estimating: 267it [02:47,  1.71it/s]Extractor Estimating: 268it [02:47,  1.70it/s]Extractor Estimating: 269it [02:48,  1.67it/s]Extractor Estimating: 270it [02:49,  1.74it/s]Extractor Estimating: 271it [02:49,  1.65it/s]Extractor Estimating: 272it [02:50,  1.64it/s]Extractor Estimating: 273it [02:50,  1.70it/s]Extractor Estimating: 274it [02:51,  1.69it/s]Extractor Estimating: 275it [02:52,  1.64it/s]Extractor Estimating: 276it [02:52,  1.61it/s]Extractor Estimating: 277it [02:53,  1.58it/s]Extractor Estimating: 278it [02:54,  1.53it/s]Extractor Estimating: 279it [02:54,  1.53it/s]Extractor Estimating: 280it [02:55,  1.51it/s]Extractor Estimating: 281it [02:56,  1.49it/s]Extractor Estimating: 282it [02:56,  1.50it/s]Extractor Estimating: 283it [02:57,  1.54it/s]Extractor Estimating: 284it [02:58,  1.50it/s]Extractor Estimating: 285it [02:58,  1.53it/s]Extractor Estimating: 286it [02:59,  1.52it/s]Extractor Estimating: 287it [03:00,  1.51it/s]Extractor Estimating: 288it [03:00,  1.52it/s]Extractor Estimating: 289it [03:01,  1.52it/s]Extractor Estimating: 290it [03:02,  1.51it/s]Extractor Estimating: 291it [03:02,  1.49it/s]Extractor Estimating: 292it [03:03,  1.48it/s]Extractor Estimating: 293it [03:04,  1.54it/s]Extractor Estimating: 294it [03:04,  1.56it/s]Extractor Estimating: 295it [03:05,  1.56it/s]Extractor Estimating: 296it [03:05,  1.56it/s]Extractor Estimating: 297it [03:06,  1.54it/s]Extractor Estimating: 298it [03:07,  1.50it/s]Extractor Estimating: 299it [03:07,  1.51it/s]Extractor Estimating: 300it [03:08,  1.52it/s]Extractor Estimating: 301it [03:09,  1.63it/s]Extractor Estimating: 302it [03:09,  1.70it/s]Extractor Estimating: 303it [03:10,  1.66it/s]Extractor Estimating: 304it [03:10,  1.69it/s]Extractor Estimating: 305it [03:11,  1.59it/s]Extractor Estimating: 306it [03:12,  1.62it/s]Extractor Estimating: 307it [03:12,  1.67it/s]Extractor Estimating: 308it [03:13,  1.73it/s]Extractor Estimating: 309it [03:13,  1.74it/s]Extractor Estimating: 310it [03:14,  1.75it/s]Extractor Estimating: 311it [03:14,  1.74it/s]Extractor Estimating: 312it [03:15,  1.74it/s]Extractor Estimating: 313it [03:16,  1.77it/s]Extractor Estimating: 314it [03:16,  1.57it/s]Extractor Estimating: 315it [03:17,  1.62it/s]Extractor Estimating: 316it [03:18,  1.61it/s]Extractor Estimating: 317it [03:18,  1.67it/s]Extractor Estimating: 318it [03:19,  1.70it/s]Extractor Estimating: 319it [03:19,  1.70it/s]Extractor Estimating: 320it [03:20,  1.60it/s]Extractor Estimating: 321it [03:21,  1.65it/s]Extractor Estimating: 322it [03:21,  1.61it/s]Extractor Estimating: 323it [03:22,  1.65it/s]Extractor Estimating: 324it [03:22,  1.73it/s]Extractor Estimating: 325it [03:23,  1.70it/s]Extractor Estimating: 326it [03:23,  1.71it/s]Extractor Estimating: 327it [03:24,  1.65it/s]Extractor Estimating: 328it [03:25,  1.67it/s]Extractor Estimating: 329it [03:25,  1.64it/s]Extractor Estimating: 330it [03:26,  1.60it/s]Extractor Estimating: 331it [03:27,  1.54it/s]Extractor Estimating: 332it [03:27,  1.54it/s]Extractor Estimating: 333it [03:28,  1.54it/s]Extractor Estimating: 334it [03:29,  1.57it/s]Extractor Estimating: 335it [03:29,  1.59it/s]Extractor Estimating: 336it [03:30,  1.51it/s]Extractor Estimating: 337it [03:31,  1.38it/s]Extractor Estimating: 338it [03:31,  1.45it/s]Extractor Estimating: 339it [03:32,  1.48it/s]Extractor Estimating: 340it [03:33,  1.53it/s]Extractor Estimating: 341it [03:33,  1.52it/s]Extractor Estimating: 342it [03:34,  1.56it/s]Extractor Estimating: 343it [03:35,  1.53it/s]Extractor Estimating: 344it [03:35,  1.57it/s]Extractor Estimating: 345it [03:36,  1.61it/s]Extractor Estimating: 346it [03:37,  1.57it/s]Extractor Estimating: 347it [03:37,  1.54it/s]Extractor Estimating: 348it [03:38,  1.57it/s]Extractor Estimating: 349it [03:38,  1.62it/s]Extractor Estimating: 350it [03:39,  1.59it/s]Extractor Estimating: 351it [03:40,  1.61it/s]Extractor Estimating: 352it [03:40,  1.63it/s]Extractor Estimating: 353it [03:41,  1.65it/s]Extractor Estimating: 354it [03:41,  1.66it/s]Extractor Estimating: 355it [03:42,  1.73it/s]Extractor Estimating: 356it [03:43,  1.73it/s]Extractor Estimating: 357it [03:43,  1.67it/s]Extractor Estimating: 358it [03:44,  1.70it/s]Extractor Estimating: 359it [03:44,  1.70it/s]Extractor Estimating: 360it [03:45,  1.69it/s]Extractor Estimating: 361it [03:46,  1.69it/s]Extractor Estimating: 362it [03:46,  1.69it/s]Extractor Estimating: 363it [03:47,  1.67it/s]Extractor Estimating: 364it [03:47,  1.66it/s]Extractor Estimating: 365it [03:48,  1.66it/s]Extractor Estimating: 366it [03:49,  1.69it/s]Extractor Estimating: 367it [03:49,  1.72it/s]Extractor Estimating: 368it [03:50,  1.71it/s]Extractor Estimating: 369it [03:50,  1.65it/s]Extractor Estimating: 370it [03:51,  1.69it/s]Extractor Estimating: 371it [03:51,  1.68it/s]Extractor Estimating: 372it [03:52,  1.63it/s]Extractor Estimating: 373it [03:53,  1.62it/s]Extractor Estimating: 374it [03:53,  1.63it/s]Extractor Estimating: 375it [03:54,  1.69it/s]Extractor Estimating: 376it [03:54,  1.72it/s]Extractor Estimating: 377it [03:55,  1.71it/s]Extractor Estimating: 378it [03:56,  1.64it/s]Extractor Estimating: 379it [03:56,  1.66it/s]Extractor Estimating: 380it [03:57,  1.65it/s]Extractor Estimating: 381it [03:58,  1.61it/s]Extractor Estimating: 382it [03:58,  1.63it/s]Extractor Estimating: 383it [03:59,  1.65it/s]Extractor Estimating: 384it [03:59,  1.67it/s]Extractor Estimating: 385it [04:00,  1.58it/s]Extractor Estimating: 386it [04:01,  1.57it/s]Extractor Estimating: 387it [04:01,  1.61it/s]Extractor Estimating: 388it [04:02,  1.60it/s]Extractor Estimating: 389it [04:03,  1.59it/s]Extractor Estimating: 390it [04:03,  1.63it/s]Extractor Estimating: 391it [04:04,  1.65it/s]Extractor Estimating: 392it [04:04,  1.71it/s]Extractor Estimating: 393it [04:05,  1.68it/s]Extractor Estimating: 394it [04:05,  1.68it/s]Extractor Estimating: 395it [04:06,  1.65it/s]Extractor Estimating: 396it [04:07,  1.65it/s]Extractor Estimating: 397it [04:07,  1.67it/s]Extractor Estimating: 398it [04:08,  1.65it/s]Extractor Estimating: 399it [04:08,  1.69it/s]Extractor Estimating: 400it [04:09,  1.66it/s]Extractor Estimating: 401it [04:10,  1.68it/s]Extractor Estimating: 402it [04:10,  1.75it/s]Extractor Estimating: 403it [04:11,  1.81it/s]Extractor Estimating: 404it [04:11,  1.83it/s]Extractor Estimating: 405it [04:12,  1.83it/s]Extractor Estimating: 406it [04:12,  1.83it/s]Extractor Estimating: 407it [04:13,  1.78it/s]Extractor Estimating: 408it [04:14,  1.75it/s]Extractor Estimating: 409it [04:14,  1.79it/s]Extractor Estimating: 410it [04:15,  1.80it/s]Extractor Estimating: 411it [04:15,  1.78it/s]Extractor Estimating: 412it [04:16,  1.81it/s]Extractor Estimating: 413it [04:16,  1.81it/s]Extractor Estimating: 414it [04:17,  1.77it/s]Extractor Estimating: 415it [04:17,  1.79it/s]Extractor Estimating: 416it [04:18,  1.81it/s]Extractor Estimating: 417it [04:19,  1.75it/s]Extractor Estimating: 418it [04:19,  1.77it/s]Extractor Estimating: 419it [04:20,  1.74it/s]Extractor Estimating: 420it [04:20,  1.76it/s]Extractor Estimating: 421it [04:21,  1.73it/s]Extractor Estimating: 422it [04:21,  1.79it/s]Extractor Estimating: 423it [04:22,  1.76it/s]Extractor Estimating: 424it [04:23,  1.77it/s]Extractor Estimating: 425it [04:23,  1.75it/s]Extractor Estimating: 426it [04:24,  1.69it/s]Extractor Estimating: 427it [04:24,  1.62it/s]Extractor Estimating: 428it [04:25,  1.58it/s]Extractor Estimating: 429it [04:26,  1.56it/s]Extractor Estimating: 430it [04:26,  1.62it/s]Extractor Estimating: 431it [04:27,  1.63it/s]Extractor Estimating: 432it [04:28,  1.64it/s]Extractor Estimating: 433it [04:28,  1.59it/s]Extractor Estimating: 434it [04:29,  1.60it/s]Extractor Estimating: 435it [04:30,  1.53it/s]Extractor Estimating: 436it [04:30,  1.52it/s]Extractor Estimating: 437it [04:31,  1.41it/s]Extractor Estimating: 438it [04:32,  1.44it/s]Extractor Estimating: 439it [04:32,  1.47it/s]Extractor Estimating: 440it [04:33,  1.48it/s]Extractor Estimating: 441it [04:34,  1.52it/s]Extractor Estimating: 442it [04:34,  1.54it/s]Extractor Estimating: 443it [04:35,  1.53it/s]Extractor Estimating: 444it [04:36,  1.52it/s]Extractor Estimating: 445it [04:36,  1.47it/s]Extractor Estimating: 446it [04:37,  1.45it/s]Extractor Estimating: 447it [04:38,  1.48it/s]Extractor Estimating: 448it [04:38,  1.47it/s]Extractor Estimating: 449it [04:39,  1.49it/s]Extractor Estimating: 450it [04:40,  1.51it/s]Extractor Estimating: 451it [04:40,  1.55it/s]Extractor Estimating: 452it [04:41,  1.53it/s]Extractor Estimating: 453it [04:42,  1.53it/s]Extractor Estimating: 454it [04:42,  1.54it/s]Extractor Estimating: 455it [04:43,  1.59it/s]Extractor Estimating: 456it [04:44,  1.53it/s]Extractor Estimating: 457it [04:44,  1.60it/s]Extractor Estimating: 458it [04:45,  1.64it/s]Extractor Estimating: 459it [04:45,  1.60it/s]Extractor Estimating: 460it [04:46,  1.64it/s]Extractor Estimating: 461it [04:47,  1.63it/s]Extractor Estimating: 462it [04:47,  1.52it/s]Extractor Estimating: 463it [04:48,  1.53it/s]Extractor Estimating: 464it [04:49,  1.57it/s]Extractor Estimating: 465it [04:49,  1.58it/s]Extractor Estimating: 466it [04:50,  1.62it/s]Extractor Estimating: 467it [04:50,  1.62it/s]Extractor Estimating: 468it [04:51,  1.61it/s]Extractor Estimating: 469it [04:52,  1.62it/s]Extractor Estimating: 470it [04:52,  1.60it/s]Extractor Estimating: 471it [04:53,  1.57it/s]Extractor Estimating: 472it [04:53,  1.61it/s]Extractor Estimating: 473it [04:54,  1.64it/s]Extractor Estimating: 474it [04:55,  1.65it/s]Extractor Estimating: 475it [04:55,  1.68it/s]Extractor Estimating: 476it [04:56,  1.62it/s]Extractor Estimating: 477it [04:57,  1.62it/s]Extractor Estimating: 478it [04:57,  1.65it/s]Extractor Estimating: 479it [04:58,  1.62it/s]Extractor Estimating: 480it [04:58,  1.56it/s]Extractor Estimating: 481it [04:59,  1.59it/s]Extractor Estimating: 482it [05:00,  1.62it/s]Extractor Estimating: 483it [05:00,  1.57it/s]Extractor Estimating: 484it [05:01,  1.53it/s]Extractor Estimating: 485it [05:02,  1.52it/s]Extractor Estimating: 486it [05:02,  1.50it/s]Extractor Estimating: 487it [05:03,  1.58it/s]Extractor Estimating: 488it [05:04,  1.57it/s]Extractor Estimating: 489it [05:04,  1.54it/s]Extractor Estimating: 490it [05:05,  1.54it/s]Extractor Estimating: 491it [05:05,  1.58it/s]Extractor Estimating: 492it [05:06,  1.57it/s]Extractor Estimating: 493it [05:07,  1.57it/s]Extractor Estimating: 494it [05:07,  1.57it/s]Extractor Estimating: 495it [05:08,  1.54it/s]Extractor Estimating: 496it [05:09,  1.55it/s]Extractor Estimating: 497it [05:09,  1.57it/s]Extractor Estimating: 498it [05:10,  1.55it/s]Extractor Estimating: 499it [05:11,  1.48it/s]Extractor Estimating: 500it [05:11,  1.55it/s]Extractor Estimating: 500it [05:11,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:06,380 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:06,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:06,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:06,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:06,388 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:30:06,995 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:30:06,996 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:30:07,681 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:30:08,737 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:30:08,737 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:11,640 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:11,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:11,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:11,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:30:11,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:30:12,273 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:30:12,274 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:30:12,825 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:30:12,978 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:30:12,978 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 17:33:07,883 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 17:33:07,947 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9978 mean pseudo reward: 0.9646233824018375
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 28464
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28564, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28564, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.053, loss:734.0985
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.059, loss:736.1025
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.049, loss:703.8663
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.056, loss:660.8380
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.048, loss:670.0245
>> valid entity prec:0.7068, rec:0.6833, f1:0.6949
>> valid relation prec:0.3919, rec:0.2162, f1:0.2787
>> valid relation with NER prec:0.3919, rec:0.2162, f1:0.2787
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.409, loss:679.7341
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.054, loss:671.9418
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.071, loss:733.3781
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.053, loss:700.8551
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.048, loss:710.9201
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6471, rec:0.6589, f1:0.6530
>> valid relation prec:0.3974, rec:0.1567, f1:0.2248
>> valid relation with NER prec:0.3974, rec:0.1567, f1:0.2248
g_step 1100, step 268, avg_time 2.408, loss:718.7441
g_step 1200, step 368, avg_time 1.065, loss:727.2234
g_step 1300, step 52, avg_time 1.052, loss:677.8430
g_step 1400, step 152, avg_time 1.072, loss:655.7120
g_step 1500, step 252, avg_time 1.062, loss:681.9170
>> valid entity prec:0.6610, rec:0.6353, f1:0.6479
>> valid relation prec:0.3737, rec:0.1350, f1:0.1983
>> valid relation with NER prec:0.3737, rec:0.1350, f1:0.1983
g_step 1600, step 352, avg_time 2.407, loss:688.2521
g_step 1700, step 36, avg_time 1.055, loss:658.7483
g_step 1800, step 136, avg_time 1.046, loss:633.8982
g_step 1900, step 236, avg_time 1.064, loss:649.8371
g_step 2000, step 336, avg_time 1.062, loss:667.2766
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6904, rec:0.5809, f1:0.6310
>> valid relation prec:0.3577, rec:0.1341, f1:0.1951
>> valid relation with NER prec:0.3577, rec:0.1341, f1:0.1951
g_step 2100, step 20, avg_time 2.414, loss:648.2174
g_step 2200, step 120, avg_time 1.053, loss:598.0254
g_step 2300, step 220, avg_time 1.072, loss:623.3955
g_step 2400, step 320, avg_time 1.055, loss:615.6873
g_step 2500, step 4, avg_time 1.055, loss:639.9965
>> valid entity prec:0.6655, rec:0.6578, f1:0.6616
>> valid relation prec:0.3419, rec:0.1721, f1:0.2290
>> valid relation with NER prec:0.3419, rec:0.1721, f1:0.2290
g_step 2600, step 104, avg_time 2.403, loss:566.6483
g_step 2700, step 204, avg_time 1.063, loss:609.2051
g_step 2800, step 304, avg_time 1.058, loss:601.8032
g_step 2900, step 404, avg_time 1.073, loss:603.7199
g_step 3000, step 88, avg_time 1.056, loss:558.9750
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6725, rec:0.6725, f1:0.6725
>> valid relation prec:0.3550, rec:0.1907, f1:0.2481
>> valid relation with NER prec:0.3550, rec:0.1907, f1:0.2481
g_step 3100, step 188, avg_time 2.411, loss:539.5034
g_step 3200, step 288, avg_time 1.053, loss:570.8484
g_step 3300, step 388, avg_time 1.062, loss:592.0653
g_step 3400, step 72, avg_time 1.068, loss:540.7627
g_step 3500, step 172, avg_time 1.060, loss:529.5261
>> valid entity prec:0.6697, rec:0.6810, f1:0.6753
>> valid relation prec:0.2640, rec:0.1307, f1:0.1748
>> valid relation with NER prec:0.2640, rec:0.1307, f1:0.1748
g_step 3600, step 272, avg_time 2.414, loss:542.5218
g_step 3700, step 372, avg_time 1.062, loss:551.5510
g_step 3800, step 56, avg_time 1.045, loss:527.3584
g_step 3900, step 156, avg_time 1.060, loss:521.8228
g_step 4000, step 256, avg_time 1.063, loss:525.6185
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.7044, rec:0.6320, f1:0.6662
>> valid relation prec:0.3105, rec:0.1636, f1:0.2143
>> valid relation with NER prec:0.3105, rec:0.1636, f1:0.2143
g_step 4100, step 356, avg_time 2.414, loss:536.7312
g_step 4200, step 40, avg_time 1.052, loss:535.3568
g_step 4300, step 140, avg_time 1.063, loss:477.2914
g_step 4400, step 240, avg_time 1.059, loss:507.2220
g_step 4500, step 340, avg_time 1.062, loss:523.5954
>> valid entity prec:0.6743, rec:0.6979, f1:0.6859
>> valid relation prec:0.2669, rec:0.1490, f1:0.1912
>> valid relation with NER prec:0.2669, rec:0.1490, f1:0.1912
g_step 4600, step 24, avg_time 2.402, loss:500.7186
g_step 4700, step 124, avg_time 1.061, loss:459.9299
g_step 4800, step 224, avg_time 1.062, loss:478.1707
g_step 4900, step 324, avg_time 1.054, loss:483.1610
g_step 5000, step 8, avg_time 1.062, loss:496.1846
learning rate was adjusted to 0.0008
>> valid entity prec:0.6626, rec:0.6325, f1:0.6472
>> valid relation prec:0.3415, rec:0.1630, f1:0.2207
>> valid relation with NER prec:0.3415, rec:0.1630, f1:0.2207
g_step 5100, step 108, avg_time 2.415, loss:435.2698
g_step 5200, step 208, avg_time 1.062, loss:458.9315
g_step 5300, step 308, avg_time 1.055, loss:482.4556
g_step 5400, step 408, avg_time 1.053, loss:480.6436
g_step 5500, step 92, avg_time 1.058, loss:408.8858
>> valid entity prec:0.6713, rec:0.6609, f1:0.6661
>> valid relation prec:0.2913, rec:0.1521, f1:0.1999
>> valid relation with NER prec:0.2913, rec:0.1521, f1:0.1999
g_step 5600, step 192, avg_time 2.406, loss:439.2382
g_step 5700, step 292, avg_time 1.061, loss:450.5932
g_step 5800, step 392, avg_time 1.053, loss:460.7545
g_step 5900, step 76, avg_time 1.062, loss:424.2000
g_step 6000, step 176, avg_time 1.069, loss:423.0511
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6801, rec:0.6284, f1:0.6532
>> valid relation prec:0.2826, rec:0.1350, f1:0.1827
>> valid relation with NER prec:0.2826, rec:0.1350, f1:0.1827
g_step 6100, step 276, avg_time 2.403, loss:424.5698
g_step 6200, step 376, avg_time 1.055, loss:445.1033
g_step 6300, step 60, avg_time 1.059, loss:385.9821
g_step 6400, step 160, avg_time 1.066, loss:391.5632
g_step 6500, step 260, avg_time 1.059, loss:421.6983
>> valid entity prec:0.6532, rec:0.6364, f1:0.6447
>> valid relation prec:0.2668, rec:0.1350, f1:0.1793
>> valid relation with NER prec:0.2668, rec:0.1350, f1:0.1793
g_step 6600, step 360, avg_time 2.412, loss:423.3558
g_step 6700, step 44, avg_time 1.044, loss:419.3672
g_step 6800, step 144, avg_time 1.071, loss:393.9752
g_step 6900, step 244, avg_time 1.050, loss:402.2906
g_step 7000, step 344, avg_time 1.064, loss:400.7417
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6920, rec:0.6536, f1:0.6723
>> valid relation prec:0.2646, rec:0.1447, f1:0.1871
>> valid relation with NER prec:0.2646, rec:0.1447, f1:0.1871
g_step 7100, step 28, avg_time 2.408, loss:406.5542
g_step 7200, step 128, avg_time 1.062, loss:366.1345
g_step 7300, step 228, avg_time 1.061, loss:376.5297
g_step 7400, step 328, avg_time 1.058, loss:395.2781
g_step 7500, step 12, avg_time 1.052, loss:391.4655
>> valid entity prec:0.6718, rec:0.6813, f1:0.6765
>> valid relation prec:0.2913, rec:0.1590, f1:0.2057
>> valid relation with NER prec:0.2913, rec:0.1590, f1:0.2057
g_step 7600, step 112, avg_time 2.426, loss:371.8188
g_step 7700, step 212, avg_time 1.059, loss:367.2337
g_step 7800, step 312, avg_time 1.058, loss:377.6854
g_step 7900, step 412, avg_time 1.058, loss:381.1273
g_step 8000, step 96, avg_time 1.050, loss:335.7815
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6873, rec:0.6513, f1:0.6688
>> valid relation prec:0.3150, rec:0.1699, f1:0.2207
>> valid relation with NER prec:0.3150, rec:0.1699, f1:0.2207
g_step 8100, step 196, avg_time 2.421, loss:356.2629
g_step 8200, step 296, avg_time 1.057, loss:361.0349
g_step 8300, step 396, avg_time 1.060, loss:364.6040
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 17:33:07 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 17:33:07 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_17-33-07_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 17:33:08 - WARNING - datasets.builder -   Using custom data configuration default-39121c35b97dc6a7
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-39121c35b97dc6a7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 17:33:09,782 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:33:09,783 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:33:09,784 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:33:09,785 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:33:09,867 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:09,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:09,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:09,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:09,883 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:09,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:33:09,883 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 17:33:10,026 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:33:13,144 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 17:33:13,167 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-39121c35b97dc6a7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.97ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.83ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.15ba/s] 36%|███▋      | 4/11 [00:01<00:02,  3.47ba/s] 45%|████▌     | 5/11 [00:01<00:01,  3.80ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.04ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.20ba/s] 73%|███████▎  | 8/11 [00:01<00:00,  4.31ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.37ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.43ba/s]100%|██████████| 11/11 [00:02<00:00,  4.50ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.98ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.20ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.16ba/s]100%|██████████| 4/4 [00:00<00:00,  5.19ba/s]100%|██████████| 4/4 [00:00<00:00,  4.74ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  5.91ba/s] 27%|██▋       | 3/11 [00:00<00:00,  8.53ba/s] 36%|███▋      | 4/11 [00:00<00:00,  8.96ba/s] 55%|█████▍    | 6/11 [00:00<00:00,  9.50ba/s] 73%|███████▎  | 8/11 [00:00<00:00,  9.71ba/s] 82%|████████▏ | 9/11 [00:00<00:00,  9.73ba/s] 91%|█████████ | 10/11 [00:01<00:00,  9.75ba/s]100%|██████████| 11/11 [00:01<00:00, 10.25ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.45ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.63ba/s]100%|██████████| 4/4 [00:00<00:00, 10.89ba/s]
[INFO|trainer.py:414] 2023-08-28 17:33:18,691 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 17:33:18,706 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 17:33:18,706 >>   Num examples = 10011
[INFO|trainer.py:1149] 2023-08-28 17:33:18,706 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 17:33:18,706 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 17:33:18,706 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 17:33:18,706 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 17:33:18,706 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<05:18,  2.45it/s]  0%|          | 2/780 [00:00<06:26,  2.01it/s]  0%|          | 3/780 [00:01<05:45,  2.25it/s]  1%|          | 4/780 [00:01<05:18,  2.43it/s]  1%|          | 5/780 [00:02<04:50,  2.66it/s]  1%|          | 6/780 [00:02<04:27,  2.90it/s]  1%|          | 7/780 [00:02<04:13,  3.05it/s]  1%|          | 8/780 [00:02<04:03,  3.17it/s]  1%|          | 9/780 [00:03<03:57,  3.24it/s]  1%|▏         | 10/780 [00:03<03:56,  3.26it/s]  1%|▏         | 11/780 [00:03<03:51,  3.32it/s]  2%|▏         | 12/780 [00:04<03:47,  3.37it/s]  2%|▏         | 13/780 [00:04<03:49,  3.35it/s]  2%|▏         | 14/780 [00:04<03:48,  3.35it/s]  2%|▏         | 15/780 [00:04<03:45,  3.39it/s]  2%|▏         | 16/780 [00:05<03:43,  3.41it/s]  2%|▏         | 17/780 [00:05<03:42,  3.43it/s]  2%|▏         | 18/780 [00:05<03:41,  3.45it/s]  2%|▏         | 19/780 [00:06<03:40,  3.46it/s]  3%|▎         | 20/780 [00:06<03:39,  3.46it/s]  3%|▎         | 21/780 [00:06<03:38,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.47it/s]  3%|▎         | 23/780 [00:07<03:38,  3.47it/s]  3%|▎         | 24/780 [00:07<03:37,  3.47it/s]  3%|▎         | 25/780 [00:07<03:37,  3.46it/s]  3%|▎         | 26/780 [00:08<03:37,  3.47it/s]  3%|▎         | 27/780 [00:08<03:36,  3.47it/s]  4%|▎         | 28/780 [00:08<03:36,  3.47it/s]  4%|▎         | 29/780 [00:08<03:36,  3.47it/s]  4%|▍         | 30/780 [00:09<03:35,  3.47it/s]  4%|▍         | 31/780 [00:09<03:35,  3.47it/s]  4%|▍         | 32/780 [00:09<03:35,  3.48it/s]  4%|▍         | 33/780 [00:10<03:34,  3.48it/s]  4%|▍         | 34/780 [00:10<03:34,  3.48it/s]  4%|▍         | 35/780 [00:10<03:34,  3.48it/s]  5%|▍         | 36/780 [00:10<03:34,  3.48it/s]  5%|▍         | 37/780 [00:11<03:33,  3.48it/s]  5%|▍         | 38/780 [00:11<03:33,  3.48it/s]  5%|▌         | 39/780 [00:11<03:33,  3.48it/s]  5%|▌         | 40/780 [00:12<03:32,  3.48it/s]  5%|▌         | 41/780 [00:12<03:33,  3.47it/s]  5%|▌         | 42/780 [00:12<03:39,  3.35it/s]  6%|▌         | 43/780 [00:13<03:37,  3.39it/s]  6%|▌         | 44/780 [00:13<03:35,  3.42it/s]  6%|▌         | 45/780 [00:13<03:34,  3.43it/s]  6%|▌         | 46/780 [00:13<03:32,  3.45it/s]  6%|▌         | 47/780 [00:14<03:32,  3.46it/s]  6%|▌         | 48/780 [00:14<03:31,  3.46it/s]  6%|▋         | 49/780 [00:14<03:30,  3.47it/s]  6%|▋         | 50/780 [00:15<03:30,  3.47it/s]  7%|▋         | 51/780 [00:15<03:30,  3.47it/s]  7%|▋         | 52/780 [00:15<03:29,  3.47it/s]  7%|▋         | 53/780 [00:15<03:29,  3.47it/s]  7%|▋         | 54/780 [00:16<03:29,  3.47it/s]  7%|▋         | 55/780 [00:16<03:28,  3.47it/s]  7%|▋         | 56/780 [00:16<03:28,  3.47it/s]  7%|▋         | 57/780 [00:17<03:28,  3.47it/s]  7%|▋         | 58/780 [00:17<03:27,  3.47it/s]  8%|▊         | 59/780 [00:17<03:27,  3.47it/s]  8%|▊         | 60/780 [00:17<03:29,  3.43it/s]  8%|▊         | 61/780 [00:18<03:28,  3.44it/s]  8%|▊         | 62/780 [00:18<03:27,  3.45it/s]  8%|▊         | 63/780 [00:18<03:27,  3.46it/s]  8%|▊         | 64/780 [00:19<03:26,  3.46it/s]  8%|▊         | 65/780 [00:19<03:26,  3.47it/s]  8%|▊         | 66/780 [00:19<03:25,  3.47it/s]  9%|▊         | 67/780 [00:19<03:25,  3.47it/s]  9%|▊         | 68/780 [00:20<03:25,  3.47it/s]  9%|▉         | 69/780 [00:20<03:24,  3.47it/s]  9%|▉         | 70/780 [00:20<03:24,  3.47it/s]  9%|▉         | 71/780 [00:21<03:24,  3.47it/s]  9%|▉         | 72/780 [00:21<03:23,  3.47it/s]  9%|▉         | 73/780 [00:21<03:23,  3.47it/s]  9%|▉         | 74/780 [00:21<03:23,  3.47it/s] 10%|▉         | 75/780 [00:22<03:22,  3.47it/s] 10%|▉         | 76/780 [00:22<03:22,  3.47it/s] 10%|▉         | 77/780 [00:22<03:22,  3.47it/s] 10%|█         | 78/780 [00:23<03:32,  3.30it/s] 10%|█         | 79/780 [00:23<03:29,  3.35it/s] 10%|█         | 80/780 [00:23<03:26,  3.38it/s] 10%|█         | 81/780 [00:24<03:24,  3.41it/s] 11%|█         | 82/780 [00:24<03:23,  3.43it/s] 11%|█         | 83/780 [00:24<03:22,  3.44it/s] 11%|█         | 84/780 [00:24<03:21,  3.45it/s] 11%|█         | 85/780 [00:25<03:21,  3.46it/s] 11%|█         | 86/780 [00:25<03:20,  3.46it/s] 11%|█         | 87/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 88/780 [00:26<03:19,  3.46it/s] 11%|█▏        | 89/780 [00:26<03:19,  3.47it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.47it/s] 12%|█▏        | 91/780 [00:26<03:18,  3.47it/s] 12%|█▏        | 92/780 [00:27<03:18,  3.47it/s] 12%|█▏        | 93/780 [00:27<03:18,  3.47it/s] 12%|█▏        | 94/780 [00:27<03:17,  3.47it/s] 12%|█▏        | 95/780 [00:28<03:18,  3.45it/s] 12%|█▏        | 96/780 [00:28<03:17,  3.46it/s] 12%|█▏        | 97/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 98/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 99/780 [00:29<03:16,  3.47it/s] 13%|█▎        | 100/780 [00:29<03:16,  3.47it/s] 13%|█▎        | 101/780 [00:29<03:15,  3.47it/s] 13%|█▎        | 102/780 [00:30<03:15,  3.47it/s] 13%|█▎        | 103/780 [00:30<03:15,  3.47it/s] 13%|█▎        | 104/780 [00:30<03:14,  3.47it/s] 13%|█▎        | 105/780 [00:30<03:14,  3.47it/s] 14%|█▎        | 106/780 [00:31<03:14,  3.47it/s] 14%|█▎        | 107/780 [00:31<03:14,  3.47it/s] 14%|█▍        | 108/780 [00:31<03:13,  3.47it/s] 14%|█▍        | 109/780 [00:32<03:13,  3.47it/s] 14%|█▍        | 110/780 [00:32<03:13,  3.47it/s] 14%|█▍        | 111/780 [00:32<03:12,  3.47it/s] 14%|█▍        | 112/780 [00:32<03:12,  3.47it/s] 14%|█▍        | 113/780 [00:33<03:26,  3.23it/s] 15%|█▍        | 114/780 [00:33<03:22,  3.30it/s] 15%|█▍        | 115/780 [00:33<03:18,  3.35it/s] 15%|█▍        | 116/780 [00:34<03:16,  3.38it/s] 15%|█▌        | 117/780 [00:34<03:14,  3.41it/s] 15%|█▌        | 118/780 [00:34<03:13,  3.42it/s] 15%|█▌        | 119/780 [00:35<03:12,  3.44it/s] 15%|█▌        | 120/780 [00:35<03:11,  3.44it/s] 16%|█▌        | 121/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.46it/s] 16%|█▌        | 123/780 [00:36<03:10,  3.46it/s] 16%|█▌        | 124/780 [00:36<03:09,  3.46it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.46it/s] 16%|█▌        | 126/780 [00:37<03:09,  3.46it/s] 16%|█▋        | 127/780 [00:37<03:08,  3.46it/s] 16%|█▋        | 128/780 [00:37<03:08,  3.46it/s] 17%|█▋        | 129/780 [00:37<03:07,  3.47it/s] 17%|█▋        | 130/780 [00:38<03:07,  3.46it/s] 17%|█▋        | 131/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.46it/s] 17%|█▋        | 133/780 [00:39<03:07,  3.46it/s] 17%|█▋        | 134/780 [00:39<03:06,  3.46it/s] 17%|█▋        | 135/780 [00:39<03:06,  3.46it/s] 17%|█▋        | 136/780 [00:39<03:05,  3.46it/s] 18%|█▊        | 137/780 [00:40<03:05,  3.47it/s] 18%|█▊        | 138/780 [00:40<03:05,  3.46it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.46it/s] 18%|█▊        | 140/780 [00:41<03:04,  3.46it/s] 18%|█▊        | 141/780 [00:41<03:04,  3.46it/s] 18%|█▊        | 142/780 [00:41<03:04,  3.47it/s] 18%|█▊        | 143/780 [00:42<03:03,  3.46it/s] 18%|█▊        | 144/780 [00:42<03:03,  3.46it/s] 19%|█▊        | 145/780 [00:42<03:03,  3.46it/s] 19%|█▊        | 146/780 [00:42<03:02,  3.47it/s] 19%|█▉        | 147/780 [00:43<03:02,  3.47it/s] 19%|█▉        | 148/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 149/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 150/780 [00:44<03:02,  3.46it/s] 19%|█▉        | 151/780 [00:44<03:01,  3.46it/s] 19%|█▉        | 152/780 [00:44<03:01,  3.46it/s] 20%|█▉        | 153/780 [00:44<03:01,  3.46it/s] 20%|█▉        | 154/780 [00:45<03:00,  3.46it/s] 20%|█▉        | 155/780 [00:45<03:00,  3.46it/s] 20%|██        | 156/780 [00:45<03:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 17:34:04,586 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:34:04,586 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 17:34:04,586 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.46it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.84it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.94it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.16it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.86it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.47it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.11it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.81it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.81it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.83it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.92it/s][A
 14%|█▍        | 63/438 [00:01<00:07, 46.89it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.89it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.95it/s][A
 18%|█▊        | 78/438 [00:01<00:08, 44.14it/s][A
 19%|█▉        | 84/438 [00:01<00:07, 45.85it/s][A
 20%|██        | 89/438 [00:01<00:07, 46.21it/s][A
 21%|██▏       | 94/438 [00:01<00:07, 46.25it/s][A
 23%|██▎       | 99/438 [00:02<00:07, 46.42it/s][A
 24%|██▎       | 104/438 [00:02<00:07, 46.59it/s][A
 25%|██▍       | 109/438 [00:02<00:07, 46.74it/s][A
 26%|██▌       | 114/438 [00:02<00:06, 46.82it/s][A
 27%|██▋       | 119/438 [00:02<00:06, 46.67it/s][A
 28%|██▊       | 124/438 [00:02<00:06, 46.57it/s][A
 29%|██▉       | 129/438 [00:02<00:06, 46.62it/s][A
 31%|███       | 134/438 [00:02<00:06, 46.63it/s][A
 32%|███▏      | 139/438 [00:02<00:06, 46.73it/s][A
 33%|███▎      | 144/438 [00:03<00:06, 46.68it/s][A
 34%|███▍      | 149/438 [00:03<00:06, 46.66it/s][A
 35%|███▌      | 154/438 [00:03<00:06, 46.70it/s][A
 36%|███▋      | 159/438 [00:03<00:06, 42.51it/s][A
 37%|███▋      | 164/438 [00:03<00:06, 43.80it/s][A
 39%|███▊      | 169/438 [00:03<00:06, 44.77it/s][A
 40%|███▉      | 174/438 [00:03<00:05, 45.34it/s][A
 41%|████      | 179/438 [00:03<00:05, 45.89it/s][A
 42%|████▏     | 184/438 [00:03<00:05, 46.15it/s][A
 43%|████▎     | 189/438 [00:04<00:05, 46.27it/s][A
 44%|████▍     | 194/438 [00:04<00:05, 46.50it/s][A
 45%|████▌     | 199/438 [00:04<00:05, 46.49it/s][A
 47%|████▋     | 204/438 [00:04<00:05, 46.37it/s][A
 48%|████▊     | 209/438 [00:04<00:04, 46.50it/s][A
 49%|████▉     | 214/438 [00:04<00:04, 46.58it/s][A
 50%|█████     | 219/438 [00:04<00:04, 46.66it/s][A
 51%|█████     | 224/438 [00:04<00:04, 46.81it/s][A
 52%|█████▏    | 229/438 [00:04<00:04, 46.79it/s][A
 53%|█████▎    | 234/438 [00:05<00:04, 46.80it/s][A
 55%|█████▍    | 239/438 [00:05<00:04, 46.90it/s][A
 56%|█████▌    | 244/438 [00:05<00:04, 46.68it/s][A
 57%|█████▋    | 249/438 [00:05<00:04, 46.62it/s][A
 58%|█████▊    | 254/438 [00:05<00:03, 46.57it/s][A
 59%|█████▉    | 259/438 [00:05<00:03, 46.61it/s][A
 60%|██████    | 264/438 [00:05<00:03, 46.75it/s][A
 61%|██████▏   | 269/438 [00:05<00:03, 46.85it/s][A
 63%|██████▎   | 274/438 [00:05<00:03, 46.80it/s][A
 64%|██████▎   | 279/438 [00:05<00:03, 46.85it/s][A
 65%|██████▍   | 284/438 [00:06<00:03, 46.89it/s][A
 66%|██████▌   | 289/438 [00:06<00:03, 46.72it/s][A
 67%|██████▋   | 294/438 [00:06<00:03, 46.74it/s][A
 68%|██████▊   | 299/438 [00:06<00:02, 46.62it/s][A
 69%|██████▉   | 304/438 [00:06<00:02, 46.57it/s][A
 71%|███████   | 309/438 [00:06<00:02, 46.71it/s][A
 72%|███████▏  | 314/438 [00:06<00:02, 46.81it/s][A
 73%|███████▎  | 319/438 [00:06<00:02, 46.78it/s][A
 74%|███████▍  | 324/438 [00:06<00:02, 46.90it/s][A
 75%|███████▌  | 329/438 [00:07<00:02, 46.82it/s][A
 76%|███████▋  | 334/438 [00:07<00:02, 46.79it/s][A
 77%|███████▋  | 339/438 [00:07<00:02, 46.81it/s][A
 79%|███████▊  | 344/438 [00:07<00:02, 46.66it/s][A
 80%|███████▉  | 349/438 [00:07<00:01, 46.63it/s][A
 81%|████████  | 354/438 [00:07<00:01, 46.65it/s][A
 82%|████████▏ | 359/438 [00:07<00:01, 46.61it/s][A
 83%|████████▎ | 364/438 [00:07<00:01, 46.79it/s][A
 84%|████████▍ | 369/438 [00:07<00:01, 46.89it/s][A
 85%|████████▌ | 374/438 [00:08<00:01, 46.82it/s][A
 87%|████████▋ | 379/438 [00:08<00:01, 46.79it/s][A
 88%|████████▊ | 384/438 [00:08<00:01, 46.78it/s][A
 89%|████████▉ | 389/438 [00:08<00:01, 46.69it/s][A
 90%|████████▉ | 394/438 [00:08<00:00, 46.76it/s][A
 91%|█████████ | 399/438 [00:08<00:00, 46.58it/s][A
 92%|█████████▏| 404/438 [00:08<00:00, 46.51it/s][A
 93%|█████████▎| 409/438 [00:08<00:00, 46.52it/s][A
 95%|█████████▍| 414/438 [00:08<00:00, 46.53it/s][A
 96%|█████████▌| 419/438 [00:08<00:00, 46.71it/s][A
 97%|█████████▋| 424/438 [00:09<00:00, 46.73it/s][A
 98%|█████████▊| 429/438 [00:09<00:00, 46.67it/s][A
 99%|█████████▉| 434/438 [00:09<00:00, 46.70it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.70it/s][A 20%|██        | 156/780 [00:55<03:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:34:14,066 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 17:34:14,101 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:34:20,261 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:34:20,318 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:34:20,351 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:13<1:28:59,  8.57s/it] 20%|██        | 158/780 [01:13<1:03:09,  6.09s/it] 20%|██        | 159/780 [01:14<45:01,  4.35s/it]   21%|██        | 160/780 [01:14<32:21,  3.13s/it] 21%|██        | 161/780 [01:14<23:30,  2.28s/it] 21%|██        | 162/780 [01:15<17:19,  1.68s/it] 21%|██        | 163/780 [01:15<12:59,  1.26s/it] 21%|██        | 164/780 [01:15<09:57,  1.03it/s] 21%|██        | 165/780 [01:15<07:50,  1.31it/s] 21%|██▏       | 166/780 [01:16<06:57,  1.47it/s] 21%|██▏       | 167/780 [01:16<05:44,  1.78it/s] 22%|██▏       | 168/780 [01:17<04:53,  2.08it/s] 22%|██▏       | 169/780 [01:17<04:17,  2.37it/s] 22%|██▏       | 170/780 [01:17<03:52,  2.62it/s] 22%|██▏       | 171/780 [01:17<03:35,  2.83it/s] 22%|██▏       | 172/780 [01:18<03:22,  3.00it/s] 22%|██▏       | 173/780 [01:18<03:14,  3.12it/s] 22%|██▏       | 174/780 [01:18<03:08,  3.22it/s] 22%|██▏       | 175/780 [01:19<03:03,  3.29it/s] 23%|██▎       | 176/780 [01:19<03:12,  3.14it/s] 23%|██▎       | 177/780 [01:19<03:06,  3.23it/s] 23%|██▎       | 178/780 [01:19<03:02,  3.30it/s] 23%|██▎       | 179/780 [01:20<02:59,  3.35it/s] 23%|██▎       | 180/780 [01:20<02:57,  3.39it/s] 23%|██▎       | 181/780 [01:20<02:55,  3.41it/s] 23%|██▎       | 182/780 [01:21<02:54,  3.43it/s] 23%|██▎       | 183/780 [01:21<02:53,  3.44it/s] 24%|██▎       | 184/780 [01:21<02:52,  3.45it/s] 24%|██▎       | 185/780 [01:21<02:52,  3.46it/s] 24%|██▍       | 186/780 [01:22<02:51,  3.46it/s] 24%|██▍       | 187/780 [01:22<02:52,  3.44it/s] 24%|██▍       | 188/780 [01:22<02:51,  3.45it/s] 24%|██▍       | 189/780 [01:23<02:51,  3.46it/s] 24%|██▍       | 190/780 [01:23<02:50,  3.46it/s] 24%|██▍       | 191/780 [01:23<02:50,  3.46it/s] 25%|██▍       | 192/780 [01:24<02:49,  3.46it/s] 25%|██▍       | 193/780 [01:24<02:49,  3.47it/s] 25%|██▍       | 194/780 [01:24<02:48,  3.47it/s] 25%|██▌       | 195/780 [01:24<02:48,  3.47it/s] 25%|██▌       | 196/780 [01:25<02:48,  3.47it/s] 25%|██▌       | 197/780 [01:25<02:48,  3.47it/s] 25%|██▌       | 198/780 [01:25<02:51,  3.40it/s] 26%|██▌       | 199/780 [01:26<02:49,  3.42it/s] 26%|██▌       | 200/780 [01:26<02:48,  3.44it/s] 26%|██▌       | 201/780 [01:26<02:47,  3.45it/s] 26%|██▌       | 202/780 [01:26<02:47,  3.45it/s] 26%|██▌       | 203/780 [01:27<02:46,  3.46it/s] 26%|██▌       | 204/780 [01:27<02:46,  3.46it/s] 26%|██▋       | 205/780 [01:27<02:45,  3.47it/s] 26%|██▋       | 206/780 [01:28<02:45,  3.47it/s] 27%|██▋       | 207/780 [01:28<02:45,  3.47it/s] 27%|██▋       | 208/780 [01:28<02:44,  3.47it/s] 27%|██▋       | 209/780 [01:28<02:45,  3.45it/s] 27%|██▋       | 210/780 [01:29<02:44,  3.45it/s] 27%|██▋       | 211/780 [01:29<02:44,  3.46it/s] 27%|██▋       | 212/780 [01:29<02:43,  3.46it/s] 27%|██▋       | 213/780 [01:30<02:43,  3.47it/s] 27%|██▋       | 214/780 [01:30<02:43,  3.47it/s] 28%|██▊       | 215/780 [01:30<02:42,  3.47it/s] 28%|██▊       | 216/780 [01:30<02:42,  3.47it/s] 28%|██▊       | 217/780 [01:31<02:42,  3.47it/s] 28%|██▊       | 218/780 [01:31<02:42,  3.47it/s] 28%|██▊       | 219/780 [01:31<02:41,  3.47it/s] 28%|██▊       | 220/780 [01:32<02:46,  3.36it/s] 28%|██▊       | 221/780 [01:32<02:44,  3.39it/s] 28%|██▊       | 222/780 [01:32<02:43,  3.42it/s] 29%|██▊       | 223/780 [01:33<02:42,  3.43it/s] 29%|██▊       | 224/780 [01:33<02:41,  3.44it/s] 29%|██▉       | 225/780 [01:33<02:40,  3.45it/s] 29%|██▉       | 226/780 [01:33<02:40,  3.46it/s] 29%|██▉       | 227/780 [01:34<02:39,  3.46it/s] 29%|██▉       | 228/780 [01:34<02:39,  3.46it/s] 29%|██▉       | 229/780 [01:34<02:38,  3.47it/s] 29%|██▉       | 230/780 [01:35<02:38,  3.47it/s] 30%|██▉       | 231/780 [01:35<02:39,  3.43it/s] 30%|██▉       | 232/780 [01:35<02:39,  3.44it/s] 30%|██▉       | 233/780 [01:35<02:38,  3.45it/s] 30%|███       | 234/780 [01:36<02:37,  3.46it/s] 30%|███       | 235/780 [01:36<02:37,  3.46it/s] 30%|███       | 236/780 [01:36<02:55,  3.09it/s] 30%|███       | 237/780 [01:37<02:49,  3.20it/s] 31%|███       | 238/780 [01:37<02:45,  3.27it/s] 31%|███       | 239/780 [01:37<02:42,  3.33it/s] 31%|███       | 240/780 [01:38<02:40,  3.37it/s] 31%|███       | 241/780 [01:38<02:38,  3.40it/s] 31%|███       | 242/780 [01:38<02:37,  3.42it/s] 31%|███       | 243/780 [01:38<02:36,  3.43it/s] 31%|███▏      | 244/780 [01:39<02:35,  3.44it/s] 31%|███▏      | 245/780 [01:39<02:35,  3.45it/s] 32%|███▏      | 246/780 [01:39<02:37,  3.38it/s] 32%|███▏      | 247/780 [01:40<02:36,  3.41it/s] 32%|███▏      | 248/780 [01:40<02:35,  3.43it/s] 32%|███▏      | 249/780 [01:40<02:34,  3.44it/s] 32%|███▏      | 250/780 [01:40<02:33,  3.45it/s] 32%|███▏      | 251/780 [01:41<02:33,  3.46it/s] 32%|███▏      | 252/780 [01:41<02:32,  3.46it/s] 32%|███▏      | 253/780 [01:41<02:32,  3.46it/s] 33%|███▎      | 254/780 [01:42<02:31,  3.46it/s] 33%|███▎      | 255/780 [01:42<02:31,  3.47it/s] 33%|███▎      | 256/780 [01:42<02:31,  3.46it/s] 33%|███▎      | 257/780 [01:42<02:30,  3.47it/s] 33%|███▎      | 258/780 [01:43<02:30,  3.47it/s] 33%|███▎      | 259/780 [01:43<02:30,  3.47it/s] 33%|███▎      | 260/780 [01:43<02:29,  3.47it/s] 33%|███▎      | 261/780 [01:44<02:29,  3.47it/s] 34%|███▎      | 262/780 [01:44<02:29,  3.47it/s] 34%|███▎      | 263/780 [01:44<02:29,  3.47it/s] 34%|███▍      | 264/780 [01:45<02:36,  3.30it/s] 34%|███▍      | 265/780 [01:45<02:33,  3.35it/s] 34%|███▍      | 266/780 [01:45<02:32,  3.38it/s] 34%|███▍      | 267/780 [01:45<02:30,  3.41it/s] 34%|███▍      | 268/780 [01:46<02:29,  3.42it/s] 34%|███▍      | 269/780 [01:46<02:28,  3.44it/s] 35%|███▍      | 270/780 [01:46<02:27,  3.45it/s] 35%|███▍      | 271/780 [01:47<02:27,  3.45it/s] 35%|███▍      | 272/780 [01:47<02:26,  3.46it/s] 35%|███▌      | 273/780 [01:47<02:28,  3.42it/s] 35%|███▌      | 274/780 [01:47<02:27,  3.44it/s] 35%|███▌      | 275/780 [01:48<02:26,  3.45it/s] 35%|███▌      | 276/780 [01:48<02:25,  3.45it/s] 36%|███▌      | 277/780 [01:48<02:25,  3.46it/s] 36%|███▌      | 278/780 [01:49<02:25,  3.46it/s] 36%|███▌      | 279/780 [01:49<02:33,  3.26it/s] 36%|███▌      | 280/780 [01:49<02:31,  3.31it/s] 36%|███▌      | 281/780 [01:50<02:30,  3.32it/s] 36%|███▌      | 282/780 [01:50<02:28,  3.36it/s] 36%|███▋      | 283/780 [01:50<02:26,  3.39it/s] 36%|███▋      | 284/780 [01:50<02:25,  3.42it/s] 37%|███▋      | 285/780 [01:51<02:24,  3.43it/s] 37%|███▋      | 286/780 [01:51<02:23,  3.44it/s] 37%|███▋      | 287/780 [01:51<02:22,  3.45it/s] 37%|███▋      | 288/780 [01:52<02:22,  3.46it/s] 37%|███▋      | 289/780 [01:52<02:21,  3.46it/s] 37%|███▋      | 290/780 [01:52<02:21,  3.46it/s] 37%|███▋      | 291/780 [01:52<02:21,  3.46it/s] 37%|███▋      | 292/780 [01:53<02:20,  3.46it/s] 38%|███▊      | 293/780 [01:53<02:20,  3.47it/s] 38%|███▊      | 294/780 [01:53<02:20,  3.47it/s] 38%|███▊      | 295/780 [01:54<02:19,  3.47it/s] 38%|███▊      | 296/780 [01:54<02:19,  3.47it/s] 38%|███▊      | 297/780 [01:54<02:19,  3.47it/s] 38%|███▊      | 298/780 [01:54<02:18,  3.47it/s] 38%|███▊      | 299/780 [01:55<02:19,  3.44it/s] 38%|███▊      | 300/780 [01:55<02:19,  3.45it/s] 39%|███▊      | 301/780 [01:55<02:18,  3.46it/s] 39%|███▊      | 302/780 [01:56<02:18,  3.46it/s] 39%|███▉      | 303/780 [01:56<02:17,  3.46it/s] 39%|███▉      | 304/780 [01:56<02:17,  3.46it/s] 39%|███▉      | 305/780 [01:56<02:17,  3.47it/s] 39%|███▉      | 306/780 [01:57<02:23,  3.31it/s] 39%|███▉      | 307/780 [01:57<02:20,  3.36it/s] 39%|███▉      | 308/780 [01:57<02:19,  3.39it/s] 40%|███▉      | 309/780 [01:58<02:18,  3.41it/s] 40%|███▉      | 310/780 [01:58<02:17,  3.42it/s] 40%|███▉      | 311/780 [01:58<02:16,  3.44it/s] 40%|████      | 312/780 [01:59<02:15,  3.44it/s][INFO|trainer.py:2140] 2023-08-28 17:35:17,827 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:35:17,827 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 17:35:17,827 >>   Batch size = 8
{'eval_loss': 0.9625846147537231, 'eval_runtime': 9.4085, 'eval_samples_per_second': 371.684, 'eval_steps_per_second': 46.554, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.90it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.77it/s][A
  4%|▍         | 18/438 [00:00<00:08, 49.02it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.31it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.81it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.40it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.07it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.56it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.97it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.15it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.39it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.53it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.66it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.78it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.84it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.60it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.64it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.65it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.65it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.77it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.83it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.78it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.87it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.92it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.79it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.72it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.56it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.60it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.72it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.78it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.82it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.79it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.84it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.81it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.76it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.68it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.57it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.61it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.63it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.77it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.76it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.75it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.80it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.80it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.75it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.71it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.65it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.72it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.71it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.74it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.75it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.77it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.71it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.74it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.75it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.73it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.73it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.62it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.61it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.69it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.72it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.75it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.77it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.68it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.69it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.70it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.61it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.62it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.69it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.71it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.78it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.78it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.68it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.75it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.77it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.73it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.71it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.67it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.70it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.74it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.73it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.78it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.78it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.68it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.65it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.75it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.77it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.77it/s][A 40%|████      | 312/780 [02:08<02:15,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:35:27,248 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 17:35:27,300 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:35:31,509 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:35:31,531 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:35:31,543 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:23<58:08,  7.47s/it] 40%|████      | 314/780 [02:23<41:22,  5.33s/it] 40%|████      | 315/780 [02:23<29:34,  3.82s/it] 41%|████      | 316/780 [02:24<21:19,  2.76s/it] 41%|████      | 317/780 [02:24<15:33,  2.02s/it] 41%|████      | 318/780 [02:24<11:31,  1.50s/it] 41%|████      | 319/780 [02:24<08:43,  1.13s/it] 41%|████      | 320/780 [02:25<06:45,  1.14it/s] 41%|████      | 321/780 [02:25<05:22,  1.42it/s] 41%|████▏     | 322/780 [02:25<04:24,  1.73it/s] 41%|████▏     | 323/780 [02:26<03:44,  2.04it/s] 42%|████▏     | 324/780 [02:26<03:16,  2.32it/s] 42%|████▏     | 325/780 [02:26<02:56,  2.57it/s] 42%|████▏     | 326/780 [02:27<02:42,  2.79it/s] 42%|████▏     | 327/780 [02:27<02:32,  2.96it/s] 42%|████▏     | 328/780 [02:27<02:25,  3.10it/s] 42%|████▏     | 329/780 [02:27<02:20,  3.20it/s] 42%|████▏     | 330/780 [02:28<02:17,  3.28it/s] 42%|████▏     | 331/780 [02:28<02:14,  3.33it/s] 43%|████▎     | 332/780 [02:28<02:12,  3.37it/s] 43%|████▎     | 333/780 [02:29<02:11,  3.40it/s] 43%|████▎     | 334/780 [02:29<02:10,  3.42it/s] 43%|████▎     | 335/780 [02:29<02:09,  3.44it/s] 43%|████▎     | 336/780 [02:29<02:09,  3.42it/s] 43%|████▎     | 337/780 [02:30<02:08,  3.44it/s] 43%|████▎     | 338/780 [02:30<02:08,  3.45it/s] 43%|████▎     | 339/780 [02:30<02:07,  3.45it/s] 44%|████▎     | 340/780 [02:31<02:07,  3.46it/s] 44%|████▎     | 341/780 [02:31<02:06,  3.46it/s] 44%|████▍     | 342/780 [02:31<02:06,  3.46it/s] 44%|████▍     | 343/780 [02:31<02:06,  3.47it/s] 44%|████▍     | 344/780 [02:32<02:05,  3.47it/s] 44%|████▍     | 345/780 [02:32<02:05,  3.47it/s] 44%|████▍     | 346/780 [02:32<02:05,  3.47it/s] 44%|████▍     | 347/780 [02:33<02:06,  3.42it/s] 45%|████▍     | 348/780 [02:33<02:05,  3.44it/s] 45%|████▍     | 349/780 [02:33<02:05,  3.45it/s] 45%|████▍     | 350/780 [02:33<02:04,  3.45it/s] 45%|████▌     | 351/780 [02:34<02:04,  3.46it/s] 45%|████▌     | 352/780 [02:34<02:03,  3.46it/s] 45%|████▌     | 353/780 [02:34<02:03,  3.46it/s] 45%|████▌     | 354/780 [02:35<02:02,  3.47it/s] 46%|████▌     | 355/780 [02:35<02:02,  3.47it/s] 46%|████▌     | 356/780 [02:35<02:02,  3.46it/s] 46%|████▌     | 357/780 [02:35<02:02,  3.47it/s] 46%|████▌     | 358/780 [02:36<02:07,  3.31it/s] 46%|████▌     | 359/780 [02:36<02:05,  3.35it/s] 46%|████▌     | 360/780 [02:36<02:03,  3.39it/s] 46%|████▋     | 361/780 [02:37<02:02,  3.41it/s] 46%|████▋     | 362/780 [02:37<02:01,  3.43it/s] 47%|████▋     | 363/780 [02:37<02:01,  3.44it/s] 47%|████▋     | 364/780 [02:38<02:00,  3.44it/s] 47%|████▋     | 365/780 [02:38<02:00,  3.45it/s] 47%|████▋     | 366/780 [02:38<01:59,  3.45it/s] 47%|████▋     | 367/780 [02:38<01:59,  3.46it/s] 47%|████▋     | 368/780 [02:39<01:59,  3.46it/s] 47%|████▋     | 369/780 [02:39<01:58,  3.46it/s] 47%|████▋     | 370/780 [02:39<01:58,  3.46it/s] 48%|████▊     | 371/780 [02:40<01:58,  3.46it/s] 48%|████▊     | 372/780 [02:40<01:57,  3.47it/s] 48%|████▊     | 373/780 [02:40<01:57,  3.47it/s] 48%|████▊     | 374/780 [02:40<01:57,  3.47it/s] 48%|████▊     | 375/780 [02:41<01:57,  3.45it/s] 48%|████▊     | 376/780 [02:41<01:56,  3.46it/s] 48%|████▊     | 377/780 [02:41<01:56,  3.46it/s] 48%|████▊     | 378/780 [02:42<01:56,  3.46it/s] 49%|████▊     | 379/780 [02:42<01:55,  3.46it/s] 49%|████▊     | 380/780 [02:42<01:55,  3.46it/s] 49%|████▉     | 381/780 [02:42<01:55,  3.46it/s] 49%|████▉     | 382/780 [02:43<01:54,  3.46it/s] 49%|████▉     | 383/780 [02:43<01:54,  3.47it/s] 49%|████▉     | 384/780 [02:43<01:54,  3.46it/s] 49%|████▉     | 385/780 [02:44<01:53,  3.47it/s] 49%|████▉     | 386/780 [02:44<01:53,  3.47it/s] 50%|████▉     | 387/780 [02:44<01:53,  3.47it/s] 50%|████▉     | 388/780 [02:44<01:53,  3.47it/s] 50%|████▉     | 389/780 [02:45<01:52,  3.46it/s] 50%|█████     | 390/780 [02:45<01:52,  3.47it/s] 50%|█████     | 391/780 [02:45<01:52,  3.47it/s] 50%|█████     | 392/780 [02:46<01:51,  3.47it/s] 50%|█████     | 393/780 [02:46<01:52,  3.43it/s] 51%|█████     | 394/780 [02:46<01:52,  3.44it/s] 51%|█████     | 395/780 [02:46<01:51,  3.45it/s] 51%|█████     | 396/780 [02:47<01:51,  3.45it/s] 51%|█████     | 397/780 [02:47<01:50,  3.46it/s] 51%|█████     | 398/780 [02:47<01:51,  3.42it/s] 51%|█████     | 399/780 [02:48<01:51,  3.43it/s] 51%|█████▏    | 400/780 [02:48<01:54,  3.31it/s] 51%|█████▏    | 401/780 [02:48<01:52,  3.36it/s] 52%|█████▏    | 402/780 [02:49<01:51,  3.39it/s] 52%|█████▏    | 403/780 [02:49<01:50,  3.41it/s] 52%|█████▏    | 404/780 [02:49<02:02,  3.07it/s] 52%|█████▏    | 405/780 [02:50<01:58,  3.16it/s] 52%|█████▏    | 406/780 [02:50<01:55,  3.25it/s] 52%|█████▏    | 407/780 [02:50<01:52,  3.31it/s] 52%|█████▏    | 408/780 [02:50<01:50,  3.36it/s] 52%|█████▏    | 409/780 [02:51<01:49,  3.39it/s] 53%|█████▎    | 410/780 [02:51<01:49,  3.39it/s] 53%|█████▎    | 411/780 [02:51<01:48,  3.41it/s] 53%|█████▎    | 412/780 [02:52<01:47,  3.43it/s] 53%|█████▎    | 413/780 [02:52<01:46,  3.44it/s] 53%|█████▎    | 414/780 [02:52<01:46,  3.45it/s] 53%|█████▎    | 415/780 [02:52<01:45,  3.45it/s] 53%|█████▎    | 416/780 [02:53<01:45,  3.45it/s] 53%|█████▎    | 417/780 [02:53<01:44,  3.46it/s] 54%|█████▎    | 418/780 [02:53<01:44,  3.46it/s] 54%|█████▎    | 419/780 [02:54<01:44,  3.46it/s] 54%|█████▍    | 420/780 [02:54<01:43,  3.46it/s] 54%|█████▍    | 421/780 [02:54<01:43,  3.46it/s] 54%|█████▍    | 422/780 [02:54<01:43,  3.46it/s] 54%|█████▍    | 423/780 [02:55<01:43,  3.47it/s] 54%|█████▍    | 424/780 [02:55<01:42,  3.47it/s] 54%|█████▍    | 425/780 [02:55<01:42,  3.46it/s] 55%|█████▍    | 426/780 [02:56<01:42,  3.46it/s] 55%|█████▍    | 427/780 [02:56<01:41,  3.46it/s] 55%|█████▍    | 428/780 [02:56<01:47,  3.29it/s] 55%|█████▌    | 429/780 [02:57<01:45,  3.34it/s] 55%|█████▌    | 430/780 [02:57<01:43,  3.38it/s] 55%|█████▌    | 431/780 [02:57<01:42,  3.40it/s] 55%|█████▌    | 432/780 [02:57<01:41,  3.42it/s] 56%|█████▌    | 433/780 [02:58<01:41,  3.43it/s] 56%|█████▌    | 434/780 [02:58<01:40,  3.44it/s] 56%|█████▌    | 435/780 [02:58<01:39,  3.45it/s] 56%|█████▌    | 436/780 [02:59<01:39,  3.45it/s] 56%|█████▌    | 437/780 [02:59<01:39,  3.46it/s] 56%|█████▌    | 438/780 [02:59<01:38,  3.46it/s] 56%|█████▋    | 439/780 [02:59<01:38,  3.46it/s] 56%|█████▋    | 440/780 [03:00<01:38,  3.46it/s] 57%|█████▋    | 441/780 [03:00<01:37,  3.46it/s] 57%|█████▋    | 442/780 [03:00<01:37,  3.47it/s] 57%|█████▋    | 443/780 [03:01<01:37,  3.46it/s] 57%|█████▋    | 444/780 [03:01<01:37,  3.46it/s] 57%|█████▋    | 445/780 [03:01<01:36,  3.46it/s] 57%|█████▋    | 446/780 [03:01<01:37,  3.42it/s] 57%|█████▋    | 447/780 [03:02<01:36,  3.43it/s] 57%|█████▋    | 448/780 [03:02<01:36,  3.44it/s] 58%|█████▊    | 449/780 [03:02<01:35,  3.45it/s] 58%|█████▊    | 450/780 [03:03<01:35,  3.45it/s] 58%|█████▊    | 451/780 [03:03<01:35,  3.45it/s] 58%|█████▊    | 452/780 [03:03<01:34,  3.46it/s] 58%|█████▊    | 453/780 [03:03<01:34,  3.46it/s] 58%|█████▊    | 454/780 [03:04<01:34,  3.46it/s] 58%|█████▊    | 455/780 [03:04<01:33,  3.46it/s] 58%|█████▊    | 456/780 [03:04<01:33,  3.46it/s] 59%|█████▊    | 457/780 [03:05<01:33,  3.46it/s] 59%|█████▊    | 458/780 [03:05<01:32,  3.46it/s] 59%|█████▉    | 459/780 [03:05<01:32,  3.46it/s] 59%|█████▉    | 460/780 [03:05<01:32,  3.46it/s] 59%|█████▉    | 461/780 [03:06<01:32,  3.46it/s] 59%|█████▉    | 462/780 [03:06<01:31,  3.46it/s] 59%|█████▉    | 463/780 [03:06<01:32,  3.43it/s] 59%|█████▉    | 464/780 [03:07<01:31,  3.44it/s] 60%|█████▉    | 465/780 [03:07<01:31,  3.45it/s] 60%|█████▉    | 466/780 [03:07<01:30,  3.45it/s] 60%|█████▉    | 467/780 [03:08<01:30,  3.45it/s] 60%|██████    | 468/780 [03:08<01:30,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 17:36:27,131 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:36:27,131 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 17:36:27,131 >>   Batch size = 8
{'eval_loss': 0.9726204872131348, 'eval_runtime': 9.3727, 'eval_samples_per_second': 373.105, 'eval_steps_per_second': 46.732, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.66it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.75it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.88it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.16it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.71it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.45it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.98it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.82it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.77it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.75it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.78it/s][A
 14%|█▍        | 63/438 [00:01<00:07, 46.89it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.83it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.95it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.78it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.77it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.58it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.64it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.64it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.68it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.78it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.72it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.87it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.87it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.70it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.74it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.62it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.64it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.61it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.72it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.77it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.75it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.78it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.81it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.77it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.58it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.62it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.72it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.68it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.68it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.70it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.76it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.83it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.82it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.73it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.67it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.71it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.67it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.68it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.43it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.99it/s][A
 60%|██████    | 263/438 [00:05<00:03, 45.62it/s][A
 61%|██████    | 268/438 [00:05<00:03, 45.98it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.28it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.49it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.50it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.30it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.51it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.53it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.65it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.62it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.65it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.74it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.77it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.71it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.79it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.64it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.66it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.64it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.68it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.66it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.73it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.82it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.70it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.69it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.66it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.75it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.75it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.69it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.64it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.58it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.68it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.62it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.72it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.68it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.73it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.73it/s][A 60%|██████    | 468/780 [03:17<01:30,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:36:36,627 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 17:36:36,675 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:36:40,902 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:36:40,949 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:36:41,014 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:33<39:45,  7.67s/it] 60%|██████    | 470/780 [03:33<28:12,  5.46s/it] 60%|██████    | 471/780 [03:33<20:07,  3.91s/it] 61%|██████    | 472/780 [03:34<14:29,  2.82s/it] 61%|██████    | 473/780 [03:34<10:33,  2.06s/it] 61%|██████    | 474/780 [03:34<07:48,  1.53s/it] 61%|██████    | 475/780 [03:34<05:52,  1.16s/it] 61%|██████    | 476/780 [03:35<04:32,  1.12it/s] 61%|██████    | 477/780 [03:35<03:36,  1.40it/s] 61%|██████▏   | 478/780 [03:35<02:56,  1.71it/s] 61%|██████▏   | 479/780 [03:36<02:29,  2.01it/s] 62%|██████▏   | 480/780 [03:36<02:10,  2.30it/s] 62%|██████▏   | 481/780 [03:36<01:56,  2.56it/s] 62%|██████▏   | 482/780 [03:36<01:47,  2.78it/s] 62%|██████▏   | 483/780 [03:37<01:40,  2.95it/s] 62%|██████▏   | 484/780 [03:37<01:35,  3.09it/s] 62%|██████▏   | 485/780 [03:37<01:32,  3.20it/s] 62%|██████▏   | 486/780 [03:38<01:29,  3.27it/s] 62%|██████▏   | 487/780 [03:38<01:27,  3.33it/s] 63%|██████▎   | 488/780 [03:38<01:26,  3.37it/s] 63%|██████▎   | 489/780 [03:38<01:25,  3.40it/s] 63%|██████▎   | 490/780 [03:39<01:24,  3.42it/s] 63%|██████▎   | 491/780 [03:39<01:24,  3.43it/s] 63%|██████▎   | 492/780 [03:39<01:25,  3.38it/s] 63%|██████▎   | 493/780 [03:40<01:24,  3.41it/s] 63%|██████▎   | 494/780 [03:40<01:23,  3.43it/s] 63%|██████▎   | 495/780 [03:40<01:22,  3.44it/s] 64%|██████▎   | 496/780 [03:41<01:22,  3.45it/s] 64%|██████▎   | 497/780 [03:41<01:21,  3.45it/s] 64%|██████▍   | 498/780 [03:41<01:21,  3.46it/s] 64%|██████▍   | 499/780 [03:41<01:21,  3.46it/s] 64%|██████▍   | 500/780 [03:42<01:20,  3.46it/s]                                                  64%|██████▍   | 500/780 [03:42<01:20,  3.46it/s] 64%|██████▍   | 501/780 [03:42<01:20,  3.47it/s] 64%|██████▍   | 502/780 [03:42<01:20,  3.47it/s] 64%|██████▍   | 503/780 [03:43<01:21,  3.42it/s] 65%|██████▍   | 504/780 [03:43<01:20,  3.43it/s] 65%|██████▍   | 505/780 [03:43<01:19,  3.45it/s] 65%|██████▍   | 506/780 [03:43<01:19,  3.45it/s] 65%|██████▌   | 507/780 [03:44<01:18,  3.46it/s] 65%|██████▌   | 508/780 [03:44<01:18,  3.46it/s] 65%|██████▌   | 509/780 [03:44<01:18,  3.46it/s] 65%|██████▌   | 510/780 [03:45<01:17,  3.46it/s] 66%|██████▌   | 511/780 [03:45<01:17,  3.46it/s] 66%|██████▌   | 512/780 [03:45<01:17,  3.47it/s] 66%|██████▌   | 513/780 [03:45<01:16,  3.47it/s] 66%|██████▌   | 514/780 [03:46<01:16,  3.47it/s] 66%|██████▌   | 515/780 [03:46<01:16,  3.47it/s] 66%|██████▌   | 516/780 [03:46<01:16,  3.47it/s] 66%|██████▋   | 517/780 [03:47<01:15,  3.47it/s] 66%|██████▋   | 518/780 [03:47<01:15,  3.47it/s] 67%|██████▋   | 519/780 [03:47<01:15,  3.47it/s] 67%|██████▋   | 520/780 [03:47<01:15,  3.45it/s] 67%|██████▋   | 521/780 [03:48<01:14,  3.46it/s] 67%|██████▋   | 522/780 [03:48<01:14,  3.46it/s] 67%|██████▋   | 523/780 [03:48<01:14,  3.46it/s] 67%|██████▋   | 524/780 [03:49<01:13,  3.47it/s] 67%|██████▋   | 525/780 [03:49<01:13,  3.47it/s] 67%|██████▋   | 526/780 [03:50<01:51,  2.28it/s] 68%|██████▊   | 527/780 [03:50<01:39,  2.53it/s] 68%|██████▊   | 528/780 [03:50<01:31,  2.75it/s] 68%|██████▊   | 529/780 [03:51<01:26,  2.90it/s] 68%|██████▊   | 530/780 [03:51<01:22,  3.05it/s] 68%|██████▊   | 531/780 [03:51<01:18,  3.16it/s] 68%|██████▊   | 532/780 [03:51<01:16,  3.24it/s] 68%|██████▊   | 533/780 [03:52<01:14,  3.31it/s] 68%|██████▊   | 534/780 [03:52<01:13,  3.35it/s] 69%|██████▊   | 535/780 [03:52<01:12,  3.39it/s] 69%|██████▊   | 536/780 [03:53<01:11,  3.41it/s] 69%|██████▉   | 537/780 [03:53<01:10,  3.43it/s] 69%|██████▉   | 538/780 [03:53<01:10,  3.44it/s] 69%|██████▉   | 539/780 [03:53<01:09,  3.44it/s] 69%|██████▉   | 540/780 [03:54<01:09,  3.44it/s] 69%|██████▉   | 541/780 [03:54<01:09,  3.45it/s] 69%|██████▉   | 542/780 [03:54<01:08,  3.45it/s] 70%|██████▉   | 543/780 [03:55<01:08,  3.45it/s] 70%|██████▉   | 544/780 [03:55<01:08,  3.46it/s] 70%|██████▉   | 545/780 [03:55<01:07,  3.46it/s] 70%|███████   | 546/780 [03:55<01:07,  3.46it/s] 70%|███████   | 547/780 [03:56<01:07,  3.46it/s] 70%|███████   | 548/780 [03:56<01:07,  3.46it/s] 70%|███████   | 549/780 [03:56<01:06,  3.46it/s] 71%|███████   | 550/780 [03:57<01:06,  3.46it/s] 71%|███████   | 551/780 [03:57<01:06,  3.45it/s] 71%|███████   | 552/780 [03:57<01:06,  3.45it/s] 71%|███████   | 553/780 [03:58<01:05,  3.45it/s] 71%|███████   | 554/780 [03:58<01:05,  3.46it/s] 71%|███████   | 555/780 [03:58<01:05,  3.45it/s] 71%|███████▏  | 556/780 [03:58<01:04,  3.46it/s] 71%|███████▏  | 557/780 [03:59<01:04,  3.46it/s] 72%|███████▏  | 558/780 [03:59<01:04,  3.46it/s] 72%|███████▏  | 559/780 [03:59<01:03,  3.46it/s] 72%|███████▏  | 560/780 [04:00<01:03,  3.46it/s] 72%|███████▏  | 561/780 [04:00<01:03,  3.46it/s] 72%|███████▏  | 562/780 [04:00<01:03,  3.45it/s] 72%|███████▏  | 563/780 [04:00<01:02,  3.45it/s] 72%|███████▏  | 564/780 [04:01<01:02,  3.45it/s] 72%|███████▏  | 565/780 [04:01<01:02,  3.46it/s] 73%|███████▎  | 566/780 [04:01<01:01,  3.45it/s] 73%|███████▎  | 567/780 [04:02<01:01,  3.46it/s] 73%|███████▎  | 568/780 [04:02<01:01,  3.46it/s] 73%|███████▎  | 569/780 [04:02<01:01,  3.46it/s] 73%|███████▎  | 570/780 [04:02<01:00,  3.46it/s] 73%|███████▎  | 571/780 [04:03<01:00,  3.46it/s] 73%|███████▎  | 572/780 [04:03<01:00,  3.45it/s] 73%|███████▎  | 573/780 [04:03<01:00,  3.44it/s] 74%|███████▎  | 574/780 [04:04<00:59,  3.45it/s] 74%|███████▎  | 575/780 [04:04<00:59,  3.46it/s] 74%|███████▍  | 576/780 [04:04<00:58,  3.46it/s] 74%|███████▍  | 577/780 [04:04<00:58,  3.47it/s] 74%|███████▍  | 578/780 [04:05<00:58,  3.47it/s] 74%|███████▍  | 579/780 [04:05<00:57,  3.47it/s] 74%|███████▍  | 580/780 [04:05<00:57,  3.47it/s] 74%|███████▍  | 581/780 [04:06<00:57,  3.47it/s] 75%|███████▍  | 582/780 [04:06<00:57,  3.47it/s] 75%|███████▍  | 583/780 [04:06<00:56,  3.47it/s] 75%|███████▍  | 584/780 [04:06<00:57,  3.42it/s] 75%|███████▌  | 585/780 [04:07<00:56,  3.44it/s] 75%|███████▌  | 586/780 [04:07<00:56,  3.45it/s] 75%|███████▌  | 587/780 [04:07<00:55,  3.45it/s] 75%|███████▌  | 588/780 [04:08<00:55,  3.46it/s] 76%|███████▌  | 589/780 [04:08<00:55,  3.46it/s] 76%|███████▌  | 590/780 [04:08<00:54,  3.46it/s] 76%|███████▌  | 591/780 [04:08<00:54,  3.47it/s] 76%|███████▌  | 592/780 [04:09<00:54,  3.47it/s] 76%|███████▌  | 593/780 [04:09<00:53,  3.47it/s] 76%|███████▌  | 594/780 [04:09<00:53,  3.47it/s] 76%|███████▋  | 595/780 [04:10<00:57,  3.20it/s] 76%|███████▋  | 596/780 [04:10<00:56,  3.28it/s] 77%|███████▋  | 597/780 [04:10<00:54,  3.33it/s] 77%|███████▋  | 598/780 [04:11<00:53,  3.37it/s] 77%|███████▋  | 599/780 [04:11<00:53,  3.40it/s] 77%|███████▋  | 600/780 [04:11<00:52,  3.42it/s] 77%|███████▋  | 601/780 [04:11<00:52,  3.43it/s] 77%|███████▋  | 602/780 [04:12<00:51,  3.44it/s] 77%|███████▋  | 603/780 [04:12<00:51,  3.45it/s] 77%|███████▋  | 604/780 [04:12<00:50,  3.46it/s] 78%|███████▊  | 605/780 [04:13<00:50,  3.46it/s] 78%|███████▊  | 606/780 [04:13<00:51,  3.40it/s] 78%|███████▊  | 607/780 [04:13<00:50,  3.42it/s] 78%|███████▊  | 608/780 [04:13<00:50,  3.43it/s] 78%|███████▊  | 609/780 [04:14<00:49,  3.44it/s] 78%|███████▊  | 610/780 [04:14<00:49,  3.45it/s] 78%|███████▊  | 611/780 [04:14<00:48,  3.46it/s] 78%|███████▊  | 612/780 [04:15<00:48,  3.46it/s] 79%|███████▊  | 613/780 [04:15<00:48,  3.46it/s] 79%|███████▊  | 614/780 [04:15<00:47,  3.46it/s] 79%|███████▉  | 615/780 [04:16<00:47,  3.46it/s] 79%|███████▉  | 616/780 [04:16<00:47,  3.46it/s] 79%|███████▉  | 617/780 [04:16<00:47,  3.46it/s] 79%|███████▉  | 618/780 [04:16<00:46,  3.46it/s] 79%|███████▉  | 619/780 [04:17<00:46,  3.47it/s] 79%|███████▉  | 620/780 [04:17<00:46,  3.47it/s] 80%|███████▉  | 621/780 [04:17<00:45,  3.47it/s] 80%|███████▉  | 622/780 [04:18<00:45,  3.47it/s] 80%|███████▉  | 623/780 [04:18<00:45,  3.47it/s] 80%|████████  | 624/780 [04:18<00:45,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 17:37:37,439 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:37:37,439 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 17:37:37,439 >>   Batch size = 8
{'eval_loss': 0.984402060508728, 'eval_runtime': 9.3862, 'eval_samples_per_second': 372.569, 'eval_steps_per_second': 46.664, 'epoch': 3.0}
{'loss': 0.7191, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.43it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.73it/s][A
  4%|▍         | 18/438 [00:00<00:08, 49.00it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.27it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.71it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.37it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.00it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.70it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.65it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.79it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.93it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.86it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.88it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.90it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.68it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.63it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.50it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.51it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.54it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.59it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.72it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.79it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.83it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.89it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.57it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.41it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.53it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.47it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.56it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.72it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.80it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.82it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.80it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.80it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.75it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.64it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.63it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.63it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.73it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.73it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.82it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.86it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.77it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.80it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.69it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.66it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.61it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.63it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.70it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.79it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.75it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.79it/s][A
 61%|██████    | 268/438 [00:05<00:03, 45.94it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.22it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.27it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.39it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.44it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.48it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.64it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.73it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.65it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.68it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.77it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.77it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.74it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.59it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.65it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.69it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.70it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.61it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.67it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.68it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.73it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.79it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.76it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.53it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.58it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.60it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.71it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.75it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.75it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.71it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.77it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.77it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.77it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.64it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.69it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.69it/s][A 80%|████████  | 624/780 [04:28<00:45,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:37:46,925 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 17:37:46,961 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:37:51,283 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:37:51,332 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:37:51,345 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:42<19:04,  7.38s/it] 80%|████████  | 626/780 [04:42<13:29,  5.26s/it] 80%|████████  | 627/780 [04:43<09:36,  3.77s/it] 81%|████████  | 628/780 [04:43<06:53,  2.72s/it] 81%|████████  | 629/780 [04:43<05:00,  1.99s/it] 81%|████████  | 630/780 [04:43<03:42,  1.48s/it] 81%|████████  | 631/780 [04:44<02:47,  1.12s/it] 81%|████████  | 632/780 [04:44<02:09,  1.15it/s] 81%|████████  | 633/780 [04:44<01:42,  1.43it/s] 81%|████████▏ | 634/780 [04:45<01:23,  1.74it/s] 81%|████████▏ | 635/780 [04:45<01:10,  2.05it/s] 82%|████████▏ | 636/780 [04:45<01:01,  2.34it/s] 82%|████████▏ | 637/780 [04:46<00:55,  2.57it/s] 82%|████████▏ | 638/780 [04:46<00:50,  2.79it/s] 82%|████████▏ | 639/780 [04:46<00:47,  2.96it/s] 82%|████████▏ | 640/780 [04:46<00:45,  3.10it/s] 82%|████████▏ | 641/780 [04:47<00:43,  3.20it/s] 82%|████████▏ | 642/780 [04:47<00:42,  3.28it/s] 82%|████████▏ | 643/780 [04:47<00:41,  3.33it/s] 83%|████████▎ | 644/780 [04:48<00:40,  3.36it/s] 83%|████████▎ | 645/780 [04:48<00:39,  3.39it/s] 83%|████████▎ | 646/780 [04:48<00:39,  3.41it/s] 83%|████████▎ | 647/780 [04:48<00:38,  3.43it/s] 83%|████████▎ | 648/780 [04:49<00:38,  3.43it/s] 83%|████████▎ | 649/780 [04:49<00:38,  3.44it/s] 83%|████████▎ | 650/780 [04:49<00:38,  3.35it/s] 83%|████████▎ | 651/780 [04:50<00:39,  3.28it/s] 84%|████████▎ | 652/780 [04:50<00:38,  3.31it/s] 84%|████████▎ | 653/780 [04:50<00:37,  3.36it/s] 84%|████████▍ | 654/780 [04:50<00:37,  3.39it/s] 84%|████████▍ | 655/780 [04:51<00:36,  3.41it/s] 84%|████████▍ | 656/780 [04:51<00:36,  3.43it/s] 84%|████████▍ | 657/780 [04:51<00:35,  3.44it/s] 84%|████████▍ | 658/780 [04:52<00:35,  3.45it/s] 84%|████████▍ | 659/780 [04:52<00:35,  3.43it/s] 85%|████████▍ | 660/780 [04:52<00:34,  3.44it/s] 85%|████████▍ | 661/780 [04:53<00:34,  3.45it/s] 85%|████████▍ | 662/780 [04:53<00:34,  3.46it/s] 85%|████████▌ | 663/780 [04:53<00:33,  3.46it/s] 85%|████████▌ | 664/780 [04:53<00:33,  3.46it/s] 85%|████████▌ | 665/780 [04:54<00:33,  3.46it/s] 85%|████████▌ | 666/780 [04:54<00:32,  3.46it/s] 86%|████████▌ | 667/780 [04:54<00:32,  3.47it/s] 86%|████████▌ | 668/780 [04:55<00:32,  3.47it/s] 86%|████████▌ | 669/780 [04:55<00:32,  3.47it/s] 86%|████████▌ | 670/780 [04:55<00:32,  3.39it/s] 86%|████████▌ | 671/780 [04:55<00:31,  3.42it/s] 86%|████████▌ | 672/780 [04:56<00:31,  3.43it/s] 86%|████████▋ | 673/780 [04:56<00:31,  3.44it/s] 86%|████████▋ | 674/780 [04:56<00:30,  3.45it/s] 87%|████████▋ | 675/780 [04:57<00:30,  3.46it/s] 87%|████████▋ | 676/780 [04:57<00:30,  3.46it/s] 87%|████████▋ | 677/780 [04:57<00:29,  3.46it/s] 87%|████████▋ | 678/780 [04:57<00:29,  3.46it/s] 87%|████████▋ | 679/780 [04:58<00:29,  3.47it/s] 87%|████████▋ | 680/780 [04:58<00:28,  3.47it/s] 87%|████████▋ | 681/780 [04:58<00:29,  3.40it/s] 87%|████████▋ | 682/780 [04:59<00:28,  3.42it/s] 88%|████████▊ | 683/780 [04:59<00:28,  3.44it/s] 88%|████████▊ | 684/780 [04:59<00:27,  3.44it/s] 88%|████████▊ | 685/780 [04:59<00:27,  3.45it/s] 88%|████████▊ | 686/780 [05:00<00:27,  3.46it/s] 88%|████████▊ | 687/780 [05:00<00:26,  3.46it/s] 88%|████████▊ | 688/780 [05:00<00:26,  3.46it/s] 88%|████████▊ | 689/780 [05:01<00:26,  3.46it/s] 88%|████████▊ | 690/780 [05:01<00:25,  3.47it/s] 89%|████████▊ | 691/780 [05:01<00:25,  3.47it/s] 89%|████████▊ | 692/780 [05:02<00:25,  3.42it/s] 89%|████████▉ | 693/780 [05:02<00:25,  3.44it/s] 89%|████████▉ | 694/780 [05:02<00:24,  3.44it/s] 89%|████████▉ | 695/780 [05:02<00:24,  3.45it/s] 89%|████████▉ | 696/780 [05:03<00:24,  3.46it/s] 89%|████████▉ | 697/780 [05:03<00:24,  3.46it/s] 89%|████████▉ | 698/780 [05:03<00:23,  3.46it/s] 90%|████████▉ | 699/780 [05:04<00:23,  3.46it/s] 90%|████████▉ | 700/780 [05:04<00:23,  3.46it/s] 90%|████████▉ | 701/780 [05:04<00:22,  3.46it/s] 90%|█████████ | 702/780 [05:04<00:22,  3.47it/s] 90%|█████████ | 703/780 [05:05<00:22,  3.45it/s] 90%|█████████ | 704/780 [05:05<00:22,  3.45it/s] 90%|█████████ | 705/780 [05:05<00:21,  3.45it/s] 91%|█████████ | 706/780 [05:06<00:21,  3.46it/s] 91%|█████████ | 707/780 [05:06<00:21,  3.46it/s] 91%|█████████ | 708/780 [05:06<00:20,  3.46it/s] 91%|█████████ | 709/780 [05:06<00:20,  3.47it/s] 91%|█████████ | 710/780 [05:07<00:20,  3.47it/s] 91%|█████████ | 711/780 [05:07<00:19,  3.47it/s] 91%|█████████▏| 712/780 [05:07<00:19,  3.47it/s] 91%|█████████▏| 713/780 [05:08<00:19,  3.47it/s] 92%|█████████▏| 714/780 [05:08<00:19,  3.42it/s] 92%|█████████▏| 715/780 [05:08<00:18,  3.43it/s] 92%|█████████▏| 716/780 [05:08<00:18,  3.44it/s] 92%|█████████▏| 717/780 [05:09<00:18,  3.45it/s] 92%|█████████▏| 718/780 [05:09<00:17,  3.45it/s] 92%|█████████▏| 719/780 [05:09<00:17,  3.46it/s] 92%|█████████▏| 720/780 [05:10<00:17,  3.46it/s] 92%|█████████▏| 721/780 [05:10<00:17,  3.46it/s] 93%|█████████▎| 722/780 [05:10<00:16,  3.46it/s] 93%|█████████▎| 723/780 [05:10<00:16,  3.46it/s] 93%|█████████▎| 724/780 [05:11<00:16,  3.47it/s] 93%|█████████▎| 725/780 [05:11<00:15,  3.46it/s] 93%|█████████▎| 726/780 [05:11<00:15,  3.46it/s] 93%|█████████▎| 727/780 [05:12<00:15,  3.46it/s] 93%|█████████▎| 728/780 [05:12<00:15,  3.46it/s] 93%|█████████▎| 729/780 [05:12<00:14,  3.47it/s] 94%|█████████▎| 730/780 [05:12<00:14,  3.47it/s] 94%|█████████▎| 731/780 [05:13<00:14,  3.47it/s] 94%|█████████▍| 732/780 [05:13<00:13,  3.47it/s] 94%|█████████▍| 733/780 [05:13<00:13,  3.47it/s] 94%|█████████▍| 734/780 [05:14<00:13,  3.47it/s] 94%|█████████▍| 735/780 [05:14<00:12,  3.47it/s] 94%|█████████▍| 736/780 [05:14<00:12,  3.46it/s] 94%|█████████▍| 737/780 [05:15<00:12,  3.46it/s] 95%|█████████▍| 738/780 [05:15<00:12,  3.46it/s] 95%|█████████▍| 739/780 [05:15<00:11,  3.46it/s] 95%|█████████▍| 740/780 [05:15<00:11,  3.46it/s] 95%|█████████▌| 741/780 [05:16<00:11,  3.46it/s] 95%|█████████▌| 742/780 [05:16<00:10,  3.46it/s] 95%|█████████▌| 743/780 [05:16<00:10,  3.46it/s] 95%|█████████▌| 744/780 [05:17<00:10,  3.46it/s] 96%|█████████▌| 745/780 [05:17<00:10,  3.46it/s] 96%|█████████▌| 746/780 [05:17<00:09,  3.47it/s] 96%|█████████▌| 747/780 [05:17<00:09,  3.47it/s] 96%|█████████▌| 748/780 [05:18<00:09,  3.47it/s] 96%|█████████▌| 749/780 [05:18<00:08,  3.47it/s] 96%|█████████▌| 750/780 [05:18<00:08,  3.47it/s] 96%|█████████▋| 751/780 [05:19<00:08,  3.47it/s] 96%|█████████▋| 752/780 [05:19<00:08,  3.47it/s] 97%|█████████▋| 753/780 [05:19<00:07,  3.47it/s] 97%|█████████▋| 754/780 [05:19<00:07,  3.38it/s] 97%|█████████▋| 755/780 [05:20<00:07,  3.41it/s] 97%|█████████▋| 756/780 [05:20<00:07,  3.42it/s] 97%|█████████▋| 757/780 [05:20<00:06,  3.44it/s] 97%|█████████▋| 758/780 [05:21<00:06,  3.44it/s] 97%|█████████▋| 759/780 [05:21<00:06,  3.45it/s] 97%|█████████▋| 760/780 [05:21<00:05,  3.45it/s] 98%|█████████▊| 761/780 [05:21<00:05,  3.46it/s] 98%|█████████▊| 762/780 [05:22<00:05,  3.46it/s] 98%|█████████▊| 763/780 [05:22<00:04,  3.46it/s] 98%|█████████▊| 764/780 [05:22<00:04,  3.46it/s] 98%|█████████▊| 765/780 [05:23<00:04,  3.44it/s] 98%|█████████▊| 766/780 [05:23<00:04,  3.45it/s] 98%|█████████▊| 767/780 [05:23<00:03,  3.45it/s] 98%|█████████▊| 768/780 [05:23<00:03,  3.46it/s] 99%|█████████▊| 769/780 [05:24<00:03,  3.46it/s] 99%|█████████▊| 770/780 [05:24<00:02,  3.46it/s] 99%|█████████▉| 771/780 [05:24<00:02,  3.46it/s] 99%|█████████▉| 772/780 [05:25<00:02,  3.46it/s] 99%|█████████▉| 773/780 [05:25<00:02,  3.46it/s] 99%|█████████▉| 774/780 [05:25<00:01,  3.46it/s] 99%|█████████▉| 775/780 [05:26<00:01,  3.46it/s] 99%|█████████▉| 776/780 [05:26<00:01,  3.39it/s]100%|█████████▉| 777/780 [05:26<00:00,  3.41it/s]100%|█████████▉| 778/780 [05:26<00:00,  3.43it/s]100%|█████████▉| 779/780 [05:27<00:00,  3.44it/s]100%|██████████| 780/780 [05:27<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 17:38:46,178 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:38:46,178 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 17:38:46,178 >>   Batch size = 8
{'eval_loss': 0.9924571514129639, 'eval_runtime': 9.3774, 'eval_samples_per_second': 372.919, 'eval_steps_per_second': 46.708, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.13it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.64it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.84it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.20it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.79it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.50it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.04it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.86it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.62it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.77it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.85it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.83it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.82it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.87it/s][A
 18%|█▊        | 78/438 [00:01<00:09, 38.33it/s][A
 19%|█▉        | 83/438 [00:01<00:08, 40.54it/s][A
 20%|██        | 88/438 [00:01<00:08, 42.22it/s][A
 21%|██        | 93/438 [00:02<00:07, 43.43it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 44.51it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.20it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 45.61it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 45.98it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.06it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.25it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.40it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.47it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 46.44it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.62it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.68it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.74it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.74it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.63it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.63it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.60it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.70it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.76it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.69it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.74it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.70it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.74it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.70it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.77it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.78it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.68it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.64it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.69it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.69it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.72it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.67it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.73it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.71it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.72it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.78it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.67it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 46.68it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.66it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.57it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.65it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.69it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.67it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.65it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.73it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.74it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.75it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.66it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.66it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.78it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.74it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.61it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.63it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.62it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 45.26it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 45.63it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.08it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.21it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.35it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.47it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.58it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.69it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.49it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.54it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.53it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.64it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.62it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.75it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.80it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.77it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.77it/s][A100%|██████████| 780/780 [05:36<00:00,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:38:55,663 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 17:38:55,692 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:39:00,518 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:39:00,544 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:39:00,555 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:39:11,058 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:39:11,069 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-156 (score: 0.9625846147537231).
                                                 100%|██████████| 780/780 [05:59<00:00,  3.45it/s]100%|██████████| 780/780 [05:59<00:00,  2.17it/s]
[INFO|trainer.py:1894] 2023-08-28 17:39:17,878 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 17:39:17,932 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:39:25,108 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:39:25,121 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:39:25,131 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:39:25,303 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:25,303 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:25,303 >>   train_loss               =      0.707
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:25,303 >>   train_runtime            = 0:05:59.11
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:25,303 >>   train_samples            =      10011
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:25,303 >>   train_samples_per_second =    139.385
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:25,303 >>   train_steps_per_second   =      2.172
{'eval_loss': 0.997166097164154, 'eval_runtime': 9.4572, 'eval_samples_per_second': 369.77, 'eval_steps_per_second': 46.314, 'epoch': 5.0}
{'train_runtime': 359.1143, 'train_samples_per_second': 139.385, 'train_steps_per_second': 2.172, 'train_loss': 0.7070313086876503, 'epoch': 5.0}
08/28/2023 17:39:25 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:39:25,415 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:39:25,415 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 17:39:25,415 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 58.20it/s]  3%|▎         | 12/438 [00:00<00:08, 51.41it/s]  4%|▍         | 18/438 [00:00<00:08, 49.58it/s]  5%|▌         | 23/438 [00:00<00:08, 48.73it/s]  6%|▋         | 28/438 [00:00<00:08, 48.35it/s]  8%|▊         | 33/438 [00:00<00:08, 47.95it/s]  9%|▊         | 38/438 [00:00<00:08, 47.83it/s] 10%|▉         | 43/438 [00:00<00:08, 47.66it/s] 11%|█         | 48/438 [00:00<00:08, 47.25it/s] 12%|█▏        | 53/438 [00:01<00:08, 47.29it/s] 13%|█▎        | 58/438 [00:01<00:08, 47.19it/s] 14%|█▍        | 63/438 [00:01<00:07, 47.23it/s] 16%|█▌        | 68/438 [00:01<00:07, 47.28it/s] 17%|█▋        | 73/438 [00:01<00:07, 47.32it/s] 18%|█▊        | 78/438 [00:01<00:07, 47.28it/s] 19%|█▉        | 83/438 [00:01<00:07, 47.36it/s] 20%|██        | 88/438 [00:01<00:07, 47.31it/s] 21%|██        | 93/438 [00:01<00:07, 47.22it/s] 22%|██▏       | 98/438 [00:02<00:07, 47.10it/s] 24%|██▎       | 103/438 [00:02<00:07, 47.14it/s] 25%|██▍       | 108/438 [00:02<00:06, 47.17it/s] 26%|██▌       | 113/438 [00:02<00:06, 47.07it/s] 27%|██▋       | 118/438 [00:02<00:06, 47.18it/s] 28%|██▊       | 123/438 [00:02<00:06, 47.16it/s] 29%|██▉       | 128/438 [00:02<00:06, 47.19it/s] 30%|███       | 133/438 [00:02<00:06, 46.90it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.96it/s] 33%|███▎      | 143/438 [00:03<00:06, 47.01it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.99it/s] 35%|███▍      | 153/438 [00:03<00:06, 47.07it/s] 36%|███▌      | 158/438 [00:03<00:05, 47.06it/s] 37%|███▋      | 163/438 [00:03<00:05, 47.13it/s] 38%|███▊      | 168/438 [00:03<00:05, 47.22it/s] 39%|███▉      | 173/438 [00:03<00:05, 47.11it/s] 41%|████      | 178/438 [00:03<00:05, 47.14it/s] 42%|████▏     | 183/438 [00:03<00:05, 47.14it/s] 43%|████▎     | 188/438 [00:03<00:05, 47.14it/s] 44%|████▍     | 193/438 [00:04<00:05, 47.08it/s] 45%|████▌     | 198/438 [00:04<00:05, 47.08it/s] 46%|████▋     | 203/438 [00:04<00:04, 47.06it/s] 47%|████▋     | 208/438 [00:04<00:04, 47.12it/s] 49%|████▊     | 213/438 [00:04<00:04, 47.25it/s] 50%|████▉     | 218/438 [00:04<00:04, 47.21it/s] 51%|█████     | 223/438 [00:04<00:04, 47.11it/s] 52%|█████▏    | 228/438 [00:04<00:04, 47.10it/s] 53%|█████▎    | 233/438 [00:04<00:04, 47.18it/s] 54%|█████▍    | 238/438 [00:05<00:04, 47.19it/s] 55%|█████▌    | 243/438 [00:05<00:04, 47.11it/s] 57%|█████▋    | 248/438 [00:05<00:04, 47.01it/s] 58%|█████▊    | 253/438 [00:05<00:03, 47.06it/s] 59%|█████▉    | 258/438 [00:05<00:03, 47.09it/s] 60%|██████    | 263/438 [00:05<00:03, 47.17it/s] 61%|██████    | 268/438 [00:05<00:03, 47.16it/s] 62%|██████▏   | 273/438 [00:05<00:03, 47.00it/s] 63%|██████▎   | 278/438 [00:05<00:03, 47.03it/s] 65%|██████▍   | 283/438 [00:05<00:03, 47.13it/s] 66%|██████▌   | 288/438 [00:06<00:03, 47.04it/s] 67%|██████▋   | 293/438 [00:06<00:03, 47.03it/s] 68%|██████▊   | 298/438 [00:06<00:02, 47.04it/s] 69%|██████▉   | 303/438 [00:06<00:02, 47.08it/s] 70%|███████   | 308/438 [00:06<00:02, 47.08it/s] 71%|███████▏  | 313/438 [00:06<00:02, 47.12it/s] 73%|███████▎  | 318/438 [00:06<00:02, 47.07it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.97it/s] 75%|███████▍  | 328/438 [00:06<00:02, 47.06it/s] 76%|███████▌  | 333/438 [00:07<00:02, 47.12it/s] 77%|███████▋  | 338/438 [00:07<00:02, 47.09it/s] 78%|███████▊  | 343/438 [00:07<00:02, 47.02it/s] 79%|███████▉  | 348/438 [00:07<00:01, 47.08it/s] 81%|████████  | 353/438 [00:07<00:01, 47.04it/s] 82%|████████▏ | 358/438 [00:07<00:01, 47.12it/s] 83%|████████▎ | 363/438 [00:07<00:01, 47.18it/s] 84%|████████▍ | 368/438 [00:07<00:01, 47.03it/s] 85%|████████▌ | 373/438 [00:07<00:01, 47.11it/s] 86%|████████▋ | 378/438 [00:07<00:01, 46.97it/s] 87%|████████▋ | 383/438 [00:08<00:01, 47.03it/s] 89%|████████▊ | 388/438 [00:08<00:01, 47.04it/s] 90%|████████▉ | 393/438 [00:08<00:00, 46.97it/s] 91%|█████████ | 398/438 [00:08<00:00, 47.02it/s] 92%|█████████▏| 403/438 [00:08<00:00, 47.07it/s] 93%|█████████▎| 408/438 [00:08<00:00, 47.09it/s] 94%|█████████▍| 413/438 [00:08<00:00, 47.07it/s] 95%|█████████▌| 418/438 [00:08<00:00, 47.02it/s] 97%|█████████▋| 423/438 [00:08<00:00, 45.19it/s] 98%|█████████▊| 428/438 [00:09<00:00, 45.73it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.16it/s]100%|██████████| 438/438 [00:09<00:00, 46.46it/s]100%|██████████| 438/438 [00:09<00:00, 47.17it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:39:34,722 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:34,722 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:34,722 >>   eval_loss               =     0.9626
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:34,723 >>   eval_runtime            = 0:00:09.30
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:34,723 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:34,723 >>   eval_samples_per_second =    375.731
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:34,723 >>   eval_steps_per_second   =      47.06
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:39:34,723 >>   perplexity              =     2.6185
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:43,407 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:43,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:43,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:43,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:43,422 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:39:44,392 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:39:44,393 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:39:45,040 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:39:46,078 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:39:46,078 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:49,266 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:49,272 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:49,272 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:49,272 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:39:49,272 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:39:49,648 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:39:49,650 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:39:49,927 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:39:50,096 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:39:50,096 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.70it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.55it/s]Extractor Predicting: 11it [00:06,  1.54it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:17,  1.64it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:18,  1.64it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:20,  1.60it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:24,  1.56it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:27,  1.55it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.55it/s]Extractor Predicting: 49it [00:31,  1.53it/s]Extractor Predicting: 50it [00:32,  1.40it/s]Extractor Predicting: 51it [00:32,  1.42it/s]Extractor Predicting: 52it [00:33,  1.43it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:34,  1.48it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:36,  1.51it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.49it/s]Extractor Predicting: 61it [00:39,  1.48it/s]Extractor Predicting: 62it [00:40,  1.46it/s]Extractor Predicting: 63it [00:40,  1.45it/s]Extractor Predicting: 64it [00:41,  1.46it/s]Extractor Predicting: 65it [00:42,  1.44it/s]Extractor Predicting: 66it [00:42,  1.43it/s]Extractor Predicting: 67it [00:43,  1.42it/s]Extractor Predicting: 68it [00:44,  1.42it/s]Extractor Predicting: 69it [00:45,  1.42it/s]Extractor Predicting: 70it [00:45,  1.42it/s]Extractor Predicting: 71it [00:46,  1.41it/s]Extractor Predicting: 72it [00:47,  1.41it/s]Extractor Predicting: 73it [00:47,  1.39it/s]Extractor Predicting: 74it [00:48,  1.40it/s]Extractor Predicting: 75it [00:49,  1.39it/s]Extractor Predicting: 76it [00:50,  1.41it/s]Extractor Predicting: 77it [00:50,  1.41it/s]Extractor Predicting: 78it [00:51,  1.40it/s]Extractor Predicting: 79it [00:52,  1.43it/s]Extractor Predicting: 80it [00:52,  1.40it/s]Extractor Predicting: 81it [00:53,  1.38it/s]Extractor Predicting: 82it [00:54,  1.39it/s]Extractor Predicting: 83it [00:54,  1.43it/s]Extractor Predicting: 84it [00:55,  1.43it/s]Extractor Predicting: 85it [00:56,  1.44it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:57,  1.40it/s]Extractor Predicting: 88it [00:58,  1.41it/s]Extractor Predicting: 89it [00:59,  1.44it/s]Extractor Predicting: 90it [00:59,  1.46it/s]Extractor Predicting: 91it [01:00,  1.48it/s]Extractor Predicting: 92it [01:01,  1.45it/s]Extractor Predicting: 93it [01:01,  1.48it/s]Extractor Predicting: 94it [01:02,  1.49it/s]Extractor Predicting: 95it [01:03,  1.48it/s]Extractor Predicting: 96it [01:03,  1.44it/s]Extractor Predicting: 97it [01:04,  1.45it/s]Extractor Predicting: 98it [01:05,  1.45it/s]Extractor Predicting: 99it [01:05,  1.47it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:07,  1.47it/s]Extractor Predicting: 102it [01:08,  1.45it/s]Extractor Predicting: 103it [01:08,  1.42it/s]Extractor Predicting: 104it [01:09,  1.44it/s]Extractor Predicting: 105it [01:10,  1.46it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:12,  1.45it/s]Extractor Predicting: 110it [01:13,  1.44it/s]Extractor Predicting: 111it [01:14,  1.44it/s]Extractor Predicting: 112it [01:14,  1.44it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:16,  1.45it/s]Extractor Predicting: 115it [01:17,  1.45it/s]Extractor Predicting: 116it [01:17,  1.45it/s]Extractor Predicting: 117it [01:18,  1.49it/s]Extractor Predicting: 118it [01:19,  1.51it/s]Extractor Predicting: 119it [01:19,  1.55it/s]Extractor Predicting: 120it [01:20,  1.54it/s]Extractor Predicting: 121it [01:20,  1.52it/s]Extractor Predicting: 122it [01:21,  1.58it/s]Extractor Predicting: 123it [01:22,  1.60it/s]Extractor Predicting: 124it [01:22,  1.58it/s]Extractor Predicting: 125it [01:23,  1.59it/s]Extractor Predicting: 126it [01:24,  1.48it/s]Extractor Predicting: 127it [01:24,  1.54it/s]Extractor Predicting: 128it [01:25,  1.56it/s]Extractor Predicting: 129it [01:25,  1.61it/s]Extractor Predicting: 130it [01:26,  1.55it/s]Extractor Predicting: 131it [01:27,  1.57it/s]Extractor Predicting: 132it [01:27,  1.64it/s]Extractor Predicting: 133it [01:28,  1.66it/s]Extractor Predicting: 134it [01:29,  1.59it/s]Extractor Predicting: 135it [01:29,  1.58it/s]Extractor Predicting: 136it [01:30,  1.63it/s]Extractor Predicting: 137it [01:30,  1.64it/s]Extractor Predicting: 138it [01:31,  1.64it/s]Extractor Predicting: 139it [01:32,  1.65it/s]Extractor Predicting: 140it [01:32,  1.68it/s]Extractor Predicting: 141it [01:33,  1.69it/s]Extractor Predicting: 142it [01:33,  1.72it/s]Extractor Predicting: 143it [01:34,  1.69it/s]Extractor Predicting: 144it [01:35,  1.66it/s]Extractor Predicting: 145it [01:35,  1.86it/s]Extractor Predicting: 145it [01:35,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:35,945 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:35,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:35,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:35,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:35,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:41:36,588 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:41:36,589 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:41:37,208 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:41:38,254 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:41:38,255 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:41,629 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:41,639 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:41,639 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:41,639 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:41:41,639 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:41:42,305 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:41:42,306 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:41:42,877 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:41:43,040 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:41:43,040 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6906474820143885,
  "recall": 0.2745210180154418,
  "score": 0.39287906691221613,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.50it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.48it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.54it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.49it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:16,  1.50it/s]Extractor Predicting: 25it [00:16,  1.41it/s]Extractor Predicting: 26it [00:17,  1.42it/s]Extractor Predicting: 27it [00:18,  1.42it/s]Extractor Predicting: 28it [00:18,  1.44it/s]Extractor Predicting: 29it [00:19,  1.43it/s]Extractor Predicting: 30it [00:20,  1.41it/s]Extractor Predicting: 31it [00:21,  1.43it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:22,  1.46it/s]Extractor Predicting: 34it [00:23,  1.47it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.55it/s]Extractor Predicting: 39it [00:26,  1.54it/s]Extractor Predicting: 40it [00:26,  1.55it/s]Extractor Predicting: 41it [00:27,  1.54it/s]Extractor Predicting: 42it [00:28,  1.53it/s]Extractor Predicting: 43it [00:28,  1.53it/s]Extractor Predicting: 44it [00:29,  1.53it/s]Extractor Predicting: 45it [00:30,  1.50it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:31,  1.47it/s]Extractor Predicting: 48it [00:32,  1.49it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:34,  1.50it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:35,  1.51it/s]Extractor Predicting: 54it [00:36,  1.52it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:37,  1.51it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:38,  1.56it/s]Extractor Predicting: 59it [00:39,  1.56it/s]Extractor Predicting: 60it [00:40,  1.56it/s]Extractor Predicting: 61it [00:40,  1.54it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.53it/s]Extractor Predicting: 64it [00:42,  1.53it/s]Extractor Predicting: 65it [00:43,  1.51it/s]Extractor Predicting: 66it [00:44,  1.49it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:45,  1.55it/s]Extractor Predicting: 69it [00:45,  1.56it/s]Extractor Predicting: 70it [00:46,  1.56it/s]Extractor Predicting: 71it [00:47,  1.54it/s]Extractor Predicting: 72it [00:47,  1.55it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:49,  1.52it/s]Extractor Predicting: 75it [00:49,  1.52it/s]Extractor Predicting: 76it [00:50,  1.55it/s]Extractor Predicting: 77it [00:51,  1.50it/s]Extractor Predicting: 78it [00:51,  1.50it/s]Extractor Predicting: 79it [00:52,  1.51it/s]Extractor Predicting: 80it [00:53,  1.50it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:55,  1.50it/s]Extractor Predicting: 85it [00:56,  1.52it/s]Extractor Predicting: 86it [00:57,  1.50it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:59,  1.50it/s]Extractor Predicting: 90it [00:59,  1.52it/s]Extractor Predicting: 91it [01:00,  1.52it/s]Extractor Predicting: 92it [01:01,  1.50it/s]Extractor Predicting: 93it [01:01,  1.48it/s]Extractor Predicting: 94it [01:02,  1.51it/s]Extractor Predicting: 95it [01:03,  1.50it/s]Extractor Predicting: 96it [01:03,  1.49it/s]Extractor Predicting: 97it [01:04,  1.51it/s]Extractor Predicting: 98it [01:05,  1.51it/s]Extractor Predicting: 99it [01:05,  1.52it/s]Extractor Predicting: 100it [01:06,  1.52it/s]Extractor Predicting: 101it [01:07,  1.55it/s]Extractor Predicting: 102it [01:07,  1.55it/s]Extractor Predicting: 103it [01:08,  1.53it/s]Extractor Predicting: 104it [01:09,  1.49it/s]Extractor Predicting: 105it [01:09,  1.48it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:11,  1.48it/s]Extractor Predicting: 108it [01:11,  1.50it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:13,  1.50it/s]Extractor Predicting: 111it [01:13,  1.50it/s]Extractor Predicting: 112it [01:14,  1.47it/s]Extractor Predicting: 113it [01:15,  1.47it/s]Extractor Predicting: 114it [01:15,  1.44it/s]Extractor Predicting: 115it [01:16,  1.43it/s]Extractor Predicting: 116it [01:17,  1.42it/s]Extractor Predicting: 117it [01:18,  1.44it/s]Extractor Predicting: 118it [01:18,  1.45it/s]Extractor Predicting: 119it [01:19,  1.46it/s]Extractor Predicting: 120it [01:20,  1.47it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.52it/s]Extractor Predicting: 123it [01:21,  1.53it/s]Extractor Predicting: 124it [01:22,  1.35it/s]Extractor Predicting: 125it [01:23,  1.39it/s]Extractor Predicting: 126it [01:24,  1.40it/s]Extractor Predicting: 127it [01:24,  1.42it/s]Extractor Predicting: 128it [01:25,  1.42it/s]Extractor Predicting: 129it [01:26,  1.46it/s]Extractor Predicting: 130it [01:26,  1.47it/s]Extractor Predicting: 131it [01:27,  1.49it/s]Extractor Predicting: 132it [01:28,  1.52it/s]Extractor Predicting: 133it [01:28,  1.48it/s]Extractor Predicting: 134it [01:29,  1.48it/s]Extractor Predicting: 135it [01:30,  1.48it/s]Extractor Predicting: 136it [01:31,  1.48it/s]Extractor Predicting: 137it [01:31,  1.47it/s]Extractor Predicting: 138it [01:32,  1.50it/s]Extractor Predicting: 139it [01:32,  1.50it/s]Extractor Predicting: 140it [01:33,  1.47it/s]Extractor Predicting: 141it [01:34,  1.49it/s]Extractor Predicting: 142it [01:35,  1.49it/s]Extractor Predicting: 143it [01:35,  1.50it/s]Extractor Predicting: 144it [01:36,  1.48it/s]Extractor Predicting: 145it [01:37,  1.50it/s]Extractor Predicting: 146it [01:37,  1.50it/s]Extractor Predicting: 147it [01:38,  1.54it/s]Extractor Predicting: 148it [01:38,  1.51it/s]Extractor Predicting: 149it [01:39,  1.52it/s]Extractor Predicting: 150it [01:40,  1.53it/s]Extractor Predicting: 151it [01:40,  1.56it/s]Extractor Predicting: 152it [01:41,  1.58it/s]Extractor Predicting: 153it [01:42,  1.57it/s]Extractor Predicting: 154it [01:42,  1.57it/s]Extractor Predicting: 155it [01:43,  1.56it/s]Extractor Predicting: 156it [01:44,  1.56it/s]Extractor Predicting: 157it [01:44,  1.53it/s]Extractor Predicting: 158it [01:45,  1.49it/s]Extractor Predicting: 159it [01:46,  1.50it/s]Extractor Predicting: 160it [01:46,  1.54it/s]Extractor Predicting: 161it [01:47,  1.54it/s]Extractor Predicting: 162it [01:48,  1.55it/s]Extractor Predicting: 163it [01:48,  1.56it/s]Extractor Predicting: 164it [01:49,  1.55it/s]Extractor Predicting: 165it [01:49,  1.54it/s]Extractor Predicting: 166it [01:50,  1.54it/s]Extractor Predicting: 167it [01:51,  1.54it/s]Extractor Predicting: 168it [01:51,  1.54it/s]Extractor Predicting: 169it [01:52,  1.56it/s]Extractor Predicting: 170it [01:53,  1.57it/s]Extractor Predicting: 171it [01:53,  1.55it/s]Extractor Predicting: 172it [01:54,  1.52it/s]Extractor Predicting: 173it [01:55,  1.53it/s]Extractor Predicting: 174it [01:55,  1.54it/s]Extractor Predicting: 175it [01:56,  1.57it/s]Extractor Predicting: 176it [01:57,  1.58it/s]Extractor Predicting: 177it [01:57,  1.52it/s]Extractor Predicting: 178it [01:58,  1.55it/s]Extractor Predicting: 179it [01:59,  1.53it/s]Extractor Predicting: 180it [01:59,  1.53it/s]Extractor Predicting: 181it [02:00,  1.55it/s]Extractor Predicting: 182it [02:01,  1.52it/s]Extractor Predicting: 183it [02:01,  1.54it/s]Extractor Predicting: 184it [02:02,  1.51it/s]Extractor Predicting: 185it [02:02,  1.54it/s]Extractor Predicting: 186it [02:03,  1.57it/s]Extractor Predicting: 187it [02:04,  1.62it/s]Extractor Predicting: 188it [02:04,  1.59it/s]Extractor Predicting: 189it [02:05,  1.54it/s]Extractor Predicting: 190it [02:06,  1.53it/s]Extractor Predicting: 191it [02:06,  1.51it/s]Extractor Predicting: 192it [02:07,  1.54it/s]Extractor Predicting: 193it [02:08,  1.54it/s]Extractor Predicting: 194it [02:08,  1.56it/s]Extractor Predicting: 195it [02:09,  1.58it/s]Extractor Predicting: 196it [02:09,  1.58it/s]Extractor Predicting: 197it [02:10,  1.53it/s]Extractor Predicting: 198it [02:11,  1.54it/s]Extractor Predicting: 199it [02:11,  1.52it/s]Extractor Predicting: 200it [02:12,  1.53it/s]Extractor Predicting: 201it [02:13,  1.53it/s]Extractor Predicting: 202it [02:13,  1.52it/s]Extractor Predicting: 203it [02:14,  1.52it/s]Extractor Predicting: 204it [02:15,  1.50it/s]Extractor Predicting: 205it [02:16,  1.47it/s]Extractor Predicting: 206it [02:16,  1.51it/s]Extractor Predicting: 207it [02:17,  1.49it/s]Extractor Predicting: 208it [02:17,  1.53it/s]Extractor Predicting: 209it [02:18,  1.55it/s]Extractor Predicting: 210it [02:19,  1.54it/s]Extractor Predicting: 211it [02:19,  1.52it/s]Extractor Predicting: 212it [02:20,  1.51it/s]Extractor Predicting: 213it [02:21,  1.50it/s]Extractor Predicting: 214it [02:21,  1.50it/s]Extractor Predicting: 215it [02:22,  1.49it/s]Extractor Predicting: 216it [02:23,  1.50it/s]Extractor Predicting: 217it [02:24,  1.44it/s]Extractor Predicting: 218it [02:24,  1.46it/s]Extractor Predicting: 219it [02:25,  1.47it/s]Extractor Predicting: 220it [02:25,  1.50it/s]Extractor Predicting: 221it [02:26,  1.54it/s]Extractor Predicting: 222it [02:27,  1.48it/s]Extractor Predicting: 223it [02:28,  1.46it/s]Extractor Predicting: 224it [02:28,  1.51it/s]Extractor Predicting: 225it [02:29,  1.52it/s]Extractor Predicting: 226it [02:29,  1.50it/s]Extractor Predicting: 227it [02:30,  1.48it/s]Extractor Predicting: 228it [02:31,  1.49it/s]Extractor Predicting: 229it [02:31,  1.53it/s]Extractor Predicting: 230it [02:32,  1.49it/s]Extractor Predicting: 231it [02:33,  1.50it/s]Extractor Predicting: 232it [02:33,  1.53it/s]Extractor Predicting: 233it [02:34,  1.50it/s]Extractor Predicting: 234it [02:35,  1.48it/s]Extractor Predicting: 235it [02:35,  1.50it/s]Extractor Predicting: 236it [02:36,  1.50it/s]Extractor Predicting: 237it [02:37,  1.47it/s]Extractor Predicting: 238it [02:38,  1.49it/s]Extractor Predicting: 239it [02:38,  1.49it/s]Extractor Predicting: 240it [02:39,  1.48it/s]Extractor Predicting: 241it [02:40,  1.50it/s]Extractor Predicting: 242it [02:40,  1.52it/s]Extractor Predicting: 243it [02:41,  1.48it/s]Extractor Predicting: 244it [02:42,  1.51it/s]Extractor Predicting: 245it [02:42,  1.49it/s]Extractor Predicting: 246it [02:43,  1.51it/s]Extractor Predicting: 247it [02:44,  1.35it/s]Extractor Predicting: 248it [02:44,  1.39it/s]Extractor Predicting: 249it [02:45,  1.43it/s]Extractor Predicting: 250it [02:46,  1.44it/s]Extractor Predicting: 251it [02:46,  1.47it/s]Extractor Predicting: 252it [02:47,  1.46it/s]Extractor Predicting: 253it [02:48,  1.47it/s]Extractor Predicting: 254it [02:48,  1.49it/s]Extractor Predicting: 255it [02:49,  1.50it/s]Extractor Predicting: 256it [02:50,  1.51it/s]Extractor Predicting: 257it [02:50,  1.54it/s]Extractor Predicting: 258it [02:51,  1.51it/s]Extractor Predicting: 259it [02:52,  1.55it/s]Extractor Predicting: 260it [02:52,  1.53it/s]Extractor Predicting: 261it [02:53,  1.54it/s]Extractor Predicting: 262it [02:54,  1.54it/s]Extractor Predicting: 263it [02:54,  1.54it/s]Extractor Predicting: 264it [02:55,  1.50it/s]Extractor Predicting: 265it [02:56,  1.50it/s]Extractor Predicting: 266it [02:56,  1.50it/s]Extractor Predicting: 267it [02:57,  1.52it/s]Extractor Predicting: 268it [02:58,  1.54it/s]Extractor Predicting: 269it [02:58,  1.54it/s]Extractor Predicting: 270it [02:59,  1.53it/s]Extractor Predicting: 271it [03:00,  1.54it/s]Extractor Predicting: 272it [03:00,  1.58it/s]Extractor Predicting: 273it [03:01,  1.58it/s]Extractor Predicting: 274it [03:01,  1.59it/s]Extractor Predicting: 275it [03:02,  1.56it/s]Extractor Predicting: 276it [03:03,  1.54it/s]Extractor Predicting: 277it [03:03,  1.56it/s]Extractor Predicting: 278it [03:04,  1.52it/s]Extractor Predicting: 279it [03:05,  1.53it/s]Extractor Predicting: 280it [03:05,  1.52it/s]Extractor Predicting: 281it [03:06,  1.53it/s]Extractor Predicting: 282it [03:07,  1.51it/s]Extractor Predicting: 283it [03:07,  1.51it/s]Extractor Predicting: 284it [03:08,  1.52it/s]Extractor Predicting: 285it [03:09,  1.52it/s]Extractor Predicting: 286it [03:09,  1.50it/s]Extractor Predicting: 287it [03:10,  1.51it/s]Extractor Predicting: 288it [03:11,  1.53it/s]Extractor Predicting: 289it [03:11,  1.53it/s]Extractor Predicting: 290it [03:12,  1.51it/s]Extractor Predicting: 291it [03:13,  1.51it/s]Extractor Predicting: 292it [03:13,  1.53it/s]Extractor Predicting: 293it [03:14,  1.53it/s]Extractor Predicting: 294it [03:15,  1.51it/s]Extractor Predicting: 295it [03:15,  1.54it/s]Extractor Predicting: 296it [03:16,  1.55it/s]Extractor Predicting: 297it [03:17,  1.51it/s]Extractor Predicting: 298it [03:17,  1.55it/s]Extractor Predicting: 299it [03:18,  1.54it/s]Extractor Predicting: 300it [03:18,  1.52it/s]Extractor Predicting: 301it [03:19,  1.57it/s]Extractor Predicting: 302it [03:20,  1.56it/s]Extractor Predicting: 303it [03:20,  1.59it/s]Extractor Predicting: 304it [03:21,  1.57it/s]Extractor Predicting: 305it [03:22,  1.55it/s]Extractor Predicting: 306it [03:22,  1.54it/s]Extractor Predicting: 307it [03:23,  1.52it/s]Extractor Predicting: 308it [03:24,  1.51it/s]Extractor Predicting: 309it [03:24,  1.52it/s]Extractor Predicting: 310it [03:25,  1.50it/s]Extractor Predicting: 311it [03:26,  1.49it/s]Extractor Predicting: 312it [03:26,  1.46it/s]Extractor Predicting: 313it [03:27,  1.47it/s]Extractor Predicting: 314it [03:28,  1.48it/s]Extractor Predicting: 315it [03:28,  1.51it/s]Extractor Predicting: 316it [03:29,  1.53it/s]Extractor Predicting: 317it [03:30,  1.51it/s]Extractor Predicting: 318it [03:30,  1.53it/s]Extractor Predicting: 319it [03:31,  1.52it/s]Extractor Predicting: 320it [03:32,  1.51it/s]Extractor Predicting: 321it [03:32,  1.51it/s]Extractor Predicting: 322it [03:33,  1.50it/s]Extractor Predicting: 323it [03:34,  1.55it/s]Extractor Predicting: 324it [03:34,  1.52it/s]Extractor Predicting: 325it [03:35,  1.53it/s]Extractor Predicting: 326it [03:36,  1.52it/s]Extractor Predicting: 327it [03:36,  1.55it/s]Extractor Predicting: 328it [03:37,  1.56it/s]Extractor Predicting: 329it [03:37,  1.53it/s]Extractor Predicting: 330it [03:38,  1.50it/s]Extractor Predicting: 331it [03:39,  1.50it/s]Extractor Predicting: 332it [03:40,  1.49it/s]Extractor Predicting: 333it [03:40,  1.50it/s]Extractor Predicting: 334it [03:41,  1.50it/s]Extractor Predicting: 335it [03:42,  1.50it/s]Extractor Predicting: 336it [03:42,  1.53it/s]Extractor Predicting: 337it [03:43,  1.55it/s]Extractor Predicting: 338it [03:43,  1.54it/s]Extractor Predicting: 339it [03:44,  1.52it/s]Extractor Predicting: 340it [03:45,  1.53it/s]Extractor Predicting: 341it [03:45,  1.55it/s]Extractor Predicting: 342it [03:46,  1.51it/s]Extractor Predicting: 343it [03:47,  1.53it/s]Extractor Predicting: 344it [03:47,  1.53it/s]Extractor Predicting: 345it [03:48,  1.55it/s]Extractor Predicting: 346it [03:49,  1.50it/s]Extractor Predicting: 347it [03:49,  1.51it/s]Extractor Predicting: 348it [03:50,  1.52it/s]Extractor Predicting: 349it [03:51,  1.50it/s]Extractor Predicting: 350it [03:51,  1.51it/s]Extractor Predicting: 351it [03:52,  1.47it/s]Extractor Predicting: 352it [03:53,  1.46it/s]Extractor Predicting: 353it [03:53,  1.47it/s]Extractor Predicting: 354it [03:54,  1.48it/s]Extractor Predicting: 355it [03:55,  1.50it/s]Extractor Predicting: 356it [03:55,  1.51it/s]Extractor Predicting: 357it [03:56,  1.54it/s]Extractor Predicting: 358it [03:57,  1.55it/s]Extractor Predicting: 359it [03:57,  1.54it/s]Extractor Predicting: 360it [03:58,  1.52it/s]Extractor Predicting: 361it [03:59,  1.53it/s]Extractor Predicting: 362it [03:59,  1.53it/s]Extractor Predicting: 363it [04:00,  1.55it/s]Extractor Predicting: 364it [04:01,  1.55it/s]Extractor Predicting: 365it [04:01,  1.55it/s]Extractor Predicting: 366it [04:02,  1.54it/s]Extractor Predicting: 367it [04:03,  1.53it/s]Extractor Predicting: 368it [04:03,  1.37it/s]Extractor Predicting: 369it [04:04,  1.42it/s]Extractor Predicting: 370it [04:05,  1.48it/s]Extractor Predicting: 371it [04:05,  1.49it/s]Extractor Predicting: 372it [04:06,  1.51it/s]Extractor Predicting: 373it [04:07,  1.52it/s]Extractor Predicting: 374it [04:07,  1.51it/s]Extractor Predicting: 375it [04:08,  1.53it/s]Extractor Predicting: 376it [04:09,  1.52it/s]Extractor Predicting: 377it [04:09,  1.53it/s]Extractor Predicting: 378it [04:10,  1.56it/s]Extractor Predicting: 379it [04:11,  1.51it/s]Extractor Predicting: 380it [04:11,  1.52it/s]Extractor Predicting: 381it [04:12,  1.56it/s]Extractor Predicting: 382it [04:12,  1.55it/s]Extractor Predicting: 383it [04:13,  1.50it/s]Extractor Predicting: 384it [04:14,  1.51it/s]Extractor Predicting: 385it [04:15,  1.52it/s]Extractor Predicting: 386it [04:15,  1.56it/s]Extractor Predicting: 387it [04:16,  1.58it/s]Extractor Predicting: 388it [04:16,  1.57it/s]Extractor Predicting: 389it [04:17,  1.54it/s]Extractor Predicting: 390it [04:18,  1.56it/s]Extractor Predicting: 391it [04:18,  1.56it/s]Extractor Predicting: 392it [04:19,  1.57it/s]Extractor Predicting: 393it [04:20,  1.57it/s]Extractor Predicting: 394it [04:20,  1.60it/s]Extractor Predicting: 395it [04:21,  1.56it/s]Extractor Predicting: 396it [04:21,  1.56it/s]Extractor Predicting: 397it [04:22,  1.57it/s]Extractor Predicting: 398it [04:23,  1.57it/s]Extractor Predicting: 399it [04:23,  1.59it/s]Extractor Predicting: 400it [04:24,  1.63it/s]Extractor Predicting: 401it [04:25,  1.60it/s]Extractor Predicting: 402it [04:25,  1.60it/s]Extractor Predicting: 403it [04:26,  1.58it/s]Extractor Predicting: 404it [04:27,  1.57it/s]Extractor Predicting: 405it [04:27,  1.57it/s]Extractor Predicting: 406it [04:28,  1.56it/s]Extractor Predicting: 407it [04:28,  1.55it/s]Extractor Predicting: 408it [04:29,  1.56it/s]Extractor Predicting: 409it [04:30,  1.57it/s]Extractor Predicting: 409it [04:30,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:23,175 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:23,181 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:23,182 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:23,182 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:23,182 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:46:23,829 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:46:23,830 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:46:24,384 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:46:25,468 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:46:25,468 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:28,718 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:28,720 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:28,720 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:28,720 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:46:28,720 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:46:29,419 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:46:29,430 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:46:30,034 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:46:30,207 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:46:30,207 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.45052786643751536,
  "recall": 0.18699684092530316,
  "score": 0.26429497335445773,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.39it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.51it/s]Extractor Predicting: 10it [00:06,  1.49it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.46it/s]
[INFO|configuration_utils.py:515] 2023-08-28 17:46:39,709 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:46:39,711 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:46:39,723 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:46:39,724 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 17:46:39,735 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:46:45,115 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 17:46:45,122 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 17:46:45,154 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:46:45,154 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:46:45,162 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:46:45,166 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:46:45,166 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:46:45,166 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:46:45,166 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:46:45,166 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:46:45,166 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.22666666666666666,
  "recall": 0.024745269286754003,
  "score": 0.04461942257217848,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 17:46:45,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:46,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:46,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:47,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:48,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:49,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:50,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:50,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:51,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:52,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:52,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:53,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:54,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:54,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:55,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:56,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:57,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:57,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:58,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:59,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:46:59,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:49, 15.22s/it][WARNING|generation_utils.py:914] 2023-08-28 17:47:00,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:01,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:02,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:02,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:03,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:04,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:04,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:05,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:06,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:07,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:07,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:08,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:09,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:10,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:10,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:11,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:12,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:13,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:14,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:14,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:15,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:40, 15.59s/it][WARNING|generation_utils.py:914] 2023-08-28 17:47:16,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:17,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:18,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:18,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:19,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:20,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:21,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:22,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:22,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:24,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:24,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:25,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:26,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:27,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:27,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:28,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:29,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:30,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:30,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:31,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:32,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:47<04:32, 16.04s/it][WARNING|generation_utils.py:914] 2023-08-28 17:47:33,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:33,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:34,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:35,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:35,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:36,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:37,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:37,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:38,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:39,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:39,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:40,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:41,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:42,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:42,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:43,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:44,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:45,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:46,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:46,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:47,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:48,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:03<04:15, 15.97s/it][WARNING|generation_utils.py:914] 2023-08-28 17:47:48,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:49,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:50,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:50,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:51,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:52,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:53,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:53,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:54,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:55,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:56,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:56,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:57,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:58,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:59,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:47:59,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:00,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:01,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:01,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:02,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:03,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:03,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:04,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:05,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:06,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:07,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:22<04:15, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 17:48:07,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:08,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:09,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:09,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:10,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:11,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:12,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:12,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:13,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:14,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:14,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:15,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:16,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:17,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:17,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:18,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:18,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:19,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:20,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:21,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:21,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:22,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:23,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:38<03:54, 16.78s/it][WARNING|generation_utils.py:914] 2023-08-28 17:48:24,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:24,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:25,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:26,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:26,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:27,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:28,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:29,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:30,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:30,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:31,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:32,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:33,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:33,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:34,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:35,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:35,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:36,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:37,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:38,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:39,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:40,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:40,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:41,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:57<03:45, 17.33s/it][WARNING|generation_utils.py:914] 2023-08-28 17:48:42,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:43,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:44,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:45,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:45,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:46,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:48,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:48,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:49,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:50,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:51,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:52,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:52,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:53,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:54,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:55,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:56,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:56,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:57,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:58,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:59,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:48:59,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:00,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:01,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:02,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:17<03:39, 18.32s/it][WARNING|generation_utils.py:914] 2023-08-28 17:49:03,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:04,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:04,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:05,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:06,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:06,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:07,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:08,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:09,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:10,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:10,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:11,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:12,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:13,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:13,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:14,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:15,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:16,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:16,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:17,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:18,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:19,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:20,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:35<03:19, 18.12s/it][WARNING|generation_utils.py:914] 2023-08-28 17:49:20,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:21,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:22,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:23,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:23,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:24,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:25,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:25,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:26,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:28,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:28,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:29,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:30,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:31,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:31,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:32,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:33,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:33,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:34,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:35,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:36,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:51<02:56, 17.60s/it][WARNING|generation_utils.py:914] 2023-08-28 17:49:37,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:37,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:38,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:39,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:40,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:41,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:41,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:42,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:43,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:43,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:44,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:45,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:46,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:47,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:47,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:48,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:49,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:50,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:50,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:51,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:52,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:52,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:08<02:35, 17.31s/it][WARNING|generation_utils.py:914] 2023-08-28 17:49:53,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:54,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:55,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:56,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:56,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:57,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:58,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:58,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:49:59,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:00,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:00,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:01,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:02,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:03,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:03,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:04,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:05,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:06,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:06,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:07,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:08,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:09,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:09,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:25<02:17, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-28 17:50:10,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:11,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:11,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:12,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:13,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:14,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:14,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:15,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:16,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:16,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:17,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:18,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:18,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:19,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:20,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:20,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:21,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:22,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:22,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:23,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:23,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:39<01:53, 16.18s/it][WARNING|generation_utils.py:914] 2023-08-28 17:50:24,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:25,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:25,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:26,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:27,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:28,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:28,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:29,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:30,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:30,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:31,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:32,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:32,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:33,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:34,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:34,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:35,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:36,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:37,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:37,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:38,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:53<01:34, 15.72s/it][WARNING|generation_utils.py:914] 2023-08-28 17:50:39,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:39,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:40,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:41,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:42,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:42,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:43,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:44,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:45,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:45,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:46,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:47,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:48,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:48,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:49,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:50,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:50,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:51,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:52,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:53,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:53,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:54,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:09<01:19, 15.88s/it][WARNING|generation_utils.py:914] 2023-08-28 17:50:55,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:56,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:56,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:57,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:58,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:59,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:50:59,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:00,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:01,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:01,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:02,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:03,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:04,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:04,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:05,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:06,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:06,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:07,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:08,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:09,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:10,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:10,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:11,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:26<01:04, 16.17s/it][WARNING|generation_utils.py:914] 2023-08-28 17:51:12,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:12,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:13,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:14,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:14,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:15,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:16,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:16,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:17,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:18,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:18,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:19,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:20,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:20,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:21,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:22,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:22,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:23,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:24,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:24,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:25,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:26,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:26,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:42<00:47, 15.87s/it][WARNING|generation_utils.py:914] 2023-08-28 17:51:27,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:28,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:28,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:29,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:30,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:30,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:31,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:32,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:32,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:33,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:34,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:34,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:35,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:36,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:37,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:37,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:38,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:39,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:40,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:40,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:41,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:56<00:31, 15.56s/it][WARNING|generation_utils.py:914] 2023-08-28 17:51:42,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:43,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:43,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:44,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:45,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:46,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:46,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:47,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:48,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:49,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:50,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:50,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:51,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:52,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:53,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:54,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:55,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:56,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:56,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:57,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:58,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:51:59,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:00,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:15<00:16, 16.45s/it][WARNING|generation_utils.py:914] 2023-08-28 17:52:00,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:01,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:02,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:02,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:03,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:04,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:05,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:05,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:06,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:07,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:08,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:09,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:09,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:10,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:11,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:12,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:13,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:13,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:14,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:15,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:52:16,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:31<00:00, 16.39s/it]Generating: 100%|██████████| 20/20 [05:31<00:00, 16.58s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:24,459 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:24,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:24,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:24,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:24,471 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:52:25,130 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:52:25,131 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:52:25,698 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:52:26,785 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:52:26,785 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:29,764 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:29,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:29,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:29,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:52:29,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:52:30,539 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:52:30,540 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:52:31,234 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:52:31,399 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:52:31,399 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9330357142857143, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 531, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9300595238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 549, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.8778409090909091, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 189, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 238, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 283, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 356, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 440, 'raw': 608}
{'target': 600, 'success': 466, 'raw': 640}
{'target': 600, 'success': 490, 'raw': 672}
{'target': 600, 'success': 513, 'raw': 704}
{'target': 600, 'success': 539, 'raw': 736}
{'target': 600, 'success': 564, 'raw': 768}
{'target': 600, 'success': 585, 'raw': 800}
{'target': 600, 'success': 608, 'raw': 832}
{'prompt': 'Relation : voice type .', 'success_rate': 0.7307692307692307, 'errors': {'', '(\'The Beatles\', \'voice type\', \'\', \'He is the son of actor Jon Voight , the lead singer of the American folk rock band " The Beatles " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 526, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 607, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8247282608695652, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8138020833333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 213, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 289, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 339, 'raw': 448}
{'target': 600, 'success': 364, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 445, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 517, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 590, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.7675, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 407, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.84375, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9002976190476191, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8274456521739131, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 364, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8806818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 575, 'raw': 704}
{'target': 600, 'success': 603, 'raw': 736}
{'prompt': 'Relation : position held .', 'success_rate': 0.8192934782608695, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 451, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8179347826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–1230 ) he married the fourth of his six sons , John , who married Queen Anne ; the fourth died under mysterious circumstances ; he died in 1234 . Head Entity : John , Tail Entity : Catholic .\n']
['Relation : religion . Context : Later in the year ( 1143–1230 ) he married the fourth of his six sons , John , who married Queen Anne ; the fourth died under mysterious circumstances ; he died in 1234 . Head Entity : John , Tail Entity : Catholic .\n', 'Relation : religion . Context : Eusebius was the second of his two sons , Eusebius V , and his first wife , Maria Aurelia . Head Entity : Eusebi , Tail Entity : Christian .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 285, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : religion .', 'success_rate': 0.8260869565217391, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 576, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 14423
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14523, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.39it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.54it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:03,  1.53it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.57it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.61it/s]Extractor Estimating: 11it [00:06,  1.63it/s]Extractor Estimating: 12it [00:07,  1.60it/s]Extractor Estimating: 13it [00:08,  1.63it/s]Extractor Estimating: 14it [00:08,  1.59it/s]Extractor Estimating: 15it [00:09,  1.60it/s]Extractor Estimating: 16it [00:10,  1.65it/s]Extractor Estimating: 17it [00:10,  1.52it/s]Extractor Estimating: 18it [00:11,  1.57it/s]Extractor Estimating: 19it [00:12,  1.60it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:13,  1.63it/s]Extractor Estimating: 22it [00:13,  1.60it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:15,  1.64it/s]Extractor Estimating: 25it [00:15,  1.70it/s]Extractor Estimating: 26it [00:16,  1.63it/s]Extractor Estimating: 27it [00:16,  1.60it/s]Extractor Estimating: 28it [00:17,  1.58it/s]Extractor Estimating: 29it [00:18,  1.59it/s]Extractor Estimating: 30it [00:18,  1.55it/s]Extractor Estimating: 31it [00:19,  1.53it/s]Extractor Estimating: 32it [00:20,  1.50it/s]Extractor Estimating: 33it [00:20,  1.56it/s]Extractor Estimating: 34it [00:21,  1.56it/s]Extractor Estimating: 35it [00:22,  1.56it/s]Extractor Estimating: 36it [00:22,  1.56it/s]Extractor Estimating: 37it [00:23,  1.55it/s]Extractor Estimating: 38it [00:24,  1.54it/s]Extractor Estimating: 39it [00:24,  1.52it/s]Extractor Estimating: 40it [00:25,  1.53it/s]Extractor Estimating: 41it [00:26,  1.57it/s]Extractor Estimating: 42it [00:26,  1.54it/s]Extractor Estimating: 43it [00:27,  1.56it/s]Extractor Estimating: 44it [00:27,  1.54it/s]Extractor Estimating: 45it [00:28,  1.49it/s]Extractor Estimating: 46it [00:29,  1.51it/s]Extractor Estimating: 47it [00:29,  1.54it/s]Extractor Estimating: 48it [00:30,  1.53it/s]Extractor Estimating: 49it [00:31,  1.52it/s]Extractor Estimating: 50it [00:31,  1.57it/s]Extractor Estimating: 51it [00:32,  1.56it/s]Extractor Estimating: 52it [00:33,  1.56it/s]Extractor Estimating: 53it [00:33,  1.63it/s]Extractor Estimating: 54it [00:34,  1.60it/s]Extractor Estimating: 55it [00:35,  1.54it/s]Extractor Estimating: 56it [00:35,  1.58it/s]Extractor Estimating: 57it [00:36,  1.61it/s]Extractor Estimating: 58it [00:36,  1.61it/s]Extractor Estimating: 59it [00:37,  1.60it/s]Extractor Estimating: 60it [00:38,  1.40it/s]Extractor Estimating: 61it [00:39,  1.46it/s]Extractor Estimating: 62it [00:39,  1.46it/s]Extractor Estimating: 63it [00:40,  1.50it/s]Extractor Estimating: 64it [00:40,  1.55it/s]Extractor Estimating: 65it [00:41,  1.41it/s]Extractor Estimating: 66it [00:42,  1.44it/s]Extractor Estimating: 67it [00:43,  1.47it/s]Extractor Estimating: 68it [00:43,  1.53it/s]Extractor Estimating: 69it [00:44,  1.55it/s]Extractor Estimating: 70it [00:45,  1.39it/s]Extractor Estimating: 71it [00:45,  1.50it/s]Extractor Estimating: 72it [00:46,  1.45it/s]Extractor Estimating: 73it [00:47,  1.52it/s]Extractor Estimating: 74it [00:47,  1.57it/s]Extractor Estimating: 75it [00:48,  1.56it/s]Extractor Estimating: 76it [00:49,  1.54it/s]Extractor Estimating: 77it [00:49,  1.50it/s]Extractor Estimating: 78it [00:50,  1.52it/s]Extractor Estimating: 79it [00:50,  1.55it/s]Extractor Estimating: 80it [00:51,  1.49it/s]Extractor Estimating: 81it [00:52,  1.54it/s]Extractor Estimating: 82it [00:52,  1.59it/s]Extractor Estimating: 83it [00:53,  1.61it/s]Extractor Estimating: 84it [00:54,  1.59it/s]Extractor Estimating: 85it [00:54,  1.52it/s]Extractor Estimating: 86it [00:55,  1.47it/s]Extractor Estimating: 87it [00:56,  1.51it/s]Extractor Estimating: 88it [00:56,  1.50it/s]Extractor Estimating: 89it [00:57,  1.49it/s]Extractor Estimating: 90it [00:58,  1.49it/s]Extractor Estimating: 91it [00:58,  1.53it/s]Extractor Estimating: 92it [00:59,  1.53it/s]Extractor Estimating: 93it [01:00,  1.50it/s]Extractor Estimating: 94it [01:00,  1.49it/s]Extractor Estimating: 95it [01:01,  1.48it/s]Extractor Estimating: 96it [01:02,  1.51it/s]Extractor Estimating: 97it [01:02,  1.50it/s]Extractor Estimating: 98it [01:03,  1.54it/s]Extractor Estimating: 99it [01:04,  1.54it/s]Extractor Estimating: 100it [01:04,  1.61it/s]Extractor Estimating: 101it [01:05,  1.62it/s]Extractor Estimating: 102it [01:05,  1.62it/s]Extractor Estimating: 103it [01:06,  1.64it/s]Extractor Estimating: 104it [01:07,  1.62it/s]Extractor Estimating: 105it [01:07,  1.60it/s]Extractor Estimating: 106it [01:08,  1.54it/s]Extractor Estimating: 107it [01:09,  1.60it/s]Extractor Estimating: 108it [01:09,  1.64it/s]Extractor Estimating: 109it [01:10,  1.61it/s]Extractor Estimating: 110it [01:10,  1.55it/s]Extractor Estimating: 111it [01:11,  1.56it/s]Extractor Estimating: 112it [01:12,  1.56it/s]Extractor Estimating: 113it [01:12,  1.59it/s]Extractor Estimating: 114it [01:13,  1.61it/s]Extractor Estimating: 115it [01:14,  1.64it/s]Extractor Estimating: 116it [01:14,  1.64it/s]Extractor Estimating: 117it [01:15,  1.65it/s]Extractor Estimating: 118it [01:15,  1.71it/s]Extractor Estimating: 119it [01:16,  1.72it/s]Extractor Estimating: 120it [01:17,  1.66it/s]Extractor Estimating: 121it [01:17,  1.62it/s]Extractor Estimating: 122it [01:18,  1.53it/s]Extractor Estimating: 123it [01:19,  1.52it/s]Extractor Estimating: 124it [01:19,  1.55it/s]Extractor Estimating: 125it [01:20,  1.55it/s]Extractor Estimating: 126it [01:20,  1.60it/s]Extractor Estimating: 127it [01:21,  1.58it/s]Extractor Estimating: 128it [01:22,  1.63it/s]Extractor Estimating: 129it [01:22,  1.63it/s]Extractor Estimating: 130it [01:23,  1.66it/s]Extractor Estimating: 131it [01:23,  1.66it/s]Extractor Estimating: 132it [01:24,  1.65it/s]Extractor Estimating: 133it [01:25,  1.67it/s]Extractor Estimating: 134it [01:25,  1.71it/s]Extractor Estimating: 135it [01:26,  1.70it/s]Extractor Estimating: 136it [01:26,  1.69it/s]Extractor Estimating: 137it [01:27,  1.71it/s]Extractor Estimating: 138it [01:28,  1.66it/s]Extractor Estimating: 139it [01:28,  1.66it/s]Extractor Estimating: 140it [01:29,  1.54it/s]Extractor Estimating: 141it [01:30,  1.60it/s]Extractor Estimating: 142it [01:30,  1.65it/s]Extractor Estimating: 143it [01:31,  1.64it/s]Extractor Estimating: 144it [01:31,  1.66it/s]Extractor Estimating: 145it [01:32,  1.72it/s]Extractor Estimating: 146it [01:32,  1.69it/s]Extractor Estimating: 147it [01:33,  1.74it/s]Extractor Estimating: 148it [01:34,  1.69it/s]Extractor Estimating: 149it [01:34,  1.70it/s]Extractor Estimating: 150it [01:35,  1.69it/s]Extractor Estimating: 151it [01:35,  1.71it/s]Extractor Estimating: 152it [01:36,  1.65it/s]Extractor Estimating: 153it [01:37,  1.69it/s]Extractor Estimating: 154it [01:37,  1.66it/s]Extractor Estimating: 155it [01:38,  1.63it/s]Extractor Estimating: 156it [01:39,  1.54it/s]Extractor Estimating: 157it [01:39,  1.51it/s]Extractor Estimating: 158it [01:40,  1.52it/s]Extractor Estimating: 159it [01:40,  1.58it/s]Extractor Estimating: 160it [01:41,  1.52it/s]Extractor Estimating: 161it [01:42,  1.54it/s]Extractor Estimating: 162it [01:42,  1.56it/s]Extractor Estimating: 163it [01:43,  1.55it/s]Extractor Estimating: 164it [01:44,  1.50it/s]Extractor Estimating: 165it [01:44,  1.56it/s]Extractor Estimating: 166it [01:45,  1.60it/s]Extractor Estimating: 167it [01:46,  1.56it/s]Extractor Estimating: 168it [01:46,  1.59it/s]Extractor Estimating: 169it [01:47,  1.57it/s]Extractor Estimating: 170it [01:48,  1.52it/s]Extractor Estimating: 171it [01:48,  1.57it/s]Extractor Estimating: 172it [01:49,  1.56it/s]Extractor Estimating: 173it [01:49,  1.60it/s]Extractor Estimating: 174it [01:50,  1.59it/s]Extractor Estimating: 175it [01:51,  1.66it/s]Extractor Estimating: 176it [01:51,  1.61it/s]Extractor Estimating: 177it [01:52,  1.39it/s]Extractor Estimating: 178it [01:53,  1.45it/s]Extractor Estimating: 179it [01:53,  1.49it/s]Extractor Estimating: 180it [01:54,  1.41it/s]Extractor Estimating: 181it [01:56,  1.10it/s]Extractor Estimating: 182it [01:56,  1.20it/s]Extractor Estimating: 183it [01:57,  1.33it/s]Extractor Estimating: 184it [01:58,  1.40it/s]Extractor Estimating: 185it [01:58,  1.36it/s]Extractor Estimating: 186it [01:59,  1.41it/s]Extractor Estimating: 187it [02:00,  1.46it/s]Extractor Estimating: 188it [02:00,  1.46it/s]Extractor Estimating: 189it [02:01,  1.50it/s]Extractor Estimating: 190it [02:02,  1.49it/s]Extractor Estimating: 191it [02:02,  1.50it/s]Extractor Estimating: 192it [02:03,  1.51it/s]Extractor Estimating: 193it [02:04,  1.51it/s]Extractor Estimating: 194it [02:04,  1.57it/s]Extractor Estimating: 195it [02:05,  1.56it/s]Extractor Estimating: 196it [02:05,  1.64it/s]Extractor Estimating: 197it [02:06,  1.67it/s]Extractor Estimating: 198it [02:07,  1.49it/s]Extractor Estimating: 199it [02:07,  1.55it/s]Extractor Estimating: 200it [02:08,  1.56it/s]Extractor Estimating: 201it [02:09,  1.50it/s]Extractor Estimating: 202it [02:09,  1.50it/s]Extractor Estimating: 203it [02:10,  1.52it/s]Extractor Estimating: 204it [02:11,  1.51it/s]Extractor Estimating: 205it [02:11,  1.47it/s]Extractor Estimating: 206it [02:12,  1.48it/s]Extractor Estimating: 207it [02:13,  1.46it/s]Extractor Estimating: 208it [02:13,  1.44it/s]Extractor Estimating: 209it [02:14,  1.44it/s]Extractor Estimating: 210it [02:15,  1.45it/s]Extractor Estimating: 211it [02:15,  1.48it/s]Extractor Estimating: 212it [02:16,  1.50it/s]Extractor Estimating: 213it [02:17,  1.49it/s]Extractor Estimating: 214it [02:17,  1.51it/s]Extractor Estimating: 215it [02:18,  1.56it/s]Extractor Estimating: 216it [02:19,  1.55it/s]Extractor Estimating: 217it [02:19,  1.52it/s]Extractor Estimating: 218it [02:20,  1.48it/s]Extractor Estimating: 219it [02:21,  1.50it/s]Extractor Estimating: 220it [02:21,  1.51it/s]Extractor Estimating: 221it [02:22,  1.52it/s]Extractor Estimating: 222it [02:23,  1.49it/s]Extractor Estimating: 223it [02:23,  1.50it/s]Extractor Estimating: 224it [02:24,  1.50it/s]Extractor Estimating: 225it [02:25,  1.32it/s]Extractor Estimating: 226it [02:26,  1.41it/s]Extractor Estimating: 227it [02:26,  1.46it/s]Extractor Estimating: 228it [02:27,  1.44it/s]Extractor Estimating: 229it [02:28,  1.51it/s]Extractor Estimating: 230it [02:28,  1.57it/s]Extractor Estimating: 231it [02:29,  1.55it/s]Extractor Estimating: 232it [02:29,  1.59it/s]Extractor Estimating: 233it [02:30,  1.56it/s]Extractor Estimating: 234it [02:31,  1.56it/s]Extractor Estimating: 235it [02:31,  1.46it/s]Extractor Estimating: 236it [02:32,  1.39it/s]Extractor Estimating: 237it [02:33,  1.44it/s]Extractor Estimating: 238it [02:34,  1.47it/s]Extractor Estimating: 239it [02:35,  1.20it/s]Extractor Estimating: 240it [02:35,  1.33it/s]Extractor Estimating: 241it [02:36,  1.44it/s]Extractor Estimating: 242it [02:37,  1.48it/s]Extractor Estimating: 243it [02:37,  1.50it/s]Extractor Estimating: 244it [02:38,  1.37it/s]Extractor Estimating: 245it [02:39,  1.42it/s]Extractor Estimating: 246it [02:39,  1.49it/s]Extractor Estimating: 247it [02:40,  1.54it/s]Extractor Estimating: 248it [02:41,  1.53it/s]Extractor Estimating: 249it [02:42,  1.24it/s]Extractor Estimating: 250it [02:42,  1.29it/s]Extractor Estimating: 251it [02:43,  1.41it/s]Extractor Estimating: 252it [02:44,  1.47it/s]Extractor Estimating: 253it [02:44,  1.42it/s]Extractor Estimating: 254it [02:45,  1.46it/s]Extractor Estimating: 255it [02:46,  1.50it/s]Extractor Estimating: 256it [02:46,  1.56it/s]Extractor Estimating: 257it [02:47,  1.57it/s]Extractor Estimating: 258it [02:47,  1.58it/s]Extractor Estimating: 259it [02:48,  1.61it/s]Extractor Estimating: 260it [02:49,  1.62it/s]Extractor Estimating: 261it [02:49,  1.65it/s]Extractor Estimating: 262it [02:50,  1.64it/s]Extractor Estimating: 263it [02:51,  1.29it/s]Extractor Estimating: 264it [02:52,  1.34it/s]Extractor Estimating: 265it [02:52,  1.41it/s]Extractor Estimating: 266it [02:53,  1.45it/s]Extractor Estimating: 267it [02:54,  1.28it/s]Extractor Estimating: 268it [02:54,  1.39it/s]Extractor Estimating: 269it [02:55,  1.46it/s]Extractor Estimating: 270it [02:56,  1.50it/s]Extractor Estimating: 271it [02:56,  1.58it/s]Extractor Estimating: 272it [02:57,  1.58it/s]Extractor Estimating: 273it [02:58,  1.60it/s]Extractor Estimating: 274it [02:58,  1.61it/s]Extractor Estimating: 275it [02:59,  1.56it/s]Extractor Estimating: 276it [02:59,  1.56it/s]Extractor Estimating: 277it [03:00,  1.49it/s]Extractor Estimating: 278it [03:01,  1.56it/s]Extractor Estimating: 279it [03:01,  1.55it/s]Extractor Estimating: 280it [03:02,  1.57it/s]Extractor Estimating: 281it [03:03,  1.54it/s]Extractor Estimating: 282it [03:03,  1.60it/s]Extractor Estimating: 283it [03:04,  1.58it/s]Extractor Estimating: 284it [03:05,  1.55it/s]Extractor Estimating: 285it [03:05,  1.58it/s]Extractor Estimating: 286it [03:06,  1.58it/s]Extractor Estimating: 287it [03:07,  1.57it/s]Extractor Estimating: 288it [03:07,  1.53it/s]Extractor Estimating: 289it [03:08,  1.50it/s]Extractor Estimating: 290it [03:09,  1.48it/s]Extractor Estimating: 291it [03:09,  1.47it/s]Extractor Estimating: 292it [03:10,  1.45it/s]Extractor Estimating: 293it [03:11,  1.47it/s]Extractor Estimating: 294it [03:11,  1.45it/s]Extractor Estimating: 295it [03:12,  1.47it/s]Extractor Estimating: 296it [03:13,  1.51it/s]Extractor Estimating: 297it [03:13,  1.53it/s]Extractor Estimating: 298it [03:14,  1.50it/s]Extractor Estimating: 299it [03:15,  1.42it/s]Extractor Estimating: 300it [03:15,  1.45it/s]Extractor Estimating: 301it [03:16,  1.57it/s]Extractor Estimating: 302it [03:17,  1.59it/s]Extractor Estimating: 303it [03:17,  1.58it/s]Extractor Estimating: 304it [03:18,  1.54it/s]Extractor Estimating: 305it [03:18,  1.60it/s]Extractor Estimating: 306it [03:19,  1.65it/s]Extractor Estimating: 307it [03:20,  1.68it/s]Extractor Estimating: 308it [03:20,  1.66it/s]Extractor Estimating: 309it [03:21,  1.70it/s]Extractor Estimating: 310it [03:21,  1.72it/s]Extractor Estimating: 311it [03:22,  1.74it/s]Extractor Estimating: 312it [03:23,  1.65it/s]Extractor Estimating: 313it [03:23,  1.67it/s]Extractor Estimating: 314it [03:24,  1.70it/s]Extractor Estimating: 315it [03:24,  1.71it/s]Extractor Estimating: 316it [03:25,  1.66it/s]Extractor Estimating: 317it [03:25,  1.72it/s]Extractor Estimating: 318it [03:26,  1.71it/s]Extractor Estimating: 319it [03:27,  1.73it/s]Extractor Estimating: 320it [03:27,  1.71it/s]Extractor Estimating: 321it [03:28,  1.72it/s]Extractor Estimating: 322it [03:28,  1.67it/s]Extractor Estimating: 323it [03:29,  1.69it/s]Extractor Estimating: 324it [03:30,  1.55it/s]Extractor Estimating: 325it [03:30,  1.63it/s]Extractor Estimating: 326it [03:31,  1.62it/s]Extractor Estimating: 327it [03:32,  1.58it/s]Extractor Estimating: 328it [03:32,  1.60it/s]Extractor Estimating: 329it [03:33,  1.60it/s]Extractor Estimating: 330it [03:33,  1.63it/s]Extractor Estimating: 331it [03:34,  1.61it/s]Extractor Estimating: 332it [03:35,  1.64it/s]Extractor Estimating: 333it [03:35,  1.62it/s]Extractor Estimating: 334it [03:36,  1.60it/s]Extractor Estimating: 335it [03:37,  1.60it/s]Extractor Estimating: 336it [03:37,  1.60it/s]Extractor Estimating: 337it [03:38,  1.59it/s]Extractor Estimating: 338it [03:38,  1.57it/s]Extractor Estimating: 339it [03:39,  1.56it/s]Extractor Estimating: 340it [03:40,  1.63it/s]Extractor Estimating: 341it [03:40,  1.67it/s]Extractor Estimating: 342it [03:41,  1.64it/s]Extractor Estimating: 343it [03:41,  1.65it/s]Extractor Estimating: 344it [03:42,  1.65it/s]Extractor Estimating: 345it [03:43,  1.62it/s]Extractor Estimating: 346it [03:43,  1.60it/s]Extractor Estimating: 347it [03:44,  1.60it/s]Extractor Estimating: 348it [03:45,  1.63it/s]Extractor Estimating: 349it [03:45,  1.66it/s]Extractor Estimating: 350it [03:46,  1.64it/s]Extractor Estimating: 351it [03:46,  1.67it/s]Extractor Estimating: 352it [03:47,  1.66it/s]Extractor Estimating: 353it [03:48,  1.61it/s]Extractor Estimating: 354it [03:48,  1.43it/s]Extractor Estimating: 355it [03:49,  1.45it/s]Extractor Estimating: 356it [03:50,  1.49it/s]Extractor Estimating: 357it [03:50,  1.53it/s]Extractor Estimating: 358it [03:51,  1.51it/s]Extractor Estimating: 359it [03:52,  1.35it/s]Extractor Estimating: 360it [03:53,  1.43it/s]Extractor Estimating: 361it [03:53,  1.50it/s]Extractor Estimating: 362it [03:54,  1.57it/s]Extractor Estimating: 363it [03:54,  1.60it/s]Extractor Estimating: 364it [03:55,  1.63it/s]Extractor Estimating: 365it [03:56,  1.67it/s]Extractor Estimating: 366it [03:56,  1.69it/s]Extractor Estimating: 367it [03:57,  1.71it/s]Extractor Estimating: 368it [03:57,  1.71it/s]Extractor Estimating: 369it [03:58,  1.67it/s]Extractor Estimating: 370it [03:59,  1.62it/s]Extractor Estimating: 371it [03:59,  1.62it/s]Extractor Estimating: 372it [04:00,  1.62it/s]Extractor Estimating: 373it [04:00,  1.66it/s]Extractor Estimating: 374it [04:01,  1.68it/s]Extractor Estimating: 375it [04:02,  1.59it/s]Extractor Estimating: 376it [04:02,  1.61it/s]Extractor Estimating: 377it [04:03,  1.55it/s]Extractor Estimating: 378it [04:03,  1.61it/s]Extractor Estimating: 379it [04:04,  1.60it/s]Extractor Estimating: 380it [04:05,  1.63it/s]Extractor Estimating: 381it [04:05,  1.66it/s]Extractor Estimating: 382it [04:06,  1.72it/s]Extractor Estimating: 383it [04:06,  1.68it/s]Extractor Estimating: 384it [04:07,  1.68it/s]Extractor Estimating: 385it [04:08,  1.68it/s]Extractor Estimating: 386it [04:08,  1.68it/s]Extractor Estimating: 387it [04:09,  1.67it/s]Extractor Estimating: 388it [04:10,  1.59it/s]Extractor Estimating: 389it [04:10,  1.62it/s]Extractor Estimating: 390it [04:11,  1.61it/s]Extractor Estimating: 391it [04:11,  1.68it/s]Extractor Estimating: 392it [04:12,  1.70it/s]Extractor Estimating: 393it [04:12,  1.68it/s]Extractor Estimating: 394it [04:13,  1.67it/s]Extractor Estimating: 395it [04:14,  1.62it/s]Extractor Estimating: 396it [04:14,  1.65it/s]Extractor Estimating: 397it [04:15,  1.61it/s]Extractor Estimating: 398it [04:16,  1.56it/s]Extractor Estimating: 399it [04:16,  1.54it/s]Extractor Estimating: 400it [04:17,  1.56it/s]Extractor Estimating: 401it [04:18,  1.63it/s]Extractor Estimating: 402it [04:18,  1.71it/s]Extractor Estimating: 403it [04:19,  1.70it/s]Extractor Estimating: 404it [04:19,  1.76it/s]Extractor Estimating: 405it [04:20,  1.78it/s]Extractor Estimating: 406it [04:20,  1.81it/s]Extractor Estimating: 407it [04:21,  1.78it/s]Extractor Estimating: 408it [04:21,  1.78it/s]Extractor Estimating: 409it [04:22,  1.68it/s]Extractor Estimating: 410it [04:23,  1.70it/s]Extractor Estimating: 411it [04:23,  1.76it/s]Extractor Estimating: 412it [04:24,  1.75it/s]Extractor Estimating: 413it [04:24,  1.78it/s]Extractor Estimating: 414it [04:25,  1.79it/s]Extractor Estimating: 415it [04:25,  1.71it/s]Extractor Estimating: 416it [04:26,  1.75it/s]Extractor Estimating: 417it [04:27,  1.76it/s]Extractor Estimating: 418it [04:27,  1.78it/s]Extractor Estimating: 419it [04:28,  1.82it/s]Extractor Estimating: 420it [04:28,  1.77it/s]Extractor Estimating: 421it [04:29,  1.78it/s]Extractor Estimating: 422it [04:29,  1.84it/s]Extractor Estimating: 423it [04:30,  1.86it/s]Extractor Estimating: 424it [04:30,  1.84it/s]Extractor Estimating: 425it [04:31,  1.85it/s]Extractor Estimating: 426it [04:32,  1.73it/s]Extractor Estimating: 427it [04:32,  1.62it/s]Extractor Estimating: 428it [04:33,  1.62it/s]Extractor Estimating: 429it [04:34,  1.42it/s]Extractor Estimating: 430it [04:34,  1.46it/s]Extractor Estimating: 431it [04:35,  1.50it/s]Extractor Estimating: 432it [04:36,  1.52it/s]Extractor Estimating: 433it [04:36,  1.56it/s]Extractor Estimating: 434it [04:37,  1.58it/s]Extractor Estimating: 435it [04:37,  1.62it/s]Extractor Estimating: 436it [04:38,  1.59it/s]Extractor Estimating: 437it [04:39,  1.59it/s]Extractor Estimating: 438it [04:39,  1.62it/s]Extractor Estimating: 439it [04:40,  1.66it/s]Extractor Estimating: 440it [04:40,  1.70it/s]Extractor Estimating: 441it [04:41,  1.67it/s]Extractor Estimating: 442it [04:42,  1.64it/s]Extractor Estimating: 443it [04:42,  1.65it/s]Extractor Estimating: 444it [04:43,  1.60it/s]Extractor Estimating: 445it [04:44,  1.57it/s]Extractor Estimating: 446it [04:44,  1.60it/s]Extractor Estimating: 447it [04:45,  1.53it/s]Extractor Estimating: 448it [04:46,  1.52it/s]Extractor Estimating: 449it [04:46,  1.58it/s]Extractor Estimating: 450it [04:47,  1.57it/s]Extractor Estimating: 451it [04:47,  1.60it/s]Extractor Estimating: 452it [04:48,  1.59it/s]Extractor Estimating: 453it [04:49,  1.60it/s]Extractor Estimating: 454it [04:49,  1.62it/s]Extractor Estimating: 455it [04:50,  1.54it/s]Extractor Estimating: 456it [04:51,  1.60it/s]Extractor Estimating: 457it [04:51,  1.59it/s]Extractor Estimating: 458it [04:52,  1.58it/s]Extractor Estimating: 459it [04:53,  1.56it/s]Extractor Estimating: 460it [04:53,  1.51it/s]Extractor Estimating: 461it [04:54,  1.56it/s]Extractor Estimating: 462it [04:55,  1.55it/s]Extractor Estimating: 463it [04:55,  1.58it/s]Extractor Estimating: 464it [04:56,  1.57it/s]Extractor Estimating: 465it [04:56,  1.57it/s]Extractor Estimating: 466it [04:57,  1.55it/s]Extractor Estimating: 467it [04:58,  1.51it/s]Extractor Estimating: 468it [04:58,  1.54it/s]Extractor Estimating: 469it [04:59,  1.52it/s]Extractor Estimating: 470it [05:00,  1.55it/s]Extractor Estimating: 471it [05:00,  1.54it/s]Extractor Estimating: 472it [05:01,  1.60it/s]Extractor Estimating: 473it [05:02,  1.55it/s]Extractor Estimating: 474it [05:02,  1.54it/s]Extractor Estimating: 475it [05:03,  1.55it/s]Extractor Estimating: 476it [05:04,  1.54it/s]Extractor Estimating: 477it [05:04,  1.53it/s]Extractor Estimating: 478it [05:05,  1.41it/s]Extractor Estimating: 479it [05:06,  1.46it/s]Extractor Estimating: 480it [05:06,  1.49it/s]Extractor Estimating: 481it [05:07,  1.50it/s]Extractor Estimating: 482it [05:08,  1.48it/s]Extractor Estimating: 483it [05:08,  1.47it/s]Extractor Estimating: 484it [05:09,  1.49it/s]Extractor Estimating: 485it [05:10,  1.51it/s]Extractor Estimating: 486it [05:10,  1.54it/s]Extractor Estimating: 487it [05:11,  1.51it/s]Extractor Estimating: 488it [05:12,  1.51it/s]Extractor Estimating: 489it [05:12,  1.48it/s]Extractor Estimating: 490it [05:13,  1.52it/s]Extractor Estimating: 491it [05:14,  1.54it/s]Extractor Estimating: 492it [05:14,  1.42it/s]Extractor Estimating: 493it [05:15,  1.50it/s]Extractor Estimating: 494it [05:16,  1.49it/s]Extractor Estimating: 495it [05:16,  1.49it/s]Extractor Estimating: 496it [05:17,  1.51it/s]Extractor Estimating: 497it [05:18,  1.43it/s]Extractor Estimating: 498it [05:19,  1.37it/s]Extractor Estimating: 499it [05:19,  1.39it/s]Extractor Estimating: 500it [05:20,  1.41it/s]Extractor Estimating: 500it [05:20,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:09,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:09,175 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:09,175 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:09,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:09,176 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:58:09,478 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:58:09,479 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:58:09,750 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:58:10,791 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:58:10,791 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:12,886 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:12,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:12,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:12,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:58:12,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:58:13,346 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:58:13,347 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:58:13,621 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:58:13,774 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:58:13,774 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 21:03:35,387 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 21:03:35,417 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9992 mean pseudo reward: 0.9666051557599239
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 25602
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25702, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25702, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.080, loss:665.9106
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.068, loss:631.8976
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.072, loss:624.9965
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.065, loss:637.8814
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.072, loss:595.7071
>> valid entity prec:0.6835, rec:0.7118, f1:0.6974
>> valid relation prec:0.4294, rec:0.2382, f1:0.3064
>> valid relation with NER prec:0.4294, rec:0.2382, f1:0.3064
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.430, loss:592.2059
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.069, loss:633.9286
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.084, loss:647.6438
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.081, loss:614.2911
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.074, loss:630.4690
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.7286, rec:0.6211, f1:0.6705
>> valid relation prec:0.4343, rec:0.1558, f1:0.2294
>> valid relation with NER prec:0.4343, rec:0.1558, f1:0.2294
g_step 1100, step 266, avg_time 2.417, loss:635.6325
g_step 1200, step 366, avg_time 1.074, loss:651.6836
g_step 1300, step 49, avg_time 1.072, loss:637.3600
g_step 1400, step 149, avg_time 1.085, loss:592.1472
g_step 1500, step 249, avg_time 1.085, loss:609.3088
>> valid entity prec:0.6529, rec:0.6852, f1:0.6686
>> valid relation prec:0.3991, rec:0.2076, f1:0.2731
>> valid relation with NER prec:0.3991, rec:0.2076, f1:0.2731
g_step 1600, step 349, avg_time 2.420, loss:615.2784
g_step 1700, step 32, avg_time 1.062, loss:582.4179
g_step 1800, step 132, avg_time 1.080, loss:565.7972
g_step 1900, step 232, avg_time 1.078, loss:577.3669
g_step 2000, step 332, avg_time 1.072, loss:575.5732
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.7315, rec:0.6515, f1:0.6892
>> valid relation prec:0.4406, rec:0.2088, f1:0.2833
>> valid relation with NER prec:0.4406, rec:0.2088, f1:0.2833
g_step 2100, step 15, avg_time 2.410, loss:561.6626
g_step 2200, step 115, avg_time 1.073, loss:545.9940
g_step 2300, step 215, avg_time 1.080, loss:537.6635
g_step 2400, step 315, avg_time 1.053, loss:548.8897
g_step 2500, step 415, avg_time 1.083, loss:561.9603
>> valid entity prec:0.6672, rec:0.7213, f1:0.6932
>> valid relation prec:0.3853, rec:0.2256, f1:0.2846
>> valid relation with NER prec:0.3853, rec:0.2256, f1:0.2846
g_step 2600, step 98, avg_time 2.418, loss:495.3347
g_step 2700, step 198, avg_time 1.064, loss:515.5994
g_step 2800, step 298, avg_time 1.073, loss:519.8925
g_step 2900, step 398, avg_time 1.078, loss:550.9506
g_step 3000, step 81, avg_time 1.085, loss:485.8707
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.7186, rec:0.6791, f1:0.6983
>> valid relation prec:0.3614, rec:0.1730, f1:0.2340
>> valid relation with NER prec:0.3614, rec:0.1730, f1:0.2340
new max entity f1 on valid!
g_step 3100, step 181, avg_time 2.420, loss:475.8822
g_step 3200, step 281, avg_time 1.061, loss:493.1806
g_step 3300, step 381, avg_time 1.077, loss:512.4910
g_step 3400, step 64, avg_time 1.076, loss:486.5477
g_step 3500, step 164, avg_time 1.070, loss:465.3346
>> valid entity prec:0.6754, rec:0.7044, f1:0.6896
>> valid relation prec:0.3715, rec:0.1802, f1:0.2426
>> valid relation with NER prec:0.3715, rec:0.1802, f1:0.2426
g_step 3600, step 264, avg_time 2.433, loss:482.2632
g_step 3700, step 364, avg_time 1.065, loss:488.7310
g_step 3800, step 47, avg_time 1.072, loss:454.3708
g_step 3900, step 147, avg_time 1.073, loss:447.8078
g_step 4000, step 247, avg_time 1.077, loss:469.8080
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6609, rec:0.6730, f1:0.6669
>> valid relation prec:0.3588, rec:0.1747, f1:0.2350
>> valid relation with NER prec:0.3588, rec:0.1747, f1:0.2350
g_step 4100, step 347, avg_time 2.431, loss:485.1425
g_step 4200, step 30, avg_time 1.076, loss:452.0090
g_step 4300, step 130, avg_time 1.074, loss:434.4026
g_step 4400, step 230, avg_time 1.079, loss:424.0242
g_step 4500, step 330, avg_time 1.072, loss:445.2988
>> valid entity prec:0.6842, rec:0.6265, f1:0.6541
>> valid relation prec:0.3751, rec:0.1945, f1:0.2561
>> valid relation with NER prec:0.3751, rec:0.1945, f1:0.2561
g_step 4600, step 13, avg_time 2.427, loss:437.0137
g_step 4700, step 113, avg_time 1.076, loss:396.8568
g_step 4800, step 213, avg_time 1.071, loss:413.2437
g_step 4900, step 313, avg_time 1.072, loss:433.1932
g_step 5000, step 413, avg_time 1.075, loss:452.3669
learning rate was adjusted to 0.0008
>> valid entity prec:0.6911, rec:0.6545, f1:0.6723
>> valid relation prec:0.3067, rec:0.1613, f1:0.2114
>> valid relation with NER prec:0.3067, rec:0.1613, f1:0.2114
g_step 5100, step 96, avg_time 2.430, loss:382.5711
g_step 5200, step 196, avg_time 1.079, loss:396.3246
g_step 5300, step 296, avg_time 1.076, loss:395.1136
g_step 5400, step 396, avg_time 1.072, loss:400.5375
g_step 5500, step 79, avg_time 1.061, loss:345.0000
>> valid entity prec:0.7126, rec:0.6558, f1:0.6830
>> valid relation prec:0.3996, rec:0.2116, f1:0.2767
>> valid relation with NER prec:0.3996, rec:0.2116, f1:0.2767
g_step 5600, step 179, avg_time 2.418, loss:369.0553
g_step 5700, step 279, avg_time 1.063, loss:401.6152
g_step 5800, step 379, avg_time 1.085, loss:408.1723
g_step 5900, step 62, avg_time 1.068, loss:372.1421
g_step 6000, step 162, avg_time 1.070, loss:347.9491
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6868, rec:0.6419, f1:0.6636
>> valid relation prec:0.2565, rec:0.1476, f1:0.1873
>> valid relation with NER prec:0.2565, rec:0.1476, f1:0.1873
g_step 6100, step 262, avg_time 2.433, loss:375.4441
g_step 6200, step 362, avg_time 1.087, loss:396.3315
g_step 6300, step 45, avg_time 1.050, loss:374.6230
g_step 6400, step 145, avg_time 1.093, loss:334.5164
g_step 6500, step 245, avg_time 1.078, loss:359.8469
>> valid entity prec:0.6605, rec:0.6968, f1:0.6782
>> valid relation prec:0.2327, rec:0.1450, f1:0.1786
>> valid relation with NER prec:0.2327, rec:0.1450, f1:0.1786
g_step 6600, step 345, avg_time 2.431, loss:359.2964
g_step 6700, step 28, avg_time 1.070, loss:352.2697
g_step 6800, step 128, avg_time 1.083, loss:331.0572
g_step 6900, step 228, avg_time 1.071, loss:354.5919
g_step 7000, step 328, avg_time 1.078, loss:349.9032
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6885, rec:0.6004, f1:0.6414
>> valid relation prec:0.3108, rec:0.1538, f1:0.2058
>> valid relation with NER prec:0.3108, rec:0.1538, f1:0.2058
g_step 7100, step 11, avg_time 2.418, loss:341.1811
g_step 7200, step 111, avg_time 1.074, loss:322.2652
g_step 7300, step 211, avg_time 1.069, loss:328.2585
g_step 7400, step 311, avg_time 1.073, loss:320.4231
g_step 7500, step 411, avg_time 1.071, loss:325.0558
>> valid entity prec:0.6904, rec:0.6615, f1:0.6756
>> valid relation prec:0.3548, rec:0.1876, f1:0.2454
>> valid relation with NER prec:0.3548, rec:0.1876, f1:0.2454
g_step 7600, step 94, avg_time 2.410, loss:295.0796
g_step 7700, step 194, avg_time 1.075, loss:299.2203
g_step 7800, step 294, avg_time 1.090, loss:319.9187
g_step 7900, step 394, avg_time 1.073, loss:335.9461
g_step 8000, step 77, avg_time 1.077, loss:302.8628
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.7133, rec:0.6353, f1:0.6720
>> valid relation prec:0.3042, rec:0.1624, f1:0.2118
>> valid relation with NER prec:0.3042, rec:0.1624, f1:0.2118
g_step 8100, step 177, avg_time 2.428, loss:304.5299
g_step 8200, step 277, avg_time 1.072, loss:288.2188
g_step 8300, step 377, avg_time 1.074, loss:308.4856
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 21:03:35 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 21:03:35 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_21-03-35_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 21:03:36 - WARNING - datasets.builder -   Using custom data configuration default-a429f7bdd68768e1
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-a429f7bdd68768e1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 21:03:36,963 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:03:36,964 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:03:36,965 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:03:36,966 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:03:36,976 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:03:36,981 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:03:36,981 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:03:36,981 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:03:36,981 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:03:36,981 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:03:36,981 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 21:03:37,113 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:03:40,247 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 21:03:40,249 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-a429f7bdd68768e1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:05,  1.72ba/s] 18%|█▊        | 2/11 [00:00<00:03,  2.77ba/s] 27%|██▋       | 3/11 [00:00<00:02,  3.42ba/s] 36%|███▋      | 4/11 [00:01<00:01,  3.81ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.07ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.21ba/s] 64%|██████▎   | 7/11 [00:01<00:00,  4.34ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.44ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.50ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.55ba/s]100%|██████████| 11/11 [00:02<00:00,  4.37ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.02ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.18ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.06ba/s]100%|██████████| 4/4 [00:00<00:00,  5.11ba/s]100%|██████████| 4/4 [00:00<00:00,  4.68ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  5.11ba/s] 18%|█▊        | 2/11 [00:00<00:01,  7.16ba/s] 36%|███▋      | 4/11 [00:00<00:00,  8.80ba/s] 45%|████▌     | 5/11 [00:00<00:00,  9.06ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.47ba/s] 73%|███████▎  | 8/11 [00:00<00:00,  9.59ba/s] 91%|█████████ | 10/11 [00:01<00:00,  9.80ba/s]100%|██████████| 11/11 [00:01<00:00, 10.03ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.19ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.66ba/s]100%|██████████| 4/4 [00:00<00:00,  9.73ba/s]
[INFO|trainer.py:414] 2023-08-28 21:03:46,209 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 21:03:46,227 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 21:03:46,227 >>   Num examples = 10004
[INFO|trainer.py:1149] 2023-08-28 21:03:46,227 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 21:03:46,227 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 21:03:46,227 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 21:03:46,227 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 21:03:46,227 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:59,  3.25it/s]  0%|          | 2/780 [00:00<03:50,  3.38it/s]  0%|          | 3/780 [00:00<03:46,  3.43it/s]  1%|          | 4/780 [00:01<03:45,  3.45it/s]  1%|          | 5/780 [00:01<03:44,  3.46it/s]  1%|          | 6/780 [00:01<03:43,  3.47it/s]  1%|          | 7/780 [00:02<03:42,  3.47it/s]  1%|          | 8/780 [00:02<03:42,  3.47it/s]  1%|          | 9/780 [00:02<03:44,  3.44it/s]  1%|▏         | 10/780 [00:02<03:43,  3.45it/s]  1%|▏         | 11/780 [00:03<03:42,  3.46it/s]  2%|▏         | 12/780 [00:03<03:41,  3.47it/s]  2%|▏         | 13/780 [00:03<03:41,  3.47it/s]  2%|▏         | 14/780 [00:04<03:40,  3.47it/s]  2%|▏         | 15/780 [00:04<03:40,  3.47it/s]  2%|▏         | 16/780 [00:04<03:39,  3.47it/s]  2%|▏         | 17/780 [00:04<03:39,  3.47it/s]  2%|▏         | 18/780 [00:05<03:39,  3.48it/s]  2%|▏         | 19/780 [00:05<03:39,  3.47it/s]  3%|▎         | 20/780 [00:05<03:38,  3.48it/s]  3%|▎         | 21/780 [00:06<03:38,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.48it/s]  3%|▎         | 23/780 [00:06<03:37,  3.48it/s]  3%|▎         | 24/780 [00:06<03:37,  3.48it/s]  3%|▎         | 25/780 [00:07<03:37,  3.48it/s]  3%|▎         | 26/780 [00:07<03:36,  3.48it/s]  3%|▎         | 27/780 [00:07<03:58,  3.15it/s]  4%|▎         | 28/780 [00:08<03:51,  3.24it/s]  4%|▎         | 29/780 [00:08<03:46,  3.31it/s]  4%|▍         | 30/780 [00:08<03:43,  3.36it/s]  4%|▍         | 31/780 [00:09<03:40,  3.39it/s]  4%|▍         | 32/780 [00:09<03:39,  3.41it/s]  4%|▍         | 33/780 [00:09<03:37,  3.43it/s]  4%|▍         | 34/780 [00:09<03:36,  3.44it/s]  4%|▍         | 35/780 [00:10<03:35,  3.45it/s]  5%|▍         | 36/780 [00:10<03:35,  3.45it/s]  5%|▍         | 37/780 [00:10<03:34,  3.46it/s]  5%|▍         | 38/780 [00:11<03:34,  3.46it/s]  5%|▌         | 39/780 [00:11<03:34,  3.46it/s]  5%|▌         | 40/780 [00:11<03:33,  3.46it/s]  5%|▌         | 41/780 [00:11<03:33,  3.46it/s]  5%|▌         | 42/780 [00:12<03:33,  3.46it/s]  6%|▌         | 43/780 [00:12<03:32,  3.47it/s]  6%|▌         | 44/780 [00:12<03:32,  3.47it/s]  6%|▌         | 45/780 [00:13<03:34,  3.42it/s]  6%|▌         | 46/780 [00:13<03:33,  3.44it/s]  6%|▌         | 47/780 [00:13<03:32,  3.45it/s]  6%|▌         | 48/780 [00:13<03:31,  3.46it/s]  6%|▋         | 49/780 [00:14<03:31,  3.46it/s]  6%|▋         | 50/780 [00:14<03:30,  3.46it/s]  7%|▋         | 51/780 [00:14<03:30,  3.46it/s]  7%|▋         | 52/780 [00:15<03:30,  3.46it/s]  7%|▋         | 53/780 [00:15<03:29,  3.47it/s]  7%|▋         | 54/780 [00:15<03:29,  3.47it/s]  7%|▋         | 55/780 [00:15<03:28,  3.47it/s]  7%|▋         | 56/780 [00:16<03:28,  3.47it/s]  7%|▋         | 57/780 [00:16<03:28,  3.47it/s]  7%|▋         | 58/780 [00:16<03:28,  3.47it/s]  8%|▊         | 59/780 [00:17<03:27,  3.47it/s]  8%|▊         | 60/780 [00:17<03:27,  3.47it/s]  8%|▊         | 61/780 [00:17<03:27,  3.47it/s]  8%|▊         | 62/780 [00:17<03:26,  3.47it/s]  8%|▊         | 63/780 [00:18<03:27,  3.46it/s]  8%|▊         | 64/780 [00:18<03:26,  3.46it/s]  8%|▊         | 65/780 [00:18<03:26,  3.46it/s]  8%|▊         | 66/780 [00:19<03:26,  3.46it/s]  9%|▊         | 67/780 [00:19<03:25,  3.46it/s]  9%|▊         | 68/780 [00:19<03:25,  3.46it/s]  9%|▉         | 69/780 [00:20<03:25,  3.47it/s]  9%|▉         | 70/780 [00:20<03:24,  3.46it/s]  9%|▉         | 71/780 [00:20<03:24,  3.47it/s]  9%|▉         | 72/780 [00:20<03:24,  3.46it/s]  9%|▉         | 73/780 [00:21<03:24,  3.46it/s]  9%|▉         | 74/780 [00:21<03:23,  3.47it/s] 10%|▉         | 75/780 [00:21<03:23,  3.46it/s] 10%|▉         | 76/780 [00:22<03:23,  3.46it/s] 10%|▉         | 77/780 [00:22<03:22,  3.46it/s] 10%|█         | 78/780 [00:22<03:22,  3.47it/s] 10%|█         | 79/780 [00:22<03:22,  3.46it/s] 10%|█         | 80/780 [00:23<03:27,  3.37it/s] 10%|█         | 81/780 [00:23<03:25,  3.40it/s] 11%|█         | 82/780 [00:23<03:23,  3.42it/s] 11%|█         | 83/780 [00:24<03:22,  3.44it/s] 11%|█         | 84/780 [00:24<03:22,  3.45it/s] 11%|█         | 85/780 [00:24<03:21,  3.45it/s] 11%|█         | 86/780 [00:24<03:20,  3.46it/s] 11%|█         | 87/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 88/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 89/780 [00:25<03:19,  3.46it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 91/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 92/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 93/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 94/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 95/780 [00:27<03:17,  3.46it/s] 12%|█▏        | 96/780 [00:27<03:17,  3.46it/s] 12%|█▏        | 97/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 98/780 [00:28<03:33,  3.20it/s] 13%|█▎        | 99/780 [00:28<03:28,  3.27it/s] 13%|█▎        | 100/780 [00:29<03:24,  3.33it/s] 13%|█▎        | 101/780 [00:29<03:21,  3.37it/s] 13%|█▎        | 102/780 [00:29<03:19,  3.40it/s] 13%|█▎        | 103/780 [00:29<03:18,  3.42it/s] 13%|█▎        | 104/780 [00:30<03:17,  3.43it/s] 13%|█▎        | 105/780 [00:30<03:16,  3.44it/s] 14%|█▎        | 106/780 [00:30<03:15,  3.44it/s] 14%|█▎        | 107/780 [00:31<03:15,  3.45it/s] 14%|█▍        | 108/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 109/780 [00:31<03:14,  3.46it/s] 14%|█▍        | 110/780 [00:31<03:13,  3.46it/s] 14%|█▍        | 111/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 113/780 [00:32<03:12,  3.46it/s] 15%|█▍        | 114/780 [00:33<03:12,  3.46it/s] 15%|█▍        | 115/780 [00:33<03:13,  3.43it/s] 15%|█▍        | 116/780 [00:33<03:12,  3.44it/s] 15%|█▌        | 117/780 [00:33<03:12,  3.45it/s] 15%|█▌        | 118/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 119/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 120/780 [00:34<03:10,  3.46it/s] 16%|█▌        | 121/780 [00:35<03:10,  3.46it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.46it/s] 16%|█▌        | 123/780 [00:35<03:09,  3.46it/s] 16%|█▌        | 124/780 [00:36<03:09,  3.46it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.46it/s] 16%|█▌        | 126/780 [00:36<03:08,  3.46it/s] 16%|█▋        | 127/780 [00:36<03:08,  3.46it/s] 16%|█▋        | 128/780 [00:37<03:08,  3.46it/s] 17%|█▋        | 129/780 [00:37<03:08,  3.46it/s] 17%|█▋        | 130/780 [00:37<03:07,  3.46it/s] 17%|█▋        | 131/780 [00:38<03:07,  3.46it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.46it/s] 17%|█▋        | 133/780 [00:38<03:18,  3.26it/s] 17%|█▋        | 134/780 [00:38<03:14,  3.32it/s] 17%|█▋        | 135/780 [00:39<03:11,  3.36it/s] 17%|█▋        | 136/780 [00:39<03:09,  3.39it/s] 18%|█▊        | 137/780 [00:39<03:08,  3.41it/s] 18%|█▊        | 138/780 [00:40<03:07,  3.43it/s] 18%|█▊        | 139/780 [00:40<03:06,  3.44it/s] 18%|█▊        | 140/780 [00:40<03:05,  3.44it/s] 18%|█▊        | 141/780 [00:40<03:05,  3.44it/s] 18%|█▊        | 142/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 143/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 144/780 [00:41<03:04,  3.45it/s] 19%|█▊        | 145/780 [00:42<03:03,  3.46it/s] 19%|█▊        | 146/780 [00:42<03:03,  3.46it/s] 19%|█▉        | 147/780 [00:42<03:03,  3.46it/s] 19%|█▉        | 148/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 149/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 150/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 151/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 152/780 [00:44<03:01,  3.46it/s] 20%|█▉        | 153/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 154/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 155/780 [00:45<03:00,  3.45it/s] 20%|██        | 156/780 [00:45<03:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 21:04:31,595 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:04:31,595 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 21:04:31,595 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.34it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.71it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.96it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.32it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.85it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.53it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.21it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.96it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.86it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.79it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.79it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.86it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.92it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.89it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.95it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.90it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.81it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.88it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.73it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.77it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.78it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.80it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.88it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.94it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.88it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.82it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.82it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.79it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.82it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.86it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.65it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.82it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.90it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.84it/s][A
 41%|████      | 178/438 [00:03<00:05, 45.00it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 45.91it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.22it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.41it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.53it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.63it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.63it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.68it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.77it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.68it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.81it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.87it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.86it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.81it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 45.70it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 46.09it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.24it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.25it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.51it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.68it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.75it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.79it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.75it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.73it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.71it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.79it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.82it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.91it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.95it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.82it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.76it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.76it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.79it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.78it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.78it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.80it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.88it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.85it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.86it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.77it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.71it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.67it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 45.98it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.32it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.42it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.58it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.66it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.67it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.72it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.74it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.64it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.70it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.91it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.91it/s][A 20%|██        | 156/780 [00:54<03:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:04:41,042 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 21:04:41,072 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:04:47,654 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:04:47,846 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:04:47,868 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:11<1:22:06,  7.91s/it] 20%|██        | 158/780 [01:11<58:19,  5.63s/it]   20%|██        | 159/780 [01:11<41:39,  4.02s/it] 21%|██        | 160/780 [01:11<30:00,  2.90s/it] 21%|██        | 161/780 [01:12<21:51,  2.12s/it] 21%|██        | 162/780 [01:12<16:09,  1.57s/it] 21%|██        | 163/780 [01:12<12:11,  1.19s/it] 21%|██        | 164/780 [01:13<09:24,  1.09it/s] 21%|██        | 165/780 [01:13<07:27,  1.37it/s] 21%|██▏       | 166/780 [01:13<06:05,  1.68it/s] 21%|██▏       | 167/780 [01:13<05:08,  1.99it/s] 22%|██▏       | 168/780 [01:14<04:28,  2.28it/s] 22%|██▏       | 169/780 [01:14<04:02,  2.52it/s] 22%|██▏       | 170/780 [01:14<03:42,  2.74it/s] 22%|██▏       | 171/780 [01:15<03:27,  2.93it/s] 22%|██▏       | 172/780 [01:15<03:17,  3.07it/s] 22%|██▏       | 173/780 [01:15<03:10,  3.18it/s] 22%|██▏       | 174/780 [01:15<03:05,  3.26it/s] 22%|██▏       | 175/780 [01:16<03:02,  3.32it/s] 23%|██▎       | 176/780 [01:16<02:59,  3.36it/s] 23%|██▎       | 177/780 [01:16<02:57,  3.39it/s] 23%|██▎       | 178/780 [01:17<02:56,  3.42it/s] 23%|██▎       | 179/780 [01:17<02:55,  3.43it/s] 23%|██▎       | 180/780 [01:17<02:55,  3.42it/s] 23%|██▎       | 181/780 [01:17<02:54,  3.44it/s] 23%|██▎       | 182/780 [01:18<02:53,  3.45it/s] 23%|██▎       | 183/780 [01:18<02:52,  3.45it/s] 24%|██▎       | 184/780 [01:18<02:52,  3.46it/s] 24%|██▎       | 185/780 [01:19<02:51,  3.46it/s] 24%|██▍       | 186/780 [01:19<02:51,  3.46it/s] 24%|██▍       | 187/780 [01:19<02:51,  3.46it/s] 24%|██▍       | 188/780 [01:19<02:51,  3.46it/s] 24%|██▍       | 189/780 [01:20<02:50,  3.46it/s] 24%|██▍       | 190/780 [01:20<02:50,  3.46it/s] 24%|██▍       | 191/780 [01:20<02:52,  3.42it/s] 25%|██▍       | 192/780 [01:21<02:51,  3.44it/s] 25%|██▍       | 193/780 [01:21<02:50,  3.45it/s] 25%|██▍       | 194/780 [01:21<02:58,  3.29it/s] 25%|██▌       | 195/780 [01:22<02:55,  3.34it/s] 25%|██▌       | 196/780 [01:22<02:52,  3.38it/s] 25%|██▌       | 197/780 [01:22<02:51,  3.40it/s] 25%|██▌       | 198/780 [01:22<02:50,  3.42it/s] 26%|██▌       | 199/780 [01:23<02:49,  3.43it/s] 26%|██▌       | 200/780 [01:23<02:48,  3.44it/s] 26%|██▌       | 201/780 [01:23<02:47,  3.45it/s] 26%|██▌       | 202/780 [01:24<02:47,  3.45it/s] 26%|██▌       | 203/780 [01:24<02:46,  3.46it/s] 26%|██▌       | 204/780 [01:24<02:47,  3.44it/s] 26%|██▋       | 205/780 [01:24<02:46,  3.45it/s] 26%|██▋       | 206/780 [01:25<02:46,  3.46it/s] 27%|██▋       | 207/780 [01:25<02:45,  3.46it/s] 27%|██▋       | 208/780 [01:25<02:45,  3.46it/s] 27%|██▋       | 209/780 [01:26<02:44,  3.46it/s] 27%|██▋       | 210/780 [01:26<02:44,  3.46it/s] 27%|██▋       | 211/780 [01:26<02:44,  3.46it/s] 27%|██▋       | 212/780 [01:26<02:43,  3.46it/s] 27%|██▋       | 213/780 [01:27<02:43,  3.47it/s] 27%|██▋       | 214/780 [01:27<02:43,  3.47it/s] 28%|██▊       | 215/780 [01:27<02:46,  3.39it/s] 28%|██▊       | 216/780 [01:28<02:45,  3.41it/s] 28%|██▊       | 217/780 [01:28<02:44,  3.42it/s] 28%|██▊       | 218/780 [01:28<02:43,  3.44it/s] 28%|██▊       | 219/780 [01:29<02:42,  3.44it/s] 28%|██▊       | 220/780 [01:29<02:42,  3.45it/s] 28%|██▊       | 221/780 [01:29<02:41,  3.45it/s] 28%|██▊       | 222/780 [01:29<02:41,  3.46it/s] 29%|██▊       | 223/780 [01:30<02:41,  3.46it/s] 29%|██▊       | 224/780 [01:30<02:40,  3.46it/s] 29%|██▉       | 225/780 [01:30<02:40,  3.46it/s] 29%|██▉       | 226/780 [01:31<02:42,  3.40it/s] 29%|██▉       | 227/780 [01:31<02:41,  3.42it/s] 29%|██▉       | 228/780 [01:31<02:40,  3.44it/s] 29%|██▉       | 229/780 [01:31<02:40,  3.44it/s] 29%|██▉       | 230/780 [01:32<02:39,  3.45it/s] 30%|██▉       | 231/780 [01:32<02:38,  3.45it/s] 30%|██▉       | 232/780 [01:32<02:38,  3.46it/s] 30%|██▉       | 233/780 [01:33<02:38,  3.46it/s] 30%|███       | 234/780 [01:33<02:37,  3.46it/s] 30%|███       | 235/780 [01:33<02:37,  3.46it/s] 30%|███       | 236/780 [01:33<02:37,  3.46it/s] 30%|███       | 237/780 [01:34<02:37,  3.45it/s] 31%|███       | 238/780 [01:34<02:36,  3.46it/s] 31%|███       | 239/780 [01:34<02:36,  3.46it/s] 31%|███       | 240/780 [01:35<02:36,  3.46it/s] 31%|███       | 241/780 [01:35<02:35,  3.46it/s] 31%|███       | 242/780 [01:35<02:35,  3.46it/s] 31%|███       | 243/780 [01:35<02:35,  3.46it/s] 31%|███▏      | 244/780 [01:36<02:34,  3.46it/s] 31%|███▏      | 245/780 [01:36<02:34,  3.46it/s] 32%|███▏      | 246/780 [01:36<02:34,  3.46it/s] 32%|███▏      | 247/780 [01:37<02:33,  3.46it/s] 32%|███▏      | 248/780 [01:37<02:35,  3.43it/s] 32%|███▏      | 249/780 [01:37<02:34,  3.44it/s] 32%|███▏      | 250/780 [01:37<02:33,  3.44it/s] 32%|███▏      | 251/780 [01:38<02:33,  3.45it/s] 32%|███▏      | 252/780 [01:38<02:32,  3.45it/s] 32%|███▏      | 253/780 [01:38<02:32,  3.46it/s] 33%|███▎      | 254/780 [01:39<02:32,  3.46it/s] 33%|███▎      | 255/780 [01:39<02:31,  3.46it/s] 33%|███▎      | 256/780 [01:39<02:31,  3.46it/s] 33%|███▎      | 257/780 [01:40<02:31,  3.46it/s] 33%|███▎      | 258/780 [01:40<02:30,  3.46it/s] 33%|███▎      | 259/780 [01:40<02:32,  3.41it/s] 33%|███▎      | 260/780 [01:40<02:31,  3.43it/s] 33%|███▎      | 261/780 [01:41<02:30,  3.44it/s] 34%|███▎      | 262/780 [01:41<02:30,  3.44it/s] 34%|███▎      | 263/780 [01:41<02:29,  3.45it/s] 34%|███▍      | 264/780 [01:42<02:29,  3.45it/s] 34%|███▍      | 265/780 [01:42<02:28,  3.46it/s] 34%|███▍      | 266/780 [01:42<02:28,  3.46it/s] 34%|███▍      | 267/780 [01:42<02:28,  3.46it/s] 34%|███▍      | 268/780 [01:43<02:28,  3.46it/s] 34%|███▍      | 269/780 [01:43<02:27,  3.46it/s] 35%|███▍      | 270/780 [01:43<02:33,  3.32it/s] 35%|███▍      | 271/780 [01:44<02:31,  3.36it/s] 35%|███▍      | 272/780 [01:44<02:29,  3.39it/s] 35%|███▌      | 273/780 [01:44<02:28,  3.41it/s] 35%|███▌      | 274/780 [01:44<02:27,  3.43it/s] 35%|███▌      | 275/780 [01:45<02:26,  3.44it/s] 35%|███▌      | 276/780 [01:45<02:26,  3.44it/s] 36%|███▌      | 277/780 [01:45<02:25,  3.45it/s] 36%|███▌      | 278/780 [01:46<02:25,  3.45it/s] 36%|███▌      | 279/780 [01:46<02:25,  3.45it/s] 36%|███▌      | 280/780 [01:46<02:24,  3.46it/s] 36%|███▌      | 281/780 [01:47<02:25,  3.43it/s] 36%|███▌      | 282/780 [01:47<02:24,  3.44it/s] 36%|███▋      | 283/780 [01:47<02:24,  3.45it/s] 36%|███▋      | 284/780 [01:47<02:31,  3.28it/s] 37%|███▋      | 285/780 [01:48<02:28,  3.33it/s] 37%|███▋      | 286/780 [01:48<02:26,  3.36it/s] 37%|███▋      | 287/780 [01:48<02:25,  3.39it/s] 37%|███▋      | 288/780 [01:49<02:24,  3.41it/s] 37%|███▋      | 289/780 [01:49<02:37,  3.12it/s] 37%|███▋      | 290/780 [01:49<02:32,  3.20it/s] 37%|███▋      | 291/780 [01:50<02:29,  3.27it/s] 37%|███▋      | 292/780 [01:50<02:26,  3.32it/s] 38%|███▊      | 293/780 [01:50<02:24,  3.37it/s] 38%|███▊      | 294/780 [01:50<02:23,  3.39it/s] 38%|███▊      | 295/780 [01:51<02:22,  3.41it/s] 38%|███▊      | 296/780 [01:51<02:21,  3.43it/s] 38%|███▊      | 297/780 [01:51<02:20,  3.44it/s] 38%|███▊      | 298/780 [01:52<02:19,  3.44it/s] 38%|███▊      | 299/780 [01:52<02:19,  3.45it/s] 38%|███▊      | 300/780 [01:52<02:19,  3.45it/s] 39%|███▊      | 301/780 [01:52<02:18,  3.45it/s] 39%|███▊      | 302/780 [01:53<02:18,  3.46it/s] 39%|███▉      | 303/780 [01:53<02:17,  3.46it/s] 39%|███▉      | 304/780 [01:53<02:17,  3.46it/s] 39%|███▉      | 305/780 [01:54<02:17,  3.46it/s] 39%|███▉      | 306/780 [01:54<02:17,  3.46it/s] 39%|███▉      | 307/780 [01:54<02:16,  3.46it/s] 39%|███▉      | 308/780 [01:54<02:16,  3.46it/s] 40%|███▉      | 309/780 [01:55<02:16,  3.46it/s] 40%|███▉      | 310/780 [01:55<02:19,  3.37it/s] 40%|███▉      | 311/780 [01:55<02:18,  3.40it/s] 40%|████      | 312/780 [01:56<02:17,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 21:05:42,416 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:05:42,417 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 21:05:42,417 >>   Batch size = 8
{'eval_loss': 0.9846031665802002, 'eval_runtime': 9.413, 'eval_samples_per_second': 371.508, 'eval_steps_per_second': 46.531, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.25it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.73it/s][A
  4%|▍         | 18/438 [00:00<00:08, 49.01it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.30it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.78it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.44it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.25it/s][A
 10%|▉         | 43/438 [00:00<00:08, 47.01it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.87it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.75it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.91it/s][A
 14%|█▍        | 63/438 [00:01<00:07, 46.93it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.89it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.97it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.94it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.69it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.65it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.63it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.66it/s][A
 24%|██▎       | 103/438 [00:02<00:09, 35.88it/s][A
 25%|██▍       | 108/438 [00:02<00:08, 38.56it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 40.84it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 42.59it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 43.75it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.67it/s][A
 30%|███       | 133/438 [00:02<00:06, 45.39it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 45.88it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 45.59it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 45.75it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.07it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.29it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.54it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.67it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.69it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.77it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.82it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.67it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.59it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.51it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.61it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.68it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.84it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.90it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.79it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.85it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 46.74it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.66it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.41it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.50it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.61it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.76it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.78it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.93it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.73it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 46.75it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.67it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.62it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.74it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.81it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.77it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.80it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.73it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.68it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.82it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.80it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.72it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.87it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.86it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.67it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.82it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.73it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.74it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.82it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.81it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.77it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.76it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.76it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.77it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.81it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.70it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.79it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.88it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.74it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.77it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.79it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.75it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.85it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.85it/s][A 40%|████      | 312/780 [02:05<02:17,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:05:51,980 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 21:05:52,035 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:05:57,122 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:05:57,152 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:05:57,162 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:21<1:00:21,  7.75s/it] 40%|████      | 314/780 [02:21<42:55,  5.53s/it]   40%|████      | 315/780 [02:21<30:39,  3.96s/it] 41%|████      | 316/780 [02:22<22:05,  2.86s/it] 41%|████      | 317/780 [02:22<16:05,  2.09s/it] 41%|████      | 318/780 [02:22<11:54,  1.55s/it] 41%|████      | 319/780 [02:23<08:58,  1.17s/it] 41%|████      | 320/780 [02:23<06:56,  1.11it/s] 41%|████      | 321/780 [02:23<05:30,  1.39it/s] 41%|████▏     | 322/780 [02:23<04:30,  1.69it/s] 41%|████▏     | 323/780 [02:24<03:48,  2.00it/s] 42%|████▏     | 324/780 [02:24<03:18,  2.29it/s] 42%|████▏     | 325/780 [02:24<02:58,  2.55it/s] 42%|████▏     | 326/780 [02:25<02:43,  2.77it/s] 42%|████▏     | 327/780 [02:25<02:33,  2.95it/s] 42%|████▏     | 328/780 [02:25<02:26,  3.09it/s] 42%|████▏     | 329/780 [02:25<02:21,  3.19it/s] 42%|████▏     | 330/780 [02:26<02:19,  3.24it/s] 42%|████▏     | 331/780 [02:26<02:15,  3.30it/s] 43%|████▎     | 332/780 [02:26<02:13,  3.35it/s] 43%|████▎     | 333/780 [02:27<02:12,  3.38it/s] 43%|████▎     | 334/780 [02:27<02:10,  3.41it/s] 43%|████▎     | 335/780 [02:27<02:09,  3.43it/s] 43%|████▎     | 336/780 [02:27<02:09,  3.44it/s] 43%|████▎     | 337/780 [02:28<02:08,  3.44it/s] 43%|████▎     | 338/780 [02:28<02:08,  3.45it/s] 43%|████▎     | 339/780 [02:28<02:07,  3.46it/s] 44%|████▎     | 340/780 [02:29<02:07,  3.46it/s] 44%|████▎     | 341/780 [02:29<02:07,  3.44it/s] 44%|████▍     | 342/780 [02:29<02:07,  3.45it/s] 44%|████▍     | 343/780 [02:30<02:06,  3.45it/s] 44%|████▍     | 344/780 [02:30<02:06,  3.46it/s] 44%|████▍     | 345/780 [02:30<02:05,  3.46it/s] 44%|████▍     | 346/780 [02:30<02:05,  3.46it/s] 44%|████▍     | 347/780 [02:31<02:04,  3.47it/s] 45%|████▍     | 348/780 [02:31<02:04,  3.47it/s] 45%|████▍     | 349/780 [02:31<02:04,  3.46it/s] 45%|████▍     | 350/780 [02:32<02:04,  3.46it/s] 45%|████▌     | 351/780 [02:32<02:03,  3.46it/s] 45%|████▌     | 352/780 [02:32<02:05,  3.41it/s] 45%|████▌     | 353/780 [02:32<02:04,  3.43it/s] 45%|████▌     | 354/780 [02:33<02:03,  3.44it/s] 46%|████▌     | 355/780 [02:33<02:03,  3.45it/s] 46%|████▌     | 356/780 [02:33<02:02,  3.46it/s] 46%|████▌     | 357/780 [02:34<02:02,  3.46it/s] 46%|████▌     | 358/780 [02:34<02:01,  3.46it/s] 46%|████▌     | 359/780 [02:34<02:01,  3.46it/s] 46%|████▌     | 360/780 [02:34<02:01,  3.46it/s] 46%|████▋     | 361/780 [02:35<02:00,  3.46it/s] 46%|████▋     | 362/780 [02:35<02:00,  3.46it/s] 47%|████▋     | 363/780 [02:35<02:03,  3.37it/s] 47%|████▋     | 364/780 [02:36<02:02,  3.40it/s] 47%|████▋     | 365/780 [02:36<02:01,  3.42it/s] 47%|████▋     | 366/780 [02:36<02:00,  3.43it/s] 47%|████▋     | 367/780 [02:36<02:00,  3.44it/s] 47%|████▋     | 368/780 [02:37<01:59,  3.44it/s] 47%|████▋     | 369/780 [02:37<01:59,  3.45it/s] 47%|████▋     | 370/780 [02:37<01:58,  3.46it/s] 48%|████▊     | 371/780 [02:38<01:58,  3.46it/s] 48%|████▊     | 372/780 [02:38<01:57,  3.46it/s] 48%|████▊     | 373/780 [02:38<01:57,  3.46it/s] 48%|████▊     | 374/780 [02:39<01:57,  3.45it/s] 48%|████▊     | 375/780 [02:39<01:57,  3.45it/s] 48%|████▊     | 376/780 [02:39<01:56,  3.46it/s] 48%|████▊     | 377/780 [02:39<01:56,  3.46it/s] 48%|████▊     | 378/780 [02:40<01:56,  3.46it/s] 49%|████▊     | 379/780 [02:40<01:55,  3.46it/s] 49%|████▊     | 380/780 [02:40<01:55,  3.46it/s] 49%|████▉     | 381/780 [02:41<01:55,  3.46it/s] 49%|████▉     | 382/780 [02:41<01:54,  3.46it/s] 49%|████▉     | 383/780 [02:41<01:54,  3.46it/s] 49%|████▉     | 384/780 [02:41<01:54,  3.46it/s] 49%|████▉     | 385/780 [02:42<01:54,  3.44it/s] 49%|████▉     | 386/780 [02:42<01:54,  3.45it/s] 50%|████▉     | 387/780 [02:42<01:53,  3.45it/s] 50%|████▉     | 388/780 [02:43<01:53,  3.45it/s] 50%|████▉     | 389/780 [02:43<01:53,  3.45it/s] 50%|█████     | 390/780 [02:43<01:52,  3.46it/s] 50%|█████     | 391/780 [02:43<01:52,  3.46it/s] 50%|█████     | 392/780 [02:44<01:52,  3.46it/s] 50%|█████     | 393/780 [02:44<01:51,  3.46it/s] 51%|█████     | 394/780 [02:44<01:51,  3.46it/s] 51%|█████     | 395/780 [02:45<01:51,  3.46it/s] 51%|█████     | 396/780 [02:45<01:51,  3.44it/s] 51%|█████     | 397/780 [02:45<01:51,  3.44it/s] 51%|█████     | 398/780 [02:45<01:50,  3.45it/s] 51%|█████     | 399/780 [02:46<01:50,  3.45it/s] 51%|█████▏    | 400/780 [02:46<01:50,  3.45it/s] 51%|█████▏    | 401/780 [02:46<01:49,  3.46it/s] 52%|█████▏    | 402/780 [02:47<01:49,  3.46it/s] 52%|█████▏    | 403/780 [02:47<01:48,  3.46it/s] 52%|█████▏    | 404/780 [02:47<01:48,  3.46it/s] 52%|█████▏    | 405/780 [02:47<01:48,  3.45it/s] 52%|█████▏    | 406/780 [02:48<01:48,  3.45it/s] 52%|█████▏    | 407/780 [02:48<02:08,  2.89it/s] 52%|█████▏    | 408/780 [02:49<02:02,  3.04it/s] 52%|█████▏    | 409/780 [02:49<01:57,  3.16it/s] 53%|█████▎    | 410/780 [02:49<01:56,  3.19it/s] 53%|█████▎    | 411/780 [02:49<01:53,  3.26it/s] 53%|█████▎    | 412/780 [02:50<01:50,  3.32it/s] 53%|█████▎    | 413/780 [02:50<01:49,  3.36it/s] 53%|█████▎    | 414/780 [02:50<01:47,  3.39it/s] 53%|█████▎    | 415/780 [02:51<01:46,  3.41it/s] 53%|█████▎    | 416/780 [02:51<01:46,  3.43it/s] 53%|█████▎    | 417/780 [02:51<01:46,  3.42it/s] 54%|█████▎    | 418/780 [02:51<01:45,  3.43it/s] 54%|█████▎    | 419/780 [02:52<01:44,  3.44it/s] 54%|█████▍    | 420/780 [02:52<01:44,  3.45it/s] 54%|█████▍    | 421/780 [02:52<01:44,  3.45it/s] 54%|█████▍    | 422/780 [02:53<01:43,  3.45it/s] 54%|█████▍    | 423/780 [02:53<01:43,  3.46it/s] 54%|█████▍    | 424/780 [02:53<01:42,  3.46it/s] 54%|█████▍    | 425/780 [02:53<01:42,  3.46it/s] 55%|█████▍    | 426/780 [02:54<01:42,  3.46it/s] 55%|█████▍    | 427/780 [02:54<01:42,  3.46it/s] 55%|█████▍    | 428/780 [02:54<01:41,  3.46it/s] 55%|█████▌    | 429/780 [02:55<01:41,  3.46it/s] 55%|█████▌    | 430/780 [02:55<01:41,  3.46it/s] 55%|█████▌    | 431/780 [02:55<01:40,  3.46it/s] 55%|█████▌    | 432/780 [02:55<01:40,  3.46it/s] 56%|█████▌    | 433/780 [02:56<01:40,  3.46it/s] 56%|█████▌    | 434/780 [02:56<01:40,  3.46it/s] 56%|█████▌    | 435/780 [02:56<01:42,  3.38it/s] 56%|█████▌    | 436/780 [02:57<01:41,  3.40it/s] 56%|█████▌    | 437/780 [02:57<01:40,  3.42it/s] 56%|█████▌    | 438/780 [02:57<01:39,  3.43it/s] 56%|█████▋    | 439/780 [02:58<01:39,  3.44it/s] 56%|█████▋    | 440/780 [02:58<01:38,  3.44it/s] 57%|█████▋    | 441/780 [02:58<01:38,  3.45it/s] 57%|█████▋    | 442/780 [02:58<01:37,  3.45it/s] 57%|█████▋    | 443/780 [02:59<01:37,  3.46it/s] 57%|█████▋    | 444/780 [02:59<01:37,  3.46it/s] 57%|█████▋    | 445/780 [02:59<01:36,  3.46it/s] 57%|█████▋    | 446/780 [03:00<01:36,  3.46it/s] 57%|█████▋    | 447/780 [03:00<01:36,  3.46it/s] 57%|█████▋    | 448/780 [03:00<01:36,  3.46it/s] 58%|█████▊    | 449/780 [03:00<01:35,  3.46it/s] 58%|█████▊    | 450/780 [03:01<01:35,  3.46it/s] 58%|█████▊    | 451/780 [03:01<01:35,  3.46it/s] 58%|█████▊    | 452/780 [03:01<01:34,  3.46it/s] 58%|█████▊    | 453/780 [03:02<01:35,  3.43it/s] 58%|█████▊    | 454/780 [03:02<01:34,  3.44it/s] 58%|█████▊    | 455/780 [03:02<01:34,  3.45it/s] 58%|█████▊    | 456/780 [03:02<01:33,  3.45it/s] 59%|█████▊    | 457/780 [03:03<01:33,  3.45it/s] 59%|█████▊    | 458/780 [03:03<01:33,  3.45it/s] 59%|█████▉    | 459/780 [03:03<01:32,  3.46it/s] 59%|█████▉    | 460/780 [03:04<01:32,  3.46it/s] 59%|█████▉    | 461/780 [03:04<01:32,  3.46it/s] 59%|█████▉    | 462/780 [03:04<01:31,  3.46it/s] 59%|█████▉    | 463/780 [03:04<01:31,  3.46it/s] 59%|█████▉    | 464/780 [03:05<01:31,  3.46it/s] 60%|█████▉    | 465/780 [03:05<01:31,  3.46it/s] 60%|█████▉    | 466/780 [03:05<01:30,  3.46it/s] 60%|█████▉    | 467/780 [03:06<01:30,  3.46it/s] 60%|██████    | 468/780 [03:06<01:30,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 21:06:52,701 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:06:52,701 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 21:06:52,701 >>   Batch size = 8
{'eval_loss': 0.9946061372756958, 'eval_runtime': 9.5051, 'eval_samples_per_second': 367.909, 'eval_steps_per_second': 46.081, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.02it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.57it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.92it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.24it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.63it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.47it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.33it/s][A
 10%|▉         | 43/438 [00:00<00:08, 47.01it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.91it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.87it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.80it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.84it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.93it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.91it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.92it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.90it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.85it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.84it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.80it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.81it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.89it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.66it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 45.37it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 45.83it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.16it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.43it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.62it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.70it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.80it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.53it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.53it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.64it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.73it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.82it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.86it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.88it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.92it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.95it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.78it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.76it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.72it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.76it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.84it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.93it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.90it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.93it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.90it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.84it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.75it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.73it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.66it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.76it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.87it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.84it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.94it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.92it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.80it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.80it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.86it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.72it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.74it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.85it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.85it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.89it/s][A
 75%|███████▍  | 328/438 [00:06<00:02, 46.94it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.83it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.86it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.76it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.74it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.72it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.69it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.80it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.85it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.85it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.91it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.92it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.80it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.78it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.74it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.75it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.65it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.81it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.75it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.84it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.87it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.84it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.95it/s][A                                                 
                                                 [A 60%|██████    | 468/780 [03:15<01:30,  3.46it/s]
100%|██████████| 438/438 [00:09<00:00, 46.95it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:07:02,150 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 21:07:02,187 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:07:07,873 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:07:07,931 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:07:07,945 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:30<38:42,  7.47s/it] 60%|██████    | 470/780 [03:30<27:32,  5.33s/it] 60%|██████    | 471/780 [03:31<19:40,  3.82s/it] 61%|██████    | 472/780 [03:31<14:09,  2.76s/it] 61%|██████    | 473/780 [03:31<10:19,  2.02s/it] 61%|██████    | 474/780 [03:32<07:38,  1.50s/it] 61%|██████    | 475/780 [03:32<05:46,  1.14s/it] 61%|██████    | 476/780 [03:32<04:27,  1.13it/s] 61%|██████    | 477/780 [03:33<03:33,  1.42it/s] 61%|██████▏   | 478/780 [03:33<02:54,  1.73it/s] 61%|██████▏   | 479/780 [03:33<02:28,  2.03it/s] 62%|██████▏   | 480/780 [03:33<02:09,  2.32it/s] 62%|██████▏   | 481/780 [03:34<01:56,  2.56it/s] 62%|██████▏   | 482/780 [03:34<01:47,  2.78it/s] 62%|██████▏   | 483/780 [03:34<01:40,  2.95it/s] 62%|██████▏   | 484/780 [03:35<01:35,  3.09it/s] 62%|██████▏   | 485/780 [03:35<01:32,  3.20it/s] 62%|██████▏   | 486/780 [03:35<01:29,  3.27it/s] 62%|██████▏   | 487/780 [03:35<01:28,  3.33it/s] 63%|██████▎   | 488/780 [03:36<01:26,  3.37it/s] 63%|██████▎   | 489/780 [03:36<01:25,  3.40it/s] 63%|██████▎   | 490/780 [03:36<01:25,  3.41it/s] 63%|██████▎   | 491/780 [03:37<01:24,  3.42it/s] 63%|██████▎   | 492/780 [03:37<01:25,  3.38it/s] 63%|██████▎   | 493/780 [03:37<01:24,  3.41it/s] 63%|██████▎   | 494/780 [03:37<01:23,  3.42it/s] 63%|██████▎   | 495/780 [03:38<01:22,  3.44it/s] 64%|██████▎   | 496/780 [03:38<01:22,  3.45it/s] 64%|██████▎   | 497/780 [03:38<01:22,  3.45it/s] 64%|██████▍   | 498/780 [03:39<01:21,  3.46it/s] 64%|██████▍   | 499/780 [03:39<01:21,  3.46it/s] 64%|██████▍   | 500/780 [03:39<01:20,  3.46it/s]                                                  64%|██████▍   | 500/780 [03:39<01:20,  3.46it/s] 64%|██████▍   | 501/780 [03:39<01:20,  3.46it/s] 64%|██████▍   | 502/780 [03:40<01:20,  3.47it/s] 64%|██████▍   | 503/780 [03:40<01:20,  3.44it/s] 65%|██████▍   | 504/780 [03:40<01:19,  3.45it/s] 65%|██████▍   | 505/780 [03:41<01:19,  3.46it/s] 65%|██████▍   | 506/780 [03:41<01:19,  3.46it/s] 65%|██████▌   | 507/780 [03:41<01:18,  3.46it/s] 65%|██████▌   | 508/780 [03:41<01:18,  3.46it/s] 65%|██████▌   | 509/780 [03:42<01:18,  3.46it/s] 65%|██████▌   | 510/780 [03:42<01:17,  3.46it/s] 66%|██████▌   | 511/780 [03:42<01:17,  3.47it/s] 66%|██████▌   | 512/780 [03:43<01:17,  3.46it/s] 66%|██████▌   | 513/780 [03:43<01:17,  3.47it/s] 66%|██████▌   | 514/780 [03:43<01:20,  3.29it/s] 66%|██████▌   | 515/780 [03:44<01:19,  3.34it/s] 66%|██████▌   | 516/780 [03:44<01:18,  3.38it/s] 66%|██████▋   | 517/780 [03:44<01:17,  3.41it/s] 66%|██████▋   | 518/780 [03:44<01:16,  3.42it/s] 67%|██████▋   | 519/780 [03:45<01:15,  3.44it/s] 67%|██████▋   | 520/780 [03:45<01:15,  3.45it/s] 67%|██████▋   | 521/780 [03:45<01:15,  3.45it/s] 67%|██████▋   | 522/780 [03:46<01:14,  3.46it/s] 67%|██████▋   | 523/780 [03:46<01:14,  3.46it/s] 67%|██████▋   | 524/780 [03:46<01:13,  3.46it/s] 67%|██████▋   | 525/780 [03:46<01:13,  3.46it/s] 67%|██████▋   | 526/780 [03:47<01:13,  3.46it/s] 68%|██████▊   | 527/780 [03:47<01:13,  3.46it/s] 68%|██████▊   | 528/780 [03:47<01:12,  3.46it/s] 68%|██████▊   | 529/780 [03:48<01:14,  3.37it/s] 68%|██████▊   | 530/780 [03:48<01:13,  3.40it/s] 68%|██████▊   | 531/780 [03:48<01:12,  3.42it/s] 68%|██████▊   | 532/780 [03:48<01:12,  3.43it/s] 68%|██████▊   | 533/780 [03:49<01:11,  3.44it/s] 68%|██████▊   | 534/780 [03:49<01:11,  3.45it/s] 69%|██████▊   | 535/780 [03:49<01:15,  3.24it/s] 69%|██████▊   | 536/780 [03:50<01:14,  3.29it/s] 69%|██████▉   | 537/780 [03:50<01:12,  3.34it/s] 69%|██████▉   | 538/780 [03:50<01:11,  3.38it/s] 69%|██████▉   | 539/780 [03:51<01:10,  3.40it/s] 69%|██████▉   | 540/780 [03:51<01:10,  3.40it/s] 69%|██████▉   | 541/780 [03:51<01:09,  3.42it/s] 69%|██████▉   | 542/780 [03:51<01:09,  3.43it/s] 70%|██████▉   | 543/780 [03:52<01:08,  3.44it/s] 70%|██████▉   | 544/780 [03:52<01:08,  3.45it/s] 70%|██████▉   | 545/780 [03:52<01:08,  3.45it/s] 70%|███████   | 546/780 [03:53<01:07,  3.45it/s] 70%|███████   | 547/780 [03:53<01:07,  3.46it/s] 70%|███████   | 548/780 [03:53<01:07,  3.46it/s] 70%|███████   | 549/780 [03:53<01:06,  3.46it/s] 71%|███████   | 550/780 [03:54<01:06,  3.46it/s] 71%|███████   | 551/780 [03:54<01:06,  3.43it/s] 71%|███████   | 552/780 [03:54<01:06,  3.44it/s] 71%|███████   | 553/780 [03:55<01:05,  3.45it/s] 71%|███████   | 554/780 [03:55<01:05,  3.45it/s] 71%|███████   | 555/780 [03:55<01:05,  3.45it/s] 71%|███████▏  | 556/780 [03:55<01:04,  3.46it/s] 71%|███████▏  | 557/780 [03:56<01:04,  3.46it/s] 72%|███████▏  | 558/780 [03:56<01:04,  3.46it/s] 72%|███████▏  | 559/780 [03:56<01:03,  3.46it/s] 72%|███████▏  | 560/780 [03:57<01:03,  3.46it/s] 72%|███████▏  | 561/780 [03:57<01:03,  3.46it/s] 72%|███████▏  | 562/780 [03:57<01:03,  3.46it/s] 72%|███████▏  | 563/780 [03:58<01:02,  3.46it/s] 72%|███████▏  | 564/780 [03:58<01:02,  3.45it/s] 72%|███████▏  | 565/780 [03:58<01:02,  3.46it/s] 73%|███████▎  | 566/780 [03:58<01:01,  3.46it/s] 73%|███████▎  | 567/780 [03:59<01:01,  3.46it/s] 73%|███████▎  | 568/780 [03:59<01:01,  3.46it/s] 73%|███████▎  | 569/780 [03:59<01:00,  3.46it/s] 73%|███████▎  | 570/780 [04:00<01:00,  3.46it/s] 73%|███████▎  | 571/780 [04:00<01:00,  3.46it/s] 73%|███████▎  | 572/780 [04:00<01:00,  3.46it/s] 73%|███████▎  | 573/780 [04:00<00:59,  3.46it/s] 74%|███████▎  | 574/780 [04:01<00:59,  3.46it/s] 74%|███████▎  | 575/780 [04:01<00:59,  3.46it/s] 74%|███████▍  | 576/780 [04:01<00:58,  3.46it/s] 74%|███████▍  | 577/780 [04:02<00:58,  3.46it/s] 74%|███████▍  | 578/780 [04:02<00:58,  3.46it/s] 74%|███████▍  | 579/780 [04:02<00:58,  3.46it/s] 74%|███████▍  | 580/780 [04:02<00:57,  3.46it/s] 74%|███████▍  | 581/780 [04:03<00:57,  3.47it/s] 75%|███████▍  | 582/780 [04:03<01:00,  3.29it/s] 75%|███████▍  | 583/780 [04:03<00:58,  3.34it/s] 75%|███████▍  | 584/780 [04:04<00:58,  3.38it/s] 75%|███████▌  | 585/780 [04:04<00:57,  3.40it/s] 75%|███████▌  | 586/780 [04:04<00:56,  3.42it/s] 75%|███████▌  | 587/780 [04:05<00:56,  3.43it/s] 75%|███████▌  | 588/780 [04:05<00:55,  3.44it/s] 76%|███████▌  | 589/780 [04:05<00:55,  3.45it/s] 76%|███████▌  | 590/780 [04:05<00:55,  3.45it/s] 76%|███████▌  | 591/780 [04:06<00:54,  3.45it/s] 76%|███████▌  | 592/780 [04:06<00:54,  3.46it/s] 76%|███████▌  | 593/780 [04:06<00:54,  3.46it/s] 76%|███████▌  | 594/780 [04:07<00:53,  3.46it/s] 76%|███████▋  | 595/780 [04:07<00:53,  3.46it/s] 76%|███████▋  | 596/780 [04:07<00:53,  3.46it/s] 77%|███████▋  | 597/780 [04:07<00:52,  3.46it/s] 77%|███████▋  | 598/780 [04:08<00:52,  3.46it/s] 77%|███████▋  | 599/780 [04:08<00:52,  3.45it/s] 77%|███████▋  | 600/780 [04:08<00:52,  3.45it/s] 77%|███████▋  | 601/780 [04:09<00:51,  3.45it/s] 77%|███████▋  | 602/780 [04:09<00:51,  3.46it/s] 77%|███████▋  | 603/780 [04:09<00:51,  3.46it/s] 77%|███████▋  | 604/780 [04:09<00:50,  3.46it/s] 78%|███████▊  | 605/780 [04:10<00:50,  3.46it/s] 78%|███████▊  | 606/780 [04:10<00:50,  3.46it/s] 78%|███████▊  | 607/780 [04:10<00:50,  3.46it/s] 78%|███████▊  | 608/780 [04:11<00:49,  3.46it/s] 78%|███████▊  | 609/780 [04:11<00:49,  3.46it/s] 78%|███████▊  | 610/780 [04:11<00:49,  3.46it/s] 78%|███████▊  | 611/780 [04:11<00:48,  3.46it/s] 78%|███████▊  | 612/780 [04:12<00:48,  3.46it/s] 79%|███████▊  | 613/780 [04:12<00:48,  3.46it/s] 79%|███████▊  | 614/780 [04:12<00:47,  3.46it/s] 79%|███████▉  | 615/780 [04:13<00:47,  3.46it/s] 79%|███████▉  | 616/780 [04:13<00:47,  3.46it/s] 79%|███████▉  | 617/780 [04:13<00:47,  3.45it/s] 79%|███████▉  | 618/780 [04:13<00:46,  3.45it/s] 79%|███████▉  | 619/780 [04:14<00:46,  3.46it/s] 79%|███████▉  | 620/780 [04:14<00:46,  3.46it/s] 80%|███████▉  | 621/780 [04:14<00:45,  3.46it/s] 80%|███████▉  | 622/780 [04:15<00:45,  3.46it/s] 80%|███████▉  | 623/780 [04:15<00:45,  3.46it/s] 80%|████████  | 624/780 [04:15<00:45,  3.46it/s][INFO|trainer.py:2140] 2023-08-28 21:08:01,977 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:08:01,977 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 21:08:01,977 >>   Batch size = 8
{'eval_loss': 1.002126932144165, 'eval_runtime': 9.4003, 'eval_samples_per_second': 372.008, 'eval_steps_per_second': 46.594, 'epoch': 3.0}
{'loss': 0.6349, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.53it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.81it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.94it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.28it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.74it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.39it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.21it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.84it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.85it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.88it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.86it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.87it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.93it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.80it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.93it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.86it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.75it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.75it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.77it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.84it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.78it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.88it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.81it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.82it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.62it/s][A
 30%|███       | 133/438 [00:02<00:06, 45.39it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 45.83it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.15it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.38it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.53it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.65it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.67it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.59it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.46it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.71it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.80it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.81it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.87it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.86it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.85it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.83it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.74it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.67it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.67it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.80it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.85it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.84it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.88it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.87it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.87it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.78it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.70it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.79it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.81it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.89it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.76it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.81it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.90it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.83it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.85it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.88it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.83it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.75it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.84it/s][A
 75%|███████▍  | 328/438 [00:06<00:02, 46.78it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.82it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.92it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.83it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.78it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.77it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.77it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.77it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 42.17it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 43.46it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 44.50it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 45.24it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 45.72it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.08it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.43it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.53it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.39it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.37it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.42it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.62it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.69it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.72it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.90it/s][A                                                 
                                                 [A 80%|████████  | 624/780 [04:25<00:45,  3.46it/s]
100%|██████████| 438/438 [00:09<00:00, 46.90it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:08:11,552 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-28 21:08:11,687 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:08:15,370 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:08:15,570 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:08:15,659 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:40<19:38,  7.61s/it] 80%|████████  | 626/780 [04:40<13:54,  5.42s/it] 80%|████████  | 627/780 [04:40<09:53,  3.88s/it] 81%|████████  | 628/780 [04:41<07:05,  2.80s/it] 81%|████████  | 629/780 [04:41<05:09,  2.05s/it] 81%|████████  | 630/780 [04:41<03:47,  1.52s/it] 81%|████████  | 631/780 [04:42<02:51,  1.15s/it] 81%|████████  | 632/780 [04:42<02:11,  1.12it/s] 81%|████████  | 633/780 [04:42<01:44,  1.41it/s] 81%|████████▏ | 634/780 [04:42<01:25,  1.71it/s] 81%|████████▏ | 635/780 [04:43<01:11,  2.02it/s] 82%|████████▏ | 636/780 [04:43<01:02,  2.31it/s] 82%|████████▏ | 637/780 [04:43<00:55,  2.56it/s] 82%|████████▏ | 638/780 [04:44<00:51,  2.78it/s] 82%|████████▏ | 639/780 [04:44<00:47,  2.95it/s] 82%|████████▏ | 640/780 [04:44<00:45,  3.09it/s] 82%|████████▏ | 641/780 [04:45<00:43,  3.19it/s] 82%|████████▏ | 642/780 [04:45<00:42,  3.27it/s] 82%|████████▏ | 643/780 [04:45<00:41,  3.33it/s] 83%|████████▎ | 644/780 [04:45<00:40,  3.37it/s] 83%|████████▎ | 645/780 [04:46<00:39,  3.40it/s] 83%|████████▎ | 646/780 [04:46<00:39,  3.42it/s] 83%|████████▎ | 647/780 [04:46<00:38,  3.43it/s] 83%|████████▎ | 648/780 [04:47<00:38,  3.43it/s] 83%|████████▎ | 649/780 [04:47<00:38,  3.44it/s] 83%|████████▎ | 650/780 [04:47<00:37,  3.45it/s] 83%|████████▎ | 651/780 [04:47<00:37,  3.45it/s] 84%|████████▎ | 652/780 [04:48<00:37,  3.44it/s] 84%|████████▎ | 653/780 [04:48<00:36,  3.44it/s] 84%|████████▍ | 654/780 [04:48<00:36,  3.45it/s] 84%|████████▍ | 655/780 [04:49<00:36,  3.46it/s] 84%|████████▍ | 656/780 [04:49<00:35,  3.46it/s] 84%|████████▍ | 657/780 [04:49<00:35,  3.46it/s] 84%|████████▍ | 658/780 [04:50<00:53,  2.28it/s] 84%|████████▍ | 659/780 [04:50<00:48,  2.49it/s] 85%|████████▍ | 660/780 [04:51<00:44,  2.72it/s] 85%|████████▍ | 661/780 [04:51<00:40,  2.91it/s] 85%|████████▍ | 662/780 [04:51<00:38,  3.06it/s] 85%|████████▌ | 663/780 [04:51<00:36,  3.17it/s] 85%|████████▌ | 664/780 [04:52<00:35,  3.25it/s] 85%|████████▌ | 665/780 [04:52<00:34,  3.31it/s] 85%|████████▌ | 666/780 [04:52<00:34,  3.35it/s] 86%|████████▌ | 667/780 [04:53<00:33,  3.39it/s] 86%|████████▌ | 668/780 [04:53<00:32,  3.41it/s] 86%|████████▌ | 669/780 [04:53<00:32,  3.43it/s] 86%|████████▌ | 670/780 [04:53<00:32,  3.42it/s] 86%|████████▌ | 671/780 [04:54<00:31,  3.43it/s] 86%|████████▌ | 672/780 [04:54<00:31,  3.44it/s] 86%|████████▋ | 673/780 [04:54<00:31,  3.45it/s] 86%|████████▋ | 674/780 [04:55<00:30,  3.46it/s] 87%|████████▋ | 675/780 [04:55<00:30,  3.46it/s] 87%|████████▋ | 676/780 [04:55<00:30,  3.46it/s] 87%|████████▋ | 677/780 [04:55<00:29,  3.46it/s] 87%|████████▋ | 678/780 [04:56<00:29,  3.46it/s] 87%|████████▋ | 679/780 [04:56<00:29,  3.46it/s] 87%|████████▋ | 680/780 [04:56<00:28,  3.46it/s] 87%|████████▋ | 681/780 [04:57<00:28,  3.46it/s] 87%|████████▋ | 682/780 [04:57<00:28,  3.46it/s] 88%|████████▊ | 683/780 [04:57<00:28,  3.46it/s] 88%|████████▊ | 684/780 [04:57<00:27,  3.46it/s] 88%|████████▊ | 685/780 [04:58<00:27,  3.46it/s] 88%|████████▊ | 686/780 [04:58<00:27,  3.46it/s] 88%|████████▊ | 687/780 [04:58<00:26,  3.46it/s] 88%|████████▊ | 688/780 [04:59<00:26,  3.46it/s] 88%|████████▊ | 689/780 [04:59<00:26,  3.47it/s] 88%|████████▊ | 690/780 [04:59<00:26,  3.45it/s] 89%|████████▊ | 691/780 [04:59<00:25,  3.46it/s] 89%|████████▊ | 692/780 [05:00<00:25,  3.46it/s] 89%|████████▉ | 693/780 [05:00<00:25,  3.46it/s] 89%|████████▉ | 694/780 [05:00<00:24,  3.47it/s] 89%|████████▉ | 695/780 [05:01<00:24,  3.46it/s] 89%|████████▉ | 696/780 [05:01<00:24,  3.47it/s] 89%|████████▉ | 697/780 [05:01<00:23,  3.47it/s] 89%|████████▉ | 698/780 [05:02<00:23,  3.47it/s] 90%|████████▉ | 699/780 [05:02<00:23,  3.46it/s] 90%|████████▉ | 700/780 [05:02<00:23,  3.46it/s] 90%|████████▉ | 701/780 [05:02<00:22,  3.45it/s] 90%|█████████ | 702/780 [05:03<00:22,  3.46it/s] 90%|█████████ | 703/780 [05:03<00:22,  3.46it/s] 90%|█████████ | 704/780 [05:03<00:21,  3.46it/s] 90%|█████████ | 705/780 [05:04<00:21,  3.46it/s] 91%|█████████ | 706/780 [05:04<00:21,  3.46it/s] 91%|█████████ | 707/780 [05:04<00:21,  3.46it/s] 91%|█████████ | 708/780 [05:04<00:20,  3.46it/s] 91%|█████████ | 709/780 [05:05<00:20,  3.46it/s] 91%|█████████ | 710/780 [05:05<00:20,  3.46it/s] 91%|█████████ | 711/780 [05:05<00:19,  3.46it/s] 91%|█████████▏| 712/780 [05:06<00:19,  3.44it/s] 91%|█████████▏| 713/780 [05:06<00:19,  3.45it/s] 92%|█████████▏| 714/780 [05:06<00:19,  3.45it/s] 92%|█████████▏| 715/780 [05:06<00:18,  3.46it/s] 92%|█████████▏| 716/780 [05:07<00:18,  3.46it/s] 92%|█████████▏| 717/780 [05:07<00:18,  3.46it/s] 92%|█████████▏| 718/780 [05:07<00:17,  3.46it/s] 92%|█████████▏| 719/780 [05:08<00:17,  3.46it/s] 92%|█████████▏| 720/780 [05:08<00:17,  3.46it/s] 92%|█████████▏| 721/780 [05:08<00:17,  3.46it/s] 93%|█████████▎| 722/780 [05:08<00:16,  3.46it/s] 93%|█████████▎| 723/780 [05:09<00:16,  3.43it/s] 93%|█████████▎| 724/780 [05:09<00:16,  3.44it/s] 93%|█████████▎| 725/780 [05:09<00:15,  3.44it/s] 93%|█████████▎| 726/780 [05:10<00:15,  3.45it/s] 93%|█████████▎| 727/780 [05:10<00:15,  3.45it/s] 93%|█████████▎| 728/780 [05:10<00:15,  3.46it/s] 93%|█████████▎| 729/780 [05:10<00:14,  3.46it/s] 94%|█████████▎| 730/780 [05:11<00:14,  3.46it/s] 94%|█████████▎| 731/780 [05:11<00:14,  3.46it/s] 94%|█████████▍| 732/780 [05:11<00:13,  3.46it/s] 94%|█████████▍| 733/780 [05:12<00:13,  3.46it/s] 94%|█████████▍| 734/780 [05:12<00:13,  3.45it/s] 94%|█████████▍| 735/780 [05:12<00:13,  3.45it/s] 94%|█████████▍| 736/780 [05:13<00:12,  3.45it/s] 94%|█████████▍| 737/780 [05:13<00:12,  3.45it/s] 95%|█████████▍| 738/780 [05:13<00:12,  3.46it/s] 95%|█████████▍| 739/780 [05:13<00:11,  3.46it/s] 95%|█████████▍| 740/780 [05:14<00:11,  3.46it/s] 95%|█████████▌| 741/780 [05:14<00:11,  3.46it/s] 95%|█████████▌| 742/780 [05:14<00:10,  3.46it/s] 95%|█████████▌| 743/780 [05:15<00:10,  3.46it/s] 95%|█████████▌| 744/780 [05:15<00:10,  3.46it/s] 96%|█████████▌| 745/780 [05:15<00:10,  3.35it/s] 96%|█████████▌| 746/780 [05:15<00:10,  3.37it/s] 96%|█████████▌| 747/780 [05:16<00:09,  3.40it/s] 96%|█████████▌| 748/780 [05:16<00:09,  3.42it/s] 96%|█████████▌| 749/780 [05:16<00:09,  3.43it/s] 96%|█████████▌| 750/780 [05:17<00:08,  3.44it/s] 96%|█████████▋| 751/780 [05:17<00:08,  3.44it/s] 96%|█████████▋| 752/780 [05:17<00:08,  3.45it/s] 97%|█████████▋| 753/780 [05:17<00:07,  3.45it/s] 97%|█████████▋| 754/780 [05:18<00:07,  3.46it/s] 97%|█████████▋| 755/780 [05:18<00:07,  3.46it/s] 97%|█████████▋| 756/780 [05:18<00:07,  3.42it/s] 97%|█████████▋| 757/780 [05:19<00:06,  3.43it/s] 97%|█████████▋| 758/780 [05:19<00:06,  3.44it/s] 97%|█████████▋| 759/780 [05:19<00:06,  3.45it/s] 97%|█████████▋| 760/780 [05:19<00:05,  3.45it/s] 98%|█████████▊| 761/780 [05:20<00:05,  3.45it/s] 98%|█████████▊| 762/780 [05:20<00:05,  3.46it/s] 98%|█████████▊| 763/780 [05:20<00:04,  3.46it/s] 98%|█████████▊| 764/780 [05:21<00:04,  3.46it/s] 98%|█████████▊| 765/780 [05:21<00:04,  3.46it/s] 98%|█████████▊| 766/780 [05:21<00:04,  3.46it/s] 98%|█████████▊| 767/780 [05:22<00:03,  3.39it/s] 98%|█████████▊| 768/780 [05:22<00:03,  3.41it/s] 99%|█████████▊| 769/780 [05:22<00:03,  3.42it/s] 99%|█████████▊| 770/780 [05:22<00:02,  3.43it/s] 99%|█████████▉| 771/780 [05:23<00:02,  3.44it/s] 99%|█████████▉| 772/780 [05:23<00:02,  3.45it/s] 99%|█████████▉| 773/780 [05:23<00:02,  3.45it/s] 99%|█████████▉| 774/780 [05:24<00:01,  3.45it/s] 99%|█████████▉| 775/780 [05:24<00:01,  3.46it/s] 99%|█████████▉| 776/780 [05:24<00:01,  3.46it/s]100%|█████████▉| 777/780 [05:24<00:00,  3.46it/s]100%|█████████▉| 778/780 [05:25<00:00,  3.40it/s]100%|█████████▉| 779/780 [05:25<00:00,  3.42it/s]100%|██████████| 780/780 [05:25<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 21:09:12,034 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:09:12,034 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 21:09:12,034 >>   Batch size = 8
{'eval_loss': 1.0151158571243286, 'eval_runtime': 9.4418, 'eval_samples_per_second': 370.375, 'eval_steps_per_second': 46.39, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.23it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.67it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.94it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.23it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.80it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.46it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.31it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.98it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.88it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.89it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.88it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.29it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.69it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.05it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.27it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.52it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.66it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.67it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.69it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.71it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.74it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.64it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.70it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.87it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.85it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.90it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.80it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.73it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.74it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.80it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.78it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.80it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.89it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.97it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.94it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.82it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.77it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.77it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.83it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.28it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.71it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.06it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.42it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.53it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.59it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.64it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.68it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.50it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.54it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.67it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.75it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.80it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.89it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.90it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.79it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.85it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.71it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.73it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.77it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.72it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.71it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.83it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.88it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.87it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.92it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.73it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.71it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.77it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.77it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.74it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.79it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.89it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.88it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.94it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.88it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.72it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.73it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.76it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.69it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.84it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.90it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.79it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.83it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.88it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.82it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.69it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.85it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.85it/s][A100%|██████████| 780/780 [05:35<00:00,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 21:09:21,568 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-28 21:09:21,635 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:09:27,184 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:09:27,203 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:09:27,212 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 21:09:39,056 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 21:09:39,060 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156 (score: 0.9846031665802002).
                                                 100%|██████████| 780/780 [05:57<00:00,  3.43it/s]100%|██████████| 780/780 [05:57<00:00,  2.18it/s]
[INFO|trainer.py:1894] 2023-08-28 21:09:43,667 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 21:09:43,701 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 21:09:49,748 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 21:09:49,790 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 21:09:49,809 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:09:49,998 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:49,998 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:49,998 >>   train_loss               =     0.6246
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:49,998 >>   train_runtime            = 0:05:57.42
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:49,998 >>   train_samples            =      10004
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:49,998 >>   train_samples_per_second =    139.945
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:49,998 >>   train_steps_per_second   =      2.182
{'eval_loss': 1.019753098487854, 'eval_runtime': 9.3724, 'eval_samples_per_second': 373.118, 'eval_steps_per_second': 46.733, 'epoch': 5.0}
{'train_runtime': 357.427, 'train_samples_per_second': 139.945, 'train_steps_per_second': 2.182, 'train_loss': 0.6245510003505609, 'epoch': 5.0}
08/28/2023 21:09:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 21:09:50,047 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 21:09:50,048 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 21:09:50,048 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 58.16it/s]  3%|▎         | 12/438 [00:00<00:08, 51.33it/s]  4%|▍         | 18/438 [00:00<00:08, 49.50it/s]  5%|▌         | 23/438 [00:00<00:08, 48.87it/s]  6%|▋         | 28/438 [00:00<00:08, 48.37it/s]  8%|▊         | 33/438 [00:00<00:08, 48.02it/s]  9%|▊         | 38/438 [00:00<00:08, 47.87it/s] 10%|▉         | 43/438 [00:00<00:08, 47.69it/s] 11%|█         | 48/438 [00:00<00:08, 47.47it/s] 12%|█▏        | 53/438 [00:01<00:08, 47.41it/s] 13%|█▎        | 58/438 [00:01<00:08, 47.24it/s] 14%|█▍        | 63/438 [00:01<00:07, 47.17it/s] 16%|█▌        | 68/438 [00:01<00:07, 47.33it/s] 17%|█▋        | 73/438 [00:01<00:07, 47.32it/s] 18%|█▊        | 78/438 [00:01<00:07, 47.41it/s] 19%|█▉        | 83/438 [00:01<00:07, 47.46it/s] 20%|██        | 88/438 [00:01<00:07, 47.30it/s] 21%|██        | 93/438 [00:01<00:07, 47.27it/s] 22%|██▏       | 98/438 [00:02<00:07, 47.24it/s] 24%|██▎       | 103/438 [00:02<00:07, 47.18it/s] 25%|██▍       | 108/438 [00:02<00:06, 47.17it/s] 26%|██▌       | 113/438 [00:02<00:06, 47.16it/s] 27%|██▋       | 118/438 [00:02<00:06, 47.12it/s] 28%|██▊       | 123/438 [00:02<00:06, 47.28it/s] 29%|██▉       | 128/438 [00:02<00:06, 47.39it/s] 30%|███       | 133/438 [00:02<00:06, 46.50it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.70it/s] 33%|███▎      | 143/438 [00:03<00:06, 46.88it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.89it/s] 35%|███▍      | 153/438 [00:03<00:06, 47.07it/s] 36%|███▌      | 158/438 [00:03<00:05, 47.03it/s] 37%|███▋      | 163/438 [00:03<00:05, 47.03it/s] 38%|███▊      | 168/438 [00:03<00:05, 47.10it/s] 39%|███▉      | 173/438 [00:03<00:05, 47.25it/s] 41%|████      | 178/438 [00:03<00:05, 47.18it/s] 42%|████▏     | 183/438 [00:03<00:05, 47.19it/s] 43%|████▎     | 188/438 [00:03<00:05, 47.17it/s] 44%|████▍     | 193/438 [00:04<00:05, 47.18it/s] 45%|████▌     | 198/438 [00:04<00:05, 47.19it/s] 46%|████▋     | 203/438 [00:04<00:04, 47.28it/s] 47%|████▋     | 208/438 [00:04<00:04, 47.18it/s] 49%|████▊     | 213/438 [00:04<00:04, 47.10it/s] 50%|████▉     | 218/438 [00:04<00:04, 47.16it/s] 51%|█████     | 223/438 [00:04<00:04, 47.14it/s] 52%|█████▏    | 228/438 [00:04<00:04, 47.16it/s] 53%|█████▎    | 233/438 [00:04<00:04, 47.16it/s] 54%|█████▍    | 238/438 [00:05<00:04, 47.16it/s] 55%|█████▌    | 243/438 [00:05<00:04, 47.17it/s] 57%|█████▋    | 248/438 [00:05<00:04, 47.24it/s] 58%|█████▊    | 253/438 [00:05<00:03, 47.14it/s] 59%|█████▉    | 258/438 [00:05<00:03, 47.20it/s] 60%|██████    | 263/438 [00:05<00:03, 47.20it/s] 61%|██████    | 268/438 [00:05<00:03, 47.12it/s] 62%|██████▏   | 273/438 [00:05<00:03, 47.07it/s] 63%|██████▎   | 278/438 [00:05<00:03, 46.40it/s] 65%|██████▍   | 283/438 [00:05<00:03, 46.60it/s] 66%|██████▌   | 288/438 [00:06<00:03, 46.88it/s] 67%|██████▋   | 293/438 [00:06<00:03, 46.93it/s] 68%|██████▊   | 298/438 [00:06<00:02, 47.00it/s] 69%|██████▉   | 303/438 [00:06<00:02, 47.02it/s] 70%|███████   | 308/438 [00:06<00:02, 47.04it/s] 71%|███████▏  | 313/438 [00:06<00:02, 47.13it/s] 73%|███████▎  | 318/438 [00:06<00:02, 47.09it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.99it/s] 75%|███████▍  | 328/438 [00:06<00:02, 47.07it/s] 76%|███████▌  | 333/438 [00:07<00:02, 47.05it/s] 77%|███████▋  | 338/438 [00:07<00:02, 47.06it/s] 78%|███████▊  | 343/438 [00:07<00:02, 47.17it/s] 79%|███████▉  | 348/438 [00:07<00:01, 47.13it/s] 81%|████████  | 353/438 [00:07<00:01, 47.14it/s] 82%|████████▏ | 358/438 [00:07<00:01, 47.12it/s] 83%|████████▎ | 363/438 [00:07<00:01, 47.16it/s] 84%|████████▍ | 368/438 [00:07<00:01, 47.09it/s] 85%|████████▌ | 373/438 [00:07<00:01, 46.96it/s] 86%|████████▋ | 378/438 [00:07<00:01, 47.13it/s] 87%|████████▋ | 383/438 [00:08<00:01, 47.14it/s] 89%|████████▊ | 388/438 [00:08<00:01, 47.04it/s] 90%|████████▉ | 393/438 [00:08<00:00, 47.15it/s] 91%|█████████ | 398/438 [00:08<00:00, 47.14it/s] 92%|█████████▏| 403/438 [00:08<00:00, 47.01it/s] 93%|█████████▎| 408/438 [00:08<00:00, 47.01it/s] 94%|█████████▍| 413/438 [00:08<00:00, 47.08it/s] 95%|█████████▌| 418/438 [00:08<00:00, 47.09it/s] 97%|█████████▋| 423/438 [00:08<00:00, 45.90it/s] 98%|█████████▊| 428/438 [00:09<00:00, 46.27it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.56it/s]100%|██████████| 438/438 [00:09<00:00, 46.86it/s]100%|██████████| 438/438 [00:09<00:00, 47.21it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 21:09:59,347 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:59,348 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:59,348 >>   eval_loss               =     0.9846
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:59,348 >>   eval_runtime            = 0:00:09.29
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:59,348 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:59,348 >>   eval_samples_per_second =    376.032
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:59,348 >>   eval_steps_per_second   =     47.098
[INFO|trainer_pt_utils.py:913] 2023-08-28 21:09:59,348 >>   perplexity              =     2.6767
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:05,732 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:05,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:05,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:05,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:05,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:10:06,109 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:10:06,110 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:10:06,411 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:10:07,582 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:10:07,582 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:10,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:10,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:10,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:10,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:10:10,217 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:10:10,576 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:10:10,577 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:10:10,844 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:10:11,054 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:10:11,054 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.62it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.52it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.54it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.56it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.55it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:17,  1.63it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:18,  1.62it/s]Extractor Predicting: 30it [00:19,  1.63it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:20,  1.60it/s]Extractor Predicting: 33it [00:21,  1.61it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:26,  1.53it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:29,  1.58it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:31,  1.42it/s]Extractor Predicting: 50it [00:32,  1.42it/s]Extractor Predicting: 51it [00:32,  1.44it/s]Extractor Predicting: 52it [00:33,  1.43it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:34,  1.48it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:36,  1.50it/s]Extractor Predicting: 58it [00:37,  1.50it/s]Extractor Predicting: 59it [00:38,  1.51it/s]Extractor Predicting: 60it [00:38,  1.48it/s]Extractor Predicting: 61it [00:39,  1.45it/s]Extractor Predicting: 62it [00:40,  1.43it/s]Extractor Predicting: 63it [00:41,  1.43it/s]Extractor Predicting: 64it [00:41,  1.43it/s]Extractor Predicting: 65it [00:42,  1.42it/s]Extractor Predicting: 66it [00:43,  1.40it/s]Extractor Predicting: 67it [00:44,  1.39it/s]Extractor Predicting: 68it [00:44,  1.40it/s]Extractor Predicting: 69it [00:45,  1.40it/s]Extractor Predicting: 70it [00:46,  1.40it/s]Extractor Predicting: 71it [00:46,  1.39it/s]Extractor Predicting: 72it [00:47,  1.39it/s]Extractor Predicting: 73it [00:48,  1.38it/s]Extractor Predicting: 74it [00:49,  1.39it/s]Extractor Predicting: 75it [00:49,  1.38it/s]Extractor Predicting: 76it [00:50,  1.40it/s]Extractor Predicting: 77it [00:51,  1.40it/s]Extractor Predicting: 78it [00:51,  1.39it/s]Extractor Predicting: 79it [00:52,  1.42it/s]Extractor Predicting: 80it [00:53,  1.39it/s]Extractor Predicting: 81it [00:54,  1.36it/s]Extractor Predicting: 82it [00:54,  1.37it/s]Extractor Predicting: 83it [00:55,  1.41it/s]Extractor Predicting: 84it [00:56,  1.42it/s]Extractor Predicting: 85it [00:56,  1.43it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:58,  1.40it/s]Extractor Predicting: 88it [00:59,  1.41it/s]Extractor Predicting: 89it [00:59,  1.44it/s]Extractor Predicting: 90it [01:00,  1.46it/s]Extractor Predicting: 91it [01:01,  1.46it/s]Extractor Predicting: 92it [01:01,  1.44it/s]Extractor Predicting: 93it [01:02,  1.48it/s]Extractor Predicting: 94it [01:03,  1.49it/s]Extractor Predicting: 95it [01:03,  1.48it/s]Extractor Predicting: 96it [01:04,  1.47it/s]Extractor Predicting: 97it [01:05,  1.48it/s]Extractor Predicting: 98it [01:05,  1.47it/s]Extractor Predicting: 99it [01:06,  1.49it/s]Extractor Predicting: 100it [01:07,  1.46it/s]Extractor Predicting: 101it [01:07,  1.47it/s]Extractor Predicting: 102it [01:08,  1.46it/s]Extractor Predicting: 103it [01:09,  1.42it/s]Extractor Predicting: 104it [01:09,  1.45it/s]Extractor Predicting: 105it [01:10,  1.45it/s]Extractor Predicting: 106it [01:11,  1.46it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:13,  1.45it/s]Extractor Predicting: 110it [01:14,  1.43it/s]Extractor Predicting: 111it [01:14,  1.44it/s]Extractor Predicting: 112it [01:15,  1.44it/s]Extractor Predicting: 113it [01:16,  1.45it/s]Extractor Predicting: 114it [01:16,  1.45it/s]Extractor Predicting: 115it [01:17,  1.44it/s]Extractor Predicting: 116it [01:18,  1.44it/s]Extractor Predicting: 117it [01:18,  1.48it/s]Extractor Predicting: 118it [01:19,  1.50it/s]Extractor Predicting: 119it [01:20,  1.55it/s]Extractor Predicting: 120it [01:20,  1.56it/s]Extractor Predicting: 121it [01:21,  1.54it/s]Extractor Predicting: 122it [01:21,  1.59it/s]Extractor Predicting: 123it [01:22,  1.61it/s]Extractor Predicting: 124it [01:23,  1.59it/s]Extractor Predicting: 125it [01:23,  1.59it/s]Extractor Predicting: 126it [01:24,  1.62it/s]Extractor Predicting: 127it [01:25,  1.65it/s]Extractor Predicting: 128it [01:25,  1.64it/s]Extractor Predicting: 129it [01:26,  1.66it/s]Extractor Predicting: 130it [01:26,  1.61it/s]Extractor Predicting: 131it [01:27,  1.60it/s]Extractor Predicting: 132it [01:28,  1.51it/s]Extractor Predicting: 133it [01:28,  1.56it/s]Extractor Predicting: 134it [01:29,  1.51it/s]Extractor Predicting: 135it [01:30,  1.55it/s]Extractor Predicting: 136it [01:30,  1.58it/s]Extractor Predicting: 137it [01:31,  1.60it/s]Extractor Predicting: 138it [01:31,  1.60it/s]Extractor Predicting: 139it [01:32,  1.61it/s]Extractor Predicting: 140it [01:33,  1.66it/s]Extractor Predicting: 141it [01:33,  1.66it/s]Extractor Predicting: 142it [01:34,  1.69it/s]Extractor Predicting: 143it [01:34,  1.66it/s]Extractor Predicting: 144it [01:35,  1.64it/s]Extractor Predicting: 145it [01:35,  1.82it/s]Extractor Predicting: 145it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:56,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:56,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:56,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:56,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:11:56,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:11:57,540 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:11:57,541 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:11:57,842 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:11:58,902 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:11:58,902 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:01,103 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:01,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:01,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:01,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:12:01,118 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:12:01,943 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:12:01,944 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:12:02,233 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:12:02,427 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:12:02,427 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.7170059093893631,
  "recall": 0.31226765799256506,
  "score": 0.4350597609561753,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.52it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:10,  1.53it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.48it/s]Extractor Predicting: 21it [00:14,  1.50it/s]Extractor Predicting: 22it [00:14,  1.45it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:16,  1.50it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:19,  1.46it/s]Extractor Predicting: 30it [00:20,  1.43it/s]Extractor Predicting: 31it [00:20,  1.44it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.48it/s]Extractor Predicting: 36it [00:24,  1.50it/s]Extractor Predicting: 37it [00:24,  1.56it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.54it/s]Extractor Predicting: 40it [00:27,  1.41it/s]Extractor Predicting: 41it [00:27,  1.44it/s]Extractor Predicting: 42it [00:28,  1.46it/s]Extractor Predicting: 43it [00:29,  1.47it/s]Extractor Predicting: 44it [00:29,  1.48it/s]Extractor Predicting: 45it [00:30,  1.47it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:31,  1.51it/s]Extractor Predicting: 48it [00:32,  1.50it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:34,  1.50it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:35,  1.51it/s]Extractor Predicting: 54it [00:36,  1.52it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:37,  1.50it/s]Extractor Predicting: 57it [00:38,  1.53it/s]Extractor Predicting: 58it [00:38,  1.55it/s]Extractor Predicting: 59it [00:39,  1.56it/s]Extractor Predicting: 60it [00:40,  1.56it/s]Extractor Predicting: 61it [00:40,  1.53it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.52it/s]Extractor Predicting: 64it [00:42,  1.52it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:44,  1.49it/s]Extractor Predicting: 67it [00:44,  1.52it/s]Extractor Predicting: 68it [00:45,  1.55it/s]Extractor Predicting: 69it [00:46,  1.57it/s]Extractor Predicting: 70it [00:46,  1.56it/s]Extractor Predicting: 71it [00:47,  1.53it/s]Extractor Predicting: 72it [00:48,  1.55it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:49,  1.50it/s]Extractor Predicting: 75it [00:50,  1.51it/s]Extractor Predicting: 76it [00:50,  1.53it/s]Extractor Predicting: 77it [00:51,  1.49it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:52,  1.46it/s]Extractor Predicting: 80it [00:53,  1.46it/s]Extractor Predicting: 81it [00:54,  1.48it/s]Extractor Predicting: 82it [00:54,  1.47it/s]Extractor Predicting: 83it [00:55,  1.48it/s]Extractor Predicting: 84it [00:56,  1.48it/s]Extractor Predicting: 85it [00:56,  1.51it/s]Extractor Predicting: 86it [00:57,  1.49it/s]Extractor Predicting: 87it [00:58,  1.49it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [01:00,  1.52it/s]Extractor Predicting: 91it [01:00,  1.51it/s]Extractor Predicting: 92it [01:01,  1.50it/s]Extractor Predicting: 93it [01:02,  1.48it/s]Extractor Predicting: 94it [01:02,  1.50it/s]Extractor Predicting: 95it [01:03,  1.49it/s]Extractor Predicting: 96it [01:04,  1.48it/s]Extractor Predicting: 97it [01:04,  1.51it/s]Extractor Predicting: 98it [01:05,  1.51it/s]Extractor Predicting: 99it [01:06,  1.52it/s]Extractor Predicting: 100it [01:06,  1.53it/s]Extractor Predicting: 101it [01:07,  1.55it/s]Extractor Predicting: 102it [01:08,  1.54it/s]Extractor Predicting: 103it [01:08,  1.53it/s]Extractor Predicting: 104it [01:09,  1.49it/s]Extractor Predicting: 105it [01:10,  1.48it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:12,  1.50it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:13,  1.50it/s]Extractor Predicting: 111it [01:14,  1.51it/s]Extractor Predicting: 112it [01:14,  1.48it/s]Extractor Predicting: 113it [01:15,  1.48it/s]Extractor Predicting: 114it [01:16,  1.46it/s]Extractor Predicting: 115it [01:16,  1.44it/s]Extractor Predicting: 116it [01:17,  1.43it/s]Extractor Predicting: 117it [01:18,  1.43it/s]Extractor Predicting: 118it [01:18,  1.45it/s]Extractor Predicting: 119it [01:19,  1.46it/s]Extractor Predicting: 120it [01:20,  1.47it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.52it/s]Extractor Predicting: 123it [01:22,  1.53it/s]Extractor Predicting: 124it [01:22,  1.52it/s]Extractor Predicting: 125it [01:23,  1.50it/s]Extractor Predicting: 126it [01:24,  1.48it/s]Extractor Predicting: 127it [01:24,  1.48it/s]Extractor Predicting: 128it [01:25,  1.48it/s]Extractor Predicting: 129it [01:26,  1.50it/s]Extractor Predicting: 130it [01:26,  1.50it/s]Extractor Predicting: 131it [01:27,  1.51it/s]Extractor Predicting: 132it [01:28,  1.53it/s]Extractor Predicting: 133it [01:28,  1.50it/s]Extractor Predicting: 134it [01:29,  1.49it/s]Extractor Predicting: 135it [01:30,  1.49it/s]Extractor Predicting: 136it [01:30,  1.48it/s]Extractor Predicting: 137it [01:31,  1.47it/s]Extractor Predicting: 138it [01:32,  1.35it/s]Extractor Predicting: 139it [01:33,  1.38it/s]Extractor Predicting: 140it [01:33,  1.37it/s]Extractor Predicting: 141it [01:34,  1.41it/s]Extractor Predicting: 142it [01:35,  1.44it/s]Extractor Predicting: 143it [01:35,  1.45it/s]Extractor Predicting: 144it [01:36,  1.44it/s]Extractor Predicting: 145it [01:37,  1.47it/s]Extractor Predicting: 146it [01:38,  1.47it/s]Extractor Predicting: 147it [01:38,  1.47it/s]Extractor Predicting: 148it [01:39,  1.46it/s]Extractor Predicting: 149it [01:40,  1.49it/s]Extractor Predicting: 150it [01:40,  1.50it/s]Extractor Predicting: 151it [01:41,  1.54it/s]Extractor Predicting: 152it [01:41,  1.56it/s]Extractor Predicting: 153it [01:42,  1.56it/s]Extractor Predicting: 154it [01:43,  1.57it/s]Extractor Predicting: 155it [01:43,  1.55it/s]Extractor Predicting: 156it [01:44,  1.55it/s]Extractor Predicting: 157it [01:45,  1.53it/s]Extractor Predicting: 158it [01:45,  1.49it/s]Extractor Predicting: 159it [01:46,  1.51it/s]Extractor Predicting: 160it [01:47,  1.55it/s]Extractor Predicting: 161it [01:47,  1.54it/s]Extractor Predicting: 162it [01:48,  1.55it/s]Extractor Predicting: 163it [01:49,  1.57it/s]Extractor Predicting: 164it [01:49,  1.56it/s]Extractor Predicting: 165it [01:50,  1.54it/s]Extractor Predicting: 166it [01:51,  1.54it/s]Extractor Predicting: 167it [01:51,  1.53it/s]Extractor Predicting: 168it [01:52,  1.54it/s]Extractor Predicting: 169it [01:52,  1.57it/s]Extractor Predicting: 170it [01:53,  1.58it/s]Extractor Predicting: 171it [01:54,  1.54it/s]Extractor Predicting: 172it [01:54,  1.53it/s]Extractor Predicting: 173it [01:55,  1.52it/s]Extractor Predicting: 174it [01:56,  1.55it/s]Extractor Predicting: 175it [01:56,  1.57it/s]Extractor Predicting: 176it [01:57,  1.58it/s]Extractor Predicting: 177it [01:58,  1.52it/s]Extractor Predicting: 178it [01:58,  1.55it/s]Extractor Predicting: 179it [01:59,  1.53it/s]Extractor Predicting: 180it [02:00,  1.53it/s]Extractor Predicting: 181it [02:00,  1.54it/s]Extractor Predicting: 182it [02:01,  1.50it/s]Extractor Predicting: 183it [02:02,  1.52it/s]Extractor Predicting: 184it [02:02,  1.53it/s]Extractor Predicting: 185it [02:03,  1.55it/s]Extractor Predicting: 186it [02:03,  1.58it/s]Extractor Predicting: 187it [02:04,  1.62it/s]Extractor Predicting: 188it [02:05,  1.59it/s]Extractor Predicting: 189it [02:05,  1.54it/s]Extractor Predicting: 190it [02:06,  1.54it/s]Extractor Predicting: 191it [02:07,  1.51it/s]Extractor Predicting: 192it [02:07,  1.54it/s]Extractor Predicting: 193it [02:08,  1.54it/s]Extractor Predicting: 194it [02:09,  1.56it/s]Extractor Predicting: 195it [02:09,  1.55it/s]Extractor Predicting: 196it [02:10,  1.56it/s]Extractor Predicting: 197it [02:11,  1.52it/s]Extractor Predicting: 198it [02:11,  1.53it/s]Extractor Predicting: 199it [02:12,  1.52it/s]Extractor Predicting: 200it [02:13,  1.53it/s]Extractor Predicting: 201it [02:13,  1.53it/s]Extractor Predicting: 202it [02:14,  1.52it/s]Extractor Predicting: 203it [02:15,  1.52it/s]Extractor Predicting: 204it [02:15,  1.49it/s]Extractor Predicting: 205it [02:16,  1.46it/s]Extractor Predicting: 206it [02:17,  1.50it/s]Extractor Predicting: 207it [02:17,  1.50it/s]Extractor Predicting: 208it [02:18,  1.54it/s]Extractor Predicting: 209it [02:18,  1.55it/s]Extractor Predicting: 210it [02:19,  1.53it/s]Extractor Predicting: 211it [02:20,  1.52it/s]Extractor Predicting: 212it [02:20,  1.51it/s]Extractor Predicting: 213it [02:21,  1.50it/s]Extractor Predicting: 214it [02:22,  1.50it/s]Extractor Predicting: 215it [02:23,  1.49it/s]Extractor Predicting: 216it [02:23,  1.50it/s]Extractor Predicting: 217it [02:24,  1.48it/s]Extractor Predicting: 218it [02:25,  1.46it/s]Extractor Predicting: 219it [02:25,  1.47it/s]Extractor Predicting: 220it [02:26,  1.49it/s]Extractor Predicting: 221it [02:26,  1.54it/s]Extractor Predicting: 222it [02:27,  1.48it/s]Extractor Predicting: 223it [02:28,  1.46it/s]Extractor Predicting: 224it [02:29,  1.51it/s]Extractor Predicting: 225it [02:29,  1.51it/s]Extractor Predicting: 226it [02:30,  1.49it/s]Extractor Predicting: 227it [02:31,  1.50it/s]Extractor Predicting: 228it [02:31,  1.51it/s]Extractor Predicting: 229it [02:32,  1.54it/s]Extractor Predicting: 230it [02:33,  1.49it/s]Extractor Predicting: 231it [02:33,  1.51it/s]Extractor Predicting: 232it [02:34,  1.54it/s]Extractor Predicting: 233it [02:35,  1.50it/s]Extractor Predicting: 234it [02:35,  1.32it/s]Extractor Predicting: 235it [02:36,  1.37it/s]Extractor Predicting: 236it [02:37,  1.41it/s]Extractor Predicting: 237it [02:38,  1.41it/s]Extractor Predicting: 238it [02:38,  1.43it/s]Extractor Predicting: 239it [02:39,  1.44it/s]Extractor Predicting: 240it [02:40,  1.39it/s]Extractor Predicting: 241it [02:40,  1.42it/s]Extractor Predicting: 242it [02:41,  1.46it/s]Extractor Predicting: 243it [02:42,  1.45it/s]Extractor Predicting: 244it [02:42,  1.47it/s]Extractor Predicting: 245it [02:43,  1.47it/s]Extractor Predicting: 246it [02:44,  1.48it/s]Extractor Predicting: 247it [02:44,  1.48it/s]Extractor Predicting: 248it [02:45,  1.47it/s]Extractor Predicting: 249it [02:46,  1.48it/s]Extractor Predicting: 250it [02:46,  1.48it/s]Extractor Predicting: 251it [02:47,  1.50it/s]Extractor Predicting: 252it [02:48,  1.48it/s]Extractor Predicting: 253it [02:48,  1.50it/s]Extractor Predicting: 254it [02:49,  1.50it/s]Extractor Predicting: 255it [02:50,  1.50it/s]Extractor Predicting: 256it [02:50,  1.50it/s]Extractor Predicting: 257it [02:51,  1.53it/s]Extractor Predicting: 258it [02:52,  1.53it/s]Extractor Predicting: 259it [02:52,  1.55it/s]Extractor Predicting: 260it [02:53,  1.53it/s]Extractor Predicting: 261it [02:54,  1.53it/s]Extractor Predicting: 262it [02:54,  1.53it/s]Extractor Predicting: 263it [02:55,  1.46it/s]Extractor Predicting: 264it [02:56,  1.45it/s]Extractor Predicting: 265it [02:56,  1.47it/s]Extractor Predicting: 266it [02:57,  1.48it/s]Extractor Predicting: 267it [02:58,  1.50it/s]Extractor Predicting: 268it [02:58,  1.43it/s]Extractor Predicting: 269it [02:59,  1.46it/s]Extractor Predicting: 270it [03:00,  1.47it/s]Extractor Predicting: 271it [03:00,  1.49it/s]Extractor Predicting: 272it [03:01,  1.54it/s]Extractor Predicting: 273it [03:02,  1.43it/s]Extractor Predicting: 274it [03:02,  1.48it/s]Extractor Predicting: 275it [03:03,  1.48it/s]Extractor Predicting: 276it [03:04,  1.48it/s]Extractor Predicting: 277it [03:04,  1.52it/s]Extractor Predicting: 278it [03:05,  1.40it/s]Extractor Predicting: 279it [03:06,  1.44it/s]Extractor Predicting: 280it [03:07,  1.45it/s]Extractor Predicting: 281it [03:07,  1.48it/s]Extractor Predicting: 282it [03:08,  1.48it/s]Extractor Predicting: 283it [03:09,  1.42it/s]Extractor Predicting: 284it [03:09,  1.46it/s]Extractor Predicting: 285it [03:10,  1.47it/s]Extractor Predicting: 286it [03:11,  1.47it/s]Extractor Predicting: 287it [03:11,  1.49it/s]Extractor Predicting: 288it [03:12,  1.42it/s]Extractor Predicting: 289it [03:13,  1.45it/s]Extractor Predicting: 290it [03:13,  1.47it/s]Extractor Predicting: 291it [03:14,  1.48it/s]Extractor Predicting: 292it [03:15,  1.51it/s]Extractor Predicting: 293it [03:16,  1.26it/s]Extractor Predicting: 294it [03:17,  1.32it/s]Extractor Predicting: 295it [03:17,  1.39it/s]Extractor Predicting: 296it [03:18,  1.43it/s]Extractor Predicting: 297it [03:19,  1.40it/s]Extractor Predicting: 298it [03:19,  1.46it/s]Extractor Predicting: 299it [03:20,  1.48it/s]Extractor Predicting: 300it [03:20,  1.48it/s]Extractor Predicting: 301it [03:21,  1.54it/s]Extractor Predicting: 302it [03:22,  1.44it/s]Extractor Predicting: 303it [03:23,  1.41it/s]Extractor Predicting: 304it [03:23,  1.44it/s]Extractor Predicting: 305it [03:24,  1.46it/s]Extractor Predicting: 306it [03:25,  1.49it/s]Extractor Predicting: 307it [03:25,  1.49it/s]Extractor Predicting: 308it [03:26,  1.43it/s]Extractor Predicting: 309it [03:27,  1.46it/s]Extractor Predicting: 310it [03:27,  1.46it/s]Extractor Predicting: 311it [03:28,  1.47it/s]Extractor Predicting: 312it [03:29,  1.44it/s]Extractor Predicting: 313it [03:29,  1.43it/s]Extractor Predicting: 314it [03:30,  1.45it/s]Extractor Predicting: 315it [03:31,  1.49it/s]Extractor Predicting: 316it [03:31,  1.51it/s]Extractor Predicting: 317it [03:32,  1.50it/s]Extractor Predicting: 318it [03:33,  1.48it/s]Extractor Predicting: 319it [03:33,  1.48it/s]Extractor Predicting: 320it [03:34,  1.49it/s]Extractor Predicting: 321it [03:35,  1.50it/s]Extractor Predicting: 322it [03:35,  1.49it/s]Extractor Predicting: 323it [03:36,  1.53it/s]Extractor Predicting: 324it [03:37,  1.50it/s]Extractor Predicting: 325it [03:37,  1.52it/s]Extractor Predicting: 326it [03:38,  1.52it/s]Extractor Predicting: 327it [03:39,  1.54it/s]Extractor Predicting: 328it [03:39,  1.54it/s]Extractor Predicting: 329it [03:40,  1.51it/s]Extractor Predicting: 330it [03:41,  1.50it/s]Extractor Predicting: 331it [03:41,  1.49it/s]Extractor Predicting: 332it [03:42,  1.48it/s]Extractor Predicting: 333it [03:43,  1.50it/s]Extractor Predicting: 334it [03:43,  1.49it/s]Extractor Predicting: 335it [03:44,  1.50it/s]Extractor Predicting: 336it [03:45,  1.53it/s]Extractor Predicting: 337it [03:45,  1.55it/s]Extractor Predicting: 338it [03:46,  1.53it/s]Extractor Predicting: 339it [03:47,  1.51it/s]Extractor Predicting: 340it [03:47,  1.52it/s]Extractor Predicting: 341it [03:48,  1.55it/s]Extractor Predicting: 342it [03:49,  1.34it/s]Extractor Predicting: 343it [03:50,  1.39it/s]Extractor Predicting: 344it [03:50,  1.42it/s]Extractor Predicting: 345it [03:51,  1.47it/s]Extractor Predicting: 346it [03:51,  1.48it/s]Extractor Predicting: 347it [03:52,  1.48it/s]Extractor Predicting: 348it [03:53,  1.49it/s]Extractor Predicting: 349it [03:54,  1.47it/s]Extractor Predicting: 350it [03:54,  1.48it/s]Extractor Predicting: 351it [03:55,  1.47it/s]Extractor Predicting: 352it [03:56,  1.45it/s]Extractor Predicting: 353it [03:56,  1.46it/s]Extractor Predicting: 354it [03:57,  1.46it/s]Extractor Predicting: 355it [03:58,  1.49it/s]Extractor Predicting: 356it [03:58,  1.49it/s]Extractor Predicting: 357it [03:59,  1.52it/s]Extractor Predicting: 358it [04:00,  1.51it/s]Extractor Predicting: 359it [04:00,  1.51it/s]Extractor Predicting: 360it [04:01,  1.50it/s]Extractor Predicting: 361it [04:02,  1.51it/s]Extractor Predicting: 362it [04:02,  1.52it/s]Extractor Predicting: 363it [04:03,  1.53it/s]Extractor Predicting: 364it [04:03,  1.54it/s]Extractor Predicting: 365it [04:04,  1.54it/s]Extractor Predicting: 366it [04:05,  1.53it/s]Extractor Predicting: 367it [04:05,  1.52it/s]Extractor Predicting: 368it [04:06,  1.52it/s]Extractor Predicting: 369it [04:07,  1.53it/s]Extractor Predicting: 370it [04:07,  1.56it/s]Extractor Predicting: 371it [04:08,  1.55it/s]Extractor Predicting: 372it [04:09,  1.55it/s]Extractor Predicting: 373it [04:10,  1.36it/s]Extractor Predicting: 374it [04:10,  1.40it/s]Extractor Predicting: 375it [04:11,  1.45it/s]Extractor Predicting: 376it [04:12,  1.48it/s]Extractor Predicting: 377it [04:12,  1.50it/s]Extractor Predicting: 378it [04:13,  1.46it/s]Extractor Predicting: 379it [04:14,  1.44it/s]Extractor Predicting: 380it [04:14,  1.47it/s]Extractor Predicting: 381it [04:15,  1.52it/s]Extractor Predicting: 382it [04:16,  1.53it/s]Extractor Predicting: 383it [04:16,  1.47it/s]Extractor Predicting: 384it [04:17,  1.49it/s]Extractor Predicting: 385it [04:18,  1.51it/s]Extractor Predicting: 386it [04:18,  1.56it/s]Extractor Predicting: 387it [04:19,  1.58it/s]Extractor Predicting: 388it [04:19,  1.56it/s]Extractor Predicting: 389it [04:20,  1.54it/s]Extractor Predicting: 390it [04:21,  1.56it/s]Extractor Predicting: 391it [04:21,  1.56it/s]Extractor Predicting: 392it [04:22,  1.57it/s]Extractor Predicting: 393it [04:23,  1.58it/s]Extractor Predicting: 394it [04:23,  1.61it/s]Extractor Predicting: 395it [04:24,  1.57it/s]Extractor Predicting: 396it [04:25,  1.57it/s]Extractor Predicting: 397it [04:25,  1.58it/s]Extractor Predicting: 398it [04:26,  1.58it/s]Extractor Predicting: 399it [04:26,  1.61it/s]Extractor Predicting: 400it [04:27,  1.64it/s]Extractor Predicting: 401it [04:28,  1.60it/s]Extractor Predicting: 402it [04:28,  1.61it/s]Extractor Predicting: 403it [04:29,  1.59it/s]Extractor Predicting: 404it [04:30,  1.57it/s]Extractor Predicting: 405it [04:30,  1.56it/s]Extractor Predicting: 406it [04:31,  1.56it/s]Extractor Predicting: 407it [04:31,  1.55it/s]Extractor Predicting: 408it [04:32,  1.56it/s]Extractor Predicting: 409it [04:33,  1.59it/s]Extractor Predicting: 409it [04:33,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:44,965 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:44,969 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:44,969 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:44,969 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:44,970 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:16:45,585 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:16:45,586 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:16:46,172 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:16:47,319 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:16:47,319 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:50,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:50,914 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:50,914 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:50,914 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:16:50,914 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:16:51,667 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:16:51,669 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:16:52,260 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:16:52,710 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:16:52,710 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.45906862745098037,
  "recall": 0.19086925506980537,
  "score": 0.2696321888720939,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.33it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:07,  1.33it/s]Extractor Predicting: 11it [00:07,  1.40it/s]Extractor Predicting: 12it [00:08,  1.40it/s]Extractor Predicting: 13it [00:09,  1.43it/s]Extractor Predicting: 13it [00:09,  1.42it/s]
[INFO|configuration_utils.py:515] 2023-08-28 21:17:08,119 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:17:08,279 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 21:17:08,414 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:17:08,415 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 21:17:08,462 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 21:17:24,840 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 21:17:25,396 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 21:17:25,973 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 21:17:25,975 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 21:17:26,083 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:17:26,152 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:17:26,152 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:17:26,152 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:17:26,152 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:17:26,152 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 21:17:26,152 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.24615384615384617,
  "recall": 0.023289665211062592,
  "score": 0.04255319148936171,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 21:17:26,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:27,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:28,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:29,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:30,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:30,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:31,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:32,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:33,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:33,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:34,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:35,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:36,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:36,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:37,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:38,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:39,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:40,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:40,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:41,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:50, 15.27s/it][WARNING|generation_utils.py:914] 2023-08-28 21:17:42,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:42,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:43,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:44,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:44,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:45,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:46,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:47,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:47,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:48,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:49,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:50,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:50,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:51,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:52,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:53,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:53,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:54,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:55,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:55,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:29<04:25, 14.73s/it][WARNING|generation_utils.py:914] 2023-08-28 21:17:56,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:57,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:58,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:58,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:17:59,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:00,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:01,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:01,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:02,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:03,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:04,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:05,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:05,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:06,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:07,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:08,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:09,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:09,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:10,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:11,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:12,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:45<04:23, 15.47s/it][WARNING|generation_utils.py:914] 2023-08-28 21:18:12,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:13,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:14,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:15,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:15,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:16,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:17,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:18,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:18,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:19,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:20,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:20,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:21,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:22,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:22,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:23,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:24,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:25,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:25,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:26,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:27,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:00<04:03, 15.25s/it][WARNING|generation_utils.py:914] 2023-08-28 21:18:28,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:28,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:29,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:30,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:30,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:31,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:32,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:33,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:33,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:34,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:34,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:35,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:36,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:37,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:37,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:38,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:39,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:40,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:40,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:41,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:42,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:42,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:43,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:44,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:44,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:18<04:02, 16.17s/it][WARNING|generation_utils.py:914] 2023-08-28 21:18:45,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:46,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:46,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:47,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:48,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:49,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:49,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:50,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:51,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:52,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:52,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:53,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:53,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:54,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:55,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:55,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:56,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:57,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:57,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:58,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:18:59,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:00,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:33<03:41, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-28 21:19:00,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:01,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:02,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:03,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:03,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:04,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:05,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:06,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:07,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:07,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:08,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:09,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:10,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:10,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:11,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:12,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:13,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:13,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:14,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:15,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:16,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:17,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:50<03:30, 16.20s/it][WARNING|generation_utils.py:914] 2023-08-28 21:19:17,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:18,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:19,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:20,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:20,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:21,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:22,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:23,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:24,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:24,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:25,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:26,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:27,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:28,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:29,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:29,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:31,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:31,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:32,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:33,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:34,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:34,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:35,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:36,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:10<03:26, 17.24s/it][WARNING|generation_utils.py:914] 2023-08-28 21:19:37,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:37,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:38,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:39,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:40,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:41,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:41,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:42,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:43,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:43,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:44,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:45,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:46,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:47,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:47,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:48,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:49,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:50,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:51,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:51,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:52,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:52,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:53,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:27<03:09, 17.26s/it][WARNING|generation_utils.py:914] 2023-08-28 21:19:54,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:55,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:55,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:56,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:57,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:58,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:59,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:19:59,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:00,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:01,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:01,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:02,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:03,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:04,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:05,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:06,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:06,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:07,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:08,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:09,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:09,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:43<02:48, 16.86s/it][WARNING|generation_utils.py:914] 2023-08-28 21:20:10,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:11,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:11,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:12,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:13,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:13,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:14,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:15,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:16,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:16,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:17,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:18,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:18,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:19,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:20,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:20,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:21,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:21,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:22,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:23,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:23,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:24,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:58<02:26, 16.23s/it][WARNING|generation_utils.py:914] 2023-08-28 21:20:25,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:26,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:26,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:27,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:28,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:28,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:29,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:30,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:31,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:31,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:32,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:33,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:33,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:34,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:35,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:36,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:36,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:37,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:38,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:38,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:39,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:40,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:14<02:08, 16.08s/it][WARNING|generation_utils.py:914] 2023-08-28 21:20:41,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:41,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:42,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:43,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:43,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:44,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:44,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:45,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:46,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:46,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:47,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:48,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:48,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:49,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:50,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:51,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:51,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:52,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:52,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:53,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:27<01:46, 15.20s/it][WARNING|generation_utils.py:914] 2023-08-28 21:20:54,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:54,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:55,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:56,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:56,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:57,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:58,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:59,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:20:59,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:00,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:01,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:01,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:02,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:03,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:04,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:05,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:05,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:06,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:07,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:08,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:41<01:30, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-28 21:21:08,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:09,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:10,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:11,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:11,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:12,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:13,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:14,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:14,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:15,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:16,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:17,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:18,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:18,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:19,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:20,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:20,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:21,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:22,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:23,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:24,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:57<01:16, 15.30s/it][WARNING|generation_utils.py:914] 2023-08-28 21:21:24,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:25,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:26,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:27,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:27,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:28,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:28,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:29,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:30,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:31,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:32,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:32,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:33,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:34,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:35,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:35,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:36,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:37,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:37,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:38,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:39,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:39,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:13<01:02, 15.53s/it][WARNING|generation_utils.py:914] 2023-08-28 21:21:40,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:41,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:42,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:42,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:43,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:44,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:44,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:45,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:45,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:46,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:47,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:47,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:48,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:49,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:50,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:50,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:51,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:51,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:52,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:53,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:53,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:27<00:45, 15.01s/it][WARNING|generation_utils.py:914] 2023-08-28 21:21:54,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:55,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:56,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:56,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:57,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:58,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:58,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:21:59,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:00,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:00,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:01,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:02,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:03,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:03,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:04,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:05,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:05,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:06,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:07,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:07,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:41<00:29, 14.73s/it][WARNING|generation_utils.py:914] 2023-08-28 21:22:08,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:09,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:10,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:11,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:12,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:13,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:13,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:14,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:15,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:15,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:16,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:17,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:18,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:18,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:19,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:20,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:21,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:22,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:22,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:23,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:24,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:24,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:58<00:15, 15.44s/it][WARNING|generation_utils.py:914] 2023-08-28 21:22:25,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:26,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:27,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:28,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:28,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:29,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:30,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:31,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:31,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:32,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:33,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:34,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:34,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:35,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:36,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:37,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:37,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:38,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:39,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:40,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 21:22:41,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:14<00:00, 15.63s/it]Generating: 100%|██████████| 20/20 [05:14<00:00, 15.75s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:49,483 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:49,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:49,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:49,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:49,489 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:22:50,134 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:22:50,135 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:22:50,707 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:22:51,765 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:22:51,765 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:54,760 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:54,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:54,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:54,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:22:54,774 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:22:55,403 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:22:55,404 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:22:55,996 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:22:56,153 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:22:56,153 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.9375, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 570, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : military branch .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9255952380952381, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9181547619047619, 'errors': {''}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 376, 'raw': 480}
{'target': 600, 'success': 401, 'raw': 512}
{'target': 600, 'success': 424, 'raw': 544}
{'target': 600, 'success': 448, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 497, 'raw': 640}
{'target': 600, 'success': 521, 'raw': 672}
{'target': 600, 'success': 541, 'raw': 704}
{'target': 600, 'success': 568, 'raw': 736}
{'target': 600, 'success': 591, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : voice type .', 'success_rate': 0.77, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 502, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 556, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8721590909090909, 'errors': {''}}
['Relation : field of work . Context : Later in the year , the University of New Brunswick partnered with the Canada Institute of Hydrogeography to study polar ice caps from a borehole located offshore north of Péri , Quebec . Head Entity : N. New Brunswick , Tail Entity : University of New Brunswick .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8764204545454546, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : language of work or name . Context : Later in life he studied at the University of Copenhagen , where he received a doctorate in composition and had two PhD degrees from the same department . Head Entity : Erik Erik Erik , Tail Entity : Danish .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 176, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 254, 'raw': 320}
{'target': 600, 'success': 282, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 496, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 547, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', 'too many values to unpack (expected 2)', '(\'saxophone\', \'language of work or name\', \'\\n\', \'His father used the term " \\n in poetry and has also played the saxophone in other songs .\')'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 182, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 236, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8355978260869565, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9270833333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', '(\'JX-99C\', \'manufacturer\', \'\', \'It is a successor to the " JX-99C " , which replaced the previous JX -97C .\')'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 577, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8607954545454546, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9515625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 464, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : position held .', 'success_rate': 0.8835227272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.9270833333333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 312, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.965625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : religion .', 'success_rate': 0.8877840909090909, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.9241071428571429, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 12242
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12342, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.52it/s]Extractor Estimating: 2it [00:01,  1.38it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.48it/s]Extractor Estimating: 5it [00:03,  1.57it/s]Extractor Estimating: 6it [00:04,  1.45it/s]Extractor Estimating: 7it [00:04,  1.50it/s]Extractor Estimating: 8it [00:05,  1.57it/s]Extractor Estimating: 9it [00:05,  1.55it/s]Extractor Estimating: 10it [00:06,  1.54it/s]Extractor Estimating: 11it [00:07,  1.51it/s]Extractor Estimating: 12it [00:07,  1.56it/s]Extractor Estimating: 13it [00:08,  1.57it/s]Extractor Estimating: 14it [00:09,  1.58it/s]Extractor Estimating: 15it [00:09,  1.60it/s]Extractor Estimating: 16it [00:10,  1.58it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:11,  1.61it/s]Extractor Estimating: 19it [00:12,  1.65it/s]Extractor Estimating: 20it [00:12,  1.68it/s]Extractor Estimating: 21it [00:13,  1.69it/s]Extractor Estimating: 22it [00:14,  1.65it/s]Extractor Estimating: 23it [00:14,  1.62it/s]Extractor Estimating: 24it [00:15,  1.61it/s]Extractor Estimating: 25it [00:15,  1.69it/s]Extractor Estimating: 26it [00:16,  1.67it/s]Extractor Estimating: 27it [00:17,  1.66it/s]Extractor Estimating: 28it [00:17,  1.64it/s]Extractor Estimating: 29it [00:18,  1.66it/s]Extractor Estimating: 30it [00:18,  1.65it/s]Extractor Estimating: 31it [00:19,  1.64it/s]Extractor Estimating: 32it [00:20,  1.51it/s]Extractor Estimating: 33it [00:20,  1.56it/s]Extractor Estimating: 34it [00:21,  1.62it/s]Extractor Estimating: 35it [00:21,  1.65it/s]Extractor Estimating: 36it [00:22,  1.59it/s]Extractor Estimating: 37it [00:23,  1.61it/s]Extractor Estimating: 38it [00:23,  1.59it/s]Extractor Estimating: 39it [00:24,  1.56it/s]Extractor Estimating: 40it [00:25,  1.62it/s]Extractor Estimating: 41it [00:25,  1.64it/s]Extractor Estimating: 42it [00:26,  1.60it/s]Extractor Estimating: 43it [00:27,  1.61it/s]Extractor Estimating: 44it [00:27,  1.66it/s]Extractor Estimating: 45it [00:28,  1.67it/s]Extractor Estimating: 46it [00:28,  1.68it/s]Extractor Estimating: 47it [00:29,  1.66it/s]Extractor Estimating: 48it [00:29,  1.69it/s]Extractor Estimating: 49it [00:30,  1.70it/s]Extractor Estimating: 50it [00:31,  1.70it/s]Extractor Estimating: 51it [00:31,  1.71it/s]Extractor Estimating: 52it [00:32,  1.75it/s]Extractor Estimating: 53it [00:32,  1.72it/s]Extractor Estimating: 54it [00:33,  1.81it/s]Extractor Estimating: 55it [00:33,  1.78it/s]Extractor Estimating: 56it [00:34,  1.76it/s]Extractor Estimating: 57it [00:35,  1.72it/s]Extractor Estimating: 58it [00:35,  1.76it/s]Extractor Estimating: 59it [00:36,  1.79it/s]Extractor Estimating: 60it [00:36,  1.77it/s]Extractor Estimating: 61it [00:37,  1.77it/s]Extractor Estimating: 62it [00:37,  1.78it/s]Extractor Estimating: 63it [00:38,  1.76it/s]Extractor Estimating: 64it [00:39,  1.76it/s]Extractor Estimating: 65it [00:39,  1.78it/s]Extractor Estimating: 66it [00:40,  1.72it/s]Extractor Estimating: 67it [00:40,  1.66it/s]Extractor Estimating: 68it [00:41,  1.70it/s]Extractor Estimating: 69it [00:41,  1.72it/s]Extractor Estimating: 70it [00:42,  1.75it/s]Extractor Estimating: 71it [00:43,  1.78it/s]Extractor Estimating: 72it [00:43,  1.78it/s]Extractor Estimating: 73it [00:44,  1.79it/s]Extractor Estimating: 74it [00:44,  1.82it/s]Extractor Estimating: 75it [00:45,  1.77it/s]Extractor Estimating: 76it [00:45,  1.72it/s]Extractor Estimating: 77it [00:46,  1.68it/s]Extractor Estimating: 78it [00:47,  1.66it/s]Extractor Estimating: 79it [00:47,  1.64it/s]Extractor Estimating: 80it [00:48,  1.62it/s]Extractor Estimating: 81it [00:49,  1.55it/s]Extractor Estimating: 82it [00:49,  1.54it/s]Extractor Estimating: 83it [00:50,  1.57it/s]Extractor Estimating: 84it [00:51,  1.58it/s]Extractor Estimating: 85it [00:51,  1.53it/s]Extractor Estimating: 86it [00:52,  1.58it/s]Extractor Estimating: 87it [00:52,  1.58it/s]Extractor Estimating: 88it [00:53,  1.57it/s]Extractor Estimating: 89it [00:54,  1.60it/s]Extractor Estimating: 90it [00:54,  1.60it/s]Extractor Estimating: 91it [00:55,  1.60it/s]Extractor Estimating: 92it [00:56,  1.58it/s]Extractor Estimating: 93it [00:56,  1.58it/s]Extractor Estimating: 94it [00:57,  1.61it/s]Extractor Estimating: 95it [00:57,  1.58it/s]Extractor Estimating: 96it [00:58,  1.57it/s]Extractor Estimating: 97it [00:59,  1.58it/s]Extractor Estimating: 98it [00:59,  1.59it/s]Extractor Estimating: 99it [01:00,  1.60it/s]Extractor Estimating: 100it [01:01,  1.56it/s]Extractor Estimating: 101it [01:01,  1.56it/s]Extractor Estimating: 102it [01:02,  1.46it/s]Extractor Estimating: 103it [01:03,  1.48it/s]Extractor Estimating: 104it [01:03,  1.49it/s]Extractor Estimating: 105it [01:04,  1.51it/s]Extractor Estimating: 106it [01:05,  1.53it/s]Extractor Estimating: 107it [01:05,  1.54it/s]Extractor Estimating: 108it [01:06,  1.58it/s]Extractor Estimating: 109it [01:06,  1.62it/s]Extractor Estimating: 110it [01:07,  1.62it/s]Extractor Estimating: 111it [01:08,  1.60it/s]Extractor Estimating: 112it [01:08,  1.64it/s]Extractor Estimating: 113it [01:09,  1.59it/s]Extractor Estimating: 114it [01:10,  1.61it/s]Extractor Estimating: 115it [01:10,  1.61it/s]Extractor Estimating: 116it [01:11,  1.62it/s]Extractor Estimating: 117it [01:11,  1.66it/s]Extractor Estimating: 118it [01:12,  1.68it/s]Extractor Estimating: 119it [01:13,  1.67it/s]Extractor Estimating: 120it [01:13,  1.70it/s]Extractor Estimating: 121it [01:14,  1.70it/s]Extractor Estimating: 122it [01:14,  1.66it/s]Extractor Estimating: 123it [01:15,  1.54it/s]Extractor Estimating: 124it [01:16,  1.55it/s]Extractor Estimating: 125it [01:16,  1.54it/s]Extractor Estimating: 126it [01:17,  1.63it/s]Extractor Estimating: 127it [01:18,  1.67it/s]Extractor Estimating: 128it [01:18,  1.64it/s]Extractor Estimating: 129it [01:19,  1.69it/s]Extractor Estimating: 130it [01:19,  1.73it/s]Extractor Estimating: 131it [01:20,  1.73it/s]Extractor Estimating: 132it [01:20,  1.79it/s]Extractor Estimating: 133it [01:21,  1.78it/s]Extractor Estimating: 134it [01:22,  1.65it/s]Extractor Estimating: 135it [01:22,  1.67it/s]Extractor Estimating: 136it [01:23,  1.75it/s]Extractor Estimating: 137it [01:23,  1.80it/s]Extractor Estimating: 138it [01:24,  1.77it/s]Extractor Estimating: 139it [01:24,  1.77it/s]Extractor Estimating: 140it [01:25,  1.81it/s]Extractor Estimating: 141it [01:25,  1.81it/s]Extractor Estimating: 142it [01:26,  1.81it/s]Extractor Estimating: 143it [01:27,  1.82it/s]Extractor Estimating: 144it [01:27,  1.83it/s]Extractor Estimating: 145it [01:28,  1.73it/s]Extractor Estimating: 146it [01:28,  1.74it/s]Extractor Estimating: 147it [01:29,  1.72it/s]Extractor Estimating: 148it [01:30,  1.66it/s]Extractor Estimating: 149it [01:30,  1.70it/s]Extractor Estimating: 150it [01:31,  1.70it/s]Extractor Estimating: 151it [01:31,  1.68it/s]Extractor Estimating: 152it [01:32,  1.66it/s]Extractor Estimating: 153it [01:32,  1.70it/s]Extractor Estimating: 154it [01:33,  1.68it/s]Extractor Estimating: 155it [01:34,  1.72it/s]Extractor Estimating: 156it [01:34,  1.73it/s]Extractor Estimating: 157it [01:35,  1.69it/s]Extractor Estimating: 158it [01:35,  1.73it/s]Extractor Estimating: 159it [01:36,  1.72it/s]Extractor Estimating: 160it [01:37,  1.69it/s]Extractor Estimating: 161it [01:37,  1.70it/s]Extractor Estimating: 162it [01:38,  1.70it/s]Extractor Estimating: 163it [01:38,  1.67it/s]Extractor Estimating: 164it [01:39,  1.60it/s]Extractor Estimating: 165it [01:40,  1.63it/s]Extractor Estimating: 166it [01:40,  1.66it/s]Extractor Estimating: 167it [01:41,  1.65it/s]Extractor Estimating: 168it [01:41,  1.67it/s]Extractor Estimating: 169it [01:42,  1.67it/s]Extractor Estimating: 170it [01:43,  1.59it/s]Extractor Estimating: 171it [01:43,  1.55it/s]Extractor Estimating: 172it [01:44,  1.58it/s]Extractor Estimating: 173it [01:45,  1.64it/s]Extractor Estimating: 174it [01:45,  1.64it/s]Extractor Estimating: 175it [01:46,  1.64it/s]Extractor Estimating: 176it [01:46,  1.70it/s]Extractor Estimating: 177it [01:47,  1.68it/s]Extractor Estimating: 178it [01:48,  1.66it/s]Extractor Estimating: 179it [01:48,  1.73it/s]Extractor Estimating: 180it [01:49,  1.69it/s]Extractor Estimating: 181it [01:49,  1.60it/s]Extractor Estimating: 182it [01:50,  1.64it/s]Extractor Estimating: 183it [01:51,  1.44it/s]Extractor Estimating: 184it [01:52,  1.45it/s]Extractor Estimating: 185it [01:52,  1.49it/s]Extractor Estimating: 186it [01:53,  1.47it/s]Extractor Estimating: 187it [01:54,  1.48it/s]Extractor Estimating: 188it [01:54,  1.54it/s]Extractor Estimating: 189it [01:55,  1.53it/s]Extractor Estimating: 190it [01:55,  1.54it/s]Extractor Estimating: 191it [01:56,  1.56it/s]Extractor Estimating: 192it [01:57,  1.55it/s]Extractor Estimating: 193it [01:57,  1.57it/s]Extractor Estimating: 194it [01:58,  1.60it/s]Extractor Estimating: 195it [01:58,  1.64it/s]Extractor Estimating: 196it [01:59,  1.66it/s]Extractor Estimating: 197it [02:00,  1.66it/s]Extractor Estimating: 198it [02:00,  1.66it/s]Extractor Estimating: 199it [02:01,  1.65it/s]Extractor Estimating: 200it [02:01,  1.67it/s]Extractor Estimating: 201it [02:02,  1.67it/s]Extractor Estimating: 202it [02:03,  1.64it/s]Extractor Estimating: 203it [02:03,  1.57it/s]Extractor Estimating: 204it [02:04,  1.55it/s]Extractor Estimating: 205it [02:05,  1.57it/s]Extractor Estimating: 206it [02:05,  1.55it/s]Extractor Estimating: 207it [02:06,  1.54it/s]Extractor Estimating: 208it [02:07,  1.58it/s]Extractor Estimating: 209it [02:07,  1.58it/s]Extractor Estimating: 210it [02:08,  1.49it/s]Extractor Estimating: 211it [02:09,  1.49it/s]Extractor Estimating: 212it [02:09,  1.45it/s]Extractor Estimating: 213it [02:10,  1.52it/s]Extractor Estimating: 214it [02:11,  1.49it/s]Extractor Estimating: 215it [02:11,  1.52it/s]Extractor Estimating: 216it [02:12,  1.58it/s]Extractor Estimating: 217it [02:13,  1.53it/s]Extractor Estimating: 218it [02:13,  1.53it/s]Extractor Estimating: 219it [02:14,  1.47it/s]Extractor Estimating: 220it [02:15,  1.50it/s]Extractor Estimating: 221it [02:15,  1.55it/s]Extractor Estimating: 222it [02:16,  1.58it/s]Extractor Estimating: 223it [02:16,  1.61it/s]Extractor Estimating: 224it [02:17,  1.59it/s]Extractor Estimating: 225it [02:18,  1.57it/s]Extractor Estimating: 226it [02:18,  1.61it/s]Extractor Estimating: 227it [02:19,  1.62it/s]Extractor Estimating: 228it [02:20,  1.63it/s]Extractor Estimating: 229it [02:20,  1.68it/s]Extractor Estimating: 230it [02:21,  1.68it/s]Extractor Estimating: 231it [02:21,  1.64it/s]Extractor Estimating: 232it [02:22,  1.65it/s]Extractor Estimating: 233it [02:22,  1.66it/s]Extractor Estimating: 234it [02:23,  1.60it/s]Extractor Estimating: 235it [02:24,  1.67it/s]Extractor Estimating: 236it [02:24,  1.74it/s]Extractor Estimating: 237it [02:25,  1.77it/s]Extractor Estimating: 238it [02:25,  1.78it/s]Extractor Estimating: 239it [02:26,  1.72it/s]Extractor Estimating: 240it [02:27,  1.53it/s]Extractor Estimating: 241it [02:27,  1.51it/s]Extractor Estimating: 242it [02:28,  1.57it/s]Extractor Estimating: 243it [02:29,  1.61it/s]Extractor Estimating: 244it [02:29,  1.66it/s]Extractor Estimating: 245it [02:30,  1.56it/s]Extractor Estimating: 246it [02:30,  1.66it/s]Extractor Estimating: 247it [02:31,  1.74it/s]Extractor Estimating: 248it [02:32,  1.75it/s]Extractor Estimating: 249it [02:32,  1.67it/s]Extractor Estimating: 250it [02:33,  1.71it/s]Extractor Estimating: 251it [02:33,  1.56it/s]Extractor Estimating: 252it [02:34,  1.63it/s]Extractor Estimating: 253it [02:35,  1.67it/s]Extractor Estimating: 254it [02:35,  1.66it/s]Extractor Estimating: 255it [02:36,  1.68it/s]Extractor Estimating: 256it [02:37,  1.44it/s]Extractor Estimating: 257it [02:37,  1.54it/s]Extractor Estimating: 258it [02:38,  1.59it/s]Extractor Estimating: 259it [02:38,  1.63it/s]Extractor Estimating: 260it [02:39,  1.65it/s]Extractor Estimating: 261it [02:40,  1.64it/s]Extractor Estimating: 262it [02:40,  1.71it/s]Extractor Estimating: 263it [02:41,  1.71it/s]Extractor Estimating: 264it [02:41,  1.74it/s]Extractor Estimating: 265it [02:42,  1.77it/s]Extractor Estimating: 266it [02:42,  1.82it/s]Extractor Estimating: 267it [02:43,  1.83it/s]Extractor Estimating: 268it [02:43,  1.90it/s]Extractor Estimating: 269it [02:44,  1.91it/s]Extractor Estimating: 270it [02:44,  1.85it/s]Extractor Estimating: 271it [02:45,  1.80it/s]Extractor Estimating: 272it [02:46,  1.77it/s]Extractor Estimating: 273it [02:46,  1.72it/s]Extractor Estimating: 274it [02:47,  1.72it/s]Extractor Estimating: 275it [02:47,  1.72it/s]Extractor Estimating: 276it [02:48,  1.62it/s]Extractor Estimating: 277it [02:49,  1.57it/s]Extractor Estimating: 278it [02:50,  1.54it/s]Extractor Estimating: 279it [02:50,  1.52it/s]Extractor Estimating: 280it [02:51,  1.52it/s]Extractor Estimating: 281it [02:51,  1.56it/s]Extractor Estimating: 282it [02:52,  1.57it/s]Extractor Estimating: 283it [02:53,  1.41it/s]Extractor Estimating: 284it [02:54,  1.45it/s]Extractor Estimating: 285it [02:54,  1.53it/s]Extractor Estimating: 286it [02:55,  1.55it/s]Extractor Estimating: 287it [02:55,  1.51it/s]Extractor Estimating: 288it [02:56,  1.54it/s]Extractor Estimating: 289it [02:57,  1.43it/s]Extractor Estimating: 290it [02:58,  1.46it/s]Extractor Estimating: 291it [02:58,  1.46it/s]Extractor Estimating: 292it [02:59,  1.45it/s]Extractor Estimating: 293it [03:00,  1.47it/s]Extractor Estimating: 294it [03:00,  1.52it/s]Extractor Estimating: 295it [03:01,  1.52it/s]Extractor Estimating: 296it [03:02,  1.53it/s]Extractor Estimating: 297it [03:02,  1.56it/s]Extractor Estimating: 298it [03:03,  1.54it/s]Extractor Estimating: 299it [03:03,  1.51it/s]Extractor Estimating: 300it [03:04,  1.57it/s]Extractor Estimating: 301it [03:05,  1.64it/s]Extractor Estimating: 302it [03:05,  1.67it/s]Extractor Estimating: 303it [03:06,  1.73it/s]Extractor Estimating: 304it [03:06,  1.75it/s]Extractor Estimating: 305it [03:07,  1.75it/s]Extractor Estimating: 306it [03:07,  1.77it/s]Extractor Estimating: 307it [03:08,  1.80it/s]Extractor Estimating: 308it [03:09,  1.73it/s]Extractor Estimating: 309it [03:09,  1.76it/s]Extractor Estimating: 310it [03:10,  1.72it/s]Extractor Estimating: 311it [03:10,  1.73it/s]Extractor Estimating: 312it [03:11,  1.71it/s]Extractor Estimating: 313it [03:12,  1.66it/s]Extractor Estimating: 314it [03:12,  1.74it/s]Extractor Estimating: 315it [03:13,  1.74it/s]Extractor Estimating: 316it [03:13,  1.80it/s]Extractor Estimating: 317it [03:14,  1.72it/s]Extractor Estimating: 318it [03:14,  1.66it/s]Extractor Estimating: 319it [03:15,  1.67it/s]Extractor Estimating: 320it [03:16,  1.68it/s]Extractor Estimating: 321it [03:16,  1.78it/s]Extractor Estimating: 322it [03:17,  1.82it/s]Extractor Estimating: 323it [03:17,  1.88it/s]Extractor Estimating: 324it [03:18,  1.68it/s]Extractor Estimating: 325it [03:18,  1.69it/s]Extractor Estimating: 326it [03:19,  1.73it/s]Extractor Estimating: 327it [03:20,  1.70it/s]Extractor Estimating: 328it [03:20,  1.72it/s]Extractor Estimating: 329it [03:21,  1.68it/s]Extractor Estimating: 330it [03:21,  1.67it/s]Extractor Estimating: 331it [03:22,  1.66it/s]Extractor Estimating: 332it [03:23,  1.58it/s]Extractor Estimating: 333it [03:23,  1.67it/s]Extractor Estimating: 334it [03:24,  1.41it/s]Extractor Estimating: 335it [03:25,  1.46it/s]Extractor Estimating: 336it [03:25,  1.54it/s]Extractor Estimating: 337it [03:26,  1.59it/s]Extractor Estimating: 338it [03:27,  1.59it/s]Extractor Estimating: 339it [03:27,  1.52it/s]Extractor Estimating: 340it [03:28,  1.48it/s]Extractor Estimating: 341it [03:29,  1.49it/s]Extractor Estimating: 342it [03:29,  1.52it/s]Extractor Estimating: 343it [03:30,  1.55it/s]Extractor Estimating: 344it [03:31,  1.51it/s]Extractor Estimating: 345it [03:31,  1.53it/s]Extractor Estimating: 346it [03:32,  1.54it/s]Extractor Estimating: 347it [03:32,  1.60it/s]Extractor Estimating: 348it [03:33,  1.60it/s]Extractor Estimating: 349it [03:34,  1.58it/s]Extractor Estimating: 350it [03:34,  1.67it/s]Extractor Estimating: 351it [03:35,  1.73it/s]Extractor Estimating: 352it [03:35,  1.71it/s]Extractor Estimating: 353it [03:36,  1.69it/s]Extractor Estimating: 354it [03:37,  1.73it/s]Extractor Estimating: 355it [03:37,  1.63it/s]Extractor Estimating: 356it [03:38,  1.65it/s]Extractor Estimating: 357it [03:38,  1.66it/s]Extractor Estimating: 358it [03:39,  1.69it/s]Extractor Estimating: 359it [03:40,  1.71it/s]Extractor Estimating: 360it [03:40,  1.69it/s]Extractor Estimating: 361it [03:41,  1.70it/s]Extractor Estimating: 362it [03:41,  1.66it/s]Extractor Estimating: 363it [03:42,  1.62it/s]Extractor Estimating: 364it [03:43,  1.68it/s]Extractor Estimating: 365it [03:43,  1.56it/s]Extractor Estimating: 366it [03:44,  1.61it/s]Extractor Estimating: 367it [03:45,  1.61it/s]Extractor Estimating: 368it [03:45,  1.64it/s]Extractor Estimating: 369it [03:46,  1.69it/s]Extractor Estimating: 370it [03:46,  1.62it/s]Extractor Estimating: 371it [03:47,  1.67it/s]Extractor Estimating: 372it [03:47,  1.67it/s]Extractor Estimating: 373it [03:48,  1.60it/s]Extractor Estimating: 374it [03:49,  1.63it/s]Extractor Estimating: 375it [03:49,  1.66it/s]Extractor Estimating: 376it [03:50,  1.73it/s]Extractor Estimating: 377it [03:50,  1.76it/s]Extractor Estimating: 378it [03:51,  1.83it/s]Extractor Estimating: 379it [03:52,  1.70it/s]Extractor Estimating: 380it [03:52,  1.53it/s]Extractor Estimating: 381it [03:53,  1.60it/s]Extractor Estimating: 382it [03:53,  1.67it/s]Extractor Estimating: 383it [03:54,  1.74it/s]Extractor Estimating: 384it [03:55,  1.75it/s]Extractor Estimating: 385it [03:55,  1.74it/s]Extractor Estimating: 386it [03:56,  1.57it/s]Extractor Estimating: 387it [03:56,  1.64it/s]Extractor Estimating: 388it [03:57,  1.72it/s]Extractor Estimating: 389it [03:58,  1.73it/s]Extractor Estimating: 390it [03:58,  1.73it/s]Extractor Estimating: 391it [03:59,  1.79it/s]Extractor Estimating: 392it [03:59,  1.79it/s]Extractor Estimating: 393it [04:00,  1.80it/s]Extractor Estimating: 394it [04:00,  1.82it/s]Extractor Estimating: 395it [04:01,  1.77it/s]Extractor Estimating: 396it [04:01,  1.76it/s]Extractor Estimating: 397it [04:02,  1.78it/s]Extractor Estimating: 398it [04:03,  1.76it/s]Extractor Estimating: 399it [04:03,  1.73it/s]Extractor Estimating: 400it [04:04,  1.78it/s]Extractor Estimating: 401it [04:04,  1.82it/s]Extractor Estimating: 402it [04:05,  1.73it/s]Extractor Estimating: 403it [04:05,  1.72it/s]Extractor Estimating: 404it [04:06,  1.75it/s]Extractor Estimating: 405it [04:07,  1.76it/s]Extractor Estimating: 406it [04:07,  1.81it/s]Extractor Estimating: 407it [04:08,  1.80it/s]Extractor Estimating: 408it [04:08,  1.85it/s]Extractor Estimating: 409it [04:09,  1.89it/s]Extractor Estimating: 410it [04:09,  1.88it/s]Extractor Estimating: 411it [04:10,  1.84it/s]Extractor Estimating: 412it [04:10,  1.82it/s]Extractor Estimating: 413it [04:11,  1.82it/s]Extractor Estimating: 414it [04:11,  1.79it/s]Extractor Estimating: 415it [04:12,  1.83it/s]Extractor Estimating: 416it [04:13,  1.83it/s]Extractor Estimating: 417it [04:13,  1.77it/s]Extractor Estimating: 418it [04:14,  1.81it/s]Extractor Estimating: 419it [04:14,  1.81it/s]Extractor Estimating: 420it [04:15,  1.73it/s]Extractor Estimating: 421it [04:15,  1.75it/s]Extractor Estimating: 422it [04:16,  1.73it/s]Extractor Estimating: 423it [04:17,  1.75it/s]Extractor Estimating: 424it [04:17,  1.75it/s]Extractor Estimating: 425it [04:18,  1.71it/s]Extractor Estimating: 426it [04:18,  1.61it/s]Extractor Estimating: 427it [04:19,  1.58it/s]Extractor Estimating: 428it [04:20,  1.59it/s]Extractor Estimating: 429it [04:20,  1.56it/s]Extractor Estimating: 430it [04:21,  1.59it/s]Extractor Estimating: 431it [04:22,  1.54it/s]Extractor Estimating: 432it [04:22,  1.54it/s]Extractor Estimating: 433it [04:23,  1.55it/s]Extractor Estimating: 434it [04:24,  1.56it/s]Extractor Estimating: 435it [04:24,  1.59it/s]Extractor Estimating: 436it [04:25,  1.63it/s]Extractor Estimating: 437it [04:25,  1.62it/s]Extractor Estimating: 438it [04:26,  1.65it/s]Extractor Estimating: 439it [04:27,  1.64it/s]Extractor Estimating: 440it [04:27,  1.65it/s]Extractor Estimating: 441it [04:28,  1.60it/s]Extractor Estimating: 442it [04:29,  1.39it/s]Extractor Estimating: 443it [04:29,  1.47it/s]Extractor Estimating: 444it [04:30,  1.46it/s]Extractor Estimating: 445it [04:31,  1.51it/s]Extractor Estimating: 446it [04:31,  1.58it/s]Extractor Estimating: 447it [04:32,  1.57it/s]Extractor Estimating: 448it [04:33,  1.59it/s]Extractor Estimating: 449it [04:33,  1.62it/s]Extractor Estimating: 450it [04:34,  1.58it/s]Extractor Estimating: 451it [04:34,  1.59it/s]Extractor Estimating: 452it [04:35,  1.44it/s]Extractor Estimating: 453it [04:36,  1.52it/s]Extractor Estimating: 454it [04:36,  1.59it/s]Extractor Estimating: 455it [04:37,  1.61it/s]Extractor Estimating: 456it [04:38,  1.64it/s]Extractor Estimating: 457it [04:39,  1.20it/s]Extractor Estimating: 458it [04:40,  1.30it/s]Extractor Estimating: 459it [04:40,  1.42it/s]Extractor Estimating: 460it [04:41,  1.51it/s]Extractor Estimating: 461it [04:41,  1.55it/s]Extractor Estimating: 462it [04:42,  1.61it/s]Extractor Estimating: 463it [04:42,  1.64it/s]Extractor Estimating: 464it [04:43,  1.62it/s]Extractor Estimating: 465it [04:44,  1.59it/s]Extractor Estimating: 466it [04:44,  1.55it/s]Extractor Estimating: 467it [04:45,  1.60it/s]Extractor Estimating: 468it [04:46,  1.61it/s]Extractor Estimating: 469it [04:46,  1.58it/s]Extractor Estimating: 470it [04:47,  1.59it/s]Extractor Estimating: 471it [04:48,  1.57it/s]Extractor Estimating: 472it [04:48,  1.56it/s]Extractor Estimating: 473it [04:49,  1.63it/s]Extractor Estimating: 474it [04:49,  1.63it/s]Extractor Estimating: 475it [04:50,  1.57it/s]Extractor Estimating: 476it [04:51,  1.59it/s]Extractor Estimating: 477it [04:51,  1.55it/s]Extractor Estimating: 478it [04:52,  1.58it/s]Extractor Estimating: 479it [04:53,  1.58it/s]Extractor Estimating: 480it [04:53,  1.59it/s]Extractor Estimating: 481it [04:54,  1.61it/s]Extractor Estimating: 482it [04:54,  1.62it/s]Extractor Estimating: 483it [04:55,  1.49it/s]Extractor Estimating: 484it [04:56,  1.54it/s]Extractor Estimating: 485it [04:56,  1.51it/s]Extractor Estimating: 486it [04:57,  1.59it/s]Extractor Estimating: 487it [04:58,  1.65it/s]Extractor Estimating: 488it [04:58,  1.63it/s]Extractor Estimating: 489it [04:59,  1.58it/s]Extractor Estimating: 490it [05:00,  1.52it/s]Extractor Estimating: 491it [05:00,  1.51it/s]Extractor Estimating: 492it [05:01,  1.53it/s]Extractor Estimating: 493it [05:01,  1.60it/s]Extractor Estimating: 494it [05:02,  1.67it/s]Extractor Estimating: 495it [05:03,  1.60it/s]Extractor Estimating: 496it [05:03,  1.59it/s]Extractor Estimating: 497it [05:04,  1.56it/s]Extractor Estimating: 498it [05:05,  1.59it/s]Extractor Estimating: 499it [05:05,  1.55it/s]Extractor Estimating: 500it [05:06,  1.68it/s]Extractor Estimating: 500it [05:06,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:19,441 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:19,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:19,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:19,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:19,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:28:20,168 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:28:20,170 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:28:20,813 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:28:21,846 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:28:21,846 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:26,819 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:26,832 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:26,832 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:26,832 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:28:26,832 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:28:28,103 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:28:28,104 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:28:28,680 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:28:28,829 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:28:28,829 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 00:32:45,418 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 00:32:45,451 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9998 mean pseudo reward: 0.9438529009828733
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 21922
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22022, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22022, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.077, loss:599.7676
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.061, loss:585.8382
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.068, loss:566.6176
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.061, loss:536.6628
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.073, loss:557.8103
>> valid entity prec:0.6968, rec:0.6715, f1:0.6839
>> valid relation prec:0.4932, rec:0.2682, f1:0.3475
>> valid relation with NER prec:0.4932, rec:0.2682, f1:0.3475
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.418, loss:561.5511
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.070, loss:541.8778
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.063, loss:552.2022
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.066, loss:537.8454
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.083, loss:564.2791
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6034, rec:0.6879, f1:0.6429
>> valid relation prec:0.3873, rec:0.1916, f1:0.2564
>> valid relation with NER prec:0.3873, rec:0.1916, f1:0.2564
g_step 1100, step 266, avg_time 2.406, loss:529.3443
g_step 1200, step 366, avg_time 1.064, loss:563.6768
g_step 1300, step 49, avg_time 1.060, loss:534.0081
g_step 1400, step 149, avg_time 1.069, loss:512.6200
g_step 1500, step 249, avg_time 1.058, loss:516.4705
>> valid entity prec:0.7079, rec:0.5821, f1:0.6389
>> valid relation prec:0.4906, rec:0.2248, f1:0.3083
>> valid relation with NER prec:0.4906, rec:0.2248, f1:0.3083
g_step 1600, step 349, avg_time 2.427, loss:548.4492
g_step 1700, step 32, avg_time 1.058, loss:529.7582
g_step 1800, step 132, avg_time 1.077, loss:482.9059
g_step 1900, step 232, avg_time 1.060, loss:494.1987
g_step 2000, step 332, avg_time 1.056, loss:498.7855
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.7323, rec:0.6235, f1:0.6735
>> valid relation prec:0.4308, rec:0.2082, f1:0.2807
>> valid relation with NER prec:0.4308, rec:0.2082, f1:0.2807
g_step 2100, step 15, avg_time 2.421, loss:496.0290
g_step 2200, step 115, avg_time 1.055, loss:450.1775
g_step 2300, step 215, avg_time 1.053, loss:451.7640
g_step 2400, step 315, avg_time 1.076, loss:463.2394
g_step 2500, step 415, avg_time 1.077, loss:509.4635
>> valid entity prec:0.7184, rec:0.5765, f1:0.6397
>> valid relation prec:0.4426, rec:0.1842, f1:0.2601
>> valid relation with NER prec:0.4426, rec:0.1842, f1:0.2601
g_step 2600, step 98, avg_time 2.410, loss:429.3852
g_step 2700, step 198, avg_time 1.066, loss:440.1090
g_step 2800, step 298, avg_time 1.075, loss:445.6880
g_step 2900, step 398, avg_time 1.061, loss:436.4765
g_step 3000, step 81, avg_time 1.075, loss:413.3138
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6822, rec:0.6559, f1:0.6688
>> valid relation prec:0.4132, rec:0.2130, f1:0.2811
>> valid relation with NER prec:0.4132, rec:0.2130, f1:0.2811
g_step 3100, step 181, avg_time 2.404, loss:424.8073
g_step 3200, step 281, avg_time 1.071, loss:427.8311
g_step 3300, step 381, avg_time 1.072, loss:434.2510
g_step 3400, step 64, avg_time 1.056, loss:401.5804
g_step 3500, step 164, avg_time 1.070, loss:400.6330
>> valid entity prec:0.7202, rec:0.6406, f1:0.6780
>> valid relation prec:0.4484, rec:0.2225, f1:0.2974
>> valid relation with NER prec:0.4484, rec:0.2225, f1:0.2974
g_step 3600, step 264, avg_time 2.420, loss:411.0493
g_step 3700, step 364, avg_time 1.073, loss:416.0411
g_step 3800, step 47, avg_time 1.060, loss:422.7165
g_step 3900, step 147, avg_time 1.067, loss:382.0868
g_step 4000, step 247, avg_time 1.066, loss:381.8578
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.7139, rec:0.6333, f1:0.6712
>> valid relation prec:0.3921, rec:0.1970, f1:0.2623
>> valid relation with NER prec:0.3921, rec:0.1970, f1:0.2623
g_step 4100, step 347, avg_time 2.406, loss:415.2230
g_step 4200, step 30, avg_time 1.046, loss:387.6030
g_step 4300, step 130, avg_time 1.068, loss:362.7312
g_step 4400, step 230, avg_time 1.063, loss:373.7643
g_step 4500, step 330, avg_time 1.065, loss:382.7732
>> valid entity prec:0.6811, rec:0.6615, f1:0.6712
>> valid relation prec:0.3445, rec:0.1999, f1:0.2530
>> valid relation with NER prec:0.3445, rec:0.1999, f1:0.2530
g_step 4600, step 13, avg_time 2.417, loss:357.5721
g_step 4700, step 113, avg_time 1.071, loss:351.2716
g_step 4800, step 213, avg_time 1.072, loss:374.4561
g_step 4900, step 313, avg_time 1.055, loss:365.0274
g_step 5000, step 413, avg_time 1.073, loss:369.7438
learning rate was adjusted to 0.0008
>> valid entity prec:0.6770, rec:0.6730, f1:0.6750
>> valid relation prec:0.4091, rec:0.2065, f1:0.2744
>> valid relation with NER prec:0.4091, rec:0.2065, f1:0.2744
g_step 5100, step 96, avg_time 2.431, loss:331.6868
g_step 5200, step 196, avg_time 1.071, loss:337.0145
g_step 5300, step 296, avg_time 1.056, loss:356.5146
g_step 5400, step 396, avg_time 1.066, loss:353.0482
g_step 5500, step 79, avg_time 1.048, loss:309.2406
>> valid entity prec:0.7188, rec:0.6814, f1:0.6996
>> valid relation prec:0.3850, rec:0.2356, f1:0.2924
>> valid relation with NER prec:0.3850, rec:0.2356, f1:0.2924
new max entity f1 on valid!
g_step 5600, step 179, avg_time 2.422, loss:319.3335
g_step 5700, step 279, avg_time 1.065, loss:352.5226
g_step 5800, step 379, avg_time 1.079, loss:341.9936
g_step 5900, step 62, avg_time 1.068, loss:321.3572
g_step 6000, step 162, avg_time 1.061, loss:318.6841
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.7041, rec:0.5766, f1:0.6340
>> valid relation prec:0.4721, rec:0.1764, f1:0.2569
>> valid relation with NER prec:0.4721, rec:0.1764, f1:0.2569
g_step 6100, step 262, avg_time 2.428, loss:323.0112
g_step 6200, step 362, avg_time 1.071, loss:345.7132
g_step 6300, step 45, avg_time 1.074, loss:306.1094
g_step 6400, step 145, avg_time 1.069, loss:291.9852
g_step 6500, step 245, avg_time 1.074, loss:314.1415
>> valid entity prec:0.6381, rec:0.6416, f1:0.6398
>> valid relation prec:0.3888, rec:0.2110, f1:0.2736
>> valid relation with NER prec:0.3888, rec:0.2110, f1:0.2736
g_step 6600, step 345, avg_time 2.405, loss:309.3908
g_step 6700, step 28, avg_time 1.062, loss:316.3752
g_step 6800, step 128, avg_time 1.076, loss:290.8934
g_step 6900, step 228, avg_time 1.058, loss:281.5244
g_step 7000, step 328, avg_time 1.060, loss:309.5282
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.7187, rec:0.6000, f1:0.6540
>> valid relation prec:0.4430, rec:0.1879, f1:0.2639
>> valid relation with NER prec:0.4430, rec:0.1879, f1:0.2639
g_step 7100, step 11, avg_time 2.428, loss:290.4503
g_step 7200, step 111, avg_time 1.065, loss:276.2572
g_step 7300, step 211, avg_time 1.077, loss:288.6565
g_step 7400, step 311, avg_time 1.058, loss:273.1552
g_step 7500, step 411, avg_time 1.058, loss:281.4189
>> valid entity prec:0.6799, rec:0.6644, f1:0.6720
>> valid relation prec:0.3659, rec:0.2142, f1:0.2702
>> valid relation with NER prec:0.3659, rec:0.2142, f1:0.2702
g_step 7600, step 94, avg_time 2.428, loss:261.0726
g_step 7700, step 194, avg_time 1.078, loss:266.5912
g_step 7800, step 294, avg_time 1.049, loss:278.7430
g_step 7900, step 394, avg_time 1.074, loss:265.3054
g_step 8000, step 77, avg_time 1.055, loss:258.2296
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6794, rec:0.6532, f1:0.6660
>> valid relation prec:0.3580, rec:0.2005, f1:0.2570
>> valid relation with NER prec:0.3580, rec:0.2005, f1:0.2570
g_step 8100, step 177, avg_time 2.414, loss:247.3390
g_step 8200, step 277, avg_time 1.069, loss:261.4591
g_step 8300, step 377, avg_time 1.076, loss:253.8812
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:32:45 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:32:45 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-32-45_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:32:46 - WARNING - datasets.builder -   Using custom data configuration default-daed749d92df1959
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-daed749d92df1959/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:32:46,716 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:32:46,718 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:32:46,718 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:32:46,719 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:32:46,725 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:32:46,728 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:32:46,729 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:32:46,729 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:32:46,729 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:32:46,729 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:32:46,729 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:32:46,869 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:32:49,958 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:32:49,960 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-daed749d92df1959/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.21ba/s] 20%|██        | 2/10 [00:00<00:01,  4.01ba/s] 30%|███       | 3/10 [00:00<00:01,  4.38ba/s] 40%|████      | 4/10 [00:00<00:01,  4.57ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.62ba/s] 60%|██████    | 6/10 [00:01<00:01,  3.83ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.10ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.28ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.43ba/s]100%|██████████| 10/10 [00:02<00:00,  4.55ba/s]100%|██████████| 10/10 [00:02<00:00,  4.31ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.94ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.19ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.29ba/s]100%|██████████| 4/4 [00:00<00:00,  5.32ba/s]100%|██████████| 4/4 [00:00<00:00,  4.84ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  7.68ba/s] 30%|███       | 3/10 [00:00<00:00,  9.33ba/s] 40%|████      | 4/10 [00:00<00:00,  9.52ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.65ba/s] 70%|███████   | 7/10 [00:00<00:00,  9.88ba/s] 90%|█████████ | 9/10 [00:00<00:00,  9.93ba/s]100%|██████████| 10/10 [00:01<00:00,  9.91ba/s]100%|██████████| 10/10 [00:01<00:00,  9.72ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.38ba/s] 50%|█████     | 2/4 [00:00<00:00,  9.26ba/s]100%|██████████| 4/4 [00:00<00:00, 11.43ba/s]100%|██████████| 4/4 [00:00<00:00, 10.80ba/s]
[INFO|trainer.py:414] 2023-08-29 00:32:54,950 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:32:54,960 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:32:54,961 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-29 00:32:54,961 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:32:54,961 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:32:54,961 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:32:54,961 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:32:54,961 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:53,  3.33it/s]  0%|          | 2/780 [00:00<03:47,  3.42it/s]  0%|          | 3/780 [00:00<03:45,  3.44it/s]  1%|          | 4/780 [00:01<03:44,  3.46it/s]  1%|          | 5/780 [00:01<03:44,  3.45it/s]  1%|          | 6/780 [00:01<03:43,  3.46it/s]  1%|          | 7/780 [00:02<03:42,  3.47it/s]  1%|          | 8/780 [00:02<03:42,  3.47it/s]  1%|          | 9/780 [00:02<03:41,  3.47it/s]  1%|▏         | 10/780 [00:02<03:41,  3.48it/s]  1%|▏         | 11/780 [00:03<03:41,  3.48it/s]  2%|▏         | 12/780 [00:03<03:40,  3.48it/s]  2%|▏         | 13/780 [00:03<03:40,  3.48it/s]  2%|▏         | 14/780 [00:04<03:40,  3.48it/s]  2%|▏         | 15/780 [00:04<03:40,  3.48it/s]  2%|▏         | 16/780 [00:04<03:40,  3.46it/s]  2%|▏         | 17/780 [00:04<03:40,  3.46it/s]  2%|▏         | 18/780 [00:05<03:39,  3.47it/s]  2%|▏         | 19/780 [00:05<03:39,  3.47it/s]  3%|▎         | 20/780 [00:05<03:39,  3.47it/s]  3%|▎         | 21/780 [00:06<03:38,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.47it/s]  3%|▎         | 23/780 [00:06<03:38,  3.47it/s]  3%|▎         | 24/780 [00:06<03:37,  3.47it/s]  3%|▎         | 25/780 [00:07<03:37,  3.47it/s]  3%|▎         | 26/780 [00:07<03:37,  3.47it/s]  3%|▎         | 27/780 [00:07<03:37,  3.47it/s]  4%|▎         | 28/780 [00:08<03:36,  3.47it/s]  4%|▎         | 29/780 [00:08<03:36,  3.47it/s]  4%|▍         | 30/780 [00:08<03:36,  3.47it/s]  4%|▍         | 31/780 [00:08<03:35,  3.47it/s]  4%|▍         | 32/780 [00:09<03:35,  3.47it/s]  4%|▍         | 33/780 [00:09<03:35,  3.47it/s]  4%|▍         | 34/780 [00:09<03:34,  3.47it/s]  4%|▍         | 35/780 [00:10<03:34,  3.47it/s]  5%|▍         | 36/780 [00:10<03:34,  3.47it/s]  5%|▍         | 37/780 [00:10<03:33,  3.47it/s]  5%|▍         | 38/780 [00:10<03:34,  3.46it/s]  5%|▌         | 39/780 [00:11<03:33,  3.46it/s]  5%|▌         | 40/780 [00:11<03:33,  3.47it/s]  5%|▌         | 41/780 [00:11<03:33,  3.47it/s]  5%|▌         | 42/780 [00:12<03:32,  3.47it/s]  6%|▌         | 43/780 [00:12<03:32,  3.47it/s]  6%|▌         | 44/780 [00:12<03:32,  3.47it/s]  6%|▌         | 45/780 [00:12<03:31,  3.47it/s]  6%|▌         | 46/780 [00:13<03:31,  3.47it/s]  6%|▌         | 47/780 [00:13<03:31,  3.47it/s]  6%|▌         | 48/780 [00:13<03:30,  3.47it/s]  6%|▋         | 49/780 [00:14<03:32,  3.45it/s]  6%|▋         | 50/780 [00:14<03:31,  3.45it/s]  7%|▋         | 51/780 [00:14<03:30,  3.46it/s]  7%|▋         | 52/780 [00:15<03:30,  3.46it/s]  7%|▋         | 53/780 [00:15<03:30,  3.46it/s]  7%|▋         | 54/780 [00:15<03:29,  3.46it/s]  7%|▋         | 55/780 [00:15<03:29,  3.47it/s]  7%|▋         | 56/780 [00:16<03:28,  3.47it/s]  7%|▋         | 57/780 [00:16<03:28,  3.47it/s]  7%|▋         | 58/780 [00:16<03:28,  3.47it/s]  8%|▊         | 59/780 [00:17<03:27,  3.47it/s]  8%|▊         | 60/780 [00:17<03:28,  3.46it/s]  8%|▊         | 61/780 [00:17<03:27,  3.46it/s]  8%|▊         | 62/780 [00:17<03:27,  3.46it/s]  8%|▊         | 63/780 [00:18<03:26,  3.46it/s]  8%|▊         | 64/780 [00:18<03:26,  3.46it/s]  8%|▊         | 65/780 [00:18<03:26,  3.47it/s]  8%|▊         | 66/780 [00:19<03:26,  3.46it/s]  9%|▊         | 67/780 [00:19<03:25,  3.46it/s]  9%|▊         | 68/780 [00:19<03:25,  3.47it/s]  9%|▉         | 69/780 [00:19<03:25,  3.47it/s]  9%|▉         | 70/780 [00:20<03:24,  3.47it/s]  9%|▉         | 71/780 [00:20<03:24,  3.46it/s]  9%|▉         | 72/780 [00:20<03:24,  3.46it/s]  9%|▉         | 73/780 [00:21<03:24,  3.46it/s]  9%|▉         | 74/780 [00:21<03:23,  3.46it/s] 10%|▉         | 75/780 [00:21<03:23,  3.46it/s] 10%|▉         | 76/780 [00:21<03:23,  3.47it/s] 10%|▉         | 77/780 [00:22<03:22,  3.47it/s] 10%|█         | 78/780 [00:22<03:22,  3.47it/s] 10%|█         | 79/780 [00:22<03:22,  3.46it/s] 10%|█         | 80/780 [00:23<03:21,  3.47it/s] 10%|█         | 81/780 [00:23<03:21,  3.46it/s] 11%|█         | 82/780 [00:23<03:21,  3.46it/s] 11%|█         | 83/780 [00:23<03:21,  3.46it/s] 11%|█         | 84/780 [00:24<03:20,  3.47it/s] 11%|█         | 85/780 [00:24<03:21,  3.45it/s] 11%|█         | 86/780 [00:24<03:20,  3.46it/s] 11%|█         | 87/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 88/780 [00:25<03:19,  3.46it/s] 11%|█▏        | 89/780 [00:25<03:19,  3.46it/s] 12%|█▏        | 90/780 [00:25<03:19,  3.46it/s] 12%|█▏        | 91/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 92/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 93/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 94/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 95/780 [00:27<03:17,  3.46it/s] 12%|█▏        | 96/780 [00:27<03:18,  3.45it/s] 12%|█▏        | 97/780 [00:27<03:17,  3.46it/s] 13%|█▎        | 98/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 99/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 100/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.46it/s] 13%|█▎        | 102/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 103/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 104/780 [00:30<03:15,  3.46it/s] 13%|█▎        | 105/780 [00:30<03:15,  3.46it/s] 14%|█▎        | 106/780 [00:30<03:14,  3.46it/s] 14%|█▎        | 107/780 [00:30<03:15,  3.45it/s] 14%|█▍        | 108/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 109/780 [00:31<03:14,  3.46it/s] 14%|█▍        | 110/780 [00:31<03:13,  3.46it/s] 14%|█▍        | 111/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 113/780 [00:32<03:13,  3.46it/s] 15%|█▍        | 114/780 [00:32<03:12,  3.46it/s] 15%|█▍        | 115/780 [00:33<03:12,  3.46it/s] 15%|█▍        | 116/780 [00:33<03:12,  3.46it/s] 15%|█▌        | 117/780 [00:33<03:11,  3.46it/s] 15%|█▌        | 118/780 [00:34<03:13,  3.42it/s] 15%|█▌        | 119/780 [00:34<03:12,  3.43it/s] 15%|█▌        | 120/780 [00:34<03:11,  3.44it/s] 16%|█▌        | 121/780 [00:34<03:11,  3.45it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 123/780 [00:35<03:10,  3.45it/s] 16%|█▌        | 124/780 [00:35<03:09,  3.45it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.45it/s] 16%|█▌        | 126/780 [00:36<03:09,  3.46it/s] 16%|█▋        | 127/780 [00:36<03:09,  3.45it/s] 16%|█▋        | 128/780 [00:36<03:08,  3.45it/s] 17%|█▋        | 129/780 [00:37<03:09,  3.44it/s] 17%|█▋        | 130/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 131/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 133/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 134/780 [00:38<03:07,  3.45it/s] 17%|█▋        | 135/780 [00:39<03:06,  3.45it/s] 17%|█▋        | 136/780 [00:39<03:06,  3.45it/s] 18%|█▊        | 137/780 [00:39<03:06,  3.46it/s] 18%|█▊        | 138/780 [00:39<03:05,  3.46it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.46it/s] 18%|█▊        | 140/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 141/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 142/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 143/780 [00:41<03:04,  3.45it/s] 18%|█▊        | 144/780 [00:41<03:04,  3.45it/s] 19%|█▊        | 145/780 [00:41<03:03,  3.46it/s] 19%|█▊        | 146/780 [00:42<03:03,  3.46it/s] 19%|█▉        | 147/780 [00:42<03:03,  3.45it/s] 19%|█▉        | 148/780 [00:42<03:02,  3.45it/s] 19%|█▉        | 149/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 150/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 151/780 [00:43<03:02,  3.44it/s] 19%|█▉        | 152/780 [00:43<03:02,  3.45it/s] 20%|█▉        | 153/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 154/780 [00:44<03:01,  3.45it/s] 20%|█▉        | 155/780 [00:44<03:00,  3.45it/s] 20%|██        | 156/780 [00:45<03:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:33:40,093 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:33:40,094 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:33:40,094 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.42it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.81it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.99it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.24it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.72it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.43it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.17it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.83it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.86it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.81it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.94it/s][A
 14%|█▍        | 63/438 [00:01<00:07, 46.99it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.96it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.96it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.79it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.62it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.58it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.59it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.60it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.72it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.88it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.87it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.87it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.87it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.68it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.53it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.55it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.63it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.69it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.79it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.83it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.93it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.78it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.77it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.60it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.67it/s][A
 43%|████▎     | 188/438 [00:03<00:05, 46.58it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.61it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.75it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.78it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.82it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.85it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.72it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.65it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.63it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.67it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.62it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.63it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.83it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 37.72it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 40.14it/s][A
 60%|██████    | 263/438 [00:05<00:04, 41.93it/s][A
 61%|██████    | 268/438 [00:05<00:03, 43.30it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 44.36it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 45.18it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 45.76it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.11it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 45.80it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 45.95it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.15it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.44it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.58it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.69it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.78it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.87it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.87it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.76it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.69it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.67it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.67it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.74it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.78it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.82it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 46.79it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.78it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.89it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.79it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.55it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.74it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.72it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.69it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.88it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 46.83it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.85it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.93it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.84it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.84it/s][A                                                 
                                                 [A 20%|██        | 156/780 [00:54<03:00,  3.45it/s]
100%|██████████| 438/438 [00:09<00:00, 46.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:33:49,589 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 00:33:49,606 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:33:51,965 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:33:51,981 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:33:51,991 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:02<55:40,  5.36s/it] 20%|██        | 158/780 [01:02<39:49,  3.84s/it] 20%|██        | 159/780 [01:02<28:43,  2.78s/it] 21%|██        | 160/780 [01:03<20:58,  2.03s/it] 21%|██        | 161/780 [01:03<15:32,  1.51s/it] 21%|██        | 162/780 [01:03<11:45,  1.14s/it] 21%|██        | 163/780 [01:04<09:06,  1.13it/s] 21%|██        | 164/780 [01:04<07:15,  1.42it/s] 21%|██        | 165/780 [01:04<05:57,  1.72it/s] 21%|██▏       | 166/780 [01:04<05:02,  2.03it/s] 21%|██▏       | 167/780 [01:05<04:24,  2.32it/s] 22%|██▏       | 168/780 [01:05<03:58,  2.57it/s] 22%|██▏       | 169/780 [01:05<03:39,  2.78it/s] 22%|██▏       | 170/780 [01:06<03:26,  2.96it/s] 22%|██▏       | 171/780 [01:06<03:16,  3.09it/s] 22%|██▏       | 172/780 [01:06<03:10,  3.20it/s] 22%|██▏       | 173/780 [01:06<03:05,  3.27it/s] 22%|██▏       | 174/780 [01:07<03:02,  3.33it/s] 22%|██▏       | 175/780 [01:07<02:59,  3.37it/s] 23%|██▎       | 176/780 [01:07<02:57,  3.39it/s] 23%|██▎       | 177/780 [01:08<02:56,  3.41it/s] 23%|██▎       | 178/780 [01:08<02:55,  3.43it/s] 23%|██▎       | 179/780 [01:08<02:54,  3.44it/s] 23%|██▎       | 180/780 [01:08<02:54,  3.44it/s] 23%|██▎       | 181/780 [01:09<02:53,  3.44it/s] 23%|██▎       | 182/780 [01:09<02:53,  3.45it/s] 23%|██▎       | 183/780 [01:09<02:52,  3.45it/s] 24%|██▎       | 184/780 [01:10<02:52,  3.46it/s] 24%|██▎       | 185/780 [01:10<02:51,  3.46it/s] 24%|██▍       | 186/780 [01:10<02:51,  3.46it/s] 24%|██▍       | 187/780 [01:10<02:51,  3.46it/s] 24%|██▍       | 188/780 [01:11<02:51,  3.46it/s] 24%|██▍       | 189/780 [01:11<02:50,  3.46it/s] 24%|██▍       | 190/780 [01:11<02:50,  3.46it/s] 24%|██▍       | 191/780 [01:12<02:51,  3.44it/s] 25%|██▍       | 192/780 [01:12<02:50,  3.45it/s] 25%|██▍       | 193/780 [01:12<02:50,  3.45it/s] 25%|██▍       | 194/780 [01:12<02:49,  3.45it/s] 25%|██▌       | 195/780 [01:13<02:49,  3.46it/s] 25%|██▌       | 196/780 [01:13<02:48,  3.46it/s] 25%|██▌       | 197/780 [01:13<02:48,  3.46it/s] 25%|██▌       | 198/780 [01:14<02:48,  3.46it/s] 26%|██▌       | 199/780 [01:14<02:47,  3.46it/s] 26%|██▌       | 200/780 [01:14<02:47,  3.46it/s] 26%|██▌       | 201/780 [01:15<02:47,  3.46it/s] 26%|██▌       | 202/780 [01:15<02:47,  3.45it/s] 26%|██▌       | 203/780 [01:15<02:46,  3.46it/s] 26%|██▌       | 204/780 [01:15<02:46,  3.46it/s] 26%|██▋       | 205/780 [01:16<02:46,  3.46it/s] 26%|██▋       | 206/780 [01:16<02:46,  3.46it/s] 27%|██▋       | 207/780 [01:16<02:45,  3.46it/s] 27%|██▋       | 208/780 [01:17<02:45,  3.46it/s] 27%|██▋       | 209/780 [01:17<02:45,  3.46it/s] 27%|██▋       | 210/780 [01:17<02:44,  3.46it/s] 27%|██▋       | 211/780 [01:17<02:44,  3.46it/s] 27%|██▋       | 212/780 [01:18<02:44,  3.46it/s] 27%|██▋       | 213/780 [01:18<02:44,  3.44it/s] 27%|██▋       | 214/780 [01:18<02:44,  3.45it/s] 28%|██▊       | 215/780 [01:19<02:43,  3.45it/s] 28%|██▊       | 216/780 [01:19<02:43,  3.45it/s] 28%|██▊       | 217/780 [01:19<02:42,  3.45it/s] 28%|██▊       | 218/780 [01:19<02:42,  3.46it/s] 28%|██▊       | 219/780 [01:20<02:42,  3.45it/s] 28%|██▊       | 220/780 [01:20<02:42,  3.46it/s] 28%|██▊       | 221/780 [01:20<02:41,  3.46it/s] 28%|██▊       | 222/780 [01:21<02:41,  3.46it/s] 29%|██▊       | 223/780 [01:21<02:41,  3.46it/s] 29%|██▊       | 224/780 [01:21<02:41,  3.44it/s] 29%|██▉       | 225/780 [01:21<02:41,  3.44it/s] 29%|██▉       | 226/780 [01:22<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:22<02:40,  3.45it/s] 29%|██▉       | 228/780 [01:22<02:39,  3.45it/s] 29%|██▉       | 229/780 [01:23<02:39,  3.45it/s] 29%|██▉       | 230/780 [01:23<02:39,  3.46it/s] 30%|██▉       | 231/780 [01:23<02:38,  3.46it/s] 30%|██▉       | 232/780 [01:23<02:38,  3.46it/s] 30%|██▉       | 233/780 [01:24<02:38,  3.46it/s] 30%|███       | 234/780 [01:24<02:37,  3.46it/s] 30%|███       | 235/780 [01:24<02:37,  3.46it/s] 30%|███       | 236/780 [01:25<02:37,  3.46it/s] 30%|███       | 237/780 [01:25<02:36,  3.46it/s] 31%|███       | 238/780 [01:25<02:36,  3.46it/s] 31%|███       | 239/780 [01:26<02:37,  3.43it/s] 31%|███       | 240/780 [01:26<02:36,  3.44it/s] 31%|███       | 241/780 [01:26<02:36,  3.45it/s] 31%|███       | 242/780 [01:26<02:35,  3.45it/s] 31%|███       | 243/780 [01:27<02:35,  3.45it/s] 31%|███▏      | 244/780 [01:27<02:35,  3.45it/s] 31%|███▏      | 245/780 [01:27<02:34,  3.46it/s] 32%|███▏      | 246/780 [01:28<02:34,  3.46it/s] 32%|███▏      | 247/780 [01:28<02:34,  3.46it/s] 32%|███▏      | 248/780 [01:28<02:33,  3.46it/s] 32%|███▏      | 249/780 [01:28<02:33,  3.46it/s] 32%|███▏      | 250/780 [01:29<02:33,  3.45it/s] 32%|███▏      | 251/780 [01:29<02:33,  3.45it/s] 32%|███▏      | 252/780 [01:29<02:32,  3.45it/s] 32%|███▏      | 253/780 [01:30<02:32,  3.46it/s] 33%|███▎      | 254/780 [01:30<02:32,  3.46it/s] 33%|███▎      | 255/780 [01:30<02:31,  3.46it/s] 33%|███▎      | 256/780 [01:30<02:31,  3.46it/s] 33%|███▎      | 257/780 [01:31<02:31,  3.46it/s] 33%|███▎      | 258/780 [01:31<02:30,  3.46it/s] 33%|███▎      | 259/780 [01:31<02:30,  3.46it/s] 33%|███▎      | 260/780 [01:32<02:30,  3.46it/s] 33%|███▎      | 261/780 [01:32<02:30,  3.45it/s] 34%|███▎      | 262/780 [01:32<02:30,  3.45it/s] 34%|███▎      | 263/780 [01:32<02:29,  3.45it/s] 34%|███▍      | 264/780 [01:33<02:29,  3.46it/s] 34%|███▍      | 265/780 [01:33<02:29,  3.46it/s] 34%|███▍      | 266/780 [01:33<02:28,  3.46it/s] 34%|███▍      | 267/780 [01:34<02:28,  3.45it/s] 34%|███▍      | 268/780 [01:34<02:28,  3.46it/s] 34%|███▍      | 269/780 [01:34<02:27,  3.46it/s] 35%|███▍      | 270/780 [01:34<02:27,  3.46it/s] 35%|███▍      | 271/780 [01:35<02:27,  3.46it/s] 35%|███▍      | 272/780 [01:35<02:26,  3.46it/s] 35%|███▌      | 273/780 [01:35<02:26,  3.46it/s] 35%|███▌      | 274/780 [01:36<02:26,  3.45it/s] 35%|███▌      | 275/780 [01:36<02:26,  3.45it/s] 35%|███▌      | 276/780 [01:36<02:25,  3.46it/s] 36%|███▌      | 277/780 [01:37<02:25,  3.46it/s] 36%|███▌      | 278/780 [01:37<02:25,  3.46it/s] 36%|███▌      | 279/780 [01:37<02:24,  3.46it/s] 36%|███▌      | 280/780 [01:37<02:24,  3.46it/s] 36%|███▌      | 281/780 [01:38<02:24,  3.46it/s] 36%|███▌      | 282/780 [01:38<02:24,  3.46it/s] 36%|███▋      | 283/780 [01:38<02:23,  3.46it/s] 36%|███▋      | 284/780 [01:39<02:23,  3.46it/s] 37%|███▋      | 285/780 [01:39<02:23,  3.46it/s] 37%|███▋      | 286/780 [01:39<02:22,  3.46it/s] 37%|███▋      | 287/780 [01:39<02:22,  3.46it/s] 37%|███▋      | 288/780 [01:40<02:22,  3.46it/s] 37%|███▋      | 289/780 [01:40<02:21,  3.46it/s] 37%|███▋      | 290/780 [01:40<02:21,  3.46it/s] 37%|███▋      | 291/780 [01:41<02:21,  3.46it/s] 37%|███▋      | 292/780 [01:41<02:21,  3.45it/s] 38%|███▊      | 293/780 [01:41<02:21,  3.45it/s] 38%|███▊      | 294/780 [01:41<02:20,  3.45it/s] 38%|███▊      | 295/780 [01:42<02:20,  3.45it/s] 38%|███▊      | 296/780 [01:42<02:20,  3.46it/s] 38%|███▊      | 297/780 [01:42<02:19,  3.46it/s] 38%|███▊      | 298/780 [01:43<02:19,  3.46it/s] 38%|███▊      | 299/780 [01:43<02:19,  3.46it/s] 38%|███▊      | 300/780 [01:43<02:18,  3.46it/s] 39%|███▊      | 301/780 [01:43<02:18,  3.46it/s] 39%|███▊      | 302/780 [01:44<02:18,  3.46it/s] 39%|███▉      | 303/780 [01:44<02:17,  3.46it/s] 39%|███▉      | 304/780 [01:44<02:17,  3.46it/s] 39%|███▉      | 305/780 [01:45<02:17,  3.46it/s] 39%|███▉      | 306/780 [01:45<02:17,  3.46it/s] 39%|███▉      | 307/780 [01:45<02:16,  3.46it/s] 39%|███▉      | 308/780 [01:45<02:16,  3.46it/s] 40%|███▉      | 309/780 [01:46<02:22,  3.30it/s] 40%|███▉      | 310/780 [01:46<02:20,  3.35it/s] 40%|███▉      | 311/780 [01:46<02:18,  3.38it/s] 40%|████      | 312/780 [01:47<02:17,  3.40it/s][INFO|trainer.py:2140] 2023-08-29 00:34:42,176 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:34:42,176 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:34:42,176 >>   Batch size = 8
{'eval_loss': 1.007078766822815, 'eval_runtime': 9.465, 'eval_samples_per_second': 369.466, 'eval_steps_per_second': 46.276, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.05it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.60it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.81it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.04it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.71it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.49it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.15it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.79it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.04it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.30it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.41it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.56it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.55it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.61it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.76it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.76it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.69it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.66it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.62it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.73it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.80it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.69it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.70it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.73it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.76it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.80it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.67it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.72it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.75it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 43.12it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.92it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 45.41it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 45.80it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.08it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.27it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.40it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.54it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.57it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.56it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.70it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.67it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.63it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.67it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.73it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.72it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.73it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.62it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.61it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.70it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.75it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.71it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.75it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.66it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.75it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.71it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.69it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.63it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.70it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.75it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.70it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.65it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.69it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.78it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.80it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.86it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.68it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.71it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.73it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.73it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.70it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.70it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.71it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.74it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.75it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.64it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.72it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.67it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.72it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.66it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.62it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.66it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.72it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.70it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.71it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.73it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.73it/s][A                                                 
                                                 [A 40%|████      | 312/780 [01:56<02:17,  3.40it/s]
100%|██████████| 438/438 [00:09<00:00, 46.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:34:51,632 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 00:34:51,656 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:34:53,859 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:34:53,877 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:34:53,885 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:04<41:19,  5.31s/it] 40%|████      | 314/780 [02:04<29:33,  3.81s/it] 40%|████      | 315/780 [02:04<21:19,  2.75s/it] 41%|████      | 316/780 [02:05<15:33,  2.01s/it] 41%|████      | 317/780 [02:05<11:32,  1.49s/it] 41%|████      | 318/780 [02:05<08:43,  1.13s/it] 41%|████      | 319/780 [02:05<06:45,  1.14it/s] 41%|████      | 320/780 [02:06<05:23,  1.42it/s] 41%|████      | 321/780 [02:06<04:25,  1.73it/s] 41%|████▏     | 322/780 [02:06<03:45,  2.03it/s] 41%|████▏     | 323/780 [02:07<03:16,  2.32it/s] 42%|████▏     | 324/780 [02:07<02:57,  2.58it/s] 42%|████▏     | 325/780 [02:07<02:43,  2.77it/s] 42%|████▏     | 326/780 [02:07<02:33,  2.95it/s] 42%|████▏     | 327/780 [02:08<02:26,  3.09it/s] 42%|████▏     | 328/780 [02:08<02:21,  3.19it/s] 42%|████▏     | 329/780 [02:08<02:18,  3.27it/s] 42%|████▏     | 330/780 [02:09<02:15,  3.32it/s] 42%|████▏     | 331/780 [02:09<02:13,  3.36it/s] 43%|████▎     | 332/780 [02:09<02:12,  3.39it/s] 43%|████▎     | 333/780 [02:09<02:11,  3.41it/s] 43%|████▎     | 334/780 [02:10<02:10,  3.43it/s] 43%|████▎     | 335/780 [02:10<02:09,  3.44it/s] 43%|████▎     | 336/780 [02:10<02:09,  3.43it/s] 43%|████▎     | 337/780 [02:11<02:08,  3.44it/s] 43%|████▎     | 338/780 [02:11<02:08,  3.45it/s] 43%|████▎     | 339/780 [02:11<02:07,  3.45it/s] 44%|████▎     | 340/780 [02:12<02:07,  3.45it/s] 44%|████▎     | 341/780 [02:12<02:07,  3.45it/s] 44%|████▍     | 342/780 [02:12<02:06,  3.46it/s] 44%|████▍     | 343/780 [02:12<02:06,  3.46it/s] 44%|████▍     | 344/780 [02:13<02:06,  3.46it/s] 44%|████▍     | 345/780 [02:13<02:05,  3.46it/s] 44%|████▍     | 346/780 [02:13<02:05,  3.46it/s] 44%|████▍     | 347/780 [02:14<02:05,  3.45it/s] 45%|████▍     | 348/780 [02:14<02:05,  3.46it/s] 45%|████▍     | 349/780 [02:14<02:04,  3.46it/s] 45%|████▍     | 350/780 [02:14<02:04,  3.46it/s] 45%|████▌     | 351/780 [02:15<02:04,  3.46it/s] 45%|████▌     | 352/780 [02:15<02:03,  3.46it/s] 45%|████▌     | 353/780 [02:15<02:03,  3.46it/s] 45%|████▌     | 354/780 [02:16<02:03,  3.46it/s] 46%|████▌     | 355/780 [02:16<02:02,  3.46it/s] 46%|████▌     | 356/780 [02:16<02:02,  3.46it/s] 46%|████▌     | 357/780 [02:16<02:02,  3.46it/s] 46%|████▌     | 358/780 [02:17<02:02,  3.45it/s] 46%|████▌     | 359/780 [02:17<02:02,  3.45it/s] 46%|████▌     | 360/780 [02:17<02:01,  3.45it/s] 46%|████▋     | 361/780 [02:18<02:01,  3.45it/s] 46%|████▋     | 362/780 [02:18<02:01,  3.45it/s] 47%|████▋     | 363/780 [02:18<02:00,  3.45it/s] 47%|████▋     | 364/780 [02:18<02:00,  3.46it/s] 47%|████▋     | 365/780 [02:19<02:00,  3.46it/s] 47%|████▋     | 366/780 [02:19<01:59,  3.46it/s] 47%|████▋     | 367/780 [02:19<01:59,  3.46it/s] 47%|████▋     | 368/780 [02:20<01:59,  3.46it/s] 47%|████▋     | 369/780 [02:20<01:59,  3.45it/s] 47%|████▋     | 370/780 [02:20<01:58,  3.45it/s] 48%|████▊     | 371/780 [02:20<01:58,  3.45it/s] 48%|████▊     | 372/780 [02:21<01:58,  3.45it/s] 48%|████▊     | 373/780 [02:21<01:57,  3.46it/s] 48%|████▊     | 374/780 [02:21<01:57,  3.46it/s] 48%|████▊     | 375/780 [02:22<01:57,  3.46it/s] 48%|████▊     | 376/780 [02:22<01:56,  3.46it/s] 48%|████▊     | 377/780 [02:22<01:56,  3.46it/s] 48%|████▊     | 378/780 [02:22<01:56,  3.46it/s] 49%|████▊     | 379/780 [02:23<01:56,  3.46it/s] 49%|████▊     | 380/780 [02:23<01:56,  3.44it/s] 49%|████▉     | 381/780 [02:23<01:55,  3.45it/s] 49%|████▉     | 382/780 [02:24<01:55,  3.45it/s] 49%|████▉     | 383/780 [02:24<01:54,  3.45it/s] 49%|████▉     | 384/780 [02:24<01:54,  3.46it/s] 49%|████▉     | 385/780 [02:25<01:54,  3.46it/s] 49%|████▉     | 386/780 [02:25<01:53,  3.46it/s] 50%|████▉     | 387/780 [02:25<01:53,  3.46it/s] 50%|████▉     | 388/780 [02:25<01:53,  3.46it/s] 50%|████▉     | 389/780 [02:26<01:53,  3.46it/s] 50%|█████     | 390/780 [02:26<01:52,  3.46it/s] 50%|█████     | 391/780 [02:26<01:52,  3.46it/s] 50%|█████     | 392/780 [02:27<01:52,  3.46it/s] 50%|█████     | 393/780 [02:27<01:52,  3.45it/s] 51%|█████     | 394/780 [02:27<01:51,  3.45it/s] 51%|█████     | 395/780 [02:27<01:51,  3.45it/s] 51%|█████     | 396/780 [02:28<01:51,  3.45it/s] 51%|█████     | 397/780 [02:28<01:50,  3.46it/s] 51%|█████     | 398/780 [02:28<01:50,  3.46it/s] 51%|█████     | 399/780 [02:29<01:50,  3.46it/s] 51%|█████▏    | 400/780 [02:29<01:49,  3.46it/s] 51%|█████▏    | 401/780 [02:29<01:49,  3.46it/s] 52%|█████▏    | 402/780 [02:29<01:49,  3.46it/s] 52%|█████▏    | 403/780 [02:30<01:49,  3.46it/s] 52%|█████▏    | 404/780 [02:30<01:49,  3.45it/s] 52%|█████▏    | 405/780 [02:30<01:48,  3.45it/s] 52%|█████▏    | 406/780 [02:31<01:48,  3.45it/s] 52%|█████▏    | 407/780 [02:31<01:48,  3.45it/s] 52%|█████▏    | 408/780 [02:31<01:47,  3.45it/s] 52%|█████▏    | 409/780 [02:31<01:47,  3.45it/s] 53%|█████▎    | 410/780 [02:32<01:47,  3.46it/s] 53%|█████▎    | 411/780 [02:32<01:46,  3.45it/s] 53%|█████▎    | 412/780 [02:32<01:46,  3.46it/s] 53%|█████▎    | 413/780 [02:33<01:46,  3.46it/s] 53%|█████▎    | 414/780 [02:33<01:45,  3.45it/s] 53%|█████▎    | 415/780 [02:33<01:46,  3.44it/s] 53%|█████▎    | 416/780 [02:33<01:45,  3.45it/s] 53%|█████▎    | 417/780 [02:34<01:45,  3.45it/s] 54%|█████▎    | 418/780 [02:34<01:44,  3.45it/s] 54%|█████▎    | 419/780 [02:34<01:44,  3.45it/s] 54%|█████▍    | 420/780 [02:35<01:44,  3.46it/s] 54%|█████▍    | 421/780 [02:35<01:43,  3.45it/s] 54%|█████▍    | 422/780 [02:35<01:43,  3.45it/s] 54%|█████▍    | 423/780 [02:36<01:43,  3.46it/s] 54%|█████▍    | 424/780 [02:36<01:43,  3.46it/s] 54%|█████▍    | 425/780 [02:36<01:42,  3.46it/s] 55%|█████▍    | 426/780 [02:36<01:42,  3.45it/s] 55%|█████▍    | 427/780 [02:37<01:42,  3.45it/s] 55%|█████▍    | 428/780 [02:37<01:41,  3.45it/s] 55%|█████▌    | 429/780 [02:37<01:41,  3.45it/s] 55%|█████▌    | 430/780 [02:38<01:41,  3.45it/s] 55%|█████▌    | 431/780 [02:38<01:40,  3.46it/s] 55%|█████▌    | 432/780 [02:38<01:40,  3.46it/s] 56%|█████▌    | 433/780 [02:38<01:40,  3.46it/s] 56%|█████▌    | 434/780 [02:39<01:40,  3.46it/s] 56%|█████▌    | 435/780 [02:39<01:39,  3.46it/s] 56%|█████▌    | 436/780 [02:39<01:39,  3.46it/s] 56%|█████▌    | 437/780 [02:40<01:39,  3.45it/s] 56%|█████▌    | 438/780 [02:40<01:39,  3.45it/s] 56%|█████▋    | 439/780 [02:40<01:38,  3.45it/s] 56%|█████▋    | 440/780 [02:40<01:38,  3.45it/s] 57%|█████▋    | 441/780 [02:41<01:38,  3.45it/s] 57%|█████▋    | 442/780 [02:41<01:37,  3.45it/s] 57%|█████▋    | 443/780 [02:41<01:37,  3.46it/s] 57%|█████▋    | 444/780 [02:42<01:37,  3.45it/s] 57%|█████▋    | 445/780 [02:42<01:36,  3.46it/s] 57%|█████▋    | 446/780 [02:42<01:36,  3.46it/s] 57%|█████▋    | 447/780 [02:42<01:36,  3.46it/s] 57%|█████▋    | 448/780 [02:43<01:36,  3.43it/s] 58%|█████▊    | 449/780 [02:43<01:36,  3.44it/s] 58%|█████▊    | 450/780 [02:43<01:35,  3.44it/s] 58%|█████▊    | 451/780 [02:44<01:35,  3.45it/s] 58%|█████▊    | 452/780 [02:44<01:35,  3.45it/s] 58%|█████▊    | 453/780 [02:44<01:34,  3.45it/s] 58%|█████▊    | 454/780 [02:45<01:34,  3.45it/s] 58%|█████▊    | 455/780 [02:45<01:34,  3.45it/s] 58%|█████▊    | 456/780 [02:45<01:33,  3.45it/s] 59%|█████▊    | 457/780 [02:45<01:33,  3.45it/s] 59%|█████▊    | 458/780 [02:46<01:33,  3.45it/s] 59%|█████▉    | 459/780 [02:46<01:33,  3.45it/s] 59%|█████▉    | 460/780 [02:46<01:32,  3.45it/s] 59%|█████▉    | 461/780 [02:47<01:32,  3.45it/s] 59%|█████▉    | 462/780 [02:47<01:32,  3.45it/s] 59%|█████▉    | 463/780 [02:47<01:31,  3.45it/s] 59%|█████▉    | 464/780 [02:47<01:31,  3.45it/s] 60%|█████▉    | 465/780 [02:48<01:31,  3.45it/s] 60%|█████▉    | 466/780 [02:48<01:30,  3.45it/s] 60%|█████▉    | 467/780 [02:48<01:30,  3.45it/s] 60%|██████    | 468/780 [02:49<01:30,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 00:35:44,070 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:35:44,070 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:35:44,070 >>   Batch size = 8
{'eval_loss': 1.025963544845581, 'eval_runtime': 9.4215, 'eval_samples_per_second': 371.172, 'eval_steps_per_second': 46.489, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.52it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.63it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.86it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.99it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.63it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.30it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.06it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.77it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.76it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.79it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.81it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.86it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 46.12it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.83it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.77it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.70it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.55it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.61it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.62it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.72it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.74it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.68it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.79it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.81it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.71it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.75it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.55it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.67it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.74it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.68it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.70it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.73it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.76it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.71it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.71it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.62it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.69it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.71it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.73it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.72it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.62it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.75it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.78it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.65it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.69it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.74it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.70it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.80it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.71it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.70it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.76it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.80it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.70it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.65it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.57it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.61it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.74it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.66it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.72it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.77it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.66it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.68it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.75it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.60it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.70it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.76it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.68it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.68it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.79it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.76it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.65it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.65it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.69it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.75it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.75it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.66it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.76it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.53it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.78it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.80it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.69it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.65it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.73it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.72it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.78it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.66it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.73it/s][A                                                 
                                                 [A 60%|██████    | 468/780 [02:58<01:30,  3.45it/s]
100%|██████████| 438/438 [00:09<00:00, 46.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:35:53,488 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 00:35:53,514 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:35:55,985 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:35:56,006 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:35:56,014 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:06<27:46,  5.36s/it] 60%|██████    | 470/780 [03:06<19:50,  3.84s/it] 60%|██████    | 471/780 [03:06<14:17,  2.77s/it] 61%|██████    | 472/780 [03:07<10:24,  2.03s/it] 61%|██████    | 473/780 [03:07<07:42,  1.51s/it] 61%|██████    | 474/780 [03:07<05:49,  1.14s/it] 61%|██████    | 475/780 [03:07<04:30,  1.13it/s] 61%|██████    | 476/780 [03:08<03:34,  1.42it/s] 61%|██████    | 477/780 [03:08<02:56,  1.72it/s] 61%|██████▏   | 478/780 [03:08<02:29,  2.03it/s] 61%|██████▏   | 479/780 [03:09<02:10,  2.31it/s] 62%|██████▏   | 480/780 [03:09<01:56,  2.57it/s] 62%|██████▏   | 481/780 [03:09<01:47,  2.78it/s] 62%|██████▏   | 482/780 [03:10<01:40,  2.95it/s] 62%|██████▏   | 483/780 [03:10<01:36,  3.09it/s] 62%|██████▏   | 484/780 [03:10<01:32,  3.19it/s] 62%|██████▏   | 485/780 [03:10<01:30,  3.27it/s] 62%|██████▏   | 486/780 [03:11<01:28,  3.32it/s] 62%|██████▏   | 487/780 [03:11<01:27,  3.36it/s] 63%|██████▎   | 488/780 [03:11<01:26,  3.39it/s] 63%|██████▎   | 489/780 [03:12<01:25,  3.41it/s] 63%|██████▎   | 490/780 [03:12<01:24,  3.43it/s] 63%|██████▎   | 491/780 [03:12<01:24,  3.43it/s] 63%|██████▎   | 492/780 [03:12<01:23,  3.43it/s] 63%|██████▎   | 493/780 [03:13<01:23,  3.44it/s] 63%|██████▎   | 494/780 [03:13<01:22,  3.45it/s] 63%|██████▎   | 495/780 [03:13<01:22,  3.45it/s] 64%|██████▎   | 496/780 [03:14<01:22,  3.45it/s] 64%|██████▎   | 497/780 [03:14<01:21,  3.46it/s] 64%|██████▍   | 498/780 [03:14<01:21,  3.46it/s] 64%|██████▍   | 499/780 [03:14<01:21,  3.46it/s] 64%|██████▍   | 500/780 [03:15<01:21,  3.46it/s]                                                  64%|██████▍   | 500/780 [03:15<01:21,  3.46it/s] 64%|██████▍   | 501/780 [03:15<01:20,  3.46it/s] 64%|██████▍   | 502/780 [03:15<01:20,  3.45it/s] 64%|██████▍   | 503/780 [03:16<01:20,  3.45it/s] 65%|██████▍   | 504/780 [03:16<01:20,  3.45it/s] 65%|██████▍   | 505/780 [03:16<01:19,  3.45it/s] 65%|██████▍   | 506/780 [03:16<01:19,  3.45it/s] 65%|██████▌   | 507/780 [03:17<01:18,  3.46it/s] 65%|██████▌   | 508/780 [03:17<01:18,  3.45it/s] 65%|██████▌   | 509/780 [03:17<01:18,  3.46it/s] 65%|██████▌   | 510/780 [03:18<01:18,  3.46it/s] 66%|██████▌   | 511/780 [03:18<01:17,  3.46it/s] 66%|██████▌   | 512/780 [03:18<01:17,  3.46it/s] 66%|██████▌   | 513/780 [03:18<01:17,  3.45it/s] 66%|██████▌   | 514/780 [03:19<01:17,  3.45it/s] 66%|██████▌   | 515/780 [03:19<01:16,  3.45it/s] 66%|██████▌   | 516/780 [03:19<01:16,  3.45it/s] 66%|██████▋   | 517/780 [03:20<01:16,  3.46it/s] 66%|██████▋   | 518/780 [03:20<01:15,  3.46it/s] 67%|██████▋   | 519/780 [03:20<01:15,  3.46it/s] 67%|██████▋   | 520/780 [03:21<01:15,  3.46it/s] 67%|██████▋   | 521/780 [03:21<01:14,  3.46it/s] 67%|██████▋   | 522/780 [03:21<01:14,  3.46it/s] 67%|██████▋   | 523/780 [03:21<01:14,  3.46it/s] 67%|██████▋   | 524/780 [03:22<01:13,  3.46it/s] 67%|██████▋   | 525/780 [03:22<01:14,  3.44it/s] 67%|██████▋   | 526/780 [03:22<01:13,  3.44it/s] 68%|██████▊   | 527/780 [03:23<01:13,  3.45it/s] 68%|██████▊   | 528/780 [03:23<01:13,  3.45it/s] 68%|██████▊   | 529/780 [03:23<01:12,  3.45it/s] 68%|██████▊   | 530/780 [03:23<01:12,  3.45it/s] 68%|██████▊   | 531/780 [03:24<01:12,  3.46it/s] 68%|██████▊   | 532/780 [03:24<01:11,  3.46it/s] 68%|██████▊   | 533/780 [03:24<01:11,  3.46it/s] 68%|██████▊   | 534/780 [03:25<01:11,  3.46it/s] 69%|██████▊   | 535/780 [03:25<01:10,  3.46it/s] 69%|██████▊   | 536/780 [03:25<01:10,  3.46it/s] 69%|██████▉   | 537/780 [03:25<01:10,  3.45it/s] 69%|██████▉   | 538/780 [03:26<01:10,  3.45it/s] 69%|██████▉   | 539/780 [03:26<01:09,  3.46it/s] 69%|██████▉   | 540/780 [03:26<01:09,  3.46it/s] 69%|██████▉   | 541/780 [03:27<01:09,  3.45it/s] 69%|██████▉   | 542/780 [03:27<01:08,  3.45it/s] 70%|██████▉   | 543/780 [03:27<01:08,  3.45it/s] 70%|██████▉   | 544/780 [03:27<01:08,  3.45it/s] 70%|██████▉   | 545/780 [03:28<01:08,  3.45it/s] 70%|███████   | 546/780 [03:28<01:07,  3.45it/s] 70%|███████   | 547/780 [03:28<01:07,  3.45it/s] 70%|███████   | 548/780 [03:29<01:07,  3.45it/s] 70%|███████   | 549/780 [03:29<01:06,  3.45it/s] 71%|███████   | 550/780 [03:29<01:06,  3.45it/s] 71%|███████   | 551/780 [03:29<01:06,  3.45it/s] 71%|███████   | 552/780 [03:30<01:05,  3.46it/s] 71%|███████   | 553/780 [03:30<01:05,  3.46it/s] 71%|███████   | 554/780 [03:30<01:05,  3.46it/s] 71%|███████   | 555/780 [03:31<01:05,  3.45it/s] 71%|███████▏  | 556/780 [03:31<01:04,  3.45it/s] 71%|███████▏  | 557/780 [03:31<01:04,  3.45it/s] 72%|███████▏  | 558/780 [03:32<01:04,  3.45it/s] 72%|███████▏  | 559/780 [03:32<01:04,  3.45it/s] 72%|███████▏  | 560/780 [03:32<01:03,  3.45it/s] 72%|███████▏  | 561/780 [03:32<01:03,  3.45it/s] 72%|███████▏  | 562/780 [03:33<01:03,  3.45it/s] 72%|███████▏  | 563/780 [03:33<01:02,  3.45it/s] 72%|███████▏  | 564/780 [03:33<01:02,  3.45it/s] 72%|███████▏  | 565/780 [03:34<01:02,  3.46it/s] 73%|███████▎  | 566/780 [03:34<01:01,  3.46it/s] 73%|███████▎  | 567/780 [03:34<01:01,  3.46it/s] 73%|███████▎  | 568/780 [03:34<01:01,  3.46it/s] 73%|███████▎  | 569/780 [03:35<01:01,  3.44it/s] 73%|███████▎  | 570/780 [03:35<01:00,  3.45it/s] 73%|███████▎  | 571/780 [03:35<01:00,  3.45it/s] 73%|███████▎  | 572/780 [03:36<01:00,  3.45it/s] 73%|███████▎  | 573/780 [03:36<00:59,  3.45it/s] 74%|███████▎  | 574/780 [03:36<00:59,  3.45it/s] 74%|███████▎  | 575/780 [03:36<00:59,  3.45it/s] 74%|███████▍  | 576/780 [03:37<00:59,  3.46it/s] 74%|███████▍  | 577/780 [03:37<00:58,  3.46it/s] 74%|███████▍  | 578/780 [03:37<00:58,  3.45it/s] 74%|███████▍  | 579/780 [03:38<00:58,  3.45it/s] 74%|███████▍  | 580/780 [03:38<00:58,  3.44it/s] 74%|███████▍  | 581/780 [03:38<00:57,  3.45it/s] 75%|███████▍  | 582/780 [03:38<00:57,  3.45it/s] 75%|███████▍  | 583/780 [03:39<00:57,  3.45it/s] 75%|███████▍  | 584/780 [03:39<00:56,  3.45it/s] 75%|███████▌  | 585/780 [03:39<00:56,  3.45it/s] 75%|███████▌  | 586/780 [03:40<00:56,  3.45it/s] 75%|███████▌  | 587/780 [03:40<00:55,  3.46it/s] 75%|███████▌  | 588/780 [03:40<00:55,  3.45it/s] 76%|███████▌  | 589/780 [03:40<00:55,  3.45it/s] 76%|███████▌  | 590/780 [03:41<00:55,  3.45it/s] 76%|███████▌  | 591/780 [03:41<00:54,  3.45it/s] 76%|███████▌  | 592/780 [03:41<00:54,  3.45it/s] 76%|███████▌  | 593/780 [03:42<00:54,  3.45it/s] 76%|███████▌  | 594/780 [03:42<00:53,  3.45it/s] 76%|███████▋  | 595/780 [03:42<00:53,  3.45it/s] 76%|███████▋  | 596/780 [03:43<00:53,  3.45it/s] 77%|███████▋  | 597/780 [03:43<00:52,  3.45it/s] 77%|███████▋  | 598/780 [03:43<00:52,  3.46it/s] 77%|███████▋  | 599/780 [03:43<00:52,  3.45it/s] 77%|███████▋  | 600/780 [03:44<00:52,  3.45it/s] 77%|███████▋  | 601/780 [03:44<00:51,  3.45it/s] 77%|███████▋  | 602/780 [03:44<00:51,  3.44it/s] 77%|███████▋  | 603/780 [03:45<00:51,  3.45it/s] 77%|███████▋  | 604/780 [03:45<00:51,  3.45it/s] 78%|███████▊  | 605/780 [03:45<00:50,  3.45it/s] 78%|███████▊  | 606/780 [03:45<00:50,  3.45it/s] 78%|███████▊  | 607/780 [03:46<00:50,  3.45it/s] 78%|███████▊  | 608/780 [03:46<00:49,  3.45it/s] 78%|███████▊  | 609/780 [03:46<00:49,  3.45it/s] 78%|███████▊  | 610/780 [03:47<00:49,  3.45it/s] 78%|███████▊  | 611/780 [03:47<00:48,  3.45it/s] 78%|███████▊  | 612/780 [03:47<00:48,  3.45it/s] 79%|███████▊  | 613/780 [03:47<00:48,  3.44it/s] 79%|███████▊  | 614/780 [03:48<00:48,  3.45it/s] 79%|███████▉  | 615/780 [03:48<00:47,  3.45it/s] 79%|███████▉  | 616/780 [03:48<00:47,  3.45it/s] 79%|███████▉  | 617/780 [03:49<00:47,  3.44it/s] 79%|███████▉  | 618/780 [03:49<00:47,  3.44it/s] 79%|███████▉  | 619/780 [03:49<00:46,  3.45it/s] 79%|███████▉  | 620/780 [03:49<00:46,  3.45it/s] 80%|███████▉  | 621/780 [03:50<00:46,  3.45it/s] 80%|███████▉  | 622/780 [03:50<00:47,  3.31it/s] 80%|███████▉  | 623/780 [03:50<00:46,  3.35it/s] 80%|████████  | 624/780 [03:51<00:47,  3.29it/s][INFO|trainer.py:2140] 2023-08-29 00:36:46,215 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:36:46,215 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:36:46,215 >>   Batch size = 8
{'eval_loss': 1.0405246019363403, 'eval_runtime': 9.3934, 'eval_samples_per_second': 372.282, 'eval_steps_per_second': 46.628, 'epoch': 3.0}
{'loss': 0.5249, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.25it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.48it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.66it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.90it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.60it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.28it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.07it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.91it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.85it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.89it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.76it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.67it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.76it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.78it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.75it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.80it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.71it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.65it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.74it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.71it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.67it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.65it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.70it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.68it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.74it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.74it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.71it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.68it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.64it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.77it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.69it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.66it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.67it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.66it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.70it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.66it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.65it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.68it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.75it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.73it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.64it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.65it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.67it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.71it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.73it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.64it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.65it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.69it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.72it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.71it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.67it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.63it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.64it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.59it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.62it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.67it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.65it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.65it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.63it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.56it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.60it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.56it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.60it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.65it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.67it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.67it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.61it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.59it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.68it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.68it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.64it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.69it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.65it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.67it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.60it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.62it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.61it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.69it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.61it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.66it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.59it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.65it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.63it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.67it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.68it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.71it/s][A                                                 
                                                 [A 80%|████████  | 624/780 [04:00<00:47,  3.29it/s]
100%|██████████| 438/438 [00:09<00:00, 46.71it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:36:55,638 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 00:36:55,657 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:36:57,993 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:36:58,018 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:36:58,024 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:08<13:42,  5.30s/it] 80%|████████  | 626/780 [04:08<09:45,  3.80s/it] 80%|████████  | 627/780 [04:08<07:00,  2.75s/it] 81%|████████  | 628/780 [04:09<05:05,  2.01s/it] 81%|████████  | 629/780 [04:09<03:45,  1.49s/it] 81%|████████  | 630/780 [04:09<02:49,  1.13s/it] 81%|████████  | 631/780 [04:09<02:10,  1.14it/s] 81%|████████  | 632/780 [04:10<01:43,  1.42it/s] 81%|████████  | 633/780 [04:10<01:24,  1.73it/s] 81%|████████▏ | 634/780 [04:10<01:11,  2.04it/s] 81%|████████▏ | 635/780 [04:11<01:02,  2.32it/s] 82%|████████▏ | 636/780 [04:11<00:55,  2.58it/s] 82%|████████▏ | 637/780 [04:11<00:51,  2.78it/s] 82%|████████▏ | 638/780 [04:11<00:48,  2.96it/s] 82%|████████▏ | 639/780 [04:12<00:45,  3.09it/s] 82%|████████▏ | 640/780 [04:12<00:43,  3.19it/s] 82%|████████▏ | 641/780 [04:12<00:42,  3.27it/s] 82%|████████▏ | 642/780 [04:13<00:41,  3.32it/s] 82%|████████▏ | 643/780 [04:13<00:40,  3.36it/s] 83%|████████▎ | 644/780 [04:13<00:40,  3.39it/s] 83%|████████▎ | 645/780 [04:13<00:39,  3.41it/s] 83%|████████▎ | 646/780 [04:14<00:39,  3.43it/s] 83%|████████▎ | 647/780 [04:14<00:38,  3.44it/s] 83%|████████▎ | 648/780 [04:14<00:38,  3.43it/s] 83%|████████▎ | 649/780 [04:15<00:38,  3.44it/s] 83%|████████▎ | 650/780 [04:15<00:37,  3.44it/s] 83%|████████▎ | 651/780 [04:15<00:37,  3.45it/s] 84%|████████▎ | 652/780 [04:15<00:37,  3.45it/s] 84%|████████▎ | 653/780 [04:16<00:36,  3.45it/s] 84%|████████▍ | 654/780 [04:16<00:36,  3.46it/s] 84%|████████▍ | 655/780 [04:16<00:36,  3.46it/s] 84%|████████▍ | 656/780 [04:17<00:35,  3.46it/s] 84%|████████▍ | 657/780 [04:17<00:35,  3.46it/s] 84%|████████▍ | 658/780 [04:17<00:35,  3.46it/s] 84%|████████▍ | 659/780 [04:18<00:35,  3.42it/s] 85%|████████▍ | 660/780 [04:18<00:34,  3.43it/s] 85%|████████▍ | 661/780 [04:18<00:34,  3.44it/s] 85%|████████▍ | 662/780 [04:18<00:34,  3.45it/s] 85%|████████▌ | 663/780 [04:19<00:33,  3.45it/s] 85%|████████▌ | 664/780 [04:19<00:33,  3.45it/s] 85%|████████▌ | 665/780 [04:19<00:33,  3.45it/s] 85%|████████▌ | 666/780 [04:20<00:32,  3.46it/s] 86%|████████▌ | 667/780 [04:20<00:32,  3.46it/s] 86%|████████▌ | 668/780 [04:20<00:32,  3.46it/s] 86%|████████▌ | 669/780 [04:20<00:32,  3.46it/s] 86%|████████▌ | 670/780 [04:21<00:31,  3.45it/s] 86%|████████▌ | 671/780 [04:21<00:31,  3.45it/s] 86%|████████▌ | 672/780 [04:21<00:31,  3.45it/s] 86%|████████▋ | 673/780 [04:22<00:31,  3.45it/s] 86%|████████▋ | 674/780 [04:22<00:30,  3.45it/s] 87%|████████▋ | 675/780 [04:22<00:30,  3.45it/s] 87%|████████▋ | 676/780 [04:22<00:30,  3.46it/s] 87%|████████▋ | 677/780 [04:23<00:29,  3.46it/s] 87%|████████▋ | 678/780 [04:23<00:29,  3.46it/s] 87%|████████▋ | 679/780 [04:23<00:29,  3.46it/s] 87%|████████▋ | 680/780 [04:24<00:28,  3.46it/s] 87%|████████▋ | 681/780 [04:24<00:28,  3.44it/s] 87%|████████▋ | 682/780 [04:24<00:28,  3.45it/s] 88%|████████▊ | 683/780 [04:24<00:28,  3.45it/s] 88%|████████▊ | 684/780 [04:25<00:27,  3.45it/s] 88%|████████▊ | 685/780 [04:25<00:27,  3.45it/s] 88%|████████▊ | 686/780 [04:25<00:27,  3.46it/s] 88%|████████▊ | 687/780 [04:26<00:26,  3.46it/s] 88%|████████▊ | 688/780 [04:26<00:26,  3.46it/s] 88%|████████▊ | 689/780 [04:26<00:26,  3.46it/s] 88%|████████▊ | 690/780 [04:26<00:26,  3.46it/s] 89%|████████▊ | 691/780 [04:27<00:25,  3.46it/s] 89%|████████▊ | 692/780 [04:27<00:25,  3.46it/s] 89%|████████▉ | 693/780 [04:27<00:25,  3.46it/s] 89%|████████▉ | 694/780 [04:28<00:24,  3.46it/s] 89%|████████▉ | 695/780 [04:28<00:24,  3.46it/s] 89%|████████▉ | 696/780 [04:28<00:24,  3.45it/s] 89%|████████▉ | 697/780 [04:29<00:24,  3.45it/s] 89%|████████▉ | 698/780 [04:29<00:23,  3.45it/s] 90%|████████▉ | 699/780 [04:29<00:23,  3.45it/s] 90%|████████▉ | 700/780 [04:29<00:23,  3.45it/s] 90%|████████▉ | 701/780 [04:30<00:22,  3.45it/s] 90%|█████████ | 702/780 [04:30<00:22,  3.45it/s] 90%|█████████ | 703/780 [04:30<00:22,  3.45it/s] 90%|█████████ | 704/780 [04:31<00:22,  3.45it/s] 90%|█████████ | 705/780 [04:31<00:21,  3.45it/s] 91%|█████████ | 706/780 [04:31<00:21,  3.45it/s] 91%|█████████ | 707/780 [04:31<00:21,  3.45it/s] 91%|█████████ | 708/780 [04:32<00:20,  3.45it/s] 91%|█████████ | 709/780 [04:32<00:20,  3.45it/s] 91%|█████████ | 710/780 [04:32<00:20,  3.45it/s] 91%|█████████ | 711/780 [04:33<00:19,  3.46it/s] 91%|█████████▏| 712/780 [04:33<00:19,  3.45it/s] 91%|█████████▏| 713/780 [04:33<00:19,  3.45it/s] 92%|█████████▏| 714/780 [04:33<00:19,  3.45it/s] 92%|█████████▏| 715/780 [04:34<00:18,  3.45it/s] 92%|█████████▏| 716/780 [04:34<00:18,  3.45it/s] 92%|█████████▏| 717/780 [04:34<00:18,  3.46it/s] 92%|█████████▏| 718/780 [04:35<00:17,  3.45it/s] 92%|█████████▏| 719/780 [04:35<00:17,  3.46it/s] 92%|█████████▏| 720/780 [04:35<00:17,  3.45it/s] 92%|█████████▏| 721/780 [04:35<00:17,  3.46it/s] 93%|█████████▎| 722/780 [04:36<00:16,  3.46it/s] 93%|█████████▎| 723/780 [04:36<00:16,  3.44it/s] 93%|█████████▎| 724/780 [04:36<00:16,  3.44it/s] 93%|█████████▎| 725/780 [04:37<00:15,  3.45it/s] 93%|█████████▎| 726/780 [04:37<00:15,  3.45it/s] 93%|█████████▎| 727/780 [04:37<00:15,  3.45it/s] 93%|█████████▎| 728/780 [04:38<00:15,  3.45it/s] 93%|█████████▎| 729/780 [04:38<00:14,  3.45it/s] 94%|█████████▎| 730/780 [04:38<00:14,  3.46it/s] 94%|█████████▎| 731/780 [04:38<00:14,  3.46it/s] 94%|█████████▍| 732/780 [04:39<00:13,  3.45it/s] 94%|█████████▍| 733/780 [04:39<00:13,  3.46it/s] 94%|█████████▍| 734/780 [04:39<00:13,  3.42it/s] 94%|█████████▍| 735/780 [04:40<00:13,  3.43it/s] 94%|█████████▍| 736/780 [04:40<00:12,  3.44it/s] 94%|█████████▍| 737/780 [04:40<00:12,  3.44it/s] 95%|█████████▍| 738/780 [04:40<00:12,  3.45it/s] 95%|█████████▍| 739/780 [04:41<00:11,  3.45it/s] 95%|█████████▍| 740/780 [04:41<00:11,  3.45it/s] 95%|█████████▌| 741/780 [04:41<00:11,  3.45it/s] 95%|█████████▌| 742/780 [04:42<00:11,  3.45it/s] 95%|█████████▌| 743/780 [04:42<00:10,  3.45it/s] 95%|█████████▌| 744/780 [04:42<00:10,  3.46it/s] 96%|█████████▌| 745/780 [04:42<00:10,  3.44it/s] 96%|█████████▌| 746/780 [04:43<00:09,  3.45it/s] 96%|█████████▌| 747/780 [04:43<00:09,  3.45it/s] 96%|█████████▌| 748/780 [04:43<00:09,  3.45it/s] 96%|█████████▌| 749/780 [04:44<00:08,  3.45it/s] 96%|█████████▌| 750/780 [04:44<00:08,  3.45it/s] 96%|█████████▋| 751/780 [04:44<00:08,  3.46it/s] 96%|█████████▋| 752/780 [04:44<00:08,  3.45it/s] 97%|█████████▋| 753/780 [04:45<00:07,  3.45it/s] 97%|█████████▋| 754/780 [04:45<00:07,  3.45it/s] 97%|█████████▋| 755/780 [04:45<00:07,  3.46it/s] 97%|█████████▋| 756/780 [04:46<00:06,  3.45it/s] 97%|█████████▋| 757/780 [04:46<00:06,  3.45it/s] 97%|█████████▋| 758/780 [04:46<00:06,  3.45it/s] 97%|█████████▋| 759/780 [04:46<00:06,  3.45it/s] 97%|█████████▋| 760/780 [04:47<00:05,  3.45it/s] 98%|█████████▊| 761/780 [04:47<00:05,  3.45it/s] 98%|█████████▊| 762/780 [04:47<00:05,  3.45it/s] 98%|█████████▊| 763/780 [04:48<00:04,  3.45it/s] 98%|█████████▊| 764/780 [04:48<00:04,  3.46it/s] 98%|█████████▊| 765/780 [04:48<00:04,  3.46it/s] 98%|█████████▊| 766/780 [04:49<00:04,  3.46it/s] 98%|█████████▊| 767/780 [04:49<00:03,  3.40it/s] 98%|█████████▊| 768/780 [04:49<00:03,  3.42it/s] 99%|█████████▊| 769/780 [04:49<00:03,  3.43it/s] 99%|█████████▊| 770/780 [04:50<00:02,  3.44it/s] 99%|█████████▉| 771/780 [04:50<00:02,  3.44it/s] 99%|█████████▉| 772/780 [04:50<00:02,  3.29it/s] 99%|█████████▉| 773/780 [04:51<00:02,  3.32it/s] 99%|█████████▉| 774/780 [04:51<00:01,  3.36it/s] 99%|█████████▉| 775/780 [04:51<00:01,  3.39it/s] 99%|█████████▉| 776/780 [04:51<00:01,  3.41it/s]100%|█████████▉| 777/780 [04:52<00:00,  3.42it/s]100%|█████████▉| 778/780 [04:52<00:00,  3.41it/s]100%|█████████▉| 779/780 [04:52<00:00,  3.42it/s]100%|██████████| 780/780 [04:53<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 00:37:48,109 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:37:48,109 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:37:48,109 >>   Batch size = 8
{'eval_loss': 1.0498472452163696, 'eval_runtime': 9.3998, 'eval_samples_per_second': 372.03, 'eval_steps_per_second': 46.597, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.21it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.65it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.73it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.11it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.65it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.26it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.10it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.87it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.82it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.76it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.78it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.72it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.79it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.75it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.72it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.70it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.74it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.69it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.52it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.76it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.65it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.58it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.72it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.68it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.74it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.71it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.71it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.73it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.72it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.73it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.72it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.69it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.67it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.71it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.73it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.69it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.71it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.68it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.64it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.74it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.68it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.67it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.65it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.69it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.70it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.78it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.75it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.70it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.68it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.63it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.70it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.73it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.66it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.72it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.68it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.68it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.71it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.68it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.60it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.67it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.63it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.72it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.62it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.69it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.66it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.58it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.73it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.72it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.56it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.63it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.68it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.61it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.64it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.65it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.73it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.75it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.65it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.68it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.61it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.65it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.72it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.69it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.68it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.72it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.73it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.85it/s][A                                                 
                                                 [A100%|██████████| 780/780 [05:02<00:00,  3.43it/s]
100%|██████████| 438/438 [00:09<00:00, 46.85it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:37:57,502 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 00:37:57,517 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:37:59,768 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:37:59,784 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:37:59,797 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:38:04,347 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:38:04,350 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156 (score: 1.007078766822815).
                                                 100%|██████████| 780/780 [05:11<00:00,  3.43it/s]100%|██████████| 780/780 [05:11<00:00,  2.51it/s]
[INFO|trainer.py:1894] 2023-08-29 00:38:06,094 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 00:38:06,109 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:38:08,138 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:38:08,154 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:38:08,162 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:38:08,344 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:08,345 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:08,345 >>   train_loss               =     0.5154
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:08,345 >>   train_runtime            = 0:05:11.12
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:08,345 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:08,345 >>   train_samples_per_second =    160.689
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:08,345 >>   train_steps_per_second   =      2.507
{'eval_loss': 1.0549603700637817, 'eval_runtime': 9.3719, 'eval_samples_per_second': 373.137, 'eval_steps_per_second': 46.735, 'epoch': 5.0}
{'train_runtime': 311.1285, 'train_samples_per_second': 160.689, 'train_steps_per_second': 2.507, 'train_loss': 0.5153674003405448, 'epoch': 5.0}
08/29/2023 00:38:08 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:38:08,388 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:38:08,388 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 00:38:08,388 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 58.71it/s]  3%|▎         | 12/438 [00:00<00:08, 51.19it/s]  4%|▍         | 18/438 [00:00<00:08, 49.26it/s]  5%|▌         | 23/438 [00:00<00:08, 48.44it/s]  6%|▋         | 28/438 [00:00<00:08, 47.91it/s]  8%|▊         | 33/438 [00:00<00:08, 47.53it/s]  9%|▊         | 38/438 [00:00<00:08, 47.38it/s] 10%|▉         | 43/438 [00:00<00:08, 47.23it/s] 11%|█         | 48/438 [00:00<00:08, 47.17it/s] 12%|█▏        | 53/438 [00:01<00:08, 46.99it/s] 13%|█▎        | 58/438 [00:01<00:08, 47.01it/s] 14%|█▍        | 63/438 [00:01<00:07, 46.95it/s] 16%|█▌        | 68/438 [00:01<00:07, 46.99it/s] 17%|█▋        | 73/438 [00:01<00:07, 46.94it/s] 18%|█▊        | 78/438 [00:01<00:07, 47.01it/s] 19%|█▉        | 83/438 [00:01<00:07, 46.90it/s] 20%|██        | 88/438 [00:01<00:07, 46.93it/s] 21%|██        | 93/438 [00:01<00:07, 46.98it/s] 22%|██▏       | 98/438 [00:02<00:07, 46.97it/s] 24%|██▎       | 103/438 [00:02<00:07, 46.91it/s] 25%|██▍       | 108/438 [00:02<00:07, 46.99it/s] 26%|██▌       | 113/438 [00:02<00:06, 46.89it/s] 27%|██▋       | 118/438 [00:02<00:06, 46.88it/s] 28%|██▊       | 123/438 [00:02<00:06, 46.96it/s] 29%|██▉       | 128/438 [00:02<00:06, 46.99it/s] 30%|███       | 133/438 [00:02<00:06, 46.85it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.91it/s] 33%|███▎      | 143/438 [00:03<00:06, 46.94it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.91it/s] 35%|███▍      | 153/438 [00:03<00:06, 46.92it/s] 36%|███▌      | 158/438 [00:03<00:05, 46.86it/s] 37%|███▋      | 163/438 [00:03<00:05, 46.91it/s] 38%|███▊      | 168/438 [00:03<00:05, 46.94it/s] 39%|███▉      | 173/438 [00:03<00:05, 46.96it/s] 41%|████      | 178/438 [00:03<00:05, 46.86it/s] 42%|████▏     | 183/438 [00:03<00:05, 46.90it/s] 43%|████▎     | 188/438 [00:03<00:05, 46.88it/s] 44%|████▍     | 193/438 [00:04<00:05, 46.83it/s] 45%|████▌     | 198/438 [00:04<00:05, 46.93it/s] 46%|████▋     | 203/438 [00:04<00:05, 46.84it/s] 47%|████▋     | 208/438 [00:04<00:04, 46.81it/s] 49%|████▊     | 213/438 [00:04<00:04, 46.85it/s] 50%|████▉     | 218/438 [00:04<00:04, 46.90it/s] 51%|█████     | 223/438 [00:04<00:04, 46.91it/s] 52%|█████▏    | 228/438 [00:04<00:04, 46.99it/s] 53%|█████▎    | 233/438 [00:04<00:04, 46.90it/s] 54%|█████▍    | 238/438 [00:05<00:04, 46.82it/s] 55%|█████▌    | 243/438 [00:05<00:04, 46.87it/s] 57%|█████▋    | 248/438 [00:05<00:04, 46.85it/s] 58%|█████▊    | 253/438 [00:05<00:03, 46.79it/s] 59%|█████▉    | 258/438 [00:05<00:03, 46.82it/s] 60%|██████    | 263/438 [00:05<00:03, 46.81it/s] 61%|██████    | 268/438 [00:05<00:03, 46.87it/s] 62%|██████▏   | 273/438 [00:05<00:03, 46.94it/s] 63%|██████▎   | 278/438 [00:05<00:03, 46.84it/s] 65%|██████▍   | 283/438 [00:06<00:03, 46.95it/s] 66%|██████▌   | 288/438 [00:06<00:03, 46.82it/s] 67%|██████▋   | 293/438 [00:06<00:03, 46.67it/s] 68%|██████▊   | 298/438 [00:06<00:02, 46.77it/s] 69%|██████▉   | 303/438 [00:06<00:02, 46.81it/s] 70%|███████   | 308/438 [00:06<00:02, 46.76it/s] 71%|███████▏  | 313/438 [00:06<00:02, 46.90it/s] 73%|███████▎  | 318/438 [00:06<00:02, 46.88it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.83it/s] 75%|███████▍  | 328/438 [00:06<00:02, 46.87it/s] 76%|███████▌  | 333/438 [00:07<00:02, 46.87it/s] 77%|███████▋  | 338/438 [00:07<00:02, 46.87it/s] 78%|███████▊  | 343/438 [00:07<00:02, 46.74it/s] 79%|███████▉  | 348/438 [00:07<00:01, 46.84it/s] 81%|████████  | 353/438 [00:07<00:01, 46.73it/s] 82%|████████▏ | 358/438 [00:07<00:01, 46.88it/s] 83%|████████▎ | 363/438 [00:07<00:01, 46.98it/s] 84%|████████▍ | 368/438 [00:07<00:01, 46.79it/s] 85%|████████▌ | 373/438 [00:07<00:01, 46.86it/s] 86%|████████▋ | 378/438 [00:08<00:01, 46.85it/s] 87%|████████▋ | 383/438 [00:08<00:01, 46.83it/s] 89%|████████▊ | 388/438 [00:08<00:01, 46.86it/s] 90%|████████▉ | 393/438 [00:08<00:00, 46.86it/s] 91%|█████████ | 398/438 [00:08<00:00, 46.76it/s] 92%|█████████▏| 403/438 [00:08<00:00, 46.81it/s] 93%|█████████▎| 408/438 [00:08<00:00, 46.89it/s] 94%|█████████▍| 413/438 [00:08<00:00, 46.82it/s] 95%|█████████▌| 418/438 [00:08<00:00, 46.86it/s] 97%|█████████▋| 423/438 [00:08<00:00, 46.87it/s] 98%|█████████▊| 428/438 [00:09<00:00, 46.79it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.78it/s]100%|██████████| 438/438 [00:09<00:00, 46.90it/s]100%|██████████| 438/438 [00:09<00:00, 47.00it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:38:17,730 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:17,730 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:17,730 >>   eval_loss               =     1.0071
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:17,730 >>   eval_runtime            = 0:00:09.34
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:17,730 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:17,730 >>   eval_samples_per_second =    374.345
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:17,730 >>   eval_steps_per_second   =     46.887
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:38:17,730 >>   perplexity              =     2.7376
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:24,518 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:24,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:24,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:24,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:24,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:38:25,173 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:38:25,174 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:38:25,847 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:38:26,867 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:38:26,870 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:30,009 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:30,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:30,012 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:30,013 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:38:30,013 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:38:30,716 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:38:30,717 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:38:31,320 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:38:31,478 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:38:31,478 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.72it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.55it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:06,  1.53it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.56it/s]Extractor Predicting: 15it [00:09,  1.57it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.61it/s]Extractor Predicting: 27it [00:17,  1.63it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.57it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.54it/s]Extractor Predicting: 39it [00:24,  1.56it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.56it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:31,  1.53it/s]Extractor Predicting: 50it [00:31,  1.50it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.48it/s]Extractor Predicting: 53it [00:33,  1.50it/s]Extractor Predicting: 54it [00:34,  1.50it/s]Extractor Predicting: 55it [00:35,  1.51it/s]Extractor Predicting: 56it [00:35,  1.52it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:38,  1.50it/s]Extractor Predicting: 61it [00:39,  1.48it/s]Extractor Predicting: 62it [00:39,  1.46it/s]Extractor Predicting: 63it [00:40,  1.45it/s]Extractor Predicting: 64it [00:41,  1.46it/s]Extractor Predicting: 65it [00:42,  1.44it/s]Extractor Predicting: 66it [00:42,  1.43it/s]Extractor Predicting: 67it [00:43,  1.42it/s]Extractor Predicting: 68it [00:44,  1.42it/s]Extractor Predicting: 69it [00:45,  1.31it/s]Extractor Predicting: 70it [00:45,  1.34it/s]Extractor Predicting: 71it [00:46,  1.35it/s]Extractor Predicting: 72it [00:47,  1.36it/s]Extractor Predicting: 73it [00:48,  1.36it/s]Extractor Predicting: 74it [00:48,  1.37it/s]Extractor Predicting: 75it [00:49,  1.36it/s]Extractor Predicting: 76it [00:50,  1.39it/s]Extractor Predicting: 77it [00:50,  1.39it/s]Extractor Predicting: 78it [00:51,  1.39it/s]Extractor Predicting: 79it [00:52,  1.41it/s]Extractor Predicting: 80it [00:53,  1.39it/s]Extractor Predicting: 81it [00:53,  1.37it/s]Extractor Predicting: 82it [00:54,  1.37it/s]Extractor Predicting: 83it [00:55,  1.42it/s]Extractor Predicting: 84it [00:55,  1.42it/s]Extractor Predicting: 85it [00:56,  1.43it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:57,  1.39it/s]Extractor Predicting: 88it [00:58,  1.41it/s]Extractor Predicting: 89it [00:59,  1.43it/s]Extractor Predicting: 90it [01:00,  1.45it/s]Extractor Predicting: 91it [01:00,  1.47it/s]Extractor Predicting: 92it [01:01,  1.44it/s]Extractor Predicting: 93it [01:02,  1.47it/s]Extractor Predicting: 94it [01:02,  1.48it/s]Extractor Predicting: 95it [01:03,  1.48it/s]Extractor Predicting: 96it [01:04,  1.47it/s]Extractor Predicting: 97it [01:04,  1.48it/s]Extractor Predicting: 98it [01:05,  1.47it/s]Extractor Predicting: 99it [01:06,  1.48it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:07,  1.50it/s]Extractor Predicting: 102it [01:08,  1.47it/s]Extractor Predicting: 103it [01:08,  1.43it/s]Extractor Predicting: 104it [01:09,  1.45it/s]Extractor Predicting: 105it [01:10,  1.46it/s]Extractor Predicting: 106it [01:10,  1.46it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:12,  1.45it/s]Extractor Predicting: 110it [01:13,  1.44it/s]Extractor Predicting: 111it [01:14,  1.44it/s]Extractor Predicting: 112it [01:15,  1.44it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:16,  1.45it/s]Extractor Predicting: 115it [01:17,  1.45it/s]Extractor Predicting: 116it [01:17,  1.44it/s]Extractor Predicting: 117it [01:18,  1.48it/s]Extractor Predicting: 118it [01:19,  1.50it/s]Extractor Predicting: 119it [01:19,  1.54it/s]Extractor Predicting: 120it [01:20,  1.56it/s]Extractor Predicting: 121it [01:21,  1.54it/s]Extractor Predicting: 122it [01:21,  1.59it/s]Extractor Predicting: 123it [01:22,  1.60it/s]Extractor Predicting: 124it [01:22,  1.58it/s]Extractor Predicting: 125it [01:23,  1.60it/s]Extractor Predicting: 126it [01:24,  1.62it/s]Extractor Predicting: 127it [01:24,  1.65it/s]Extractor Predicting: 128it [01:25,  1.64it/s]Extractor Predicting: 129it [01:25,  1.66it/s]Extractor Predicting: 130it [01:26,  1.60it/s]Extractor Predicting: 131it [01:27,  1.61it/s]Extractor Predicting: 132it [01:27,  1.67it/s]Extractor Predicting: 133it [01:28,  1.68it/s]Extractor Predicting: 134it [01:28,  1.60it/s]Extractor Predicting: 135it [01:29,  1.62it/s]Extractor Predicting: 136it [01:30,  1.66it/s]Extractor Predicting: 137it [01:30,  1.66it/s]Extractor Predicting: 138it [01:31,  1.65it/s]Extractor Predicting: 139it [01:31,  1.65it/s]Extractor Predicting: 140it [01:32,  1.70it/s]Extractor Predicting: 141it [01:33,  1.70it/s]Extractor Predicting: 142it [01:33,  1.73it/s]Extractor Predicting: 143it [01:34,  1.70it/s]Extractor Predicting: 144it [01:34,  1.67it/s]Extractor Predicting: 145it [01:35,  1.85it/s]Extractor Predicting: 145it [01:35,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:14,926 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:14,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:14,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:14,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:14,930 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:40:15,231 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:40:15,232 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:40:15,500 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:40:16,523 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:40:16,523 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:18,649 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:18,653 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:18,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:18,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:40:18,654 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:40:18,977 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:40:18,978 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:40:19,247 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:40:19,397 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:40:19,397 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.7148834429169157,
  "recall": 0.3420074349442379,
  "score": 0.4626692456479691,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.52it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:18,  1.46it/s]Extractor Predicting: 28it [00:18,  1.46it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.42it/s]Extractor Predicting: 31it [00:20,  1.43it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:22,  1.46it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.49it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.54it/s]Extractor Predicting: 39it [00:26,  1.53it/s]Extractor Predicting: 40it [00:26,  1.55it/s]Extractor Predicting: 41it [00:27,  1.54it/s]Extractor Predicting: 42it [00:28,  1.53it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:29,  1.52it/s]Extractor Predicting: 45it [00:30,  1.50it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:31,  1.53it/s]Extractor Predicting: 48it [00:32,  1.53it/s]Extractor Predicting: 49it [00:32,  1.52it/s]Extractor Predicting: 50it [00:33,  1.50it/s]Extractor Predicting: 51it [00:34,  1.51it/s]Extractor Predicting: 52it [00:34,  1.51it/s]Extractor Predicting: 53it [00:35,  1.52it/s]Extractor Predicting: 54it [00:36,  1.53it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:37,  1.37it/s]Extractor Predicting: 57it [00:38,  1.43it/s]Extractor Predicting: 58it [00:38,  1.48it/s]Extractor Predicting: 59it [00:39,  1.50it/s]Extractor Predicting: 60it [00:40,  1.51it/s]Extractor Predicting: 61it [00:40,  1.50it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:42,  1.50it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.48it/s]Extractor Predicting: 67it [00:44,  1.50it/s]Extractor Predicting: 68it [00:45,  1.54it/s]Extractor Predicting: 69it [00:46,  1.55it/s]Extractor Predicting: 70it [00:46,  1.54it/s]Extractor Predicting: 71it [00:47,  1.53it/s]Extractor Predicting: 72it [00:48,  1.54it/s]Extractor Predicting: 73it [00:48,  1.49it/s]Extractor Predicting: 74it [00:49,  1.50it/s]Extractor Predicting: 75it [00:50,  1.51it/s]Extractor Predicting: 76it [00:50,  1.53it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:52,  1.48it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:53,  1.48it/s]Extractor Predicting: 81it [00:54,  1.50it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:55,  1.48it/s]Extractor Predicting: 84it [00:56,  1.48it/s]Extractor Predicting: 85it [00:56,  1.50it/s]Extractor Predicting: 86it [00:57,  1.49it/s]Extractor Predicting: 87it [00:58,  1.48it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:00,  1.50it/s]Extractor Predicting: 92it [01:01,  1.49it/s]Extractor Predicting: 93it [01:02,  1.47it/s]Extractor Predicting: 94it [01:02,  1.49it/s]Extractor Predicting: 95it [01:03,  1.48it/s]Extractor Predicting: 96it [01:04,  1.47it/s]Extractor Predicting: 97it [01:04,  1.49it/s]Extractor Predicting: 98it [01:05,  1.50it/s]Extractor Predicting: 99it [01:06,  1.51it/s]Extractor Predicting: 100it [01:06,  1.52it/s]Extractor Predicting: 101it [01:07,  1.54it/s]Extractor Predicting: 102it [01:08,  1.54it/s]Extractor Predicting: 103it [01:08,  1.53it/s]Extractor Predicting: 104it [01:09,  1.49it/s]Extractor Predicting: 105it [01:10,  1.47it/s]Extractor Predicting: 106it [01:10,  1.48it/s]Extractor Predicting: 107it [01:11,  1.48it/s]Extractor Predicting: 108it [01:12,  1.49it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:13,  1.49it/s]Extractor Predicting: 111it [01:14,  1.50it/s]Extractor Predicting: 112it [01:14,  1.47it/s]Extractor Predicting: 113it [01:15,  1.46it/s]Extractor Predicting: 114it [01:16,  1.45it/s]Extractor Predicting: 115it [01:17,  1.43it/s]Extractor Predicting: 116it [01:17,  1.42it/s]Extractor Predicting: 117it [01:18,  1.44it/s]Extractor Predicting: 118it [01:19,  1.44it/s]Extractor Predicting: 119it [01:19,  1.45it/s]Extractor Predicting: 120it [01:20,  1.46it/s]Extractor Predicting: 121it [01:21,  1.49it/s]Extractor Predicting: 122it [01:21,  1.51it/s]Extractor Predicting: 123it [01:22,  1.52it/s]Extractor Predicting: 124it [01:23,  1.51it/s]Extractor Predicting: 125it [01:23,  1.49it/s]Extractor Predicting: 126it [01:24,  1.47it/s]Extractor Predicting: 127it [01:25,  1.47it/s]Extractor Predicting: 128it [01:25,  1.48it/s]Extractor Predicting: 129it [01:26,  1.50it/s]Extractor Predicting: 130it [01:27,  1.50it/s]Extractor Predicting: 131it [01:27,  1.51it/s]Extractor Predicting: 132it [01:28,  1.53it/s]Extractor Predicting: 133it [01:29,  1.50it/s]Extractor Predicting: 134it [01:29,  1.49it/s]Extractor Predicting: 135it [01:30,  1.47it/s]Extractor Predicting: 136it [01:31,  1.47it/s]Extractor Predicting: 137it [01:31,  1.46it/s]Extractor Predicting: 138it [01:32,  1.51it/s]Extractor Predicting: 139it [01:33,  1.50it/s]Extractor Predicting: 140it [01:33,  1.47it/s]Extractor Predicting: 141it [01:34,  1.49it/s]Extractor Predicting: 142it [01:35,  1.49it/s]Extractor Predicting: 143it [01:35,  1.50it/s]Extractor Predicting: 144it [01:36,  1.48it/s]Extractor Predicting: 145it [01:37,  1.50it/s]Extractor Predicting: 146it [01:37,  1.50it/s]Extractor Predicting: 147it [01:38,  1.54it/s]Extractor Predicting: 148it [01:39,  1.51it/s]Extractor Predicting: 149it [01:39,  1.53it/s]Extractor Predicting: 150it [01:40,  1.54it/s]Extractor Predicting: 151it [01:41,  1.56it/s]Extractor Predicting: 152it [01:41,  1.58it/s]Extractor Predicting: 153it [01:42,  1.57it/s]Extractor Predicting: 154it [01:42,  1.58it/s]Extractor Predicting: 155it [01:43,  1.57it/s]Extractor Predicting: 156it [01:44,  1.57it/s]Extractor Predicting: 157it [01:44,  1.54it/s]Extractor Predicting: 158it [01:45,  1.50it/s]Extractor Predicting: 159it [01:46,  1.51it/s]Extractor Predicting: 160it [01:46,  1.55it/s]Extractor Predicting: 161it [01:47,  1.54it/s]Extractor Predicting: 162it [01:48,  1.56it/s]Extractor Predicting: 163it [01:48,  1.57it/s]Extractor Predicting: 164it [01:49,  1.56it/s]Extractor Predicting: 165it [01:50,  1.55it/s]Extractor Predicting: 166it [01:50,  1.54it/s]Extractor Predicting: 167it [01:51,  1.54it/s]Extractor Predicting: 168it [01:52,  1.36it/s]Extractor Predicting: 169it [01:52,  1.43it/s]Extractor Predicting: 170it [01:53,  1.48it/s]Extractor Predicting: 171it [01:54,  1.47it/s]Extractor Predicting: 172it [01:54,  1.47it/s]Extractor Predicting: 173it [01:55,  1.48it/s]Extractor Predicting: 174it [01:56,  1.51it/s]Extractor Predicting: 175it [01:56,  1.54it/s]Extractor Predicting: 176it [01:57,  1.55it/s]Extractor Predicting: 177it [01:58,  1.50it/s]Extractor Predicting: 178it [01:58,  1.54it/s]Extractor Predicting: 179it [01:59,  1.52it/s]Extractor Predicting: 180it [02:00,  1.52it/s]Extractor Predicting: 181it [02:00,  1.53it/s]Extractor Predicting: 182it [02:01,  1.50it/s]Extractor Predicting: 183it [02:02,  1.52it/s]Extractor Predicting: 184it [02:02,  1.53it/s]Extractor Predicting: 185it [02:03,  1.55it/s]Extractor Predicting: 186it [02:04,  1.57it/s]Extractor Predicting: 187it [02:04,  1.61it/s]Extractor Predicting: 188it [02:05,  1.58it/s]Extractor Predicting: 189it [02:05,  1.53it/s]Extractor Predicting: 190it [02:06,  1.52it/s]Extractor Predicting: 191it [02:07,  1.50it/s]Extractor Predicting: 192it [02:07,  1.53it/s]Extractor Predicting: 193it [02:08,  1.53it/s]Extractor Predicting: 194it [02:09,  1.54it/s]Extractor Predicting: 195it [02:09,  1.56it/s]Extractor Predicting: 196it [02:10,  1.56it/s]Extractor Predicting: 197it [02:11,  1.52it/s]Extractor Predicting: 198it [02:11,  1.53it/s]Extractor Predicting: 199it [02:12,  1.51it/s]Extractor Predicting: 200it [02:13,  1.52it/s]Extractor Predicting: 201it [02:13,  1.52it/s]Extractor Predicting: 202it [02:14,  1.52it/s]Extractor Predicting: 203it [02:15,  1.52it/s]Extractor Predicting: 204it [02:15,  1.49it/s]Extractor Predicting: 205it [02:16,  1.46it/s]Extractor Predicting: 206it [02:17,  1.50it/s]Extractor Predicting: 207it [02:17,  1.50it/s]Extractor Predicting: 208it [02:18,  1.54it/s]Extractor Predicting: 209it [02:19,  1.55it/s]Extractor Predicting: 210it [02:19,  1.54it/s]Extractor Predicting: 211it [02:20,  1.52it/s]Extractor Predicting: 212it [02:21,  1.52it/s]Extractor Predicting: 213it [02:21,  1.50it/s]Extractor Predicting: 214it [02:22,  1.50it/s]Extractor Predicting: 215it [02:23,  1.49it/s]Extractor Predicting: 216it [02:23,  1.50it/s]Extractor Predicting: 217it [02:24,  1.48it/s]Extractor Predicting: 218it [02:25,  1.49it/s]Extractor Predicting: 219it [02:25,  1.48it/s]Extractor Predicting: 220it [02:26,  1.51it/s]Extractor Predicting: 221it [02:27,  1.55it/s]Extractor Predicting: 222it [02:27,  1.49it/s]Extractor Predicting: 223it [02:28,  1.46it/s]Extractor Predicting: 224it [02:29,  1.51it/s]Extractor Predicting: 225it [02:29,  1.52it/s]Extractor Predicting: 226it [02:30,  1.50it/s]Extractor Predicting: 227it [02:31,  1.50it/s]Extractor Predicting: 228it [02:31,  1.52it/s]Extractor Predicting: 229it [02:32,  1.54it/s]Extractor Predicting: 230it [02:33,  1.49it/s]Extractor Predicting: 231it [02:33,  1.51it/s]Extractor Predicting: 232it [02:34,  1.53it/s]Extractor Predicting: 233it [02:35,  1.50it/s]Extractor Predicting: 234it [02:35,  1.48it/s]Extractor Predicting: 235it [02:36,  1.50it/s]Extractor Predicting: 236it [02:37,  1.50it/s]Extractor Predicting: 237it [02:37,  1.48it/s]Extractor Predicting: 238it [02:38,  1.49it/s]Extractor Predicting: 239it [02:39,  1.49it/s]Extractor Predicting: 240it [02:39,  1.48it/s]Extractor Predicting: 241it [02:40,  1.50it/s]Extractor Predicting: 242it [02:41,  1.52it/s]Extractor Predicting: 243it [02:41,  1.49it/s]Extractor Predicting: 244it [02:42,  1.51it/s]Extractor Predicting: 245it [02:43,  1.50it/s]Extractor Predicting: 246it [02:43,  1.51it/s]Extractor Predicting: 247it [02:44,  1.51it/s]Extractor Predicting: 248it [02:45,  1.51it/s]Extractor Predicting: 249it [02:45,  1.52it/s]Extractor Predicting: 250it [02:46,  1.50it/s]Extractor Predicting: 251it [02:47,  1.52it/s]Extractor Predicting: 252it [02:47,  1.50it/s]Extractor Predicting: 253it [02:48,  1.51it/s]Extractor Predicting: 254it [02:49,  1.52it/s]Extractor Predicting: 255it [02:49,  1.52it/s]Extractor Predicting: 256it [02:50,  1.52it/s]Extractor Predicting: 257it [02:50,  1.55it/s]Extractor Predicting: 258it [02:51,  1.54it/s]Extractor Predicting: 259it [02:52,  1.56it/s]Extractor Predicting: 260it [02:52,  1.54it/s]Extractor Predicting: 261it [02:53,  1.55it/s]Extractor Predicting: 262it [02:54,  1.54it/s]Extractor Predicting: 263it [02:54,  1.55it/s]Extractor Predicting: 264it [02:55,  1.50it/s]Extractor Predicting: 265it [02:56,  1.50it/s]Extractor Predicting: 266it [02:56,  1.51it/s]Extractor Predicting: 267it [02:57,  1.52it/s]Extractor Predicting: 268it [02:58,  1.54it/s]Extractor Predicting: 269it [02:58,  1.54it/s]Extractor Predicting: 270it [02:59,  1.53it/s]Extractor Predicting: 271it [03:00,  1.54it/s]Extractor Predicting: 272it [03:00,  1.58it/s]Extractor Predicting: 273it [03:01,  1.58it/s]Extractor Predicting: 274it [03:01,  1.58it/s]Extractor Predicting: 275it [03:02,  1.56it/s]Extractor Predicting: 276it [03:03,  1.54it/s]Extractor Predicting: 277it [03:03,  1.56it/s]Extractor Predicting: 278it [03:04,  1.53it/s]Extractor Predicting: 279it [03:05,  1.55it/s]Extractor Predicting: 280it [03:05,  1.52it/s]Extractor Predicting: 281it [03:06,  1.53it/s]Extractor Predicting: 282it [03:07,  1.52it/s]Extractor Predicting: 283it [03:07,  1.52it/s]Extractor Predicting: 284it [03:08,  1.53it/s]Extractor Predicting: 285it [03:09,  1.52it/s]Extractor Predicting: 286it [03:09,  1.50it/s]Extractor Predicting: 287it [03:10,  1.51it/s]Extractor Predicting: 288it [03:11,  1.52it/s]Extractor Predicting: 289it [03:11,  1.53it/s]Extractor Predicting: 290it [03:12,  1.52it/s]Extractor Predicting: 291it [03:13,  1.51it/s]Extractor Predicting: 292it [03:13,  1.53it/s]Extractor Predicting: 293it [03:14,  1.53it/s]Extractor Predicting: 294it [03:15,  1.51it/s]Extractor Predicting: 295it [03:15,  1.54it/s]Extractor Predicting: 296it [03:16,  1.54it/s]Extractor Predicting: 297it [03:17,  1.51it/s]Extractor Predicting: 298it [03:17,  1.54it/s]Extractor Predicting: 299it [03:18,  1.54it/s]Extractor Predicting: 300it [03:19,  1.52it/s]Extractor Predicting: 301it [03:19,  1.57it/s]Extractor Predicting: 302it [03:20,  1.56it/s]Extractor Predicting: 303it [03:20,  1.59it/s]Extractor Predicting: 304it [03:21,  1.57it/s]Extractor Predicting: 305it [03:22,  1.55it/s]Extractor Predicting: 306it [03:22,  1.54it/s]Extractor Predicting: 307it [03:23,  1.53it/s]Extractor Predicting: 308it [03:24,  1.51it/s]Extractor Predicting: 309it [03:24,  1.52it/s]Extractor Predicting: 310it [03:25,  1.32it/s]Extractor Predicting: 311it [03:26,  1.35it/s]Extractor Predicting: 312it [03:27,  1.36it/s]Extractor Predicting: 313it [03:27,  1.40it/s]Extractor Predicting: 314it [03:28,  1.42it/s]Extractor Predicting: 315it [03:29,  1.47it/s]Extractor Predicting: 316it [03:29,  1.48it/s]Extractor Predicting: 317it [03:30,  1.48it/s]Extractor Predicting: 318it [03:31,  1.49it/s]Extractor Predicting: 319it [03:31,  1.48it/s]Extractor Predicting: 320it [03:32,  1.48it/s]Extractor Predicting: 321it [03:33,  1.48it/s]Extractor Predicting: 322it [03:33,  1.47it/s]Extractor Predicting: 323it [03:34,  1.52it/s]Extractor Predicting: 324it [03:35,  1.49it/s]Extractor Predicting: 325it [03:35,  1.51it/s]Extractor Predicting: 326it [03:36,  1.50it/s]Extractor Predicting: 327it [03:37,  1.52it/s]Extractor Predicting: 328it [03:37,  1.53it/s]Extractor Predicting: 329it [03:38,  1.51it/s]Extractor Predicting: 330it [03:39,  1.48it/s]Extractor Predicting: 331it [03:39,  1.48it/s]Extractor Predicting: 332it [03:40,  1.47it/s]Extractor Predicting: 333it [03:41,  1.49it/s]Extractor Predicting: 334it [03:41,  1.49it/s]Extractor Predicting: 335it [03:42,  1.49it/s]Extractor Predicting: 336it [03:43,  1.53it/s]Extractor Predicting: 337it [03:43,  1.54it/s]Extractor Predicting: 338it [03:44,  1.53it/s]Extractor Predicting: 339it [03:45,  1.51it/s]Extractor Predicting: 340it [03:45,  1.52it/s]Extractor Predicting: 341it [03:46,  1.54it/s]Extractor Predicting: 342it [03:47,  1.50it/s]Extractor Predicting: 343it [03:47,  1.51it/s]Extractor Predicting: 344it [03:48,  1.52it/s]Extractor Predicting: 345it [03:49,  1.54it/s]Extractor Predicting: 346it [03:49,  1.53it/s]Extractor Predicting: 347it [03:50,  1.52it/s]Extractor Predicting: 348it [03:51,  1.53it/s]Extractor Predicting: 349it [03:51,  1.50it/s]Extractor Predicting: 350it [03:52,  1.50it/s]Extractor Predicting: 351it [03:53,  1.48it/s]Extractor Predicting: 352it [03:53,  1.46it/s]Extractor Predicting: 353it [03:54,  1.47it/s]Extractor Predicting: 354it [03:55,  1.47it/s]Extractor Predicting: 355it [03:55,  1.49it/s]Extractor Predicting: 356it [03:56,  1.50it/s]Extractor Predicting: 357it [03:57,  1.53it/s]Extractor Predicting: 358it [03:57,  1.54it/s]Extractor Predicting: 359it [03:58,  1.53it/s]Extractor Predicting: 360it [03:59,  1.51it/s]Extractor Predicting: 361it [03:59,  1.52it/s]Extractor Predicting: 362it [04:00,  1.52it/s]Extractor Predicting: 363it [04:01,  1.55it/s]Extractor Predicting: 364it [04:01,  1.55it/s]Extractor Predicting: 365it [04:02,  1.54it/s]Extractor Predicting: 366it [04:03,  1.53it/s]Extractor Predicting: 367it [04:03,  1.52it/s]Extractor Predicting: 368it [04:04,  1.54it/s]Extractor Predicting: 369it [04:04,  1.54it/s]Extractor Predicting: 370it [04:05,  1.58it/s]Extractor Predicting: 371it [04:06,  1.56it/s]Extractor Predicting: 372it [04:06,  1.56it/s]Extractor Predicting: 373it [04:07,  1.55it/s]Extractor Predicting: 374it [04:08,  1.54it/s]Extractor Predicting: 375it [04:08,  1.55it/s]Extractor Predicting: 376it [04:09,  1.55it/s]Extractor Predicting: 377it [04:10,  1.55it/s]Extractor Predicting: 378it [04:10,  1.57it/s]Extractor Predicting: 379it [04:11,  1.52it/s]Extractor Predicting: 380it [04:12,  1.53it/s]Extractor Predicting: 381it [04:12,  1.56it/s]Extractor Predicting: 382it [04:13,  1.56it/s]Extractor Predicting: 383it [04:14,  1.51it/s]Extractor Predicting: 384it [04:14,  1.51it/s]Extractor Predicting: 385it [04:15,  1.53it/s]Extractor Predicting: 386it [04:15,  1.57it/s]Extractor Predicting: 387it [04:16,  1.58it/s]Extractor Predicting: 388it [04:17,  1.58it/s]Extractor Predicting: 389it [04:17,  1.54it/s]Extractor Predicting: 390it [04:18,  1.56it/s]Extractor Predicting: 391it [04:19,  1.56it/s]Extractor Predicting: 392it [04:19,  1.57it/s]Extractor Predicting: 393it [04:20,  1.58it/s]Extractor Predicting: 394it [04:21,  1.61it/s]Extractor Predicting: 395it [04:21,  1.57it/s]Extractor Predicting: 396it [04:22,  1.56it/s]Extractor Predicting: 397it [04:22,  1.57it/s]Extractor Predicting: 398it [04:23,  1.58it/s]Extractor Predicting: 399it [04:24,  1.59it/s]Extractor Predicting: 400it [04:24,  1.63it/s]Extractor Predicting: 401it [04:25,  1.59it/s]Extractor Predicting: 402it [04:26,  1.60it/s]Extractor Predicting: 403it [04:26,  1.58it/s]Extractor Predicting: 404it [04:27,  1.57it/s]Extractor Predicting: 405it [04:27,  1.57it/s]Extractor Predicting: 406it [04:28,  1.56it/s]Extractor Predicting: 407it [04:29,  1.55it/s]Extractor Predicting: 408it [04:29,  1.56it/s]Extractor Predicting: 409it [04:30,  1.56it/s]Extractor Predicting: 409it [04:30,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:57,645 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:57,649 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:57,649 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:57,649 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:44:57,649 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:44:58,365 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:44:58,366 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:44:58,632 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:44:59,682 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:44:59,682 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:45:01,002 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:45:01,006 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:45:01,006 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:45:01,006 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:45:01,007 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:45:01,699 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:45:01,700 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:45:02,278 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:45:02,437 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:45:02,438 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.43914388860237236,
  "recall": 0.17354529705492713,
  "score": 0.2487765685486816,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.35it/s]Extractor Predicting: 3it [00:02,  1.37it/s]Extractor Predicting: 4it [00:02,  1.42it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.45it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:45:11,963 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:45:11,964 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:45:11,968 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:45:11,968 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:45:11,972 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:45:14,988 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:45:14,994 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:45:15,004 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:45:15,005 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:45:15,016 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:45:15,019 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:45:15,019 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:45:15,019 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:45:15,019 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:45:15,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:45:15,020 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.22058823529411764,
  "recall": 0.021834061135371178,
  "score": 0.039735099337748346,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:45:15,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:16,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:16,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:17,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:18,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:18,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:19,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:20,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:20,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:21,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:22,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:23,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:23,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:24,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:25,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:25,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:26,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:27,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:28,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:28,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:33, 14.38s/it][WARNING|generation_utils.py:914] 2023-08-29 00:45:29,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:30,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:31,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:31,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:32,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:33,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:33,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:34,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:35,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:35,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:36,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:37,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:38,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:38,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:39,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:40,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:40,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:41,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:42,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:42,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:28<04:15, 14.19s/it][WARNING|generation_utils.py:914] 2023-08-29 00:45:43,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:44,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:45,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:46,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:46,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:47,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:48,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:49,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:49,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:50,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:51,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:51,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:52,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:53,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:54,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:54,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:55,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:56,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:56,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:57,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:58,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:43<04:10, 14.72s/it][WARNING|generation_utils.py:914] 2023-08-29 00:45:59,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:45:59,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:00,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:01,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:01,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:02,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:03,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:03,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:04,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:05,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:05,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:06,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:07,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:07,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:08,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:09,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:10,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:10,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:11,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:12,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:57<03:51, 14.46s/it][WARNING|generation_utils.py:914] 2023-08-29 00:46:13,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:13,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:14,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:15,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:16,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:17,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:17,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:18,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:19,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:19,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:20,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:21,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:21,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:22,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:23,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:23,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:24,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:25,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:26,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:26,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:27,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:28,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:13<03:43, 14.89s/it][WARNING|generation_utils.py:914] 2023-08-29 00:46:28,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:29,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:30,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:30,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:31,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:32,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:33,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:33,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:34,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:35,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:35,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:36,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:37,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:37,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:38,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:39,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:39,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:40,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:41,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:42,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:42,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:43,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:44,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:29<03:33, 15.27s/it][WARNING|generation_utils.py:914] 2023-08-29 00:46:44,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:45,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:46,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:47,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:47,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:48,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:49,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:49,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:50,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:51,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:52,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:53,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:53,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:54,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:55,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:56,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:56,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:57,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:58,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:59,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:46:59,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:00,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:46<03:24, 15.69s/it][WARNING|generation_utils.py:914] 2023-08-29 00:47:01,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:02,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:02,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:03,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:04,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:05,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:05,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:06,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:07,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:08,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:09,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:10,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:10,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:11,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:12,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:12,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:13,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:14,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:15,193 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:15,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:16,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:17,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:02<03:12, 16.02s/it][WARNING|generation_utils.py:914] 2023-08-29 00:47:18,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:18,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:19,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:20,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:20,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:21,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:22,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:23,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:23,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:24,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:25,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:25,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:26,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:27,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:28,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:28,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:29,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:30,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:31,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:31,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:32,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:33,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:34,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:19<02:59, 16.29s/it][WARNING|generation_utils.py:914] 2023-08-29 00:47:34,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:35,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:36,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:36,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:37,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:38,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:38,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:39,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:40,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:40,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:41,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:42,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:42,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:43,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:44,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:44,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:45,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:46,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:46,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:47,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:32<02:33, 15.36s/it][WARNING|generation_utils.py:914] 2023-08-29 00:47:48,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:48,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:49,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:50,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:51,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:51,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:52,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:53,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:53,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:54,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:55,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:55,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:56,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:57,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:57,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:58,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:59,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:47:59,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:00,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:01,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:01,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:47<02:15, 15.03s/it][WARNING|generation_utils.py:914] 2023-08-29 00:48:02,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:03,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:03,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:04,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:05,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:06,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:07,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:07,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:08,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:09,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:09,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:10,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:11,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:12,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:13,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:13,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:14,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:15,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:15,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:16,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:17,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:17,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:03<02:02, 15.33s/it][WARNING|generation_utils.py:914] 2023-08-29 00:48:18,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:19,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:19,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:20,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:21,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:21,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:22,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:23,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:23,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:24,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:24,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:25,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:26,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:27,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:27,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:28,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:28,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:29,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:30,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:30,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:31,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:16<01:43, 14.76s/it][WARNING|generation_utils.py:914] 2023-08-29 00:48:31,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:32,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:33,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:34,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:34,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:35,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:36,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:36,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:37,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:38,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:39,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:39,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:40,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:41,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:42,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:42,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:43,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:44,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:45,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:45,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:31<01:28, 14.72s/it][WARNING|generation_utils.py:914] 2023-08-29 00:48:46,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:47,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:47,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:48,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:49,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:49,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:50,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:51,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:51,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:52,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:53,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:54,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:54,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:55,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:56,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:56,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:57,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:58,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:59,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:48:59,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:00,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:45<01:13, 14.64s/it][WARNING|generation_utils.py:914] 2023-08-29 00:49:01,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:01,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:02,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:03,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:03,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:04,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:05,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:05,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:06,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:07,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:07,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:08,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:09,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:09,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:10,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:11,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:11,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:12,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:13,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:13,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:14,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:59<00:57, 14.49s/it][WARNING|generation_utils.py:914] 2023-08-29 00:49:15,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:16,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:16,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:17,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:17,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:18,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:19,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:19,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:20,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:21,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:21,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:22,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:22,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:23,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:24,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:24,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:25,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:26,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:26,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:27,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:28,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:13<00:42, 14.28s/it][WARNING|generation_utils.py:914] 2023-08-29 00:49:28,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:29,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:30,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:31,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:31,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:32,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:33,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:33,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:34,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:35,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:36,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:36,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:37,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:38,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:38,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:39,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:40,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:40,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:41,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:42,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:27<00:28, 14.20s/it][WARNING|generation_utils.py:914] 2023-08-29 00:49:43,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:43,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:44,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:45,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:45,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:46,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:47,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:47,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:48,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:49,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:50,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:50,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:51,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:52,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:52,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:53,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:54,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:54,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:55,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:56,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:57,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:42<00:14, 14.45s/it][WARNING|generation_utils.py:914] 2023-08-29 00:49:58,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:58,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:49:59,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:00,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:00,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:01,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:02,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:03,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:03,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:04,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:05,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:05,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:06,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:07,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:07,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:08,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:09,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:10,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:10,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:50:11,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:57<00:00, 14.47s/it]Generating: 100%|██████████| 20/20 [04:57<00:00, 14.87s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:18,830 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:18,835 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:18,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:18,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:18,836 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:50:19,148 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:50:19,149 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:50:19,839 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:50:20,902 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:50:20,902 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:22,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:22,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:22,225 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:22,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:50:22,226 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:50:22,555 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:50:22,556 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:50:23,216 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:50:23,378 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:50:23,379 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.9640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : military branch .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : place served by transport hub .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : screenwriter .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 522, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : voice type .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8519021739130435, 'errors': {'', "('Netherlands', 'competition class', '', 'He represented the Netherlands at the 2000 Summer Olympics .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : field of work .', 'success_rate': 0.8849431818181818, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 582, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : language of work or name .', 'success_rate': 0.8693181818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8505434782608695, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 545, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 371, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 581, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9546875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 507, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : owned by .', 'success_rate': 0.9270833333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 551, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 606, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.9017857142857143, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 564, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 191, 'raw': 192}
{'target': 600, 'success': 222, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 316, 'raw': 320}
{'target': 600, 'success': 348, 'raw': 352}
{'target': 600, 'success': 378, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 439, 'raw': 448}
{'target': 600, 'success': 470, 'raw': 480}
{'target': 600, 'success': 500, 'raw': 512}
{'target': 600, 'success': 531, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 594, 'raw': 608}
{'target': 600, 'success': 624, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.975, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : religion .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : said to be the same as .', 'success_rate': 0.959375, 'errors': {'', '(\'Riddle of the Tailor\', \'said to be the same as\', \'\', \'She is an example of what is commonly called the " Riddle of the Tailor " , where a human being is said to be the same as every other human being .\')'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 10739
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10839, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.36it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:02,  1.43it/s]Extractor Estimating: 4it [00:02,  1.50it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:04,  1.53it/s]Extractor Estimating: 7it [00:04,  1.58it/s]Extractor Estimating: 8it [00:05,  1.56it/s]Extractor Estimating: 9it [00:05,  1.66it/s]Extractor Estimating: 10it [00:06,  1.70it/s]Extractor Estimating: 11it [00:06,  1.70it/s]Extractor Estimating: 12it [00:07,  1.72it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.69it/s]Extractor Estimating: 16it [00:10,  1.62it/s]Extractor Estimating: 17it [00:10,  1.60it/s]Extractor Estimating: 18it [00:11,  1.58it/s]Extractor Estimating: 19it [00:11,  1.62it/s]Extractor Estimating: 20it [00:12,  1.60it/s]Extractor Estimating: 21it [00:13,  1.60it/s]Extractor Estimating: 22it [00:13,  1.63it/s]Extractor Estimating: 23it [00:14,  1.55it/s]Extractor Estimating: 24it [00:15,  1.58it/s]Extractor Estimating: 25it [00:15,  1.63it/s]Extractor Estimating: 26it [00:16,  1.65it/s]Extractor Estimating: 27it [00:16,  1.67it/s]Extractor Estimating: 28it [00:17,  1.63it/s]Extractor Estimating: 29it [00:18,  1.65it/s]Extractor Estimating: 30it [00:18,  1.69it/s]Extractor Estimating: 31it [00:19,  1.67it/s]Extractor Estimating: 32it [00:19,  1.69it/s]Extractor Estimating: 33it [00:20,  1.74it/s]Extractor Estimating: 34it [00:21,  1.56it/s]Extractor Estimating: 35it [00:21,  1.65it/s]Extractor Estimating: 36it [00:22,  1.64it/s]Extractor Estimating: 37it [00:22,  1.60it/s]Extractor Estimating: 38it [00:23,  1.55it/s]Extractor Estimating: 39it [00:24,  1.56it/s]Extractor Estimating: 40it [00:24,  1.62it/s]Extractor Estimating: 41it [00:25,  1.63it/s]Extractor Estimating: 42it [00:25,  1.67it/s]Extractor Estimating: 43it [00:26,  1.65it/s]Extractor Estimating: 44it [00:27,  1.66it/s]Extractor Estimating: 45it [00:27,  1.65it/s]Extractor Estimating: 46it [00:28,  1.66it/s]Extractor Estimating: 47it [00:29,  1.62it/s]Extractor Estimating: 48it [00:29,  1.61it/s]Extractor Estimating: 49it [00:30,  1.61it/s]Extractor Estimating: 50it [00:30,  1.58it/s]Extractor Estimating: 51it [00:31,  1.60it/s]Extractor Estimating: 52it [00:32,  1.68it/s]Extractor Estimating: 53it [00:32,  1.67it/s]Extractor Estimating: 54it [00:33,  1.71it/s]Extractor Estimating: 55it [00:33,  1.73it/s]Extractor Estimating: 56it [00:34,  1.77it/s]Extractor Estimating: 57it [00:34,  1.76it/s]Extractor Estimating: 58it [00:35,  1.83it/s]Extractor Estimating: 59it [00:35,  1.89it/s]Extractor Estimating: 60it [00:36,  1.96it/s]Extractor Estimating: 61it [00:36,  1.93it/s]Extractor Estimating: 62it [00:37,  1.91it/s]Extractor Estimating: 63it [00:37,  1.93it/s]Extractor Estimating: 64it [00:38,  1.84it/s]Extractor Estimating: 65it [00:39,  1.89it/s]Extractor Estimating: 66it [00:39,  1.93it/s]Extractor Estimating: 67it [00:40,  1.90it/s]Extractor Estimating: 68it [00:40,  1.83it/s]Extractor Estimating: 69it [00:41,  1.82it/s]Extractor Estimating: 70it [00:41,  1.78it/s]Extractor Estimating: 71it [00:42,  1.84it/s]Extractor Estimating: 72it [00:42,  1.87it/s]Extractor Estimating: 73it [00:43,  1.87it/s]Extractor Estimating: 74it [00:43,  1.84it/s]Extractor Estimating: 75it [00:44,  1.85it/s]Extractor Estimating: 76it [00:45,  1.72it/s]Extractor Estimating: 77it [00:45,  1.65it/s]Extractor Estimating: 78it [00:46,  1.63it/s]Extractor Estimating: 79it [00:47,  1.61it/s]Extractor Estimating: 80it [00:47,  1.58it/s]Extractor Estimating: 81it [00:48,  1.55it/s]Extractor Estimating: 82it [00:49,  1.59it/s]Extractor Estimating: 83it [00:49,  1.54it/s]Extractor Estimating: 84it [00:50,  1.57it/s]Extractor Estimating: 85it [00:50,  1.61it/s]Extractor Estimating: 86it [00:51,  1.62it/s]Extractor Estimating: 87it [00:52,  1.63it/s]Extractor Estimating: 88it [00:52,  1.59it/s]Extractor Estimating: 89it [00:53,  1.60it/s]Extractor Estimating: 90it [00:54,  1.60it/s]Extractor Estimating: 91it [00:54,  1.54it/s]Extractor Estimating: 92it [00:55,  1.52it/s]Extractor Estimating: 93it [00:56,  1.54it/s]Extractor Estimating: 94it [00:56,  1.51it/s]Extractor Estimating: 95it [00:57,  1.53it/s]Extractor Estimating: 96it [00:57,  1.55it/s]Extractor Estimating: 97it [00:58,  1.53it/s]Extractor Estimating: 98it [00:59,  1.53it/s]Extractor Estimating: 99it [00:59,  1.55it/s]Extractor Estimating: 100it [01:00,  1.56it/s]Extractor Estimating: 101it [01:01,  1.58it/s]Extractor Estimating: 102it [01:01,  1.61it/s]Extractor Estimating: 103it [01:02,  1.62it/s]Extractor Estimating: 104it [01:02,  1.64it/s]Extractor Estimating: 105it [01:03,  1.59it/s]Extractor Estimating: 106it [01:04,  1.64it/s]Extractor Estimating: 107it [01:04,  1.56it/s]Extractor Estimating: 108it [01:05,  1.58it/s]Extractor Estimating: 109it [01:06,  1.58it/s]Extractor Estimating: 110it [01:06,  1.60it/s]Extractor Estimating: 111it [01:07,  1.60it/s]Extractor Estimating: 112it [01:07,  1.64it/s]Extractor Estimating: 113it [01:08,  1.62it/s]Extractor Estimating: 114it [01:09,  1.66it/s]Extractor Estimating: 115it [01:09,  1.72it/s]Extractor Estimating: 116it [01:10,  1.75it/s]Extractor Estimating: 117it [01:10,  1.70it/s]Extractor Estimating: 118it [01:11,  1.65it/s]Extractor Estimating: 119it [01:12,  1.47it/s]Extractor Estimating: 120it [01:13,  1.43it/s]Extractor Estimating: 121it [01:13,  1.49it/s]Extractor Estimating: 122it [01:14,  1.57it/s]Extractor Estimating: 123it [01:14,  1.57it/s]Extractor Estimating: 124it [01:15,  1.63it/s]Extractor Estimating: 125it [01:16,  1.63it/s]Extractor Estimating: 126it [01:16,  1.71it/s]Extractor Estimating: 127it [01:17,  1.76it/s]Extractor Estimating: 128it [01:17,  1.79it/s]Extractor Estimating: 129it [01:18,  1.75it/s]Extractor Estimating: 130it [01:18,  1.76it/s]Extractor Estimating: 131it [01:19,  1.71it/s]Extractor Estimating: 132it [01:20,  1.74it/s]Extractor Estimating: 133it [01:20,  1.71it/s]Extractor Estimating: 134it [01:21,  1.74it/s]Extractor Estimating: 135it [01:21,  1.74it/s]Extractor Estimating: 136it [01:22,  1.79it/s]Extractor Estimating: 137it [01:22,  1.81it/s]Extractor Estimating: 138it [01:23,  1.85it/s]Extractor Estimating: 139it [01:23,  1.82it/s]Extractor Estimating: 140it [01:24,  1.77it/s]Extractor Estimating: 141it [01:25,  1.76it/s]Extractor Estimating: 142it [01:25,  1.78it/s]Extractor Estimating: 143it [01:26,  1.75it/s]Extractor Estimating: 144it [01:26,  1.78it/s]Extractor Estimating: 145it [01:27,  1.73it/s]Extractor Estimating: 146it [01:27,  1.76it/s]Extractor Estimating: 147it [01:28,  1.81it/s]Extractor Estimating: 148it [01:28,  1.83it/s]Extractor Estimating: 149it [01:29,  1.77it/s]Extractor Estimating: 150it [01:30,  1.78it/s]Extractor Estimating: 151it [01:30,  1.74it/s]Extractor Estimating: 152it [01:31,  1.69it/s]Extractor Estimating: 153it [01:31,  1.70it/s]Extractor Estimating: 154it [01:32,  1.76it/s]Extractor Estimating: 155it [01:33,  1.70it/s]Extractor Estimating: 156it [01:33,  1.71it/s]Extractor Estimating: 157it [01:34,  1.70it/s]Extractor Estimating: 158it [01:34,  1.73it/s]Extractor Estimating: 159it [01:35,  1.70it/s]Extractor Estimating: 160it [01:36,  1.73it/s]Extractor Estimating: 161it [01:36,  1.70it/s]Extractor Estimating: 162it [01:37,  1.66it/s]Extractor Estimating: 163it [01:37,  1.63it/s]Extractor Estimating: 164it [01:38,  1.67it/s]Extractor Estimating: 165it [01:38,  1.72it/s]Extractor Estimating: 166it [01:39,  1.77it/s]Extractor Estimating: 167it [01:40,  1.69it/s]Extractor Estimating: 168it [01:40,  1.72it/s]Extractor Estimating: 169it [01:41,  1.74it/s]Extractor Estimating: 170it [01:41,  1.68it/s]Extractor Estimating: 171it [01:42,  1.60it/s]Extractor Estimating: 172it [01:43,  1.65it/s]Extractor Estimating: 173it [01:43,  1.64it/s]Extractor Estimating: 174it [01:44,  1.71it/s]Extractor Estimating: 175it [01:44,  1.66it/s]Extractor Estimating: 176it [01:45,  1.67it/s]Extractor Estimating: 177it [01:46,  1.60it/s]Extractor Estimating: 178it [01:46,  1.63it/s]Extractor Estimating: 179it [01:47,  1.68it/s]Extractor Estimating: 180it [01:47,  1.69it/s]Extractor Estimating: 181it [01:48,  1.64it/s]Extractor Estimating: 182it [01:49,  1.69it/s]Extractor Estimating: 183it [01:49,  1.69it/s]Extractor Estimating: 184it [01:50,  1.63it/s]Extractor Estimating: 185it [01:51,  1.57it/s]Extractor Estimating: 186it [01:51,  1.53it/s]Extractor Estimating: 187it [01:52,  1.53it/s]Extractor Estimating: 188it [01:53,  1.56it/s]Extractor Estimating: 189it [01:53,  1.60it/s]Extractor Estimating: 190it [01:54,  1.66it/s]Extractor Estimating: 191it [01:54,  1.69it/s]Extractor Estimating: 192it [01:55,  1.71it/s]Extractor Estimating: 193it [01:55,  1.67it/s]Extractor Estimating: 194it [01:56,  1.62it/s]Extractor Estimating: 195it [01:57,  1.62it/s]Extractor Estimating: 196it [01:57,  1.64it/s]Extractor Estimating: 197it [01:58,  1.60it/s]Extractor Estimating: 198it [01:59,  1.61it/s]Extractor Estimating: 199it [01:59,  1.62it/s]Extractor Estimating: 200it [02:00,  1.62it/s]Extractor Estimating: 201it [02:00,  1.64it/s]Extractor Estimating: 202it [02:01,  1.62it/s]Extractor Estimating: 203it [02:02,  1.65it/s]Extractor Estimating: 204it [02:02,  1.63it/s]Extractor Estimating: 205it [02:03,  1.59it/s]Extractor Estimating: 206it [02:04,  1.58it/s]Extractor Estimating: 207it [02:04,  1.49it/s]Extractor Estimating: 208it [02:05,  1.54it/s]Extractor Estimating: 209it [02:06,  1.56it/s]Extractor Estimating: 210it [02:06,  1.51it/s]Extractor Estimating: 211it [02:07,  1.54it/s]Extractor Estimating: 212it [02:08,  1.57it/s]Extractor Estimating: 213it [02:08,  1.54it/s]Extractor Estimating: 214it [02:09,  1.54it/s]Extractor Estimating: 215it [02:10,  1.53it/s]Extractor Estimating: 216it [02:10,  1.51it/s]Extractor Estimating: 217it [02:11,  1.50it/s]Extractor Estimating: 218it [02:11,  1.53it/s]Extractor Estimating: 219it [02:12,  1.53it/s]Extractor Estimating: 220it [02:13,  1.57it/s]Extractor Estimating: 221it [02:13,  1.57it/s]Extractor Estimating: 222it [02:14,  1.58it/s]Extractor Estimating: 223it [02:15,  1.57it/s]Extractor Estimating: 224it [02:15,  1.48it/s]Extractor Estimating: 225it [02:16,  1.50it/s]Extractor Estimating: 226it [02:17,  1.56it/s]Extractor Estimating: 227it [02:17,  1.61it/s]Extractor Estimating: 228it [02:18,  1.65it/s]Extractor Estimating: 229it [02:18,  1.72it/s]Extractor Estimating: 230it [02:19,  1.73it/s]Extractor Estimating: 231it [02:19,  1.71it/s]Extractor Estimating: 232it [02:20,  1.72it/s]Extractor Estimating: 233it [02:21,  1.72it/s]Extractor Estimating: 234it [02:21,  1.68it/s]Extractor Estimating: 235it [02:22,  1.73it/s]Extractor Estimating: 236it [02:22,  1.77it/s]Extractor Estimating: 237it [02:23,  1.72it/s]Extractor Estimating: 238it [02:24,  1.70it/s]Extractor Estimating: 239it [02:24,  1.72it/s]Extractor Estimating: 240it [02:25,  1.72it/s]Extractor Estimating: 241it [02:25,  1.77it/s]Extractor Estimating: 242it [02:26,  1.84it/s]Extractor Estimating: 243it [02:26,  1.79it/s]Extractor Estimating: 244it [02:27,  1.79it/s]Extractor Estimating: 245it [02:27,  1.81it/s]Extractor Estimating: 246it [02:28,  1.76it/s]Extractor Estimating: 247it [02:29,  1.76it/s]Extractor Estimating: 248it [02:29,  1.79it/s]Extractor Estimating: 249it [02:30,  1.80it/s]Extractor Estimating: 250it [02:30,  1.80it/s]Extractor Estimating: 251it [02:31,  1.83it/s]Extractor Estimating: 252it [02:31,  1.77it/s]Extractor Estimating: 253it [02:32,  1.73it/s]Extractor Estimating: 254it [02:33,  1.69it/s]Extractor Estimating: 255it [02:33,  1.76it/s]Extractor Estimating: 256it [02:34,  1.73it/s]Extractor Estimating: 257it [02:34,  1.72it/s]Extractor Estimating: 258it [02:35,  1.71it/s]Extractor Estimating: 259it [02:35,  1.78it/s]Extractor Estimating: 260it [02:36,  1.78it/s]Extractor Estimating: 261it [02:36,  1.80it/s]Extractor Estimating: 262it [02:37,  1.81it/s]Extractor Estimating: 263it [02:38,  1.77it/s]Extractor Estimating: 264it [02:38,  1.74it/s]Extractor Estimating: 265it [02:39,  1.75it/s]Extractor Estimating: 266it [02:39,  1.79it/s]Extractor Estimating: 267it [02:40,  1.78it/s]Extractor Estimating: 268it [02:40,  1.84it/s]Extractor Estimating: 269it [02:41,  1.84it/s]Extractor Estimating: 270it [02:41,  1.85it/s]Extractor Estimating: 271it [02:42,  1.83it/s]Extractor Estimating: 272it [02:43,  1.80it/s]Extractor Estimating: 273it [02:43,  1.83it/s]Extractor Estimating: 274it [02:44,  1.79it/s]Extractor Estimating: 275it [02:44,  1.82it/s]Extractor Estimating: 276it [02:45,  1.67it/s]Extractor Estimating: 277it [02:46,  1.59it/s]Extractor Estimating: 278it [02:46,  1.57it/s]Extractor Estimating: 279it [02:47,  1.48it/s]Extractor Estimating: 280it [02:48,  1.52it/s]Extractor Estimating: 281it [02:48,  1.50it/s]Extractor Estimating: 282it [02:49,  1.47it/s]Extractor Estimating: 283it [02:50,  1.52it/s]Extractor Estimating: 284it [02:50,  1.47it/s]Extractor Estimating: 285it [02:51,  1.47it/s]Extractor Estimating: 286it [02:52,  1.50it/s]Extractor Estimating: 287it [02:52,  1.50it/s]Extractor Estimating: 288it [02:53,  1.50it/s]Extractor Estimating: 289it [02:54,  1.43it/s]Extractor Estimating: 290it [02:55,  1.43it/s]Extractor Estimating: 291it [02:55,  1.45it/s]Extractor Estimating: 292it [02:56,  1.46it/s]Extractor Estimating: 293it [02:57,  1.47it/s]Extractor Estimating: 294it [02:57,  1.51it/s]Extractor Estimating: 295it [02:58,  1.51it/s]Extractor Estimating: 296it [02:59,  1.52it/s]Extractor Estimating: 297it [02:59,  1.50it/s]Extractor Estimating: 298it [03:00,  1.53it/s]Extractor Estimating: 299it [03:00,  1.54it/s]Extractor Estimating: 300it [03:01,  1.56it/s]Extractor Estimating: 301it [03:02,  1.63it/s]Extractor Estimating: 302it [03:02,  1.52it/s]Extractor Estimating: 303it [03:03,  1.58it/s]Extractor Estimating: 304it [03:04,  1.57it/s]Extractor Estimating: 305it [03:04,  1.59it/s]Extractor Estimating: 306it [03:05,  1.61it/s]Extractor Estimating: 307it [03:05,  1.69it/s]Extractor Estimating: 308it [03:06,  1.68it/s]Extractor Estimating: 309it [03:07,  1.65it/s]Extractor Estimating: 310it [03:07,  1.77it/s]Extractor Estimating: 311it [03:08,  1.78it/s]Extractor Estimating: 312it [03:08,  1.80it/s]Extractor Estimating: 313it [03:09,  1.77it/s]Extractor Estimating: 314it [03:09,  1.70it/s]Extractor Estimating: 315it [03:10,  1.76it/s]Extractor Estimating: 316it [03:10,  1.74it/s]Extractor Estimating: 317it [03:11,  1.72it/s]Extractor Estimating: 318it [03:12,  1.66it/s]Extractor Estimating: 319it [03:12,  1.71it/s]Extractor Estimating: 320it [03:13,  1.74it/s]Extractor Estimating: 321it [03:13,  1.80it/s]Extractor Estimating: 322it [03:14,  1.78it/s]Extractor Estimating: 323it [03:14,  1.77it/s]Extractor Estimating: 324it [03:15,  1.78it/s]Extractor Estimating: 325it [03:16,  1.81it/s]Extractor Estimating: 326it [03:16,  1.76it/s]Extractor Estimating: 327it [03:17,  1.71it/s]Extractor Estimating: 328it [03:17,  1.69it/s]Extractor Estimating: 329it [03:18,  1.74it/s]Extractor Estimating: 330it [03:19,  1.74it/s]Extractor Estimating: 331it [03:19,  1.76it/s]Extractor Estimating: 332it [03:20,  1.68it/s]Extractor Estimating: 333it [03:20,  1.64it/s]Extractor Estimating: 334it [03:21,  1.64it/s]Extractor Estimating: 335it [03:22,  1.59it/s]Extractor Estimating: 336it [03:22,  1.60it/s]Extractor Estimating: 337it [03:23,  1.60it/s]Extractor Estimating: 338it [03:24,  1.56it/s]Extractor Estimating: 339it [03:24,  1.55it/s]Extractor Estimating: 340it [03:25,  1.54it/s]Extractor Estimating: 341it [03:26,  1.53it/s]Extractor Estimating: 342it [03:26,  1.50it/s]Extractor Estimating: 343it [03:27,  1.60it/s]Extractor Estimating: 344it [03:27,  1.67it/s]Extractor Estimating: 345it [03:28,  1.65it/s]Extractor Estimating: 346it [03:29,  1.59it/s]Extractor Estimating: 347it [03:29,  1.59it/s]Extractor Estimating: 348it [03:30,  1.58it/s]Extractor Estimating: 349it [03:30,  1.60it/s]Extractor Estimating: 350it [03:31,  1.61it/s]Extractor Estimating: 351it [03:32,  1.70it/s]Extractor Estimating: 352it [03:32,  1.74it/s]Extractor Estimating: 353it [03:33,  1.73it/s]Extractor Estimating: 354it [03:33,  1.77it/s]Extractor Estimating: 355it [03:34,  1.76it/s]Extractor Estimating: 356it [03:34,  1.81it/s]Extractor Estimating: 357it [03:35,  1.81it/s]Extractor Estimating: 358it [03:35,  1.82it/s]Extractor Estimating: 359it [03:36,  1.81it/s]Extractor Estimating: 360it [03:37,  1.77it/s]Extractor Estimating: 361it [03:37,  1.77it/s]Extractor Estimating: 362it [03:38,  1.80it/s]Extractor Estimating: 363it [03:38,  1.87it/s]Extractor Estimating: 364it [03:39,  1.81it/s]Extractor Estimating: 365it [03:39,  1.77it/s]Extractor Estimating: 366it [03:40,  1.79it/s]Extractor Estimating: 367it [03:40,  1.81it/s]Extractor Estimating: 368it [03:41,  1.82it/s]Extractor Estimating: 369it [03:42,  1.82it/s]Extractor Estimating: 370it [03:42,  1.76it/s]Extractor Estimating: 371it [03:43,  1.50it/s]Extractor Estimating: 372it [03:44,  1.57it/s]Extractor Estimating: 373it [03:44,  1.63it/s]Extractor Estimating: 374it [03:45,  1.69it/s]Extractor Estimating: 375it [03:45,  1.68it/s]Extractor Estimating: 376it [03:46,  1.78it/s]Extractor Estimating: 377it [03:46,  1.76it/s]Extractor Estimating: 378it [03:47,  1.75it/s]Extractor Estimating: 379it [03:48,  1.70it/s]Extractor Estimating: 380it [03:48,  1.74it/s]Extractor Estimating: 381it [03:49,  1.69it/s]Extractor Estimating: 382it [03:49,  1.72it/s]Extractor Estimating: 383it [03:50,  1.82it/s]Extractor Estimating: 384it [03:50,  1.80it/s]Extractor Estimating: 385it [03:51,  1.81it/s]Extractor Estimating: 386it [03:52,  1.77it/s]Extractor Estimating: 387it [03:52,  1.78it/s]Extractor Estimating: 388it [03:53,  1.81it/s]Extractor Estimating: 389it [03:53,  1.80it/s]Extractor Estimating: 390it [03:54,  1.80it/s]Extractor Estimating: 391it [03:54,  1.79it/s]Extractor Estimating: 392it [03:55,  1.88it/s]Extractor Estimating: 393it [03:55,  1.84it/s]Extractor Estimating: 394it [03:56,  1.82it/s]Extractor Estimating: 395it [03:56,  1.86it/s]Extractor Estimating: 396it [03:57,  1.85it/s]Extractor Estimating: 397it [03:57,  1.89it/s]Extractor Estimating: 398it [03:58,  1.83it/s]Extractor Estimating: 399it [03:59,  1.76it/s]Extractor Estimating: 400it [03:59,  1.75it/s]Extractor Estimating: 401it [04:00,  1.74it/s]Extractor Estimating: 402it [04:00,  1.79it/s]Extractor Estimating: 403it [04:01,  1.60it/s]Extractor Estimating: 404it [04:02,  1.66it/s]Extractor Estimating: 405it [04:02,  1.74it/s]Extractor Estimating: 406it [04:03,  1.77it/s]Extractor Estimating: 407it [04:03,  1.75it/s]Extractor Estimating: 408it [04:04,  1.79it/s]Extractor Estimating: 409it [04:04,  1.78it/s]Extractor Estimating: 410it [04:05,  1.79it/s]Extractor Estimating: 411it [04:06,  1.80it/s]Extractor Estimating: 412it [04:06,  1.80it/s]Extractor Estimating: 413it [04:07,  1.82it/s]Extractor Estimating: 414it [04:07,  1.84it/s]Extractor Estimating: 415it [04:08,  1.81it/s]Extractor Estimating: 416it [04:08,  1.88it/s]Extractor Estimating: 417it [04:09,  1.83it/s]Extractor Estimating: 418it [04:09,  1.83it/s]Extractor Estimating: 419it [04:10,  1.82it/s]Extractor Estimating: 420it [04:10,  1.85it/s]Extractor Estimating: 421it [04:11,  1.81it/s]Extractor Estimating: 422it [04:12,  1.73it/s]Extractor Estimating: 423it [04:12,  1.68it/s]Extractor Estimating: 424it [04:13,  1.72it/s]Extractor Estimating: 425it [04:13,  1.74it/s]Extractor Estimating: 426it [04:14,  1.58it/s]Extractor Estimating: 427it [04:15,  1.56it/s]Extractor Estimating: 428it [04:15,  1.60it/s]Extractor Estimating: 429it [04:16,  1.66it/s]Extractor Estimating: 430it [04:17,  1.57it/s]Extractor Estimating: 431it [04:17,  1.56it/s]Extractor Estimating: 432it [04:18,  1.58it/s]Extractor Estimating: 433it [04:19,  1.56it/s]Extractor Estimating: 434it [04:19,  1.60it/s]Extractor Estimating: 435it [04:20,  1.53it/s]Extractor Estimating: 436it [04:21,  1.50it/s]Extractor Estimating: 437it [04:21,  1.50it/s]Extractor Estimating: 438it [04:22,  1.49it/s]Extractor Estimating: 439it [04:23,  1.50it/s]Extractor Estimating: 440it [04:23,  1.50it/s]Extractor Estimating: 441it [04:24,  1.50it/s]Extractor Estimating: 442it [04:25,  1.49it/s]Extractor Estimating: 443it [04:25,  1.53it/s]Extractor Estimating: 444it [04:26,  1.57it/s]Extractor Estimating: 445it [04:26,  1.57it/s]Extractor Estimating: 446it [04:27,  1.60it/s]Extractor Estimating: 447it [04:28,  1.60it/s]Extractor Estimating: 448it [04:28,  1.57it/s]Extractor Estimating: 449it [04:29,  1.54it/s]Extractor Estimating: 450it [04:30,  1.57it/s]Extractor Estimating: 451it [04:30,  1.60it/s]Extractor Estimating: 452it [04:31,  1.64it/s]Extractor Estimating: 453it [04:31,  1.65it/s]Extractor Estimating: 454it [04:32,  1.66it/s]Extractor Estimating: 455it [04:33,  1.68it/s]Extractor Estimating: 456it [04:33,  1.73it/s]Extractor Estimating: 457it [04:34,  1.76it/s]Extractor Estimating: 458it [04:34,  1.67it/s]Extractor Estimating: 459it [04:35,  1.68it/s]Extractor Estimating: 460it [04:36,  1.66it/s]Extractor Estimating: 461it [04:36,  1.60it/s]Extractor Estimating: 462it [04:37,  1.65it/s]Extractor Estimating: 463it [04:37,  1.69it/s]Extractor Estimating: 464it [04:38,  1.75it/s]Extractor Estimating: 465it [04:38,  1.72it/s]Extractor Estimating: 466it [04:39,  1.73it/s]Extractor Estimating: 467it [04:40,  1.71it/s]Extractor Estimating: 468it [04:40,  1.70it/s]Extractor Estimating: 469it [04:41,  1.69it/s]Extractor Estimating: 470it [04:41,  1.74it/s]Extractor Estimating: 471it [04:42,  1.68it/s]Extractor Estimating: 472it [04:43,  1.66it/s]Extractor Estimating: 473it [04:43,  1.65it/s]Extractor Estimating: 474it [04:44,  1.69it/s]Extractor Estimating: 475it [04:44,  1.71it/s]Extractor Estimating: 476it [04:45,  1.60it/s]Extractor Estimating: 477it [04:46,  1.61it/s]Extractor Estimating: 478it [04:46,  1.61it/s]Extractor Estimating: 479it [04:47,  1.65it/s]Extractor Estimating: 480it [04:48,  1.60it/s]Extractor Estimating: 481it [04:48,  1.62it/s]Extractor Estimating: 482it [04:49,  1.60it/s]Extractor Estimating: 483it [04:49,  1.60it/s]Extractor Estimating: 484it [04:50,  1.53it/s]Extractor Estimating: 485it [04:51,  1.50it/s]Extractor Estimating: 486it [04:51,  1.51it/s]Extractor Estimating: 487it [04:52,  1.52it/s]Extractor Estimating: 488it [04:53,  1.53it/s]Extractor Estimating: 489it [04:53,  1.59it/s]Extractor Estimating: 490it [04:54,  1.57it/s]Extractor Estimating: 491it [04:55,  1.59it/s]Extractor Estimating: 492it [04:55,  1.45it/s]Extractor Estimating: 493it [04:56,  1.52it/s]Extractor Estimating: 494it [04:57,  1.54it/s]Extractor Estimating: 495it [04:57,  1.52it/s]Extractor Estimating: 496it [04:58,  1.46it/s]Extractor Estimating: 497it [04:59,  1.51it/s]Extractor Estimating: 498it [04:59,  1.53it/s]Extractor Estimating: 499it [05:00,  1.52it/s]Extractor Estimating: 500it [05:01,  1.49it/s]Extractor Estimating: 500it [05:01,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:36,742 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:36,748 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:36,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:36,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:36,749 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:55:37,060 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:55:37,061 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:37,326 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:38,396 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:38,396 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:40,928 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:40,932 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:40,932 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:40,932 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:55:40,933 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:55:41,261 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:55:41,262 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:55:41,573 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:55:41,736 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:55:41,736 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 04:02:18,501 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 04:02:18,528 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9999 mean pseudo reward: 0.93228659997766
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 17818
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17918, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17918, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.069, loss:572.2645
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.112, loss:521.6311
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.078, loss:511.5407
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.088, loss:527.2783
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 83, avg_time 1.084, loss:501.8479
>> valid entity prec:0.7044, rec:0.6654, f1:0.6843
>> valid relation prec:0.6241, rec:0.3106, f1:0.4147
>> valid relation with NER prec:0.6241, rec:0.3106, f1:0.4147
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 183, avg_time 2.436, loss:477.0935
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 283, avg_time 1.081, loss:496.4805
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 383, avg_time 1.084, loss:479.4906
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 66, avg_time 1.086, loss:477.0264
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 166, avg_time 1.061, loss:484.8188
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6417, rec:0.6743, f1:0.6576
>> valid relation prec:0.4824, rec:0.3057, f1:0.3742
>> valid relation with NER prec:0.4824, rec:0.3057, f1:0.3742
g_step 1100, step 266, avg_time 2.433, loss:470.9360
g_step 1200, step 366, avg_time 1.079, loss:480.6905
g_step 1300, step 49, avg_time 1.099, loss:459.5885
g_step 1400, step 149, avg_time 1.065, loss:425.9057
g_step 1500, step 249, avg_time 1.072, loss:442.1564
>> valid entity prec:0.6838, rec:0.6897, f1:0.6867
>> valid relation prec:0.5388, rec:0.3294, f1:0.4089
>> valid relation with NER prec:0.5388, rec:0.3294, f1:0.4089
new max entity f1 on valid!
g_step 1600, step 349, avg_time 2.450, loss:468.7054
g_step 1700, step 32, avg_time 1.062, loss:436.4149
g_step 1800, step 132, avg_time 1.095, loss:412.4900
g_step 1900, step 232, avg_time 1.065, loss:416.3769
g_step 2000, step 332, avg_time 1.073, loss:423.1128
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6609, rec:0.6571, f1:0.6590
>> valid relation prec:0.5426, rec:0.2802, f1:0.3696
>> valid relation with NER prec:0.5426, rec:0.2802, f1:0.3696
g_step 2100, step 15, avg_time 2.444, loss:430.7791
g_step 2200, step 115, avg_time 1.086, loss:384.2955
g_step 2300, step 215, avg_time 1.077, loss:391.1258
g_step 2400, step 315, avg_time 1.085, loss:405.5910
g_step 2500, step 415, avg_time 1.086, loss:416.0888
>> valid entity prec:0.6441, rec:0.7029, f1:0.6722
>> valid relation prec:0.4907, rec:0.3163, f1:0.3846
>> valid relation with NER prec:0.4907, rec:0.3163, f1:0.3846
g_step 2600, step 98, avg_time 2.441, loss:357.5132
g_step 2700, step 198, avg_time 1.076, loss:386.0038
g_step 2800, step 298, avg_time 1.079, loss:394.7486
g_step 2900, step 398, avg_time 1.089, loss:370.4270
g_step 3000, step 81, avg_time 1.071, loss:368.8012
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6474, rec:0.7103, f1:0.6774
>> valid relation prec:0.4568, rec:0.3054, f1:0.3661
>> valid relation with NER prec:0.4568, rec:0.3054, f1:0.3661
g_step 3100, step 181, avg_time 2.447, loss:354.3968
g_step 3200, step 281, avg_time 1.090, loss:347.9446
g_step 3300, step 381, avg_time 1.082, loss:365.2158
g_step 3400, step 64, avg_time 1.062, loss:350.6582
g_step 3500, step 164, avg_time 1.080, loss:334.0759
>> valid entity prec:0.7122, rec:0.6905, f1:0.7012
>> valid relation prec:0.5288, rec:0.3303, f1:0.4066
>> valid relation with NER prec:0.5288, rec:0.3303, f1:0.4066
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 264, avg_time 2.466, loss:335.9753
g_step 3700, step 364, avg_time 1.071, loss:347.3771
g_step 3800, step 47, avg_time 1.088, loss:315.6827
g_step 3900, step 147, avg_time 1.066, loss:329.0140
g_step 4000, step 247, avg_time 1.084, loss:324.2661
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.7083, rec:0.6717, f1:0.6895
>> valid relation prec:0.4643, rec:0.2808, f1:0.3500
>> valid relation with NER prec:0.4643, rec:0.2808, f1:0.3500
g_step 4100, step 347, avg_time 2.455, loss:324.3409
g_step 4200, step 30, avg_time 1.082, loss:332.6809
g_step 4300, step 130, avg_time 1.075, loss:291.1255
g_step 4400, step 230, avg_time 1.091, loss:321.7668
g_step 4500, step 330, avg_time 1.092, loss:324.4855
>> valid entity prec:0.6964, rec:0.7006, f1:0.6985
>> valid relation prec:0.5360, rec:0.3320, f1:0.4100
>> valid relation with NER prec:0.5360, rec:0.3320, f1:0.4100
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4600, step 13, avg_time 2.431, loss:329.9173
g_step 4700, step 113, avg_time 1.093, loss:274.3119
g_step 4800, step 213, avg_time 1.068, loss:307.9218
g_step 4900, step 313, avg_time 1.093, loss:318.5100
g_step 5000, step 413, avg_time 1.083, loss:302.3165
learning rate was adjusted to 0.0008
>> valid entity prec:0.7417, rec:0.6728, f1:0.7056
>> valid relation prec:0.5790, rec:0.3260, f1:0.4171
>> valid relation with NER prec:0.5790, rec:0.3260, f1:0.4171
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5100, step 96, avg_time 2.425, loss:281.6380
g_step 5200, step 196, avg_time 1.095, loss:282.0829
g_step 5300, step 296, avg_time 1.086, loss:296.7522
g_step 5400, step 396, avg_time 1.085, loss:295.7612
g_step 5500, step 79, avg_time 1.068, loss:266.9201
>> valid entity prec:0.6976, rec:0.6910, f1:0.6943
>> valid relation prec:0.4978, rec:0.2965, f1:0.3717
>> valid relation with NER prec:0.4978, rec:0.2965, f1:0.3717
g_step 5600, step 179, avg_time 2.425, loss:260.0677
g_step 5700, step 279, avg_time 1.075, loss:285.6511
g_step 5800, step 379, avg_time 1.090, loss:295.3980
g_step 5900, step 62, avg_time 1.094, loss:261.2420
g_step 6000, step 162, avg_time 1.075, loss:260.1862
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6627, rec:0.7259, f1:0.6928
>> valid relation prec:0.4469, rec:0.3406, f1:0.3866
>> valid relation with NER prec:0.4469, rec:0.3406, f1:0.3866
g_step 6100, step 262, avg_time 2.440, loss:268.4016
g_step 6200, step 362, avg_time 1.083, loss:284.1574
g_step 6300, step 45, avg_time 1.088, loss:262.7351
g_step 6400, step 145, avg_time 1.103, loss:248.4411
g_step 6500, step 245, avg_time 1.082, loss:262.5130
>> valid entity prec:0.6598, rec:0.6814, f1:0.6704
>> valid relation prec:0.4613, rec:0.3065, f1:0.3683
>> valid relation with NER prec:0.4613, rec:0.3065, f1:0.3683
g_step 6600, step 345, avg_time 2.430, loss:267.1235
g_step 6700, step 28, avg_time 1.095, loss:264.5291
g_step 6800, step 128, avg_time 1.068, loss:230.0075
g_step 6900, step 228, avg_time 1.077, loss:247.5829
g_step 7000, step 328, avg_time 1.075, loss:265.2586
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6904, rec:0.6621, f1:0.6759
>> valid relation prec:0.3740, rec:0.2382, f1:0.2911
>> valid relation with NER prec:0.3740, rec:0.2382, f1:0.2911
g_step 7100, step 11, avg_time 2.453, loss:250.8426
g_step 7200, step 111, avg_time 1.080, loss:226.4347
g_step 7300, step 211, avg_time 1.080, loss:228.3686
g_step 7400, step 311, avg_time 1.081, loss:242.4085
g_step 7500, step 411, avg_time 1.072, loss:251.1752
>> valid entity prec:0.6882, rec:0.6747, f1:0.6814
>> valid relation prec:0.4599, rec:0.3146, f1:0.3736
>> valid relation with NER prec:0.4599, rec:0.3146, f1:0.3736
g_step 7600, step 94, avg_time 2.449, loss:221.1273
g_step 7700, step 194, avg_time 1.062, loss:231.7845
g_step 7800, step 294, avg_time 1.080, loss:221.8595
g_step 7900, step 394, avg_time 1.090, loss:249.4117
g_step 8000, step 77, avg_time 1.073, loss:202.8279
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6866, rec:0.6579, f1:0.6719
>> valid relation prec:0.4468, rec:0.3040, f1:0.3618
>> valid relation with NER prec:0.4468, rec:0.3040, f1:0.3618
g_step 8100, step 177, avg_time 2.442, loss:222.4891
g_step 8200, step 277, avg_time 1.082, loss:233.7807
g_step 8300, step 377, avg_time 1.089, loss:235.7196
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 04:02:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 04:02:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_04-02-18_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 04:02:19 - WARNING - datasets.builder -   Using custom data configuration default-adbbc2b73cce93c0
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-adbbc2b73cce93c0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 04:02:19,770 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:02:19,771 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:02:19,771 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:02:19,772 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:02:19,779 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:19,782 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:19,782 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:19,783 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:19,783 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:19,783 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:02:19,783 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 04:02:19,910 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:02:22,992 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 04:02:22,997 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-adbbc2b73cce93c0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.30ba/s] 20%|██        | 2/10 [00:00<00:01,  4.13ba/s] 30%|███       | 3/10 [00:00<00:01,  4.46ba/s] 40%|████      | 4/10 [00:00<00:01,  4.68ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.82ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.88ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.94ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.97ba/s] 90%|█████████ | 9/10 [00:01<00:00,  4.96ba/s]100%|██████████| 10/10 [00:02<00:00,  4.95ba/s]100%|██████████| 10/10 [00:02<00:00,  4.76ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.91ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.17ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.32ba/s]100%|██████████| 4/4 [00:00<00:00,  5.36ba/s]100%|██████████| 4/4 [00:00<00:00,  4.84ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  7.86ba/s] 30%|███       | 3/10 [00:00<00:00,  9.42ba/s] 40%|████      | 4/10 [00:00<00:00,  9.55ba/s] 60%|██████    | 6/10 [00:00<00:00,  9.79ba/s] 80%|████████  | 8/10 [00:00<00:00,  9.96ba/s]100%|██████████| 10/10 [00:01<00:00,  9.96ba/s]100%|██████████| 10/10 [00:01<00:00,  9.78ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.81ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.94ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.37ba/s]100%|██████████| 4/4 [00:00<00:00, 10.53ba/s]
[INFO|trainer.py:414] 2023-08-29 04:02:27,724 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 04:02:27,738 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 04:02:27,738 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 04:02:27,738 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 04:02:27,738 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 04:02:27,738 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 04:02:27,738 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 04:02:27,738 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:57,  3.27it/s]  0%|          | 2/780 [00:00<03:49,  3.39it/s]  0%|          | 3/780 [00:00<03:46,  3.43it/s]  1%|          | 4/780 [00:01<03:44,  3.45it/s]  1%|          | 5/780 [00:01<03:43,  3.46it/s]  1%|          | 6/780 [00:01<03:43,  3.46it/s]  1%|          | 7/780 [00:02<03:43,  3.46it/s]  1%|          | 8/780 [00:02<03:42,  3.47it/s]  1%|          | 9/780 [00:02<03:42,  3.47it/s]  1%|▏         | 10/780 [00:02<03:41,  3.47it/s]  1%|▏         | 11/780 [00:03<03:41,  3.48it/s]  2%|▏         | 12/780 [00:03<03:40,  3.48it/s]  2%|▏         | 13/780 [00:03<03:40,  3.48it/s]  2%|▏         | 14/780 [00:04<03:40,  3.48it/s]  2%|▏         | 15/780 [00:04<03:40,  3.48it/s]  2%|▏         | 16/780 [00:04<03:39,  3.48it/s]  2%|▏         | 17/780 [00:04<03:39,  3.47it/s]  2%|▏         | 18/780 [00:05<03:39,  3.47it/s]  2%|▏         | 19/780 [00:05<03:39,  3.47it/s]  3%|▎         | 20/780 [00:05<03:38,  3.47it/s]  3%|▎         | 21/780 [00:06<03:38,  3.47it/s]  3%|▎         | 22/780 [00:06<03:38,  3.47it/s]  3%|▎         | 23/780 [00:06<03:37,  3.47it/s]  3%|▎         | 24/780 [00:06<03:37,  3.48it/s]  3%|▎         | 25/780 [00:07<03:37,  3.47it/s]  3%|▎         | 26/780 [00:07<03:37,  3.47it/s]  3%|▎         | 27/780 [00:07<03:36,  3.47it/s]  4%|▎         | 28/780 [00:08<03:39,  3.43it/s]  4%|▎         | 29/780 [00:08<03:38,  3.44it/s]  4%|▍         | 30/780 [00:08<03:37,  3.45it/s]  4%|▍         | 31/780 [00:08<03:36,  3.45it/s]  4%|▍         | 32/780 [00:09<03:36,  3.46it/s]  4%|▍         | 33/780 [00:09<03:35,  3.46it/s]  4%|▍         | 34/780 [00:09<03:35,  3.46it/s]  4%|▍         | 35/780 [00:10<03:34,  3.47it/s]  5%|▍         | 36/780 [00:10<03:34,  3.47it/s]  5%|▍         | 37/780 [00:10<03:34,  3.47it/s]  5%|▍         | 38/780 [00:10<03:33,  3.47it/s]  5%|▌         | 39/780 [00:11<03:34,  3.46it/s]  5%|▌         | 40/780 [00:11<03:33,  3.46it/s]  5%|▌         | 41/780 [00:11<03:33,  3.46it/s]  5%|▌         | 42/780 [00:12<03:33,  3.46it/s]  6%|▌         | 43/780 [00:12<03:32,  3.46it/s]  6%|▌         | 44/780 [00:12<03:32,  3.47it/s]  6%|▌         | 45/780 [00:12<03:31,  3.47it/s]  6%|▌         | 46/780 [00:13<03:31,  3.47it/s]  6%|▌         | 47/780 [00:13<03:31,  3.47it/s]  6%|▌         | 48/780 [00:13<03:31,  3.47it/s]  6%|▋         | 49/780 [00:14<03:30,  3.47it/s]  6%|▋         | 50/780 [00:14<03:30,  3.46it/s]  7%|▋         | 51/780 [00:14<03:30,  3.46it/s]  7%|▋         | 52/780 [00:15<03:30,  3.46it/s]  7%|▋         | 53/780 [00:15<03:29,  3.47it/s]  7%|▋         | 54/780 [00:15<03:29,  3.47it/s]  7%|▋         | 55/780 [00:15<03:29,  3.47it/s]  7%|▋         | 56/780 [00:16<03:28,  3.47it/s]  7%|▋         | 57/780 [00:16<03:28,  3.47it/s]  7%|▋         | 58/780 [00:16<03:28,  3.47it/s]  8%|▊         | 59/780 [00:17<03:27,  3.47it/s]  8%|▊         | 60/780 [00:17<03:27,  3.47it/s]  8%|▊         | 61/780 [00:17<03:28,  3.46it/s]  8%|▊         | 62/780 [00:17<03:27,  3.46it/s]  8%|▊         | 63/780 [00:18<03:27,  3.46it/s]  8%|▊         | 64/780 [00:18<03:26,  3.46it/s]  8%|▊         | 65/780 [00:18<03:26,  3.46it/s]  8%|▊         | 66/780 [00:19<03:26,  3.47it/s]  9%|▊         | 67/780 [00:19<03:25,  3.46it/s]  9%|▊         | 68/780 [00:19<03:25,  3.47it/s]  9%|▉         | 69/780 [00:19<03:25,  3.47it/s]  9%|▉         | 70/780 [00:20<03:24,  3.47it/s]  9%|▉         | 71/780 [00:20<03:24,  3.47it/s]  9%|▉         | 72/780 [00:20<03:24,  3.46it/s]  9%|▉         | 73/780 [00:21<03:24,  3.46it/s]  9%|▉         | 74/780 [00:21<03:23,  3.46it/s] 10%|▉         | 75/780 [00:21<03:23,  3.46it/s] 10%|▉         | 76/780 [00:21<03:23,  3.46it/s] 10%|▉         | 77/780 [00:22<03:22,  3.47it/s] 10%|█         | 78/780 [00:22<03:22,  3.47it/s] 10%|█         | 79/780 [00:22<03:22,  3.47it/s] 10%|█         | 80/780 [00:23<03:21,  3.47it/s] 10%|█         | 81/780 [00:23<03:21,  3.47it/s] 11%|█         | 82/780 [00:23<03:21,  3.47it/s] 11%|█         | 83/780 [00:23<03:21,  3.47it/s] 11%|█         | 84/780 [00:24<03:20,  3.47it/s] 11%|█         | 85/780 [00:24<03:20,  3.47it/s] 11%|█         | 86/780 [00:24<03:21,  3.44it/s] 11%|█         | 87/780 [00:25<03:20,  3.45it/s] 11%|█▏        | 88/780 [00:25<03:20,  3.46it/s] 11%|█▏        | 89/780 [00:25<03:19,  3.46it/s] 12%|█▏        | 90/780 [00:25<03:19,  3.46it/s] 12%|█▏        | 91/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 92/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 93/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 94/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 95/780 [00:27<03:17,  3.47it/s] 12%|█▏        | 96/780 [00:27<03:17,  3.47it/s] 12%|█▏        | 97/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 98/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 99/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 100/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.46it/s] 13%|█▎        | 102/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 103/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 104/780 [00:30<03:15,  3.46it/s] 13%|█▎        | 105/780 [00:30<03:14,  3.46it/s] 14%|█▎        | 106/780 [00:30<03:14,  3.46it/s] 14%|█▎        | 107/780 [00:30<03:14,  3.46it/s] 14%|█▍        | 108/780 [00:31<03:15,  3.44it/s] 14%|█▍        | 109/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 110/780 [00:31<03:14,  3.45it/s] 14%|█▍        | 111/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 112/780 [00:32<03:13,  3.46it/s] 14%|█▍        | 113/780 [00:32<03:12,  3.46it/s] 15%|█▍        | 114/780 [00:32<03:12,  3.46it/s] 15%|█▍        | 115/780 [00:33<03:11,  3.46it/s] 15%|█▍        | 116/780 [00:33<03:11,  3.46it/s] 15%|█▌        | 117/780 [00:33<03:11,  3.46it/s] 15%|█▌        | 118/780 [00:34<03:11,  3.46it/s] 15%|█▌        | 119/780 [00:34<03:11,  3.45it/s] 15%|█▌        | 120/780 [00:34<03:10,  3.46it/s] 16%|█▌        | 121/780 [00:34<03:10,  3.46it/s] 16%|█▌        | 122/780 [00:35<03:10,  3.46it/s] 16%|█▌        | 123/780 [00:35<03:10,  3.46it/s] 16%|█▌        | 124/780 [00:35<03:09,  3.46it/s] 16%|█▌        | 125/780 [00:36<03:09,  3.46it/s] 16%|█▌        | 126/780 [00:36<03:08,  3.46it/s] 16%|█▋        | 127/780 [00:36<03:08,  3.46it/s] 16%|█▋        | 128/780 [00:36<03:08,  3.46it/s] 17%|█▋        | 129/780 [00:37<03:07,  3.46it/s] 17%|█▋        | 130/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 131/780 [00:37<03:07,  3.45it/s] 17%|█▋        | 132/780 [00:38<03:07,  3.46it/s] 17%|█▋        | 133/780 [00:38<03:07,  3.46it/s] 17%|█▋        | 134/780 [00:38<03:06,  3.46it/s] 17%|█▋        | 135/780 [00:38<03:06,  3.46it/s] 17%|█▋        | 136/780 [00:39<03:06,  3.46it/s] 18%|█▊        | 137/780 [00:39<03:05,  3.46it/s] 18%|█▊        | 138/780 [00:39<03:05,  3.46it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.46it/s] 18%|█▊        | 140/780 [00:40<03:04,  3.46it/s] 18%|█▊        | 141/780 [00:40<03:04,  3.46it/s] 18%|█▊        | 142/780 [00:41<03:04,  3.46it/s] 18%|█▊        | 143/780 [00:41<03:04,  3.46it/s] 18%|█▊        | 144/780 [00:41<03:03,  3.46it/s] 19%|█▊        | 145/780 [00:41<03:03,  3.46it/s] 19%|█▊        | 146/780 [00:42<03:03,  3.46it/s] 19%|█▉        | 147/780 [00:42<03:03,  3.46it/s] 19%|█▉        | 148/780 [00:42<03:02,  3.46it/s] 19%|█▉        | 149/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 150/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 151/780 [00:43<03:01,  3.46it/s] 19%|█▉        | 152/780 [00:43<03:01,  3.45it/s] 20%|█▉        | 153/780 [00:44<03:01,  3.46it/s] 20%|█▉        | 154/780 [00:44<03:01,  3.46it/s] 20%|█▉        | 155/780 [00:44<03:00,  3.46it/s] 20%|██        | 156/780 [00:45<03:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 04:03:12,848 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:03:12,848 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 04:03:12,848 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.27it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.47it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.85it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.26it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.80it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.56it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.23it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.86it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.84it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.97it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.90it/s][A
 14%|█▍        | 63/438 [00:01<00:07, 46.94it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.93it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.91it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.99it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.90it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.82it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.74it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.82it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.86it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.95it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.86it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.94it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.94it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.87it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.85it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.75it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.79it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.83it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.92it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.81it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.92it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.93it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.98it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.89it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.83it/s][A
 43%|████▎     | 188/438 [00:03<00:05, 46.84it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.79it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.75it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.80it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.83it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.80it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.88it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.89it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.85it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.84it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.87it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.81it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.84it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.72it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.85it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.82it/s][A
 61%|██████    | 268/438 [00:05<00:03, 45.77it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.18it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.27it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.49it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.55it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.66it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.78it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.79it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.74it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.81it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.83it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.83it/s][A
 75%|███████▍  | 328/438 [00:06<00:02, 46.80it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.83it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.75it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.88it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.85it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.80it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.82it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.85it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.89it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.95it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.86it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.76it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.88it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.80it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.88it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.80it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.84it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.78it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.94it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.90it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.85it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.89it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.89it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.89it/s][A 20%|██        | 156/780 [00:54<03:00,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:03:22,251 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 04:03:22,265 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:03:24,397 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:03:24,417 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:03:24,435 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:01<53:27,  5.15s/it] 20%|██        | 158/780 [01:01<38:16,  3.69s/it] 20%|██        | 159/780 [01:02<27:39,  2.67s/it] 21%|██        | 160/780 [01:02<20:13,  1.96s/it] 21%|██        | 161/780 [01:02<15:01,  1.46s/it] 21%|██        | 162/780 [01:03<11:23,  1.11s/it] 21%|██        | 163/780 [01:03<08:51,  1.16it/s] 21%|██        | 164/780 [01:03<07:04,  1.45it/s] 21%|██        | 165/780 [01:03<05:50,  1.76it/s] 21%|██▏       | 166/780 [01:04<04:57,  2.06it/s] 21%|██▏       | 167/780 [01:04<04:21,  2.35it/s] 22%|██▏       | 168/780 [01:04<03:55,  2.60it/s] 22%|██▏       | 169/780 [01:05<03:38,  2.80it/s] 22%|██▏       | 170/780 [01:05<03:25,  2.97it/s] 22%|██▏       | 171/780 [01:05<03:16,  3.11it/s] 22%|██▏       | 172/780 [01:05<03:09,  3.20it/s] 22%|██▏       | 173/780 [01:06<03:05,  3.28it/s] 22%|██▏       | 174/780 [01:06<03:01,  3.33it/s] 22%|██▏       | 175/780 [01:06<02:59,  3.37it/s] 23%|██▎       | 176/780 [01:07<02:57,  3.40it/s] 23%|██▎       | 177/780 [01:07<02:56,  3.42it/s] 23%|██▎       | 178/780 [01:07<02:55,  3.43it/s] 23%|██▎       | 179/780 [01:07<02:54,  3.44it/s] 23%|██▎       | 180/780 [01:08<02:54,  3.44it/s] 23%|██▎       | 181/780 [01:08<02:53,  3.45it/s] 23%|██▎       | 182/780 [01:08<02:53,  3.45it/s] 23%|██▎       | 183/780 [01:09<02:52,  3.46it/s] 24%|██▎       | 184/780 [01:09<02:52,  3.46it/s] 24%|██▎       | 185/780 [01:09<02:52,  3.46it/s] 24%|██▍       | 186/780 [01:09<02:51,  3.46it/s] 24%|██▍       | 187/780 [01:10<02:51,  3.46it/s] 24%|██▍       | 188/780 [01:10<02:51,  3.46it/s] 24%|██▍       | 189/780 [01:10<02:50,  3.46it/s] 24%|██▍       | 190/780 [01:11<02:50,  3.46it/s] 24%|██▍       | 191/780 [01:11<02:50,  3.45it/s] 25%|██▍       | 192/780 [01:11<02:50,  3.45it/s] 25%|██▍       | 193/780 [01:11<02:49,  3.46it/s] 25%|██▍       | 194/780 [01:12<02:49,  3.46it/s] 25%|██▌       | 195/780 [01:12<02:49,  3.46it/s] 25%|██▌       | 196/780 [01:12<02:48,  3.46it/s] 25%|██▌       | 197/780 [01:13<02:48,  3.46it/s] 25%|██▌       | 198/780 [01:13<02:48,  3.46it/s] 26%|██▌       | 199/780 [01:13<02:47,  3.46it/s] 26%|██▌       | 200/780 [01:13<02:47,  3.46it/s] 26%|██▌       | 201/780 [01:14<02:47,  3.46it/s] 26%|██▌       | 202/780 [01:14<02:47,  3.46it/s] 26%|██▌       | 203/780 [01:14<02:46,  3.46it/s] 26%|██▌       | 204/780 [01:15<02:46,  3.46it/s] 26%|██▋       | 205/780 [01:15<02:46,  3.46it/s] 26%|██▋       | 206/780 [01:15<02:45,  3.46it/s] 27%|██▋       | 207/780 [01:16<02:45,  3.46it/s] 27%|██▋       | 208/780 [01:16<02:45,  3.46it/s] 27%|██▋       | 209/780 [01:16<02:44,  3.46it/s] 27%|██▋       | 210/780 [01:16<02:44,  3.46it/s] 27%|██▋       | 211/780 [01:17<02:44,  3.46it/s] 27%|██▋       | 212/780 [01:17<02:44,  3.46it/s] 27%|██▋       | 213/780 [01:17<02:44,  3.45it/s] 27%|██▋       | 214/780 [01:18<02:44,  3.45it/s] 28%|██▊       | 215/780 [01:18<02:43,  3.45it/s] 28%|██▊       | 216/780 [01:18<02:43,  3.46it/s] 28%|██▊       | 217/780 [01:18<02:42,  3.46it/s] 28%|██▊       | 218/780 [01:19<02:42,  3.46it/s] 28%|██▊       | 219/780 [01:19<02:42,  3.46it/s] 28%|██▊       | 220/780 [01:19<02:41,  3.46it/s] 28%|██▊       | 221/780 [01:20<02:41,  3.46it/s] 28%|██▊       | 222/780 [01:20<02:41,  3.46it/s] 29%|██▊       | 223/780 [01:20<02:40,  3.46it/s] 29%|██▊       | 224/780 [01:20<02:41,  3.44it/s] 29%|██▉       | 225/780 [01:21<02:40,  3.45it/s] 29%|██▉       | 226/780 [01:21<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:21<02:39,  3.46it/s] 29%|██▉       | 228/780 [01:22<02:39,  3.46it/s] 29%|██▉       | 229/780 [01:22<02:39,  3.46it/s] 29%|██▉       | 230/780 [01:22<02:38,  3.46it/s] 30%|██▉       | 231/780 [01:22<02:38,  3.46it/s] 30%|██▉       | 232/780 [01:23<02:38,  3.46it/s] 30%|██▉       | 233/780 [01:23<02:38,  3.46it/s] 30%|███       | 234/780 [01:23<02:37,  3.46it/s] 30%|███       | 235/780 [01:24<02:37,  3.46it/s] 30%|███       | 236/780 [01:24<02:37,  3.46it/s] 30%|███       | 237/780 [01:24<02:36,  3.46it/s] 31%|███       | 238/780 [01:24<02:36,  3.46it/s] 31%|███       | 239/780 [01:25<02:36,  3.46it/s] 31%|███       | 240/780 [01:25<02:36,  3.46it/s] 31%|███       | 241/780 [01:25<02:35,  3.46it/s] 31%|███       | 242/780 [01:26<02:35,  3.46it/s] 31%|███       | 243/780 [01:26<02:35,  3.46it/s] 31%|███▏      | 244/780 [01:26<02:34,  3.46it/s] 31%|███▏      | 245/780 [01:26<02:34,  3.46it/s] 32%|███▏      | 246/780 [01:27<02:34,  3.46it/s] 32%|███▏      | 247/780 [01:27<02:33,  3.46it/s] 32%|███▏      | 248/780 [01:27<02:33,  3.46it/s] 32%|███▏      | 249/780 [01:28<02:33,  3.46it/s] 32%|███▏      | 250/780 [01:28<02:33,  3.46it/s] 32%|███▏      | 251/780 [01:28<02:32,  3.46it/s] 32%|███▏      | 252/780 [01:29<02:32,  3.46it/s] 32%|███▏      | 253/780 [01:29<02:32,  3.45it/s] 33%|███▎      | 254/780 [01:29<02:32,  3.45it/s] 33%|███▎      | 255/780 [01:29<02:32,  3.45it/s] 33%|███▎      | 256/780 [01:30<02:31,  3.46it/s] 33%|███▎      | 257/780 [01:30<02:31,  3.46it/s] 33%|███▎      | 258/780 [01:30<02:30,  3.46it/s] 33%|███▎      | 259/780 [01:31<02:30,  3.46it/s] 33%|███▎      | 260/780 [01:31<02:30,  3.46it/s] 33%|███▎      | 261/780 [01:31<02:29,  3.46it/s] 34%|███▎      | 262/780 [01:31<02:29,  3.46it/s] 34%|███▎      | 263/780 [01:32<02:29,  3.46it/s] 34%|███▍      | 264/780 [01:32<02:29,  3.45it/s] 34%|███▍      | 265/780 [01:32<02:29,  3.45it/s] 34%|███▍      | 266/780 [01:33<02:28,  3.46it/s] 34%|███▍      | 267/780 [01:33<02:28,  3.46it/s] 34%|███▍      | 268/780 [01:33<02:28,  3.46it/s] 34%|███▍      | 269/780 [01:33<02:27,  3.46it/s] 35%|███▍      | 270/780 [01:34<02:27,  3.46it/s] 35%|███▍      | 271/780 [01:34<02:27,  3.46it/s] 35%|███▍      | 272/780 [01:34<02:26,  3.46it/s] 35%|███▌      | 273/780 [01:35<02:26,  3.46it/s] 35%|███▌      | 274/780 [01:35<02:26,  3.46it/s] 35%|███▌      | 275/780 [01:35<02:25,  3.46it/s] 35%|███▌      | 276/780 [01:35<02:25,  3.46it/s] 36%|███▌      | 277/780 [01:36<02:25,  3.46it/s] 36%|███▌      | 278/780 [01:36<02:25,  3.45it/s] 36%|███▌      | 279/780 [01:36<02:25,  3.45it/s] 36%|███▌      | 280/780 [01:37<02:24,  3.45it/s] 36%|███▌      | 281/780 [01:37<02:24,  3.45it/s] 36%|███▌      | 282/780 [01:37<02:24,  3.46it/s] 36%|███▋      | 283/780 [01:37<02:23,  3.46it/s] 36%|███▋      | 284/780 [01:38<02:23,  3.46it/s] 37%|███▋      | 285/780 [01:38<02:23,  3.46it/s] 37%|███▋      | 286/780 [01:38<02:22,  3.46it/s] 37%|███▋      | 287/780 [01:39<02:22,  3.46it/s] 37%|███▋      | 288/780 [01:39<02:22,  3.46it/s] 37%|███▋      | 289/780 [01:39<02:21,  3.46it/s] 37%|███▋      | 290/780 [01:40<02:21,  3.46it/s] 37%|███▋      | 291/780 [01:40<02:21,  3.46it/s] 37%|███▋      | 292/780 [01:40<02:21,  3.46it/s] 38%|███▊      | 293/780 [01:40<02:20,  3.46it/s] 38%|███▊      | 294/780 [01:41<02:20,  3.46it/s] 38%|███▊      | 295/780 [01:41<02:20,  3.46it/s] 38%|███▊      | 296/780 [01:41<02:20,  3.44it/s] 38%|███▊      | 297/780 [01:42<02:20,  3.44it/s] 38%|███▊      | 298/780 [01:42<02:19,  3.44it/s] 38%|███▊      | 299/780 [01:42<02:19,  3.45it/s] 38%|███▊      | 300/780 [01:42<02:19,  3.45it/s] 39%|███▊      | 301/780 [01:43<02:18,  3.45it/s] 39%|███▊      | 302/780 [01:43<02:18,  3.45it/s] 39%|███▉      | 303/780 [01:43<02:18,  3.45it/s] 39%|███▉      | 304/780 [01:44<02:17,  3.45it/s] 39%|███▉      | 305/780 [01:44<02:17,  3.45it/s] 39%|███▉      | 306/780 [01:44<02:17,  3.45it/s] 39%|███▉      | 307/780 [01:44<02:16,  3.45it/s] 39%|███▉      | 308/780 [01:45<02:16,  3.45it/s] 40%|███▉      | 309/780 [01:45<02:16,  3.45it/s] 40%|███▉      | 310/780 [01:45<02:16,  3.45it/s] 40%|███▉      | 311/780 [01:46<02:15,  3.45it/s] 40%|████      | 312/780 [01:46<02:15,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 04:04:14,168 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:04:14,168 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 04:04:14,168 >>   Batch size = 8
{'eval_loss': 1.060680627822876, 'eval_runtime': 9.3764, 'eval_samples_per_second': 372.958, 'eval_steps_per_second': 46.713, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.30it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.64it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.71it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.13it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.68it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.33it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.14it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.82it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.63it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.71it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.69it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.72it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.77it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.81it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.78it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.72it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.72it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.59it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.76it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.71it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.76it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.66it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.70it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.78it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.76it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.74it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.62it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.65it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.73it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.71it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.66it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.71it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.72it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.80it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.66it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.69it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.60it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.58it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.59it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 43.11it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 44.91it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.44it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.93it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.07it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.37it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.48it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.52it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.62it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.58it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.49it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.50it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.65it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.69it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.68it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.76it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.74it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.78it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.77it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.63it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.64it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.60it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.70it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.71it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.74it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.66it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.74it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.70it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.63it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.70it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.68it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.61it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.66it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.69it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.70it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.78it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.69it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.66it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.70it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.68it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.71it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.71it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.64it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.64it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.70it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.67it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.68it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.79it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.79it/s][A 40%|████      | 312/780 [01:55<02:15,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:04:23,611 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 04:04:23,624 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:04:25,986 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:04:26,007 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:04:26,016 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:03<41:38,  5.35s/it] 40%|████      | 314/780 [02:03<29:46,  3.83s/it] 40%|████      | 315/780 [02:04<21:27,  2.77s/it] 41%|████      | 316/780 [02:04<15:39,  2.03s/it] 41%|████      | 317/780 [02:04<11:36,  1.50s/it] 41%|████      | 318/780 [02:04<08:46,  1.14s/it] 41%|████      | 319/780 [02:05<06:47,  1.13it/s] 41%|████      | 320/780 [02:05<05:24,  1.42it/s] 41%|████      | 321/780 [02:05<04:26,  1.72it/s] 41%|████▏     | 322/780 [02:06<03:45,  2.03it/s] 41%|████▏     | 323/780 [02:06<03:17,  2.32it/s] 42%|████▏     | 324/780 [02:06<02:57,  2.57it/s] 42%|████▏     | 325/780 [02:07<02:43,  2.78it/s] 42%|████▏     | 326/780 [02:07<02:33,  2.96it/s] 42%|████▏     | 327/780 [02:07<02:26,  3.09it/s] 42%|████▏     | 328/780 [02:07<02:21,  3.20it/s] 42%|████▏     | 329/780 [02:08<02:17,  3.27it/s] 42%|████▏     | 330/780 [02:08<02:15,  3.33it/s] 42%|████▏     | 331/780 [02:08<02:13,  3.37it/s] 43%|████▎     | 332/780 [02:09<02:12,  3.39it/s] 43%|████▎     | 333/780 [02:09<02:10,  3.42it/s] 43%|████▎     | 334/780 [02:09<02:10,  3.43it/s] 43%|████▎     | 335/780 [02:09<02:09,  3.44it/s] 43%|████▎     | 336/780 [02:10<02:09,  3.42it/s] 43%|████▎     | 337/780 [02:10<02:08,  3.44it/s] 43%|████▎     | 338/780 [02:10<02:08,  3.44it/s] 43%|████▎     | 339/780 [02:11<02:07,  3.45it/s] 44%|████▎     | 340/780 [02:11<02:07,  3.45it/s] 44%|████▎     | 341/780 [02:11<02:07,  3.46it/s] 44%|████▍     | 342/780 [02:11<02:06,  3.46it/s] 44%|████▍     | 343/780 [02:12<02:06,  3.46it/s] 44%|████▍     | 344/780 [02:12<02:05,  3.46it/s] 44%|████▍     | 345/780 [02:12<02:05,  3.46it/s] 44%|████▍     | 346/780 [02:13<02:05,  3.46it/s] 44%|████▍     | 347/780 [02:13<02:06,  3.41it/s] 45%|████▍     | 348/780 [02:13<02:06,  3.43it/s] 45%|████▍     | 349/780 [02:13<02:05,  3.44it/s] 45%|████▍     | 350/780 [02:14<02:04,  3.44it/s] 45%|████▌     | 351/780 [02:14<02:04,  3.45it/s] 45%|████▌     | 352/780 [02:14<02:03,  3.45it/s] 45%|████▌     | 353/780 [02:15<02:03,  3.46it/s] 45%|████▌     | 354/780 [02:15<02:03,  3.46it/s] 46%|████▌     | 355/780 [02:15<02:02,  3.46it/s] 46%|████▌     | 356/780 [02:15<02:02,  3.46it/s] 46%|████▌     | 357/780 [02:16<02:02,  3.46it/s] 46%|████▌     | 358/780 [02:16<02:02,  3.45it/s] 46%|████▌     | 359/780 [02:16<02:02,  3.45it/s] 46%|████▌     | 360/780 [02:17<02:01,  3.45it/s] 46%|████▋     | 361/780 [02:17<02:01,  3.45it/s] 46%|████▋     | 362/780 [02:17<02:00,  3.45it/s] 47%|████▋     | 363/780 [02:18<02:00,  3.46it/s] 47%|████▋     | 364/780 [02:18<02:00,  3.46it/s] 47%|████▋     | 365/780 [02:18<02:00,  3.46it/s] 47%|████▋     | 366/780 [02:18<01:59,  3.46it/s] 47%|████▋     | 367/780 [02:19<01:59,  3.46it/s] 47%|████▋     | 368/780 [02:19<01:59,  3.46it/s] 47%|████▋     | 369/780 [02:19<01:59,  3.45it/s] 47%|████▋     | 370/780 [02:20<01:58,  3.45it/s] 48%|████▊     | 371/780 [02:20<01:58,  3.46it/s] 48%|████▊     | 372/780 [02:20<01:58,  3.46it/s] 48%|████▊     | 373/780 [02:20<01:57,  3.46it/s] 48%|████▊     | 374/780 [02:21<01:57,  3.46it/s] 48%|████▊     | 375/780 [02:21<01:57,  3.46it/s] 48%|████▊     | 376/780 [02:21<01:56,  3.46it/s] 48%|████▊     | 377/780 [02:22<01:56,  3.46it/s] 48%|████▊     | 378/780 [02:22<01:56,  3.46it/s] 49%|████▊     | 379/780 [02:22<01:55,  3.46it/s] 49%|████▊     | 380/780 [02:22<01:56,  3.44it/s] 49%|████▉     | 381/780 [02:23<01:55,  3.45it/s] 49%|████▉     | 382/780 [02:23<01:55,  3.45it/s] 49%|████▉     | 383/780 [02:23<01:54,  3.46it/s] 49%|████▉     | 384/780 [02:24<01:54,  3.46it/s] 49%|████▉     | 385/780 [02:24<01:54,  3.46it/s] 49%|████▉     | 386/780 [02:24<01:53,  3.46it/s] 50%|████▉     | 387/780 [02:24<01:53,  3.46it/s] 50%|████▉     | 388/780 [02:25<01:53,  3.46it/s] 50%|████▉     | 389/780 [02:25<01:52,  3.46it/s] 50%|█████     | 390/780 [02:25<01:52,  3.46it/s] 50%|█████     | 391/780 [02:26<01:52,  3.46it/s] 50%|█████     | 392/780 [02:26<01:52,  3.46it/s] 50%|█████     | 393/780 [02:26<01:51,  3.46it/s] 51%|█████     | 394/780 [02:26<01:51,  3.46it/s] 51%|█████     | 395/780 [02:27<01:51,  3.46it/s] 51%|█████     | 396/780 [02:27<01:50,  3.46it/s] 51%|█████     | 397/780 [02:27<01:51,  3.44it/s] 51%|█████     | 398/780 [02:28<01:50,  3.45it/s] 51%|█████     | 399/780 [02:28<01:50,  3.45it/s] 51%|█████▏    | 400/780 [02:28<01:50,  3.45it/s] 51%|█████▏    | 401/780 [02:29<01:49,  3.45it/s] 52%|█████▏    | 402/780 [02:29<01:49,  3.46it/s] 52%|█████▏    | 403/780 [02:29<01:48,  3.46it/s] 52%|█████▏    | 404/780 [02:29<01:48,  3.46it/s] 52%|█████▏    | 405/780 [02:30<01:48,  3.45it/s] 52%|█████▏    | 406/780 [02:30<01:48,  3.45it/s] 52%|█████▏    | 407/780 [02:30<01:48,  3.45it/s] 52%|█████▏    | 408/780 [02:31<01:48,  3.44it/s] 52%|█████▏    | 409/780 [02:31<01:47,  3.44it/s] 53%|█████▎    | 410/780 [02:31<01:47,  3.45it/s] 53%|█████▎    | 411/780 [02:31<01:46,  3.45it/s] 53%|█████▎    | 412/780 [02:32<01:46,  3.46it/s] 53%|█████▎    | 413/780 [02:32<01:46,  3.46it/s] 53%|█████▎    | 414/780 [02:32<01:45,  3.46it/s] 53%|█████▎    | 415/780 [02:33<01:45,  3.46it/s] 53%|█████▎    | 416/780 [02:33<01:45,  3.46it/s] 53%|█████▎    | 417/780 [02:33<01:44,  3.46it/s] 54%|█████▎    | 418/780 [02:33<01:44,  3.46it/s] 54%|█████▎    | 419/780 [02:34<01:44,  3.45it/s] 54%|█████▍    | 420/780 [02:34<01:44,  3.45it/s] 54%|█████▍    | 421/780 [02:34<01:43,  3.46it/s] 54%|█████▍    | 422/780 [02:35<01:43,  3.46it/s] 54%|█████▍    | 423/780 [02:35<01:43,  3.46it/s] 54%|█████▍    | 424/780 [02:35<01:42,  3.46it/s] 54%|█████▍    | 425/780 [02:35<01:42,  3.46it/s] 55%|█████▍    | 426/780 [02:36<01:42,  3.46it/s] 55%|█████▍    | 427/780 [02:36<01:41,  3.46it/s] 55%|█████▍    | 428/780 [02:36<01:41,  3.46it/s] 55%|█████▌    | 429/780 [02:37<01:41,  3.46it/s] 55%|█████▌    | 430/780 [02:37<01:41,  3.46it/s] 55%|█████▌    | 431/780 [02:37<01:40,  3.46it/s] 55%|█████▌    | 432/780 [02:37<01:40,  3.45it/s] 56%|█████▌    | 433/780 [02:38<01:40,  3.45it/s] 56%|█████▌    | 434/780 [02:38<01:40,  3.45it/s] 56%|█████▌    | 435/780 [02:38<01:39,  3.45it/s] 56%|█████▌    | 436/780 [02:39<01:39,  3.46it/s] 56%|█████▌    | 437/780 [02:39<01:39,  3.46it/s] 56%|█████▌    | 438/780 [02:39<01:38,  3.46it/s] 56%|█████▋    | 439/780 [02:40<01:38,  3.46it/s] 56%|█████▋    | 440/780 [02:40<01:38,  3.46it/s] 57%|█████▋    | 441/780 [02:40<01:38,  3.46it/s] 57%|█████▋    | 442/780 [02:40<01:37,  3.46it/s] 57%|█████▋    | 443/780 [02:41<01:37,  3.45it/s] 57%|█████▋    | 444/780 [02:41<01:37,  3.45it/s] 57%|█████▋    | 445/780 [02:41<01:37,  3.45it/s] 57%|█████▋    | 446/780 [02:42<01:36,  3.45it/s] 57%|█████▋    | 447/780 [02:42<01:36,  3.45it/s] 57%|█████▋    | 448/780 [02:42<01:36,  3.45it/s] 58%|█████▊    | 449/780 [02:42<01:35,  3.45it/s] 58%|█████▊    | 450/780 [02:43<01:35,  3.45it/s] 58%|█████▊    | 451/780 [02:43<01:35,  3.45it/s] 58%|█████▊    | 452/780 [02:43<01:35,  3.45it/s] 58%|█████▊    | 453/780 [02:44<01:34,  3.45it/s] 58%|█████▊    | 454/780 [02:44<01:34,  3.45it/s] 58%|█████▊    | 455/780 [02:44<01:34,  3.45it/s] 58%|█████▊    | 456/780 [02:44<01:33,  3.45it/s] 59%|█████▊    | 457/780 [02:45<01:33,  3.45it/s] 59%|█████▊    | 458/780 [02:45<01:33,  3.45it/s] 59%|█████▉    | 459/780 [02:45<01:32,  3.45it/s] 59%|█████▉    | 460/780 [02:46<01:32,  3.45it/s] 59%|█████▉    | 461/780 [02:46<01:32,  3.45it/s] 59%|█████▉    | 462/780 [02:46<01:32,  3.45it/s] 59%|█████▉    | 463/780 [02:46<01:31,  3.45it/s] 59%|█████▉    | 464/780 [02:47<01:31,  3.45it/s] 60%|█████▉    | 465/780 [02:47<01:31,  3.45it/s] 60%|█████▉    | 466/780 [02:47<01:31,  3.45it/s] 60%|█████▉    | 467/780 [02:48<01:30,  3.44it/s] 60%|██████    | 468/780 [02:48<01:30,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 04:05:16,192 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:05:16,192 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 04:05:16,192 >>   Batch size = 8
{'eval_loss': 1.0814480781555176, 'eval_runtime': 9.4198, 'eval_samples_per_second': 371.239, 'eval_steps_per_second': 46.498, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.56it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.77it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.77it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.20it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.67it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.44it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.21it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.64it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.65it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.80it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.88it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.83it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.75it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.76it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.74it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.54it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.50it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.37it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.51it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.60it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.75it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.92it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 45.47it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 45.96it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.20it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.27it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.26it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.31it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.45it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.57it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.55it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.56it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.63it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.67it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.63it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.60it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.53it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.48it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.53it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.53it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.56it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.57it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.68it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.62it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.59it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.56it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.68it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.50it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.60it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.52it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.63it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.65it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.63it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.56it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.62it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.62it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.60it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.62it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.52it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.59it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.59it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.63it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.53it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.58it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.58it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.65it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.49it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.55it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.59it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.58it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.64it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.55it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.58it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.57it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.55it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.62it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.64it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.52it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.62it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.56it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.64it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.63it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.61it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.57it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.50it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.66it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.80it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.80it/s][A 60%|██████    | 468/780 [02:57<01:30,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:05:25,648 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 04:05:25,680 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:05:28,001 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:05:28,017 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:05:28,031 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:05<27:17,  5.27s/it] 60%|██████    | 470/780 [03:05<19:29,  3.77s/it] 60%|██████    | 471/780 [03:05<14:02,  2.73s/it] 61%|██████    | 472/780 [03:06<10:14,  2.00s/it] 61%|██████    | 473/780 [03:06<07:35,  1.48s/it] 61%|██████    | 474/780 [03:06<05:44,  1.13s/it] 61%|██████    | 475/780 [03:07<04:26,  1.14it/s] 61%|██████    | 476/780 [03:07<03:32,  1.43it/s] 61%|██████    | 477/780 [03:07<02:54,  1.74it/s] 61%|██████▏   | 478/780 [03:07<02:27,  2.04it/s] 61%|██████▏   | 479/780 [03:08<02:09,  2.33it/s] 62%|██████▏   | 480/780 [03:08<01:56,  2.58it/s] 62%|██████▏   | 481/780 [03:08<01:47,  2.79it/s] 62%|██████▏   | 482/780 [03:09<01:40,  2.96it/s] 62%|██████▏   | 483/780 [03:09<01:35,  3.10it/s] 62%|██████▏   | 484/780 [03:09<01:32,  3.20it/s] 62%|██████▏   | 485/780 [03:09<01:30,  3.27it/s] 62%|██████▏   | 486/780 [03:10<01:28,  3.33it/s] 62%|██████▏   | 487/780 [03:10<01:26,  3.37it/s] 63%|██████▎   | 488/780 [03:10<01:25,  3.40it/s] 63%|██████▎   | 489/780 [03:11<01:25,  3.42it/s] 63%|██████▎   | 490/780 [03:11<01:24,  3.43it/s] 63%|██████▎   | 491/780 [03:11<01:24,  3.44it/s] 63%|██████▎   | 492/780 [03:11<01:23,  3.44it/s] 63%|██████▎   | 493/780 [03:12<01:23,  3.44it/s] 63%|██████▎   | 494/780 [03:12<01:22,  3.45it/s] 63%|██████▎   | 495/780 [03:12<01:22,  3.45it/s] 64%|██████▎   | 496/780 [03:13<01:22,  3.46it/s] 64%|██████▎   | 497/780 [03:13<01:21,  3.45it/s] 64%|██████▍   | 498/780 [03:13<01:21,  3.46it/s] 64%|██████▍   | 499/780 [03:13<01:21,  3.46it/s] 64%|██████▍   | 500/780 [03:14<01:20,  3.46it/s]                                                  64%|██████▍   | 500/780 [03:14<01:20,  3.46it/s] 64%|██████▍   | 501/780 [03:14<01:20,  3.46it/s] 64%|██████▍   | 502/780 [03:14<01:20,  3.46it/s] 64%|██████▍   | 503/780 [03:15<01:20,  3.45it/s] 65%|██████▍   | 504/780 [03:15<01:19,  3.45it/s] 65%|██████▍   | 505/780 [03:15<01:19,  3.45it/s] 65%|██████▍   | 506/780 [03:15<01:19,  3.46it/s] 65%|██████▌   | 507/780 [03:16<01:18,  3.46it/s] 65%|██████▌   | 508/780 [03:16<01:18,  3.46it/s] 65%|██████▌   | 509/780 [03:16<01:18,  3.46it/s] 65%|██████▌   | 510/780 [03:17<01:17,  3.46it/s] 66%|██████▌   | 511/780 [03:17<01:17,  3.46it/s] 66%|██████▌   | 512/780 [03:17<01:17,  3.46it/s] 66%|██████▌   | 513/780 [03:18<01:17,  3.46it/s] 66%|██████▌   | 514/780 [03:18<01:17,  3.45it/s] 66%|██████▌   | 515/780 [03:18<01:16,  3.45it/s] 66%|██████▌   | 516/780 [03:18<01:16,  3.46it/s] 66%|██████▋   | 517/780 [03:19<01:16,  3.46it/s] 66%|██████▋   | 518/780 [03:19<01:15,  3.46it/s] 67%|██████▋   | 519/780 [03:19<01:15,  3.46it/s] 67%|██████▋   | 520/780 [03:20<01:15,  3.46it/s] 67%|██████▋   | 521/780 [03:20<01:14,  3.46it/s] 67%|██████▋   | 522/780 [03:20<01:14,  3.46it/s] 67%|██████▋   | 523/780 [03:20<01:14,  3.46it/s] 67%|██████▋   | 524/780 [03:21<01:13,  3.46it/s] 67%|██████▋   | 525/780 [03:21<01:13,  3.45it/s] 67%|██████▋   | 526/780 [03:21<01:13,  3.45it/s] 68%|██████▊   | 527/780 [03:22<01:13,  3.45it/s] 68%|██████▊   | 528/780 [03:22<01:12,  3.46it/s] 68%|██████▊   | 529/780 [03:22<01:12,  3.46it/s] 68%|██████▊   | 530/780 [03:22<01:12,  3.46it/s] 68%|██████▊   | 531/780 [03:23<01:11,  3.46it/s] 68%|██████▊   | 532/780 [03:23<01:11,  3.46it/s] 68%|██████▊   | 533/780 [03:23<01:11,  3.46it/s] 68%|██████▊   | 534/780 [03:24<01:11,  3.46it/s] 69%|██████▊   | 535/780 [03:24<01:10,  3.46it/s] 69%|██████▊   | 536/780 [03:24<01:10,  3.44it/s] 69%|██████▉   | 537/780 [03:24<01:10,  3.45it/s] 69%|██████▉   | 538/780 [03:25<01:10,  3.45it/s] 69%|██████▉   | 539/780 [03:25<01:09,  3.45it/s] 69%|██████▉   | 540/780 [03:25<01:09,  3.45it/s] 69%|██████▉   | 541/780 [03:26<01:09,  3.46it/s] 69%|██████▉   | 542/780 [03:26<01:08,  3.46it/s] 70%|██████▉   | 543/780 [03:26<01:08,  3.46it/s] 70%|██████▉   | 544/780 [03:26<01:08,  3.46it/s] 70%|██████▉   | 545/780 [03:27<01:07,  3.46it/s] 70%|███████   | 546/780 [03:27<01:07,  3.46it/s] 70%|███████   | 547/780 [03:27<01:07,  3.46it/s] 70%|███████   | 548/780 [03:28<01:07,  3.46it/s] 70%|███████   | 549/780 [03:28<01:06,  3.46it/s] 71%|███████   | 550/780 [03:28<01:06,  3.46it/s] 71%|███████   | 551/780 [03:28<01:06,  3.46it/s] 71%|███████   | 552/780 [03:29<01:06,  3.45it/s] 71%|███████   | 553/780 [03:29<01:05,  3.45it/s] 71%|███████   | 554/780 [03:29<01:05,  3.46it/s] 71%|███████   | 555/780 [03:30<01:05,  3.46it/s] 71%|███████▏  | 556/780 [03:30<01:04,  3.46it/s] 71%|███████▏  | 557/780 [03:30<01:04,  3.46it/s] 72%|███████▏  | 558/780 [03:31<01:04,  3.46it/s] 72%|███████▏  | 559/780 [03:31<01:03,  3.46it/s] 72%|███████▏  | 560/780 [03:31<01:03,  3.46it/s] 72%|███████▏  | 561/780 [03:31<01:03,  3.46it/s] 72%|███████▏  | 562/780 [03:32<01:02,  3.46it/s] 72%|███████▏  | 563/780 [03:32<01:02,  3.45it/s] 72%|███████▏  | 564/780 [03:32<01:02,  3.45it/s] 72%|███████▏  | 565/780 [03:33<01:02,  3.46it/s] 73%|███████▎  | 566/780 [03:33<01:01,  3.45it/s] 73%|███████▎  | 567/780 [03:33<01:01,  3.45it/s] 73%|███████▎  | 568/780 [03:33<01:01,  3.46it/s] 73%|███████▎  | 569/780 [03:34<01:01,  3.46it/s] 73%|███████▎  | 570/780 [03:34<01:00,  3.46it/s] 73%|███████▎  | 571/780 [03:34<01:00,  3.46it/s] 73%|███████▎  | 572/780 [03:35<01:00,  3.46it/s] 73%|███████▎  | 573/780 [03:35<00:59,  3.46it/s] 74%|███████▎  | 574/780 [03:35<00:59,  3.45it/s] 74%|███████▎  | 575/780 [03:35<00:59,  3.45it/s] 74%|███████▍  | 576/780 [03:36<00:59,  3.45it/s] 74%|███████▍  | 577/780 [03:36<00:58,  3.46it/s] 74%|███████▍  | 578/780 [03:36<00:58,  3.46it/s] 74%|███████▍  | 579/780 [03:37<00:58,  3.46it/s] 74%|███████▍  | 580/780 [03:37<00:57,  3.46it/s] 74%|███████▍  | 581/780 [03:37<00:57,  3.46it/s] 75%|███████▍  | 582/780 [03:37<00:57,  3.46it/s] 75%|███████▍  | 583/780 [03:38<00:56,  3.46it/s] 75%|███████▍  | 584/780 [03:38<00:56,  3.46it/s] 75%|███████▌  | 585/780 [03:38<00:56,  3.45it/s] 75%|███████▌  | 586/780 [03:39<00:56,  3.46it/s] 75%|███████▌  | 587/780 [03:39<00:55,  3.46it/s] 75%|███████▌  | 588/780 [03:39<00:55,  3.46it/s] 76%|███████▌  | 589/780 [03:39<00:55,  3.46it/s] 76%|███████▌  | 590/780 [03:40<00:54,  3.46it/s] 76%|███████▌  | 591/780 [03:40<00:54,  3.46it/s] 76%|███████▌  | 592/780 [03:40<00:54,  3.46it/s] 76%|███████▌  | 593/780 [03:41<00:54,  3.46it/s] 76%|███████▌  | 594/780 [03:41<00:53,  3.46it/s] 76%|███████▋  | 595/780 [03:41<00:53,  3.46it/s] 76%|███████▋  | 596/780 [03:42<00:53,  3.46it/s] 77%|███████▋  | 597/780 [03:42<00:52,  3.46it/s] 77%|███████▋  | 598/780 [03:42<00:52,  3.46it/s] 77%|███████▋  | 599/780 [03:42<00:52,  3.46it/s] 77%|███████▋  | 600/780 [03:43<00:52,  3.46it/s] 77%|███████▋  | 601/780 [03:43<00:51,  3.46it/s] 77%|███████▋  | 602/780 [03:43<00:51,  3.45it/s] 77%|███████▋  | 603/780 [03:44<00:51,  3.45it/s] 77%|███████▋  | 604/780 [03:44<00:50,  3.45it/s] 78%|███████▊  | 605/780 [03:44<00:50,  3.44it/s] 78%|███████▊  | 606/780 [03:44<00:50,  3.45it/s] 78%|███████▊  | 607/780 [03:45<00:50,  3.45it/s] 78%|███████▊  | 608/780 [03:45<00:49,  3.45it/s] 78%|███████▊  | 609/780 [03:45<00:49,  3.45it/s] 78%|███████▊  | 610/780 [03:46<00:49,  3.45it/s] 78%|███████▊  | 611/780 [03:46<00:48,  3.45it/s] 78%|███████▊  | 612/780 [03:46<00:48,  3.45it/s] 79%|███████▊  | 613/780 [03:46<00:48,  3.45it/s] 79%|███████▊  | 614/780 [03:47<00:48,  3.45it/s] 79%|███████▉  | 615/780 [03:47<00:47,  3.45it/s] 79%|███████▉  | 616/780 [03:47<00:47,  3.45it/s] 79%|███████▉  | 617/780 [03:48<00:47,  3.45it/s] 79%|███████▉  | 618/780 [03:48<00:46,  3.45it/s] 79%|███████▉  | 619/780 [03:48<00:46,  3.45it/s] 79%|███████▉  | 620/780 [03:48<00:46,  3.45it/s] 80%|███████▉  | 621/780 [03:49<00:46,  3.45it/s] 80%|███████▉  | 622/780 [03:49<00:46,  3.43it/s] 80%|███████▉  | 623/780 [03:49<00:45,  3.44it/s] 80%|████████  | 624/780 [03:50<00:45,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 04:06:17,912 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:06:17,912 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 04:06:17,912 >>   Batch size = 8
{'eval_loss': 1.0985221862792969, 'eval_runtime': 9.4266, 'eval_samples_per_second': 370.972, 'eval_steps_per_second': 46.464, 'epoch': 3.0}
{'loss': 0.4184, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.06it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.63it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.74it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.03it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.59it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.34it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.44it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.36it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.38it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.55it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.71it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.62it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.70it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.74it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.78it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.73it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.63it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.60it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.62it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.68it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.77it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.68it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.77it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.74it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.69it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.73it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.71it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.69it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.62it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.71it/s][A
 36%|███▌      | 158/438 [00:03<00:05, 46.73it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.82it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.77it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.67it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.68it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.65it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.64it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.74it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.66it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.73it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.71it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.78it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.84it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.74it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.68it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.70it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.77it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.69it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.75it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.61it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.68it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.75it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.65it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.70it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.67it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.64it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.72it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.77it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 46.66it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.68it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.68it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.74it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.78it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.72it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.64it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.74it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.70it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.72it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.72it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.57it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.66it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.71it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.76it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.66it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.69it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.66it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.68it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.62it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.66it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.57it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.68it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.75it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.76it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.64it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.64it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.66it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.76it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.76it/s][A 80%|████████  | 624/780 [03:59<00:45,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:06:27,336 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 04:06:27,359 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:06:29,554 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:06:29,570 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:06:29,590 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:06<13:26,  5.20s/it] 80%|████████  | 626/780 [04:07<09:34,  3.73s/it] 80%|████████  | 627/780 [04:07<06:52,  2.70s/it] 81%|████████  | 628/780 [04:07<05:00,  1.97s/it] 81%|████████  | 629/780 [04:07<03:41,  1.47s/it] 81%|████████  | 630/780 [04:08<02:47,  1.11s/it] 81%|████████  | 631/780 [04:08<02:09,  1.15it/s] 81%|████████  | 632/780 [04:08<01:42,  1.44it/s] 81%|████████  | 633/780 [04:09<01:24,  1.75it/s] 81%|████████▏ | 634/780 [04:09<01:11,  2.05it/s] 81%|████████▏ | 635/780 [04:09<01:02,  2.34it/s] 82%|████████▏ | 636/780 [04:09<00:55,  2.59it/s] 82%|████████▏ | 637/780 [04:10<00:51,  2.79it/s] 82%|████████▏ | 638/780 [04:10<00:47,  2.97it/s] 82%|████████▏ | 639/780 [04:10<00:45,  3.10it/s] 82%|████████▏ | 640/780 [04:11<00:43,  3.20it/s] 82%|████████▏ | 641/780 [04:11<00:42,  3.27it/s] 82%|████████▏ | 642/780 [04:11<00:41,  3.33it/s] 82%|████████▏ | 643/780 [04:11<00:40,  3.37it/s] 83%|████████▎ | 644/780 [04:12<00:40,  3.40it/s] 83%|████████▎ | 645/780 [04:12<00:39,  3.41it/s] 83%|████████▎ | 646/780 [04:12<00:39,  3.43it/s] 83%|████████▎ | 647/780 [04:13<00:38,  3.44it/s] 83%|████████▎ | 648/780 [04:13<00:38,  3.43it/s] 83%|████████▎ | 649/780 [04:13<00:38,  3.44it/s] 83%|████████▎ | 650/780 [04:14<00:37,  3.45it/s] 83%|████████▎ | 651/780 [04:14<00:37,  3.45it/s] 84%|████████▎ | 652/780 [04:14<00:37,  3.45it/s] 84%|████████▎ | 653/780 [04:14<00:36,  3.46it/s] 84%|████████▍ | 654/780 [04:15<00:36,  3.46it/s] 84%|████████▍ | 655/780 [04:15<00:36,  3.46it/s] 84%|████████▍ | 656/780 [04:15<00:35,  3.46it/s] 84%|████████▍ | 657/780 [04:16<00:35,  3.46it/s] 84%|████████▍ | 658/780 [04:16<00:35,  3.46it/s] 84%|████████▍ | 659/780 [04:16<00:35,  3.45it/s] 85%|████████▍ | 660/780 [04:16<00:34,  3.45it/s] 85%|████████▍ | 661/780 [04:17<00:34,  3.46it/s] 85%|████████▍ | 662/780 [04:17<00:34,  3.46it/s] 85%|████████▌ | 663/780 [04:17<00:33,  3.46it/s] 85%|████████▌ | 664/780 [04:18<00:33,  3.46it/s] 85%|████████▌ | 665/780 [04:18<00:33,  3.46it/s] 85%|████████▌ | 666/780 [04:18<00:32,  3.46it/s] 86%|████████▌ | 667/780 [04:18<00:32,  3.46it/s] 86%|████████▌ | 668/780 [04:19<00:32,  3.46it/s] 86%|████████▌ | 669/780 [04:19<00:32,  3.46it/s] 86%|████████▌ | 670/780 [04:19<00:31,  3.46it/s] 86%|████████▌ | 671/780 [04:20<00:31,  3.46it/s] 86%|████████▌ | 672/780 [04:20<00:31,  3.46it/s] 86%|████████▋ | 673/780 [04:20<00:30,  3.46it/s] 86%|████████▋ | 674/780 [04:20<00:30,  3.46it/s] 87%|████████▋ | 675/780 [04:21<00:30,  3.46it/s] 87%|████████▋ | 676/780 [04:21<00:30,  3.46it/s] 87%|████████▋ | 677/780 [04:21<00:29,  3.46it/s] 87%|████████▋ | 678/780 [04:22<00:29,  3.46it/s] 87%|████████▋ | 679/780 [04:22<00:29,  3.46it/s] 87%|████████▋ | 680/780 [04:22<00:28,  3.46it/s] 87%|████████▋ | 681/780 [04:22<00:28,  3.45it/s] 87%|████████▋ | 682/780 [04:23<00:28,  3.45it/s] 88%|████████▊ | 683/780 [04:23<00:28,  3.45it/s] 88%|████████▊ | 684/780 [04:23<00:27,  3.46it/s] 88%|████████▊ | 685/780 [04:24<00:27,  3.46it/s] 88%|████████▊ | 686/780 [04:24<00:27,  3.46it/s] 88%|████████▊ | 687/780 [04:24<00:26,  3.46it/s] 88%|████████▊ | 688/780 [04:25<00:26,  3.46it/s] 88%|████████▊ | 689/780 [04:25<00:26,  3.46it/s] 88%|████████▊ | 690/780 [04:25<00:26,  3.46it/s] 89%|████████▊ | 691/780 [04:25<00:25,  3.46it/s] 89%|████████▊ | 692/780 [04:26<00:25,  3.45it/s] 89%|████████▉ | 693/780 [04:26<00:25,  3.45it/s] 89%|████████▉ | 694/780 [04:26<00:24,  3.45it/s] 89%|████████▉ | 695/780 [04:27<00:24,  3.45it/s] 89%|████████▉ | 696/780 [04:27<00:24,  3.46it/s] 89%|████████▉ | 697/780 [04:27<00:24,  3.46it/s] 89%|████████▉ | 698/780 [04:27<00:23,  3.46it/s] 90%|████████▉ | 699/780 [04:28<00:23,  3.46it/s] 90%|████████▉ | 700/780 [04:28<00:23,  3.46it/s] 90%|████████▉ | 701/780 [04:28<00:22,  3.46it/s] 90%|█████████ | 702/780 [04:29<00:22,  3.46it/s] 90%|█████████ | 703/780 [04:29<00:22,  3.46it/s] 90%|█████████ | 704/780 [04:29<00:21,  3.46it/s] 90%|█████████ | 705/780 [04:29<00:21,  3.46it/s] 91%|█████████ | 706/780 [04:30<00:21,  3.46it/s] 91%|█████████ | 707/780 [04:30<00:21,  3.45it/s] 91%|█████████ | 708/780 [04:30<00:20,  3.45it/s] 91%|█████████ | 709/780 [04:31<00:20,  3.45it/s] 91%|█████████ | 710/780 [04:31<00:20,  3.46it/s] 91%|█████████ | 711/780 [04:31<00:19,  3.46it/s] 91%|█████████▏| 712/780 [04:31<00:19,  3.46it/s] 91%|█████████▏| 713/780 [04:32<00:19,  3.46it/s] 92%|█████████▏| 714/780 [04:32<00:19,  3.46it/s] 92%|█████████▏| 715/780 [04:32<00:18,  3.46it/s] 92%|█████████▏| 716/780 [04:33<00:18,  3.46it/s] 92%|█████████▏| 717/780 [04:33<00:18,  3.46it/s] 92%|█████████▏| 718/780 [04:33<00:17,  3.45it/s] 92%|█████████▏| 719/780 [04:33<00:17,  3.45it/s] 92%|█████████▏| 720/780 [04:34<00:17,  3.45it/s] 92%|█████████▏| 721/780 [04:34<00:17,  3.46it/s] 93%|█████████▎| 722/780 [04:34<00:16,  3.45it/s] 93%|█████████▎| 723/780 [04:35<00:16,  3.46it/s] 93%|█████████▎| 724/780 [04:35<00:16,  3.46it/s] 93%|█████████▎| 725/780 [04:35<00:15,  3.46it/s] 93%|█████████▎| 726/780 [04:35<00:15,  3.46it/s] 93%|█████████▎| 727/780 [04:36<00:15,  3.46it/s] 93%|█████████▎| 728/780 [04:36<00:15,  3.46it/s] 93%|█████████▎| 729/780 [04:36<00:14,  3.45it/s] 94%|█████████▎| 730/780 [04:37<00:14,  3.46it/s] 94%|█████████▎| 731/780 [04:37<00:14,  3.46it/s] 94%|█████████▍| 732/780 [04:37<00:13,  3.46it/s] 94%|█████████▍| 733/780 [04:38<00:13,  3.46it/s] 94%|█████████▍| 734/780 [04:38<00:13,  3.46it/s] 94%|█████████▍| 735/780 [04:38<00:13,  3.46it/s] 94%|█████████▍| 736/780 [04:38<00:12,  3.46it/s] 94%|█████████▍| 737/780 [04:39<00:12,  3.46it/s] 95%|█████████▍| 738/780 [04:39<00:12,  3.46it/s] 95%|█████████▍| 739/780 [04:39<00:11,  3.46it/s] 95%|█████████▍| 740/780 [04:40<00:11,  3.44it/s] 95%|█████████▌| 741/780 [04:40<00:11,  3.44it/s] 95%|█████████▌| 742/780 [04:40<00:11,  3.45it/s] 95%|█████████▌| 743/780 [04:40<00:10,  3.45it/s] 95%|█████████▌| 744/780 [04:41<00:10,  3.45it/s] 96%|█████████▌| 745/780 [04:41<00:10,  3.45it/s] 96%|█████████▌| 746/780 [04:41<00:09,  3.46it/s] 96%|█████████▌| 747/780 [04:42<00:09,  3.46it/s] 96%|█████████▌| 748/780 [04:42<00:09,  3.45it/s] 96%|█████████▌| 749/780 [04:42<00:08,  3.45it/s] 96%|█████████▌| 750/780 [04:42<00:08,  3.45it/s] 96%|█████████▋| 751/780 [04:43<00:08,  3.45it/s] 96%|█████████▋| 752/780 [04:43<00:08,  3.45it/s] 97%|█████████▋| 753/780 [04:43<00:07,  3.45it/s] 97%|█████████▋| 754/780 [04:44<00:07,  3.46it/s] 97%|█████████▋| 755/780 [04:44<00:07,  3.46it/s] 97%|█████████▋| 756/780 [04:44<00:06,  3.46it/s] 97%|█████████▋| 757/780 [04:44<00:06,  3.46it/s] 97%|█████████▋| 758/780 [04:45<00:06,  3.46it/s] 97%|█████████▋| 759/780 [04:45<00:06,  3.46it/s] 97%|█████████▋| 760/780 [04:45<00:05,  3.46it/s] 98%|█████████▊| 761/780 [04:46<00:05,  3.46it/s] 98%|█████████▊| 762/780 [04:46<00:05,  3.45it/s] 98%|█████████▊| 763/780 [04:46<00:04,  3.45it/s] 98%|█████████▊| 764/780 [04:46<00:04,  3.45it/s] 98%|█████████▊| 765/780 [04:47<00:04,  3.46it/s] 98%|█████████▊| 766/780 [04:47<00:04,  3.46it/s] 98%|█████████▊| 767/780 [04:47<00:03,  3.46it/s] 98%|█████████▊| 768/780 [04:48<00:03,  3.46it/s] 99%|█████████▊| 769/780 [04:48<00:03,  3.46it/s] 99%|█████████▊| 770/780 [04:48<00:02,  3.46it/s] 99%|█████████▉| 771/780 [04:49<00:02,  3.45it/s] 99%|█████████▉| 772/780 [04:49<00:02,  3.45it/s] 99%|█████████▉| 773/780 [04:49<00:02,  3.43it/s] 99%|█████████▉| 774/780 [04:49<00:01,  3.43it/s] 99%|█████████▉| 775/780 [04:50<00:01,  3.44it/s] 99%|█████████▉| 776/780 [04:50<00:01,  3.44it/s]100%|█████████▉| 777/780 [04:50<00:00,  3.45it/s]100%|█████████▉| 778/780 [04:51<00:00,  3.33it/s]100%|█████████▉| 779/780 [04:51<00:00,  3.36it/s]100%|██████████| 780/780 [04:51<00:00,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 04:07:19,417 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:07:19,417 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 04:07:19,417 >>   Batch size = 8
{'eval_loss': 1.1078171730041504, 'eval_runtime': 9.403, 'eval_samples_per_second': 371.902, 'eval_steps_per_second': 46.581, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.06it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.31it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.58it/s][A
  5%|▌         | 23/438 [00:00<00:08, 48.02it/s][A
  6%|▋         | 28/438 [00:00<00:08, 47.55it/s][A
  8%|▊         | 33/438 [00:00<00:08, 47.35it/s][A
  9%|▊         | 38/438 [00:00<00:08, 47.10it/s][A
 10%|▉         | 43/438 [00:00<00:08, 46.89it/s][A
 11%|█         | 48/438 [00:01<00:08, 46.70it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 46.79it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 46.76it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 46.79it/s][A
 16%|█▌        | 68/438 [00:01<00:07, 46.78it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 46.79it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 46.81it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 46.87it/s][A
 20%|██        | 88/438 [00:01<00:07, 46.84it/s][A
 21%|██        | 93/438 [00:01<00:07, 46.69it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 46.70it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 46.66it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 46.74it/s][A
 26%|██▌       | 113/438 [00:02<00:06, 46.74it/s][A
 27%|██▋       | 118/438 [00:02<00:06, 46.76it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 46.73it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 46.81it/s][A
 30%|███       | 133/438 [00:02<00:06, 46.74it/s][A
 32%|███▏      | 138/438 [00:02<00:06, 46.74it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 46.51it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 46.61it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 46.65it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 46.67it/s][A
 37%|███▋      | 163/438 [00:03<00:05, 46.69it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 46.74it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 46.76it/s][A
 41%|████      | 178/438 [00:03<00:05, 46.74it/s][A
 42%|████▏     | 183/438 [00:03<00:05, 46.81it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 46.69it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 46.67it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 46.70it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 46.63it/s][A
 47%|████▋     | 208/438 [00:04<00:04, 46.69it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 46.74it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 46.68it/s][A
 51%|█████     | 223/438 [00:04<00:04, 46.80it/s][A
 52%|█████▏    | 228/438 [00:04<00:04, 46.53it/s][A
 53%|█████▎    | 233/438 [00:04<00:04, 46.57it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 46.70it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 46.56it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 46.59it/s][A
 58%|█████▊    | 253/438 [00:05<00:03, 46.62it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 46.65it/s][A
 60%|██████    | 263/438 [00:05<00:03, 46.73it/s][A
 61%|██████    | 268/438 [00:05<00:03, 46.83it/s][A
 62%|██████▏   | 273/438 [00:05<00:03, 46.63it/s][A
 63%|██████▎   | 278/438 [00:05<00:03, 46.68it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 46.71it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 46.64it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 46.73it/s][A
 68%|██████▊   | 298/438 [00:06<00:02, 46.67it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 46.65it/s][A
 70%|███████   | 308/438 [00:06<00:02, 46.70it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 46.79it/s][A
 73%|███████▎  | 318/438 [00:06<00:02, 46.71it/s][A
 74%|███████▎  | 323/438 [00:06<00:02, 46.76it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 46.65it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 46.69it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 46.77it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 46.72it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 46.53it/s][A
 81%|████████  | 353/438 [00:07<00:01, 46.64it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 46.60it/s][A
 83%|████████▎ | 363/438 [00:07<00:01, 46.70it/s][A
 84%|████████▍ | 368/438 [00:07<00:01, 46.78it/s][A
 85%|████████▌ | 373/438 [00:07<00:01, 46.71it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 46.73it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 46.74it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 46.77it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 46.62it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 46.66it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 46.63it/s][A
 93%|█████████▎| 408/438 [00:08<00:00, 46.72it/s][A
 94%|█████████▍| 413/438 [00:08<00:00, 46.65it/s][A
 95%|█████████▌| 418/438 [00:08<00:00, 46.73it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 46.75it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 46.70it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 46.71it/s][A
100%|██████████| 438/438 [00:09<00:00, 46.88it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 46.88it/s][A100%|██████████| 780/780 [05:01<00:00,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:07:28,807 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 04:07:28,824 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:07:31,097 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:07:31,114 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:07:31,125 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 04:07:35,416 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 04:07:35,419 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156 (score: 1.060680627822876).
                                                 100%|██████████| 780/780 [05:09<00:00,  3.39it/s]100%|██████████| 780/780 [05:09<00:00,  2.52it/s]
[INFO|trainer.py:1894] 2023-08-29 04:07:37,116 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 04:07:37,141 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:07:39,442 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:07:39,460 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:07:39,471 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:07:39,680 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:39,680 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:39,680 >>   train_loss               =     0.4101
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:39,680 >>   train_runtime            = 0:05:09.37
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:39,680 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:39,680 >>   train_samples_per_second =    161.617
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:39,680 >>   train_steps_per_second   =      2.521
{'eval_loss': 1.1130949258804321, 'eval_runtime': 9.3703, 'eval_samples_per_second': 373.201, 'eval_steps_per_second': 46.744, 'epoch': 5.0}
{'train_runtime': 309.3742, 'train_samples_per_second': 161.617, 'train_steps_per_second': 2.521, 'train_loss': 0.4101292536808894, 'epoch': 5.0}
08/29/2023 04:07:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 04:07:39,723 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:07:39,723 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-29 04:07:39,723 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 58.45it/s]  3%|▎         | 12/438 [00:00<00:08, 51.10it/s]  4%|▍         | 18/438 [00:00<00:08, 49.26it/s]  5%|▌         | 23/438 [00:00<00:08, 48.59it/s]  6%|▋         | 28/438 [00:00<00:08, 48.19it/s]  8%|▊         | 33/438 [00:00<00:08, 47.93it/s]  9%|▊         | 38/438 [00:00<00:08, 47.65it/s] 10%|▉         | 43/438 [00:00<00:08, 47.45it/s] 11%|█         | 48/438 [00:00<00:08, 47.13it/s] 12%|█▏        | 53/438 [00:01<00:08, 47.04it/s] 13%|█▎        | 58/438 [00:01<00:08, 47.08it/s] 14%|█▍        | 63/438 [00:01<00:07, 47.17it/s] 16%|█▌        | 68/438 [00:01<00:07, 47.07it/s] 17%|█▋        | 73/438 [00:01<00:07, 47.10it/s] 18%|█▊        | 78/438 [00:01<00:07, 47.23it/s] 19%|█▉        | 83/438 [00:01<00:07, 47.16it/s] 20%|██        | 88/438 [00:01<00:07, 47.06it/s] 21%|██        | 93/438 [00:01<00:07, 46.91it/s] 22%|██▏       | 98/438 [00:02<00:07, 46.74it/s] 24%|██▎       | 103/438 [00:02<00:07, 46.84it/s] 25%|██▍       | 108/438 [00:02<00:07, 47.04it/s] 26%|██▌       | 113/438 [00:02<00:06, 46.97it/s] 27%|██▋       | 118/438 [00:02<00:06, 46.97it/s] 28%|██▊       | 123/438 [00:02<00:06, 47.04it/s] 29%|██▉       | 128/438 [00:02<00:06, 47.03it/s] 30%|███       | 133/438 [00:02<00:06, 46.98it/s] 32%|███▏      | 138/438 [00:02<00:06, 46.89it/s] 33%|███▎      | 143/438 [00:03<00:06, 46.81it/s] 34%|███▍      | 148/438 [00:03<00:06, 46.75it/s] 35%|███▍      | 153/438 [00:03<00:06, 46.95it/s] 36%|███▌      | 158/438 [00:03<00:05, 46.92it/s] 37%|███▋      | 163/438 [00:03<00:05, 46.97it/s] 38%|███▊      | 168/438 [00:03<00:05, 47.11it/s] 39%|███▉      | 173/438 [00:03<00:05, 47.08it/s] 41%|████      | 178/438 [00:03<00:05, 46.94it/s] 42%|████▏     | 183/438 [00:03<00:05, 46.95it/s] 43%|████▎     | 188/438 [00:03<00:05, 46.80it/s] 44%|████▍     | 193/438 [00:04<00:05, 46.82it/s] 45%|████▌     | 198/438 [00:04<00:05, 46.92it/s] 46%|████▋     | 203/438 [00:04<00:05, 46.92it/s] 47%|████▋     | 208/438 [00:04<00:04, 46.96it/s] 49%|████▊     | 213/438 [00:04<00:04, 46.89it/s] 50%|████▉     | 218/438 [00:04<00:04, 47.03it/s] 51%|█████     | 223/438 [00:04<00:04, 46.96it/s] 52%|█████▏    | 228/438 [00:04<00:04, 46.90it/s] 53%|█████▎    | 233/438 [00:04<00:04, 46.85it/s] 54%|█████▍    | 238/438 [00:05<00:04, 46.76it/s] 55%|█████▌    | 243/438 [00:05<00:04, 46.88it/s] 57%|█████▋    | 248/438 [00:05<00:04, 46.96it/s] 58%|█████▊    | 253/438 [00:05<00:03, 46.83it/s] 59%|█████▉    | 258/438 [00:05<00:03, 46.94it/s] 60%|██████    | 263/438 [00:05<00:03, 47.01it/s] 61%|██████    | 268/438 [00:05<00:03, 46.97it/s] 62%|██████▏   | 273/438 [00:05<00:03, 46.98it/s] 63%|██████▎   | 278/438 [00:05<00:03, 46.88it/s] 65%|██████▍   | 283/438 [00:06<00:03, 46.79it/s] 66%|██████▌   | 288/438 [00:06<00:03, 46.87it/s] 67%|██████▋   | 293/438 [00:06<00:03, 46.93it/s] 68%|██████▊   | 298/438 [00:06<00:02, 46.94it/s] 69%|██████▉   | 303/438 [00:06<00:02, 46.98it/s] 70%|███████   | 308/438 [00:06<00:02, 46.97it/s] 71%|███████▏  | 313/438 [00:06<00:02, 47.00it/s] 73%|███████▎  | 318/438 [00:06<00:02, 46.89it/s] 74%|███████▎  | 323/438 [00:06<00:02, 46.92it/s] 75%|███████▍  | 328/438 [00:06<00:02, 46.80it/s] 76%|███████▌  | 333/438 [00:07<00:02, 46.94it/s] 77%|███████▋  | 338/438 [00:07<00:02, 46.78it/s] 78%|███████▊  | 343/438 [00:07<00:02, 46.84it/s] 79%|███████▉  | 348/438 [00:07<00:01, 46.91it/s] 81%|████████  | 353/438 [00:07<00:01, 46.94it/s] 82%|████████▏ | 358/438 [00:07<00:01, 46.95it/s] 83%|████████▎ | 363/438 [00:07<00:01, 46.90it/s] 84%|████████▍ | 368/438 [00:07<00:01, 46.94it/s] 85%|████████▌ | 373/438 [00:07<00:01, 46.88it/s] 86%|████████▋ | 378/438 [00:08<00:01, 46.89it/s] 87%|████████▋ | 383/438 [00:08<00:01, 46.81it/s] 89%|████████▊ | 388/438 [00:08<00:01, 46.81it/s] 90%|████████▉ | 393/438 [00:08<00:00, 46.95it/s] 91%|█████████ | 398/438 [00:08<00:00, 47.04it/s] 92%|█████████▏| 403/438 [00:08<00:00, 46.96it/s] 93%|█████████▎| 408/438 [00:08<00:00, 46.89it/s] 94%|█████████▍| 413/438 [00:08<00:00, 46.84it/s] 95%|█████████▌| 418/438 [00:08<00:00, 46.88it/s] 97%|█████████▋| 423/438 [00:08<00:00, 46.84it/s] 98%|█████████▊| 428/438 [00:09<00:00, 46.84it/s] 99%|█████████▉| 433/438 [00:09<00:00, 46.88it/s]100%|██████████| 438/438 [00:09<00:00, 46.99it/s]100%|██████████| 438/438 [00:09<00:00, 47.07it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:07:49,050 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:49,050 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:49,050 >>   eval_loss               =     1.0607
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:49,050 >>   eval_runtime            = 0:00:09.32
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:49,050 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:49,051 >>   eval_samples_per_second =    374.937
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:49,051 >>   eval_steps_per_second   =     46.961
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:07:49,051 >>   perplexity              =     2.8883
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:07:55,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:07:55,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:07:55,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:07:55,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:07:55,383 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:07:56,100 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:07:56,101 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:07:56,371 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:07:57,428 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:07:57,428 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:00,400 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:00,405 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:00,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:00,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:08:00,406 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:08:00,739 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:08:00,740 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:08:01,005 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:08:01,164 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:08:01,165 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.53it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.58it/s]Extractor Predicting: 14it [00:08,  1.57it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.58it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:14,  1.54it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:15,  1.57it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:17,  1.64it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:18,  1.63it/s]Extractor Predicting: 30it [00:19,  1.64it/s]Extractor Predicting: 31it [00:19,  1.62it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:24,  1.55it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.52it/s]Extractor Predicting: 43it [00:27,  1.55it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.55it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:29,  1.57it/s]Extractor Predicting: 48it [00:30,  1.54it/s]Extractor Predicting: 49it [00:31,  1.53it/s]Extractor Predicting: 50it [00:31,  1.50it/s]Extractor Predicting: 51it [00:32,  1.50it/s]Extractor Predicting: 52it [00:33,  1.48it/s]Extractor Predicting: 53it [00:33,  1.50it/s]Extractor Predicting: 54it [00:34,  1.51it/s]Extractor Predicting: 55it [00:35,  1.41it/s]Extractor Predicting: 56it [00:36,  1.44it/s]Extractor Predicting: 57it [00:36,  1.46it/s]Extractor Predicting: 58it [00:37,  1.47it/s]Extractor Predicting: 59it [00:38,  1.50it/s]Extractor Predicting: 60it [00:38,  1.46it/s]Extractor Predicting: 61it [00:39,  1.46it/s]Extractor Predicting: 62it [00:40,  1.44it/s]Extractor Predicting: 63it [00:40,  1.43it/s]Extractor Predicting: 64it [00:41,  1.44it/s]Extractor Predicting: 65it [00:42,  1.43it/s]Extractor Predicting: 66it [00:43,  1.42it/s]Extractor Predicting: 67it [00:43,  1.40it/s]Extractor Predicting: 68it [00:44,  1.40it/s]Extractor Predicting: 69it [00:45,  1.40it/s]Extractor Predicting: 70it [00:45,  1.40it/s]Extractor Predicting: 71it [00:46,  1.39it/s]Extractor Predicting: 72it [00:47,  1.39it/s]Extractor Predicting: 73it [00:48,  1.37it/s]Extractor Predicting: 74it [00:48,  1.38it/s]Extractor Predicting: 75it [00:49,  1.37it/s]Extractor Predicting: 76it [00:50,  1.40it/s]Extractor Predicting: 77it [00:50,  1.40it/s]Extractor Predicting: 78it [00:51,  1.39it/s]Extractor Predicting: 79it [00:52,  1.42it/s]Extractor Predicting: 80it [00:53,  1.39it/s]Extractor Predicting: 81it [00:53,  1.36it/s]Extractor Predicting: 82it [00:54,  1.37it/s]Extractor Predicting: 83it [00:55,  1.41it/s]Extractor Predicting: 84it [00:55,  1.42it/s]Extractor Predicting: 85it [00:56,  1.43it/s]Extractor Predicting: 86it [00:57,  1.42it/s]Extractor Predicting: 87it [00:58,  1.39it/s]Extractor Predicting: 88it [00:58,  1.40it/s]Extractor Predicting: 89it [00:59,  1.43it/s]Extractor Predicting: 90it [01:00,  1.45it/s]Extractor Predicting: 91it [01:00,  1.46it/s]Extractor Predicting: 92it [01:01,  1.44it/s]Extractor Predicting: 93it [01:02,  1.47it/s]Extractor Predicting: 94it [01:02,  1.48it/s]Extractor Predicting: 95it [01:03,  1.47it/s]Extractor Predicting: 96it [01:04,  1.47it/s]Extractor Predicting: 97it [01:04,  1.48it/s]Extractor Predicting: 98it [01:05,  1.46it/s]Extractor Predicting: 99it [01:06,  1.48it/s]Extractor Predicting: 100it [01:06,  1.49it/s]Extractor Predicting: 101it [01:07,  1.49it/s]Extractor Predicting: 102it [01:08,  1.47it/s]Extractor Predicting: 103it [01:09,  1.43it/s]Extractor Predicting: 104it [01:09,  1.45it/s]Extractor Predicting: 105it [01:10,  1.46it/s]Extractor Predicting: 106it [01:11,  1.47it/s]Extractor Predicting: 107it [01:11,  1.44it/s]Extractor Predicting: 108it [01:12,  1.46it/s]Extractor Predicting: 109it [01:13,  1.44it/s]Extractor Predicting: 110it [01:13,  1.43it/s]Extractor Predicting: 111it [01:14,  1.43it/s]Extractor Predicting: 112it [01:15,  1.43it/s]Extractor Predicting: 113it [01:15,  1.44it/s]Extractor Predicting: 114it [01:16,  1.45it/s]Extractor Predicting: 115it [01:17,  1.45it/s]Extractor Predicting: 116it [01:17,  1.45it/s]Extractor Predicting: 117it [01:18,  1.48it/s]Extractor Predicting: 118it [01:19,  1.51it/s]Extractor Predicting: 119it [01:19,  1.55it/s]Extractor Predicting: 120it [01:20,  1.56it/s]Extractor Predicting: 121it [01:21,  1.54it/s]Extractor Predicting: 122it [01:21,  1.59it/s]Extractor Predicting: 123it [01:22,  1.61it/s]Extractor Predicting: 124it [01:23,  1.58it/s]Extractor Predicting: 125it [01:23,  1.60it/s]Extractor Predicting: 126it [01:24,  1.62it/s]Extractor Predicting: 127it [01:24,  1.51it/s]Extractor Predicting: 128it [01:25,  1.54it/s]Extractor Predicting: 129it [01:26,  1.59it/s]Extractor Predicting: 130it [01:26,  1.55it/s]Extractor Predicting: 131it [01:27,  1.57it/s]Extractor Predicting: 132it [01:28,  1.64it/s]Extractor Predicting: 133it [01:28,  1.65it/s]Extractor Predicting: 134it [01:29,  1.58it/s]Extractor Predicting: 135it [01:29,  1.60it/s]Extractor Predicting: 136it [01:30,  1.64it/s]Extractor Predicting: 137it [01:31,  1.64it/s]Extractor Predicting: 138it [01:31,  1.63it/s]Extractor Predicting: 139it [01:32,  1.64it/s]Extractor Predicting: 140it [01:32,  1.67it/s]Extractor Predicting: 141it [01:33,  1.69it/s]Extractor Predicting: 142it [01:34,  1.71it/s]Extractor Predicting: 143it [01:34,  1.68it/s]Extractor Predicting: 144it [01:35,  1.65it/s]Extractor Predicting: 145it [01:35,  1.83it/s]Extractor Predicting: 145it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:46,212 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:46,215 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:46,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:46,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:46,216 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:09:46,842 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:09:46,843 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:09:47,415 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:09:48,440 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:09:48,440 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:51,300 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:51,308 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:51,308 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:51,308 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:09:51,308 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:09:52,084 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:09:52,085 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:09:52,634 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:09:52,783 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:09:52,783 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6923497267759563,
  "recall": 0.36231055190163,
  "score": 0.4756898817345597,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.48it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.47it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.51it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:10,  1.52it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.47it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.49it/s]Extractor Predicting: 22it [00:14,  1.44it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:16,  1.40it/s]Extractor Predicting: 26it [00:17,  1.41it/s]Extractor Predicting: 27it [00:18,  1.41it/s]Extractor Predicting: 28it [00:19,  1.42it/s]Extractor Predicting: 29it [00:19,  1.42it/s]Extractor Predicting: 30it [00:20,  1.40it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:21,  1.42it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:25,  1.54it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:26,  1.52it/s]Extractor Predicting: 40it [00:27,  1.54it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:28,  1.52it/s]Extractor Predicting: 43it [00:29,  1.51it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:30,  1.49it/s]Extractor Predicting: 46it [00:31,  1.52it/s]Extractor Predicting: 47it [00:31,  1.52it/s]Extractor Predicting: 48it [00:32,  1.51it/s]Extractor Predicting: 49it [00:33,  1.51it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:35,  1.50it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:37,  1.48it/s]Extractor Predicting: 56it [00:37,  1.49it/s]Extractor Predicting: 57it [00:38,  1.52it/s]Extractor Predicting: 58it [00:39,  1.54it/s]Extractor Predicting: 59it [00:39,  1.55it/s]Extractor Predicting: 60it [00:40,  1.55it/s]Extractor Predicting: 61it [00:40,  1.53it/s]Extractor Predicting: 62it [00:41,  1.54it/s]Extractor Predicting: 63it [00:42,  1.52it/s]Extractor Predicting: 64it [00:42,  1.52it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:44,  1.48it/s]Extractor Predicting: 67it [00:44,  1.51it/s]Extractor Predicting: 68it [00:45,  1.55it/s]Extractor Predicting: 69it [00:46,  1.56it/s]Extractor Predicting: 70it [00:46,  1.55it/s]Extractor Predicting: 71it [00:47,  1.53it/s]Extractor Predicting: 72it [00:48,  1.55it/s]Extractor Predicting: 73it [00:48,  1.50it/s]Extractor Predicting: 74it [00:49,  1.51it/s]Extractor Predicting: 75it [00:50,  1.51it/s]Extractor Predicting: 76it [00:50,  1.53it/s]Extractor Predicting: 77it [00:51,  1.49it/s]Extractor Predicting: 78it [00:52,  1.49it/s]Extractor Predicting: 79it [00:52,  1.50it/s]Extractor Predicting: 80it [00:53,  1.49it/s]Extractor Predicting: 81it [00:54,  1.51it/s]Extractor Predicting: 82it [00:54,  1.49it/s]Extractor Predicting: 83it [00:55,  1.49it/s]Extractor Predicting: 84it [00:56,  1.49it/s]Extractor Predicting: 85it [00:56,  1.52it/s]Extractor Predicting: 86it [00:57,  1.50it/s]Extractor Predicting: 87it [00:58,  1.50it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:59,  1.51it/s]Extractor Predicting: 90it [01:00,  1.52it/s]Extractor Predicting: 91it [01:00,  1.52it/s]Extractor Predicting: 92it [01:01,  1.50it/s]Extractor Predicting: 93it [01:02,  1.48it/s]Extractor Predicting: 94it [01:02,  1.51it/s]Extractor Predicting: 95it [01:03,  1.49it/s]Extractor Predicting: 96it [01:04,  1.48it/s]Extractor Predicting: 97it [01:04,  1.51it/s]Extractor Predicting: 98it [01:05,  1.51it/s]Extractor Predicting: 99it [01:06,  1.52it/s]Extractor Predicting: 100it [01:06,  1.53it/s]Extractor Predicting: 101it [01:07,  1.55it/s]Extractor Predicting: 102it [01:08,  1.55it/s]Extractor Predicting: 103it [01:08,  1.53it/s]Extractor Predicting: 104it [01:09,  1.50it/s]Extractor Predicting: 105it [01:10,  1.48it/s]Extractor Predicting: 106it [01:10,  1.48it/s]Extractor Predicting: 107it [01:11,  1.48it/s]Extractor Predicting: 108it [01:12,  1.50it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:13,  1.50it/s]Extractor Predicting: 111it [01:14,  1.50it/s]Extractor Predicting: 112it [01:14,  1.47it/s]Extractor Predicting: 113it [01:15,  1.47it/s]Extractor Predicting: 114it [01:16,  1.46it/s]Extractor Predicting: 115it [01:16,  1.44it/s]Extractor Predicting: 116it [01:17,  1.43it/s]Extractor Predicting: 117it [01:18,  1.45it/s]Extractor Predicting: 118it [01:19,  1.46it/s]Extractor Predicting: 119it [01:19,  1.47it/s]Extractor Predicting: 120it [01:20,  1.48it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.51it/s]Extractor Predicting: 123it [01:22,  1.53it/s]Extractor Predicting: 124it [01:22,  1.52it/s]Extractor Predicting: 125it [01:23,  1.50it/s]Extractor Predicting: 126it [01:24,  1.48it/s]Extractor Predicting: 127it [01:24,  1.48it/s]Extractor Predicting: 128it [01:25,  1.48it/s]Extractor Predicting: 129it [01:26,  1.51it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:27,  1.52it/s]Extractor Predicting: 132it [01:28,  1.54it/s]Extractor Predicting: 133it [01:28,  1.51it/s]Extractor Predicting: 134it [01:29,  1.50it/s]Extractor Predicting: 135it [01:30,  1.49it/s]Extractor Predicting: 136it [01:31,  1.31it/s]Extractor Predicting: 137it [01:31,  1.34it/s]Extractor Predicting: 138it [01:32,  1.42it/s]Extractor Predicting: 139it [01:33,  1.44it/s]Extractor Predicting: 140it [01:33,  1.42it/s]Extractor Predicting: 141it [01:34,  1.45it/s]Extractor Predicting: 142it [01:35,  1.46it/s]Extractor Predicting: 143it [01:35,  1.47it/s]Extractor Predicting: 144it [01:36,  1.45it/s]Extractor Predicting: 145it [01:37,  1.48it/s]Extractor Predicting: 146it [01:38,  1.48it/s]Extractor Predicting: 147it [01:38,  1.51it/s]Extractor Predicting: 148it [01:39,  1.49it/s]Extractor Predicting: 149it [01:39,  1.51it/s]Extractor Predicting: 150it [01:40,  1.51it/s]Extractor Predicting: 151it [01:41,  1.54it/s]Extractor Predicting: 152it [01:41,  1.56it/s]Extractor Predicting: 153it [01:42,  1.55it/s]Extractor Predicting: 154it [01:43,  1.56it/s]Extractor Predicting: 155it [01:43,  1.55it/s]Extractor Predicting: 156it [01:44,  1.55it/s]Extractor Predicting: 157it [01:45,  1.52it/s]Extractor Predicting: 158it [01:45,  1.48it/s]Extractor Predicting: 159it [01:46,  1.50it/s]Extractor Predicting: 160it [01:47,  1.53it/s]Extractor Predicting: 161it [01:47,  1.53it/s]Extractor Predicting: 162it [01:48,  1.54it/s]Extractor Predicting: 163it [01:49,  1.55it/s]Extractor Predicting: 164it [01:49,  1.55it/s]Extractor Predicting: 165it [01:50,  1.53it/s]Extractor Predicting: 166it [01:51,  1.52it/s]Extractor Predicting: 167it [01:51,  1.52it/s]Extractor Predicting: 168it [01:52,  1.52it/s]Extractor Predicting: 169it [01:52,  1.56it/s]Extractor Predicting: 170it [01:53,  1.56it/s]Extractor Predicting: 171it [01:54,  1.53it/s]Extractor Predicting: 172it [01:54,  1.52it/s]Extractor Predicting: 173it [01:55,  1.52it/s]Extractor Predicting: 174it [01:56,  1.54it/s]Extractor Predicting: 175it [01:56,  1.57it/s]Extractor Predicting: 176it [01:57,  1.58it/s]Extractor Predicting: 177it [01:58,  1.52it/s]Extractor Predicting: 178it [01:58,  1.55it/s]Extractor Predicting: 179it [01:59,  1.53it/s]Extractor Predicting: 180it [02:00,  1.53it/s]Extractor Predicting: 181it [02:00,  1.54it/s]Extractor Predicting: 182it [02:01,  1.52it/s]Extractor Predicting: 183it [02:02,  1.53it/s]Extractor Predicting: 184it [02:02,  1.54it/s]Extractor Predicting: 185it [02:03,  1.56it/s]Extractor Predicting: 186it [02:03,  1.58it/s]Extractor Predicting: 187it [02:04,  1.62it/s]Extractor Predicting: 188it [02:05,  1.60it/s]Extractor Predicting: 189it [02:05,  1.55it/s]Extractor Predicting: 190it [02:06,  1.54it/s]Extractor Predicting: 191it [02:07,  1.52it/s]Extractor Predicting: 192it [02:07,  1.54it/s]Extractor Predicting: 193it [02:08,  1.55it/s]Extractor Predicting: 194it [02:09,  1.56it/s]Extractor Predicting: 195it [02:09,  1.58it/s]Extractor Predicting: 196it [02:10,  1.58it/s]Extractor Predicting: 197it [02:11,  1.53it/s]Extractor Predicting: 198it [02:11,  1.54it/s]Extractor Predicting: 199it [02:12,  1.52it/s]Extractor Predicting: 200it [02:13,  1.53it/s]Extractor Predicting: 201it [02:13,  1.53it/s]Extractor Predicting: 202it [02:14,  1.53it/s]Extractor Predicting: 203it [02:14,  1.52it/s]Extractor Predicting: 204it [02:15,  1.50it/s]Extractor Predicting: 205it [02:16,  1.47it/s]Extractor Predicting: 206it [02:17,  1.51it/s]Extractor Predicting: 207it [02:17,  1.51it/s]Extractor Predicting: 208it [02:18,  1.54it/s]Extractor Predicting: 209it [02:18,  1.55it/s]Extractor Predicting: 210it [02:19,  1.54it/s]Extractor Predicting: 211it [02:20,  1.52it/s]Extractor Predicting: 212it [02:20,  1.52it/s]Extractor Predicting: 213it [02:21,  1.50it/s]Extractor Predicting: 214it [02:22,  1.50it/s]Extractor Predicting: 215it [02:22,  1.49it/s]Extractor Predicting: 216it [02:23,  1.50it/s]Extractor Predicting: 217it [02:24,  1.48it/s]Extractor Predicting: 218it [02:24,  1.49it/s]Extractor Predicting: 219it [02:25,  1.49it/s]Extractor Predicting: 220it [02:26,  1.51it/s]Extractor Predicting: 221it [02:26,  1.55it/s]Extractor Predicting: 222it [02:27,  1.49it/s]Extractor Predicting: 223it [02:28,  1.47it/s]Extractor Predicting: 224it [02:28,  1.52it/s]Extractor Predicting: 225it [02:29,  1.52it/s]Extractor Predicting: 226it [02:30,  1.51it/s]Extractor Predicting: 227it [02:30,  1.51it/s]Extractor Predicting: 228it [02:31,  1.52it/s]Extractor Predicting: 229it [02:32,  1.55it/s]Extractor Predicting: 230it [02:32,  1.50it/s]Extractor Predicting: 231it [02:33,  1.52it/s]Extractor Predicting: 232it [02:34,  1.54it/s]Extractor Predicting: 233it [02:34,  1.51it/s]Extractor Predicting: 234it [02:35,  1.49it/s]Extractor Predicting: 235it [02:36,  1.50it/s]Extractor Predicting: 236it [02:36,  1.51it/s]Extractor Predicting: 237it [02:37,  1.48it/s]Extractor Predicting: 238it [02:38,  1.49it/s]Extractor Predicting: 239it [02:38,  1.50it/s]Extractor Predicting: 240it [02:39,  1.48it/s]Extractor Predicting: 241it [02:40,  1.50it/s]Extractor Predicting: 242it [02:40,  1.52it/s]Extractor Predicting: 243it [02:41,  1.31it/s]Extractor Predicting: 244it [02:42,  1.37it/s]Extractor Predicting: 245it [02:43,  1.39it/s]Extractor Predicting: 246it [02:43,  1.43it/s]Extractor Predicting: 247it [02:44,  1.44it/s]Extractor Predicting: 248it [02:45,  1.46it/s]Extractor Predicting: 249it [02:45,  1.48it/s]Extractor Predicting: 250it [02:46,  1.47it/s]Extractor Predicting: 251it [02:47,  1.49it/s]Extractor Predicting: 252it [02:47,  1.47it/s]Extractor Predicting: 253it [02:48,  1.48it/s]Extractor Predicting: 254it [02:49,  1.49it/s]Extractor Predicting: 255it [02:49,  1.49it/s]Extractor Predicting: 256it [02:50,  1.49it/s]Extractor Predicting: 257it [02:51,  1.52it/s]Extractor Predicting: 258it [02:51,  1.52it/s]Extractor Predicting: 259it [02:52,  1.54it/s]Extractor Predicting: 260it [02:53,  1.52it/s]Extractor Predicting: 261it [02:53,  1.52it/s]Extractor Predicting: 262it [02:54,  1.51it/s]Extractor Predicting: 263it [02:55,  1.52it/s]Extractor Predicting: 264it [02:55,  1.48it/s]Extractor Predicting: 265it [02:56,  1.49it/s]Extractor Predicting: 266it [02:57,  1.49it/s]Extractor Predicting: 267it [02:57,  1.51it/s]Extractor Predicting: 268it [02:58,  1.52it/s]Extractor Predicting: 269it [02:59,  1.52it/s]Extractor Predicting: 270it [02:59,  1.51it/s]Extractor Predicting: 271it [03:00,  1.52it/s]Extractor Predicting: 272it [03:01,  1.56it/s]Extractor Predicting: 273it [03:01,  1.57it/s]Extractor Predicting: 274it [03:02,  1.57it/s]Extractor Predicting: 275it [03:03,  1.55it/s]Extractor Predicting: 276it [03:03,  1.52it/s]Extractor Predicting: 277it [03:04,  1.55it/s]Extractor Predicting: 278it [03:04,  1.52it/s]Extractor Predicting: 279it [03:05,  1.53it/s]Extractor Predicting: 280it [03:06,  1.51it/s]Extractor Predicting: 281it [03:06,  1.53it/s]Extractor Predicting: 282it [03:07,  1.51it/s]Extractor Predicting: 283it [03:08,  1.51it/s]Extractor Predicting: 284it [03:08,  1.52it/s]Extractor Predicting: 285it [03:09,  1.51it/s]Extractor Predicting: 286it [03:10,  1.50it/s]Extractor Predicting: 287it [03:10,  1.51it/s]Extractor Predicting: 288it [03:11,  1.52it/s]Extractor Predicting: 289it [03:12,  1.53it/s]Extractor Predicting: 290it [03:12,  1.52it/s]Extractor Predicting: 291it [03:13,  1.52it/s]Extractor Predicting: 292it [03:14,  1.53it/s]Extractor Predicting: 293it [03:14,  1.53it/s]Extractor Predicting: 294it [03:15,  1.51it/s]Extractor Predicting: 295it [03:16,  1.54it/s]Extractor Predicting: 296it [03:16,  1.54it/s]Extractor Predicting: 297it [03:17,  1.51it/s]Extractor Predicting: 298it [03:18,  1.54it/s]Extractor Predicting: 299it [03:18,  1.53it/s]Extractor Predicting: 300it [03:19,  1.51it/s]Extractor Predicting: 301it [03:20,  1.57it/s]Extractor Predicting: 302it [03:20,  1.56it/s]Extractor Predicting: 303it [03:21,  1.59it/s]Extractor Predicting: 304it [03:21,  1.56it/s]Extractor Predicting: 305it [03:22,  1.54it/s]Extractor Predicting: 306it [03:23,  1.54it/s]Extractor Predicting: 307it [03:23,  1.52it/s]Extractor Predicting: 308it [03:24,  1.51it/s]Extractor Predicting: 309it [03:25,  1.52it/s]Extractor Predicting: 310it [03:25,  1.50it/s]Extractor Predicting: 311it [03:26,  1.50it/s]Extractor Predicting: 312it [03:27,  1.46it/s]Extractor Predicting: 313it [03:28,  1.48it/s]Extractor Predicting: 314it [03:28,  1.48it/s]Extractor Predicting: 315it [03:29,  1.52it/s]Extractor Predicting: 316it [03:29,  1.53it/s]Extractor Predicting: 317it [03:30,  1.52it/s]Extractor Predicting: 318it [03:31,  1.52it/s]Extractor Predicting: 319it [03:31,  1.51it/s]Extractor Predicting: 320it [03:32,  1.51it/s]Extractor Predicting: 321it [03:33,  1.51it/s]Extractor Predicting: 322it [03:33,  1.50it/s]Extractor Predicting: 323it [03:34,  1.55it/s]Extractor Predicting: 324it [03:35,  1.52it/s]Extractor Predicting: 325it [03:35,  1.53it/s]Extractor Predicting: 326it [03:36,  1.52it/s]Extractor Predicting: 327it [03:37,  1.54it/s]Extractor Predicting: 328it [03:37,  1.56it/s]Extractor Predicting: 329it [03:38,  1.53it/s]Extractor Predicting: 330it [03:39,  1.51it/s]Extractor Predicting: 331it [03:39,  1.50it/s]Extractor Predicting: 332it [03:40,  1.49it/s]Extractor Predicting: 333it [03:41,  1.50it/s]Extractor Predicting: 334it [03:41,  1.50it/s]Extractor Predicting: 335it [03:42,  1.50it/s]Extractor Predicting: 336it [03:43,  1.54it/s]Extractor Predicting: 337it [03:44,  1.36it/s]Extractor Predicting: 338it [03:44,  1.40it/s]Extractor Predicting: 339it [03:45,  1.41it/s]Extractor Predicting: 340it [03:46,  1.45it/s]Extractor Predicting: 341it [03:46,  1.48it/s]Extractor Predicting: 342it [03:47,  1.46it/s]Extractor Predicting: 343it [03:48,  1.48it/s]Extractor Predicting: 344it [03:48,  1.49it/s]Extractor Predicting: 345it [03:49,  1.51it/s]Extractor Predicting: 346it [03:50,  1.51it/s]Extractor Predicting: 347it [03:50,  1.50it/s]Extractor Predicting: 348it [03:51,  1.51it/s]Extractor Predicting: 349it [03:52,  1.49it/s]Extractor Predicting: 350it [03:52,  1.49it/s]Extractor Predicting: 351it [03:53,  1.47it/s]Extractor Predicting: 352it [03:54,  1.44it/s]Extractor Predicting: 353it [03:54,  1.45it/s]Extractor Predicting: 354it [03:55,  1.45it/s]Extractor Predicting: 355it [03:56,  1.47it/s]Extractor Predicting: 356it [03:56,  1.48it/s]Extractor Predicting: 357it [03:57,  1.50it/s]Extractor Predicting: 358it [03:58,  1.51it/s]Extractor Predicting: 359it [03:58,  1.50it/s]Extractor Predicting: 360it [03:59,  1.49it/s]Extractor Predicting: 361it [04:00,  1.50it/s]Extractor Predicting: 362it [04:00,  1.51it/s]Extractor Predicting: 363it [04:01,  1.53it/s]Extractor Predicting: 364it [04:02,  1.53it/s]Extractor Predicting: 365it [04:02,  1.53it/s]Extractor Predicting: 366it [04:03,  1.52it/s]Extractor Predicting: 367it [04:04,  1.50it/s]Extractor Predicting: 368it [04:04,  1.52it/s]Extractor Predicting: 369it [04:05,  1.52it/s]Extractor Predicting: 370it [04:05,  1.56it/s]Extractor Predicting: 371it [04:06,  1.54it/s]Extractor Predicting: 372it [04:07,  1.54it/s]Extractor Predicting: 373it [04:07,  1.53it/s]Extractor Predicting: 374it [04:08,  1.52it/s]Extractor Predicting: 375it [04:09,  1.53it/s]Extractor Predicting: 376it [04:09,  1.53it/s]Extractor Predicting: 377it [04:10,  1.54it/s]Extractor Predicting: 378it [04:11,  1.56it/s]Extractor Predicting: 379it [04:11,  1.51it/s]Extractor Predicting: 380it [04:12,  1.52it/s]Extractor Predicting: 381it [04:13,  1.55it/s]Extractor Predicting: 382it [04:13,  1.55it/s]Extractor Predicting: 383it [04:14,  1.50it/s]Extractor Predicting: 384it [04:15,  1.51it/s]Extractor Predicting: 385it [04:15,  1.52it/s]Extractor Predicting: 386it [04:16,  1.57it/s]Extractor Predicting: 387it [04:17,  1.58it/s]Extractor Predicting: 388it [04:17,  1.57it/s]Extractor Predicting: 389it [04:18,  1.55it/s]Extractor Predicting: 390it [04:18,  1.56it/s]Extractor Predicting: 391it [04:19,  1.56it/s]Extractor Predicting: 392it [04:20,  1.58it/s]Extractor Predicting: 393it [04:20,  1.58it/s]Extractor Predicting: 394it [04:21,  1.61it/s]Extractor Predicting: 395it [04:22,  1.57it/s]Extractor Predicting: 396it [04:22,  1.57it/s]Extractor Predicting: 397it [04:23,  1.57it/s]Extractor Predicting: 398it [04:24,  1.58it/s]Extractor Predicting: 399it [04:24,  1.60it/s]Extractor Predicting: 400it [04:25,  1.64it/s]Extractor Predicting: 401it [04:25,  1.59it/s]Extractor Predicting: 402it [04:26,  1.60it/s]Extractor Predicting: 403it [04:27,  1.58it/s]Extractor Predicting: 404it [04:27,  1.57it/s]Extractor Predicting: 405it [04:28,  1.57it/s]Extractor Predicting: 406it [04:29,  1.56it/s]Extractor Predicting: 407it [04:29,  1.55it/s]Extractor Predicting: 408it [04:30,  1.56it/s]Extractor Predicting: 409it [04:31,  1.55it/s]Extractor Predicting: 409it [04:31,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:32,967 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:32,975 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:32,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:32,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:32,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:14:33,801 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:14:33,802 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:14:34,371 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:14:35,426 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:14:35,426 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:38,356 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:38,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:38,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:38,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:14:38,359 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:14:39,157 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:14:39,161 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:14:39,748 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:14:39,908 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:14:39,908 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3882138517618469,
  "recall": 0.19535310302659736,
  "score": 0.25991458206223306,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.36it/s]Extractor Predicting: 3it [00:02,  1.38it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.43it/s]Extractor Predicting: 6it [00:04,  1.43it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.45it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.16666666666666666,
  "recall": 0.021834061135371178,
  "score": 0.03861003861003861,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/', 'num_iter': 5, 'data_name': 'wiki', 'split': 'unseen_15_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:16<05:22, 16.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:34<05:06, 17.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:57<05:42, 20.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:13<04:53, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:32<04:41, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:48<04:07, 17.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [02:07<03:54, 18.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:25<03:36, 18.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:44<03:22, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [03:03<03:04, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:19<02:40, 17.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:35<02:18, 17.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:52<02:00, 17.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [04:08<01:41, 16.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:27<01:27, 17.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:46<01:11, 17.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [05:03<00:53, 17.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [05:19<00:34, 17.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:36<00:17, 17.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:55<00:00, 17.56s/it]Generating: 100%|██████████| 20/20 [05:55<00:00, 17.76s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 167, 'raw': 224}
{'target': 600, 'success': 192, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 370, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 423, 'raw': 544}
{'target': 600, 'success': 451, 'raw': 576}
{'target': 600, 'success': 475, 'raw': 608}
{'target': 600, 'success': 503, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 552, 'raw': 704}
{'target': 600, 'success': 579, 'raw': 736}
{'target': 600, 'success': 602, 'raw': 768}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.7838541666666666, 'errors': {'', "('Poseidon', 'from narrative universe', '', 'According to John Muir , one could go back in time as far as to the Greek myth of Poseidon ( and possibly Neptune ) .')"}}
{'target': 600, 'success': 17, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 80, 'raw': 128}
{'target': 600, 'success': 98, 'raw': 160}
{'target': 600, 'success': 119, 'raw': 192}
{'target': 600, 'success': 139, 'raw': 224}
{'target': 600, 'success': 158, 'raw': 256}
{'target': 600, 'success': 176, 'raw': 288}
{'target': 600, 'success': 191, 'raw': 320}
{'target': 600, 'success': 212, 'raw': 352}
{'target': 600, 'success': 231, 'raw': 384}
{'target': 600, 'success': 253, 'raw': 416}
{'target': 600, 'success': 272, 'raw': 448}
{'target': 600, 'success': 293, 'raw': 480}
{'target': 600, 'success': 308, 'raw': 512}
{'target': 600, 'success': 326, 'raw': 544}
{'target': 600, 'success': 345, 'raw': 576}
{'target': 600, 'success': 363, 'raw': 608}
{'target': 600, 'success': 386, 'raw': 640}
{'target': 600, 'success': 406, 'raw': 672}
{'target': 600, 'success': 423, 'raw': 704}
{'target': 600, 'success': 439, 'raw': 736}
{'target': 600, 'success': 459, 'raw': 768}
{'target': 600, 'success': 478, 'raw': 800}
{'target': 600, 'success': 497, 'raw': 832}
{'target': 600, 'success': 515, 'raw': 864}
{'target': 600, 'success': 535, 'raw': 896}
{'target': 600, 'success': 556, 'raw': 928}
{'target': 600, 'success': 577, 'raw': 960}
{'target': 600, 'success': 594, 'raw': 992}
{'target': 600, 'success': 613, 'raw': 1024}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.5986328125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.8849431818181818, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 117, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 195, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 260, 'raw': 352}
{'target': 600, 'success': 285, 'raw': 384}
{'target': 600, 'success': 308, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 458, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 509, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 567, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 616, 'raw': 800}
{'prompt': 'Relation : work location .', 'success_rate': 0.77, 'errors': {'', "('Henri Poincaré', 'work location', '', 'He was inspired by the work of Henri Poincaré ( d.')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 301, 'raw': 384}
{'target': 600, 'success': 326, 'raw': 416}
{'target': 600, 'success': 354, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : follows .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 174, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 305, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 415, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 520, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 571, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : has quality .', 'success_rate': 0.8179347826086957, 'errors': {'', "('Václavnár department', 'has quality', '', 'The next step was to obtain the right equipment from a specialist manufacturer based in Alchevé in the Václavnár department in Prague .')", "('2004 championship', 'has quality', '', 'He was a member of team that won the 2004 championship , playing alongside Tim Hardaway , Pat Mahaffy , Tim Howard and Bobby Hart .')", "('The Hateful Eight', 'has quality', '', 'A great actor , he appeared in most English TV series before moving to London in 1994 to become producer of The Hateful Eight .')"}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 150, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 199, 'raw': 256}
{'target': 600, 'success': 223, 'raw': 288}
{'target': 600, 'success': 247, 'raw': 320}
{'target': 600, 'success': 269, 'raw': 352}
{'target': 600, 'success': 292, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 388, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 534, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 581, 'raw': 768}
{'target': 600, 'success': 608, 'raw': 800}
{'prompt': 'Relation : instrument .', 'success_rate': 0.76, 'errors': {'', "('Bon Jovi', 'instrument', '', 'On the opening day of the show , the band played with New York City band Bon Jovi at New York Liberty Arena on March 3 , 2011 , as part of the Live Nation tour .')", "('My Own', 'instrument', '', 'The sound is reminiscent of jazz trumpeter Sonny Rollins on his second song , My Own , performed by Richard Strauss .')"}}
['Relation : league . Context : Steve Cagan ( born October 8 , 1987 ) is an American professional basketball player at power forward for the New York Knicks of the Arena Football League ( ADL ) . Head Entity : Arena Football League , Tail Entity : NBA .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 70, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 166, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 299, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 398, 'raw': 544}
{'target': 600, 'success': 420, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 515, 'raw': 704}
{'target': 600, 'success': 541, 'raw': 736}
{'target': 600, 'success': 566, 'raw': 768}
{'target': 600, 'success': 587, 'raw': 800}
{'target': 600, 'success': 610, 'raw': 832}
{'prompt': 'Relation : league .', 'success_rate': 0.7331730769230769, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 418, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.875, 'errors': {''}}
['Relation : located on astronomical body . Context : The Cretaceous ( κ Cr ) , a Late Cretaceous ( υ Cen ) , Jurassic ( κ Ne ) , and Mecolithic ( κ Neo ) phases occur in the southwestern parts of the world . Head Entity : Cretaceous , Tail Entity : the asteroid .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 584, 'raw': 704}
{'target': 600, 'success': 608, 'raw': 736}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8260869565217391, 'errors': {'', "('galaxy', 'located on astronomical body', '', 'Most galaxies that show high eccentricity ( i. e. with masses less than the mass of the galaxy ) are not far from their stars .')", "('Venus', 'located on astronomical body', '', 'The planet Venus , or at Venus , is one of three moons of the giant planet ; the other moons are Mimas and Helia .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 617, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8383152173913043, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8536931818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 259, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 400, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 615, 'raw': 736}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8355978260869565, 'errors': {'', "('Deep Green Resistance', 'opposite of', '', 'She is known for playing the role of Margo in the CBS science fiction drama series Deep Green Resistance .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8491847826086957, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 454, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8396739130434783, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8491847826086957, 'errors': {'', "('Chief Rabbi', 'twinned administrative body', '', 'Sajid Khan ( 12 January 1918 3 December 2003 ) was the first Muslim to ever hold the rank of Chief Rabbi ( d.')"}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 18766
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 18866, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:15, 15.53s/it]Extractor Estimating: 2it [00:16,  7.11s/it]Extractor Estimating: 3it [00:17,  4.15s/it]Extractor Estimating: 4it [00:17,  2.74s/it]Extractor Estimating: 5it [00:18,  1.97s/it]Extractor Estimating: 6it [00:19,  1.70s/it]Extractor Estimating: 7it [00:20,  1.35s/it]Extractor Estimating: 8it [00:21,  1.14s/it]Extractor Estimating: 9it [00:21,  1.02it/s]Extractor Estimating: 10it [00:22,  1.15it/s]Extractor Estimating: 11it [00:23,  1.21it/s]Extractor Estimating: 12it [00:24,  1.06it/s]Extractor Estimating: 13it [00:24,  1.15it/s]Extractor Estimating: 14it [00:25,  1.25it/s]Extractor Estimating: 15it [00:26,  1.38it/s]Extractor Estimating: 16it [00:26,  1.43it/s]Extractor Estimating: 17it [00:27,  1.47it/s]Extractor Estimating: 18it [00:28,  1.45it/s]Extractor Estimating: 19it [00:28,  1.48it/s]Extractor Estimating: 20it [00:29,  1.49it/s]Extractor Estimating: 21it [00:30,  1.19it/s]Extractor Estimating: 22it [00:31,  1.27it/s]Extractor Estimating: 23it [00:31,  1.36it/s]Extractor Estimating: 24it [00:32,  1.43it/s]Extractor Estimating: 25it [00:33,  1.49it/s]Extractor Estimating: 26it [00:33,  1.47it/s]Extractor Estimating: 27it [00:34,  1.47it/s]Extractor Estimating: 28it [00:35,  1.49it/s]Extractor Estimating: 29it [00:35,  1.52it/s]Extractor Estimating: 30it [00:36,  1.53it/s]Extractor Estimating: 31it [00:37,  1.53it/s]Extractor Estimating: 32it [00:37,  1.52it/s]Extractor Estimating: 33it [00:38,  1.51it/s]Extractor Estimating: 34it [00:39,  1.53it/s]Extractor Estimating: 35it [00:39,  1.55it/s]Extractor Estimating: 36it [00:40,  1.54it/s]Extractor Estimating: 37it [00:41,  1.55it/s]Extractor Estimating: 38it [00:41,  1.52it/s]Extractor Estimating: 39it [00:42,  1.56it/s]Extractor Estimating: 40it [00:42,  1.59it/s]Extractor Estimating: 41it [00:43,  1.51it/s]Extractor Estimating: 42it [00:44,  1.54it/s]Extractor Estimating: 43it [00:44,  1.58it/s]Extractor Estimating: 44it [00:45,  1.60it/s]Extractor Estimating: 45it [00:46,  1.59it/s]Extractor Estimating: 46it [00:47,  1.21it/s]Extractor Estimating: 47it [00:47,  1.33it/s]Extractor Estimating: 48it [00:48,  1.38it/s]Extractor Estimating: 49it [00:49,  1.45it/s]Extractor Estimating: 50it [00:49,  1.52it/s]Extractor Estimating: 51it [00:50,  1.55it/s]Extractor Estimating: 52it [00:51,  1.54it/s]Extractor Estimating: 53it [00:51,  1.54it/s]Extractor Estimating: 54it [00:52,  1.51it/s]Extractor Estimating: 55it [00:53,  1.17it/s]Extractor Estimating: 56it [00:54,  1.27it/s]Extractor Estimating: 57it [00:55,  1.34it/s]Extractor Estimating: 58it [00:55,  1.39it/s]Extractor Estimating: 59it [00:56,  1.43it/s]Extractor Estimating: 60it [00:56,  1.46it/s]Extractor Estimating: 61it [00:57,  1.46it/s]Extractor Estimating: 62it [00:58,  1.44it/s]Extractor Estimating: 63it [00:59,  1.46it/s]Extractor Estimating: 64it [00:59,  1.46it/s]Extractor Estimating: 65it [01:00,  1.47it/s]Extractor Estimating: 66it [01:01,  1.44it/s]Extractor Estimating: 67it [01:01,  1.49it/s]Extractor Estimating: 68it [01:02,  1.47it/s]Extractor Estimating: 69it [01:03,  1.49it/s]Extractor Estimating: 70it [01:03,  1.50it/s]Extractor Estimating: 71it [01:04,  1.53it/s]Extractor Estimating: 72it [01:05,  1.52it/s]Extractor Estimating: 73it [01:05,  1.51it/s]Extractor Estimating: 74it [01:06,  1.56it/s]Extractor Estimating: 75it [01:06,  1.61it/s]Extractor Estimating: 76it [01:07,  1.58it/s]Extractor Estimating: 77it [01:08,  1.62it/s]Extractor Estimating: 78it [01:08,  1.64it/s]Extractor Estimating: 79it [01:09,  1.63it/s]Extractor Estimating: 80it [01:09,  1.63it/s]Extractor Estimating: 81it [01:10,  1.63it/s]Extractor Estimating: 82it [01:11,  1.64it/s]Extractor Estimating: 83it [01:11,  1.62it/s]Extractor Estimating: 84it [01:12,  1.66it/s]Extractor Estimating: 85it [01:12,  1.64it/s]Extractor Estimating: 86it [01:13,  1.67it/s]Extractor Estimating: 87it [01:14,  1.69it/s]Extractor Estimating: 88it [01:14,  1.62it/s]Extractor Estimating: 89it [01:15,  1.65it/s]Extractor Estimating: 90it [01:16,  1.61it/s]Extractor Estimating: 91it [01:16,  1.68it/s]Extractor Estimating: 92it [01:17,  1.68it/s]Extractor Estimating: 93it [01:17,  1.69it/s]Extractor Estimating: 94it [01:18,  1.72it/s]Extractor Estimating: 95it [01:18,  1.71it/s]Extractor Estimating: 96it [01:19,  1.67it/s]Extractor Estimating: 97it [01:20,  1.66it/s]Extractor Estimating: 98it [01:20,  1.67it/s]Extractor Estimating: 99it [01:21,  1.65it/s]Extractor Estimating: 100it [01:21,  1.64it/s]Extractor Estimating: 101it [01:22,  1.60it/s]Extractor Estimating: 102it [01:23,  1.60it/s]Extractor Estimating: 103it [01:23,  1.56it/s]Extractor Estimating: 104it [01:24,  1.53it/s]Extractor Estimating: 105it [01:25,  1.54it/s]Extractor Estimating: 106it [01:25,  1.50it/s]Extractor Estimating: 107it [01:26,  1.55it/s]Extractor Estimating: 108it [01:27,  1.54it/s]Extractor Estimating: 109it [01:27,  1.52it/s]Extractor Estimating: 110it [01:28,  1.56it/s]Extractor Estimating: 111it [01:29,  1.56it/s]Extractor Estimating: 112it [01:29,  1.51it/s]Extractor Estimating: 113it [01:30,  1.53it/s]Extractor Estimating: 114it [01:31,  1.55it/s]Extractor Estimating: 115it [01:31,  1.61it/s]Extractor Estimating: 116it [01:32,  1.60it/s]Extractor Estimating: 117it [01:33,  1.55it/s]Extractor Estimating: 118it [01:33,  1.53it/s]Extractor Estimating: 119it [01:34,  1.51it/s]Extractor Estimating: 120it [01:35,  1.48it/s]Extractor Estimating: 121it [01:35,  1.47it/s]Extractor Estimating: 122it [01:36,  1.51it/s]Extractor Estimating: 123it [01:36,  1.55it/s]Extractor Estimating: 124it [01:37,  1.59it/s]Extractor Estimating: 125it [01:38,  1.59it/s]Extractor Estimating: 126it [01:38,  1.59it/s]Extractor Estimating: 127it [01:39,  1.58it/s]Extractor Estimating: 128it [01:40,  1.52it/s]Extractor Estimating: 129it [01:40,  1.50it/s]Extractor Estimating: 130it [01:41,  1.39it/s]Extractor Estimating: 131it [01:42,  1.43it/s]Extractor Estimating: 132it [01:43,  1.43it/s]Extractor Estimating: 133it [01:43,  1.46it/s]Extractor Estimating: 134it [01:44,  1.50it/s]Extractor Estimating: 135it [01:45,  1.49it/s]Extractor Estimating: 136it [01:45,  1.51it/s]Extractor Estimating: 137it [01:46,  1.52it/s]Extractor Estimating: 138it [01:46,  1.52it/s]Extractor Estimating: 139it [01:47,  1.56it/s]Extractor Estimating: 140it [01:48,  1.57it/s]Extractor Estimating: 141it [01:48,  1.59it/s]Extractor Estimating: 142it [01:49,  1.58it/s]Extractor Estimating: 143it [01:50,  1.60it/s]Extractor Estimating: 144it [01:50,  1.55it/s]Extractor Estimating: 145it [01:51,  1.57it/s]Extractor Estimating: 146it [01:52,  1.52it/s]Extractor Estimating: 147it [01:52,  1.55it/s]Extractor Estimating: 148it [01:53,  1.55it/s]Extractor Estimating: 149it [01:53,  1.56it/s]Extractor Estimating: 150it [01:54,  1.55it/s]Extractor Estimating: 151it [01:55,  1.52it/s]Extractor Estimating: 152it [01:56,  1.17it/s]Extractor Estimating: 153it [01:57,  1.28it/s]Extractor Estimating: 154it [01:57,  1.37it/s]Extractor Estimating: 155it [01:58,  1.44it/s]Extractor Estimating: 156it [01:59,  1.46it/s]Extractor Estimating: 157it [01:59,  1.48it/s]Extractor Estimating: 158it [02:00,  1.47it/s]Extractor Estimating: 159it [02:01,  1.51it/s]Extractor Estimating: 160it [02:01,  1.49it/s]Extractor Estimating: 161it [02:02,  1.46it/s]Extractor Estimating: 162it [02:03,  1.45it/s]Extractor Estimating: 163it [02:03,  1.41it/s]Extractor Estimating: 164it [02:04,  1.45it/s]Extractor Estimating: 165it [02:05,  1.46it/s]Extractor Estimating: 166it [02:05,  1.46it/s]Extractor Estimating: 167it [02:06,  1.43it/s]Extractor Estimating: 168it [02:07,  1.44it/s]Extractor Estimating: 169it [02:08,  1.43it/s]Extractor Estimating: 170it [02:08,  1.48it/s]Extractor Estimating: 171it [02:09,  1.49it/s]Extractor Estimating: 172it [02:10,  1.44it/s]Extractor Estimating: 173it [02:10,  1.48it/s]Extractor Estimating: 174it [02:11,  1.51it/s]Extractor Estimating: 175it [02:12,  1.48it/s]Extractor Estimating: 176it [02:12,  1.51it/s]Extractor Estimating: 177it [02:13,  1.54it/s]Extractor Estimating: 178it [02:14,  1.52it/s]Extractor Estimating: 179it [02:14,  1.57it/s]Extractor Estimating: 180it [02:15,  1.55it/s]Extractor Estimating: 181it [02:15,  1.57it/s]Extractor Estimating: 182it [02:16,  1.52it/s]Extractor Estimating: 183it [02:17,  1.57it/s]Extractor Estimating: 184it [02:17,  1.61it/s]Extractor Estimating: 185it [02:18,  1.65it/s]Extractor Estimating: 186it [02:19,  1.59it/s]Extractor Estimating: 187it [02:19,  1.56it/s]Extractor Estimating: 188it [02:20,  1.58it/s]Extractor Estimating: 189it [02:20,  1.56it/s]Extractor Estimating: 190it [02:21,  1.59it/s]Extractor Estimating: 191it [02:22,  1.59it/s]Extractor Estimating: 192it [02:22,  1.59it/s]Extractor Estimating: 193it [02:23,  1.62it/s]Extractor Estimating: 194it [02:24,  1.58it/s]Extractor Estimating: 195it [02:24,  1.51it/s]Extractor Estimating: 196it [02:25,  1.52it/s]Extractor Estimating: 197it [02:26,  1.56it/s]Extractor Estimating: 198it [02:26,  1.55it/s]Extractor Estimating: 199it [02:27,  1.49it/s]Extractor Estimating: 200it [02:28,  1.53it/s]Extractor Estimating: 201it [02:28,  1.51it/s]Extractor Estimating: 202it [02:29,  1.50it/s]Extractor Estimating: 203it [02:30,  1.47it/s]Extractor Estimating: 204it [02:30,  1.51it/s]Extractor Estimating: 205it [02:31,  1.53it/s]Extractor Estimating: 206it [02:32,  1.52it/s]Extractor Estimating: 207it [02:32,  1.49it/s]Extractor Estimating: 208it [02:33,  1.54it/s]Extractor Estimating: 209it [02:34,  1.47it/s]Extractor Estimating: 210it [02:34,  1.49it/s]Extractor Estimating: 211it [02:35,  1.49it/s]Extractor Estimating: 212it [02:36,  1.52it/s]Extractor Estimating: 213it [02:36,  1.55it/s]Extractor Estimating: 214it [02:37,  1.57it/s]Extractor Estimating: 215it [02:37,  1.59it/s]Extractor Estimating: 216it [02:38,  1.59it/s]Extractor Estimating: 217it [02:39,  1.60it/s]Extractor Estimating: 218it [02:39,  1.60it/s]Extractor Estimating: 219it [02:40,  1.42it/s]Extractor Estimating: 220it [02:41,  1.46it/s]Extractor Estimating: 221it [02:41,  1.53it/s]Extractor Estimating: 222it [02:42,  1.53it/s]Extractor Estimating: 223it [02:43,  1.52it/s]Extractor Estimating: 224it [02:43,  1.55it/s]Extractor Estimating: 225it [02:44,  1.55it/s]Extractor Estimating: 226it [02:45,  1.54it/s]Extractor Estimating: 227it [02:45,  1.54it/s]Extractor Estimating: 228it [02:46,  1.60it/s]Extractor Estimating: 229it [02:46,  1.58it/s]Extractor Estimating: 230it [02:47,  1.57it/s]Extractor Estimating: 231it [02:48,  1.58it/s]Extractor Estimating: 232it [02:48,  1.58it/s]Extractor Estimating: 233it [02:49,  1.56it/s]Extractor Estimating: 234it [02:50,  1.56it/s]Extractor Estimating: 235it [02:50,  1.55it/s]Extractor Estimating: 236it [02:51,  1.55it/s]Extractor Estimating: 237it [02:52,  1.52it/s]Extractor Estimating: 238it [02:52,  1.54it/s]Extractor Estimating: 239it [02:53,  1.54it/s]Extractor Estimating: 240it [02:54,  1.54it/s]Extractor Estimating: 241it [02:54,  1.54it/s]Extractor Estimating: 242it [02:55,  1.53it/s]Extractor Estimating: 243it [02:56,  1.51it/s]Extractor Estimating: 244it [02:56,  1.53it/s]Extractor Estimating: 245it [02:57,  1.55it/s]Extractor Estimating: 246it [02:58,  1.54it/s]Extractor Estimating: 247it [02:58,  1.57it/s]Extractor Estimating: 248it [02:59,  1.56it/s]Extractor Estimating: 249it [02:59,  1.58it/s]Extractor Estimating: 250it [03:00,  1.56it/s]Extractor Estimating: 251it [03:01,  1.64it/s]Extractor Estimating: 252it [03:01,  1.67it/s]Extractor Estimating: 253it [03:02,  1.64it/s]Extractor Estimating: 254it [03:02,  1.63it/s]Extractor Estimating: 255it [03:03,  1.60it/s]Extractor Estimating: 256it [03:04,  1.63it/s]Extractor Estimating: 257it [03:04,  1.64it/s]Extractor Estimating: 258it [03:05,  1.66it/s]Extractor Estimating: 259it [03:05,  1.67it/s]Extractor Estimating: 260it [03:06,  1.65it/s]Extractor Estimating: 261it [03:07,  1.64it/s]Extractor Estimating: 262it [03:07,  1.63it/s]Extractor Estimating: 263it [03:08,  1.61it/s]Extractor Estimating: 264it [03:09,  1.64it/s]Extractor Estimating: 265it [03:09,  1.69it/s]Extractor Estimating: 266it [03:10,  1.70it/s]Extractor Estimating: 267it [03:10,  1.65it/s]Extractor Estimating: 268it [03:11,  1.65it/s]Extractor Estimating: 269it [03:11,  1.66it/s]Extractor Estimating: 270it [03:12,  1.68it/s]Extractor Estimating: 271it [03:13,  1.64it/s]Extractor Estimating: 272it [03:13,  1.69it/s]Extractor Estimating: 273it [03:14,  1.72it/s]Extractor Estimating: 274it [03:14,  1.75it/s]Extractor Estimating: 275it [03:15,  1.75it/s]Extractor Estimating: 276it [03:16,  1.75it/s]Extractor Estimating: 277it [03:16,  1.74it/s]Extractor Estimating: 278it [03:17,  1.67it/s]Extractor Estimating: 279it [03:17,  1.71it/s]Extractor Estimating: 280it [03:18,  1.68it/s]Extractor Estimating: 281it [03:19,  1.63it/s]Extractor Estimating: 282it [03:19,  1.63it/s]Extractor Estimating: 283it [03:20,  1.60it/s]Extractor Estimating: 284it [03:20,  1.61it/s]Extractor Estimating: 285it [03:21,  1.62it/s]Extractor Estimating: 286it [03:22,  1.68it/s]Extractor Estimating: 287it [03:22,  1.67it/s]Extractor Estimating: 288it [03:23,  1.60it/s]Extractor Estimating: 289it [03:23,  1.62it/s]Extractor Estimating: 290it [03:24,  1.60it/s]Extractor Estimating: 291it [03:25,  1.59it/s]Extractor Estimating: 292it [03:25,  1.62it/s]Extractor Estimating: 293it [03:26,  1.62it/s]Extractor Estimating: 294it [03:27,  1.60it/s]Extractor Estimating: 295it [03:27,  1.56it/s]Extractor Estimating: 296it [03:28,  1.59it/s]Extractor Estimating: 297it [03:29,  1.60it/s]Extractor Estimating: 298it [03:29,  1.63it/s]Extractor Estimating: 299it [03:30,  1.62it/s]Extractor Estimating: 300it [03:30,  1.56it/s]Extractor Estimating: 301it [03:31,  1.54it/s]Extractor Estimating: 302it [03:32,  1.52it/s]Extractor Estimating: 303it [03:32,  1.52it/s]Extractor Estimating: 304it [03:33,  1.55it/s]Extractor Estimating: 305it [03:34,  1.53it/s]Extractor Estimating: 306it [03:34,  1.52it/s]Extractor Estimating: 307it [03:35,  1.56it/s]Extractor Estimating: 308it [03:36,  1.54it/s]Extractor Estimating: 309it [03:36,  1.49it/s]Extractor Estimating: 310it [03:37,  1.51it/s]Extractor Estimating: 311it [03:38,  1.51it/s]Extractor Estimating: 312it [03:38,  1.52it/s]Extractor Estimating: 313it [03:39,  1.51it/s]Extractor Estimating: 314it [03:40,  1.51it/s]Extractor Estimating: 315it [03:40,  1.53it/s]Extractor Estimating: 316it [03:41,  1.35it/s]Extractor Estimating: 317it [03:42,  1.40it/s]Extractor Estimating: 318it [03:43,  1.46it/s]Extractor Estimating: 319it [03:43,  1.49it/s]Extractor Estimating: 320it [03:44,  1.52it/s]Extractor Estimating: 321it [03:44,  1.50it/s]Extractor Estimating: 322it [03:45,  1.50it/s]Extractor Estimating: 323it [03:46,  1.49it/s]Extractor Estimating: 324it [03:46,  1.50it/s]Extractor Estimating: 325it [03:47,  1.54it/s]Extractor Estimating: 326it [03:48,  1.61it/s]Extractor Estimating: 327it [03:48,  1.56it/s]Extractor Estimating: 328it [03:49,  1.58it/s]Extractor Estimating: 329it [03:50,  1.61it/s]Extractor Estimating: 330it [03:50,  1.61it/s]Extractor Estimating: 331it [03:51,  1.60it/s]Extractor Estimating: 332it [03:51,  1.61it/s]Extractor Estimating: 333it [03:52,  1.61it/s]Extractor Estimating: 334it [03:53,  1.61it/s]Extractor Estimating: 335it [03:53,  1.67it/s]Extractor Estimating: 336it [03:54,  1.64it/s]Extractor Estimating: 337it [03:54,  1.63it/s]Extractor Estimating: 338it [03:55,  1.63it/s]Extractor Estimating: 339it [03:56,  1.55it/s]Extractor Estimating: 340it [03:56,  1.57it/s]Extractor Estimating: 341it [03:57,  1.60it/s]Extractor Estimating: 342it [03:58,  1.59it/s]Extractor Estimating: 343it [03:58,  1.59it/s]Extractor Estimating: 344it [03:59,  1.64it/s]Extractor Estimating: 345it [03:59,  1.61it/s]Extractor Estimating: 346it [04:00,  1.63it/s]Extractor Estimating: 347it [04:01,  1.59it/s]Extractor Estimating: 348it [04:01,  1.58it/s]Extractor Estimating: 349it [04:02,  1.59it/s]Extractor Estimating: 350it [04:03,  1.58it/s]Extractor Estimating: 351it [04:03,  1.55it/s]Extractor Estimating: 352it [04:04,  1.48it/s]Extractor Estimating: 353it [04:05,  1.49it/s]Extractor Estimating: 354it [04:05,  1.54it/s]Extractor Estimating: 355it [04:06,  1.55it/s]Extractor Estimating: 356it [04:07,  1.53it/s]Extractor Estimating: 357it [04:07,  1.50it/s]Extractor Estimating: 358it [04:08,  1.46it/s]Extractor Estimating: 359it [04:09,  1.47it/s]Extractor Estimating: 360it [04:09,  1.46it/s]Extractor Estimating: 361it [04:10,  1.47it/s]Extractor Estimating: 362it [04:11,  1.49it/s]Extractor Estimating: 363it [04:11,  1.50it/s]Extractor Estimating: 364it [04:12,  1.45it/s]Extractor Estimating: 365it [04:13,  1.44it/s]Extractor Estimating: 366it [04:14,  1.44it/s]Extractor Estimating: 367it [04:14,  1.47it/s]Extractor Estimating: 368it [04:15,  1.48it/s]Extractor Estimating: 369it [04:16,  1.49it/s]Extractor Estimating: 370it [04:16,  1.51it/s]Extractor Estimating: 371it [04:17,  1.46it/s]Extractor Estimating: 372it [04:18,  1.47it/s]Extractor Estimating: 373it [04:18,  1.49it/s]Extractor Estimating: 374it [04:19,  1.48it/s]Extractor Estimating: 375it [04:20,  1.49it/s]Extractor Estimating: 376it [04:20,  1.52it/s]Extractor Estimating: 377it [04:21,  1.48it/s]Extractor Estimating: 378it [04:22,  1.52it/s]Extractor Estimating: 379it [04:22,  1.57it/s]Extractor Estimating: 380it [04:23,  1.56it/s]Extractor Estimating: 381it [04:23,  1.53it/s]Extractor Estimating: 382it [04:24,  1.56it/s]Extractor Estimating: 383it [04:25,  1.53it/s]Extractor Estimating: 384it [04:25,  1.54it/s]Extractor Estimating: 385it [04:26,  1.57it/s]Extractor Estimating: 386it [04:27,  1.62it/s]Extractor Estimating: 387it [04:27,  1.60it/s]Extractor Estimating: 388it [04:28,  1.62it/s]Extractor Estimating: 389it [04:28,  1.61it/s]Extractor Estimating: 390it [04:29,  1.64it/s]Extractor Estimating: 391it [04:30,  1.61it/s]Extractor Estimating: 392it [04:30,  1.53it/s]Extractor Estimating: 393it [04:31,  1.53it/s]Extractor Estimating: 394it [04:32,  1.52it/s]Extractor Estimating: 395it [04:32,  1.56it/s]Extractor Estimating: 396it [04:33,  1.52it/s]Extractor Estimating: 397it [04:34,  1.61it/s]Extractor Estimating: 398it [04:34,  1.61it/s]Extractor Estimating: 399it [04:35,  1.63it/s]Extractor Estimating: 400it [04:35,  1.58it/s]Extractor Estimating: 401it [04:36,  1.52it/s]Extractor Estimating: 402it [04:37,  1.48it/s]Extractor Estimating: 403it [04:37,  1.52it/s]Extractor Estimating: 404it [04:38,  1.50it/s]Extractor Estimating: 405it [04:39,  1.49it/s]Extractor Estimating: 406it [04:40,  1.47it/s]Extractor Estimating: 407it [04:40,  1.48it/s]Extractor Estimating: 408it [04:41,  1.52it/s]Extractor Estimating: 409it [04:41,  1.54it/s]Extractor Estimating: 410it [04:42,  1.54it/s]Extractor Estimating: 411it [04:43,  1.53it/s]Extractor Estimating: 412it [04:43,  1.52it/s]Extractor Estimating: 413it [04:44,  1.57it/s]Extractor Estimating: 414it [04:45,  1.41it/s]Extractor Estimating: 415it [04:46,  1.44it/s]Extractor Estimating: 416it [04:46,  1.53it/s]Extractor Estimating: 417it [04:47,  1.54it/s]Extractor Estimating: 418it [04:47,  1.54it/s]Extractor Estimating: 419it [04:48,  1.54it/s]Extractor Estimating: 420it [04:49,  1.55it/s]Extractor Estimating: 421it [04:49,  1.53it/s]Extractor Estimating: 422it [04:50,  1.54it/s]Extractor Estimating: 423it [04:51,  1.52it/s]Extractor Estimating: 424it [04:51,  1.52it/s]Extractor Estimating: 425it [04:52,  1.52it/s]Extractor Estimating: 426it [04:53,  1.55it/s]Extractor Estimating: 427it [04:53,  1.60it/s]Extractor Estimating: 428it [04:54,  1.60it/s]Extractor Estimating: 429it [04:54,  1.62it/s]Extractor Estimating: 430it [04:55,  1.63it/s]Extractor Estimating: 431it [04:56,  1.58it/s]Extractor Estimating: 432it [04:56,  1.60it/s]Extractor Estimating: 433it [04:57,  1.65it/s]Extractor Estimating: 434it [04:57,  1.66it/s]Extractor Estimating: 435it [04:59,  1.02it/s]Extractor Estimating: 436it [05:00,  1.14it/s]Extractor Estimating: 437it [05:01,  1.24it/s]Extractor Estimating: 438it [05:01,  1.34it/s]Extractor Estimating: 439it [05:02,  1.44it/s]Extractor Estimating: 440it [05:02,  1.52it/s]Extractor Estimating: 441it [05:03,  1.54it/s]Extractor Estimating: 442it [05:04,  1.54it/s]Extractor Estimating: 443it [05:04,  1.55it/s]Extractor Estimating: 444it [05:05,  1.61it/s]Extractor Estimating: 445it [05:05,  1.61it/s]Extractor Estimating: 446it [05:06,  1.63it/s]Extractor Estimating: 447it [05:07,  1.66it/s]Extractor Estimating: 448it [05:07,  1.70it/s]Extractor Estimating: 449it [05:08,  1.69it/s]Extractor Estimating: 450it [05:08,  1.65it/s]Extractor Estimating: 451it [05:09,  1.59it/s]Extractor Estimating: 452it [05:10,  1.59it/s]Extractor Estimating: 453it [05:10,  1.51it/s]Extractor Estimating: 454it [05:11,  1.57it/s]Extractor Estimating: 455it [05:12,  1.54it/s]Extractor Estimating: 456it [05:12,  1.55it/s]Extractor Estimating: 457it [05:13,  1.57it/s]Extractor Estimating: 458it [05:14,  1.57it/s]Extractor Estimating: 459it [05:14,  1.58it/s]Extractor Estimating: 460it [05:15,  1.57it/s]Extractor Estimating: 461it [05:15,  1.61it/s]Extractor Estimating: 462it [05:16,  1.64it/s]Extractor Estimating: 463it [05:17,  1.69it/s]Extractor Estimating: 464it [05:17,  1.65it/s]Extractor Estimating: 465it [05:18,  1.63it/s]Extractor Estimating: 466it [05:19,  1.60it/s]Extractor Estimating: 467it [05:19,  1.61it/s]Extractor Estimating: 468it [05:20,  1.63it/s]Extractor Estimating: 469it [05:20,  1.61it/s]Extractor Estimating: 470it [05:21,  1.61it/s]Extractor Estimating: 471it [05:22,  1.52it/s]Extractor Estimating: 472it [05:22,  1.49it/s]Extractor Estimating: 473it [05:23,  1.49it/s]Extractor Estimating: 474it [05:24,  1.53it/s]Extractor Estimating: 475it [05:24,  1.53it/s]Extractor Estimating: 476it [05:25,  1.55it/s]Extractor Estimating: 477it [05:26,  1.58it/s]Extractor Estimating: 478it [05:26,  1.55it/s]Extractor Estimating: 479it [05:27,  1.52it/s]Extractor Estimating: 480it [05:28,  1.57it/s]Extractor Estimating: 481it [05:28,  1.58it/s]Extractor Estimating: 482it [05:29,  1.61it/s]Extractor Estimating: 483it [05:29,  1.62it/s]Extractor Estimating: 484it [05:30,  1.57it/s]Extractor Estimating: 485it [05:31,  1.61it/s]Extractor Estimating: 486it [05:31,  1.57it/s]Extractor Estimating: 487it [05:32,  1.54it/s]Extractor Estimating: 488it [05:33,  1.55it/s]Extractor Estimating: 489it [05:35,  1.04s/it]Extractor Estimating: 490it [05:35,  1.07it/s]Extractor Estimating: 491it [05:36,  1.17it/s]Extractor Estimating: 492it [05:37,  1.25it/s]Extractor Estimating: 493it [05:37,  1.34it/s]Extractor Estimating: 494it [05:38,  1.38it/s]Extractor Estimating: 495it [05:39,  1.40it/s]Extractor Estimating: 496it [05:39,  1.44it/s]Extractor Estimating: 497it [05:40,  1.47it/s]Extractor Estimating: 498it [05:41,  1.51it/s]Extractor Estimating: 499it [05:41,  1.57it/s]Extractor Estimating: 500it [05:42,  1.65it/s]Extractor Estimating: 500it [05:42,  1.46it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 9861 mean pseudo reward: 0.9513717786838507
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl'}
train vocab size: 30654
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 30754, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=30754, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.317, loss:3144.3619
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.087, loss:2289.5502
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.051, loss:1738.1230
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.058, loss:1692.9834
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 89, avg_time 1.049, loss:1626.1523
>> valid entity prec:0.5360, rec:0.5418, f1:0.5389
>> valid relation prec:0.1893, rec:0.0077, f1:0.0148
>> valid relation with NER prec:0.1893, rec:0.0077, f1:0.0148
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 189, avg_time 2.511, loss:1555.4311
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 289, avg_time 1.068, loss:1492.5300
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 389, avg_time 1.047, loss:1390.2408
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 78, avg_time 1.050, loss:1338.4994
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 178, avg_time 1.046, loss:1256.5734
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5707, rec:0.4848, f1:0.5242
>> valid relation prec:0.2989, rec:0.0202, f1:0.0378
>> valid relation with NER prec:0.2989, rec:0.0202, f1:0.0378
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 278, avg_time 2.482, loss:1284.5645
g_step 1200, step 378, avg_time 1.033, loss:1208.2614
g_step 1300, step 67, avg_time 1.042, loss:1172.3369
g_step 1400, step 167, avg_time 1.032, loss:1112.0202
g_step 1500, step 267, avg_time 1.043, loss:1044.9574
>> valid entity prec:0.5548, rec:0.4520, f1:0.4981
>> valid relation prec:0.3104, rec:0.0250, f1:0.0463
>> valid relation with NER prec:0.3104, rec:0.0250, f1:0.0463
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 1600, step 367, avg_time 2.476, loss:1138.3742
g_step 1700, step 56, avg_time 1.047, loss:1084.3549
g_step 1800, step 156, avg_time 1.038, loss:1037.5445
g_step 1900, step 256, avg_time 1.044, loss:1041.1471
g_step 2000, step 356, avg_time 1.040, loss:1058.3959
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5811, rec:0.3562, f1:0.4417
>> valid relation prec:0.2440, rec:0.0197, f1:0.0365
>> valid relation with NER prec:0.2440, rec:0.0197, f1:0.0365
g_step 2100, step 45, avg_time 2.462, loss:1040.1472
g_step 2200, step 145, avg_time 1.042, loss:973.8806
g_step 2300, step 245, avg_time 1.046, loss:986.9964
g_step 2400, step 345, avg_time 1.035, loss:968.4261
g_step 2500, step 34, avg_time 1.045, loss:1022.3582
>> valid entity prec:0.6117, rec:0.4230, f1:0.5002
>> valid relation prec:0.3071, rec:0.0301, f1:0.0548
>> valid relation with NER prec:0.3071, rec:0.0301, f1:0.0548
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2600, step 134, avg_time 2.462, loss:941.5756
g_step 2700, step 234, avg_time 1.046, loss:941.3496
g_step 2800, step 334, avg_time 1.045, loss:987.5834
g_step 2900, step 23, avg_time 1.041, loss:920.7977
g_step 3000, step 123, avg_time 1.052, loss:907.1829
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6010, rec:0.4169, f1:0.4923
>> valid relation prec:0.2267, rec:0.0228, f1:0.0415
>> valid relation with NER prec:0.2267, rec:0.0228, f1:0.0415
g_step 3100, step 223, avg_time 2.469, loss:905.4323
g_step 3200, step 323, avg_time 1.034, loss:912.9753
g_step 3300, step 12, avg_time 1.034, loss:923.6543
g_step 3400, step 112, avg_time 1.037, loss:863.8346
g_step 3500, step 212, avg_time 1.048, loss:917.6059
>> valid entity prec:0.5934, rec:0.4877, f1:0.5354
>> valid relation prec:0.2257, rec:0.0257, f1:0.0462
>> valid relation with NER prec:0.2257, rec:0.0257, f1:0.0462
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 312, avg_time 2.482, loss:877.4685
g_step 3700, step 1, avg_time 1.039, loss:876.7230
g_step 3800, step 101, avg_time 1.039, loss:848.4897
g_step 3900, step 201, avg_time 1.037, loss:881.4373
g_step 4000, step 301, avg_time 1.048, loss:839.2663
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6154, rec:0.4157, f1:0.4962
>> valid relation prec:0.1758, rec:0.0139, f1:0.0258
>> valid relation with NER prec:0.1758, rec:0.0139, f1:0.0258
g_step 4100, step 401, avg_time 2.463, loss:875.1381
g_step 4200, step 90, avg_time 1.048, loss:808.5628
g_step 4300, step 190, avg_time 1.043, loss:833.9141
g_step 4400, step 290, avg_time 1.046, loss:837.6896
g_step 4500, step 390, avg_time 1.046, loss:796.3300
>> valid entity prec:0.6115, rec:0.3884, f1:0.4751
>> valid relation prec:0.1447, rec:0.0132, f1:0.0242
>> valid relation with NER prec:0.1447, rec:0.0132, f1:0.0242
g_step 4600, step 79, avg_time 2.460, loss:818.5467
g_step 4700, step 179, avg_time 1.042, loss:810.7960
g_step 4800, step 279, avg_time 1.044, loss:811.6810
g_step 4900, step 379, avg_time 1.042, loss:789.1014
g_step 5000, step 68, avg_time 1.052, loss:768.9510
learning rate was adjusted to 0.0008
>> valid entity prec:0.5606, rec:0.4862, f1:0.5208
>> valid relation prec:0.1638, rec:0.0250, f1:0.0434
>> valid relation with NER prec:0.1638, rec:0.0250, f1:0.0434
g_step 5100, step 168, avg_time 2.481, loss:753.6799
g_step 5200, step 268, avg_time 1.038, loss:782.0646
g_step 5300, step 368, avg_time 1.039, loss:811.9874
g_step 5400, step 57, avg_time 1.038, loss:727.6443
g_step 5500, step 157, avg_time 1.052, loss:744.0084
>> valid entity prec:0.5971, rec:0.4554, f1:0.5167
>> valid relation prec:0.2098, rec:0.0349, f1:0.0598
>> valid relation with NER prec:0.2098, rec:0.0349, f1:0.0598
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 257, avg_time 2.470, loss:732.5577
g_step 5700, step 357, avg_time 1.047, loss:776.7385
g_step 5800, step 46, avg_time 1.040, loss:735.4575
g_step 5900, step 146, avg_time 1.048, loss:701.6248
g_step 6000, step 246, avg_time 1.036, loss:735.8406
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5912, rec:0.4472, f1:0.5092
>> valid relation prec:0.1685, rec:0.0329, f1:0.0551
>> valid relation with NER prec:0.1685, rec:0.0329, f1:0.0551
g_step 6100, step 346, avg_time 2.477, loss:721.0674
g_step 6200, step 35, avg_time 1.039, loss:711.1177
g_step 6300, step 135, avg_time 1.048, loss:706.7676
g_step 6400, step 235, avg_time 1.042, loss:698.9773
g_step 6500, step 335, avg_time 1.037, loss:697.2933
>> valid entity prec:0.5655, rec:0.4802, f1:0.5194
>> valid relation prec:0.1674, rec:0.0262, f1:0.0453
>> valid relation with NER prec:0.1674, rec:0.0262, f1:0.0453
g_step 6600, step 24, avg_time 2.480, loss:717.9975
g_step 6700, step 124, avg_time 1.046, loss:663.0307
g_step 6800, step 224, avg_time 1.045, loss:684.2022
g_step 6900, step 324, avg_time 1.036, loss:676.6094
g_step 7000, step 13, avg_time 1.042, loss:695.9498
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5467, rec:0.4578, f1:0.4983
>> valid relation prec:0.1212, rec:0.0260, f1:0.0428
>> valid relation with NER prec:0.1212, rec:0.0260, f1:0.0428
g_step 7100, step 113, avg_time 2.481, loss:641.3882
g_step 7200, step 213, avg_time 1.041, loss:666.2796
g_step 7300, step 313, avg_time 1.047, loss:661.1877
g_step 7400, step 2, avg_time 1.044, loss:680.0881
g_step 7500, step 102, avg_time 1.050, loss:644.0199
>> valid entity prec:0.5566, rec:0.5111, f1:0.5329
>> valid relation prec:0.1494, rec:0.0226, f1:0.0393
>> valid relation with NER prec:0.1494, rec:0.0226, f1:0.0393
g_step 7600, step 202, avg_time 2.465, loss:614.9273
g_step 7700, step 302, avg_time 1.040, loss:656.5377
g_step 7800, step 402, avg_time 1.050, loss:649.5418
g_step 7900, step 91, avg_time 1.043, loss:625.5612
g_step 8000, step 191, avg_time 1.042, loss:590.9358
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5499, rec:0.4363, f1:0.4866
>> valid relation prec:0.1181, rec:0.0243, f1:0.0403
>> valid relation with NER prec:0.1181, rec:0.0243, f1:0.0403
g_step 8100, step 291, avg_time 2.481, loss:633.3217
g_step 8200, step 391, avg_time 1.040, loss:640.7211
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 07:29:39 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 07:29:39 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_07-29-39_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 07:29:40 - WARNING - datasets.builder -   Using custom data configuration default-c01cf34ff87d143f
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c01cf34ff87d143f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 07:29:41,027 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:29:41,028 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:29:41,029 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:29:41,030 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:29:41,040 >> Didn't find file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:29:41,043 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:29:41,043 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:29:41,043 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:29:41,043 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:29:41,043 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:29:41,043 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 07:29:41,179 >> loading weights file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:29:44,278 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 07:29:44,285 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki/unseen_15_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c01cf34ff87d143f/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki/unseen_15_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 07:29:44 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x155350d66a70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:03,  2.85ba/s] 18%|█▊        | 2/11 [00:00<00:02,  3.72ba/s] 27%|██▋       | 3/11 [00:00<00:01,  4.08ba/s] 36%|███▋      | 4/11 [00:00<00:01,  4.27ba/s] 45%|████▌     | 5/11 [00:01<00:01,  4.38ba/s] 55%|█████▍    | 6/11 [00:01<00:01,  4.46ba/s] 64%|██████▎   | 7/11 [00:01<00:01,  3.73ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  3.97ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.14ba/s] 91%|█████████ | 10/11 [00:02<00:00,  4.27ba/s]100%|██████████| 11/11 [00:02<00:00,  4.46ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.82ba/s] 40%|████      | 2/5 [00:00<00:00,  4.22ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.35ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.41ba/s]100%|██████████| 5/5 [00:00<00:00,  5.18ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:01,  7.77ba/s] 27%|██▋       | 3/11 [00:00<00:00,  9.50ba/s] 45%|████▌     | 5/11 [00:00<00:00, 10.19ba/s] 64%|██████▎   | 7/11 [00:00<00:00, 10.46ba/s] 82%|████████▏ | 9/11 [00:00<00:00, 10.50ba/s]100%|██████████| 11/11 [00:00<00:00, 12.54ba/s]100%|██████████| 11/11 [00:00<00:00, 11.21ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.28ba/s] 60%|██████    | 3/5 [00:00<00:00,  9.83ba/s]100%|██████████| 5/5 [00:00<00:00, 12.82ba/s]100%|██████████| 5/5 [00:00<00:00, 11.84ba/s]
[INFO|trainer.py:414] 2023-08-29 07:29:49,515 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 07:29:49,533 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 07:29:49,533 >>   Num examples = 10075
[INFO|trainer.py:1149] 2023-08-29 07:29:49,533 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 07:29:49,533 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 07:29:49,533 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 07:29:49,533 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 07:29:49,533 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<03:55,  3.32it/s]  0%|          | 2/785 [00:00<03:49,  3.42it/s]  0%|          | 3/785 [00:00<03:46,  3.45it/s]  1%|          | 4/785 [00:01<03:46,  3.45it/s]  1%|          | 5/785 [00:01<03:45,  3.46it/s]  1%|          | 6/785 [00:01<03:44,  3.47it/s]  1%|          | 7/785 [00:02<03:43,  3.48it/s]  1%|          | 8/785 [00:02<03:43,  3.48it/s]  1%|          | 9/785 [00:02<03:42,  3.48it/s]  1%|▏         | 10/785 [00:02<03:42,  3.48it/s]  1%|▏         | 11/785 [00:03<03:42,  3.49it/s]  2%|▏         | 12/785 [00:03<03:41,  3.49it/s]  2%|▏         | 13/785 [00:03<03:41,  3.49it/s]  2%|▏         | 14/785 [00:04<03:41,  3.48it/s]  2%|▏         | 15/785 [00:04<03:41,  3.47it/s]  2%|▏         | 16/785 [00:04<03:41,  3.48it/s]  2%|▏         | 17/785 [00:04<03:40,  3.48it/s]  2%|▏         | 18/785 [00:05<03:40,  3.48it/s]  2%|▏         | 19/785 [00:05<03:39,  3.48it/s]  3%|▎         | 20/785 [00:05<03:39,  3.49it/s]  3%|▎         | 21/785 [00:06<03:39,  3.49it/s]  3%|▎         | 22/785 [00:06<03:38,  3.49it/s]  3%|▎         | 23/785 [00:06<03:38,  3.49it/s]  3%|▎         | 24/785 [00:06<03:38,  3.49it/s]  3%|▎         | 25/785 [00:07<03:38,  3.49it/s]  3%|▎         | 26/785 [00:07<03:38,  3.48it/s]  3%|▎         | 27/785 [00:07<03:37,  3.48it/s]  4%|▎         | 28/785 [00:08<03:37,  3.48it/s]  4%|▎         | 29/785 [00:08<03:37,  3.48it/s]  4%|▍         | 30/785 [00:08<03:36,  3.48it/s]  4%|▍         | 31/785 [00:08<03:36,  3.49it/s]  4%|▍         | 32/785 [00:09<03:35,  3.49it/s]  4%|▍         | 33/785 [00:09<03:35,  3.49it/s]  4%|▍         | 34/785 [00:09<03:35,  3.49it/s]  4%|▍         | 35/785 [00:10<03:35,  3.49it/s]  5%|▍         | 36/785 [00:10<03:34,  3.49it/s]  5%|▍         | 37/785 [00:10<03:34,  3.48it/s]  5%|▍         | 38/785 [00:10<03:34,  3.48it/s]  5%|▍         | 39/785 [00:11<03:34,  3.48it/s]  5%|▌         | 40/785 [00:11<03:33,  3.48it/s]  5%|▌         | 41/785 [00:11<03:33,  3.48it/s]  5%|▌         | 42/785 [00:12<03:33,  3.48it/s]  5%|▌         | 43/785 [00:12<03:32,  3.48it/s]  6%|▌         | 44/785 [00:12<03:32,  3.48it/s]  6%|▌         | 45/785 [00:12<03:32,  3.49it/s]  6%|▌         | 46/785 [00:13<03:32,  3.48it/s]  6%|▌         | 47/785 [00:13<03:31,  3.48it/s]  6%|▌         | 48/785 [00:13<03:32,  3.47it/s]  6%|▌         | 49/785 [00:14<03:31,  3.48it/s]  6%|▋         | 50/785 [00:14<03:31,  3.48it/s]  6%|▋         | 51/785 [00:14<03:30,  3.48it/s]  7%|▋         | 52/785 [00:14<03:30,  3.48it/s]  7%|▋         | 53/785 [00:15<03:30,  3.48it/s]  7%|▋         | 54/785 [00:15<03:29,  3.48it/s]  7%|▋         | 55/785 [00:15<03:29,  3.48it/s]  7%|▋         | 56/785 [00:16<03:29,  3.48it/s]  7%|▋         | 57/785 [00:16<03:29,  3.47it/s]  7%|▋         | 58/785 [00:16<03:29,  3.48it/s]  8%|▊         | 59/785 [00:16<03:29,  3.46it/s]  8%|▊         | 60/785 [00:17<03:29,  3.47it/s]  8%|▊         | 61/785 [00:17<03:28,  3.47it/s]  8%|▊         | 62/785 [00:17<03:28,  3.47it/s]  8%|▊         | 63/785 [00:18<03:27,  3.47it/s]  8%|▊         | 64/785 [00:18<03:27,  3.48it/s]  8%|▊         | 65/785 [00:18<03:27,  3.48it/s]  8%|▊         | 66/785 [00:18<03:26,  3.48it/s]  9%|▊         | 67/785 [00:19<03:26,  3.48it/s]  9%|▊         | 68/785 [00:19<03:26,  3.48it/s]  9%|▉         | 69/785 [00:19<03:25,  3.48it/s]  9%|▉         | 70/785 [00:20<03:26,  3.47it/s]  9%|▉         | 71/785 [00:20<03:25,  3.47it/s]  9%|▉         | 72/785 [00:20<03:25,  3.47it/s]  9%|▉         | 73/785 [00:20<03:24,  3.47it/s]  9%|▉         | 74/785 [00:21<03:24,  3.47it/s] 10%|▉         | 75/785 [00:21<03:24,  3.47it/s] 10%|▉         | 76/785 [00:21<03:24,  3.48it/s] 10%|▉         | 77/785 [00:22<03:23,  3.47it/s] 10%|▉         | 78/785 [00:22<03:23,  3.47it/s] 10%|█         | 79/785 [00:22<03:23,  3.48it/s] 10%|█         | 80/785 [00:23<03:22,  3.48it/s] 10%|█         | 81/785 [00:23<03:22,  3.48it/s] 10%|█         | 82/785 [00:23<03:22,  3.48it/s] 11%|█         | 83/785 [00:23<03:21,  3.48it/s] 11%|█         | 84/785 [00:24<03:21,  3.48it/s] 11%|█         | 85/785 [00:24<03:21,  3.47it/s] 11%|█         | 86/785 [00:24<03:21,  3.46it/s] 11%|█         | 87/785 [00:25<03:21,  3.47it/s] 11%|█         | 88/785 [00:25<03:20,  3.47it/s] 11%|█▏        | 89/785 [00:25<03:20,  3.47it/s] 11%|█▏        | 90/785 [00:25<03:20,  3.47it/s] 12%|█▏        | 91/785 [00:26<03:19,  3.48it/s] 12%|█▏        | 92/785 [00:26<03:19,  3.47it/s] 12%|█▏        | 93/785 [00:26<03:19,  3.48it/s] 12%|█▏        | 94/785 [00:27<03:18,  3.48it/s] 12%|█▏        | 95/785 [00:27<03:18,  3.48it/s] 12%|█▏        | 96/785 [00:27<03:18,  3.48it/s] 12%|█▏        | 97/785 [00:27<03:17,  3.48it/s] 12%|█▏        | 98/785 [00:28<03:17,  3.48it/s] 13%|█▎        | 99/785 [00:28<03:17,  3.47it/s] 13%|█▎        | 100/785 [00:28<03:17,  3.47it/s] 13%|█▎        | 101/785 [00:29<03:16,  3.48it/s] 13%|█▎        | 102/785 [00:29<03:16,  3.48it/s] 13%|█▎        | 103/785 [00:29<03:16,  3.47it/s] 13%|█▎        | 104/785 [00:29<03:16,  3.47it/s] 13%|█▎        | 105/785 [00:30<03:15,  3.47it/s] 14%|█▎        | 106/785 [00:30<03:15,  3.47it/s] 14%|█▎        | 107/785 [00:30<03:15,  3.47it/s] 14%|█▍        | 108/785 [00:31<03:14,  3.47it/s] 14%|█▍        | 109/785 [00:31<03:14,  3.48it/s] 14%|█▍        | 110/785 [00:31<03:14,  3.47it/s] 14%|█▍        | 111/785 [00:31<03:13,  3.47it/s] 14%|█▍        | 112/785 [00:32<03:13,  3.47it/s] 14%|█▍        | 113/785 [00:32<03:13,  3.48it/s] 15%|█▍        | 114/785 [00:32<03:13,  3.48it/s] 15%|█▍        | 115/785 [00:33<03:12,  3.47it/s] 15%|█▍        | 116/785 [00:33<03:12,  3.47it/s] 15%|█▍        | 117/785 [00:33<03:12,  3.47it/s] 15%|█▌        | 118/785 [00:33<03:11,  3.47it/s] 15%|█▌        | 119/785 [00:34<03:11,  3.48it/s] 15%|█▌        | 120/785 [00:34<03:11,  3.46it/s] 15%|█▌        | 121/785 [00:34<03:11,  3.47it/s] 16%|█▌        | 122/785 [00:35<03:11,  3.47it/s] 16%|█▌        | 123/785 [00:35<03:10,  3.47it/s] 16%|█▌        | 124/785 [00:35<03:10,  3.47it/s] 16%|█▌        | 125/785 [00:35<03:10,  3.47it/s] 16%|█▌        | 126/785 [00:36<03:09,  3.47it/s] 16%|█▌        | 127/785 [00:36<03:09,  3.47it/s] 16%|█▋        | 128/785 [00:36<03:09,  3.47it/s] 16%|█▋        | 129/785 [00:37<03:08,  3.47it/s] 17%|█▋        | 130/785 [00:37<03:08,  3.47it/s] 17%|█▋        | 131/785 [00:37<03:08,  3.48it/s] 17%|█▋        | 132/785 [00:37<03:07,  3.48it/s] 17%|█▋        | 133/785 [00:38<03:07,  3.48it/s] 17%|█▋        | 134/785 [00:38<03:07,  3.48it/s] 17%|█▋        | 135/785 [00:38<03:06,  3.48it/s] 17%|█▋        | 136/785 [00:39<03:06,  3.48it/s] 17%|█▋        | 137/785 [00:39<03:06,  3.48it/s] 18%|█▊        | 138/785 [00:39<03:06,  3.47it/s] 18%|█▊        | 139/785 [00:39<03:06,  3.47it/s] 18%|█▊        | 140/785 [00:40<03:05,  3.47it/s] 18%|█▊        | 141/785 [00:40<03:05,  3.47it/s] 18%|█▊        | 142/785 [00:40<03:05,  3.47it/s] 18%|█▊        | 143/785 [00:41<03:04,  3.47it/s] 18%|█▊        | 144/785 [00:41<03:04,  3.47it/s] 18%|█▊        | 145/785 [00:41<03:04,  3.47it/s] 19%|█▊        | 146/785 [00:42<03:03,  3.47it/s] 19%|█▊        | 147/785 [00:42<03:03,  3.47it/s] 19%|█▉        | 148/785 [00:42<03:03,  3.47it/s] 19%|█▉        | 149/785 [00:42<03:03,  3.47it/s] 19%|█▉        | 150/785 [00:43<03:02,  3.47it/s] 19%|█▉        | 151/785 [00:43<03:02,  3.47it/s] 19%|█▉        | 152/785 [00:43<03:02,  3.47it/s] 19%|█▉        | 153/785 [00:44<03:01,  3.47it/s] 20%|█▉        | 154/785 [00:44<03:01,  3.47it/s] 20%|█▉        | 155/785 [00:44<03:01,  3.47it/s] 20%|█▉        | 156/785 [00:44<03:01,  3.46it/s] 20%|██        | 157/785 [00:45<03:01,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 07:30:34,827 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:30:34,827 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 07:30:34,827 >>   Batch size = 8

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:09, 57.15it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.72it/s][A
  3%|▎         | 18/521 [00:00<00:10, 48.95it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.16it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.84it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.63it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.29it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.09it/s][A
  9%|▉         | 48/521 [00:01<00:10, 47.04it/s][A
 10%|█         | 53/521 [00:01<00:09, 46.95it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.98it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.91it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.97it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.98it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 47.03it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.92it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.86it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.86it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.84it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.92it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.86it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.86it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.93it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.94it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.90it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.90it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.92it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.83it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.92it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.93it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.91it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.96it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.93it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.87it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.90it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.89it/s][A
 36%|███▌      | 188/521 [00:03<00:07, 46.74it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.82it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.80it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.84it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.90it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.98it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.86it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.80it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.86it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 44.32it/s][A
 46%|████▌     | 239/521 [00:05<00:06, 46.22it/s][A
 47%|████▋     | 244/521 [00:05<00:05, 46.49it/s][A
 48%|████▊     | 249/521 [00:05<00:05, 46.64it/s][A
 49%|████▉     | 254/521 [00:05<00:05, 46.75it/s][A
 50%|████▉     | 259/521 [00:05<00:05, 46.82it/s][A
 51%|█████     | 264/521 [00:05<00:05, 46.85it/s][A
 52%|█████▏    | 269/521 [00:05<00:05, 46.79it/s][A
 53%|█████▎    | 274/521 [00:05<00:05, 46.85it/s][A
 54%|█████▎    | 279/521 [00:05<00:05, 46.68it/s][A
 55%|█████▍    | 284/521 [00:06<00:05, 46.77it/s][A
 55%|█████▌    | 289/521 [00:06<00:04, 46.91it/s][A
 56%|█████▋    | 294/521 [00:06<00:04, 46.89it/s][A
 57%|█████▋    | 299/521 [00:06<00:04, 46.93it/s][A
 58%|█████▊    | 304/521 [00:06<00:04, 46.93it/s][A
 59%|█████▉    | 309/521 [00:06<00:04, 46.89it/s][A
 60%|██████    | 314/521 [00:06<00:04, 46.91it/s][A
 61%|██████    | 319/521 [00:06<00:04, 46.79it/s][A
 62%|██████▏   | 324/521 [00:06<00:04, 46.75it/s][A
 63%|██████▎   | 329/521 [00:07<00:04, 46.68it/s][A
 64%|██████▍   | 334/521 [00:07<00:03, 46.76it/s][A
 65%|██████▌   | 339/521 [00:07<00:03, 46.77it/s][A
 66%|██████▌   | 344/521 [00:07<00:03, 46.78it/s][A
 67%|██████▋   | 349/521 [00:07<00:03, 46.88it/s][A
 68%|██████▊   | 354/521 [00:07<00:03, 46.82it/s][A
 69%|██████▉   | 359/521 [00:07<00:03, 46.87it/s][A
 70%|██████▉   | 364/521 [00:07<00:03, 46.75it/s][A
 71%|███████   | 369/521 [00:07<00:03, 46.70it/s][A
 72%|███████▏  | 374/521 [00:07<00:03, 46.81it/s][A
 73%|███████▎  | 379/521 [00:08<00:03, 46.74it/s][A
 74%|███████▎  | 384/521 [00:08<00:02, 46.74it/s][A
 75%|███████▍  | 389/521 [00:08<00:02, 46.84it/s][A
 76%|███████▌  | 394/521 [00:08<00:02, 46.83it/s][A
 77%|███████▋  | 399/521 [00:08<00:02, 46.82it/s][A
 78%|███████▊  | 404/521 [00:08<00:02, 46.84it/s][A
 79%|███████▊  | 409/521 [00:08<00:02, 46.71it/s][A
 79%|███████▉  | 414/521 [00:08<00:02, 46.67it/s][A
 80%|████████  | 419/521 [00:08<00:02, 46.83it/s][A
 81%|████████▏ | 424/521 [00:09<00:02, 46.88it/s][A
 82%|████████▏ | 429/521 [00:09<00:01, 46.77it/s][A
 83%|████████▎ | 434/521 [00:09<00:01, 46.81it/s][A
 84%|████████▍ | 439/521 [00:09<00:01, 46.87it/s][A
 85%|████████▌ | 444/521 [00:09<00:01, 46.81it/s][A
 86%|████████▌ | 449/521 [00:09<00:01, 46.82it/s][A
 87%|████████▋ | 454/521 [00:09<00:01, 46.71it/s][A
 88%|████████▊ | 459/521 [00:09<00:01, 46.75it/s][A
 89%|████████▉ | 464/521 [00:09<00:01, 46.72it/s][A
 90%|█████████ | 469/521 [00:09<00:01, 46.79it/s][A
 91%|█████████ | 474/521 [00:10<00:01, 46.79it/s][A
 92%|█████████▏| 479/521 [00:10<00:00, 46.78it/s][A
 93%|█████████▎| 484/521 [00:10<00:00, 46.81it/s][A
 94%|█████████▍| 489/521 [00:10<00:00, 46.81it/s][A
 95%|█████████▍| 494/521 [00:10<00:00, 46.74it/s][A
 96%|█████████▌| 499/521 [00:10<00:00, 46.75it/s][A
 97%|█████████▋| 504/521 [00:10<00:00, 46.81it/s][A
 98%|█████████▊| 509/521 [00:10<00:00, 46.86it/s][A
 99%|█████████▊| 514/521 [00:10<00:00, 46.71it/s][A
100%|█████████▉| 519/521 [00:11<00:00, 46.78it/s][A                                                 
                                                 [A 20%|██        | 157/785 [00:56<03:01,  3.46it/s]
100%|██████████| 521/521 [00:11<00:00, 46.78it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:30:45,986 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-29 07:30:46,003 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:30:48,027 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:30:48,042 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:30:48,053 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:03<59:18,  5.68s/it] 20%|██        | 159/785 [01:03<42:21,  4.06s/it] 20%|██        | 160/785 [01:03<30:30,  2.93s/it] 21%|██        | 161/785 [01:04<22:12,  2.14s/it] 21%|██        | 162/785 [01:04<16:25,  1.58s/it] 21%|██        | 163/785 [01:04<12:22,  1.19s/it] 21%|██        | 164/785 [01:05<09:32,  1.09it/s] 21%|██        | 165/785 [01:05<07:33,  1.37it/s] 21%|██        | 166/785 [01:05<06:10,  1.67it/s] 21%|██▏       | 167/785 [01:06<05:12,  1.98it/s] 21%|██▏       | 168/785 [01:06<04:31,  2.27it/s] 22%|██▏       | 169/785 [01:06<04:02,  2.54it/s] 22%|██▏       | 170/785 [01:06<03:43,  2.75it/s] 22%|██▏       | 171/785 [01:07<03:29,  2.94it/s] 22%|██▏       | 172/785 [01:07<03:19,  3.08it/s] 22%|██▏       | 173/785 [01:07<03:11,  3.19it/s] 22%|██▏       | 174/785 [01:08<03:06,  3.27it/s] 22%|██▏       | 175/785 [01:08<03:03,  3.33it/s] 22%|██▏       | 176/785 [01:08<03:00,  3.37it/s] 23%|██▎       | 177/785 [01:08<02:58,  3.40it/s] 23%|██▎       | 178/785 [01:09<02:57,  3.43it/s] 23%|██▎       | 179/785 [01:09<02:56,  3.44it/s] 23%|██▎       | 180/785 [01:09<02:55,  3.45it/s] 23%|██▎       | 181/785 [01:10<02:54,  3.45it/s] 23%|██▎       | 182/785 [01:10<02:54,  3.46it/s] 23%|██▎       | 183/785 [01:10<02:53,  3.47it/s] 23%|██▎       | 184/785 [01:10<02:53,  3.47it/s] 24%|██▎       | 185/785 [01:11<02:53,  3.47it/s] 24%|██▎       | 186/785 [01:11<02:52,  3.47it/s] 24%|██▍       | 187/785 [01:11<02:52,  3.47it/s] 24%|██▍       | 188/785 [01:12<02:51,  3.47it/s] 24%|██▍       | 189/785 [01:12<02:51,  3.47it/s] 24%|██▍       | 190/785 [01:12<02:51,  3.47it/s] 24%|██▍       | 191/785 [01:12<02:50,  3.48it/s] 24%|██▍       | 192/785 [01:13<02:51,  3.46it/s] 25%|██▍       | 193/785 [01:13<02:50,  3.47it/s] 25%|██▍       | 194/785 [01:13<02:50,  3.47it/s] 25%|██▍       | 195/785 [01:14<02:49,  3.47it/s] 25%|██▍       | 196/785 [01:14<02:49,  3.47it/s] 25%|██▌       | 197/785 [01:14<02:49,  3.47it/s] 25%|██▌       | 198/785 [01:14<02:48,  3.48it/s] 25%|██▌       | 199/785 [01:15<02:48,  3.47it/s] 25%|██▌       | 200/785 [01:15<02:48,  3.47it/s] 26%|██▌       | 201/785 [01:15<02:48,  3.48it/s] 26%|██▌       | 202/785 [01:16<02:47,  3.47it/s] 26%|██▌       | 203/785 [01:16<02:48,  3.46it/s] 26%|██▌       | 204/785 [01:16<02:47,  3.46it/s] 26%|██▌       | 205/785 [01:16<02:47,  3.47it/s] 26%|██▌       | 206/785 [01:17<02:46,  3.47it/s] 26%|██▋       | 207/785 [01:17<02:46,  3.47it/s] 26%|██▋       | 208/785 [01:17<02:46,  3.47it/s] 27%|██▋       | 209/785 [01:18<02:45,  3.47it/s] 27%|██▋       | 210/785 [01:18<02:45,  3.47it/s] 27%|██▋       | 211/785 [01:18<02:45,  3.47it/s] 27%|██▋       | 212/785 [01:18<02:44,  3.47it/s] 27%|██▋       | 213/785 [01:19<02:44,  3.47it/s] 27%|██▋       | 214/785 [01:19<02:44,  3.46it/s] 27%|██▋       | 215/785 [01:19<02:44,  3.46it/s] 28%|██▊       | 216/785 [01:20<02:44,  3.47it/s] 28%|██▊       | 217/785 [01:20<02:43,  3.47it/s] 28%|██▊       | 218/785 [01:20<02:43,  3.47it/s] 28%|██▊       | 219/785 [01:20<02:43,  3.47it/s] 28%|██▊       | 220/785 [01:21<02:42,  3.47it/s] 28%|██▊       | 221/785 [01:21<02:42,  3.47it/s] 28%|██▊       | 222/785 [01:21<02:42,  3.47it/s] 28%|██▊       | 223/785 [01:22<02:41,  3.47it/s] 29%|██▊       | 224/785 [01:22<02:41,  3.47it/s] 29%|██▊       | 225/785 [01:22<02:41,  3.46it/s] 29%|██▉       | 226/785 [01:23<02:41,  3.46it/s] 29%|██▉       | 227/785 [01:23<02:41,  3.46it/s] 29%|██▉       | 228/785 [01:23<02:40,  3.46it/s] 29%|██▉       | 229/785 [01:23<02:40,  3.47it/s] 29%|██▉       | 230/785 [01:24<02:40,  3.47it/s] 29%|██▉       | 231/785 [01:24<02:39,  3.47it/s] 30%|██▉       | 232/785 [01:24<02:39,  3.47it/s] 30%|██▉       | 233/785 [01:25<02:39,  3.47it/s] 30%|██▉       | 234/785 [01:25<02:38,  3.47it/s] 30%|██▉       | 235/785 [01:25<02:38,  3.47it/s] 30%|███       | 236/785 [01:25<02:38,  3.46it/s] 30%|███       | 237/785 [01:26<02:38,  3.46it/s] 30%|███       | 238/785 [01:26<02:37,  3.47it/s] 30%|███       | 239/785 [01:26<02:37,  3.47it/s] 31%|███       | 240/785 [01:27<02:37,  3.47it/s] 31%|███       | 241/785 [01:27<02:36,  3.47it/s] 31%|███       | 242/785 [01:27<02:36,  3.47it/s] 31%|███       | 243/785 [01:27<02:36,  3.47it/s] 31%|███       | 244/785 [01:28<02:35,  3.47it/s] 31%|███       | 245/785 [01:28<02:35,  3.47it/s] 31%|███▏      | 246/785 [01:28<02:35,  3.47it/s] 31%|███▏      | 247/785 [01:29<02:35,  3.47it/s] 32%|███▏      | 248/785 [01:29<02:34,  3.47it/s] 32%|███▏      | 249/785 [01:29<02:34,  3.47it/s] 32%|███▏      | 250/785 [01:29<02:34,  3.47it/s] 32%|███▏      | 251/785 [01:30<02:33,  3.47it/s] 32%|███▏      | 252/785 [01:30<02:33,  3.47it/s] 32%|███▏      | 253/785 [01:30<02:33,  3.46it/s] 32%|███▏      | 254/785 [01:31<02:33,  3.46it/s] 32%|███▏      | 255/785 [01:31<02:32,  3.47it/s] 33%|███▎      | 256/785 [01:31<02:32,  3.47it/s] 33%|███▎      | 257/785 [01:31<02:32,  3.47it/s] 33%|███▎      | 258/785 [01:32<02:31,  3.47it/s] 33%|███▎      | 259/785 [01:32<02:31,  3.47it/s] 33%|███▎      | 260/785 [01:32<02:31,  3.47it/s] 33%|███▎      | 261/785 [01:33<02:31,  3.47it/s] 33%|███▎      | 262/785 [01:33<02:30,  3.47it/s] 34%|███▎      | 263/785 [01:33<02:30,  3.47it/s] 34%|███▎      | 264/785 [01:33<02:31,  3.45it/s] 34%|███▍      | 265/785 [01:34<02:30,  3.46it/s] 34%|███▍      | 266/785 [01:34<02:29,  3.46it/s] 34%|███▍      | 267/785 [01:34<02:29,  3.46it/s] 34%|███▍      | 268/785 [01:35<02:29,  3.47it/s] 34%|███▍      | 269/785 [01:35<02:28,  3.47it/s] 34%|███▍      | 270/785 [01:35<02:28,  3.47it/s] 35%|███▍      | 271/785 [01:35<02:28,  3.47it/s] 35%|███▍      | 272/785 [01:36<02:27,  3.47it/s] 35%|███▍      | 273/785 [01:36<02:27,  3.47it/s] 35%|███▍      | 274/785 [01:36<02:27,  3.47it/s] 35%|███▌      | 275/785 [01:37<02:27,  3.45it/s] 35%|███▌      | 276/785 [01:37<02:27,  3.46it/s] 35%|███▌      | 277/785 [01:37<02:26,  3.46it/s] 35%|███▌      | 278/785 [01:38<02:26,  3.47it/s] 36%|███▌      | 279/785 [01:38<02:25,  3.47it/s] 36%|███▌      | 280/785 [01:38<02:25,  3.46it/s] 36%|███▌      | 281/785 [01:38<02:25,  3.47it/s] 36%|███▌      | 282/785 [01:39<02:25,  3.47it/s] 36%|███▌      | 283/785 [01:39<02:24,  3.47it/s] 36%|███▌      | 284/785 [01:39<02:24,  3.47it/s] 36%|███▋      | 285/785 [01:40<02:24,  3.47it/s] 36%|███▋      | 286/785 [01:40<02:24,  3.46it/s] 37%|███▋      | 287/785 [01:40<02:23,  3.46it/s] 37%|███▋      | 288/785 [01:40<02:23,  3.46it/s] 37%|███▋      | 289/785 [01:41<02:23,  3.47it/s] 37%|███▋      | 290/785 [01:41<02:22,  3.47it/s] 37%|███▋      | 291/785 [01:41<02:22,  3.47it/s] 37%|███▋      | 292/785 [01:42<02:22,  3.47it/s] 37%|███▋      | 293/785 [01:42<02:21,  3.47it/s] 37%|███▋      | 294/785 [01:42<02:21,  3.47it/s] 38%|███▊      | 295/785 [01:42<02:21,  3.47it/s] 38%|███▊      | 296/785 [01:43<02:20,  3.47it/s] 38%|███▊      | 297/785 [01:43<02:21,  3.46it/s] 38%|███▊      | 298/785 [01:43<02:20,  3.46it/s] 38%|███▊      | 299/785 [01:44<02:20,  3.47it/s] 38%|███▊      | 300/785 [01:44<02:19,  3.47it/s] 38%|███▊      | 301/785 [01:44<02:19,  3.47it/s] 38%|███▊      | 302/785 [01:44<02:19,  3.47it/s] 39%|███▊      | 303/785 [01:45<02:18,  3.47it/s] 39%|███▊      | 304/785 [01:45<02:18,  3.47it/s] 39%|███▉      | 305/785 [01:45<02:18,  3.47it/s] 39%|███▉      | 306/785 [01:46<02:18,  3.47it/s] 39%|███▉      | 307/785 [01:46<02:17,  3.47it/s] 39%|███▉      | 308/785 [01:46<02:18,  3.46it/s] 39%|███▉      | 309/785 [01:46<02:17,  3.46it/s] 39%|███▉      | 310/785 [01:47<02:17,  3.46it/s] 40%|███▉      | 311/785 [01:47<02:16,  3.46it/s] 40%|███▉      | 312/785 [01:47<02:16,  3.47it/s] 40%|███▉      | 313/785 [01:48<02:16,  3.47it/s] 40%|████      | 314/785 [01:48<02:15,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 07:31:38,041 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:31:38,041 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 07:31:38,041 >>   Batch size = 8
{'eval_loss': 0.9623410105705261, 'eval_runtime': 11.1269, 'eval_samples_per_second': 374.318, 'eval_steps_per_second': 46.823, 'epoch': 1.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.66it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.71it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.00it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.39it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.86it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.54it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.10it/s][A
  8%|▊         | 43/521 [00:00<00:10, 46.94it/s][A
  9%|▉         | 48/521 [00:01<00:10, 46.89it/s][A
 10%|█         | 53/521 [00:01<00:09, 46.89it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.98it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.94it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.92it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.85it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.99it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 44.61it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.09it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.36it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.59it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.74it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.78it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.86it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.90it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.83it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.75it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.66it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.78it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.79it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.88it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.89it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.90it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.90it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.91it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.81it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.58it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.60it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 46.70it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.81it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.78it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.85it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.84it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.82it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.72it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.77it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.78it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 46.84it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.73it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.87it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.89it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.88it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.94it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.82it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.71it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.77it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.78it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.84it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.74it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.88it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.84it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.89it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 46.83it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.76it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.80it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 46.88it/s][A
 63%|██████▎   | 328/521 [00:06<00:04, 46.81it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.70it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.77it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.83it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.87it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.83it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.79it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.79it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.83it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 46.91it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.82it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.74it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.68it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.80it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.76it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.84it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.83it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.78it/s][A
 80%|████████  | 418/521 [00:08<00:02, 46.83it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.78it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.83it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.76it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.73it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 46.75it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 46.83it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 46.82it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.73it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 46.80it/s][A
 90%|████████▉ | 468/521 [00:09<00:01, 46.81it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.85it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.89it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.80it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.71it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.75it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.78it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.83it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.85it/s][A
 98%|█████████▊| 513/521 [00:10<00:00, 46.84it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.78it/s][A                                                 
                                                 [A 40%|████      | 314/785 [01:59<02:15,  3.47it/s]
100%|██████████| 521/521 [00:11<00:00, 46.78it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:31:49,196 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-29 07:31:49,215 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:31:51,461 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:31:51,481 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:31:51,491 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:07<46:21,  5.92s/it] 40%|████      | 316/785 [02:07<33:03,  4.23s/it] 40%|████      | 317/785 [02:08<23:46,  3.05s/it] 41%|████      | 318/785 [02:08<17:16,  2.22s/it] 41%|████      | 319/785 [02:08<12:44,  1.64s/it] 41%|████      | 320/785 [02:08<09:33,  1.23s/it] 41%|████      | 321/785 [02:09<07:20,  1.05it/s] 41%|████      | 322/785 [02:09<05:47,  1.33it/s] 41%|████      | 323/785 [02:09<04:42,  1.63it/s] 41%|████▏     | 324/785 [02:10<03:57,  1.94it/s] 41%|████▏     | 325/785 [02:10<03:25,  2.24it/s] 42%|████▏     | 326/785 [02:10<03:03,  2.51it/s] 42%|████▏     | 327/785 [02:10<02:47,  2.73it/s] 42%|████▏     | 328/785 [02:11<02:36,  2.92it/s] 42%|████▏     | 329/785 [02:11<02:28,  3.06it/s] 42%|████▏     | 330/785 [02:11<02:23,  3.18it/s] 42%|████▏     | 331/785 [02:12<02:19,  3.26it/s] 42%|████▏     | 332/785 [02:12<02:16,  3.32it/s] 42%|████▏     | 333/785 [02:12<02:14,  3.37it/s] 43%|████▎     | 334/785 [02:12<02:12,  3.40it/s] 43%|████▎     | 335/785 [02:13<02:11,  3.42it/s] 43%|████▎     | 336/785 [02:13<02:10,  3.44it/s] 43%|████▎     | 337/785 [02:13<02:09,  3.45it/s] 43%|████▎     | 338/785 [02:14<02:09,  3.45it/s] 43%|████▎     | 339/785 [02:14<02:09,  3.46it/s] 43%|████▎     | 340/785 [02:14<02:08,  3.46it/s] 43%|████▎     | 341/785 [02:14<02:08,  3.46it/s] 44%|████▎     | 342/785 [02:15<02:07,  3.47it/s] 44%|████▎     | 343/785 [02:15<02:07,  3.47it/s] 44%|████▍     | 344/785 [02:15<02:07,  3.47it/s] 44%|████▍     | 345/785 [02:16<02:06,  3.47it/s] 44%|████▍     | 346/785 [02:16<02:06,  3.47it/s] 44%|████▍     | 347/785 [02:16<02:06,  3.47it/s] 44%|████▍     | 348/785 [02:16<02:05,  3.47it/s] 44%|████▍     | 349/785 [02:17<02:05,  3.46it/s] 45%|████▍     | 350/785 [02:17<02:05,  3.47it/s] 45%|████▍     | 351/785 [02:17<02:05,  3.47it/s] 45%|████▍     | 352/785 [02:18<02:04,  3.47it/s] 45%|████▍     | 353/785 [02:18<02:04,  3.47it/s] 45%|████▌     | 354/785 [02:18<02:04,  3.47it/s] 45%|████▌     | 355/785 [02:18<02:03,  3.47it/s] 45%|████▌     | 356/785 [02:19<02:03,  3.47it/s] 45%|████▌     | 357/785 [02:19<02:03,  3.47it/s] 46%|████▌     | 358/785 [02:19<02:02,  3.47it/s] 46%|████▌     | 359/785 [02:20<02:02,  3.47it/s] 46%|████▌     | 360/785 [02:20<02:02,  3.46it/s] 46%|████▌     | 361/785 [02:20<02:02,  3.47it/s] 46%|████▌     | 362/785 [02:20<02:02,  3.47it/s] 46%|████▌     | 363/785 [02:21<02:01,  3.47it/s] 46%|████▋     | 364/785 [02:21<02:01,  3.47it/s] 46%|████▋     | 365/785 [02:21<02:00,  3.47it/s] 47%|████▋     | 366/785 [02:22<02:00,  3.47it/s] 47%|████▋     | 367/785 [02:22<02:00,  3.47it/s] 47%|████▋     | 368/785 [02:22<02:00,  3.47it/s] 47%|████▋     | 369/785 [02:23<01:59,  3.47it/s] 47%|████▋     | 370/785 [02:23<01:59,  3.48it/s] 47%|████▋     | 371/785 [02:23<01:59,  3.46it/s] 47%|████▋     | 372/785 [02:23<01:59,  3.47it/s] 48%|████▊     | 373/785 [02:24<01:58,  3.47it/s] 48%|████▊     | 374/785 [02:24<01:58,  3.47it/s] 48%|████▊     | 375/785 [02:24<01:58,  3.47it/s] 48%|████▊     | 376/785 [02:25<01:57,  3.47it/s] 48%|████▊     | 377/785 [02:25<01:57,  3.47it/s] 48%|████▊     | 378/785 [02:25<01:57,  3.47it/s] 48%|████▊     | 379/785 [02:25<01:56,  3.47it/s] 48%|████▊     | 380/785 [02:26<01:56,  3.47it/s] 49%|████▊     | 381/785 [02:26<01:56,  3.47it/s] 49%|████▊     | 382/785 [02:26<01:56,  3.46it/s] 49%|████▉     | 383/785 [02:27<01:56,  3.46it/s] 49%|████▉     | 384/785 [02:27<01:55,  3.46it/s] 49%|████▉     | 385/785 [02:27<01:55,  3.47it/s] 49%|████▉     | 386/785 [02:27<01:55,  3.47it/s] 49%|████▉     | 387/785 [02:28<01:54,  3.47it/s] 49%|████▉     | 388/785 [02:28<01:54,  3.47it/s] 50%|████▉     | 389/785 [02:28<01:54,  3.47it/s] 50%|████▉     | 390/785 [02:29<01:53,  3.47it/s] 50%|████▉     | 391/785 [02:29<01:53,  3.47it/s] 50%|████▉     | 392/785 [02:29<01:53,  3.47it/s] 50%|█████     | 393/785 [02:29<01:52,  3.47it/s] 50%|█████     | 394/785 [02:30<01:52,  3.47it/s] 50%|█████     | 395/785 [02:30<01:52,  3.47it/s] 50%|█████     | 396/785 [02:30<01:51,  3.47it/s] 51%|█████     | 397/785 [02:31<01:51,  3.47it/s] 51%|█████     | 398/785 [02:31<01:51,  3.47it/s] 51%|█████     | 399/785 [02:31<01:51,  3.47it/s] 51%|█████     | 400/785 [02:31<01:50,  3.47it/s] 51%|█████     | 401/785 [02:32<01:50,  3.47it/s] 51%|█████     | 402/785 [02:32<01:50,  3.46it/s] 51%|█████▏    | 403/785 [02:32<01:50,  3.46it/s] 51%|█████▏    | 404/785 [02:33<01:49,  3.47it/s] 52%|█████▏    | 405/785 [02:33<01:49,  3.47it/s] 52%|█████▏    | 406/785 [02:33<01:49,  3.47it/s] 52%|█████▏    | 407/785 [02:33<01:48,  3.47it/s] 52%|█████▏    | 408/785 [02:34<01:48,  3.47it/s] 52%|█████▏    | 409/785 [02:34<01:48,  3.47it/s] 52%|█████▏    | 410/785 [02:34<01:48,  3.47it/s] 52%|█████▏    | 411/785 [02:35<01:47,  3.47it/s] 52%|█████▏    | 412/785 [02:35<01:47,  3.47it/s] 53%|█████▎    | 413/785 [02:35<01:47,  3.46it/s] 53%|█████▎    | 414/785 [02:35<01:47,  3.46it/s] 53%|█████▎    | 415/785 [02:36<01:46,  3.46it/s] 53%|█████▎    | 416/785 [02:36<01:46,  3.47it/s] 53%|█████▎    | 417/785 [02:36<01:46,  3.47it/s] 53%|█████▎    | 418/785 [02:37<01:45,  3.47it/s] 53%|█████▎    | 419/785 [02:37<01:45,  3.47it/s] 54%|█████▎    | 420/785 [02:37<01:45,  3.47it/s] 54%|█████▎    | 421/785 [02:37<01:44,  3.47it/s] 54%|█████▍    | 422/785 [02:38<01:44,  3.47it/s] 54%|█████▍    | 423/785 [02:38<01:44,  3.47it/s] 54%|█████▍    | 424/785 [02:38<01:44,  3.46it/s] 54%|█████▍    | 425/785 [02:39<01:43,  3.46it/s] 54%|█████▍    | 426/785 [02:39<01:43,  3.46it/s] 54%|█████▍    | 427/785 [02:39<01:43,  3.47it/s] 55%|█████▍    | 428/785 [02:40<01:42,  3.47it/s] 55%|█████▍    | 429/785 [02:40<01:42,  3.47it/s] 55%|█████▍    | 430/785 [02:40<01:42,  3.47it/s] 55%|█████▍    | 431/785 [02:40<01:42,  3.47it/s] 55%|█████▌    | 432/785 [02:41<01:41,  3.47it/s] 55%|█████▌    | 433/785 [02:41<01:41,  3.47it/s] 55%|█████▌    | 434/785 [02:41<01:41,  3.47it/s] 55%|█████▌    | 435/785 [02:42<01:42,  3.41it/s] 56%|█████▌    | 436/785 [02:42<01:41,  3.42it/s] 56%|█████▌    | 437/785 [02:42<01:41,  3.44it/s] 56%|█████▌    | 438/785 [02:42<01:40,  3.45it/s] 56%|█████▌    | 439/785 [02:43<01:40,  3.45it/s] 56%|█████▌    | 440/785 [02:43<01:39,  3.46it/s] 56%|█████▌    | 441/785 [02:43<01:39,  3.46it/s] 56%|█████▋    | 442/785 [02:44<01:38,  3.47it/s] 56%|█████▋    | 443/785 [02:44<01:38,  3.47it/s] 57%|█████▋    | 444/785 [02:44<01:38,  3.47it/s] 57%|█████▋    | 445/785 [02:44<01:37,  3.47it/s] 57%|█████▋    | 446/785 [02:45<01:38,  3.46it/s] 57%|█████▋    | 447/785 [02:45<01:37,  3.46it/s] 57%|█████▋    | 448/785 [02:45<01:37,  3.46it/s] 57%|█████▋    | 449/785 [02:46<01:36,  3.46it/s] 57%|█████▋    | 450/785 [02:46<01:36,  3.47it/s] 57%|█████▋    | 451/785 [02:46<01:36,  3.47it/s] 58%|█████▊    | 452/785 [02:46<01:36,  3.47it/s] 58%|█████▊    | 453/785 [02:47<01:35,  3.47it/s] 58%|█████▊    | 454/785 [02:47<01:35,  3.47it/s] 58%|█████▊    | 455/785 [02:47<01:35,  3.47it/s] 58%|█████▊    | 456/785 [02:48<01:34,  3.47it/s] 58%|█████▊    | 457/785 [02:48<01:34,  3.46it/s] 58%|█████▊    | 458/785 [02:48<01:34,  3.46it/s] 58%|█████▊    | 459/785 [02:48<01:34,  3.46it/s] 59%|█████▊    | 460/785 [02:49<01:33,  3.46it/s] 59%|█████▊    | 461/785 [02:49<01:33,  3.46it/s] 59%|█████▉    | 462/785 [02:49<01:33,  3.46it/s] 59%|█████▉    | 463/785 [02:50<01:32,  3.46it/s] 59%|█████▉    | 464/785 [02:50<01:34,  3.39it/s] 59%|█████▉    | 465/785 [02:50<01:33,  3.40it/s] 59%|█████▉    | 466/785 [02:51<01:33,  3.42it/s] 59%|█████▉    | 467/785 [02:51<01:32,  3.44it/s] 60%|█████▉    | 468/785 [02:51<01:32,  3.44it/s] 60%|█████▉    | 469/785 [02:51<01:31,  3.45it/s] 60%|█████▉    | 470/785 [02:52<01:31,  3.45it/s] 60%|██████    | 471/785 [02:52<01:30,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 07:32:42,112 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:32:42,113 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 07:32:42,113 >>   Batch size = 8
{'eval_loss': 0.9559677243232727, 'eval_runtime': 11.1327, 'eval_samples_per_second': 374.122, 'eval_steps_per_second': 46.799, 'epoch': 2.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:09, 57.06it/s][A
  2%|▏         | 12/521 [00:00<00:09, 50.97it/s][A
  3%|▎         | 18/521 [00:00<00:10, 48.96it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.31it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.85it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.52it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.21it/s][A
  8%|▊         | 43/521 [00:00<00:10, 46.95it/s][A
  9%|▉         | 48/521 [00:01<00:10, 46.87it/s][A
 10%|█         | 53/521 [00:01<00:09, 46.93it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.97it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 47.00it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.94it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 47.02it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.98it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.89it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.77it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.74it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.79it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.80it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.83it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.93it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.95it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.92it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.88it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.74it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.74it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.81it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.86it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.86it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.85it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.87it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.90it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.90it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.72it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.78it/s][A
 36%|███▌      | 188/521 [00:03<00:07, 46.82it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.75it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.84it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.73it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.85it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.91it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.88it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.73it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.81it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 46.83it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.77it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.85it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.85it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.69it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.80it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.85it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.77it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.78it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.81it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.72it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.74it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.80it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.82it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.74it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 46.82it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.86it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.80it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 46.81it/s][A
 63%|██████▎   | 328/521 [00:06<00:04, 46.78it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.82it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.76it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.73it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.82it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.80it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.81it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.82it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.80it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 46.64it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.66it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.65it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.72it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.73it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.79it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.81it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.85it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.73it/s][A
 80%|████████  | 418/521 [00:08<00:02, 46.77it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.83it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.75it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.84it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.77it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 46.77it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 46.83it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 46.82it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.81it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 46.82it/s][A
 90%|████████▉ | 468/521 [00:09<00:01, 46.81it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.83it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.79it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.81it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.77it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.78it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.79it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.61it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.68it/s][A
 98%|█████████▊| 513/521 [00:10<00:00, 46.67it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.74it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [03:03<01:30,  3.46it/s]
100%|██████████| 521/521 [00:11<00:00, 46.74it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:32:53,269 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-29 07:32:53,290 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:32:55,484 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:32:55,500 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:32:55,511 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:10<29:51,  5.72s/it] 60%|██████    | 473/785 [03:11<21:17,  4.09s/it] 60%|██████    | 474/785 [03:11<15:18,  2.95s/it] 61%|██████    | 475/785 [03:11<11:07,  2.15s/it] 61%|██████    | 476/785 [03:12<08:12,  1.59s/it] 61%|██████    | 477/785 [03:12<06:10,  1.20s/it] 61%|██████    | 478/785 [03:12<04:44,  1.08it/s] 61%|██████    | 479/785 [03:12<03:45,  1.36it/s] 61%|██████    | 480/785 [03:13<03:03,  1.66it/s] 61%|██████▏   | 481/785 [03:13<02:34,  1.97it/s] 61%|██████▏   | 482/785 [03:13<02:13,  2.27it/s] 62%|██████▏   | 483/785 [03:14<01:59,  2.53it/s] 62%|██████▏   | 484/785 [03:14<01:49,  2.75it/s] 62%|██████▏   | 485/785 [03:14<01:42,  2.93it/s] 62%|██████▏   | 486/785 [03:14<01:37,  3.08it/s] 62%|██████▏   | 487/785 [03:15<01:33,  3.19it/s] 62%|██████▏   | 488/785 [03:15<01:30,  3.27it/s] 62%|██████▏   | 489/785 [03:15<01:28,  3.33it/s] 62%|██████▏   | 490/785 [03:16<01:27,  3.37it/s] 63%|██████▎   | 491/785 [03:16<01:26,  3.40it/s] 63%|██████▎   | 492/785 [03:16<01:25,  3.42it/s] 63%|██████▎   | 493/785 [03:16<01:24,  3.44it/s] 63%|██████▎   | 494/785 [03:17<01:24,  3.44it/s] 63%|██████▎   | 495/785 [03:17<01:24,  3.44it/s] 63%|██████▎   | 496/785 [03:17<01:23,  3.45it/s] 63%|██████▎   | 497/785 [03:18<01:23,  3.46it/s] 63%|██████▎   | 498/785 [03:18<01:22,  3.46it/s] 64%|██████▎   | 499/785 [03:18<01:22,  3.46it/s] 64%|██████▎   | 500/785 [03:18<01:22,  3.47it/s]                                                  64%|██████▎   | 500/785 [03:18<01:22,  3.47it/s] 64%|██████▍   | 501/785 [03:19<01:21,  3.47it/s] 64%|██████▍   | 502/785 [03:19<01:21,  3.47it/s] 64%|██████▍   | 503/785 [03:19<01:21,  3.47it/s] 64%|██████▍   | 504/785 [03:20<01:20,  3.47it/s] 64%|██████▍   | 505/785 [03:20<01:20,  3.47it/s] 64%|██████▍   | 506/785 [03:20<01:20,  3.47it/s] 65%|██████▍   | 507/785 [03:20<01:20,  3.47it/s] 65%|██████▍   | 508/785 [03:21<01:19,  3.47it/s] 65%|██████▍   | 509/785 [03:21<01:19,  3.47it/s] 65%|██████▍   | 510/785 [03:21<01:19,  3.47it/s] 65%|██████▌   | 511/785 [03:22<01:18,  3.47it/s] 65%|██████▌   | 512/785 [03:22<01:18,  3.47it/s] 65%|██████▌   | 513/785 [03:22<01:18,  3.47it/s] 65%|██████▌   | 514/785 [03:22<01:18,  3.47it/s] 66%|██████▌   | 515/785 [03:23<01:17,  3.47it/s] 66%|██████▌   | 516/785 [03:23<01:17,  3.47it/s] 66%|██████▌   | 517/785 [03:23<01:17,  3.47it/s] 66%|██████▌   | 518/785 [03:24<01:16,  3.47it/s] 66%|██████▌   | 519/785 [03:24<01:16,  3.47it/s] 66%|██████▌   | 520/785 [03:24<01:16,  3.47it/s] 66%|██████▋   | 521/785 [03:24<01:16,  3.47it/s] 66%|██████▋   | 522/785 [03:25<01:15,  3.47it/s] 67%|██████▋   | 523/785 [03:25<01:15,  3.47it/s] 67%|██████▋   | 524/785 [03:25<01:15,  3.47it/s] 67%|██████▋   | 525/785 [03:26<01:14,  3.47it/s] 67%|██████▋   | 526/785 [03:26<01:14,  3.47it/s] 67%|██████▋   | 527/785 [03:26<01:14,  3.47it/s] 67%|██████▋   | 528/785 [03:27<01:14,  3.47it/s] 67%|██████▋   | 529/785 [03:27<01:13,  3.47it/s] 68%|██████▊   | 530/785 [03:27<01:13,  3.47it/s] 68%|██████▊   | 531/785 [03:27<01:13,  3.47it/s] 68%|██████▊   | 532/785 [03:28<01:12,  3.47it/s] 68%|██████▊   | 533/785 [03:28<01:12,  3.47it/s] 68%|██████▊   | 534/785 [03:28<01:12,  3.47it/s] 68%|██████▊   | 535/785 [03:29<01:12,  3.47it/s] 68%|██████▊   | 536/785 [03:29<01:11,  3.47it/s] 68%|██████▊   | 537/785 [03:29<01:11,  3.47it/s] 69%|██████▊   | 538/785 [03:29<01:11,  3.47it/s] 69%|██████▊   | 539/785 [03:30<01:11,  3.46it/s] 69%|██████▉   | 540/785 [03:30<01:10,  3.46it/s] 69%|██████▉   | 541/785 [03:30<01:10,  3.46it/s] 69%|██████▉   | 542/785 [03:31<01:10,  3.46it/s] 69%|██████▉   | 543/785 [03:31<01:09,  3.46it/s] 69%|██████▉   | 544/785 [03:31<01:09,  3.47it/s] 69%|██████▉   | 545/785 [03:31<01:09,  3.47it/s] 70%|██████▉   | 546/785 [03:32<01:08,  3.47it/s] 70%|██████▉   | 547/785 [03:32<01:08,  3.47it/s] 70%|██████▉   | 548/785 [03:32<01:08,  3.47it/s] 70%|██████▉   | 549/785 [03:33<01:08,  3.47it/s] 70%|███████   | 550/785 [03:33<01:07,  3.47it/s] 70%|███████   | 551/785 [03:33<01:07,  3.47it/s] 70%|███████   | 552/785 [03:33<01:07,  3.46it/s] 70%|███████   | 553/785 [03:34<01:06,  3.46it/s] 71%|███████   | 554/785 [03:34<01:06,  3.46it/s] 71%|███████   | 555/785 [03:34<01:06,  3.47it/s] 71%|███████   | 556/785 [03:35<01:06,  3.47it/s] 71%|███████   | 557/785 [03:35<01:05,  3.47it/s] 71%|███████   | 558/785 [03:35<01:05,  3.47it/s] 71%|███████   | 559/785 [03:35<01:05,  3.47it/s] 71%|███████▏  | 560/785 [03:36<01:04,  3.47it/s] 71%|███████▏  | 561/785 [03:36<01:04,  3.47it/s] 72%|███████▏  | 562/785 [03:36<01:04,  3.47it/s] 72%|███████▏  | 563/785 [03:37<01:04,  3.46it/s] 72%|███████▏  | 564/785 [03:37<01:03,  3.46it/s] 72%|███████▏  | 565/785 [03:37<01:03,  3.46it/s] 72%|███████▏  | 566/785 [03:37<01:03,  3.47it/s] 72%|███████▏  | 567/785 [03:38<01:02,  3.47it/s] 72%|███████▏  | 568/785 [03:38<01:02,  3.47it/s] 72%|███████▏  | 569/785 [03:38<01:02,  3.47it/s] 73%|███████▎  | 570/785 [03:39<01:01,  3.47it/s] 73%|███████▎  | 571/785 [03:39<01:01,  3.47it/s] 73%|███████▎  | 572/785 [03:39<01:01,  3.47it/s] 73%|███████▎  | 573/785 [03:39<01:01,  3.47it/s] 73%|███████▎  | 574/785 [03:40<01:00,  3.46it/s] 73%|███████▎  | 575/785 [03:40<01:00,  3.46it/s] 73%|███████▎  | 576/785 [03:40<01:00,  3.47it/s] 74%|███████▎  | 577/785 [03:41<00:59,  3.47it/s] 74%|███████▎  | 578/785 [03:41<00:59,  3.47it/s] 74%|███████▍  | 579/785 [03:41<00:59,  3.47it/s] 74%|███████▍  | 580/785 [03:41<00:59,  3.47it/s] 74%|███████▍  | 581/785 [03:42<00:58,  3.47it/s] 74%|███████▍  | 582/785 [03:42<00:58,  3.47it/s] 74%|███████▍  | 583/785 [03:42<00:58,  3.47it/s] 74%|███████▍  | 584/785 [03:43<00:57,  3.47it/s] 75%|███████▍  | 585/785 [03:43<00:57,  3.45it/s] 75%|███████▍  | 586/785 [03:43<00:57,  3.46it/s] 75%|███████▍  | 587/785 [03:44<00:57,  3.46it/s] 75%|███████▍  | 588/785 [03:44<00:56,  3.47it/s] 75%|███████▌  | 589/785 [03:44<00:56,  3.47it/s] 75%|███████▌  | 590/785 [03:44<00:56,  3.47it/s] 75%|███████▌  | 591/785 [03:45<00:55,  3.47it/s] 75%|███████▌  | 592/785 [03:45<00:55,  3.47it/s] 76%|███████▌  | 593/785 [03:45<00:55,  3.47it/s] 76%|███████▌  | 594/785 [03:46<00:55,  3.47it/s] 76%|███████▌  | 595/785 [03:46<00:54,  3.47it/s] 76%|███████▌  | 596/785 [03:46<00:54,  3.46it/s] 76%|███████▌  | 597/785 [03:46<00:54,  3.46it/s] 76%|███████▌  | 598/785 [03:47<00:53,  3.46it/s] 76%|███████▋  | 599/785 [03:47<00:53,  3.46it/s] 76%|███████▋  | 600/785 [03:47<00:53,  3.47it/s] 77%|███████▋  | 601/785 [03:48<00:53,  3.47it/s] 77%|███████▋  | 602/785 [03:48<00:52,  3.47it/s] 77%|███████▋  | 603/785 [03:48<00:52,  3.47it/s] 77%|███████▋  | 604/785 [03:48<00:52,  3.47it/s] 77%|███████▋  | 605/785 [03:49<00:51,  3.46it/s] 77%|███████▋  | 606/785 [03:49<00:51,  3.46it/s] 77%|███████▋  | 607/785 [03:49<00:51,  3.45it/s] 77%|███████▋  | 608/785 [03:50<00:51,  3.46it/s] 78%|███████▊  | 609/785 [03:50<00:50,  3.46it/s] 78%|███████▊  | 610/785 [03:50<00:51,  3.40it/s] 78%|███████▊  | 611/785 [03:50<00:50,  3.42it/s] 78%|███████▊  | 612/785 [03:51<00:50,  3.43it/s] 78%|███████▊  | 613/785 [03:51<00:49,  3.44it/s] 78%|███████▊  | 614/785 [03:51<00:49,  3.45it/s] 78%|███████▊  | 615/785 [03:52<00:49,  3.46it/s] 78%|███████▊  | 616/785 [03:52<00:48,  3.46it/s] 79%|███████▊  | 617/785 [03:52<00:48,  3.46it/s] 79%|███████▊  | 618/785 [03:52<00:48,  3.46it/s] 79%|███████▉  | 619/785 [03:53<00:47,  3.46it/s] 79%|███████▉  | 620/785 [03:53<00:47,  3.46it/s] 79%|███████▉  | 621/785 [03:53<00:47,  3.47it/s] 79%|███████▉  | 622/785 [03:54<00:47,  3.47it/s] 79%|███████▉  | 623/785 [03:54<00:46,  3.47it/s] 79%|███████▉  | 624/785 [03:54<00:46,  3.47it/s] 80%|███████▉  | 625/785 [03:54<00:46,  3.47it/s] 80%|███████▉  | 626/785 [03:55<00:45,  3.47it/s] 80%|███████▉  | 627/785 [03:55<00:45,  3.47it/s] 80%|████████  | 628/785 [03:55<00:45,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 07:33:45,518 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:33:45,519 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 07:33:45,519 >>   Batch size = 8
{'eval_loss': 0.9601996541023254, 'eval_runtime': 11.126, 'eval_samples_per_second': 374.349, 'eval_steps_per_second': 46.827, 'epoch': 3.0}
{'loss': 0.7484, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.75it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.66it/s][A
  3%|▎         | 18/521 [00:00<00:10, 48.95it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.32it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.72it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.52it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.16it/s][A
  8%|▊         | 43/521 [00:00<00:10, 46.91it/s][A
  9%|▉         | 48/521 [00:01<00:10, 46.96it/s][A
 10%|█         | 53/521 [00:01<00:09, 46.97it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.77it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.84it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.88it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.72it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.90it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.74it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.65it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.67it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.68it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.75it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.81it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.74it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.77it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.78it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.70it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.71it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.67it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.67it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.80it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.70it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.76it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.72it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.74it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.77it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.77it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.70it/s][A
 36%|███▌      | 188/521 [00:03<00:07, 46.75it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.84it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.62it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.72it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.72it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.69it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.76it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.77it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.61it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 46.70it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.77it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.68it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.75it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.81it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.74it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.72it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.74it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.75it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.77it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.74it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.73it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.74it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.70it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.73it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 46.73it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.69it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.68it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 46.76it/s][A
 63%|██████▎   | 328/521 [00:06<00:04, 46.73it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.76it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.72it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.79it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.73it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.61it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.76it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.65it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.73it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 46.79it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.69it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.75it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.86it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.66it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.66it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.71it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.70it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.75it/s][A
 80%|████████  | 418/521 [00:08<00:02, 46.68it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.63it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.80it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.69it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.73it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 46.78it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 46.60it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 46.69it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.80it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 46.67it/s][A
 90%|████████▉ | 468/521 [00:09<00:01, 46.68it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.74it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.73it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.75it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.78it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.73it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.66it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.76it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.70it/s][A
 98%|█████████▊| 513/521 [00:10<00:00, 46.74it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.75it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:07<00:45,  3.47it/s]
100%|██████████| 521/521 [00:11<00:00, 46.75it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:33:56,680 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-29 07:33:56,701 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:33:58,982 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:33:58,999 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:33:59,008 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:14<14:48,  5.70s/it] 80%|████████  | 630/785 [04:14<10:31,  4.08s/it] 80%|████████  | 631/785 [04:14<07:32,  2.94s/it] 81%|████████  | 632/785 [04:15<05:28,  2.14s/it] 81%|████████  | 633/785 [04:15<04:01,  1.59s/it] 81%|████████  | 634/785 [04:15<03:00,  1.20s/it] 81%|████████  | 635/785 [04:15<02:18,  1.08it/s] 81%|████████  | 636/785 [04:16<01:49,  1.36it/s] 81%|████████  | 637/785 [04:16<01:28,  1.67it/s] 81%|████████▏ | 638/785 [04:16<01:14,  1.98it/s] 81%|████████▏ | 639/785 [04:17<01:04,  2.27it/s] 82%|████████▏ | 640/785 [04:17<00:57,  2.53it/s] 82%|████████▏ | 641/785 [04:17<00:52,  2.75it/s] 82%|████████▏ | 642/785 [04:17<00:48,  2.93it/s] 82%|████████▏ | 643/785 [04:18<00:46,  3.07it/s] 82%|████████▏ | 644/785 [04:18<00:44,  3.18it/s] 82%|████████▏ | 645/785 [04:18<00:42,  3.27it/s] 82%|████████▏ | 646/785 [04:19<00:41,  3.33it/s] 82%|████████▏ | 647/785 [04:19<00:40,  3.37it/s] 83%|████████▎ | 648/785 [04:19<00:40,  3.40it/s] 83%|████████▎ | 649/785 [04:19<00:39,  3.42it/s] 83%|████████▎ | 650/785 [04:20<00:39,  3.44it/s] 83%|████████▎ | 651/785 [04:20<00:38,  3.45it/s] 83%|████████▎ | 652/785 [04:20<00:38,  3.45it/s] 83%|████████▎ | 653/785 [04:21<00:38,  3.46it/s] 83%|████████▎ | 654/785 [04:21<00:37,  3.46it/s] 83%|████████▎ | 655/785 [04:21<00:37,  3.46it/s] 84%|████████▎ | 656/785 [04:21<00:37,  3.47it/s] 84%|████████▎ | 657/785 [04:22<00:36,  3.47it/s] 84%|████████▍ | 658/785 [04:22<00:36,  3.47it/s] 84%|████████▍ | 659/785 [04:22<00:36,  3.47it/s] 84%|████████▍ | 660/785 [04:23<00:36,  3.47it/s] 84%|████████▍ | 661/785 [04:23<00:35,  3.47it/s] 84%|████████▍ | 662/785 [04:23<00:35,  3.47it/s] 84%|████████▍ | 663/785 [04:23<00:35,  3.46it/s] 85%|████████▍ | 664/785 [04:24<00:34,  3.47it/s] 85%|████████▍ | 665/785 [04:24<00:34,  3.47it/s] 85%|████████▍ | 666/785 [04:24<00:34,  3.47it/s] 85%|████████▍ | 667/785 [04:25<00:34,  3.47it/s] 85%|████████▌ | 668/785 [04:25<00:33,  3.47it/s] 85%|████████▌ | 669/785 [04:25<00:33,  3.47it/s] 85%|████████▌ | 670/785 [04:25<00:33,  3.47it/s] 85%|████████▌ | 671/785 [04:26<00:32,  3.47it/s] 86%|████████▌ | 672/785 [04:26<00:32,  3.47it/s] 86%|████████▌ | 673/785 [04:26<00:32,  3.47it/s] 86%|████████▌ | 674/785 [04:27<00:32,  3.46it/s] 86%|████████▌ | 675/785 [04:27<00:31,  3.47it/s] 86%|████████▌ | 676/785 [04:27<00:31,  3.47it/s] 86%|████████▌ | 677/785 [04:28<00:31,  3.47it/s] 86%|████████▋ | 678/785 [04:28<00:30,  3.47it/s] 86%|████████▋ | 679/785 [04:28<00:30,  3.47it/s] 87%|████████▋ | 680/785 [04:28<00:30,  3.47it/s] 87%|████████▋ | 681/785 [04:29<00:29,  3.47it/s] 87%|████████▋ | 682/785 [04:29<00:29,  3.47it/s] 87%|████████▋ | 683/785 [04:29<00:29,  3.47it/s] 87%|████████▋ | 684/785 [04:30<00:29,  3.47it/s] 87%|████████▋ | 685/785 [04:30<00:28,  3.46it/s] 87%|████████▋ | 686/785 [04:30<00:28,  3.46it/s] 88%|████████▊ | 687/785 [04:30<00:28,  3.47it/s] 88%|████████▊ | 688/785 [04:31<00:27,  3.47it/s] 88%|████████▊ | 689/785 [04:31<00:27,  3.47it/s] 88%|████████▊ | 690/785 [04:31<00:27,  3.47it/s] 88%|████████▊ | 691/785 [04:32<00:27,  3.47it/s] 88%|████████▊ | 692/785 [04:32<00:26,  3.47it/s] 88%|████████▊ | 693/785 [04:32<00:26,  3.47it/s] 88%|████████▊ | 694/785 [04:32<00:26,  3.47it/s] 89%|████████▊ | 695/785 [04:33<00:25,  3.47it/s] 89%|████████▊ | 696/785 [04:33<00:25,  3.47it/s] 89%|████████▉ | 697/785 [04:33<00:25,  3.47it/s] 89%|████████▉ | 698/785 [04:34<00:25,  3.47it/s] 89%|████████▉ | 699/785 [04:34<00:24,  3.47it/s] 89%|████████▉ | 700/785 [04:34<00:24,  3.47it/s] 89%|████████▉ | 701/785 [04:34<00:24,  3.47it/s] 89%|████████▉ | 702/785 [04:35<00:24,  3.45it/s] 90%|████████▉ | 703/785 [04:35<00:23,  3.45it/s] 90%|████████▉ | 704/785 [04:35<00:23,  3.46it/s] 90%|████████▉ | 705/785 [04:36<00:23,  3.46it/s] 90%|████████▉ | 706/785 [04:36<00:22,  3.46it/s] 90%|█████████ | 707/785 [04:36<00:22,  3.47it/s] 90%|█████████ | 708/785 [04:36<00:22,  3.47it/s] 90%|█████████ | 709/785 [04:37<00:21,  3.47it/s] 90%|█████████ | 710/785 [04:37<00:21,  3.47it/s] 91%|█████████ | 711/785 [04:37<00:21,  3.47it/s] 91%|█████████ | 712/785 [04:38<00:21,  3.47it/s] 91%|█████████ | 713/785 [04:38<00:20,  3.46it/s] 91%|█████████ | 714/785 [04:38<00:20,  3.47it/s] 91%|█████████ | 715/785 [04:38<00:20,  3.47it/s] 91%|█████████ | 716/785 [04:39<00:19,  3.47it/s] 91%|█████████▏| 717/785 [04:39<00:19,  3.47it/s] 91%|█████████▏| 718/785 [04:39<00:19,  3.47it/s] 92%|█████████▏| 719/785 [04:40<00:19,  3.47it/s] 92%|█████████▏| 720/785 [04:40<00:18,  3.47it/s] 92%|█████████▏| 721/785 [04:40<00:18,  3.47it/s] 92%|█████████▏| 722/785 [04:40<00:18,  3.47it/s] 92%|█████████▏| 723/785 [04:41<00:17,  3.47it/s] 92%|█████████▏| 724/785 [04:41<00:17,  3.46it/s] 92%|█████████▏| 725/785 [04:41<00:17,  3.46it/s] 92%|█████████▏| 726/785 [04:42<00:17,  3.47it/s] 93%|█████████▎| 727/785 [04:42<00:16,  3.47it/s] 93%|█████████▎| 728/785 [04:42<00:16,  3.47it/s] 93%|█████████▎| 729/785 [04:43<00:16,  3.47it/s] 93%|█████████▎| 730/785 [04:43<00:15,  3.47it/s] 93%|█████████▎| 731/785 [04:43<00:15,  3.47it/s] 93%|█████████▎| 732/785 [04:43<00:15,  3.47it/s] 93%|█████████▎| 733/785 [04:44<00:14,  3.47it/s] 94%|█████████▎| 734/785 [04:44<00:14,  3.47it/s] 94%|█████████▎| 735/785 [04:44<00:14,  3.47it/s] 94%|█████████▍| 736/785 [04:45<00:14,  3.47it/s] 94%|█████████▍| 737/785 [04:45<00:13,  3.47it/s] 94%|█████████▍| 738/785 [04:45<00:13,  3.47it/s] 94%|█████████▍| 739/785 [04:45<00:13,  3.47it/s] 94%|█████████▍| 740/785 [04:46<00:12,  3.47it/s] 94%|█████████▍| 741/785 [04:46<00:12,  3.47it/s] 95%|█████████▍| 742/785 [04:46<00:12,  3.47it/s] 95%|█████████▍| 743/785 [04:47<00:12,  3.47it/s] 95%|█████████▍| 744/785 [04:47<00:11,  3.47it/s] 95%|█████████▍| 745/785 [04:47<00:11,  3.47it/s] 95%|█████████▌| 746/785 [04:47<00:11,  3.46it/s] 95%|█████████▌| 747/785 [04:48<00:10,  3.47it/s] 95%|█████████▌| 748/785 [04:48<00:10,  3.47it/s] 95%|█████████▌| 749/785 [04:48<00:10,  3.47it/s] 96%|█████████▌| 750/785 [04:49<00:10,  3.47it/s] 96%|█████████▌| 751/785 [04:49<00:09,  3.47it/s] 96%|█████████▌| 752/785 [04:49<00:09,  3.47it/s] 96%|█████████▌| 753/785 [04:49<00:09,  3.47it/s] 96%|█████████▌| 754/785 [04:50<00:08,  3.47it/s] 96%|█████████▌| 755/785 [04:50<00:08,  3.47it/s] 96%|█████████▋| 756/785 [04:50<00:08,  3.40it/s] 96%|█████████▋| 757/785 [04:51<00:08,  3.41it/s] 97%|█████████▋| 758/785 [04:51<00:07,  3.43it/s] 97%|█████████▋| 759/785 [04:51<00:07,  3.44it/s] 97%|█████████▋| 760/785 [04:51<00:07,  3.45it/s] 97%|█████████▋| 761/785 [04:52<00:06,  3.46it/s] 97%|█████████▋| 762/785 [04:52<00:06,  3.46it/s] 97%|█████████▋| 763/785 [04:52<00:06,  3.46it/s] 97%|█████████▋| 764/785 [04:53<00:06,  3.47it/s] 97%|█████████▋| 765/785 [04:53<00:05,  3.47it/s] 98%|█████████▊| 766/785 [04:53<00:05,  3.47it/s] 98%|█████████▊| 767/785 [04:53<00:05,  3.47it/s] 98%|█████████▊| 768/785 [04:54<00:04,  3.46it/s] 98%|█████████▊| 769/785 [04:54<00:04,  3.47it/s] 98%|█████████▊| 770/785 [04:54<00:04,  3.47it/s] 98%|█████████▊| 771/785 [04:55<00:04,  3.47it/s] 98%|█████████▊| 772/785 [04:55<00:03,  3.47it/s] 98%|█████████▊| 773/785 [04:55<00:03,  3.47it/s] 99%|█████████▊| 774/785 [04:56<00:03,  3.47it/s] 99%|█████████▊| 775/785 [04:56<00:02,  3.47it/s] 99%|█████████▉| 776/785 [04:56<00:02,  3.47it/s] 99%|█████████▉| 777/785 [04:56<00:02,  3.47it/s] 99%|█████████▉| 778/785 [04:57<00:02,  3.47it/s] 99%|█████████▉| 779/785 [04:57<00:01,  3.46it/s] 99%|█████████▉| 780/785 [04:57<00:01,  3.46it/s] 99%|█████████▉| 781/785 [04:58<00:01,  3.47it/s]100%|█████████▉| 782/785 [04:58<00:00,  3.47it/s]100%|█████████▉| 783/785 [04:58<00:00,  3.47it/s]100%|█████████▉| 784/785 [04:58<00:00,  3.47it/s]100%|██████████| 785/785 [04:59<00:00,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 07:34:48,716 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:34:48,716 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 07:34:48,716 >>   Batch size = 8
{'eval_loss': 0.9637444615364075, 'eval_runtime': 11.142, 'eval_samples_per_second': 373.811, 'eval_steps_per_second': 46.76, 'epoch': 4.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.64it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.74it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.01it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.26it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.83it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.51it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.19it/s][A
  8%|▊         | 43/521 [00:00<00:10, 46.97it/s][A
  9%|▉         | 48/521 [00:01<00:10, 46.99it/s][A
 10%|█         | 53/521 [00:01<00:09, 46.97it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.90it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.91it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.90it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.90it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.91it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.79it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.58it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.72it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.78it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.78it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.77it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.81it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.79it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.77it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.77it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.74it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.67it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.73it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.73it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.74it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.75it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.85it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.74it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.67it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.73it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.73it/s][A
 36%|███▌      | 188/521 [00:03<00:07, 46.71it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.82it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.65it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.75it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.83it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.78it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.74it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.68it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.69it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 46.72it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.73it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.69it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.74it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.78it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.70it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.80it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.75it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.61it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.75it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.71it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.73it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.83it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.68it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.72it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 46.73it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.74it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.79it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 46.71it/s][A
 63%|██████▎   | 328/521 [00:06<00:04, 46.69it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.81it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.69it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.68it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.77it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.69it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.71it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.77it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.68it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 46.71it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.78it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.65it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.68it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.72it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.63it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.73it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.65it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.69it/s][A
 80%|████████  | 418/521 [00:08<00:02, 46.78it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.69it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.69it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.80it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.72it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 46.74it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 46.75it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 46.68it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.70it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 46.79it/s][A
 90%|████████▉ | 468/521 [00:09<00:01, 46.77it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.79it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.75it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.69it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.71it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.74it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.70it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.66it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.79it/s][A
 98%|█████████▊| 513/521 [00:10<00:00, 46.70it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.76it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:10<00:00,  3.47it/s]
100%|██████████| 521/521 [00:11<00:00, 46.76it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 07:34:59,876 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-29 07:34:59,890 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:35:02,184 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:35:02,195 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:35:02,201 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 07:35:06,770 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 07:35:06,774 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314 (score: 0.9559677243232727).
                                                 100%|██████████| 785/785 [05:18<00:00,  3.47it/s]100%|██████████| 785/785 [05:18<00:00,  2.46it/s]
[INFO|trainer.py:1894] 2023-08-29 07:35:08,512 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 07:35:08,543 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 07:35:10,772 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 07:35:10,791 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 07:35:10,802 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:35:11,003 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:11,003 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:11,003 >>   train_loss               =     0.7346
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:11,003 >>   train_runtime            = 0:05:18.97
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:11,003 >>   train_samples            =      10075
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:11,003 >>   train_samples_per_second =    157.928
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:11,003 >>   train_steps_per_second   =      2.461
{'eval_loss': 0.9656649231910706, 'eval_runtime': 11.1398, 'eval_samples_per_second': 373.884, 'eval_steps_per_second': 46.769, 'epoch': 5.0}
{'train_runtime': 318.9749, 'train_samples_per_second': 157.928, 'train_steps_per_second': 2.461, 'train_loss': 0.7345810422472133, 'epoch': 5.0}
08/29/2023 07:35:11 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 07:35:11,043 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 07:35:11,043 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 07:35:11,044 >>   Batch size = 8
  0%|          | 0/521 [00:00<?, ?it/s]  1%|          | 6/521 [00:00<00:08, 58.43it/s]  2%|▏         | 12/521 [00:00<00:09, 51.35it/s]  3%|▎         | 18/521 [00:00<00:10, 49.37it/s]  4%|▍         | 23/521 [00:00<00:10, 48.58it/s]  5%|▌         | 28/521 [00:00<00:10, 48.14it/s]  6%|▋         | 33/521 [00:00<00:10, 47.81it/s]  7%|▋         | 38/521 [00:00<00:10, 47.66it/s]  8%|▊         | 43/521 [00:00<00:10, 47.42it/s]  9%|▉         | 48/521 [00:00<00:10, 47.09it/s] 10%|█         | 53/521 [00:01<00:09, 46.98it/s] 11%|█         | 58/521 [00:01<00:09, 47.00it/s] 12%|█▏        | 63/521 [00:01<00:09, 47.04it/s] 13%|█▎        | 68/521 [00:01<00:09, 47.06it/s] 14%|█▍        | 73/521 [00:01<00:09, 47.09it/s] 15%|█▍        | 78/521 [00:01<00:09, 47.10it/s] 16%|█▌        | 83/521 [00:01<00:09, 47.10it/s] 17%|█▋        | 88/521 [00:01<00:09, 47.09it/s] 18%|█▊        | 93/521 [00:01<00:09, 47.01it/s] 19%|█▉        | 98/521 [00:02<00:09, 46.85it/s] 20%|█▉        | 103/521 [00:02<00:08, 46.92it/s] 21%|██        | 108/521 [00:02<00:08, 46.87it/s] 22%|██▏       | 113/521 [00:02<00:08, 46.95it/s] 23%|██▎       | 118/521 [00:02<00:08, 46.91it/s] 24%|██▎       | 123/521 [00:02<00:08, 47.00it/s] 25%|██▍       | 128/521 [00:02<00:08, 47.00it/s] 26%|██▌       | 133/521 [00:02<00:08, 47.09it/s] 26%|██▋       | 138/521 [00:02<00:08, 47.04it/s] 27%|██▋       | 143/521 [00:03<00:08, 47.00it/s] 28%|██▊       | 148/521 [00:03<00:07, 46.95it/s] 29%|██▉       | 153/521 [00:03<00:07, 46.88it/s] 30%|███       | 158/521 [00:03<00:07, 46.90it/s] 31%|███▏      | 163/521 [00:03<00:07, 46.99it/s] 32%|███▏      | 168/521 [00:03<00:07, 46.89it/s] 33%|███▎      | 173/521 [00:03<00:07, 46.92it/s] 34%|███▍      | 178/521 [00:03<00:07, 46.56it/s] 35%|███▌      | 183/521 [00:03<00:07, 46.74it/s] 36%|███▌      | 188/521 [00:03<00:07, 46.83it/s] 37%|███▋      | 193/521 [00:04<00:06, 46.89it/s] 38%|███▊      | 198/521 [00:04<00:06, 46.86it/s] 39%|███▉      | 203/521 [00:04<00:06, 46.84it/s] 40%|███▉      | 208/521 [00:04<00:06, 46.91it/s] 41%|████      | 213/521 [00:04<00:06, 46.88it/s] 42%|████▏     | 218/521 [00:04<00:06, 46.95it/s] 43%|████▎     | 223/521 [00:04<00:06, 46.96it/s] 44%|████▍     | 228/521 [00:04<00:06, 46.91it/s] 45%|████▍     | 233/521 [00:04<00:06, 46.91it/s] 46%|████▌     | 238/521 [00:05<00:06, 47.00it/s] 47%|████▋     | 243/521 [00:05<00:05, 46.92it/s] 48%|████▊     | 248/521 [00:05<00:05, 46.98it/s] 49%|████▊     | 253/521 [00:05<00:05, 46.94it/s] 50%|████▉     | 258/521 [00:05<00:05, 46.92it/s] 50%|█████     | 263/521 [00:05<00:05, 46.97it/s] 51%|█████▏    | 268/521 [00:05<00:05, 47.03it/s] 52%|█████▏    | 273/521 [00:05<00:05, 46.93it/s] 53%|█████▎    | 278/521 [00:05<00:05, 46.92it/s] 54%|█████▍    | 283/521 [00:06<00:05, 46.93it/s] 55%|█████▌    | 288/521 [00:06<00:04, 46.90it/s] 56%|█████▌    | 293/521 [00:06<00:04, 46.96it/s] 57%|█████▋    | 298/521 [00:06<00:04, 46.93it/s] 58%|█████▊    | 303/521 [00:06<00:04, 46.88it/s] 59%|█████▉    | 308/521 [00:06<00:04, 46.58it/s] 60%|██████    | 313/521 [00:06<00:04, 46.59it/s] 61%|██████    | 318/521 [00:06<00:04, 46.82it/s] 62%|██████▏   | 323/521 [00:06<00:04, 46.80it/s] 63%|██████▎   | 328/521 [00:06<00:04, 46.78it/s] 64%|██████▍   | 333/521 [00:07<00:04, 46.76it/s] 65%|██████▍   | 338/521 [00:07<00:03, 46.86it/s] 66%|██████▌   | 343/521 [00:07<00:03, 46.91it/s] 67%|██████▋   | 348/521 [00:07<00:03, 46.87it/s] 68%|██████▊   | 353/521 [00:07<00:03, 46.87it/s] 69%|██████▊   | 358/521 [00:07<00:03, 46.94it/s] 70%|██████▉   | 363/521 [00:07<00:03, 46.85it/s] 71%|███████   | 368/521 [00:07<00:03, 46.94it/s] 72%|███████▏  | 373/521 [00:07<00:03, 46.95it/s] 73%|███████▎  | 378/521 [00:08<00:03, 46.89it/s] 74%|███████▎  | 383/521 [00:08<00:02, 46.86it/s] 74%|███████▍  | 388/521 [00:08<00:02, 46.89it/s] 75%|███████▌  | 393/521 [00:08<00:02, 46.90it/s] 76%|███████▋  | 398/521 [00:08<00:02, 46.91it/s] 77%|███████▋  | 403/521 [00:08<00:02, 46.89it/s] 78%|███████▊  | 408/521 [00:08<00:02, 46.88it/s] 79%|███████▉  | 413/521 [00:08<00:02, 46.94it/s] 80%|████████  | 418/521 [00:08<00:02, 46.96it/s] 81%|████████  | 423/521 [00:08<00:02, 46.97it/s] 82%|████████▏ | 428/521 [00:09<00:01, 46.92it/s] 83%|████████▎ | 433/521 [00:09<00:01, 46.78it/s] 84%|████████▍ | 438/521 [00:09<00:01, 46.82it/s] 85%|████████▌ | 443/521 [00:09<00:01, 46.86it/s] 86%|████████▌ | 448/521 [00:09<00:01, 46.88it/s] 87%|████████▋ | 453/521 [00:09<00:01, 46.90it/s] 88%|████████▊ | 458/521 [00:09<00:01, 46.90it/s] 89%|████████▉ | 463/521 [00:09<00:01, 46.94it/s] 90%|████████▉ | 468/521 [00:09<00:01, 46.95it/s] 91%|█████████ | 473/521 [00:10<00:01, 46.87it/s] 92%|█████████▏| 478/521 [00:10<00:00, 46.95it/s] 93%|█████████▎| 483/521 [00:10<00:00, 46.81it/s] 94%|█████████▎| 488/521 [00:10<00:00, 46.88it/s] 95%|█████████▍| 493/521 [00:10<00:00, 46.90it/s] 96%|█████████▌| 498/521 [00:10<00:00, 46.84it/s] 97%|█████████▋| 503/521 [00:10<00:00, 46.86it/s] 98%|█████████▊| 508/521 [00:10<00:00, 46.91it/s] 98%|█████████▊| 513/521 [00:10<00:00, 46.89it/s] 99%|█████████▉| 518/521 [00:11<00:00, 46.93it/s]100%|██████████| 521/521 [00:11<00:00, 47.01it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 07:35:22,148 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:22,148 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:22,148 >>   eval_loss               =      0.956
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:22,148 >>   eval_runtime            = 0:00:11.10
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:22,148 >>   eval_samples            =       4165
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:22,148 >>   eval_samples_per_second =    375.077
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:22,148 >>   eval_steps_per_second   =     46.918
[INFO|trainer_pt_utils.py:913] 2023-08-29 07:35:22,148 >>   perplexity              =     2.6012
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:28,850 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:28,853 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:28,853 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:28,853 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:28,853 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:35:29,489 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:35:29,490 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:35:30,057 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:35:31,085 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:35:31,085 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:34,086 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:34,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:34,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:34,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:35:34,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:35:34,758 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:35:34,759 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:35:35,329 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:35:35,475 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:35:35,475 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-628
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-157
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-314
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-785
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/checkpoint-471
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'lowest point', 'manufacturer', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12876
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12976, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.89it/s]Extractor Predicting: 2it [00:01,  1.78it/s]Extractor Predicting: 3it [00:01,  1.72it/s]Extractor Predicting: 4it [00:02,  1.75it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.73it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.74it/s]Extractor Predicting: 12it [00:06,  1.74it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.63it/s]Extractor Predicting: 15it [00:08,  1.58it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:10,  1.53it/s]Extractor Predicting: 19it [00:11,  1.54it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:12,  1.55it/s]Extractor Predicting: 22it [00:13,  1.53it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:14,  1.50it/s]Extractor Predicting: 25it [00:15,  1.53it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:16,  1.57it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:17,  1.57it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:19,  1.57it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:21,  1.51it/s]Extractor Predicting: 35it [00:21,  1.54it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.51it/s]Extractor Predicting: 38it [00:23,  1.50it/s]Extractor Predicting: 39it [00:24,  1.50it/s]Extractor Predicting: 40it [00:25,  1.52it/s]Extractor Predicting: 41it [00:25,  1.58it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:27,  1.52it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.56it/s]Extractor Predicting: 47it [00:29,  1.54it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:30,  1.56it/s]Extractor Predicting: 50it [00:31,  1.59it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:33,  1.60it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:34,  1.58it/s]Extractor Predicting: 56it [00:35,  1.56it/s]Extractor Predicting: 57it [00:36,  1.55it/s]Extractor Predicting: 58it [00:36,  1.53it/s]Extractor Predicting: 59it [00:37,  1.55it/s]Extractor Predicting: 60it [00:37,  1.55it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.58it/s]Extractor Predicting: 63it [00:39,  1.58it/s]Extractor Predicting: 64it [00:40,  1.58it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:41,  1.56it/s]Extractor Predicting: 67it [00:42,  1.57it/s]Extractor Predicting: 68it [00:43,  1.54it/s]Extractor Predicting: 69it [00:43,  1.55it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:44,  1.57it/s]Extractor Predicting: 72it [00:45,  1.56it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:46,  1.52it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:48,  1.50it/s]Extractor Predicting: 77it [00:48,  1.48it/s]Extractor Predicting: 78it [00:49,  1.48it/s]Extractor Predicting: 79it [00:50,  1.49it/s]Extractor Predicting: 80it [00:51,  1.38it/s]Extractor Predicting: 81it [00:51,  1.38it/s]Extractor Predicting: 82it [00:52,  1.43it/s]Extractor Predicting: 83it [00:53,  1.50it/s]Extractor Predicting: 84it [00:53,  1.51it/s]Extractor Predicting: 85it [00:54,  1.52it/s]Extractor Predicting: 86it [00:55,  1.53it/s]Extractor Predicting: 87it [00:55,  1.53it/s]Extractor Predicting: 88it [00:56,  1.55it/s]Extractor Predicting: 89it [00:57,  1.54it/s]Extractor Predicting: 90it [00:57,  1.52it/s]Extractor Predicting: 91it [00:58,  1.55it/s]Extractor Predicting: 92it [00:58,  1.56it/s]Extractor Predicting: 93it [00:59,  1.53it/s]Extractor Predicting: 94it [01:00,  1.51it/s]Extractor Predicting: 95it [01:01,  1.49it/s]Extractor Predicting: 96it [01:01,  1.52it/s]Extractor Predicting: 97it [01:02,  1.54it/s]Extractor Predicting: 98it [01:02,  1.55it/s]Extractor Predicting: 99it [01:03,  1.54it/s]Extractor Predicting: 100it [01:04,  1.56it/s]Extractor Predicting: 101it [01:04,  1.54it/s]Extractor Predicting: 102it [01:05,  1.52it/s]Extractor Predicting: 103it [01:06,  1.52it/s]Extractor Predicting: 104it [01:06,  1.54it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:08,  1.55it/s]Extractor Predicting: 107it [01:08,  1.56it/s]Extractor Predicting: 108it [01:09,  1.52it/s]Extractor Predicting: 109it [01:10,  1.53it/s]Extractor Predicting: 110it [01:10,  1.52it/s]Extractor Predicting: 111it [01:11,  1.53it/s]Extractor Predicting: 112it [01:12,  1.51it/s]Extractor Predicting: 113it [01:12,  1.53it/s]Extractor Predicting: 114it [01:13,  1.51it/s]Extractor Predicting: 115it [01:14,  1.54it/s]Extractor Predicting: 116it [01:14,  1.54it/s]Extractor Predicting: 117it [01:15,  1.56it/s]Extractor Predicting: 118it [01:15,  1.54it/s]Extractor Predicting: 119it [01:16,  1.50it/s]Extractor Predicting: 120it [01:17,  1.54it/s]Extractor Predicting: 121it [01:17,  1.53it/s]Extractor Predicting: 122it [01:18,  1.52it/s]Extractor Predicting: 123it [01:19,  1.49it/s]Extractor Predicting: 124it [01:19,  1.49it/s]Extractor Predicting: 125it [01:20,  1.48it/s]Extractor Predicting: 126it [01:21,  1.49it/s]Extractor Predicting: 127it [01:21,  1.48it/s]Extractor Predicting: 128it [01:22,  1.48it/s]Extractor Predicting: 129it [01:23,  1.47it/s]Extractor Predicting: 130it [01:23,  1.49it/s]Extractor Predicting: 131it [01:24,  1.47it/s]Extractor Predicting: 132it [01:25,  1.49it/s]Extractor Predicting: 133it [01:26,  1.50it/s]Extractor Predicting: 134it [01:26,  1.51it/s]Extractor Predicting: 135it [01:27,  1.52it/s]Extractor Predicting: 136it [01:27,  1.52it/s]Extractor Predicting: 137it [01:28,  1.50it/s]Extractor Predicting: 138it [01:29,  1.49it/s]Extractor Predicting: 139it [01:30,  1.50it/s]Extractor Predicting: 140it [01:30,  1.51it/s]Extractor Predicting: 141it [01:31,  1.53it/s]Extractor Predicting: 142it [01:31,  1.50it/s]Extractor Predicting: 143it [01:32,  1.52it/s]Extractor Predicting: 144it [01:33,  1.50it/s]Extractor Predicting: 145it [01:34,  1.40it/s]Extractor Predicting: 146it [01:34,  1.41it/s]Extractor Predicting: 147it [01:35,  1.43it/s]Extractor Predicting: 148it [01:36,  1.45it/s]Extractor Predicting: 149it [01:36,  1.46it/s]Extractor Predicting: 150it [01:37,  1.48it/s]Extractor Predicting: 151it [01:38,  1.49it/s]Extractor Predicting: 152it [01:38,  1.45it/s]Extractor Predicting: 153it [01:39,  1.46it/s]Extractor Predicting: 154it [01:40,  1.59it/s]Extractor Predicting: 154it [01:40,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:25,623 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:25,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:25,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:25,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:25,632 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:37:26,260 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:37:26,261 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:37:26,826 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:37:27,973 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:37:27,973 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:30,888 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:30,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:30,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:30,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:37:30,894 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:37:31,532 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:37:31,534 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:37:32,097 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:37:32,265 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:37:32,265 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4199288256227758,
  "recall": 0.028331332533013204,
  "score": 0.05308142150247413,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27443
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27543, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.64it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.58it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.62it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.47it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:15,  1.53it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.60it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:20,  1.61it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:23,  1.57it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:25,  1.57it/s]Extractor Predicting: 42it [00:26,  1.51it/s]Extractor Predicting: 43it [00:27,  1.52it/s]Extractor Predicting: 44it [00:27,  1.51it/s]Extractor Predicting: 45it [00:28,  1.52it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:29,  1.49it/s]Extractor Predicting: 48it [00:30,  1.47it/s]Extractor Predicting: 49it [00:31,  1.49it/s]Extractor Predicting: 50it [00:31,  1.48it/s]Extractor Predicting: 51it [00:32,  1.49it/s]Extractor Predicting: 52it [00:33,  1.49it/s]Extractor Predicting: 53it [00:33,  1.49it/s]Extractor Predicting: 54it [00:34,  1.48it/s]Extractor Predicting: 55it [00:35,  1.44it/s]Extractor Predicting: 56it [00:35,  1.49it/s]Extractor Predicting: 57it [00:36,  1.52it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:37,  1.49it/s]Extractor Predicting: 60it [00:38,  1.46it/s]Extractor Predicting: 61it [00:39,  1.45it/s]Extractor Predicting: 62it [00:39,  1.49it/s]Extractor Predicting: 63it [00:40,  1.49it/s]Extractor Predicting: 64it [00:41,  1.45it/s]Extractor Predicting: 65it [00:42,  1.46it/s]Extractor Predicting: 66it [00:42,  1.47it/s]Extractor Predicting: 67it [00:43,  1.44it/s]Extractor Predicting: 68it [00:44,  1.45it/s]Extractor Predicting: 69it [00:44,  1.45it/s]Extractor Predicting: 70it [00:45,  1.45it/s]Extractor Predicting: 71it [00:46,  1.46it/s]Extractor Predicting: 72it [00:46,  1.46it/s]Extractor Predicting: 73it [00:47,  1.44it/s]Extractor Predicting: 74it [00:48,  1.43it/s]Extractor Predicting: 75it [00:48,  1.43it/s]Extractor Predicting: 76it [00:49,  1.41it/s]Extractor Predicting: 77it [00:50,  1.47it/s]Extractor Predicting: 78it [00:51,  1.44it/s]Extractor Predicting: 79it [00:51,  1.45it/s]Extractor Predicting: 80it [00:52,  1.45it/s]Extractor Predicting: 81it [00:53,  1.41it/s]Extractor Predicting: 82it [00:53,  1.43it/s]Extractor Predicting: 83it [00:54,  1.47it/s]Extractor Predicting: 84it [00:55,  1.50it/s]Extractor Predicting: 85it [00:55,  1.46it/s]Extractor Predicting: 86it [00:56,  1.44it/s]Extractor Predicting: 87it [00:57,  1.49it/s]Extractor Predicting: 88it [00:57,  1.47it/s]Extractor Predicting: 89it [00:58,  1.45it/s]Extractor Predicting: 90it [00:59,  1.43it/s]Extractor Predicting: 91it [00:59,  1.45it/s]Extractor Predicting: 92it [01:00,  1.47it/s]Extractor Predicting: 93it [01:01,  1.51it/s]Extractor Predicting: 94it [01:01,  1.51it/s]Extractor Predicting: 95it [01:02,  1.53it/s]Extractor Predicting: 96it [01:03,  1.52it/s]Extractor Predicting: 97it [01:04,  1.44it/s]Extractor Predicting: 98it [01:04,  1.44it/s]Extractor Predicting: 99it [01:05,  1.47it/s]Extractor Predicting: 100it [01:06,  1.46it/s]Extractor Predicting: 101it [01:06,  1.49it/s]Extractor Predicting: 102it [01:07,  1.49it/s]Extractor Predicting: 103it [01:08,  1.50it/s]Extractor Predicting: 104it [01:08,  1.51it/s]Extractor Predicting: 105it [01:09,  1.50it/s]Extractor Predicting: 106it [01:10,  1.49it/s]Extractor Predicting: 107it [01:10,  1.51it/s]Extractor Predicting: 108it [01:11,  1.51it/s]Extractor Predicting: 109it [01:12,  1.49it/s]Extractor Predicting: 110it [01:12,  1.40it/s]Extractor Predicting: 111it [01:13,  1.44it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:14,  1.50it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:16,  1.48it/s]Extractor Predicting: 116it [01:16,  1.49it/s]Extractor Predicting: 117it [01:17,  1.53it/s]Extractor Predicting: 118it [01:18,  1.43it/s]Extractor Predicting: 119it [01:18,  1.45it/s]Extractor Predicting: 120it [01:19,  1.47it/s]Extractor Predicting: 121it [01:20,  1.46it/s]Extractor Predicting: 122it [01:20,  1.48it/s]Extractor Predicting: 123it [01:21,  1.49it/s]Extractor Predicting: 124it [01:22,  1.52it/s]Extractor Predicting: 125it [01:22,  1.51it/s]Extractor Predicting: 126it [01:23,  1.53it/s]Extractor Predicting: 127it [01:24,  1.53it/s]Extractor Predicting: 128it [01:24,  1.55it/s]Extractor Predicting: 129it [01:25,  1.57it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:26,  1.56it/s]Extractor Predicting: 132it [01:27,  1.57it/s]Extractor Predicting: 133it [01:27,  1.57it/s]Extractor Predicting: 134it [01:28,  1.54it/s]Extractor Predicting: 135it [01:29,  1.34it/s]Extractor Predicting: 136it [01:30,  1.38it/s]Extractor Predicting: 137it [01:30,  1.41it/s]Extractor Predicting: 138it [01:31,  1.45it/s]Extractor Predicting: 139it [01:32,  1.46it/s]Extractor Predicting: 140it [01:32,  1.46it/s]Extractor Predicting: 141it [01:33,  1.52it/s]Extractor Predicting: 142it [01:34,  1.47it/s]Extractor Predicting: 143it [01:35,  1.43it/s]Extractor Predicting: 144it [01:35,  1.46it/s]Extractor Predicting: 145it [01:36,  1.49it/s]Extractor Predicting: 146it [01:37,  1.43it/s]Extractor Predicting: 147it [01:37,  1.45it/s]Extractor Predicting: 148it [01:38,  1.47it/s]Extractor Predicting: 149it [01:38,  1.56it/s]Extractor Predicting: 150it [01:39,  1.68it/s]Extractor Predicting: 151it [01:40,  1.68it/s]Extractor Predicting: 152it [01:40,  1.79it/s]Extractor Predicting: 153it [01:40,  1.87it/s]Extractor Predicting: 154it [01:41,  1.92it/s]Extractor Predicting: 155it [01:42,  1.89it/s]Extractor Predicting: 156it [01:42,  1.86it/s]Extractor Predicting: 157it [01:43,  1.86it/s]Extractor Predicting: 158it [01:43,  1.90it/s]Extractor Predicting: 159it [01:44,  1.96it/s]Extractor Predicting: 160it [01:44,  1.94it/s]Extractor Predicting: 161it [01:45,  1.91it/s]Extractor Predicting: 162it [01:45,  1.93it/s]Extractor Predicting: 163it [01:46,  1.88it/s]Extractor Predicting: 164it [01:46,  1.88it/s]Extractor Predicting: 165it [01:47,  1.92it/s]Extractor Predicting: 166it [01:47,  1.94it/s]Extractor Predicting: 167it [01:48,  1.91it/s]Extractor Predicting: 168it [01:48,  1.80it/s]Extractor Predicting: 169it [01:49,  1.72it/s]Extractor Predicting: 170it [01:50,  1.61it/s]Extractor Predicting: 171it [01:50,  1.59it/s]Extractor Predicting: 172it [01:51,  1.55it/s]Extractor Predicting: 173it [01:52,  1.50it/s]Extractor Predicting: 174it [01:53,  1.50it/s]Extractor Predicting: 175it [01:53,  1.54it/s]Extractor Predicting: 176it [01:54,  1.54it/s]Extractor Predicting: 177it [01:54,  1.55it/s]Extractor Predicting: 178it [01:55,  1.54it/s]Extractor Predicting: 179it [01:56,  1.58it/s]Extractor Predicting: 180it [01:56,  1.59it/s]Extractor Predicting: 181it [01:57,  1.60it/s]Extractor Predicting: 182it [01:58,  1.58it/s]Extractor Predicting: 183it [01:58,  1.54it/s]Extractor Predicting: 184it [01:59,  1.51it/s]Extractor Predicting: 185it [02:00,  1.54it/s]Extractor Predicting: 186it [02:00,  1.53it/s]Extractor Predicting: 187it [02:01,  1.51it/s]Extractor Predicting: 188it [02:01,  1.57it/s]Extractor Predicting: 189it [02:02,  1.56it/s]Extractor Predicting: 190it [02:03,  1.55it/s]Extractor Predicting: 191it [02:03,  1.57it/s]Extractor Predicting: 192it [02:04,  1.58it/s]Extractor Predicting: 193it [02:05,  1.56it/s]Extractor Predicting: 194it [02:05,  1.60it/s]Extractor Predicting: 195it [02:06,  1.58it/s]Extractor Predicting: 196it [02:07,  1.57it/s]Extractor Predicting: 197it [02:07,  1.58it/s]Extractor Predicting: 198it [02:08,  1.57it/s]Extractor Predicting: 199it [02:08,  1.55it/s]Extractor Predicting: 200it [02:09,  1.53it/s]Extractor Predicting: 201it [02:10,  1.53it/s]Extractor Predicting: 202it [02:11,  1.50it/s]Extractor Predicting: 203it [02:11,  1.47it/s]Extractor Predicting: 204it [02:12,  1.47it/s]Extractor Predicting: 205it [02:13,  1.48it/s]Extractor Predicting: 206it [02:13,  1.48it/s]Extractor Predicting: 207it [02:14,  1.50it/s]Extractor Predicting: 208it [02:15,  1.49it/s]Extractor Predicting: 209it [02:15,  1.51it/s]Extractor Predicting: 210it [02:16,  1.51it/s]Extractor Predicting: 211it [02:17,  1.52it/s]Extractor Predicting: 212it [02:17,  1.56it/s]Extractor Predicting: 213it [02:18,  1.53it/s]Extractor Predicting: 214it [02:18,  1.54it/s]Extractor Predicting: 215it [02:19,  1.55it/s]Extractor Predicting: 216it [02:20,  1.54it/s]Extractor Predicting: 217it [02:20,  1.53it/s]Extractor Predicting: 218it [02:21,  1.56it/s]Extractor Predicting: 219it [02:22,  1.46it/s]Extractor Predicting: 220it [02:22,  1.52it/s]Extractor Predicting: 221it [02:23,  1.51it/s]Extractor Predicting: 222it [02:24,  1.54it/s]Extractor Predicting: 223it [02:24,  1.58it/s]Extractor Predicting: 224it [02:25,  1.56it/s]Extractor Predicting: 225it [02:26,  1.60it/s]Extractor Predicting: 226it [02:26,  1.61it/s]Extractor Predicting: 227it [02:27,  1.63it/s]Extractor Predicting: 228it [02:27,  1.58it/s]Extractor Predicting: 229it [02:28,  1.58it/s]Extractor Predicting: 230it [02:29,  1.56it/s]Extractor Predicting: 231it [02:29,  1.55it/s]Extractor Predicting: 232it [02:30,  1.60it/s]Extractor Predicting: 233it [02:31,  1.58it/s]Extractor Predicting: 234it [02:31,  1.54it/s]Extractor Predicting: 235it [02:32,  1.56it/s]Extractor Predicting: 236it [02:33,  1.55it/s]Extractor Predicting: 237it [02:33,  1.64it/s]Extractor Predicting: 238it [02:34,  1.60it/s]Extractor Predicting: 239it [02:34,  1.59it/s]Extractor Predicting: 240it [02:35,  1.55it/s]Extractor Predicting: 241it [02:36,  1.55it/s]Extractor Predicting: 242it [02:36,  1.54it/s]Extractor Predicting: 243it [02:37,  1.58it/s]Extractor Predicting: 244it [02:38,  1.56it/s]Extractor Predicting: 245it [02:38,  1.54it/s]Extractor Predicting: 246it [02:39,  1.52it/s]Extractor Predicting: 247it [02:40,  1.50it/s]Extractor Predicting: 248it [02:40,  1.48it/s]Extractor Predicting: 249it [02:41,  1.49it/s]Extractor Predicting: 250it [02:42,  1.51it/s]Extractor Predicting: 251it [02:42,  1.54it/s]Extractor Predicting: 252it [02:43,  1.55it/s]Extractor Predicting: 253it [02:44,  1.52it/s]Extractor Predicting: 254it [02:44,  1.54it/s]Extractor Predicting: 255it [02:45,  1.53it/s]Extractor Predicting: 256it [02:46,  1.53it/s]Extractor Predicting: 257it [02:46,  1.53it/s]Extractor Predicting: 258it [02:47,  1.51it/s]Extractor Predicting: 259it [02:48,  1.53it/s]Extractor Predicting: 260it [02:48,  1.53it/s]Extractor Predicting: 261it [02:49,  1.35it/s]Extractor Predicting: 262it [02:50,  1.41it/s]Extractor Predicting: 263it [02:50,  1.44it/s]Extractor Predicting: 264it [02:51,  1.47it/s]Extractor Predicting: 265it [02:52,  1.48it/s]Extractor Predicting: 266it [02:52,  1.51it/s]Extractor Predicting: 267it [02:53,  1.52it/s]Extractor Predicting: 268it [02:54,  1.51it/s]Extractor Predicting: 269it [02:54,  1.52it/s]Extractor Predicting: 270it [02:55,  1.52it/s]Extractor Predicting: 271it [02:56,  1.54it/s]Extractor Predicting: 272it [02:56,  1.51it/s]Extractor Predicting: 273it [02:57,  1.48it/s]Extractor Predicting: 274it [02:58,  1.53it/s]Extractor Predicting: 275it [02:58,  1.54it/s]Extractor Predicting: 276it [02:59,  1.55it/s]Extractor Predicting: 277it [03:00,  1.52it/s]Extractor Predicting: 278it [03:00,  1.52it/s]Extractor Predicting: 279it [03:01,  1.51it/s]Extractor Predicting: 280it [03:02,  1.52it/s]Extractor Predicting: 281it [03:02,  1.46it/s]Extractor Predicting: 282it [03:03,  1.48it/s]Extractor Predicting: 283it [03:04,  1.47it/s]Extractor Predicting: 284it [03:04,  1.47it/s]Extractor Predicting: 285it [03:05,  1.43it/s]Extractor Predicting: 286it [03:06,  1.40it/s]Extractor Predicting: 287it [03:06,  1.42it/s]Extractor Predicting: 288it [03:07,  1.45it/s]Extractor Predicting: 289it [03:08,  1.45it/s]Extractor Predicting: 290it [03:08,  1.46it/s]Extractor Predicting: 291it [03:09,  1.46it/s]Extractor Predicting: 292it [03:10,  1.48it/s]Extractor Predicting: 293it [03:10,  1.49it/s]Extractor Predicting: 294it [03:11,  1.50it/s]Extractor Predicting: 295it [03:12,  1.52it/s]Extractor Predicting: 296it [03:12,  1.53it/s]Extractor Predicting: 297it [03:13,  1.55it/s]Extractor Predicting: 298it [03:14,  1.60it/s]Extractor Predicting: 299it [03:14,  1.59it/s]Extractor Predicting: 300it [03:15,  1.63it/s]Extractor Predicting: 301it [03:15,  1.61it/s]Extractor Predicting: 302it [03:16,  1.56it/s]Extractor Predicting: 303it [03:17,  1.53it/s]Extractor Predicting: 304it [03:17,  1.55it/s]Extractor Predicting: 305it [03:18,  1.55it/s]Extractor Predicting: 306it [03:19,  1.55it/s]Extractor Predicting: 307it [03:19,  1.55it/s]Extractor Predicting: 308it [03:20,  1.54it/s]Extractor Predicting: 309it [03:21,  1.52it/s]Extractor Predicting: 310it [03:21,  1.52it/s]Extractor Predicting: 311it [03:22,  1.52it/s]Extractor Predicting: 312it [03:23,  1.53it/s]Extractor Predicting: 313it [03:23,  1.51it/s]Extractor Predicting: 314it [03:24,  1.51it/s]Extractor Predicting: 315it [03:25,  1.55it/s]Extractor Predicting: 316it [03:25,  1.53it/s]Extractor Predicting: 317it [03:26,  1.51it/s]Extractor Predicting: 318it [03:27,  1.53it/s]Extractor Predicting: 319it [03:27,  1.49it/s]Extractor Predicting: 320it [03:28,  1.51it/s]Extractor Predicting: 321it [03:29,  1.52it/s]Extractor Predicting: 322it [03:29,  1.53it/s]Extractor Predicting: 323it [03:30,  1.53it/s]Extractor Predicting: 324it [03:31,  1.55it/s]Extractor Predicting: 325it [03:31,  1.57it/s]Extractor Predicting: 326it [03:32,  1.56it/s]Extractor Predicting: 327it [03:32,  1.58it/s]Extractor Predicting: 328it [03:33,  1.54it/s]Extractor Predicting: 329it [03:34,  1.56it/s]Extractor Predicting: 330it [03:34,  1.57it/s]Extractor Predicting: 331it [03:35,  1.55it/s]Extractor Predicting: 332it [03:36,  1.53it/s]Extractor Predicting: 333it [03:36,  1.51it/s]Extractor Predicting: 334it [03:37,  1.48it/s]Extractor Predicting: 335it [03:38,  1.50it/s]Extractor Predicting: 336it [03:38,  1.52it/s]Extractor Predicting: 337it [03:39,  1.51it/s]Extractor Predicting: 338it [03:40,  1.50it/s]Extractor Predicting: 339it [03:40,  1.51it/s]Extractor Predicting: 340it [03:41,  1.56it/s]Extractor Predicting: 341it [03:42,  1.53it/s]Extractor Predicting: 342it [03:42,  1.56it/s]Extractor Predicting: 343it [03:43,  1.58it/s]Extractor Predicting: 344it [03:43,  1.61it/s]Extractor Predicting: 345it [03:44,  1.63it/s]Extractor Predicting: 346it [03:45,  1.67it/s]Extractor Predicting: 347it [03:45,  1.70it/s]Extractor Predicting: 348it [03:46,  1.68it/s]Extractor Predicting: 349it [03:46,  1.68it/s]Extractor Predicting: 350it [03:47,  1.63it/s]Extractor Predicting: 351it [03:48,  1.65it/s]Extractor Predicting: 352it [03:48,  1.63it/s]Extractor Predicting: 353it [03:49,  1.61it/s]Extractor Predicting: 354it [03:50,  1.54it/s]Extractor Predicting: 355it [03:50,  1.57it/s]Extractor Predicting: 356it [03:51,  1.55it/s]Extractor Predicting: 357it [03:52,  1.55it/s]Extractor Predicting: 358it [03:52,  1.52it/s]Extractor Predicting: 359it [03:53,  1.50it/s]Extractor Predicting: 360it [03:54,  1.50it/s]Extractor Predicting: 361it [03:54,  1.50it/s]Extractor Predicting: 362it [03:55,  1.50it/s]Extractor Predicting: 363it [03:56,  1.52it/s]Extractor Predicting: 364it [03:56,  1.54it/s]Extractor Predicting: 365it [03:57,  1.51it/s]Extractor Predicting: 366it [03:58,  1.49it/s]Extractor Predicting: 367it [03:58,  1.50it/s]Extractor Predicting: 368it [03:59,  1.50it/s]Extractor Predicting: 369it [04:00,  1.49it/s]Extractor Predicting: 370it [04:00,  1.51it/s]Extractor Predicting: 371it [04:01,  1.50it/s]Extractor Predicting: 372it [04:02,  1.48it/s]Extractor Predicting: 373it [04:02,  1.48it/s]Extractor Predicting: 374it [04:03,  1.51it/s]Extractor Predicting: 375it [04:04,  1.54it/s]Extractor Predicting: 376it [04:04,  1.57it/s]Extractor Predicting: 377it [04:05,  1.43it/s]Extractor Predicting: 378it [04:06,  1.47it/s]Extractor Predicting: 379it [04:06,  1.52it/s]Extractor Predicting: 380it [04:07,  1.55it/s]Extractor Predicting: 381it [04:07,  1.57it/s]Extractor Predicting: 382it [04:08,  1.57it/s]Extractor Predicting: 383it [04:09,  1.57it/s]Extractor Predicting: 384it [04:09,  1.57it/s]Extractor Predicting: 385it [04:10,  1.59it/s]Extractor Predicting: 386it [04:11,  1.62it/s]Extractor Predicting: 387it [04:11,  1.63it/s]Extractor Predicting: 388it [04:12,  1.62it/s]Extractor Predicting: 389it [04:12,  1.63it/s]Extractor Predicting: 390it [04:13,  1.63it/s]Extractor Predicting: 391it [04:14,  1.62it/s]Extractor Predicting: 392it [04:14,  1.67it/s]Extractor Predicting: 393it [04:15,  1.65it/s]Extractor Predicting: 394it [04:15,  1.67it/s]Extractor Predicting: 395it [04:16,  1.64it/s]Extractor Predicting: 396it [04:17,  1.62it/s]Extractor Predicting: 397it [04:17,  1.60it/s]Extractor Predicting: 398it [04:18,  1.57it/s]Extractor Predicting: 399it [04:19,  1.55it/s]Extractor Predicting: 400it [04:19,  1.53it/s]Extractor Predicting: 401it [04:20,  1.54it/s]Extractor Predicting: 402it [04:21,  1.55it/s]Extractor Predicting: 403it [04:21,  1.58it/s]Extractor Predicting: 404it [04:22,  1.53it/s]Extractor Predicting: 405it [04:23,  1.52it/s]Extractor Predicting: 406it [04:23,  1.52it/s]Extractor Predicting: 407it [04:24,  1.52it/s]Extractor Predicting: 408it [04:25,  1.53it/s]Extractor Predicting: 409it [04:25,  1.50it/s]Extractor Predicting: 410it [04:26,  1.50it/s]Extractor Predicting: 411it [04:27,  1.52it/s]Extractor Predicting: 412it [04:27,  1.55it/s]Extractor Predicting: 413it [04:28,  1.52it/s]Extractor Predicting: 414it [04:29,  1.51it/s]Extractor Predicting: 415it [04:29,  1.52it/s]Extractor Predicting: 416it [04:30,  1.52it/s]Extractor Predicting: 417it [04:30,  1.53it/s]Extractor Predicting: 418it [04:31,  1.55it/s]Extractor Predicting: 419it [04:32,  1.55it/s]Extractor Predicting: 420it [04:32,  1.54it/s]Extractor Predicting: 421it [04:33,  1.55it/s]Extractor Predicting: 422it [04:34,  1.52it/s]Extractor Predicting: 423it [04:34,  1.51it/s]Extractor Predicting: 424it [04:35,  1.53it/s]Extractor Predicting: 425it [04:36,  1.52it/s]Extractor Predicting: 426it [04:36,  1.57it/s]Extractor Predicting: 427it [04:37,  1.61it/s]Extractor Predicting: 428it [04:37,  1.61it/s]Extractor Predicting: 429it [04:38,  1.64it/s]Extractor Predicting: 430it [04:39,  1.64it/s]Extractor Predicting: 431it [04:39,  1.69it/s]Extractor Predicting: 432it [04:40,  1.65it/s]Extractor Predicting: 433it [04:40,  1.62it/s]Extractor Predicting: 434it [04:41,  1.64it/s]Extractor Predicting: 435it [04:42,  1.62it/s]Extractor Predicting: 436it [04:42,  1.60it/s]Extractor Predicting: 437it [04:43,  1.65it/s]Extractor Predicting: 438it [04:44,  1.66it/s]Extractor Predicting: 439it [04:44,  1.65it/s]Extractor Predicting: 440it [04:45,  1.62it/s]Extractor Predicting: 441it [04:45,  1.62it/s]Extractor Predicting: 442it [04:46,  1.65it/s]Extractor Predicting: 443it [04:47,  1.62it/s]Extractor Predicting: 444it [04:47,  1.64it/s]Extractor Predicting: 445it [04:48,  1.65it/s]Extractor Predicting: 446it [04:48,  1.67it/s]Extractor Predicting: 447it [04:49,  1.68it/s]Extractor Predicting: 448it [04:50,  1.68it/s]Extractor Predicting: 449it [04:50,  1.68it/s]Extractor Predicting: 450it [04:51,  1.67it/s]Extractor Predicting: 451it [04:51,  1.66it/s]Extractor Predicting: 452it [04:52,  1.62it/s]Extractor Predicting: 453it [04:53,  1.61it/s]Extractor Predicting: 454it [04:53,  1.55it/s]Extractor Predicting: 455it [04:54,  1.59it/s]Extractor Predicting: 456it [04:55,  1.62it/s]Extractor Predicting: 457it [04:55,  1.63it/s]Extractor Predicting: 458it [04:56,  1.64it/s]Extractor Predicting: 459it [04:56,  1.62it/s]Extractor Predicting: 460it [04:57,  1.56it/s]Extractor Predicting: 461it [04:58,  1.54it/s]Extractor Predicting: 462it [04:58,  1.49it/s]Extractor Predicting: 463it [04:59,  1.49it/s]Extractor Predicting: 464it [05:00,  1.52it/s]Extractor Predicting: 465it [05:00,  1.53it/s]Extractor Predicting: 466it [05:01,  1.53it/s]Extractor Predicting: 467it [05:02,  1.49it/s]Extractor Predicting: 468it [05:02,  1.51it/s]Extractor Predicting: 469it [05:03,  1.53it/s]Extractor Predicting: 470it [05:04,  1.50it/s]Extractor Predicting: 471it [05:04,  1.47it/s]Extractor Predicting: 472it [05:05,  1.46it/s]Extractor Predicting: 473it [05:06,  1.49it/s]Extractor Predicting: 474it [05:06,  1.53it/s]Extractor Predicting: 475it [05:07,  1.53it/s]Extractor Predicting: 476it [05:08,  1.53it/s]Extractor Predicting: 477it [05:08,  1.49it/s]Extractor Predicting: 478it [05:09,  1.46it/s]Extractor Predicting: 479it [05:10,  1.46it/s]Extractor Predicting: 480it [05:11,  1.46it/s]Extractor Predicting: 481it [05:11,  1.48it/s]Extractor Predicting: 482it [05:12,  1.47it/s]Extractor Predicting: 483it [05:13,  1.50it/s]Extractor Predicting: 484it [05:13,  1.49it/s]Extractor Predicting: 485it [05:14,  1.54it/s]Extractor Predicting: 486it [05:14,  1.56it/s]Extractor Predicting: 487it [05:15,  1.55it/s]Extractor Predicting: 488it [05:16,  1.51it/s]Extractor Predicting: 489it [05:16,  1.51it/s]Extractor Predicting: 490it [05:17,  1.50it/s]Extractor Predicting: 491it [05:18,  1.49it/s]Extractor Predicting: 492it [05:18,  1.48it/s]Extractor Predicting: 493it [05:19,  1.49it/s]Extractor Predicting: 494it [05:20,  1.50it/s]Extractor Predicting: 495it [05:20,  1.54it/s]Extractor Predicting: 496it [05:21,  1.54it/s]Extractor Predicting: 496it [05:21,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:03,610 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:03,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:03,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:03,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:03,612 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:43:04,065 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:43:04,066 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:43:04,653 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:43:05,718 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:43:05,718 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:08,556 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:08,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:08,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:08,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:43:08,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:43:09,202 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:43:09,203 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:43:09,754 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:43:09,914 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:43:09,914 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.43110735418427726,
  "recall": 0.04284274193548387,
  "score": 0.0779399403988691,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 10529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:03,  1.47it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.49it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:10,  1.46it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.43it/s]Extractor Predicting: 18it [00:12,  1.41it/s]Extractor Predicting: 19it [00:13,  1.37it/s]Extractor Predicting: 20it [00:13,  1.31it/s]Extractor Predicting: 21it [00:14,  1.33it/s]Extractor Predicting: 22it [00:15,  1.30it/s]Extractor Predicting: 23it [00:16,  1.31it/s]Extractor Predicting: 24it [00:16,  1.35it/s]Extractor Predicting: 25it [00:17,  1.40it/s]Extractor Predicting: 26it [00:18,  1.40it/s]Extractor Predicting: 27it [00:18,  1.40it/s]Extractor Predicting: 28it [00:19,  1.40it/s]Extractor Predicting: 29it [00:20,  1.38it/s]Extractor Predicting: 30it [00:21,  1.40it/s]Extractor Predicting: 31it [00:21,  1.40it/s]Extractor Predicting: 32it [00:22,  1.41it/s]Extractor Predicting: 33it [00:23,  1.45it/s]Extractor Predicting: 34it [00:23,  1.48it/s]Extractor Predicting: 35it [00:24,  1.46it/s]Extractor Predicting: 36it [00:25,  1.46it/s]Extractor Predicting: 37it [00:25,  1.47it/s]Extractor Predicting: 38it [00:26,  1.46it/s]Extractor Predicting: 39it [00:27,  1.48it/s]Extractor Predicting: 40it [00:27,  1.50it/s]Extractor Predicting: 41it [00:28,  1.49it/s]Extractor Predicting: 42it [00:29,  1.51it/s]Extractor Predicting: 43it [00:29,  1.50it/s]Extractor Predicting: 44it [00:30,  1.51it/s]Extractor Predicting: 45it [00:31,  1.51it/s]Extractor Predicting: 46it [00:31,  1.45it/s]Extractor Predicting: 47it [00:32,  1.46it/s]Extractor Predicting: 48it [00:33,  1.48it/s]Extractor Predicting: 49it [00:33,  1.49it/s]Extractor Predicting: 50it [00:34,  1.49it/s]Extractor Predicting: 51it [00:35,  1.48it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:36,  1.49it/s]Extractor Predicting: 54it [00:37,  1.48it/s]Extractor Predicting: 55it [00:38,  1.39it/s]Extractor Predicting: 56it [00:38,  1.43it/s]Extractor Predicting: 57it [00:39,  1.48it/s]Extractor Predicting: 58it [00:40,  1.50it/s]Extractor Predicting: 59it [00:40,  1.50it/s]Extractor Predicting: 60it [00:41,  1.44it/s]Extractor Predicting: 61it [00:42,  1.48it/s]Extractor Predicting: 62it [00:42,  1.50it/s]Extractor Predicting: 63it [00:43,  1.46it/s]Extractor Predicting: 64it [00:44,  1.47it/s]Extractor Predicting: 65it [00:44,  1.44it/s]Extractor Predicting: 66it [00:45,  1.51it/s]Extractor Predicting: 67it [00:45,  1.58it/s]Extractor Predicting: 68it [00:46,  1.65it/s]Extractor Predicting: 69it [00:47,  1.72it/s]Extractor Predicting: 70it [00:47,  1.79it/s]Extractor Predicting: 71it [00:48,  1.83it/s]Extractor Predicting: 72it [00:48,  1.83it/s]Extractor Predicting: 73it [00:49,  1.85it/s]Extractor Predicting: 74it [00:49,  1.84it/s]Extractor Predicting: 75it [00:50,  1.82it/s]Extractor Predicting: 76it [00:50,  1.83it/s]Extractor Predicting: 77it [00:51,  1.81it/s]Extractor Predicting: 78it [00:51,  1.83it/s]Extractor Predicting: 79it [00:52,  1.87it/s]Extractor Predicting: 80it [00:53,  1.83it/s]Extractor Predicting: 81it [00:53,  1.82it/s]Extractor Predicting: 82it [00:54,  1.83it/s]Extractor Predicting: 83it [00:54,  1.87it/s]Extractor Predicting: 84it [00:55,  1.88it/s]Extractor Predicting: 85it [00:55,  1.88it/s]Extractor Predicting: 86it [00:56,  1.85it/s]Extractor Predicting: 87it [00:56,  1.91it/s]Extractor Predicting: 88it [00:57,  1.85it/s]Extractor Predicting: 89it [00:57,  1.82it/s]Extractor Predicting: 90it [00:58,  1.82it/s]Extractor Predicting: 91it [00:58,  1.81it/s]Extractor Predicting: 92it [00:59,  1.84it/s]Extractor Predicting: 93it [01:00,  1.85it/s]Extractor Predicting: 94it [01:00,  1.85it/s]Extractor Predicting: 95it [01:01,  1.86it/s]Extractor Predicting: 96it [01:01,  1.72it/s]Extractor Predicting: 97it [01:02,  1.65it/s]Extractor Predicting: 98it [01:03,  1.60it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.49it/s]Extractor Predicting: 101it [01:05,  1.48it/s]Extractor Predicting: 102it [01:05,  1.46it/s]Extractor Predicting: 103it [01:06,  1.46it/s]Extractor Predicting: 104it [01:07,  1.46it/s]Extractor Predicting: 105it [01:08,  1.45it/s]Extractor Predicting: 106it [01:08,  1.44it/s]Extractor Predicting: 107it [01:09,  1.44it/s]Extractor Predicting: 108it [01:10,  1.42it/s]Extractor Predicting: 109it [01:10,  1.43it/s]Extractor Predicting: 110it [01:11,  1.42it/s]Extractor Predicting: 111it [01:12,  1.42it/s]Extractor Predicting: 112it [01:12,  1.45it/s]Extractor Predicting: 113it [01:13,  1.49it/s]Extractor Predicting: 114it [01:14,  1.53it/s]Extractor Predicting: 115it [01:14,  1.55it/s]Extractor Predicting: 116it [01:15,  1.57it/s]Extractor Predicting: 117it [01:16,  1.57it/s]Extractor Predicting: 118it [01:16,  1.58it/s]Extractor Predicting: 119it [01:17,  1.56it/s]Extractor Predicting: 120it [01:17,  1.60it/s]Extractor Predicting: 121it [01:18,  1.60it/s]Extractor Predicting: 122it [01:19,  1.62it/s]Extractor Predicting: 123it [01:19,  1.61it/s]Extractor Predicting: 124it [01:20,  1.56it/s]Extractor Predicting: 125it [01:21,  1.56it/s]Extractor Predicting: 126it [01:21,  1.57it/s]Extractor Predicting: 127it [01:22,  1.55it/s]Extractor Predicting: 128it [01:23,  1.51it/s]Extractor Predicting: 129it [01:23,  1.51it/s]Extractor Predicting: 130it [01:24,  1.51it/s]Extractor Predicting: 131it [01:25,  1.47it/s]Extractor Predicting: 132it [01:25,  1.44it/s]Extractor Predicting: 133it [01:26,  1.51it/s]Extractor Predicting: 133it [01:26,  1.54it/s]
[INFO|configuration_utils.py:515] 2023-08-29 07:44:38,153 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:44:38,154 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 07:44:38,158 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:44:38,159 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 07:44:38,162 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 07:44:41,123 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 07:44:41,126 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 07:44:41,142 >> loading configuration file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 07:44:41,143 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 07:44:41,149 >> Didn't find file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:44:41,152 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:44:41,152 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:44:41,152 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:44:41,152 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:44:41,152 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 07:44:41,153 >> loading file outputs/wrapper/wiki/unseen_15_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6858710562414266,
  "recall": 0.06626905235255136,
  "score": 0.12086052695189752,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/data', model_name='outputs/wrapper/wiki/unseen_15_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 07:44:41,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:42,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:42,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:43,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:44,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:45,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:45,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:46,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:47,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:48,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:48,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:49,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:50,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:51,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:51,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:52,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:53,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:54,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:55,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:55,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:56,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<05:01, 15.87s/it][WARNING|generation_utils.py:914] 2023-08-29 07:44:57,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:58,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:58,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:44:59,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:00,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:00,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:01,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:02,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:03,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:03,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:04,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:05,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:05,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:06,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:07,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:07,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:08,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:09,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:09,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:10,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:11,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:12,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:12,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:31<04:48, 16.02s/it][WARNING|generation_utils.py:914] 2023-08-29 07:45:13,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:14,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:14,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:15,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:16,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:16,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:17,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:18,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:19,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:19,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:20,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:21,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:22,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:22,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:23,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:24,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:24,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:25,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:26,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:27,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:28,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:28,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:29,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:30,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:30,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:31,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:32,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:51<05:02, 17.79s/it][WARNING|generation_utils.py:914] 2023-08-29 07:45:33,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:33,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:34,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:35,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:35,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:36,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:37,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:37,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:38,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:39,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:39,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:40,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:41,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:42,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:42,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:43,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:44,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:44,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:45,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:46,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:05<04:16, 16.03s/it][WARNING|generation_utils.py:914] 2023-08-29 07:45:46,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:47,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:48,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:49,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:50,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:50,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:51,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:52,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:52,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:53,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:54,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:55,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:55,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:56,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:57,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:58,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:58,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:45:59,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:00,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:00,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:01,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:02,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:03,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:22<04:07, 16.50s/it][WARNING|generation_utils.py:914] 2023-08-29 07:46:03,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:04,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:05,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:05,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:06,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:07,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:07,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:08,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:09,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:10,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:11,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:11,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:12,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:13,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:13,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:14,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:15,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:15,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:16,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:17,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:17,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:37<03:42, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-29 07:46:18,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:19,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:20,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:20,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:21,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:22,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:23,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:23,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:24,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:25,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:26,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:26,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:27,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:28,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:28,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:29,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:30,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:31,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:32,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:32,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:33,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:34,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:34,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:54<03:31, 16.27s/it][WARNING|generation_utils.py:914] 2023-08-29 07:46:35,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:36,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:37,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:37,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:38,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:39,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:39,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:40,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:41,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:42,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:42,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:43,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:44,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:45,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:45,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:46,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:47,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:48,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:49,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:49,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:50,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:51,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:52,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:11<03:18, 16.56s/it][WARNING|generation_utils.py:914] 2023-08-29 07:46:52,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:53,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:54,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:55,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:55,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:56,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:57,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:58,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:58,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:46:59,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:00,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:01,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:02,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:03,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:04,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:04,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:05,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:06,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:07,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:08,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:08,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:09,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:10,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:11,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:30<03:10, 17.36s/it][WARNING|generation_utils.py:914] 2023-08-29 07:47:11,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:12,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:13,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:14,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:14,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:15,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:16,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:16,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:17,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:18,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:18,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:19,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:20,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:20,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:21,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:22,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:22,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:23,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:24,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:24,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:25,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:26,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:27,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:27,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:47<02:50, 17.10s/it][WARNING|generation_utils.py:914] 2023-08-29 07:47:28,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:29,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:29,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:30,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:31,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:31,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:32,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:33,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:33,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:34,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:35,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:35,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:36,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:37,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:38,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:38,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:39,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:39,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:40,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:41,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:41,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:42,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:01<02:27, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-29 07:47:43,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:44,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:44,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:45,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:46,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:46,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:47,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:48,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:48,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:49,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:49,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:50,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:51,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:51,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:52,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:53,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:53,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:54,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:55,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:55,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:56,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:57,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:16<02:06, 15.82s/it][WARNING|generation_utils.py:914] 2023-08-29 07:47:57,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:58,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:47:59,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:00,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:00,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:01,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:02,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:03,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:04,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:04,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:05,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:06,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:07,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:07,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:08,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:09,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:09,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:10,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:11,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:12,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:13,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:13,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:33<01:52, 16.05s/it][WARNING|generation_utils.py:914] 2023-08-29 07:48:14,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:15,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:15,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:16,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:17,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:18,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:18,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:19,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:20,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:20,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:21,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:22,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:23,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:23,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:24,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:25,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:25,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:26,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:27,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:28,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:28,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:48<01:34, 15.74s/it][WARNING|generation_utils.py:914] 2023-08-29 07:48:29,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:30,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:30,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:31,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:32,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:33,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:34,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:35,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:36,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:36,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:37,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:38,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:39,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:40,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:40,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:41,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:42,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:43,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:44,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:44,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:45,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:46,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:05<01:21, 16.37s/it][WARNING|generation_utils.py:914] 2023-08-29 07:48:47,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:48,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:48,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:49,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:50,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:51,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:51,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:52,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:53,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:54,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:54,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:55,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:56,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:57,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:57,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:58,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:48:59,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:00,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:01,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:02,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:03,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:03,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:04,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:24<01:08, 17.00s/it][WARNING|generation_utils.py:914] 2023-08-29 07:49:05,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:06,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:07,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:07,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:08,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:09,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:10,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:11,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:11,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:12,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:13,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:14,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:14,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:15,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:16,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:17,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:18,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:19,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:20,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:20,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:21,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:22,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:22,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:42<00:51, 17.27s/it][WARNING|generation_utils.py:914] 2023-08-29 07:49:23,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:24,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:25,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:25,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:26,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:27,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:27,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:28,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:29,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:30,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:30,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:31,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:32,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:32,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:33,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:34,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:34,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:35,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:36,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:36,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:37,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:38,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:57<00:33, 16.64s/it][WARNING|generation_utils.py:914] 2023-08-29 07:49:38,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:39,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:40,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:40,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:41,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:42,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:42,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:43,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:44,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:44,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:45,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:46,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:46,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:47,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:48,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:49,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:49,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:50,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:51,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:52,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:52,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:11<00:16, 16.00s/it][WARNING|generation_utils.py:914] 2023-08-29 07:49:53,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:54,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:55,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:55,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:56,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:57,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:58,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:58,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:49:59,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:00,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:01,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:02,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:02,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:03,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:04,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:05,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:05,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:06,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:07,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:08,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:08,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 07:50:09,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:29<00:00, 16.39s/it]Generating: 100%|██████████| 20/20 [05:29<00:00, 16.46s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:16,542 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:16,544 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:16,544 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:16,544 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:16,544 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:50:16,850 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:50:16,854 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:50:17,121 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:50:18,203 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:50:18,204 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:19,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:19,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:19,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:19,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:50:19,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:50:20,232 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:50:20,233 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:50:20,491 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:50:20,655 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:50:20,655 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 582, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9107142857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 511, 'raw': 608}
{'target': 600, 'success': 539, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.8315217391304348, 'errors': {'', "('the Last Stand', 'from narrative universe', '', 'In early 1977 ( as Ithotica ) , the Doctor and Clara enter the ruins of the Last Stand , discovering that the world is in ruins .')"}}
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 187, 'raw': 256}
{'target': 600, 'success': 210, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 275, 'raw': 384}
{'target': 600, 'success': 296, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 338, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 381, 'raw': 544}
{'target': 600, 'success': 405, 'raw': 576}
{'target': 600, 'success': 427, 'raw': 608}
{'target': 600, 'success': 451, 'raw': 640}
{'target': 600, 'success': 477, 'raw': 672}
{'target': 600, 'success': 502, 'raw': 704}
{'target': 600, 'success': 524, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 590, 'raw': 832}
{'target': 600, 'success': 612, 'raw': 864}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.7083333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 429, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 389, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8464673913043478, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 344, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 428, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 561, 'raw': 672}
{'target': 600, 'success': 588, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : has quality .', 'success_rate': 0.8369565217391305, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 225, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 329, 'raw': 416}
{'target': 600, 'success': 355, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 454, 'raw': 576}
{'target': 600, 'success': 478, 'raw': 608}
{'target': 600, 'success': 509, 'raw': 640}
{'target': 600, 'success': 535, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8020833333333334, 'errors': {'', "('', 'instrument', 'choroid', 'Mice in the cellic system that are present may be those of the choroid mesenteric nuclei , also called the bony mesenteric nuclei ( CMNA ) or the alkyl zircon .')", "('Bizet', 'instrument', '', 'The Bizet , which the French composer Pierre Bizet composed for the Royal Opera House , is named after a German composer with whom Ibsen had an affair .')", "('single', 'instrument', '', 'From 1945 on , the bands debut songwriter , Bill Geebsy , recorded a single that was later nominated for a Grammy Award .')", "('Les Bienfrau', 'instrument', '', 'A French libretto by Fyodor Dostoyevsky is an adaptation of his score to the song Les Bienfrau s La Cêtée dépassante by the composer Léon Blavatnik .')"}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 409, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 506, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 557, 'raw': 704}
{'target': 600, 'success': 578, 'raw': 736}
{'target': 600, 'success': 604, 'raw': 768}
{'prompt': 'Relation : league .', 'success_rate': 0.7864583333333334, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 510, 'raw': 576}
{'target': 600, 'success': 540, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 568, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.8863636363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 619, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.8792613636363636, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Duke of Austria', 'opposite of', '', 'He was the eldest son of James II of Austria , and the mother of the Duke Arnold I , daughter of the late Duke of Austria .')", "('range', 'opposite of', '', 'It ranges from .')"}}
['Relation : residence . Context : Later in the year he came to Paris , where he established himself in the style of John de Morais , whose libretto was in the style of Vincent Van Gogh , Robert Ludomann and William Shakespeare . Head Entity : Vincent van Gogh , Tail Entity : Paris .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 456, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 621, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.84375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : shares border with . Context : The CIB has the largest holdings of shares in a German company , Ligthold AG , which together form a wholly owned subsidiary , Käme , with its subsidiaries Berenberg BV and Nørrebro . Head Entity : Berenberg , Tail Entity : Swedish .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.8928571428571429, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : twinned administrative body . Context : Later in the year ( 1177 ) , he purchased the lands of Lüthold in Bavaria , near Eger , now the seat of Bavaria s parliament . Head Entity : Lethold , Tail Entity : Bavaria .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8551136363636364, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 16644
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16744, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.26it/s]Extractor Estimating: 2it [00:01,  1.36it/s]Extractor Estimating: 3it [00:02,  1.43it/s]Extractor Estimating: 4it [00:02,  1.47it/s]Extractor Estimating: 5it [00:03,  1.47it/s]Extractor Estimating: 6it [00:04,  1.47it/s]Extractor Estimating: 7it [00:04,  1.50it/s]Extractor Estimating: 8it [00:05,  1.47it/s]Extractor Estimating: 9it [00:06,  1.49it/s]Extractor Estimating: 10it [00:06,  1.49it/s]Extractor Estimating: 11it [00:07,  1.51it/s]Extractor Estimating: 12it [00:08,  1.48it/s]Extractor Estimating: 13it [00:08,  1.48it/s]Extractor Estimating: 14it [00:09,  1.49it/s]Extractor Estimating: 15it [00:10,  1.50it/s]Extractor Estimating: 16it [00:10,  1.54it/s]Extractor Estimating: 17it [00:11,  1.52it/s]Extractor Estimating: 18it [00:12,  1.54it/s]Extractor Estimating: 19it [00:12,  1.49it/s]Extractor Estimating: 20it [00:13,  1.51it/s]Extractor Estimating: 21it [00:14,  1.48it/s]Extractor Estimating: 22it [00:14,  1.52it/s]Extractor Estimating: 23it [00:15,  1.51it/s]Extractor Estimating: 24it [00:16,  1.50it/s]Extractor Estimating: 25it [00:16,  1.51it/s]Extractor Estimating: 26it [00:17,  1.48it/s]Extractor Estimating: 27it [00:18,  1.50it/s]Extractor Estimating: 28it [00:18,  1.53it/s]Extractor Estimating: 29it [00:19,  1.56it/s]Extractor Estimating: 30it [00:20,  1.56it/s]Extractor Estimating: 31it [00:20,  1.59it/s]Extractor Estimating: 32it [00:21,  1.59it/s]Extractor Estimating: 33it [00:21,  1.57it/s]Extractor Estimating: 34it [00:22,  1.51it/s]Extractor Estimating: 35it [00:23,  1.50it/s]Extractor Estimating: 36it [00:23,  1.53it/s]Extractor Estimating: 37it [00:24,  1.59it/s]Extractor Estimating: 38it [00:25,  1.62it/s]Extractor Estimating: 39it [00:25,  1.60it/s]Extractor Estimating: 40it [00:26,  1.55it/s]Extractor Estimating: 41it [00:27,  1.59it/s]Extractor Estimating: 42it [00:27,  1.52it/s]Extractor Estimating: 43it [00:28,  1.57it/s]Extractor Estimating: 44it [00:28,  1.54it/s]Extractor Estimating: 45it [00:29,  1.57it/s]Extractor Estimating: 46it [00:30,  1.57it/s]Extractor Estimating: 47it [00:30,  1.59it/s]Extractor Estimating: 48it [00:31,  1.60it/s]Extractor Estimating: 49it [00:32,  1.59it/s]Extractor Estimating: 50it [00:32,  1.56it/s]Extractor Estimating: 51it [00:33,  1.59it/s]Extractor Estimating: 52it [00:33,  1.60it/s]Extractor Estimating: 53it [00:34,  1.51it/s]Extractor Estimating: 54it [00:35,  1.53it/s]Extractor Estimating: 55it [00:36,  1.52it/s]Extractor Estimating: 56it [00:36,  1.50it/s]Extractor Estimating: 57it [00:37,  1.50it/s]Extractor Estimating: 58it [00:38,  1.54it/s]Extractor Estimating: 59it [00:38,  1.54it/s]Extractor Estimating: 60it [00:39,  1.52it/s]Extractor Estimating: 61it [00:40,  1.48it/s]Extractor Estimating: 62it [00:40,  1.47it/s]Extractor Estimating: 63it [00:41,  1.48it/s]Extractor Estimating: 64it [00:42,  1.45it/s]Extractor Estimating: 65it [00:42,  1.47it/s]Extractor Estimating: 66it [00:43,  1.46it/s]Extractor Estimating: 67it [00:44,  1.48it/s]Extractor Estimating: 68it [00:44,  1.53it/s]Extractor Estimating: 69it [00:45,  1.53it/s]Extractor Estimating: 70it [00:46,  1.53it/s]Extractor Estimating: 71it [00:46,  1.54it/s]Extractor Estimating: 72it [00:47,  1.52it/s]Extractor Estimating: 73it [00:47,  1.53it/s]Extractor Estimating: 74it [00:49,  1.32it/s]Extractor Estimating: 75it [00:49,  1.36it/s]Extractor Estimating: 76it [00:50,  1.45it/s]Extractor Estimating: 77it [00:50,  1.50it/s]Extractor Estimating: 78it [00:51,  1.58it/s]Extractor Estimating: 79it [00:51,  1.71it/s]Extractor Estimating: 80it [00:52,  1.71it/s]Extractor Estimating: 81it [00:53,  1.65it/s]Extractor Estimating: 82it [00:53,  1.70it/s]Extractor Estimating: 83it [00:54,  1.74it/s]Extractor Estimating: 84it [00:54,  1.74it/s]Extractor Estimating: 85it [00:55,  1.72it/s]Extractor Estimating: 86it [00:55,  1.73it/s]Extractor Estimating: 87it [00:56,  1.73it/s]Extractor Estimating: 88it [00:57,  1.72it/s]Extractor Estimating: 89it [00:57,  1.75it/s]Extractor Estimating: 90it [00:58,  1.62it/s]Extractor Estimating: 91it [00:59,  1.57it/s]Extractor Estimating: 92it [00:59,  1.60it/s]Extractor Estimating: 93it [01:00,  1.62it/s]Extractor Estimating: 94it [01:00,  1.66it/s]Extractor Estimating: 95it [01:01,  1.59it/s]Extractor Estimating: 96it [01:02,  1.63it/s]Extractor Estimating: 97it [01:02,  1.61it/s]Extractor Estimating: 98it [01:03,  1.65it/s]Extractor Estimating: 99it [01:03,  1.69it/s]Extractor Estimating: 100it [01:04,  1.68it/s]Extractor Estimating: 101it [01:05,  1.64it/s]Extractor Estimating: 102it [01:05,  1.66it/s]Extractor Estimating: 103it [01:06,  1.64it/s]Extractor Estimating: 104it [01:07,  1.51it/s]Extractor Estimating: 105it [01:07,  1.52it/s]Extractor Estimating: 106it [01:08,  1.52it/s]Extractor Estimating: 107it [01:09,  1.51it/s]Extractor Estimating: 108it [01:09,  1.47it/s]Extractor Estimating: 109it [01:10,  1.50it/s]Extractor Estimating: 110it [01:11,  1.51it/s]Extractor Estimating: 111it [01:11,  1.50it/s]Extractor Estimating: 112it [01:12,  1.51it/s]Extractor Estimating: 113it [01:13,  1.51it/s]Extractor Estimating: 114it [01:13,  1.56it/s]Extractor Estimating: 115it [01:14,  1.54it/s]Extractor Estimating: 116it [01:15,  1.56it/s]Extractor Estimating: 117it [01:15,  1.50it/s]Extractor Estimating: 118it [01:16,  1.49it/s]Extractor Estimating: 119it [01:17,  1.51it/s]Extractor Estimating: 120it [01:17,  1.52it/s]Extractor Estimating: 121it [01:18,  1.52it/s]Extractor Estimating: 122it [01:19,  1.51it/s]Extractor Estimating: 123it [01:19,  1.54it/s]Extractor Estimating: 124it [01:20,  1.53it/s]Extractor Estimating: 125it [01:21,  1.49it/s]Extractor Estimating: 126it [01:21,  1.51it/s]Extractor Estimating: 127it [01:22,  1.49it/s]Extractor Estimating: 128it [01:23,  1.47it/s]Extractor Estimating: 129it [01:23,  1.51it/s]Extractor Estimating: 130it [01:24,  1.56it/s]Extractor Estimating: 131it [01:24,  1.57it/s]Extractor Estimating: 132it [01:25,  1.60it/s]Extractor Estimating: 133it [01:26,  1.51it/s]Extractor Estimating: 134it [01:26,  1.56it/s]Extractor Estimating: 135it [01:27,  1.58it/s]Extractor Estimating: 136it [01:28,  1.50it/s]Extractor Estimating: 137it [01:28,  1.50it/s]Extractor Estimating: 138it [01:29,  1.53it/s]Extractor Estimating: 139it [01:30,  1.54it/s]Extractor Estimating: 140it [01:30,  1.54it/s]Extractor Estimating: 141it [01:31,  1.54it/s]Extractor Estimating: 142it [01:32,  1.52it/s]Extractor Estimating: 143it [01:32,  1.54it/s]Extractor Estimating: 144it [01:33,  1.53it/s]Extractor Estimating: 145it [01:34,  1.53it/s]Extractor Estimating: 146it [01:34,  1.52it/s]Extractor Estimating: 147it [01:35,  1.54it/s]Extractor Estimating: 148it [01:35,  1.56it/s]Extractor Estimating: 149it [01:36,  1.58it/s]Extractor Estimating: 150it [01:37,  1.60it/s]Extractor Estimating: 151it [01:37,  1.58it/s]Extractor Estimating: 152it [01:38,  1.55it/s]Extractor Estimating: 153it [01:39,  1.54it/s]Extractor Estimating: 154it [01:39,  1.53it/s]Extractor Estimating: 155it [01:40,  1.58it/s]Extractor Estimating: 156it [01:41,  1.56it/s]Extractor Estimating: 157it [01:41,  1.52it/s]Extractor Estimating: 158it [01:42,  1.54it/s]Extractor Estimating: 159it [01:43,  1.55it/s]Extractor Estimating: 160it [01:43,  1.55it/s]Extractor Estimating: 161it [01:44,  1.55it/s]Extractor Estimating: 162it [01:44,  1.56it/s]Extractor Estimating: 163it [01:45,  1.55it/s]Extractor Estimating: 164it [01:46,  1.53it/s]Extractor Estimating: 165it [01:47,  1.49it/s]Extractor Estimating: 166it [01:47,  1.49it/s]Extractor Estimating: 167it [01:48,  1.38it/s]Extractor Estimating: 168it [01:49,  1.42it/s]Extractor Estimating: 169it [01:49,  1.45it/s]Extractor Estimating: 170it [01:50,  1.40it/s]Extractor Estimating: 171it [01:51,  1.44it/s]Extractor Estimating: 172it [01:51,  1.43it/s]Extractor Estimating: 173it [01:52,  1.47it/s]Extractor Estimating: 174it [01:53,  1.50it/s]Extractor Estimating: 175it [01:53,  1.51it/s]Extractor Estimating: 176it [01:54,  1.53it/s]Extractor Estimating: 177it [01:55,  1.52it/s]Extractor Estimating: 178it [01:55,  1.53it/s]Extractor Estimating: 179it [01:56,  1.52it/s]Extractor Estimating: 180it [01:57,  1.52it/s]Extractor Estimating: 181it [01:57,  1.48it/s]Extractor Estimating: 182it [01:58,  1.55it/s]Extractor Estimating: 183it [01:59,  1.59it/s]Extractor Estimating: 184it [01:59,  1.55it/s]Extractor Estimating: 185it [02:00,  1.56it/s]Extractor Estimating: 186it [02:00,  1.57it/s]Extractor Estimating: 187it [02:01,  1.57it/s]Extractor Estimating: 188it [02:02,  1.58it/s]Extractor Estimating: 189it [02:02,  1.58it/s]Extractor Estimating: 190it [02:03,  1.52it/s]Extractor Estimating: 191it [02:04,  1.55it/s]Extractor Estimating: 192it [02:04,  1.54it/s]Extractor Estimating: 193it [02:05,  1.57it/s]Extractor Estimating: 194it [02:06,  1.53it/s]Extractor Estimating: 195it [02:06,  1.51it/s]Extractor Estimating: 196it [02:07,  1.49it/s]Extractor Estimating: 197it [02:08,  1.53it/s]Extractor Estimating: 198it [02:08,  1.48it/s]Extractor Estimating: 199it [02:09,  1.48it/s]Extractor Estimating: 200it [02:10,  1.49it/s]Extractor Estimating: 201it [02:10,  1.47it/s]Extractor Estimating: 202it [02:11,  1.49it/s]Extractor Estimating: 203it [02:12,  1.46it/s]Extractor Estimating: 204it [02:12,  1.47it/s]Extractor Estimating: 205it [02:13,  1.44it/s]Extractor Estimating: 206it [02:14,  1.47it/s]Extractor Estimating: 207it [02:15,  1.47it/s]Extractor Estimating: 208it [02:15,  1.49it/s]Extractor Estimating: 209it [02:16,  1.51it/s]Extractor Estimating: 210it [02:17,  1.45it/s]Extractor Estimating: 211it [02:17,  1.46it/s]Extractor Estimating: 212it [02:18,  1.34it/s]Extractor Estimating: 213it [02:19,  1.40it/s]Extractor Estimating: 214it [02:19,  1.39it/s]Extractor Estimating: 215it [02:20,  1.37it/s]Extractor Estimating: 216it [02:21,  1.40it/s]Extractor Estimating: 217it [02:22,  1.45it/s]Extractor Estimating: 218it [02:22,  1.46it/s]Extractor Estimating: 219it [02:23,  1.46it/s]Extractor Estimating: 220it [02:24,  1.40it/s]Extractor Estimating: 221it [02:24,  1.43it/s]Extractor Estimating: 222it [02:25,  1.48it/s]Extractor Estimating: 223it [02:26,  1.48it/s]Extractor Estimating: 224it [02:26,  1.45it/s]Extractor Estimating: 225it [02:27,  1.45it/s]Extractor Estimating: 226it [02:28,  1.49it/s]Extractor Estimating: 227it [02:28,  1.49it/s]Extractor Estimating: 228it [02:29,  1.42it/s]Extractor Estimating: 229it [02:30,  1.49it/s]Extractor Estimating: 230it [02:30,  1.47it/s]Extractor Estimating: 231it [02:31,  1.46it/s]Extractor Estimating: 232it [02:32,  1.47it/s]Extractor Estimating: 233it [02:33,  1.43it/s]Extractor Estimating: 234it [02:33,  1.43it/s]Extractor Estimating: 235it [02:34,  1.45it/s]Extractor Estimating: 236it [02:35,  1.47it/s]Extractor Estimating: 237it [02:35,  1.44it/s]Extractor Estimating: 238it [02:36,  1.44it/s]Extractor Estimating: 239it [02:37,  1.45it/s]Extractor Estimating: 240it [02:37,  1.46it/s]Extractor Estimating: 241it [02:38,  1.47it/s]Extractor Estimating: 242it [02:39,  1.50it/s]Extractor Estimating: 243it [02:40,  1.36it/s]Extractor Estimating: 244it [02:40,  1.40it/s]Extractor Estimating: 245it [02:41,  1.45it/s]Extractor Estimating: 246it [02:42,  1.46it/s]Extractor Estimating: 247it [02:42,  1.43it/s]Extractor Estimating: 248it [02:43,  1.49it/s]Extractor Estimating: 249it [02:44,  1.46it/s]Extractor Estimating: 250it [02:44,  1.48it/s]Extractor Estimating: 251it [02:45,  1.55it/s]Extractor Estimating: 252it [02:45,  1.60it/s]Extractor Estimating: 253it [02:46,  1.60it/s]Extractor Estimating: 254it [02:47,  1.70it/s]Extractor Estimating: 255it [02:47,  1.73it/s]Extractor Estimating: 256it [02:48,  1.72it/s]Extractor Estimating: 257it [02:48,  1.73it/s]Extractor Estimating: 258it [02:49,  1.70it/s]Extractor Estimating: 259it [02:49,  1.73it/s]Extractor Estimating: 260it [02:50,  1.71it/s]Extractor Estimating: 261it [02:51,  1.74it/s]Extractor Estimating: 262it [02:51,  1.67it/s]Extractor Estimating: 263it [02:52,  1.67it/s]Extractor Estimating: 264it [02:52,  1.70it/s]Extractor Estimating: 265it [02:53,  1.69it/s]Extractor Estimating: 266it [02:54,  1.70it/s]Extractor Estimating: 267it [02:54,  1.74it/s]Extractor Estimating: 268it [02:55,  1.77it/s]Extractor Estimating: 269it [02:55,  1.74it/s]Extractor Estimating: 270it [02:56,  1.73it/s]Extractor Estimating: 271it [02:56,  1.72it/s]Extractor Estimating: 272it [02:57,  1.72it/s]Extractor Estimating: 273it [02:58,  1.73it/s]Extractor Estimating: 274it [02:58,  1.74it/s]Extractor Estimating: 275it [02:59,  1.73it/s]Extractor Estimating: 276it [02:59,  1.68it/s]Extractor Estimating: 277it [03:00,  1.67it/s]Extractor Estimating: 278it [03:01,  1.66it/s]Extractor Estimating: 279it [03:01,  1.66it/s]Extractor Estimating: 280it [03:02,  1.69it/s]Extractor Estimating: 281it [03:02,  1.73it/s]Extractor Estimating: 282it [03:03,  1.69it/s]Extractor Estimating: 283it [03:03,  1.75it/s]Extractor Estimating: 284it [03:04,  1.72it/s]Extractor Estimating: 285it [03:05,  1.76it/s]Extractor Estimating: 286it [03:05,  1.78it/s]Extractor Estimating: 287it [03:06,  1.78it/s]Extractor Estimating: 288it [03:06,  1.82it/s]Extractor Estimating: 289it [03:07,  1.83it/s]Extractor Estimating: 290it [03:07,  1.77it/s]Extractor Estimating: 291it [03:08,  1.79it/s]Extractor Estimating: 292it [03:09,  1.72it/s]Extractor Estimating: 293it [03:09,  1.67it/s]Extractor Estimating: 294it [03:10,  1.69it/s]Extractor Estimating: 295it [03:10,  1.74it/s]Extractor Estimating: 296it [03:11,  1.75it/s]Extractor Estimating: 297it [03:11,  1.73it/s]Extractor Estimating: 298it [03:12,  1.66it/s]Extractor Estimating: 299it [03:13,  1.68it/s]Extractor Estimating: 300it [03:13,  1.70it/s]Extractor Estimating: 301it [03:14,  1.64it/s]Extractor Estimating: 302it [03:15,  1.54it/s]Extractor Estimating: 303it [03:15,  1.52it/s]Extractor Estimating: 304it [03:16,  1.53it/s]Extractor Estimating: 305it [03:17,  1.48it/s]Extractor Estimating: 306it [03:17,  1.49it/s]Extractor Estimating: 307it [03:18,  1.48it/s]Extractor Estimating: 308it [03:19,  1.45it/s]Extractor Estimating: 309it [03:19,  1.47it/s]Extractor Estimating: 310it [03:20,  1.46it/s]Extractor Estimating: 311it [03:21,  1.47it/s]Extractor Estimating: 312it [03:21,  1.46it/s]Extractor Estimating: 313it [03:22,  1.44it/s]Extractor Estimating: 314it [03:23,  1.42it/s]Extractor Estimating: 315it [03:24,  1.42it/s]Extractor Estimating: 316it [03:24,  1.46it/s]Extractor Estimating: 317it [03:25,  1.46it/s]Extractor Estimating: 318it [03:26,  1.46it/s]Extractor Estimating: 319it [03:26,  1.49it/s]Extractor Estimating: 320it [03:27,  1.48it/s]Extractor Estimating: 321it [03:28,  1.50it/s]Extractor Estimating: 322it [03:28,  1.47it/s]Extractor Estimating: 323it [03:29,  1.47it/s]Extractor Estimating: 324it [03:30,  1.48it/s]Extractor Estimating: 325it [03:30,  1.51it/s]Extractor Estimating: 326it [03:31,  1.55it/s]Extractor Estimating: 327it [03:32,  1.56it/s]Extractor Estimating: 328it [03:32,  1.51it/s]Extractor Estimating: 329it [03:33,  1.58it/s]Extractor Estimating: 330it [03:33,  1.59it/s]Extractor Estimating: 331it [03:34,  1.59it/s]Extractor Estimating: 332it [03:35,  1.46it/s]Extractor Estimating: 333it [03:35,  1.51it/s]Extractor Estimating: 334it [03:36,  1.57it/s]Extractor Estimating: 335it [03:37,  1.58it/s]Extractor Estimating: 336it [03:37,  1.56it/s]Extractor Estimating: 337it [03:38,  1.58it/s]Extractor Estimating: 338it [03:39,  1.63it/s]Extractor Estimating: 339it [03:39,  1.58it/s]Extractor Estimating: 340it [03:40,  1.61it/s]Extractor Estimating: 341it [03:40,  1.60it/s]Extractor Estimating: 342it [03:41,  1.64it/s]Extractor Estimating: 343it [03:42,  1.64it/s]Extractor Estimating: 344it [03:42,  1.64it/s]Extractor Estimating: 345it [03:43,  1.65it/s]Extractor Estimating: 346it [03:43,  1.63it/s]Extractor Estimating: 347it [03:44,  1.63it/s]Extractor Estimating: 348it [03:45,  1.63it/s]Extractor Estimating: 349it [03:45,  1.62it/s]Extractor Estimating: 350it [03:46,  1.61it/s]Extractor Estimating: 351it [03:47,  1.55it/s]Extractor Estimating: 352it [03:47,  1.58it/s]Extractor Estimating: 353it [03:48,  1.56it/s]Extractor Estimating: 354it [03:49,  1.51it/s]Extractor Estimating: 355it [03:49,  1.54it/s]Extractor Estimating: 356it [03:50,  1.55it/s]Extractor Estimating: 357it [03:51,  1.55it/s]Extractor Estimating: 358it [03:51,  1.54it/s]Extractor Estimating: 359it [03:52,  1.50it/s]Extractor Estimating: 360it [03:53,  1.51it/s]Extractor Estimating: 361it [03:53,  1.43it/s]Extractor Estimating: 362it [03:54,  1.43it/s]Extractor Estimating: 363it [03:55,  1.47it/s]Extractor Estimating: 364it [03:55,  1.49it/s]Extractor Estimating: 365it [03:56,  1.49it/s]Extractor Estimating: 366it [03:57,  1.52it/s]Extractor Estimating: 367it [03:57,  1.52it/s]Extractor Estimating: 368it [03:58,  1.50it/s]Extractor Estimating: 369it [03:59,  1.49it/s]Extractor Estimating: 370it [03:59,  1.49it/s]Extractor Estimating: 371it [04:00,  1.50it/s]Extractor Estimating: 372it [04:01,  1.53it/s]Extractor Estimating: 373it [04:01,  1.54it/s]Extractor Estimating: 374it [04:02,  1.52it/s]Extractor Estimating: 375it [04:03,  1.50it/s]Extractor Estimating: 376it [04:03,  1.51it/s]Extractor Estimating: 377it [04:04,  1.52it/s]Extractor Estimating: 378it [04:04,  1.60it/s]Extractor Estimating: 379it [04:05,  1.64it/s]Extractor Estimating: 380it [04:06,  1.56it/s]Extractor Estimating: 381it [04:06,  1.58it/s]Extractor Estimating: 382it [04:07,  1.54it/s]Extractor Estimating: 383it [04:08,  1.54it/s]Extractor Estimating: 384it [04:08,  1.54it/s]Extractor Estimating: 385it [04:09,  1.52it/s]Extractor Estimating: 386it [04:10,  1.49it/s]Extractor Estimating: 387it [04:10,  1.54it/s]Extractor Estimating: 388it [04:11,  1.49it/s]Extractor Estimating: 389it [04:12,  1.50it/s]Extractor Estimating: 390it [04:12,  1.53it/s]Extractor Estimating: 391it [04:13,  1.53it/s]Extractor Estimating: 392it [04:14,  1.49it/s]Extractor Estimating: 393it [04:14,  1.55it/s]Extractor Estimating: 394it [04:15,  1.58it/s]Extractor Estimating: 395it [04:15,  1.58it/s]Extractor Estimating: 396it [04:16,  1.58it/s]Extractor Estimating: 397it [04:17,  1.57it/s]Extractor Estimating: 398it [04:17,  1.51it/s]Extractor Estimating: 399it [04:18,  1.48it/s]Extractor Estimating: 400it [04:19,  1.50it/s]Extractor Estimating: 401it [04:20,  1.48it/s]Extractor Estimating: 402it [04:20,  1.45it/s]Extractor Estimating: 403it [04:21,  1.43it/s]Extractor Estimating: 404it [04:22,  1.42it/s]Extractor Estimating: 405it [04:22,  1.44it/s]Extractor Estimating: 406it [04:23,  1.48it/s]Extractor Estimating: 407it [04:24,  1.43it/s]Extractor Estimating: 408it [04:24,  1.50it/s]Extractor Estimating: 409it [04:25,  1.47it/s]Extractor Estimating: 410it [04:26,  1.50it/s]Extractor Estimating: 411it [04:27,  1.37it/s]Extractor Estimating: 412it [04:27,  1.40it/s]Extractor Estimating: 413it [04:28,  1.42it/s]Extractor Estimating: 414it [04:29,  1.48it/s]Extractor Estimating: 415it [04:29,  1.48it/s]Extractor Estimating: 416it [04:30,  1.46it/s]Extractor Estimating: 417it [04:31,  1.46it/s]Extractor Estimating: 418it [04:31,  1.45it/s]Extractor Estimating: 419it [04:32,  1.53it/s]Extractor Estimating: 420it [04:33,  1.50it/s]Extractor Estimating: 421it [04:33,  1.53it/s]Extractor Estimating: 422it [04:34,  1.52it/s]Extractor Estimating: 423it [04:34,  1.55it/s]Extractor Estimating: 424it [04:35,  1.55it/s]Extractor Estimating: 425it [04:36,  1.58it/s]Extractor Estimating: 426it [04:36,  1.59it/s]Extractor Estimating: 427it [04:37,  1.54it/s]Extractor Estimating: 428it [04:38,  1.59it/s]Extractor Estimating: 429it [04:38,  1.58it/s]Extractor Estimating: 430it [04:39,  1.62it/s]Extractor Estimating: 431it [04:39,  1.63it/s]Extractor Estimating: 432it [04:40,  1.64it/s]Extractor Estimating: 433it [04:41,  1.65it/s]Extractor Estimating: 434it [04:41,  1.66it/s]Extractor Estimating: 435it [04:42,  1.66it/s]Extractor Estimating: 436it [04:42,  1.65it/s]Extractor Estimating: 437it [04:43,  1.71it/s]Extractor Estimating: 438it [04:44,  1.67it/s]Extractor Estimating: 439it [04:44,  1.67it/s]Extractor Estimating: 440it [04:45,  1.62it/s]Extractor Estimating: 441it [04:46,  1.60it/s]Extractor Estimating: 442it [04:46,  1.64it/s]Extractor Estimating: 443it [04:47,  1.61it/s]Extractor Estimating: 444it [04:47,  1.61it/s]Extractor Estimating: 445it [04:48,  1.63it/s]Extractor Estimating: 446it [04:49,  1.64it/s]Extractor Estimating: 447it [04:49,  1.73it/s]Extractor Estimating: 448it [04:50,  1.69it/s]Extractor Estimating: 449it [04:50,  1.69it/s]Extractor Estimating: 450it [04:51,  1.66it/s]Extractor Estimating: 451it [04:52,  1.61it/s]Extractor Estimating: 452it [04:52,  1.60it/s]Extractor Estimating: 453it [04:53,  1.59it/s]Extractor Estimating: 454it [04:53,  1.62it/s]Extractor Estimating: 455it [04:54,  1.58it/s]Extractor Estimating: 456it [04:55,  1.59it/s]Extractor Estimating: 457it [04:55,  1.60it/s]Extractor Estimating: 458it [04:56,  1.58it/s]Extractor Estimating: 459it [04:57,  1.57it/s]Extractor Estimating: 460it [04:57,  1.57it/s]Extractor Estimating: 461it [04:58,  1.59it/s]Extractor Estimating: 462it [04:58,  1.63it/s]Extractor Estimating: 463it [04:59,  1.56it/s]Extractor Estimating: 464it [05:00,  1.55it/s]Extractor Estimating: 465it [05:00,  1.53it/s]Extractor Estimating: 466it [05:01,  1.58it/s]Extractor Estimating: 467it [05:02,  1.58it/s]Extractor Estimating: 468it [05:02,  1.52it/s]Extractor Estimating: 469it [05:03,  1.56it/s]Extractor Estimating: 470it [05:04,  1.58it/s]Extractor Estimating: 471it [05:04,  1.56it/s]Extractor Estimating: 472it [05:05,  1.50it/s]Extractor Estimating: 473it [05:06,  1.56it/s]Extractor Estimating: 474it [05:06,  1.55it/s]Extractor Estimating: 475it [05:07,  1.54it/s]Extractor Estimating: 476it [05:08,  1.54it/s]Extractor Estimating: 477it [05:08,  1.48it/s]Extractor Estimating: 478it [05:09,  1.50it/s]Extractor Estimating: 479it [05:10,  1.46it/s]Extractor Estimating: 480it [05:11,  1.37it/s]Extractor Estimating: 481it [05:11,  1.45it/s]Extractor Estimating: 482it [05:12,  1.48it/s]Extractor Estimating: 483it [05:12,  1.52it/s]Extractor Estimating: 484it [05:13,  1.53it/s]Extractor Estimating: 485it [05:14,  1.54it/s]Extractor Estimating: 486it [05:14,  1.53it/s]Extractor Estimating: 487it [05:15,  1.56it/s]Extractor Estimating: 488it [05:16,  1.57it/s]Extractor Estimating: 489it [05:16,  1.56it/s]Extractor Estimating: 490it [05:17,  1.56it/s]Extractor Estimating: 491it [05:17,  1.59it/s]Extractor Estimating: 492it [05:18,  1.56it/s]Extractor Estimating: 493it [05:19,  1.59it/s]Extractor Estimating: 494it [05:19,  1.59it/s]Extractor Estimating: 495it [05:20,  1.57it/s]Extractor Estimating: 496it [05:21,  1.58it/s]Extractor Estimating: 497it [05:21,  1.58it/s]Extractor Estimating: 498it [05:22,  1.59it/s]Extractor Estimating: 499it [05:23,  1.49it/s]Extractor Estimating: 500it [05:23,  1.70it/s]Extractor Estimating: 500it [05:23,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:06,192 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:06,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:06,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:06,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:06,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 07:56:06,645 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 07:56:06,646 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:56:07,050 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 07:56:08,105 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:56:08,106 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:10,120 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:10,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:10,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:10,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 07:56:10,125 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 07:56:10,599 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 07:56:10,600 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 07:56:11,002 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 07:56:11,164 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 07:56:11,164 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 11:01:13,143 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 11:01:13,516 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 9903 mean pseudo reward: 0.9259275855841559
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl'}
train vocab size: 28163
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 28263, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=28263, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.053, loss:836.0968
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.073, loss:802.0061
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.056, loss:815.5324
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.074, loss:797.5475
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 87, avg_time 1.072, loss:742.7566
>> valid entity prec:0.6100, rec:0.3955, f1:0.4799
>> valid relation prec:0.1846, rec:0.0144, f1:0.0268
>> valid relation with NER prec:0.1846, rec:0.0144, f1:0.0268
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 187, avg_time 2.509, loss:783.0553
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 287, avg_time 1.069, loss:804.7123
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 387, avg_time 1.074, loss:830.9383
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 74, avg_time 1.071, loss:771.4751
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 174, avg_time 1.057, loss:786.8652
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5938, rec:0.4661, f1:0.5223
>> valid relation prec:0.2227, rec:0.0236, f1:0.0426
>> valid relation with NER prec:0.2227, rec:0.0236, f1:0.0426
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 274, avg_time 2.504, loss:804.9677
g_step 1200, step 374, avg_time 1.082, loss:801.9695
g_step 1300, step 61, avg_time 1.069, loss:766.3890
g_step 1400, step 161, avg_time 1.058, loss:763.4829
g_step 1500, step 261, avg_time 1.061, loss:765.6901
>> valid entity prec:0.6016, rec:0.4503, f1:0.5151
>> valid relation prec:0.1579, rec:0.0144, f1:0.0264
>> valid relation with NER prec:0.1579, rec:0.0144, f1:0.0264
g_step 1600, step 361, avg_time 2.511, loss:773.6206
g_step 1700, step 48, avg_time 1.070, loss:785.7353
g_step 1800, step 148, avg_time 1.061, loss:745.5258
g_step 1900, step 248, avg_time 1.088, loss:760.6579
g_step 2000, step 348, avg_time 1.063, loss:743.3256
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5955, rec:0.4670, f1:0.5235
>> valid relation prec:0.1522, rec:0.0202, f1:0.0357
>> valid relation with NER prec:0.1522, rec:0.0202, f1:0.0357
new max entity f1 on valid!
g_step 2100, step 35, avg_time 2.499, loss:711.9455
g_step 2200, step 135, avg_time 1.073, loss:709.3472
g_step 2300, step 235, avg_time 1.054, loss:734.0840
g_step 2400, step 335, avg_time 1.064, loss:712.0133
g_step 2500, step 22, avg_time 1.082, loss:715.4338
>> valid entity prec:0.5903, rec:0.4423, f1:0.5057
>> valid relation prec:0.1603, rec:0.0233, f1:0.0407
>> valid relation with NER prec:0.1603, rec:0.0233, f1:0.0407
g_step 2600, step 122, avg_time 2.497, loss:676.8103
g_step 2700, step 222, avg_time 1.067, loss:677.4807
g_step 2800, step 322, avg_time 1.076, loss:719.0056
g_step 2900, step 9, avg_time 1.062, loss:714.4150
g_step 3000, step 109, avg_time 1.059, loss:668.1085
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6203, rec:0.4035, f1:0.4890
>> valid relation prec:0.1830, rec:0.0171, f1:0.0312
>> valid relation with NER prec:0.1830, rec:0.0171, f1:0.0312
g_step 3100, step 209, avg_time 2.514, loss:674.3135
g_step 3200, step 309, avg_time 1.078, loss:672.1063
g_step 3300, step 409, avg_time 1.070, loss:670.7933
g_step 3400, step 96, avg_time 1.089, loss:619.9802
g_step 3500, step 196, avg_time 1.060, loss:659.7157
>> valid entity prec:0.5702, rec:0.4391, f1:0.4962
>> valid relation prec:0.1808, rec:0.0231, f1:0.0409
>> valid relation with NER prec:0.1808, rec:0.0231, f1:0.0409
g_step 3600, step 296, avg_time 2.497, loss:647.4056
g_step 3700, step 396, avg_time 1.069, loss:665.4882
g_step 3800, step 83, avg_time 1.055, loss:601.9144
g_step 3900, step 183, avg_time 1.059, loss:641.1372
g_step 4000, step 283, avg_time 1.066, loss:639.0663
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5726, rec:0.4390, f1:0.4970
>> valid relation prec:0.1796, rec:0.0161, f1:0.0296
>> valid relation with NER prec:0.1796, rec:0.0161, f1:0.0296
g_step 4100, step 383, avg_time 2.495, loss:633.3155
g_step 4200, step 70, avg_time 1.062, loss:581.6199
g_step 4300, step 170, avg_time 1.060, loss:603.1426
g_step 4400, step 270, avg_time 1.068, loss:640.0145
g_step 4500, step 370, avg_time 1.067, loss:606.8108
>> valid entity prec:0.5661, rec:0.4582, f1:0.5065
>> valid relation prec:0.1442, rec:0.0180, f1:0.0321
>> valid relation with NER prec:0.1442, rec:0.0180, f1:0.0321
g_step 4600, step 57, avg_time 2.482, loss:590.9100
g_step 4700, step 157, avg_time 1.066, loss:573.8016
g_step 4800, step 257, avg_time 1.074, loss:595.9471
g_step 4900, step 357, avg_time 1.061, loss:596.1960
g_step 5000, step 44, avg_time 1.054, loss:549.1222
learning rate was adjusted to 0.0008
>> valid entity prec:0.5776, rec:0.4194, f1:0.4860
>> valid relation prec:0.1330, rec:0.0176, f1:0.0310
>> valid relation with NER prec:0.1330, rec:0.0176, f1:0.0310
g_step 5100, step 144, avg_time 2.500, loss:569.0148
g_step 5200, step 244, avg_time 1.061, loss:574.9630
g_step 5300, step 344, avg_time 1.068, loss:578.9417
g_step 5400, step 31, avg_time 1.071, loss:551.2486
g_step 5500, step 131, avg_time 1.055, loss:514.4230
>> valid entity prec:0.5770, rec:0.4292, f1:0.4923
>> valid relation prec:0.1619, rec:0.0284, f1:0.0483
>> valid relation with NER prec:0.1619, rec:0.0284, f1:0.0483
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 231, avg_time 2.494, loss:554.9898
g_step 5700, step 331, avg_time 1.074, loss:562.8538
g_step 5800, step 18, avg_time 1.063, loss:572.7260
g_step 5900, step 118, avg_time 1.054, loss:523.4102
g_step 6000, step 218, avg_time 1.064, loss:533.1605
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5743, rec:0.4329, f1:0.4937
>> valid relation prec:0.1771, rec:0.0339, f1:0.0569
>> valid relation with NER prec:0.1771, rec:0.0339, f1:0.0569
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6100, step 318, avg_time 2.497, loss:548.2989
g_step 6200, step 5, avg_time 1.062, loss:531.2198
g_step 6300, step 105, avg_time 1.058, loss:504.3064
g_step 6400, step 205, avg_time 1.063, loss:536.8192
g_step 6500, step 305, avg_time 1.074, loss:495.2621
>> valid entity prec:0.5579, rec:0.4224, f1:0.4807
>> valid relation prec:0.1327, rec:0.0207, f1:0.0358
>> valid relation with NER prec:0.1327, rec:0.0207, f1:0.0358
g_step 6600, step 405, avg_time 2.493, loss:520.7381
g_step 6700, step 92, avg_time 1.068, loss:494.7490
g_step 6800, step 192, avg_time 1.065, loss:501.4897
g_step 6900, step 292, avg_time 1.055, loss:493.3493
g_step 7000, step 392, avg_time 1.063, loss:510.3832
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5665, rec:0.4187, f1:0.4815
>> valid relation prec:0.1395, rec:0.0284, f1:0.0472
>> valid relation with NER prec:0.1395, rec:0.0284, f1:0.0472
g_step 7100, step 79, avg_time 2.498, loss:472.1520
g_step 7200, step 179, avg_time 1.054, loss:470.9847
g_step 7300, step 279, avg_time 1.071, loss:486.3988
g_step 7400, step 379, avg_time 1.058, loss:490.7663
g_step 7500, step 66, avg_time 1.057, loss:465.3225
>> valid entity prec:0.5275, rec:0.4564, f1:0.4894
>> valid relation prec:0.1255, rec:0.0250, f1:0.0417
>> valid relation with NER prec:0.1255, rec:0.0250, f1:0.0417
g_step 7600, step 166, avg_time 2.495, loss:463.3619
g_step 7700, step 266, avg_time 1.066, loss:459.7521
g_step 7800, step 366, avg_time 1.069, loss:481.1564
g_step 7900, step 53, avg_time 1.090, loss:440.6708
g_step 8000, step 153, avg_time 1.072, loss:417.5196
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5680, rec:0.3725, f1:0.4499
>> valid relation prec:0.1503, rec:0.0236, f1:0.0407
>> valid relation with NER prec:0.1503, rec:0.0236, f1:0.0407
g_step 8100, step 253, avg_time 2.495, loss:461.4213
g_step 8200, step 353, avg_time 1.053, loss:476.5427
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 11:01:13 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 11:01:13 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_11-01-13_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 11:01:15 - WARNING - datasets.builder -   Using custom data configuration default-6b1823708efaa67e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6b1823708efaa67e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:01,  1.08s/ tables]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 11:01:19,884 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:01:19,885 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:01:19,885 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:01:19,886 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:01:20,030 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:01:20,080 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:01:20,080 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:01:20,081 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:01:20,081 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:01:20,081 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:01:20,081 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 11:01:21,121 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:01:24,488 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 11:01:24,488 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6b1823708efaa67e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:01<00:12,  1.20s/ba] 18%|█▊        | 2/11 [00:01<00:05,  1.62ba/s] 27%|██▋       | 3/11 [00:01<00:03,  2.31ba/s] 36%|███▋      | 4/11 [00:01<00:02,  2.74ba/s] 45%|████▌     | 5/11 [00:02<00:01,  3.22ba/s] 55%|█████▍    | 6/11 [00:02<00:01,  3.59ba/s] 64%|██████▎   | 7/11 [00:02<00:01,  3.89ba/s] 73%|███████▎  | 8/11 [00:02<00:00,  4.10ba/s] 82%|████████▏ | 9/11 [00:02<00:00,  4.23ba/s] 91%|█████████ | 10/11 [00:03<00:00,  4.35ba/s]100%|██████████| 11/11 [00:03<00:00,  3.45ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.68ba/s] 40%|████      | 2/5 [00:00<00:01,  2.71ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.34ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.72ba/s]100%|██████████| 5/5 [00:01<00:00,  4.10ba/s]
  0%|          | 0/11 [00:00<?, ?ba/s]  9%|▉         | 1/11 [00:00<00:02,  3.62ba/s] 27%|██▋       | 3/11 [00:00<00:01,  7.10ba/s] 45%|████▌     | 5/11 [00:00<00:00,  8.65ba/s] 64%|██████▎   | 7/11 [00:00<00:00,  9.43ba/s] 82%|████████▏ | 9/11 [00:01<00:00,  9.88ba/s]100%|██████████| 11/11 [00:01<00:00,  9.83ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.73ba/s] 40%|████      | 2/5 [00:00<00:00,  6.17ba/s] 80%|████████  | 4/5 [00:00<00:00,  8.39ba/s]100%|██████████| 5/5 [00:00<00:00,  9.19ba/s]
[INFO|trainer.py:414] 2023-08-29 11:01:32,191 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 11:01:32,392 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 11:01:32,392 >>   Num examples = 10029
[INFO|trainer.py:1149] 2023-08-29 11:01:32,392 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 11:01:32,392 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 11:01:32,392 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 11:01:32,392 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 11:01:32,392 >>   Total optimization steps = 785
  0%|          | 0/785 [00:00<?, ?it/s]  0%|          | 1/785 [00:00<07:04,  1.85it/s]  0%|          | 2/785 [00:00<05:11,  2.51it/s]  0%|          | 3/785 [00:01<05:08,  2.53it/s]  1%|          | 4/785 [00:01<04:34,  2.84it/s]  1%|          | 5/785 [00:01<04:15,  3.05it/s]  1%|          | 6/785 [00:02<04:04,  3.19it/s]  1%|          | 7/785 [00:02<03:57,  3.28it/s]  1%|          | 8/785 [00:02<03:52,  3.34it/s]  1%|          | 9/785 [00:02<03:48,  3.39it/s]  1%|▏         | 10/785 [00:03<03:46,  3.42it/s]  1%|▏         | 11/785 [00:03<03:44,  3.44it/s]  2%|▏         | 12/785 [00:03<03:43,  3.46it/s]  2%|▏         | 13/785 [00:04<03:42,  3.47it/s]  2%|▏         | 14/785 [00:04<03:49,  3.36it/s]  2%|▏         | 15/785 [00:04<03:46,  3.40it/s]  2%|▏         | 16/785 [00:04<03:44,  3.43it/s]  2%|▏         | 17/785 [00:05<03:42,  3.45it/s]  2%|▏         | 18/785 [00:05<03:41,  3.46it/s]  2%|▏         | 19/785 [00:05<03:40,  3.47it/s]  3%|▎         | 20/785 [00:06<03:39,  3.48it/s]  3%|▎         | 21/785 [00:06<03:39,  3.48it/s]  3%|▎         | 22/785 [00:06<03:38,  3.48it/s]  3%|▎         | 23/785 [00:06<03:38,  3.49it/s]  3%|▎         | 24/785 [00:07<03:38,  3.49it/s]  3%|▎         | 25/785 [00:07<03:37,  3.49it/s]  3%|▎         | 26/785 [00:07<03:37,  3.49it/s]  3%|▎         | 27/785 [00:08<03:37,  3.49it/s]  4%|▎         | 28/785 [00:08<03:36,  3.49it/s]  4%|▎         | 29/785 [00:08<03:36,  3.49it/s]  4%|▍         | 30/785 [00:08<03:36,  3.49it/s]  4%|▍         | 31/785 [00:09<03:57,  3.18it/s]  4%|▍         | 32/785 [00:09<03:50,  3.27it/s]  4%|▍         | 33/785 [00:09<03:45,  3.33it/s]  4%|▍         | 34/785 [00:10<03:42,  3.38it/s]  4%|▍         | 35/785 [00:10<03:39,  3.41it/s]  5%|▍         | 36/785 [00:10<03:38,  3.43it/s]  5%|▍         | 37/785 [00:11<03:36,  3.45it/s]  5%|▍         | 38/785 [00:11<04:02,  3.08it/s]  5%|▍         | 39/785 [00:11<03:53,  3.20it/s]  5%|▌         | 40/785 [00:12<03:47,  3.28it/s]  5%|▌         | 41/785 [00:12<03:42,  3.34it/s]  5%|▌         | 42/785 [00:12<03:39,  3.38it/s]  5%|▌         | 43/785 [00:12<03:37,  3.42it/s]  6%|▌         | 44/785 [00:13<03:35,  3.44it/s]  6%|▌         | 45/785 [00:13<03:34,  3.45it/s]  6%|▌         | 46/785 [00:13<03:33,  3.46it/s]  6%|▌         | 47/785 [00:14<03:32,  3.47it/s]  6%|▌         | 48/785 [00:14<03:31,  3.48it/s]  6%|▌         | 49/785 [00:14<03:50,  3.20it/s]  6%|▋         | 50/785 [00:15<03:44,  3.28it/s]  6%|▋         | 51/785 [00:15<03:39,  3.34it/s]  7%|▋         | 52/785 [00:15<03:36,  3.38it/s]  7%|▋         | 53/785 [00:15<03:34,  3.42it/s]  7%|▋         | 54/785 [00:16<03:32,  3.44it/s]  7%|▋         | 55/785 [00:16<03:31,  3.45it/s]  7%|▋         | 56/785 [00:16<03:30,  3.46it/s]  7%|▋         | 57/785 [00:17<03:29,  3.47it/s]  7%|▋         | 58/785 [00:17<03:29,  3.47it/s]  8%|▊         | 59/785 [00:17<03:28,  3.48it/s]  8%|▊         | 60/785 [00:17<03:28,  3.48it/s]  8%|▊         | 61/785 [00:18<03:28,  3.48it/s]  8%|▊         | 62/785 [00:18<03:27,  3.48it/s]  8%|▊         | 63/785 [00:18<03:27,  3.48it/s]  8%|▊         | 64/785 [00:19<03:27,  3.48it/s]  8%|▊         | 65/785 [00:19<03:26,  3.48it/s]  8%|▊         | 66/785 [00:19<03:43,  3.22it/s]  9%|▊         | 67/785 [00:19<03:37,  3.29it/s]  9%|▊         | 68/785 [00:20<03:34,  3.35it/s]  9%|▉         | 69/785 [00:20<03:31,  3.39it/s]  9%|▉         | 70/785 [00:20<03:29,  3.41it/s]  9%|▉         | 71/785 [00:21<03:27,  3.44it/s]  9%|▉         | 72/785 [00:21<03:26,  3.45it/s]  9%|▉         | 73/785 [00:21<03:25,  3.46it/s]  9%|▉         | 74/785 [00:21<03:25,  3.47it/s] 10%|▉         | 75/785 [00:22<03:24,  3.47it/s] 10%|▉         | 76/785 [00:22<03:24,  3.47it/s] 10%|▉         | 77/785 [00:22<03:23,  3.48it/s] 10%|▉         | 78/785 [00:23<03:23,  3.48it/s] 10%|█         | 79/785 [00:23<03:22,  3.48it/s] 10%|█         | 80/785 [00:23<03:22,  3.48it/s] 10%|█         | 81/785 [00:23<03:22,  3.48it/s] 10%|█         | 82/785 [00:24<03:22,  3.48it/s] 11%|█         | 83/785 [00:24<03:21,  3.48it/s] 11%|█         | 84/785 [00:24<03:27,  3.38it/s] 11%|█         | 85/785 [00:25<03:25,  3.41it/s] 11%|█         | 86/785 [00:25<03:23,  3.43it/s] 11%|█         | 87/785 [00:25<03:22,  3.45it/s] 11%|█         | 88/785 [00:26<03:21,  3.46it/s] 11%|█▏        | 89/785 [00:26<03:20,  3.46it/s] 11%|█▏        | 90/785 [00:26<03:20,  3.47it/s] 12%|█▏        | 91/785 [00:26<03:19,  3.47it/s] 12%|█▏        | 92/785 [00:27<03:19,  3.48it/s] 12%|█▏        | 93/785 [00:27<03:19,  3.48it/s] 12%|█▏        | 94/785 [00:27<03:18,  3.48it/s] 12%|█▏        | 95/785 [00:28<03:18,  3.48it/s] 12%|█▏        | 96/785 [00:28<03:18,  3.48it/s] 12%|█▏        | 97/785 [00:28<03:17,  3.48it/s] 12%|█▏        | 98/785 [00:28<03:17,  3.48it/s] 13%|█▎        | 99/785 [00:29<03:17,  3.48it/s] 13%|█▎        | 100/785 [00:29<03:16,  3.48it/s] 13%|█▎        | 101/785 [00:29<03:22,  3.38it/s] 13%|█▎        | 102/785 [00:30<03:20,  3.41it/s] 13%|█▎        | 103/785 [00:30<03:18,  3.43it/s] 13%|█▎        | 104/785 [00:30<03:17,  3.45it/s] 13%|█▎        | 105/785 [00:30<03:16,  3.45it/s] 14%|█▎        | 106/785 [00:31<03:16,  3.46it/s] 14%|█▎        | 107/785 [00:31<03:15,  3.47it/s] 14%|█▍        | 108/785 [00:31<03:15,  3.47it/s] 14%|█▍        | 109/785 [00:32<03:14,  3.47it/s] 14%|█▍        | 110/785 [00:32<03:14,  3.47it/s] 14%|█▍        | 111/785 [00:32<03:14,  3.47it/s] 14%|█▍        | 112/785 [00:32<03:13,  3.48it/s] 14%|█▍        | 113/785 [00:33<03:13,  3.48it/s] 15%|█▍        | 114/785 [00:33<03:13,  3.48it/s] 15%|█▍        | 115/785 [00:33<03:12,  3.48it/s] 15%|█▍        | 116/785 [00:34<03:12,  3.48it/s] 15%|█▍        | 117/785 [00:34<03:12,  3.48it/s] 15%|█▌        | 118/785 [00:34<03:11,  3.48it/s] 15%|█▌        | 119/785 [00:35<03:22,  3.29it/s] 15%|█▌        | 120/785 [00:35<03:18,  3.35it/s] 15%|█▌        | 121/785 [00:35<03:16,  3.38it/s] 16%|█▌        | 122/785 [00:35<03:14,  3.41it/s] 16%|█▌        | 123/785 [00:36<03:12,  3.43it/s] 16%|█▌        | 124/785 [00:36<03:11,  3.45it/s] 16%|█▌        | 125/785 [00:36<03:11,  3.45it/s] 16%|█▌        | 126/785 [00:37<03:10,  3.45it/s] 16%|█▌        | 127/785 [00:37<03:10,  3.46it/s] 16%|█▋        | 128/785 [00:37<03:09,  3.46it/s] 16%|█▋        | 129/785 [00:37<03:09,  3.47it/s] 17%|█▋        | 130/785 [00:38<03:17,  3.32it/s] 17%|█▋        | 131/785 [00:38<03:15,  3.35it/s] 17%|█▋        | 132/785 [00:38<03:12,  3.39it/s] 17%|█▋        | 133/785 [00:39<03:10,  3.42it/s] 17%|█▋        | 134/785 [00:39<03:09,  3.43it/s] 17%|█▋        | 135/785 [00:39<03:08,  3.45it/s] 17%|█▋        | 136/785 [00:40<03:15,  3.31it/s] 17%|█▋        | 137/785 [00:40<03:12,  3.36it/s] 18%|█▊        | 138/785 [00:40<03:10,  3.40it/s] 18%|█▊        | 139/785 [00:41<04:20,  2.48it/s] 18%|█▊        | 140/785 [00:41<03:58,  2.71it/s] 18%|█▊        | 141/785 [00:41<03:42,  2.90it/s] 18%|█▊        | 142/785 [00:42<03:42,  2.89it/s] 18%|█▊        | 143/785 [00:42<03:31,  3.04it/s] 18%|█▊        | 144/785 [00:42<03:22,  3.16it/s] 18%|█▊        | 145/785 [00:43<03:17,  3.25it/s] 19%|█▊        | 146/785 [00:43<03:12,  3.31it/s] 19%|█▊        | 147/785 [00:43<03:09,  3.36it/s] 19%|█▉        | 148/785 [00:43<03:07,  3.39it/s] 19%|█▉        | 149/785 [00:44<03:06,  3.42it/s] 19%|█▉        | 150/785 [00:44<03:05,  3.43it/s] 19%|█▉        | 151/785 [00:44<03:04,  3.44it/s] 19%|█▉        | 152/785 [00:45<03:03,  3.45it/s] 19%|█▉        | 153/785 [00:45<03:10,  3.32it/s] 20%|█▉        | 154/785 [00:45<03:07,  3.36it/s] 20%|█▉        | 155/785 [00:45<03:05,  3.39it/s] 20%|█▉        | 156/785 [00:46<03:04,  3.42it/s] 20%|██        | 157/785 [00:46<02:55,  3.59it/s][INFO|trainer.py:2140] 2023-08-29 11:02:18,882 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:02:18,882 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 11:02:18,882 >>   Batch size = 8

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.87it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.75it/s][A
  3%|▎         | 18/521 [00:00<00:10, 48.94it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.14it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.78it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.55it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.45it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.14it/s][A
  9%|▉         | 48/521 [00:00<00:10, 47.03it/s][A
 10%|█         | 53/521 [00:01<00:09, 47.06it/s][A
 11%|█         | 58/521 [00:01<00:09, 47.06it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.96it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 47.04it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.91it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.97it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 47.04it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 47.01it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.88it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.94it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 47.00it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.79it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 47.02it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.97it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.87it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.94it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.99it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.91it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.89it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.93it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.89it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.96it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.92it/s][A
 32%|███▏      | 168/521 [00:03<00:08, 43.39it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 44.32it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 45.09it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 45.67it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 46.09it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.37it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.47it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.71it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.66it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.68it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.71it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.75it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.81it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 46.90it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.85it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.85it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.96it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.93it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.85it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.77it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.77it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.80it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.92it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.95it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.93it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.97it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.98it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.94it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 46.89it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.75it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.88it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 46.92it/s][A
 63%|██████▎   | 328/521 [00:06<00:04, 46.91it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.95it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.92it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.93it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.98it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.89it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.71it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.85it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.85it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 46.84it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.93it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.88it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.84it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.87it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.92it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.78it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 45.17it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 45.79it/s][A
 80%|████████  | 418/521 [00:08<00:02, 46.09it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.39it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.60it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.69it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.79it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 46.88it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 46.69it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 46.66it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.70it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 46.78it/s][A
 90%|████████▉ | 468/521 [00:09<00:01, 46.83it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.93it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.90it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.93it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.95it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.88it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.70it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.79it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.81it/s][A
 98%|█████████▊| 513/521 [00:10<00:00, 46.76it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.84it/s][A                                                 
                                                 [A 20%|██        | 157/785 [00:57<02:55,  3.59it/s]
100%|██████████| 521/521 [00:11<00:00, 46.84it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:02:30,338 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-157
[INFO|configuration_utils.py:351] 2023-08-29 11:02:30,545 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-157/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:02:34,279 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-157/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:02:34,448 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-157/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:02:34,553 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-157/special_tokens_map.json
 20%|██        | 158/785 [01:11<1:21:01,  7.75s/it] 20%|██        | 159/785 [01:12<57:47,  5.54s/it]   20%|██        | 160/785 [01:12<41:17,  3.96s/it] 21%|██        | 161/785 [01:12<29:44,  2.86s/it] 21%|██        | 162/785 [01:12<21:41,  2.09s/it] 21%|██        | 163/785 [01:13<16:02,  1.55s/it] 21%|██        | 164/785 [01:13<12:06,  1.17s/it] 21%|██        | 165/785 [01:13<09:21,  1.10it/s] 21%|██        | 166/785 [01:14<07:25,  1.39it/s] 21%|██▏       | 167/785 [01:14<06:04,  1.70it/s] 21%|██▏       | 168/785 [01:14<05:07,  2.00it/s] 22%|██▏       | 169/785 [01:14<04:28,  2.30it/s] 22%|██▏       | 170/785 [01:15<04:16,  2.40it/s] 22%|██▏       | 171/785 [01:15<03:52,  2.65it/s] 22%|██▏       | 172/785 [01:15<03:35,  2.85it/s] 22%|██▏       | 173/785 [01:16<03:23,  3.01it/s] 22%|██▏       | 174/785 [01:16<03:14,  3.14it/s] 22%|██▏       | 175/785 [01:16<03:08,  3.23it/s] 22%|██▏       | 176/785 [01:17<03:04,  3.30it/s] 23%|██▎       | 177/785 [01:17<03:01,  3.35it/s] 23%|██▎       | 178/785 [01:17<02:59,  3.39it/s] 23%|██▎       | 179/785 [01:17<02:57,  3.41it/s] 23%|██▎       | 180/785 [01:18<02:56,  3.43it/s] 23%|██▎       | 181/785 [01:18<03:00,  3.35it/s] 23%|██▎       | 182/785 [01:18<02:58,  3.39it/s] 23%|██▎       | 183/785 [01:19<02:56,  3.41it/s] 23%|██▎       | 184/785 [01:19<02:55,  3.43it/s] 24%|██▎       | 185/785 [01:19<02:54,  3.44it/s] 24%|██▎       | 186/785 [01:19<02:53,  3.45it/s] 24%|██▍       | 187/785 [01:20<02:52,  3.46it/s] 24%|██▍       | 188/785 [01:20<02:52,  3.46it/s] 24%|██▍       | 189/785 [01:20<02:51,  3.47it/s] 24%|██▍       | 190/785 [01:21<02:51,  3.47it/s] 24%|██▍       | 191/785 [01:21<02:51,  3.47it/s] 24%|██▍       | 192/785 [01:21<03:01,  3.28it/s] 25%|██▍       | 193/785 [01:22<02:57,  3.33it/s] 25%|██▍       | 194/785 [01:22<02:55,  3.37it/s] 25%|██▍       | 195/785 [01:22<02:53,  3.40it/s] 25%|██▍       | 196/785 [01:22<02:51,  3.42it/s] 25%|██▌       | 197/785 [01:23<02:50,  3.44it/s] 25%|██▌       | 198/785 [01:23<02:50,  3.45it/s] 25%|██▌       | 199/785 [01:23<02:49,  3.46it/s] 25%|██▌       | 200/785 [01:24<02:48,  3.46it/s] 26%|██▌       | 201/785 [01:24<02:48,  3.47it/s] 26%|██▌       | 202/785 [01:24<02:48,  3.47it/s] 26%|██▌       | 203/785 [01:24<02:54,  3.34it/s] 26%|██▌       | 204/785 [01:25<02:51,  3.38it/s] 26%|██▌       | 205/785 [01:25<02:50,  3.41it/s] 26%|██▌       | 206/785 [01:25<02:48,  3.43it/s] 26%|██▋       | 207/785 [01:26<02:47,  3.44it/s] 26%|██▋       | 208/785 [01:26<02:47,  3.45it/s] 27%|██▋       | 209/785 [01:26<02:46,  3.46it/s] 27%|██▋       | 210/785 [01:26<02:46,  3.46it/s] 27%|██▋       | 211/785 [01:27<02:45,  3.46it/s] 27%|██▋       | 212/785 [01:27<02:45,  3.47it/s] 27%|██▋       | 213/785 [01:27<02:44,  3.47it/s] 27%|██▋       | 214/785 [01:28<02:52,  3.31it/s] 27%|██▋       | 215/785 [01:28<02:49,  3.36it/s] 28%|██▊       | 216/785 [01:28<02:47,  3.39it/s] 28%|██▊       | 217/785 [01:28<02:46,  3.41it/s] 28%|██▊       | 218/785 [01:29<02:45,  3.43it/s] 28%|██▊       | 219/785 [01:29<02:44,  3.45it/s] 28%|██▊       | 220/785 [01:29<02:43,  3.45it/s] 28%|██▊       | 221/785 [01:30<02:42,  3.46it/s] 28%|██▊       | 222/785 [01:30<02:42,  3.46it/s] 28%|██▊       | 223/785 [01:30<02:42,  3.47it/s] 29%|██▊       | 224/785 [01:31<02:41,  3.47it/s] 29%|██▊       | 225/785 [01:31<02:48,  3.32it/s] 29%|██▉       | 226/785 [01:31<02:46,  3.37it/s] 29%|██▉       | 227/785 [01:31<02:44,  3.40it/s] 29%|██▉       | 228/785 [01:32<02:42,  3.42it/s] 29%|██▉       | 229/785 [01:32<02:55,  3.17it/s] 29%|██▉       | 230/785 [01:32<02:55,  3.17it/s] 29%|██▉       | 231/785 [01:33<02:50,  3.25it/s] 30%|██▉       | 232/785 [01:33<02:46,  3.32it/s] 30%|██▉       | 233/785 [01:33<02:44,  3.36it/s] 30%|██▉       | 234/785 [01:34<02:42,  3.39it/s] 30%|██▉       | 235/785 [01:34<02:54,  3.16it/s] 30%|███       | 236/785 [01:34<02:49,  3.25it/s] 30%|███       | 237/785 [01:34<02:45,  3.31it/s] 30%|███       | 238/785 [01:35<02:42,  3.36it/s] 30%|███       | 239/785 [01:35<02:40,  3.39it/s] 31%|███       | 240/785 [01:35<02:39,  3.42it/s] 31%|███       | 241/785 [01:36<02:38,  3.43it/s] 31%|███       | 242/785 [01:36<02:37,  3.45it/s] 31%|███       | 243/785 [01:36<02:36,  3.45it/s] 31%|███       | 244/785 [01:36<02:36,  3.46it/s] 31%|███       | 245/785 [01:37<02:35,  3.46it/s] 31%|███▏      | 246/785 [01:37<02:40,  3.36it/s] 31%|███▏      | 247/785 [01:37<02:38,  3.39it/s] 32%|███▏      | 248/785 [01:38<02:37,  3.42it/s] 32%|███▏      | 249/785 [01:38<02:41,  3.33it/s] 32%|███▏      | 250/785 [01:38<02:39,  3.36it/s] 32%|███▏      | 251/785 [01:39<02:37,  3.39it/s] 32%|███▏      | 252/785 [01:39<02:36,  3.41it/s] 32%|███▏      | 253/785 [01:39<02:34,  3.43it/s] 32%|███▏      | 254/785 [01:39<02:34,  3.45it/s] 32%|███▏      | 255/785 [01:40<02:33,  3.45it/s] 33%|███▎      | 256/785 [01:40<02:32,  3.46it/s] 33%|███▎      | 257/785 [01:40<02:32,  3.46it/s] 33%|███▎      | 258/785 [01:41<03:50,  2.29it/s] 33%|███▎      | 259/785 [01:41<03:35,  2.44it/s] 33%|███▎      | 260/785 [01:42<03:15,  2.68it/s] 33%|███▎      | 261/785 [01:42<03:02,  2.88it/s] 33%|███▎      | 262/785 [01:42<02:52,  3.03it/s] 34%|███▎      | 263/785 [01:43<02:45,  3.15it/s] 34%|███▎      | 264/785 [01:43<02:40,  3.24it/s] 34%|███▍      | 265/785 [01:43<02:37,  3.30it/s] 34%|███▍      | 266/785 [01:43<02:34,  3.35it/s] 34%|███▍      | 267/785 [01:44<02:33,  3.39it/s] 34%|███▍      | 268/785 [01:44<02:31,  3.41it/s] 34%|███▍      | 269/785 [01:44<02:30,  3.43it/s] 34%|███▍      | 270/785 [01:45<02:29,  3.44it/s] 35%|███▍      | 271/785 [01:45<02:29,  3.44it/s] 35%|███▍      | 272/785 [01:45<02:28,  3.45it/s] 35%|███▍      | 273/785 [01:45<02:28,  3.46it/s] 35%|███▍      | 274/785 [01:46<02:27,  3.46it/s] 35%|███▌      | 275/785 [01:46<02:36,  3.26it/s] 35%|███▌      | 276/785 [01:46<02:33,  3.32it/s] 35%|███▌      | 277/785 [01:47<02:30,  3.37it/s] 35%|███▌      | 278/785 [01:47<02:29,  3.40it/s] 36%|███▌      | 279/785 [01:47<02:28,  3.42it/s] 36%|███▌      | 280/785 [01:48<02:27,  3.43it/s] 36%|███▌      | 281/785 [01:48<02:26,  3.45it/s] 36%|███▌      | 282/785 [01:48<02:25,  3.45it/s] 36%|███▌      | 283/785 [01:48<02:25,  3.46it/s] 36%|███▌      | 284/785 [01:49<02:24,  3.47it/s] 36%|███▋      | 285/785 [01:49<02:24,  3.47it/s] 36%|███▋      | 286/785 [01:49<02:23,  3.47it/s] 37%|███▋      | 287/785 [01:50<02:23,  3.47it/s] 37%|███▋      | 288/785 [01:50<02:23,  3.47it/s] 37%|███▋      | 289/785 [01:50<02:22,  3.47it/s] 37%|███▋      | 290/785 [01:50<02:22,  3.47it/s] 37%|███▋      | 291/785 [01:51<02:22,  3.47it/s] 37%|███▋      | 292/785 [01:51<02:21,  3.47it/s] 37%|███▋      | 293/785 [01:51<02:27,  3.34it/s] 37%|███▋      | 294/785 [01:52<02:25,  3.38it/s] 38%|███▊      | 295/785 [01:52<02:23,  3.41it/s] 38%|███▊      | 296/785 [01:52<02:22,  3.43it/s] 38%|███▊      | 297/785 [01:52<02:21,  3.44it/s] 38%|███▊      | 298/785 [01:53<02:21,  3.45it/s] 38%|███▊      | 299/785 [01:53<02:20,  3.46it/s] 38%|███▊      | 300/785 [01:53<02:20,  3.46it/s] 38%|███▊      | 301/785 [01:54<02:19,  3.46it/s] 38%|███▊      | 302/785 [01:54<02:19,  3.47it/s] 39%|███▊      | 303/785 [01:54<02:19,  3.47it/s] 39%|███▊      | 304/785 [01:54<02:18,  3.47it/s] 39%|███▉      | 305/785 [01:55<02:18,  3.47it/s] 39%|███▉      | 306/785 [01:55<02:17,  3.47it/s] 39%|███▉      | 307/785 [01:55<02:17,  3.47it/s] 39%|███▉      | 308/785 [01:56<02:17,  3.47it/s] 39%|███▉      | 309/785 [01:56<02:17,  3.47it/s] 39%|███▉      | 310/785 [01:56<02:16,  3.47it/s] 40%|███▉      | 311/785 [01:57<02:20,  3.37it/s] 40%|███▉      | 312/785 [01:57<02:19,  3.40it/s] 40%|███▉      | 313/785 [01:57<02:17,  3.42it/s] 40%|████      | 314/785 [01:57<02:06,  3.72it/s][INFO|trainer.py:2140] 2023-08-29 11:03:30,229 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:03:30,229 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 11:03:30,229 >>   Batch size = 8
{'eval_loss': 0.9590339660644531, 'eval_runtime': 11.1461, 'eval_samples_per_second': 373.674, 'eval_steps_per_second': 46.743, 'epoch': 1.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.41it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.81it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.00it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.18it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.79it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.45it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.24it/s][A
  8%|▊         | 43/521 [00:00<00:10, 45.58it/s][A
  9%|▉         | 48/521 [00:01<00:10, 46.00it/s][A
 10%|█         | 53/521 [00:01<00:10, 46.23it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.46it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.65it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.79it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.84it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.88it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.72it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.68it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.72it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.66it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.75it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.73it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.80it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.86it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.94it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.82it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.84it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.83it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.77it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.77it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.84it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.82it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.85it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.84it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.90it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.82it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 44.08it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 44.93it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 45.50it/s][A
 38%|███▊      | 198/521 [00:04<00:07, 45.93it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.25it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.45it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.66it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.71it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.52it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.53it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 46.62it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.68it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.73it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.81it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.89it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.90it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.92it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.77it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.66it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.65it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.75it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.78it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.76it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.84it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.86it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 46.89it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.88it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.67it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 46.69it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 46.75it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.74it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.72it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.85it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.86it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.81it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.86it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.76it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.74it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 46.75it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.77it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.77it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.81it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.82it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.82it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.84it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.83it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.71it/s][A
 80%|████████  | 418/521 [00:08<00:02, 46.72it/s][A
 81%|████████  | 423/521 [00:09<00:02, 42.26it/s][A
 82%|████████▏ | 428/521 [00:09<00:02, 43.56it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 44.52it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 45.20it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 45.75it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 46.14it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 46.32it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.56it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 46.27it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 46.32it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.44it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.52it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.66it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.79it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.78it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.80it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.89it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.84it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 46.63it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 41.39it/s][A                                                 
                                                 [A 40%|████      | 314/785 [02:09<02:06,  3.72it/s]
100%|██████████| 521/521 [00:11<00:00, 41.39it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:03:41,773 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-314
[INFO|configuration_utils.py:351] 2023-08-29 11:03:42,023 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-314/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:03:47,128 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-314/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:03:47,522 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-314/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:03:47,713 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-314/special_tokens_map.json
 40%|████      | 315/785 [02:28<1:12:35,  9.27s/it] 40%|████      | 316/785 [02:28<51:33,  6.60s/it]   40%|████      | 317/785 [02:28<36:40,  4.70s/it] 41%|████      | 318/785 [02:29<26:17,  3.38s/it] 41%|████      | 319/785 [02:29<19:01,  2.45s/it] 41%|████      | 320/785 [02:29<13:57,  1.80s/it] 41%|████      | 321/785 [02:29<10:24,  1.35s/it] 41%|████      | 322/785 [02:30<07:56,  1.03s/it] 41%|████      | 323/785 [02:30<06:12,  1.24it/s] 41%|████▏     | 324/785 [02:30<04:59,  1.54it/s] 41%|████▏     | 325/785 [02:31<04:08,  1.85it/s] 42%|████▏     | 326/785 [02:31<03:33,  2.15it/s] 42%|████▏     | 327/785 [02:31<03:13,  2.37it/s] 42%|████▏     | 328/785 [02:31<02:54,  2.62it/s] 42%|████▏     | 329/785 [02:32<02:41,  2.83it/s] 42%|████▏     | 330/785 [02:32<02:31,  3.00it/s] 42%|████▏     | 331/785 [02:32<02:25,  3.13it/s] 42%|████▏     | 332/785 [02:33<02:20,  3.23it/s] 42%|████▏     | 333/785 [02:33<02:16,  3.30it/s] 43%|████▎     | 334/785 [02:33<02:14,  3.35it/s] 43%|████▎     | 335/785 [02:33<02:12,  3.39it/s] 43%|████▎     | 336/785 [02:34<02:11,  3.42it/s] 43%|████▎     | 337/785 [02:34<02:10,  3.44it/s] 43%|████▎     | 338/785 [02:34<02:14,  3.32it/s] 43%|████▎     | 339/785 [02:35<02:12,  3.37it/s] 43%|████▎     | 340/785 [02:35<02:10,  3.40it/s] 43%|████▎     | 341/785 [02:35<02:09,  3.43it/s] 44%|████▎     | 342/785 [02:35<02:08,  3.44it/s] 44%|████▎     | 343/785 [02:36<02:07,  3.45it/s] 44%|████▍     | 344/785 [02:36<02:07,  3.46it/s] 44%|████▍     | 345/785 [02:36<02:06,  3.47it/s] 44%|████▍     | 346/785 [02:37<02:06,  3.47it/s] 44%|████▍     | 347/785 [02:37<02:06,  3.47it/s] 44%|████▍     | 348/785 [02:37<02:05,  3.48it/s] 44%|████▍     | 349/785 [02:38<02:16,  3.20it/s] 45%|████▍     | 350/785 [02:38<02:12,  3.28it/s] 45%|████▍     | 351/785 [02:38<02:16,  3.18it/s] 45%|████▍     | 352/785 [02:38<02:12,  3.26it/s] 45%|████▍     | 353/785 [02:39<02:09,  3.32it/s] 45%|████▌     | 354/785 [02:39<02:07,  3.37it/s] 45%|████▌     | 355/785 [02:39<02:15,  3.17it/s] 45%|████▌     | 356/785 [02:40<02:11,  3.26it/s] 45%|████▌     | 357/785 [02:40<02:08,  3.32it/s] 46%|████▌     | 358/785 [02:40<02:06,  3.37it/s] 46%|████▌     | 359/785 [02:41<02:05,  3.40it/s] 46%|████▌     | 360/785 [02:41<02:57,  2.40it/s] 46%|████▌     | 361/785 [02:42<02:40,  2.64it/s] 46%|████▌     | 362/785 [02:42<02:28,  2.84it/s] 46%|████▌     | 363/785 [02:42<02:20,  3.01it/s] 46%|████▋     | 364/785 [02:42<02:19,  3.01it/s] 46%|████▋     | 365/785 [02:43<02:13,  3.13it/s] 47%|████▋     | 366/785 [02:43<02:09,  3.23it/s] 47%|████▋     | 367/785 [02:43<02:06,  3.30it/s] 47%|████▋     | 368/785 [02:44<02:04,  3.35it/s] 47%|████▋     | 369/785 [02:44<02:02,  3.38it/s] 47%|████▋     | 370/785 [02:44<02:01,  3.41it/s] 47%|████▋     | 371/785 [02:44<02:00,  3.43it/s] 47%|████▋     | 372/785 [02:45<01:59,  3.45it/s] 48%|████▊     | 373/785 [02:45<01:59,  3.46it/s] 48%|████▊     | 374/785 [02:45<01:58,  3.46it/s] 48%|████▊     | 375/785 [02:46<02:02,  3.36it/s] 48%|████▊     | 376/785 [02:46<02:00,  3.40it/s] 48%|████▊     | 377/785 [02:46<01:59,  3.42it/s] 48%|████▊     | 378/785 [02:47<01:58,  3.44it/s] 48%|████▊     | 379/785 [02:47<01:57,  3.45it/s] 48%|████▊     | 380/785 [02:47<01:57,  3.46it/s] 49%|████▊     | 381/785 [02:47<01:56,  3.47it/s] 49%|████▊     | 382/785 [02:48<01:56,  3.47it/s] 49%|████▉     | 383/785 [02:48<01:55,  3.47it/s] 49%|████▉     | 384/785 [02:48<01:55,  3.48it/s] 49%|████▉     | 385/785 [02:49<01:54,  3.48it/s] 49%|████▉     | 386/785 [02:49<02:01,  3.29it/s] 49%|████▉     | 387/785 [02:49<01:58,  3.35it/s] 49%|████▉     | 388/785 [02:49<01:57,  3.39it/s] 50%|████▉     | 389/785 [02:50<01:55,  3.41it/s] 50%|████▉     | 390/785 [02:50<01:55,  3.43it/s] 50%|████▉     | 391/785 [02:50<01:54,  3.45it/s] 50%|████▉     | 392/785 [02:51<01:53,  3.46it/s] 50%|█████     | 393/785 [02:51<01:53,  3.47it/s] 50%|█████     | 394/785 [02:51<01:52,  3.47it/s] 50%|█████     | 395/785 [02:51<01:52,  3.48it/s] 50%|█████     | 396/785 [02:52<01:51,  3.48it/s] 51%|█████     | 397/785 [02:52<01:56,  3.33it/s] 51%|█████     | 398/785 [02:52<01:54,  3.38it/s] 51%|█████     | 399/785 [02:53<01:53,  3.41it/s] 51%|█████     | 400/785 [02:53<01:52,  3.43it/s] 51%|█████     | 401/785 [02:53<01:51,  3.44it/s] 51%|█████     | 402/785 [02:54<01:50,  3.45it/s] 51%|█████▏    | 403/785 [02:54<01:50,  3.46it/s] 51%|█████▏    | 404/785 [02:54<01:49,  3.47it/s] 52%|█████▏    | 405/785 [02:54<01:49,  3.47it/s] 52%|█████▏    | 406/785 [02:55<01:49,  3.47it/s] 52%|█████▏    | 407/785 [02:55<01:48,  3.48it/s] 52%|█████▏    | 408/785 [02:55<01:55,  3.26it/s] 52%|█████▏    | 409/785 [02:56<01:53,  3.32it/s] 52%|█████▏    | 410/785 [02:56<01:51,  3.37it/s] 52%|█████▏    | 411/785 [02:56<01:49,  3.40it/s] 52%|█████▏    | 412/785 [02:56<01:48,  3.42it/s] 53%|█████▎    | 413/785 [02:57<01:48,  3.44it/s] 53%|█████▎    | 414/785 [02:57<01:47,  3.45it/s] 53%|█████▎    | 415/785 [02:57<01:46,  3.46it/s] 53%|█████▎    | 416/785 [02:58<01:46,  3.47it/s] 53%|█████▎    | 417/785 [02:58<01:46,  3.47it/s] 53%|█████▎    | 418/785 [02:58<01:45,  3.47it/s] 53%|█████▎    | 419/785 [02:59<01:50,  3.32it/s] 54%|█████▎    | 420/785 [02:59<01:48,  3.37it/s] 54%|█████▎    | 421/785 [02:59<01:50,  3.30it/s] 54%|█████▍    | 422/785 [02:59<01:48,  3.35it/s] 54%|█████▍    | 423/785 [03:00<01:46,  3.39it/s] 54%|█████▍    | 424/785 [03:00<01:45,  3.41it/s] 54%|█████▍    | 425/785 [03:00<01:44,  3.43it/s] 54%|█████▍    | 426/785 [03:01<01:44,  3.45it/s] 54%|█████▍    | 427/785 [03:01<01:43,  3.45it/s] 55%|█████▍    | 428/785 [03:01<01:43,  3.46it/s] 55%|█████▍    | 429/785 [03:01<01:42,  3.47it/s] 55%|█████▍    | 430/785 [03:02<01:45,  3.37it/s] 55%|█████▍    | 431/785 [03:02<01:44,  3.40it/s] 55%|█████▌    | 432/785 [03:02<01:42,  3.43it/s] 55%|█████▌    | 433/785 [03:03<01:42,  3.44it/s] 55%|█████▌    | 434/785 [03:03<01:41,  3.45it/s] 55%|█████▌    | 435/785 [03:03<01:41,  3.46it/s] 56%|█████▌    | 436/785 [03:03<01:40,  3.46it/s] 56%|█████▌    | 437/785 [03:04<01:40,  3.47it/s] 56%|█████▌    | 438/785 [03:04<01:39,  3.47it/s] 56%|█████▌    | 439/785 [03:04<01:39,  3.47it/s] 56%|█████▌    | 440/785 [03:05<01:39,  3.47it/s] 56%|█████▌    | 441/785 [03:05<01:42,  3.34it/s] 56%|█████▋    | 442/785 [03:05<01:41,  3.38it/s] 56%|█████▋    | 443/785 [03:06<01:40,  3.41it/s] 57%|█████▋    | 444/785 [03:06<01:39,  3.43it/s] 57%|█████▋    | 445/785 [03:06<01:38,  3.45it/s] 57%|█████▋    | 446/785 [03:06<01:38,  3.46it/s] 57%|█████▋    | 447/785 [03:07<01:37,  3.46it/s] 57%|█████▋    | 448/785 [03:07<01:37,  3.47it/s] 57%|█████▋    | 449/785 [03:07<01:36,  3.47it/s] 57%|█████▋    | 450/785 [03:08<01:36,  3.47it/s] 57%|█████▋    | 451/785 [03:08<01:36,  3.47it/s] 58%|█████▊    | 452/785 [03:08<01:40,  3.31it/s] 58%|█████▊    | 453/785 [03:08<01:38,  3.36it/s] 58%|█████▊    | 454/785 [03:09<01:37,  3.39it/s] 58%|█████▊    | 455/785 [03:09<01:36,  3.42it/s] 58%|█████▊    | 456/785 [03:09<01:35,  3.43it/s] 58%|█████▊    | 457/785 [03:10<01:35,  3.45it/s] 58%|█████▊    | 458/785 [03:10<01:34,  3.46it/s] 58%|█████▊    | 459/785 [03:10<01:34,  3.46it/s] 59%|█████▊    | 460/785 [03:10<01:33,  3.47it/s] 59%|█████▊    | 461/785 [03:11<01:33,  3.47it/s] 59%|█████▉    | 462/785 [03:11<01:33,  3.47it/s] 59%|█████▉    | 463/785 [03:11<01:32,  3.47it/s] 59%|█████▉    | 464/785 [03:12<01:32,  3.47it/s] 59%|█████▉    | 465/785 [03:12<01:32,  3.47it/s] 59%|█████▉    | 466/785 [03:12<01:31,  3.47it/s] 59%|█████▉    | 467/785 [03:12<01:31,  3.47it/s] 60%|█████▉    | 468/785 [03:13<01:31,  3.47it/s] 60%|█████▉    | 469/785 [03:13<01:30,  3.48it/s] 60%|█████▉    | 470/785 [03:13<01:37,  3.22it/s] 60%|██████    | 471/785 [03:14<01:28,  3.55it/s][INFO|trainer.py:2140] 2023-08-29 11:04:46,517 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:04:46,517 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 11:04:46,517 >>   Batch size = 8
{'eval_loss': 0.9654185771942139, 'eval_runtime': 11.2413, 'eval_samples_per_second': 370.509, 'eval_steps_per_second': 46.347, 'epoch': 2.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.84it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.71it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.00it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.32it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.84it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.49it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.23it/s][A
  8%|▊         | 43/521 [00:00<00:10, 46.82it/s][A
  9%|▉         | 48/521 [00:01<00:10, 46.66it/s][A
 10%|█         | 53/521 [00:01<00:10, 46.73it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.78it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.92it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.97it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 47.02it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.91it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.87it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.75it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.60it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.59it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.64it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.77it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.83it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.91it/s][A
 24%|██▎       | 123/521 [00:02<00:10, 38.20it/s][A
 25%|██▍       | 128/521 [00:02<00:09, 40.46it/s][A
 26%|██▌       | 133/521 [00:02<00:09, 42.18it/s][A
 26%|██▋       | 138/521 [00:03<00:08, 43.56it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 44.52it/s][A
 28%|██▊       | 148/521 [00:03<00:08, 45.25it/s][A
 29%|██▉       | 153/521 [00:03<00:08, 45.75it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.17it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.01it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.15it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.31it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.51it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.62it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 46.79it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.77it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.77it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.84it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.73it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.64it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.66it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.62it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.68it/s][A
 45%|████▍     | 233/521 [00:05<00:06, 46.74it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.81it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.89it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.79it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.84it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.81it/s][A
 50%|█████     | 263/521 [00:05<00:06, 39.00it/s][A
 51%|█████▏    | 268/521 [00:05<00:06, 41.02it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 42.68it/s][A
 53%|█████▎    | 278/521 [00:06<00:05, 43.90it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 44.77it/s][A
 55%|█████▌    | 288/521 [00:06<00:05, 45.41it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 45.82it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.11it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.11it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 46.21it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.35it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.43it/s][A
 62%|██████▏   | 323/521 [00:07<00:04, 46.57it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 46.70it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.83it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.83it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.70it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.59it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.54it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.61it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.55it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.60it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 46.75it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.84it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.86it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.86it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.58it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.53it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 39.93it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 41.82it/s][A
 79%|███████▉  | 413/521 [00:09<00:02, 43.26it/s][A
 80%|████████  | 418/521 [00:09<00:02, 44.30it/s][A
 81%|████████  | 423/521 [00:09<00:02, 45.12it/s][A
 82%|████████▏ | 428/521 [00:09<00:02, 45.71it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.10it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.38it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 45.77it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 46.01it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 46.23it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.42it/s][A
 89%|████████▉ | 463/521 [00:10<00:01, 46.57it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 46.69it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.76it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.90it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.93it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.72it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.45it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.54it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.61it/s][A
 98%|█████████▊| 508/521 [00:11<00:00, 46.69it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 46.73it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.75it/s][A                                                 
                                                 [A 60%|██████    | 471/785 [03:25<01:28,  3.55it/s]
100%|██████████| 521/521 [00:11<00:00, 46.75it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:04:58,028 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-471
[INFO|configuration_utils.py:351] 2023-08-29 11:04:58,178 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-471/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:05:01,872 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-471/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:05:02,036 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-471/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:05:02,128 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-471/special_tokens_map.json
 60%|██████    | 472/785 [03:38<38:47,  7.44s/it] 60%|██████    | 473/785 [03:38<27:33,  5.30s/it] 60%|██████    | 474/785 [03:38<19:42,  3.80s/it] 61%|██████    | 475/785 [03:39<14:12,  2.75s/it] 61%|██████    | 476/785 [03:39<10:21,  2.01s/it] 61%|██████    | 477/785 [03:39<07:39,  1.49s/it] 61%|██████    | 478/785 [03:40<05:47,  1.13s/it] 61%|██████    | 479/785 [03:40<04:28,  1.14it/s] 61%|██████    | 480/785 [03:40<03:33,  1.43it/s] 61%|██████▏   | 481/785 [03:40<02:55,  1.73it/s] 61%|██████▏   | 482/785 [03:41<02:28,  2.04it/s] 62%|██████▏   | 483/785 [03:41<02:26,  2.06it/s] 62%|██████▏   | 484/785 [03:42<02:19,  2.16it/s] 62%|██████▏   | 485/785 [03:42<02:03,  2.44it/s] 62%|██████▏   | 486/785 [03:42<01:51,  2.68it/s] 62%|██████▏   | 487/785 [03:42<01:43,  2.88it/s] 62%|██████▏   | 488/785 [03:43<01:37,  3.04it/s] 62%|██████▏   | 489/785 [03:43<01:33,  3.16it/s] 62%|██████▏   | 490/785 [03:43<01:30,  3.25it/s] 63%|██████▎   | 491/785 [03:44<01:28,  3.32it/s] 63%|██████▎   | 492/785 [03:44<01:31,  3.21it/s] 63%|██████▎   | 493/785 [03:44<01:28,  3.28it/s] 63%|██████▎   | 494/785 [03:44<01:27,  3.34it/s] 63%|██████▎   | 495/785 [03:45<01:25,  3.38it/s] 63%|██████▎   | 496/785 [03:45<01:24,  3.41it/s] 63%|██████▎   | 497/785 [03:45<01:23,  3.43it/s] 63%|██████▎   | 498/785 [03:46<01:23,  3.45it/s] 64%|██████▎   | 499/785 [03:46<01:22,  3.46it/s] 64%|██████▎   | 500/785 [03:46<01:22,  3.46it/s]                                                  64%|██████▎   | 500/785 [03:46<01:22,  3.46it/s] 64%|██████▍   | 501/785 [03:46<01:21,  3.47it/s] 64%|██████▍   | 502/785 [03:47<01:21,  3.47it/s] 64%|██████▍   | 503/785 [03:47<01:23,  3.38it/s] 64%|██████▍   | 504/785 [03:47<01:22,  3.41it/s] 64%|██████▍   | 505/785 [03:48<01:21,  3.43it/s] 64%|██████▍   | 506/785 [03:48<01:21,  3.44it/s] 65%|██████▍   | 507/785 [03:48<01:20,  3.45it/s] 65%|██████▍   | 508/785 [03:49<01:20,  3.46it/s] 65%|██████▍   | 509/785 [03:49<01:19,  3.47it/s] 65%|██████▍   | 510/785 [03:49<01:19,  3.47it/s] 65%|██████▌   | 511/785 [03:49<01:18,  3.47it/s] 65%|██████▌   | 512/785 [03:50<01:18,  3.48it/s] 65%|██████▌   | 513/785 [03:50<01:18,  3.48it/s] 65%|██████▌   | 514/785 [03:50<01:21,  3.32it/s] 66%|██████▌   | 515/785 [03:51<01:20,  3.37it/s] 66%|██████▌   | 516/785 [03:51<01:19,  3.40it/s] 66%|██████▌   | 517/785 [03:51<01:18,  3.42it/s] 66%|██████▌   | 518/785 [03:51<01:17,  3.44it/s] 66%|██████▌   | 519/785 [03:52<01:17,  3.45it/s] 66%|██████▌   | 520/785 [03:52<01:16,  3.46it/s] 66%|██████▋   | 521/785 [03:52<01:16,  3.47it/s] 66%|██████▋   | 522/785 [03:53<01:15,  3.47it/s] 67%|██████▋   | 523/785 [03:53<01:15,  3.48it/s] 67%|██████▋   | 524/785 [03:53<01:15,  3.48it/s] 67%|██████▋   | 525/785 [03:54<01:20,  3.22it/s] 67%|██████▋   | 526/785 [03:54<01:18,  3.29it/s] 67%|██████▋   | 527/785 [03:54<01:17,  3.35it/s] 67%|██████▋   | 528/785 [03:54<01:15,  3.39it/s] 67%|██████▋   | 529/785 [03:55<01:15,  3.41it/s] 68%|██████▊   | 530/785 [03:55<01:14,  3.43it/s] 68%|██████▊   | 531/785 [03:55<01:13,  3.45it/s] 68%|██████▊   | 532/785 [03:56<01:13,  3.45it/s] 68%|██████▊   | 533/785 [03:56<01:12,  3.46it/s] 68%|██████▊   | 534/785 [03:56<01:12,  3.47it/s] 68%|██████▊   | 535/785 [03:56<01:12,  3.47it/s] 68%|██████▊   | 536/785 [03:57<01:15,  3.30it/s] 68%|██████▊   | 537/785 [03:57<01:13,  3.35it/s] 69%|██████▊   | 538/785 [03:57<01:12,  3.39it/s] 69%|██████▊   | 539/785 [03:58<01:12,  3.41it/s] 69%|██████▉   | 540/785 [03:58<01:11,  3.43it/s] 69%|██████▉   | 541/785 [03:58<01:10,  3.45it/s] 69%|██████▉   | 542/785 [03:58<01:10,  3.46it/s] 69%|██████▉   | 543/785 [03:59<01:09,  3.46it/s] 69%|██████▉   | 544/785 [03:59<01:09,  3.47it/s] 69%|██████▉   | 545/785 [03:59<01:09,  3.47it/s] 70%|██████▉   | 546/785 [04:00<01:08,  3.47it/s] 70%|██████▉   | 547/785 [04:00<01:10,  3.39it/s] 70%|██████▉   | 548/785 [04:00<01:09,  3.41it/s] 70%|██████▉   | 549/785 [04:01<01:08,  3.43it/s] 70%|███████   | 550/785 [04:01<01:08,  3.45it/s] 70%|███████   | 551/785 [04:01<01:07,  3.46it/s] 70%|███████   | 552/785 [04:01<01:07,  3.46it/s] 70%|███████   | 553/785 [04:02<01:06,  3.47it/s] 71%|███████   | 554/785 [04:02<01:06,  3.47it/s] 71%|███████   | 555/785 [04:02<01:06,  3.47it/s] 71%|███████   | 556/785 [04:03<01:05,  3.47it/s] 71%|███████   | 557/785 [04:03<01:05,  3.48it/s] 71%|███████   | 558/785 [04:03<01:07,  3.36it/s] 71%|███████   | 559/785 [04:03<01:06,  3.39it/s] 71%|███████▏  | 560/785 [04:04<01:05,  3.42it/s] 71%|███████▏  | 561/785 [04:04<01:05,  3.43it/s] 72%|███████▏  | 562/785 [04:04<01:04,  3.44it/s] 72%|███████▏  | 563/785 [04:05<01:04,  3.45it/s] 72%|███████▏  | 564/785 [04:05<01:03,  3.46it/s] 72%|███████▏  | 565/785 [04:05<01:03,  3.47it/s] 72%|███████▏  | 566/785 [04:05<01:03,  3.47it/s] 72%|███████▏  | 567/785 [04:06<01:02,  3.47it/s] 72%|███████▏  | 568/785 [04:06<01:02,  3.47it/s] 72%|███████▏  | 569/785 [04:06<01:04,  3.35it/s] 73%|███████▎  | 570/785 [04:07<01:03,  3.39it/s] 73%|███████▎  | 571/785 [04:07<01:02,  3.42it/s] 73%|███████▎  | 572/785 [04:07<01:02,  3.43it/s] 73%|███████▎  | 573/785 [04:07<01:01,  3.45it/s] 73%|███████▎  | 574/785 [04:08<01:01,  3.46it/s] 73%|███████▎  | 575/785 [04:08<01:00,  3.46it/s] 73%|███████▎  | 576/785 [04:08<01:00,  3.47it/s] 74%|███████▎  | 577/785 [04:09<00:59,  3.47it/s] 74%|███████▎  | 578/785 [04:09<00:59,  3.47it/s] 74%|███████▍  | 579/785 [04:09<00:59,  3.47it/s] 74%|███████▍  | 580/785 [04:10<01:01,  3.35it/s] 74%|███████▍  | 581/785 [04:10<01:00,  3.39it/s] 74%|███████▍  | 582/785 [04:10<00:59,  3.41it/s] 74%|███████▍  | 583/785 [04:10<00:58,  3.43it/s] 74%|███████▍  | 584/785 [04:11<00:58,  3.45it/s] 75%|███████▍  | 585/785 [04:11<00:57,  3.46it/s] 75%|███████▍  | 586/785 [04:11<00:57,  3.46it/s] 75%|███████▍  | 587/785 [04:12<00:59,  3.31it/s] 75%|███████▍  | 588/785 [04:12<00:58,  3.36it/s] 75%|███████▌  | 589/785 [04:12<00:57,  3.39it/s] 75%|███████▌  | 590/785 [04:12<00:57,  3.42it/s] 75%|███████▌  | 591/785 [04:13<00:56,  3.43it/s] 75%|███████▌  | 592/785 [04:13<00:56,  3.45it/s] 76%|███████▌  | 593/785 [04:13<00:55,  3.45it/s] 76%|███████▌  | 594/785 [04:14<00:55,  3.46it/s] 76%|███████▌  | 595/785 [04:14<00:54,  3.47it/s] 76%|███████▌  | 596/785 [04:14<00:54,  3.47it/s] 76%|███████▌  | 597/785 [04:14<00:54,  3.47it/s] 76%|███████▌  | 598/785 [04:15<00:56,  3.29it/s] 76%|███████▋  | 599/785 [04:15<00:55,  3.34it/s] 76%|███████▋  | 600/785 [04:15<00:54,  3.38it/s] 77%|███████▋  | 601/785 [04:16<00:54,  3.41it/s] 77%|███████▋  | 602/785 [04:16<00:53,  3.43it/s] 77%|███████▋  | 603/785 [04:16<00:52,  3.44it/s] 77%|███████▋  | 604/785 [04:17<00:52,  3.45it/s] 77%|███████▋  | 605/785 [04:17<00:52,  3.46it/s] 77%|███████▋  | 606/785 [04:17<00:51,  3.46it/s] 77%|███████▋  | 607/785 [04:17<00:51,  3.47it/s] 77%|███████▋  | 608/785 [04:18<00:50,  3.47it/s] 78%|███████▊  | 609/785 [04:18<00:54,  3.24it/s] 78%|███████▊  | 610/785 [04:18<00:52,  3.31it/s] 78%|███████▊  | 611/785 [04:19<00:51,  3.35it/s] 78%|███████▊  | 612/785 [04:19<00:51,  3.39it/s] 78%|███████▊  | 613/785 [04:19<00:50,  3.42it/s] 78%|███████▊  | 614/785 [04:19<00:49,  3.43it/s] 78%|███████▊  | 615/785 [04:20<00:49,  3.45it/s] 78%|███████▊  | 616/785 [04:20<00:48,  3.45it/s] 79%|███████▊  | 617/785 [04:20<00:48,  3.46it/s] 79%|███████▊  | 618/785 [04:21<00:48,  3.46it/s] 79%|███████▉  | 619/785 [04:21<00:47,  3.47it/s] 79%|███████▉  | 620/785 [04:21<00:48,  3.37it/s] 79%|███████▉  | 621/785 [04:22<00:48,  3.40it/s] 79%|███████▉  | 622/785 [04:22<00:47,  3.42it/s] 79%|███████▉  | 623/785 [04:22<00:47,  3.44it/s] 79%|███████▉  | 624/785 [04:22<00:46,  3.45it/s] 80%|███████▉  | 625/785 [04:23<00:46,  3.46it/s] 80%|███████▉  | 626/785 [04:23<00:45,  3.46it/s] 80%|███████▉  | 627/785 [04:23<00:45,  3.47it/s] 80%|████████  | 628/785 [04:23<00:41,  3.76it/s][INFO|trainer.py:2140] 2023-08-29 11:05:56,364 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:05:56,364 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 11:05:56,364 >>   Batch size = 8
{'eval_loss': 0.9668242931365967, 'eval_runtime': 11.347, 'eval_samples_per_second': 367.059, 'eval_steps_per_second': 45.915, 'epoch': 3.0}
{'loss': 0.6658, 'learning_rate': 1.3614649681528663e-05, 'epoch': 3.18}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:09, 57.17it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.71it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.02it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.20it/s][A
  5%|▌         | 28/521 [00:00<00:11, 42.76it/s][A
  6%|▋         | 33/521 [00:00<00:11, 44.05it/s][A
  7%|▋         | 38/521 [00:00<00:10, 44.86it/s][A
  8%|▊         | 43/521 [00:00<00:10, 45.44it/s][A
  9%|▉         | 48/521 [00:01<00:10, 45.91it/s][A
 10%|█         | 53/521 [00:01<00:10, 46.22it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.42it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.57it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.43it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.48it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.61it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.75it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.78it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.83it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.84it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.89it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.70it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.56it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.64it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.62it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.65it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.79it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.82it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.84it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.84it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.78it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.74it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.66it/s][A
 32%|███▏      | 168/521 [00:03<00:08, 44.03it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 44.82it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 45.47it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 45.91it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 46.22it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.44it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.60it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.72it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.48it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.34it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.54it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.65it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.58it/s][A
 45%|████▍     | 233/521 [00:05<00:06, 46.22it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.75it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.82it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.81it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.62it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.64it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.52it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.75it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.72it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.87it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.93it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.98it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.85it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.83it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.79it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 45.24it/s][A
 60%|██████    | 313/521 [00:06<00:04, 45.80it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.04it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 46.36it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 46.53it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.56it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.68it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.70it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.48it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.61it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.68it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.64it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.76it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 46.86it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.76it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.78it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.77it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.70it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.66it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.68it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.68it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.77it/s][A
 80%|████████  | 418/521 [00:08<00:02, 46.75it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.87it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.74it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.74it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.73it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 46.67it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 44.12it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 44.98it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 45.57it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 46.03it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 46.40it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.53it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.46it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.63it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.41it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.41it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.54it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.59it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.73it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 46.89it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.93it/s][A                                                 
                                                 [A 80%|████████  | 628/785 [04:35<00:41,  3.76it/s]
100%|██████████| 521/521 [00:11<00:00, 46.93it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:06:07,880 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-628
[INFO|configuration_utils.py:351] 2023-08-29 11:06:08,143 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-628/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:06:12,357 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-628/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:06:12,538 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-628/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:06:12,615 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-628/special_tokens_map.json
 80%|████████  | 629/785 [04:49<20:07,  7.74s/it] 80%|████████  | 630/785 [04:49<14:15,  5.52s/it] 80%|████████  | 631/785 [04:49<10:07,  3.95s/it] 81%|████████  | 632/785 [04:50<07:15,  2.85s/it] 81%|████████  | 633/785 [04:50<05:16,  2.08s/it] 81%|████████  | 634/785 [04:50<03:52,  1.54s/it] 81%|████████  | 635/785 [04:50<02:54,  1.17s/it] 81%|████████  | 636/785 [04:51<02:14,  1.11it/s] 81%|████████  | 637/785 [04:51<01:46,  1.39it/s] 81%|████████▏ | 638/785 [04:51<01:26,  1.70it/s] 81%|████████▏ | 639/785 [04:52<01:12,  2.01it/s] 82%|████████▏ | 640/785 [04:52<01:03,  2.30it/s] 82%|████████▏ | 641/785 [04:52<00:58,  2.46it/s] 82%|████████▏ | 642/785 [04:52<00:53,  2.69it/s] 82%|████████▏ | 643/785 [04:53<00:49,  2.89it/s] 82%|████████▏ | 644/785 [04:53<00:46,  3.04it/s] 82%|████████▏ | 645/785 [04:53<00:44,  3.16it/s] 82%|████████▏ | 646/785 [04:54<00:42,  3.25it/s] 82%|████████▏ | 647/785 [04:54<00:41,  3.32it/s] 83%|████████▎ | 648/785 [04:54<00:40,  3.37it/s] 83%|████████▎ | 649/785 [04:54<00:40,  3.40it/s] 83%|████████▎ | 650/785 [04:55<00:39,  3.43it/s] 83%|████████▎ | 651/785 [04:55<00:38,  3.44it/s] 83%|████████▎ | 652/785 [04:55<00:40,  3.28it/s] 83%|████████▎ | 653/785 [04:56<00:39,  3.34it/s] 83%|████████▎ | 654/785 [04:56<00:38,  3.38it/s] 83%|████████▎ | 655/785 [04:56<00:38,  3.41it/s] 84%|████████▎ | 656/785 [04:57<00:37,  3.43it/s] 84%|████████▎ | 657/785 [04:57<00:37,  3.44it/s] 84%|████████▍ | 658/785 [04:57<00:36,  3.45it/s] 84%|████████▍ | 659/785 [04:57<00:36,  3.46it/s] 84%|████████▍ | 660/785 [04:58<00:36,  3.47it/s] 84%|████████▍ | 661/785 [04:58<00:35,  3.47it/s] 84%|████████▍ | 662/785 [04:58<00:35,  3.48it/s] 84%|████████▍ | 663/785 [04:59<00:37,  3.21it/s] 85%|████████▍ | 664/785 [04:59<00:36,  3.29it/s] 85%|████████▍ | 665/785 [04:59<00:35,  3.35it/s] 85%|████████▍ | 666/785 [05:00<00:35,  3.38it/s] 85%|████████▍ | 667/785 [05:00<00:34,  3.41it/s] 85%|████████▌ | 668/785 [05:00<00:34,  3.43it/s] 85%|████████▌ | 669/785 [05:00<00:33,  3.45it/s] 85%|████████▌ | 670/785 [05:01<00:33,  3.46it/s] 85%|████████▌ | 671/785 [05:01<00:32,  3.46it/s] 86%|████████▌ | 672/785 [05:01<00:32,  3.47it/s] 86%|████████▌ | 673/785 [05:02<00:32,  3.47it/s] 86%|████████▌ | 674/785 [05:02<00:34,  3.21it/s] 86%|████████▌ | 675/785 [05:02<00:33,  3.28it/s] 86%|████████▌ | 676/785 [05:02<00:32,  3.34it/s] 86%|████████▌ | 677/785 [05:03<00:31,  3.38it/s] 86%|████████▋ | 678/785 [05:03<00:31,  3.41it/s] 86%|████████▋ | 679/785 [05:03<00:30,  3.43it/s] 87%|████████▋ | 680/785 [05:04<00:30,  3.44it/s] 87%|████████▋ | 681/785 [05:04<00:30,  3.46it/s] 87%|████████▋ | 682/785 [05:04<00:29,  3.46it/s] 87%|████████▋ | 683/785 [05:04<00:29,  3.47it/s] 87%|████████▋ | 684/785 [05:05<00:29,  3.47it/s] 87%|████████▋ | 685/785 [05:05<00:30,  3.32it/s] 87%|████████▋ | 686/785 [05:05<00:29,  3.36it/s] 88%|████████▊ | 687/785 [05:06<00:28,  3.40it/s] 88%|████████▊ | 688/785 [05:06<00:28,  3.42it/s] 88%|████████▊ | 689/785 [05:06<00:27,  3.44it/s] 88%|████████▊ | 690/785 [05:07<00:27,  3.45it/s] 88%|████████▊ | 691/785 [05:07<00:27,  3.46it/s] 88%|████████▊ | 692/785 [05:07<00:26,  3.46it/s] 88%|████████▊ | 693/785 [05:07<00:26,  3.47it/s] 88%|████████▊ | 694/785 [05:08<00:26,  3.47it/s] 89%|████████▊ | 695/785 [05:08<00:25,  3.47it/s] 89%|████████▊ | 696/785 [05:08<00:26,  3.32it/s] 89%|████████▉ | 697/785 [05:09<00:26,  3.36it/s] 89%|████████▉ | 698/785 [05:09<00:25,  3.40it/s] 89%|████████▉ | 699/785 [05:09<00:25,  3.42it/s] 89%|████████▉ | 700/785 [05:09<00:24,  3.44it/s] 89%|████████▉ | 701/785 [05:10<00:24,  3.45it/s] 89%|████████▉ | 702/785 [05:10<00:24,  3.46it/s] 90%|████████▉ | 703/785 [05:10<00:23,  3.47it/s] 90%|████████▉ | 704/785 [05:11<00:23,  3.46it/s] 90%|████████▉ | 705/785 [05:11<00:23,  3.47it/s] 90%|████████▉ | 706/785 [05:11<00:22,  3.47it/s] 90%|█████████ | 707/785 [05:12<00:23,  3.33it/s] 90%|█████████ | 708/785 [05:12<00:22,  3.37it/s] 90%|█████████ | 709/785 [05:12<00:22,  3.41it/s] 90%|█████████ | 710/785 [05:12<00:21,  3.43it/s] 91%|█████████ | 711/785 [05:13<00:21,  3.44it/s] 91%|█████████ | 712/785 [05:13<00:21,  3.45it/s] 91%|█████████ | 713/785 [05:13<00:20,  3.46it/s] 91%|█████████ | 714/785 [05:14<00:20,  3.46it/s] 91%|█████████ | 715/785 [05:14<00:20,  3.47it/s] 91%|█████████ | 716/785 [05:14<00:19,  3.47it/s] 91%|█████████▏| 717/785 [05:14<00:19,  3.47it/s] 91%|█████████▏| 718/785 [05:15<00:19,  3.47it/s] 92%|█████████▏| 719/785 [05:15<00:19,  3.47it/s] 92%|█████████▏| 720/785 [05:15<00:18,  3.47it/s] 92%|█████████▏| 721/785 [05:16<00:18,  3.47it/s] 92%|█████████▏| 722/785 [05:16<00:18,  3.47it/s] 92%|█████████▏| 723/785 [05:16<00:19,  3.24it/s] 92%|█████████▏| 724/785 [05:16<00:18,  3.31it/s] 92%|█████████▏| 725/785 [05:17<00:17,  3.36it/s] 92%|█████████▏| 726/785 [05:17<00:17,  3.39it/s] 93%|█████████▎| 727/785 [05:17<00:16,  3.42it/s] 93%|█████████▎| 728/785 [05:18<00:16,  3.43it/s] 93%|█████████▎| 729/785 [05:18<00:16,  3.44it/s] 93%|█████████▎| 730/785 [05:18<00:15,  3.45it/s] 93%|█████████▎| 731/785 [05:18<00:15,  3.46it/s] 93%|█████████▎| 732/785 [05:19<00:15,  3.46it/s] 93%|█████████▎| 733/785 [05:19<00:15,  3.47it/s] 94%|█████████▎| 734/785 [05:19<00:15,  3.29it/s] 94%|█████████▎| 735/785 [05:20<00:14,  3.35it/s] 94%|█████████▍| 736/785 [05:20<00:14,  3.38it/s] 94%|█████████▍| 737/785 [05:20<00:14,  3.41it/s] 94%|█████████▍| 738/785 [05:21<00:13,  3.43it/s] 94%|█████████▍| 739/785 [05:21<00:13,  3.44it/s] 94%|█████████▍| 740/785 [05:21<00:13,  3.45it/s] 94%|█████████▍| 741/785 [05:21<00:12,  3.46it/s] 95%|█████████▍| 742/785 [05:22<00:12,  3.46it/s] 95%|█████████▍| 743/785 [05:22<00:12,  3.47it/s] 95%|█████████▍| 744/785 [05:22<00:11,  3.47it/s] 95%|█████████▍| 745/785 [05:23<00:12,  3.22it/s] 95%|█████████▌| 746/785 [05:23<00:11,  3.29it/s] 95%|█████████▌| 747/785 [05:23<00:11,  3.35it/s] 95%|█████████▌| 748/785 [05:23<00:10,  3.38it/s] 95%|█████████▌| 749/785 [05:24<00:10,  3.41it/s] 96%|█████████▌| 750/785 [05:24<00:10,  3.43it/s] 96%|█████████▌| 751/785 [05:24<00:09,  3.44it/s] 96%|█████████▌| 752/785 [05:25<00:09,  3.45it/s] 96%|█████████▌| 753/785 [05:25<00:09,  3.46it/s] 96%|█████████▌| 754/785 [05:25<00:08,  3.46it/s] 96%|█████████▌| 755/785 [05:26<00:08,  3.47it/s] 96%|█████████▋| 756/785 [05:26<00:08,  3.38it/s] 96%|█████████▋| 757/785 [05:26<00:08,  3.41it/s] 97%|█████████▋| 758/785 [05:26<00:07,  3.42it/s] 97%|█████████▋| 759/785 [05:27<00:07,  3.44it/s] 97%|█████████▋| 760/785 [05:27<00:07,  3.45it/s] 97%|█████████▋| 761/785 [05:27<00:06,  3.46it/s] 97%|█████████▋| 762/785 [05:28<00:06,  3.46it/s] 97%|█████████▋| 763/785 [05:28<00:06,  3.46it/s] 97%|█████████▋| 764/785 [05:28<00:06,  3.47it/s] 97%|█████████▋| 765/785 [05:28<00:05,  3.47it/s] 98%|█████████▊| 766/785 [05:29<00:05,  3.47it/s] 98%|█████████▊| 767/785 [05:29<00:05,  3.39it/s] 98%|█████████▊| 768/785 [05:29<00:04,  3.42it/s] 98%|█████████▊| 769/785 [05:30<00:04,  3.43it/s] 98%|█████████▊| 770/785 [05:30<00:04,  3.44it/s] 98%|█████████▊| 771/785 [05:30<00:04,  3.45it/s] 98%|█████████▊| 772/785 [05:30<00:03,  3.46it/s] 98%|█████████▊| 773/785 [05:31<00:03,  3.47it/s] 99%|█████████▊| 774/785 [05:31<00:03,  3.47it/s] 99%|█████████▊| 775/785 [05:31<00:02,  3.47it/s] 99%|█████████▉| 776/785 [05:32<00:02,  3.47it/s] 99%|█████████▉| 777/785 [05:32<00:02,  3.47it/s] 99%|█████████▉| 778/785 [05:32<00:02,  3.40it/s] 99%|█████████▉| 779/785 [05:32<00:01,  3.43it/s] 99%|█████████▉| 780/785 [05:33<00:01,  3.44it/s] 99%|█████████▉| 781/785 [05:33<00:01,  3.45it/s]100%|█████████▉| 782/785 [05:33<00:00,  3.46it/s]100%|█████████▉| 783/785 [05:34<00:00,  3.46it/s]100%|█████████▉| 784/785 [05:34<00:00,  3.47it/s]100%|██████████| 785/785 [05:34<00:00,  3.76it/s][INFO|trainer.py:2140] 2023-08-29 11:07:07,031 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:07:07,032 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 11:07:07,032 >>   Batch size = 8
{'eval_loss': 0.9748365879058838, 'eval_runtime': 11.2231, 'eval_samples_per_second': 371.108, 'eval_steps_per_second': 46.422, 'epoch': 4.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.89it/s][A
  2%|▏         | 12/521 [00:00<00:09, 51.04it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.08it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.23it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.86it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.55it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.31it/s][A
  8%|▊         | 43/521 [00:00<00:10, 44.59it/s][A
  9%|▉         | 48/521 [00:01<00:10, 45.25it/s][A
 10%|█         | 53/521 [00:01<00:10, 45.77it/s][A
 11%|█         | 58/521 [00:01<00:10, 46.19it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.45it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.61it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.65it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.75it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.69it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.63it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.45it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.65it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.75it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.83it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.92it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.95it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.79it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.80it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.74it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.73it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.74it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.65it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.77it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.89it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.94it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.83it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.81it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.77it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 43.57it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 44.50it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 45.22it/s][A
 38%|███▊      | 198/521 [00:04<00:07, 45.77it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.04it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.37it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.48it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.71it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.54it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.56it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 46.58it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.74it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.81it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.81it/s][A
 49%|████▊     | 253/521 [00:05<00:06, 44.27it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 45.10it/s][A
 50%|█████     | 263/521 [00:05<00:05, 45.50it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.03it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.26it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.48it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.64it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.75it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.69it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.67it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.65it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 46.68it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.75it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.88it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 43.14it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 42.55it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 43.79it/s][A
 65%|██████▍   | 338/521 [00:07<00:04, 44.72it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 45.36it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 45.92it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.10it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.37it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.44it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.25it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 46.32it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.48it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.59it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.68it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.75it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.87it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.79it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.77it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.80it/s][A
 80%|████████  | 418/521 [00:09<00:02, 46.75it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.73it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.78it/s][A
 83%|████████▎ | 433/521 [00:09<00:02, 39.94it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 41.81it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 43.28it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 44.26it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 45.01it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 45.57it/s][A
 89%|████████▉ | 463/521 [00:10<00:01, 46.02it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 46.24it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.20it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.31it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.45it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.60it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.32it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.64it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.66it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.71it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 46.73it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.58it/s][A                                                 
                                                 [A100%|██████████| 785/785 [05:45<00:00,  3.76it/s]
100%|██████████| 521/521 [00:11<00:00, 46.58it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 11:07:18,692 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-785
[INFO|configuration_utils.py:351] 2023-08-29 11:07:18,916 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-785/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:07:25,377 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-785/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:07:25,559 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-785/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:07:25,639 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-785/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 11:07:35,551 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 11:07:35,593 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-157 (score: 0.9590339660644531).
                                                 100%|██████████| 785/785 [06:16<00:00,  3.76it/s]100%|██████████| 785/785 [06:16<00:00,  2.09it/s]
[INFO|trainer.py:1894] 2023-08-29 11:07:48,598 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 11:07:48,799 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 11:07:54,058 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 11:07:54,440 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 11:07:54,581 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:07:55,420 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:07:55,420 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:07:55,420 >>   train_loss               =     0.6549
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:07:55,420 >>   train_runtime            = 0:06:16.17
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:07:55,420 >>   train_samples            =      10029
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:07:55,420 >>   train_samples_per_second =    133.302
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:07:55,420 >>   train_steps_per_second   =      2.087
{'eval_loss': 0.9789629578590393, 'eval_runtime': 11.2952, 'eval_samples_per_second': 368.739, 'eval_steps_per_second': 46.126, 'epoch': 5.0}
{'train_runtime': 376.1753, 'train_samples_per_second': 133.302, 'train_steps_per_second': 2.087, 'train_loss': 0.6548542581546078, 'epoch': 5.0}
08/29/2023 11:07:55 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 11:07:55,767 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 11:07:55,767 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 11:07:55,767 >>   Batch size = 8
  0%|          | 0/521 [00:00<?, ?it/s]  1%|          | 6/521 [00:00<00:08, 59.13it/s]  2%|▏         | 12/521 [00:00<00:09, 51.88it/s]  3%|▎         | 18/521 [00:00<00:10, 49.74it/s]  5%|▍         | 24/521 [00:00<00:10, 48.92it/s]  6%|▌         | 29/521 [00:00<00:10, 48.52it/s]  7%|▋         | 34/521 [00:00<00:10, 48.17it/s]  7%|▋         | 39/521 [00:00<00:10, 48.00it/s]  8%|▊         | 44/521 [00:00<00:09, 47.88it/s]  9%|▉         | 49/521 [00:01<00:09, 47.60it/s] 10%|█         | 54/521 [00:01<00:09, 47.59it/s] 11%|█▏        | 59/521 [00:01<00:09, 47.58it/s] 12%|█▏        | 64/521 [00:01<00:09, 47.51it/s] 13%|█▎        | 69/521 [00:01<00:10, 41.51it/s] 14%|█▍        | 74/521 [00:01<00:10, 43.23it/s] 15%|█▌        | 79/521 [00:01<00:09, 44.50it/s] 16%|█▌        | 84/521 [00:01<00:09, 45.36it/s] 17%|█▋        | 89/521 [00:01<00:09, 45.99it/s] 18%|█▊        | 94/521 [00:02<00:09, 46.38it/s] 19%|█▉        | 99/521 [00:02<00:09, 46.73it/s] 20%|█▉        | 104/521 [00:02<00:08, 47.03it/s] 21%|██        | 109/521 [00:02<00:08, 46.94it/s] 22%|██▏       | 114/521 [00:02<00:08, 46.99it/s] 23%|██▎       | 119/521 [00:02<00:08, 47.20it/s] 24%|██▍       | 124/521 [00:02<00:08, 47.23it/s] 25%|██▍       | 129/521 [00:02<00:08, 47.40it/s] 26%|██▌       | 134/521 [00:02<00:08, 47.41it/s] 27%|██▋       | 139/521 [00:02<00:08, 47.51it/s] 28%|██▊       | 144/521 [00:03<00:07, 47.46it/s] 29%|██▊       | 149/521 [00:03<00:07, 47.48it/s] 30%|██▉       | 154/521 [00:03<00:07, 47.41it/s] 31%|███       | 159/521 [00:03<00:07, 47.33it/s] 31%|███▏      | 164/521 [00:03<00:07, 47.33it/s] 32%|███▏      | 169/521 [00:03<00:07, 47.38it/s] 33%|███▎      | 174/521 [00:03<00:07, 47.36it/s] 34%|███▍      | 179/521 [00:03<00:07, 47.35it/s] 35%|███▌      | 184/521 [00:03<00:07, 47.39it/s] 36%|███▋      | 189/521 [00:04<00:06, 47.50it/s] 37%|███▋      | 194/521 [00:04<00:06, 47.53it/s] 38%|███▊      | 199/521 [00:04<00:06, 47.36it/s] 39%|███▉      | 204/521 [00:04<00:06, 47.39it/s] 40%|████      | 209/521 [00:04<00:07, 41.17it/s] 41%|████      | 214/521 [00:04<00:07, 42.92it/s] 42%|████▏     | 219/521 [00:04<00:06, 44.23it/s] 43%|████▎     | 224/521 [00:04<00:06, 45.15it/s] 44%|████▍     | 229/521 [00:04<00:06, 45.85it/s] 45%|████▍     | 234/521 [00:05<00:06, 46.34it/s] 46%|████▌     | 239/521 [00:05<00:06, 46.66it/s] 47%|████▋     | 244/521 [00:05<00:05, 46.88it/s] 48%|████▊     | 249/521 [00:05<00:05, 46.73it/s] 49%|████▉     | 254/521 [00:05<00:05, 46.87it/s] 50%|████▉     | 259/521 [00:05<00:05, 46.92it/s] 51%|█████     | 264/521 [00:05<00:05, 47.16it/s] 52%|█████▏    | 269/521 [00:05<00:05, 47.30it/s] 53%|█████▎    | 274/521 [00:05<00:05, 47.39it/s] 54%|█████▎    | 279/521 [00:05<00:05, 47.36it/s] 55%|█████▍    | 284/521 [00:06<00:04, 47.47it/s] 55%|█████▌    | 289/521 [00:06<00:04, 47.44it/s] 56%|█████▋    | 294/521 [00:06<00:04, 47.31it/s] 57%|█████▋    | 299/521 [00:06<00:04, 47.24it/s] 58%|█████▊    | 304/521 [00:06<00:04, 47.26it/s] 59%|█████▉    | 309/521 [00:06<00:04, 47.22it/s] 60%|██████    | 314/521 [00:06<00:04, 47.26it/s] 61%|██████    | 319/521 [00:06<00:04, 47.35it/s] 62%|██████▏   | 324/521 [00:06<00:04, 47.45it/s] 63%|██████▎   | 329/521 [00:07<00:04, 47.50it/s] 64%|██████▍   | 334/521 [00:07<00:03, 47.41it/s] 65%|██████▌   | 339/521 [00:07<00:03, 47.34it/s] 66%|██████▌   | 344/521 [00:07<00:03, 47.27it/s] 67%|██████▋   | 349/521 [00:07<00:03, 44.80it/s] 68%|██████▊   | 354/521 [00:07<00:03, 45.55it/s] 69%|██████▉   | 359/521 [00:07<00:03, 46.20it/s] 70%|██████▉   | 364/521 [00:07<00:03, 46.53it/s] 71%|███████   | 369/521 [00:07<00:03, 46.78it/s] 72%|███████▏  | 374/521 [00:07<00:03, 47.05it/s] 73%|███████▎  | 379/521 [00:08<00:03, 47.24it/s] 74%|███████▎  | 384/521 [00:08<00:02, 47.22it/s] 75%|███████▍  | 389/521 [00:08<00:02, 47.02it/s] 76%|███████▌  | 394/521 [00:08<00:02, 46.99it/s] 77%|███████▋  | 399/521 [00:08<00:02, 47.09it/s] 78%|███████▊  | 404/521 [00:08<00:02, 47.24it/s] 79%|███████▊  | 409/521 [00:08<00:02, 47.31it/s] 79%|███████▉  | 414/521 [00:08<00:02, 47.32it/s] 80%|████████  | 419/521 [00:08<00:02, 47.27it/s] 81%|████████▏ | 424/521 [00:09<00:02, 47.41it/s] 82%|████████▏ | 429/521 [00:09<00:01, 47.47it/s] 83%|████████▎ | 434/521 [00:09<00:01, 47.36it/s] 84%|████████▍ | 439/521 [00:09<00:01, 47.16it/s] 85%|████████▌ | 444/521 [00:09<00:01, 47.18it/s] 86%|████████▌ | 449/521 [00:09<00:01, 47.23it/s] 87%|████████▋ | 454/521 [00:09<00:01, 47.29it/s] 88%|████████▊ | 459/521 [00:09<00:01, 47.33it/s] 89%|████████▉ | 464/521 [00:09<00:01, 47.29it/s] 90%|█████████ | 469/521 [00:09<00:01, 47.28it/s] 91%|█████████ | 474/521 [00:10<00:00, 47.40it/s] 92%|█████████▏| 479/521 [00:10<00:00, 47.40it/s] 93%|█████████▎| 484/521 [00:10<00:00, 47.23it/s] 94%|█████████▍| 489/521 [00:10<00:00, 47.22it/s] 95%|█████████▍| 494/521 [00:10<00:00, 42.61it/s] 96%|█████████▌| 499/521 [00:10<00:00, 43.96it/s] 97%|█████████▋| 504/521 [00:10<00:00, 44.86it/s] 98%|█████████▊| 509/521 [00:10<00:00, 45.62it/s] 99%|█████████▊| 514/521 [00:10<00:00, 46.12it/s]100%|█████████▉| 519/521 [00:11<00:00, 46.44it/s]100%|██████████| 521/521 [00:11<00:00, 46.79it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 11:08:06,925 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:08:06,926 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:08:06,926 >>   eval_loss               =      0.959
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:08:06,926 >>   eval_runtime            = 0:00:11.15
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:08:06,926 >>   eval_samples            =       4165
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:08:06,926 >>   eval_samples_per_second =    373.279
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:08:06,926 >>   eval_steps_per_second   =     46.694
[INFO|trainer_pt_utils.py:913] 2023-08-29 11:08:06,926 >>   perplexity              =     2.6092
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:08:57,333 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:08:57,359 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:08:57,359 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:08:57,359 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:08:57,359 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:08:58,178 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:08:58,179 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:08:58,520 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:09:07,573 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:09:07,598 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:09:09,557 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:09:09,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:09:09,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:09:09,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:09:09,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:09:10,450 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:09:10,451 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:09:10,799 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:09:13,082 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:09:13,082 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-785
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-471
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-157
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-628
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/checkpoint-314
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'lowest point', 'manufacturer', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12876
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12976, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.84it/s]Extractor Predicting: 2it [00:01,  1.77it/s]Extractor Predicting: 3it [00:01,  1.72it/s]Extractor Predicting: 4it [00:02,  1.75it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.73it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:09,  1.58it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:11,  1.53it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:12,  1.54it/s]Extractor Predicting: 22it [00:13,  1.45it/s]Extractor Predicting: 23it [00:14,  1.46it/s]Extractor Predicting: 24it [00:15,  1.45it/s]Extractor Predicting: 25it [00:15,  1.50it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:16,  1.55it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:18,  1.56it/s]Extractor Predicting: 31it [00:19,  1.54it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:20,  1.54it/s]Extractor Predicting: 34it [00:21,  1.49it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.50it/s]Extractor Predicting: 38it [00:24,  1.49it/s]Extractor Predicting: 39it [00:24,  1.50it/s]Extractor Predicting: 40it [00:25,  1.52it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:26,  1.57it/s]Extractor Predicting: 43it [00:27,  1.52it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:29,  1.55it/s]Extractor Predicting: 47it [00:30,  1.52it/s]Extractor Predicting: 48it [00:30,  1.53it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:31,  1.58it/s]Extractor Predicting: 51it [00:32,  1.60it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:34,  1.54it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:35,  1.55it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:37,  1.50it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.57it/s]Extractor Predicting: 63it [00:40,  1.56it/s]Extractor Predicting: 64it [00:40,  1.56it/s]Extractor Predicting: 65it [00:41,  1.57it/s]Extractor Predicting: 66it [00:42,  1.56it/s]Extractor Predicting: 67it [00:42,  1.57it/s]Extractor Predicting: 68it [00:43,  1.53it/s]Extractor Predicting: 69it [00:44,  1.54it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:45,  1.56it/s]Extractor Predicting: 72it [00:46,  1.54it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:47,  1.52it/s]Extractor Predicting: 75it [00:48,  1.56it/s]Extractor Predicting: 76it [00:48,  1.49it/s]Extractor Predicting: 77it [00:49,  1.48it/s]Extractor Predicting: 78it [00:50,  1.47it/s]Extractor Predicting: 79it [00:50,  1.49it/s]Extractor Predicting: 80it [00:51,  1.48it/s]Extractor Predicting: 81it [00:52,  1.43it/s]Extractor Predicting: 82it [00:52,  1.47it/s]Extractor Predicting: 83it [00:53,  1.54it/s]Extractor Predicting: 84it [00:54,  1.54it/s]Extractor Predicting: 85it [00:54,  1.54it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:56,  1.56it/s]Extractor Predicting: 89it [00:57,  1.54it/s]Extractor Predicting: 90it [00:57,  1.53it/s]Extractor Predicting: 91it [00:58,  1.54it/s]Extractor Predicting: 92it [00:59,  1.56it/s]Extractor Predicting: 93it [00:59,  1.53it/s]Extractor Predicting: 94it [01:00,  1.51it/s]Extractor Predicting: 95it [01:01,  1.49it/s]Extractor Predicting: 96it [01:01,  1.51it/s]Extractor Predicting: 97it [01:02,  1.54it/s]Extractor Predicting: 98it [01:03,  1.55it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.56it/s]Extractor Predicting: 101it [01:05,  1.52it/s]Extractor Predicting: 102it [01:05,  1.51it/s]Extractor Predicting: 103it [01:06,  1.51it/s]Extractor Predicting: 104it [01:07,  1.54it/s]Extractor Predicting: 105it [01:07,  1.53it/s]Extractor Predicting: 106it [01:08,  1.53it/s]Extractor Predicting: 107it [01:09,  1.56it/s]Extractor Predicting: 108it [01:09,  1.51it/s]Extractor Predicting: 109it [01:10,  1.52it/s]Extractor Predicting: 110it [01:11,  1.52it/s]Extractor Predicting: 111it [01:11,  1.52it/s]Extractor Predicting: 112it [01:12,  1.50it/s]Extractor Predicting: 113it [01:13,  1.52it/s]Extractor Predicting: 114it [01:13,  1.51it/s]Extractor Predicting: 115it [01:14,  1.54it/s]Extractor Predicting: 116it [01:14,  1.55it/s]Extractor Predicting: 117it [01:15,  1.57it/s]Extractor Predicting: 118it [01:16,  1.53it/s]Extractor Predicting: 119it [01:16,  1.50it/s]Extractor Predicting: 120it [01:17,  1.54it/s]Extractor Predicting: 121it [01:18,  1.53it/s]Extractor Predicting: 122it [01:18,  1.52it/s]Extractor Predicting: 123it [01:19,  1.36it/s]Extractor Predicting: 124it [01:20,  1.40it/s]Extractor Predicting: 125it [01:21,  1.41it/s]Extractor Predicting: 126it [01:21,  1.44it/s]Extractor Predicting: 127it [01:22,  1.44it/s]Extractor Predicting: 128it [01:23,  1.44it/s]Extractor Predicting: 129it [01:23,  1.44it/s]Extractor Predicting: 130it [01:24,  1.46it/s]Extractor Predicting: 131it [01:25,  1.45it/s]Extractor Predicting: 132it [01:25,  1.47it/s]Extractor Predicting: 133it [01:26,  1.47it/s]Extractor Predicting: 134it [01:27,  1.49it/s]Extractor Predicting: 135it [01:27,  1.50it/s]Extractor Predicting: 136it [01:28,  1.51it/s]Extractor Predicting: 137it [01:29,  1.49it/s]Extractor Predicting: 138it [01:30,  1.47it/s]Extractor Predicting: 139it [01:30,  1.48it/s]Extractor Predicting: 140it [01:31,  1.50it/s]Extractor Predicting: 141it [01:31,  1.52it/s]Extractor Predicting: 142it [01:32,  1.50it/s]Extractor Predicting: 143it [01:33,  1.51it/s]Extractor Predicting: 144it [01:33,  1.49it/s]Extractor Predicting: 145it [01:34,  1.51it/s]Extractor Predicting: 146it [01:35,  1.48it/s]Extractor Predicting: 147it [01:36,  1.48it/s]Extractor Predicting: 148it [01:36,  1.47it/s]Extractor Predicting: 149it [01:37,  1.48it/s]Extractor Predicting: 150it [01:38,  1.50it/s]Extractor Predicting: 151it [01:38,  1.50it/s]Extractor Predicting: 152it [01:39,  1.46it/s]Extractor Predicting: 153it [01:40,  1.44it/s]Extractor Predicting: 154it [01:40,  1.57it/s]Extractor Predicting: 154it [01:40,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:10,540 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:10,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:10,564 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:10,564 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:10,564 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:11:11,074 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:11:11,075 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:11:11,417 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:11:12,533 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:11:12,533 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:14,114 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:14,136 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:14,137 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:14,137 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:11:14,137 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:11:14,618 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:11:14,619 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:11:15,360 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:11:15,603 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:11:15,603 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.44866920152091255,
  "recall": 0.028331332533013204,
  "score": 0.05329719963866305,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27443
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27543, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.49it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:10,  1.50it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:15,  1.54it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.58it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.59it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.57it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:25,  1.58it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:26,  1.50it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.48it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:29,  1.49it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:30,  1.46it/s]Extractor Predicting: 49it [00:31,  1.47it/s]Extractor Predicting: 50it [00:32,  1.47it/s]Extractor Predicting: 51it [00:32,  1.48it/s]Extractor Predicting: 52it [00:33,  1.49it/s]Extractor Predicting: 53it [00:34,  1.48it/s]Extractor Predicting: 54it [00:34,  1.43it/s]Extractor Predicting: 55it [00:35,  1.41it/s]Extractor Predicting: 56it [00:36,  1.46it/s]Extractor Predicting: 57it [00:36,  1.50it/s]Extractor Predicting: 58it [00:37,  1.50it/s]Extractor Predicting: 59it [00:38,  1.46it/s]Extractor Predicting: 60it [00:39,  1.44it/s]Extractor Predicting: 61it [00:39,  1.44it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.49it/s]Extractor Predicting: 64it [00:41,  1.40it/s]Extractor Predicting: 65it [00:42,  1.42it/s]Extractor Predicting: 66it [00:43,  1.44it/s]Extractor Predicting: 67it [00:43,  1.43it/s]Extractor Predicting: 68it [00:44,  1.45it/s]Extractor Predicting: 69it [00:45,  1.45it/s]Extractor Predicting: 70it [00:46,  1.43it/s]Extractor Predicting: 71it [00:46,  1.44it/s]Extractor Predicting: 72it [00:47,  1.45it/s]Extractor Predicting: 73it [00:48,  1.43it/s]Extractor Predicting: 74it [00:48,  1.43it/s]Extractor Predicting: 75it [00:49,  1.42it/s]Extractor Predicting: 76it [00:50,  1.40it/s]Extractor Predicting: 77it [00:50,  1.46it/s]Extractor Predicting: 78it [00:51,  1.43it/s]Extractor Predicting: 79it [00:52,  1.45it/s]Extractor Predicting: 80it [00:53,  1.43it/s]Extractor Predicting: 81it [00:53,  1.39it/s]Extractor Predicting: 82it [00:54,  1.42it/s]Extractor Predicting: 83it [00:55,  1.46it/s]Extractor Predicting: 84it [00:55,  1.49it/s]Extractor Predicting: 85it [00:56,  1.44it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:57,  1.48it/s]Extractor Predicting: 88it [00:58,  1.46it/s]Extractor Predicting: 89it [00:59,  1.44it/s]Extractor Predicting: 90it [00:59,  1.41it/s]Extractor Predicting: 91it [01:00,  1.44it/s]Extractor Predicting: 92it [01:01,  1.47it/s]Extractor Predicting: 93it [01:01,  1.50it/s]Extractor Predicting: 94it [01:02,  1.51it/s]Extractor Predicting: 95it [01:03,  1.51it/s]Extractor Predicting: 96it [01:03,  1.51it/s]Extractor Predicting: 97it [01:04,  1.43it/s]Extractor Predicting: 98it [01:05,  1.44it/s]Extractor Predicting: 99it [01:05,  1.47it/s]Extractor Predicting: 100it [01:06,  1.44it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:08,  1.48it/s]Extractor Predicting: 103it [01:08,  1.49it/s]Extractor Predicting: 104it [01:09,  1.50it/s]Extractor Predicting: 105it [01:10,  1.48it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:11,  1.50it/s]Extractor Predicting: 108it [01:12,  1.50it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:13,  1.39it/s]Extractor Predicting: 111it [01:14,  1.44it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:15,  1.50it/s]Extractor Predicting: 114it [01:16,  1.52it/s]Extractor Predicting: 115it [01:16,  1.46it/s]Extractor Predicting: 116it [01:17,  1.47it/s]Extractor Predicting: 117it [01:18,  1.52it/s]Extractor Predicting: 118it [01:18,  1.43it/s]Extractor Predicting: 119it [01:19,  1.45it/s]Extractor Predicting: 120it [01:20,  1.46it/s]Extractor Predicting: 121it [01:20,  1.46it/s]Extractor Predicting: 122it [01:21,  1.48it/s]Extractor Predicting: 123it [01:22,  1.49it/s]Extractor Predicting: 124it [01:22,  1.52it/s]Extractor Predicting: 125it [01:23,  1.50it/s]Extractor Predicting: 126it [01:24,  1.52it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:25,  1.55it/s]Extractor Predicting: 129it [01:26,  1.57it/s]Extractor Predicting: 130it [01:26,  1.50it/s]Extractor Predicting: 131it [01:27,  1.56it/s]Extractor Predicting: 132it [01:28,  1.57it/s]Extractor Predicting: 133it [01:28,  1.57it/s]Extractor Predicting: 134it [01:29,  1.35it/s]Extractor Predicting: 135it [01:30,  1.37it/s]Extractor Predicting: 136it [01:31,  1.40it/s]Extractor Predicting: 137it [01:31,  1.43it/s]Extractor Predicting: 138it [01:32,  1.46it/s]Extractor Predicting: 139it [01:33,  1.47it/s]Extractor Predicting: 140it [01:33,  1.45it/s]Extractor Predicting: 141it [01:34,  1.51it/s]Extractor Predicting: 142it [01:35,  1.47it/s]Extractor Predicting: 143it [01:35,  1.42it/s]Extractor Predicting: 144it [01:36,  1.46it/s]Extractor Predicting: 145it [01:37,  1.46it/s]Extractor Predicting: 146it [01:37,  1.40it/s]Extractor Predicting: 147it [01:38,  1.43it/s]Extractor Predicting: 148it [01:39,  1.46it/s]Extractor Predicting: 149it [01:39,  1.55it/s]Extractor Predicting: 150it [01:40,  1.62it/s]Extractor Predicting: 151it [01:40,  1.64it/s]Extractor Predicting: 152it [01:41,  1.76it/s]Extractor Predicting: 153it [01:41,  1.85it/s]Extractor Predicting: 154it [01:42,  1.90it/s]Extractor Predicting: 155it [01:42,  1.88it/s]Extractor Predicting: 156it [01:43,  1.81it/s]Extractor Predicting: 157it [01:44,  1.81it/s]Extractor Predicting: 158it [01:44,  1.88it/s]Extractor Predicting: 159it [01:45,  1.94it/s]Extractor Predicting: 160it [01:45,  1.93it/s]Extractor Predicting: 161it [01:46,  1.90it/s]Extractor Predicting: 162it [01:46,  1.87it/s]Extractor Predicting: 163it [01:47,  1.80it/s]Extractor Predicting: 164it [01:47,  1.82it/s]Extractor Predicting: 165it [01:48,  1.88it/s]Extractor Predicting: 166it [01:48,  1.86it/s]Extractor Predicting: 167it [01:49,  1.86it/s]Extractor Predicting: 168it [01:50,  1.76it/s]Extractor Predicting: 169it [01:50,  1.68it/s]Extractor Predicting: 170it [01:51,  1.59it/s]Extractor Predicting: 171it [01:52,  1.58it/s]Extractor Predicting: 172it [01:52,  1.53it/s]Extractor Predicting: 173it [01:53,  1.50it/s]Extractor Predicting: 174it [01:54,  1.48it/s]Extractor Predicting: 175it [01:54,  1.53it/s]Extractor Predicting: 176it [01:55,  1.53it/s]Extractor Predicting: 177it [01:56,  1.55it/s]Extractor Predicting: 178it [01:56,  1.53it/s]Extractor Predicting: 179it [01:57,  1.56it/s]Extractor Predicting: 180it [01:57,  1.58it/s]Extractor Predicting: 181it [01:58,  1.59it/s]Extractor Predicting: 182it [01:59,  1.57it/s]Extractor Predicting: 183it [01:59,  1.54it/s]Extractor Predicting: 184it [02:00,  1.49it/s]Extractor Predicting: 185it [02:01,  1.53it/s]Extractor Predicting: 186it [02:01,  1.52it/s]Extractor Predicting: 187it [02:02,  1.51it/s]Extractor Predicting: 188it [02:03,  1.57it/s]Extractor Predicting: 189it [02:03,  1.54it/s]Extractor Predicting: 190it [02:04,  1.54it/s]Extractor Predicting: 191it [02:05,  1.56it/s]Extractor Predicting: 192it [02:05,  1.57it/s]Extractor Predicting: 193it [02:06,  1.56it/s]Extractor Predicting: 194it [02:06,  1.58it/s]Extractor Predicting: 195it [02:07,  1.56it/s]Extractor Predicting: 196it [02:08,  1.56it/s]Extractor Predicting: 197it [02:08,  1.58it/s]Extractor Predicting: 198it [02:09,  1.57it/s]Extractor Predicting: 199it [02:10,  1.54it/s]Extractor Predicting: 200it [02:10,  1.52it/s]Extractor Predicting: 201it [02:11,  1.52it/s]Extractor Predicting: 202it [02:12,  1.49it/s]Extractor Predicting: 203it [02:12,  1.46it/s]Extractor Predicting: 204it [02:13,  1.46it/s]Extractor Predicting: 205it [02:14,  1.47it/s]Extractor Predicting: 206it [02:15,  1.47it/s]Extractor Predicting: 207it [02:15,  1.49it/s]Extractor Predicting: 208it [02:16,  1.50it/s]Extractor Predicting: 209it [02:16,  1.51it/s]Extractor Predicting: 210it [02:17,  1.51it/s]Extractor Predicting: 211it [02:18,  1.51it/s]Extractor Predicting: 212it [02:18,  1.55it/s]Extractor Predicting: 213it [02:19,  1.52it/s]Extractor Predicting: 214it [02:20,  1.53it/s]Extractor Predicting: 215it [02:20,  1.55it/s]Extractor Predicting: 216it [02:21,  1.54it/s]Extractor Predicting: 217it [02:22,  1.53it/s]Extractor Predicting: 218it [02:22,  1.56it/s]Extractor Predicting: 219it [02:23,  1.48it/s]Extractor Predicting: 220it [02:24,  1.54it/s]Extractor Predicting: 221it [02:24,  1.50it/s]Extractor Predicting: 222it [02:25,  1.54it/s]Extractor Predicting: 223it [02:26,  1.58it/s]Extractor Predicting: 224it [02:26,  1.57it/s]Extractor Predicting: 225it [02:27,  1.60it/s]Extractor Predicting: 226it [02:27,  1.60it/s]Extractor Predicting: 227it [02:28,  1.62it/s]Extractor Predicting: 228it [02:29,  1.58it/s]Extractor Predicting: 229it [02:29,  1.58it/s]Extractor Predicting: 230it [02:30,  1.55it/s]Extractor Predicting: 231it [02:31,  1.53it/s]Extractor Predicting: 232it [02:31,  1.59it/s]Extractor Predicting: 233it [02:32,  1.57it/s]Extractor Predicting: 234it [02:33,  1.54it/s]Extractor Predicting: 235it [02:33,  1.55it/s]Extractor Predicting: 236it [02:34,  1.54it/s]Extractor Predicting: 237it [02:34,  1.63it/s]Extractor Predicting: 238it [02:35,  1.59it/s]Extractor Predicting: 239it [02:36,  1.59it/s]Extractor Predicting: 240it [02:36,  1.55it/s]Extractor Predicting: 241it [02:37,  1.50it/s]Extractor Predicting: 242it [02:38,  1.51it/s]Extractor Predicting: 243it [02:38,  1.56it/s]Extractor Predicting: 244it [02:39,  1.55it/s]Extractor Predicting: 245it [02:40,  1.53it/s]Extractor Predicting: 246it [02:40,  1.49it/s]Extractor Predicting: 247it [02:41,  1.48it/s]Extractor Predicting: 248it [02:42,  1.46it/s]Extractor Predicting: 249it [02:42,  1.48it/s]Extractor Predicting: 250it [02:43,  1.50it/s]Extractor Predicting: 251it [02:44,  1.51it/s]Extractor Predicting: 252it [02:44,  1.53it/s]Extractor Predicting: 253it [02:45,  1.29it/s]Extractor Predicting: 254it [02:46,  1.37it/s]Extractor Predicting: 255it [02:47,  1.41it/s]Extractor Predicting: 256it [02:47,  1.45it/s]Extractor Predicting: 257it [02:48,  1.46it/s]Extractor Predicting: 258it [02:49,  1.46it/s]Extractor Predicting: 259it [02:49,  1.49it/s]Extractor Predicting: 260it [02:50,  1.50it/s]Extractor Predicting: 261it [02:51,  1.51it/s]Extractor Predicting: 262it [02:51,  1.51it/s]Extractor Predicting: 263it [02:52,  1.51it/s]Extractor Predicting: 264it [02:53,  1.53it/s]Extractor Predicting: 265it [02:53,  1.52it/s]Extractor Predicting: 266it [02:54,  1.55it/s]Extractor Predicting: 267it [02:55,  1.54it/s]Extractor Predicting: 268it [02:55,  1.52it/s]Extractor Predicting: 269it [02:56,  1.53it/s]Extractor Predicting: 270it [02:57,  1.53it/s]Extractor Predicting: 271it [02:57,  1.54it/s]Extractor Predicting: 272it [02:58,  1.49it/s]Extractor Predicting: 273it [02:59,  1.47it/s]Extractor Predicting: 274it [02:59,  1.51it/s]Extractor Predicting: 275it [03:00,  1.53it/s]Extractor Predicting: 276it [03:00,  1.54it/s]Extractor Predicting: 277it [03:01,  1.50it/s]Extractor Predicting: 278it [03:02,  1.51it/s]Extractor Predicting: 279it [03:03,  1.50it/s]Extractor Predicting: 280it [03:03,  1.52it/s]Extractor Predicting: 281it [03:04,  1.46it/s]Extractor Predicting: 282it [03:05,  1.46it/s]Extractor Predicting: 283it [03:05,  1.45it/s]Extractor Predicting: 284it [03:06,  1.47it/s]Extractor Predicting: 285it [03:07,  1.43it/s]Extractor Predicting: 286it [03:07,  1.39it/s]Extractor Predicting: 287it [03:08,  1.41it/s]Extractor Predicting: 288it [03:09,  1.44it/s]Extractor Predicting: 289it [03:10,  1.44it/s]Extractor Predicting: 290it [03:10,  1.45it/s]Extractor Predicting: 291it [03:11,  1.46it/s]Extractor Predicting: 292it [03:12,  1.45it/s]Extractor Predicting: 293it [03:12,  1.47it/s]Extractor Predicting: 294it [03:13,  1.49it/s]Extractor Predicting: 295it [03:14,  1.51it/s]Extractor Predicting: 296it [03:14,  1.53it/s]Extractor Predicting: 297it [03:15,  1.53it/s]Extractor Predicting: 298it [03:15,  1.60it/s]Extractor Predicting: 299it [03:16,  1.59it/s]Extractor Predicting: 300it [03:17,  1.63it/s]Extractor Predicting: 301it [03:17,  1.61it/s]Extractor Predicting: 302it [03:18,  1.56it/s]Extractor Predicting: 303it [03:19,  1.53it/s]Extractor Predicting: 304it [03:19,  1.52it/s]Extractor Predicting: 305it [03:20,  1.53it/s]Extractor Predicting: 306it [03:21,  1.53it/s]Extractor Predicting: 307it [03:21,  1.55it/s]Extractor Predicting: 308it [03:22,  1.54it/s]Extractor Predicting: 309it [03:23,  1.50it/s]Extractor Predicting: 310it [03:23,  1.51it/s]Extractor Predicting: 311it [03:24,  1.51it/s]Extractor Predicting: 312it [03:24,  1.53it/s]Extractor Predicting: 313it [03:25,  1.50it/s]Extractor Predicting: 314it [03:26,  1.50it/s]Extractor Predicting: 315it [03:26,  1.54it/s]Extractor Predicting: 316it [03:27,  1.52it/s]Extractor Predicting: 317it [03:28,  1.50it/s]Extractor Predicting: 318it [03:28,  1.52it/s]Extractor Predicting: 319it [03:29,  1.47it/s]Extractor Predicting: 320it [03:30,  1.49it/s]Extractor Predicting: 321it [03:31,  1.50it/s]Extractor Predicting: 322it [03:31,  1.51it/s]Extractor Predicting: 323it [03:32,  1.52it/s]Extractor Predicting: 324it [03:32,  1.52it/s]Extractor Predicting: 325it [03:33,  1.55it/s]Extractor Predicting: 326it [03:34,  1.55it/s]Extractor Predicting: 327it [03:34,  1.57it/s]Extractor Predicting: 328it [03:35,  1.54it/s]Extractor Predicting: 329it [03:36,  1.53it/s]Extractor Predicting: 330it [03:36,  1.55it/s]Extractor Predicting: 331it [03:37,  1.53it/s]Extractor Predicting: 332it [03:38,  1.52it/s]Extractor Predicting: 333it [03:38,  1.50it/s]Extractor Predicting: 334it [03:39,  1.42it/s]Extractor Predicting: 335it [03:40,  1.45it/s]Extractor Predicting: 336it [03:40,  1.48it/s]Extractor Predicting: 337it [03:41,  1.48it/s]Extractor Predicting: 338it [03:42,  1.48it/s]Extractor Predicting: 339it [03:42,  1.46it/s]Extractor Predicting: 340it [03:43,  1.53it/s]Extractor Predicting: 341it [03:44,  1.51it/s]Extractor Predicting: 342it [03:44,  1.54it/s]Extractor Predicting: 343it [03:45,  1.57it/s]Extractor Predicting: 344it [03:46,  1.57it/s]Extractor Predicting: 345it [03:46,  1.60it/s]Extractor Predicting: 346it [03:47,  1.65it/s]Extractor Predicting: 347it [03:47,  1.68it/s]Extractor Predicting: 348it [03:48,  1.67it/s]Extractor Predicting: 349it [03:49,  1.68it/s]Extractor Predicting: 350it [03:49,  1.63it/s]Extractor Predicting: 351it [03:50,  1.60it/s]Extractor Predicting: 352it [03:50,  1.59it/s]Extractor Predicting: 353it [03:51,  1.59it/s]Extractor Predicting: 354it [03:52,  1.53it/s]Extractor Predicting: 355it [03:52,  1.57it/s]Extractor Predicting: 356it [03:53,  1.53it/s]Extractor Predicting: 357it [03:54,  1.53it/s]Extractor Predicting: 358it [03:54,  1.52it/s]Extractor Predicting: 359it [03:55,  1.50it/s]Extractor Predicting: 360it [03:56,  1.50it/s]Extractor Predicting: 361it [03:56,  1.48it/s]Extractor Predicting: 362it [03:57,  1.49it/s]Extractor Predicting: 363it [03:58,  1.51it/s]Extractor Predicting: 364it [03:58,  1.53it/s]Extractor Predicting: 365it [03:59,  1.33it/s]Extractor Predicting: 366it [04:00,  1.34it/s]Extractor Predicting: 367it [04:01,  1.38it/s]Extractor Predicting: 368it [04:01,  1.42it/s]Extractor Predicting: 369it [04:02,  1.43it/s]Extractor Predicting: 370it [04:03,  1.47it/s]Extractor Predicting: 371it [04:03,  1.45it/s]Extractor Predicting: 372it [04:04,  1.45it/s]Extractor Predicting: 373it [04:05,  1.46it/s]Extractor Predicting: 374it [04:06,  1.49it/s]Extractor Predicting: 375it [04:06,  1.52it/s]Extractor Predicting: 376it [04:07,  1.54it/s]Extractor Predicting: 377it [04:07,  1.60it/s]Extractor Predicting: 378it [04:08,  1.59it/s]Extractor Predicting: 379it [04:09,  1.61it/s]Extractor Predicting: 380it [04:09,  1.61it/s]Extractor Predicting: 381it [04:10,  1.61it/s]Extractor Predicting: 382it [04:10,  1.60it/s]Extractor Predicting: 383it [04:11,  1.60it/s]Extractor Predicting: 384it [04:12,  1.59it/s]Extractor Predicting: 385it [04:12,  1.61it/s]Extractor Predicting: 386it [04:13,  1.62it/s]Extractor Predicting: 387it [04:14,  1.63it/s]Extractor Predicting: 388it [04:14,  1.62it/s]Extractor Predicting: 389it [04:15,  1.63it/s]Extractor Predicting: 390it [04:15,  1.63it/s]Extractor Predicting: 391it [04:16,  1.60it/s]Extractor Predicting: 392it [04:17,  1.65it/s]Extractor Predicting: 393it [04:17,  1.64it/s]Extractor Predicting: 394it [04:18,  1.66it/s]Extractor Predicting: 395it [04:18,  1.63it/s]Extractor Predicting: 396it [04:19,  1.62it/s]Extractor Predicting: 397it [04:20,  1.60it/s]Extractor Predicting: 398it [04:20,  1.56it/s]Extractor Predicting: 399it [04:21,  1.54it/s]Extractor Predicting: 400it [04:22,  1.53it/s]Extractor Predicting: 401it [04:22,  1.53it/s]Extractor Predicting: 402it [04:23,  1.55it/s]Extractor Predicting: 403it [04:24,  1.57it/s]Extractor Predicting: 404it [04:24,  1.52it/s]Extractor Predicting: 405it [04:25,  1.52it/s]Extractor Predicting: 406it [04:26,  1.52it/s]Extractor Predicting: 407it [04:26,  1.52it/s]Extractor Predicting: 408it [04:27,  1.51it/s]Extractor Predicting: 409it [04:28,  1.49it/s]Extractor Predicting: 410it [04:28,  1.49it/s]Extractor Predicting: 411it [04:29,  1.50it/s]Extractor Predicting: 412it [04:30,  1.54it/s]Extractor Predicting: 413it [04:30,  1.49it/s]Extractor Predicting: 414it [04:31,  1.49it/s]Extractor Predicting: 415it [04:32,  1.51it/s]Extractor Predicting: 416it [04:32,  1.51it/s]Extractor Predicting: 417it [04:33,  1.52it/s]Extractor Predicting: 418it [04:34,  1.53it/s]Extractor Predicting: 419it [04:34,  1.54it/s]Extractor Predicting: 420it [04:35,  1.53it/s]Extractor Predicting: 421it [04:36,  1.54it/s]Extractor Predicting: 422it [04:36,  1.52it/s]Extractor Predicting: 423it [04:37,  1.49it/s]Extractor Predicting: 424it [04:38,  1.52it/s]Extractor Predicting: 425it [04:38,  1.51it/s]Extractor Predicting: 426it [04:39,  1.57it/s]Extractor Predicting: 427it [04:39,  1.61it/s]Extractor Predicting: 428it [04:40,  1.57it/s]Extractor Predicting: 429it [04:41,  1.61it/s]Extractor Predicting: 430it [04:41,  1.62it/s]Extractor Predicting: 431it [04:42,  1.67it/s]Extractor Predicting: 432it [04:42,  1.64it/s]Extractor Predicting: 433it [04:43,  1.58it/s]Extractor Predicting: 434it [04:44,  1.61it/s]Extractor Predicting: 435it [04:44,  1.60it/s]Extractor Predicting: 436it [04:45,  1.59it/s]Extractor Predicting: 437it [04:46,  1.64it/s]Extractor Predicting: 438it [04:46,  1.62it/s]Extractor Predicting: 439it [04:47,  1.63it/s]Extractor Predicting: 440it [04:47,  1.60it/s]Extractor Predicting: 441it [04:48,  1.61it/s]Extractor Predicting: 442it [04:49,  1.64it/s]Extractor Predicting: 443it [04:49,  1.61it/s]Extractor Predicting: 444it [04:50,  1.63it/s]Extractor Predicting: 445it [04:50,  1.64it/s]Extractor Predicting: 446it [04:51,  1.63it/s]Extractor Predicting: 447it [04:52,  1.65it/s]Extractor Predicting: 448it [04:52,  1.67it/s]Extractor Predicting: 449it [04:53,  1.67it/s]Extractor Predicting: 450it [04:53,  1.66it/s]Extractor Predicting: 451it [04:54,  1.63it/s]Extractor Predicting: 452it [04:55,  1.60it/s]Extractor Predicting: 453it [04:55,  1.60it/s]Extractor Predicting: 454it [04:56,  1.54it/s]Extractor Predicting: 455it [04:57,  1.58it/s]Extractor Predicting: 456it [04:57,  1.61it/s]Extractor Predicting: 457it [04:58,  1.62it/s]Extractor Predicting: 458it [04:58,  1.64it/s]Extractor Predicting: 459it [04:59,  1.61it/s]Extractor Predicting: 460it [05:00,  1.56it/s]Extractor Predicting: 461it [05:00,  1.53it/s]Extractor Predicting: 462it [05:01,  1.48it/s]Extractor Predicting: 463it [05:02,  1.48it/s]Extractor Predicting: 464it [05:03,  1.52it/s]Extractor Predicting: 465it [05:03,  1.53it/s]Extractor Predicting: 466it [05:04,  1.52it/s]Extractor Predicting: 467it [05:05,  1.48it/s]Extractor Predicting: 468it [05:05,  1.50it/s]Extractor Predicting: 469it [05:06,  1.52it/s]Extractor Predicting: 470it [05:07,  1.49it/s]Extractor Predicting: 471it [05:07,  1.45it/s]Extractor Predicting: 472it [05:08,  1.46it/s]Extractor Predicting: 473it [05:09,  1.49it/s]Extractor Predicting: 474it [05:09,  1.53it/s]Extractor Predicting: 475it [05:10,  1.54it/s]Extractor Predicting: 476it [05:11,  1.51it/s]Extractor Predicting: 477it [05:11,  1.48it/s]Extractor Predicting: 478it [05:12,  1.46it/s]Extractor Predicting: 479it [05:13,  1.45it/s]Extractor Predicting: 480it [05:13,  1.46it/s]Extractor Predicting: 481it [05:14,  1.47it/s]Extractor Predicting: 482it [05:15,  1.46it/s]Extractor Predicting: 483it [05:15,  1.50it/s]Extractor Predicting: 484it [05:16,  1.49it/s]Extractor Predicting: 485it [05:17,  1.54it/s]Extractor Predicting: 486it [05:17,  1.55it/s]Extractor Predicting: 487it [05:18,  1.54it/s]Extractor Predicting: 488it [05:19,  1.51it/s]Extractor Predicting: 489it [05:20,  1.32it/s]Extractor Predicting: 490it [05:20,  1.37it/s]Extractor Predicting: 491it [05:21,  1.40it/s]Extractor Predicting: 492it [05:22,  1.41it/s]Extractor Predicting: 493it [05:22,  1.40it/s]Extractor Predicting: 494it [05:23,  1.43it/s]Extractor Predicting: 495it [05:24,  1.49it/s]Extractor Predicting: 496it [05:24,  1.51it/s]Extractor Predicting: 496it [05:24,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:16:56,852 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:16:56,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:16:56,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:16:56,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:16:56,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:16:57,655 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:16:57,656 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:16:58,304 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:16:59,429 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:16:59,429 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:17:02,529 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:17:02,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:17:02,571 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:17:02,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:17:02,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:17:03,515 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:17:03,517 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:17:04,192 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:17:04,473 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:17:04,473 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5065434949961509,
  "recall": 0.0552755376344086,
  "score": 0.0996743164432326,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 10529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.49it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.47it/s]Extractor Predicting: 13it [00:08,  1.38it/s]Extractor Predicting: 14it [00:09,  1.42it/s]Extractor Predicting: 15it [00:10,  1.40it/s]Extractor Predicting: 16it [00:11,  1.42it/s]Extractor Predicting: 17it [00:11,  1.40it/s]Extractor Predicting: 18it [00:12,  1.38it/s]Extractor Predicting: 19it [00:13,  1.35it/s]Extractor Predicting: 20it [00:14,  1.30it/s]Extractor Predicting: 21it [00:14,  1.32it/s]Extractor Predicting: 22it [00:15,  1.30it/s]Extractor Predicting: 23it [00:16,  1.29it/s]Extractor Predicting: 24it [00:17,  1.34it/s]Extractor Predicting: 25it [00:17,  1.40it/s]Extractor Predicting: 26it [00:18,  1.41it/s]Extractor Predicting: 27it [00:19,  1.40it/s]Extractor Predicting: 28it [00:19,  1.41it/s]Extractor Predicting: 29it [00:20,  1.39it/s]Extractor Predicting: 30it [00:21,  1.39it/s]Extractor Predicting: 31it [00:22,  1.40it/s]Extractor Predicting: 32it [00:22,  1.41it/s]Extractor Predicting: 33it [00:23,  1.46it/s]Extractor Predicting: 34it [00:24,  1.48it/s]Extractor Predicting: 35it [00:24,  1.46it/s]Extractor Predicting: 36it [00:25,  1.46it/s]Extractor Predicting: 37it [00:26,  1.46it/s]Extractor Predicting: 38it [00:26,  1.46it/s]Extractor Predicting: 39it [00:27,  1.49it/s]Extractor Predicting: 40it [00:28,  1.50it/s]Extractor Predicting: 41it [00:28,  1.49it/s]Extractor Predicting: 42it [00:29,  1.51it/s]Extractor Predicting: 43it [00:30,  1.51it/s]Extractor Predicting: 44it [00:30,  1.52it/s]Extractor Predicting: 45it [00:31,  1.51it/s]Extractor Predicting: 46it [00:32,  1.46it/s]Extractor Predicting: 47it [00:32,  1.46it/s]Extractor Predicting: 48it [00:33,  1.48it/s]Extractor Predicting: 49it [00:34,  1.50it/s]Extractor Predicting: 50it [00:34,  1.49it/s]Extractor Predicting: 51it [00:35,  1.49it/s]Extractor Predicting: 52it [00:36,  1.49it/s]Extractor Predicting: 53it [00:36,  1.48it/s]Extractor Predicting: 54it [00:37,  1.47it/s]Extractor Predicting: 55it [00:38,  1.50it/s]Extractor Predicting: 56it [00:38,  1.52it/s]Extractor Predicting: 57it [00:39,  1.55it/s]Extractor Predicting: 58it [00:40,  1.55it/s]Extractor Predicting: 59it [00:40,  1.54it/s]Extractor Predicting: 60it [00:41,  1.45it/s]Extractor Predicting: 61it [00:42,  1.48it/s]Extractor Predicting: 62it [00:42,  1.50it/s]Extractor Predicting: 63it [00:43,  1.46it/s]Extractor Predicting: 64it [00:44,  1.47it/s]Extractor Predicting: 65it [00:44,  1.44it/s]Extractor Predicting: 66it [00:45,  1.50it/s]Extractor Predicting: 67it [00:46,  1.57it/s]Extractor Predicting: 68it [00:46,  1.61it/s]Extractor Predicting: 69it [00:47,  1.69it/s]Extractor Predicting: 70it [00:47,  1.77it/s]Extractor Predicting: 71it [00:48,  1.81it/s]Extractor Predicting: 72it [00:48,  1.81it/s]Extractor Predicting: 73it [00:49,  1.84it/s]Extractor Predicting: 74it [00:49,  1.83it/s]Extractor Predicting: 75it [00:50,  1.81it/s]Extractor Predicting: 76it [00:50,  1.82it/s]Extractor Predicting: 77it [00:51,  1.78it/s]Extractor Predicting: 78it [00:52,  1.80it/s]Extractor Predicting: 79it [00:52,  1.84it/s]Extractor Predicting: 80it [00:53,  1.81it/s]Extractor Predicting: 81it [00:53,  1.81it/s]Extractor Predicting: 82it [00:54,  1.83it/s]Extractor Predicting: 83it [00:54,  1.86it/s]Extractor Predicting: 84it [00:55,  1.88it/s]Extractor Predicting: 85it [00:55,  1.87it/s]Extractor Predicting: 86it [00:56,  1.85it/s]Extractor Predicting: 87it [00:56,  1.86it/s]Extractor Predicting: 88it [00:57,  1.81it/s]Extractor Predicting: 89it [00:58,  1.81it/s]Extractor Predicting: 90it [00:58,  1.80it/s]Extractor Predicting: 91it [00:59,  1.80it/s]Extractor Predicting: 92it [00:59,  1.83it/s]Extractor Predicting: 93it [01:00,  1.84it/s]Extractor Predicting: 94it [01:00,  1.85it/s]Extractor Predicting: 95it [01:01,  1.85it/s]Extractor Predicting: 96it [01:02,  1.67it/s]Extractor Predicting: 97it [01:02,  1.61it/s]Extractor Predicting: 98it [01:03,  1.57it/s]Extractor Predicting: 99it [01:04,  1.48it/s]Extractor Predicting: 100it [01:04,  1.46it/s]Extractor Predicting: 101it [01:05,  1.34it/s]Extractor Predicting: 102it [01:06,  1.36it/s]Extractor Predicting: 103it [01:07,  1.37it/s]Extractor Predicting: 104it [01:07,  1.40it/s]Extractor Predicting: 105it [01:08,  1.40it/s]Extractor Predicting: 106it [01:09,  1.41it/s]Extractor Predicting: 107it [01:09,  1.42it/s]Extractor Predicting: 108it [01:10,  1.41it/s]Extractor Predicting: 109it [01:11,  1.42it/s]Extractor Predicting: 110it [01:12,  1.42it/s]Extractor Predicting: 111it [01:12,  1.41it/s]Extractor Predicting: 112it [01:13,  1.44it/s]Extractor Predicting: 113it [01:14,  1.48it/s]Extractor Predicting: 114it [01:14,  1.53it/s]Extractor Predicting: 115it [01:15,  1.54it/s]Extractor Predicting: 116it [01:15,  1.57it/s]Extractor Predicting: 117it [01:16,  1.58it/s]Extractor Predicting: 118it [01:17,  1.56it/s]Extractor Predicting: 119it [01:17,  1.56it/s]Extractor Predicting: 120it [01:18,  1.60it/s]Extractor Predicting: 121it [01:19,  1.60it/s]Extractor Predicting: 122it [01:19,  1.62it/s]Extractor Predicting: 123it [01:20,  1.61it/s]Extractor Predicting: 124it [01:20,  1.56it/s]Extractor Predicting: 125it [01:21,  1.56it/s]Extractor Predicting: 126it [01:22,  1.58it/s]Extractor Predicting: 127it [01:22,  1.53it/s]Extractor Predicting: 128it [01:23,  1.50it/s]Extractor Predicting: 129it [01:24,  1.50it/s]Extractor Predicting: 130it [01:24,  1.50it/s]Extractor Predicting: 131it [01:25,  1.48it/s]Extractor Predicting: 132it [01:26,  1.45it/s]Extractor Predicting: 133it [01:26,  1.51it/s]Extractor Predicting: 133it [01:27,  1.53it/s]
[INFO|configuration_utils.py:515] 2023-08-29 11:18:37,079 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:18:37,080 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 11:18:37,143 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:18:37,144 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 11:18:37,173 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 11:18:53,093 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 11:18:53,173 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 11:18:53,410 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 11:18:53,411 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki/unseen_15_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 11:18:53,567 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,668 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,668 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,669 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,669 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,669 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 11:18:53,669 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8441379310344828,
  "recall": 0.08111332007952286,
  "score": 0.1480048367593712,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 11:18:54,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:18:55,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:18:56,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:18:56,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:18:57,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:18:58,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:18:59,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:18:59,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:00,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:01,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:02,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:02,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:03,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:04,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:04,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:05,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:06,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:06,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:07,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:08,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:08,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:15<04:51, 15.33s/it][WARNING|generation_utils.py:914] 2023-08-29 11:19:09,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:10,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:10,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:11,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:12,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:13,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:13,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:14,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:14,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:15,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:16,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:17,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:17,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:18,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:19,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:19,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:20,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:21,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:21,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:22,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:23,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:29<04:25, 14.73s/it][WARNING|generation_utils.py:914] 2023-08-29 11:19:23,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:24,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:25,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:25,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:26,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:27,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:28,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:29,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:29,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:30,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:31,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:32,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:32,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:33,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:34,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:34,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:35,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:36,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:37,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:38,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:38,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:39,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:40,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:46<04:28, 15.80s/it][WARNING|generation_utils.py:914] 2023-08-29 11:19:40,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:41,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:42,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:42,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:43,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:44,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:44,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:45,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:46,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:46,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:47,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:48,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:48,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:49,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:49,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:50,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:51,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:51,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:52,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:53,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:59<03:53, 14.62s/it][WARNING|generation_utils.py:914] 2023-08-29 11:19:53,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:54,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:54,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:55,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:56,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:56,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:57,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:58,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:59,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:19:59,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:00,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:01,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:01,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:02,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:03,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:04,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:04,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:05,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:06,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:06,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:07,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:08,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:15<03:44, 14.95s/it][WARNING|generation_utils.py:914] 2023-08-29 11:20:09,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:09,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:10,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:11,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:11,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:12,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:13,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:14,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:14,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:15,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:16,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:16,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:17,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:18,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:18,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:19,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:20,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:20,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:21,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:22,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:22,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:29<03:25, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-29 11:20:23,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:24,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:24,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:25,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:26,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:26,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:27,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:28,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:29,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:29,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:30,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:31,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:31,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:32,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:33,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:34,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:34,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:35,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:36,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:36,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:37,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:38,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:44<03:15, 15.01s/it][WARNING|generation_utils.py:914] 2023-08-29 11:20:39,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:39,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:40,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:41,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:41,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:42,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:43,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:43,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:44,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:45,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:45,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:46,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:47,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:48,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:48,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:49,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:49,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:50,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:51,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:52,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:52,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:53,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:00<03:01, 15.09s/it][WARNING|generation_utils.py:914] 2023-08-29 11:20:54,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:55,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:55,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:56,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:57,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:57,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:58,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:20:59,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:00,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:01,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:01,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:02,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:03,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:04,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:04,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:05,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:06,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:07,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:07,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:08,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:09,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:10,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:10,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:17<02:53, 15.78s/it][WARNING|generation_utils.py:914] 2023-08-29 11:21:11,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:12,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:13,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:13,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:14,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:15,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:15,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:16,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:17,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:17,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:18,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:19,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:19,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:20,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:21,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:21,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:22,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:23,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:23,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:24,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:25,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:25,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:26,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:32<02:36, 15.69s/it][WARNING|generation_utils.py:914] 2023-08-29 11:21:27,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:27,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:28,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:29,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:30,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:30,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:31,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:31,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:32,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:33,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:33,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:34,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:35,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:35,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:36,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:37,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:37,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:38,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:39,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:39,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:40,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:46<02:16, 15.12s/it][WARNING|generation_utils.py:914] 2023-08-29 11:21:40,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:41,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:42,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:42,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:43,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:44,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:44,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:45,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:45,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:46,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:47,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:47,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:48,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:48,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:49,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:50,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:50,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:51,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:52,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:52,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:53,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:53,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:00<01:57, 14.63s/it][WARNING|generation_utils.py:914] 2023-08-29 11:21:54,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:55,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:55,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:56,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:57,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:58,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:58,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:21:59,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:00,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:01,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:01,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:02,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:03,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:04,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:04,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:05,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:06,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:07,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:08,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:08,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:09,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:15<01:44, 14.94s/it][WARNING|generation_utils.py:914] 2023-08-29 11:22:10,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:10,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:11,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:12,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:12,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:13,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:13,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:14,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:15,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:15,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:16,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:17,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:17,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:18,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:19,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:19,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:20,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:21,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:21,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:22,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:22,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:29<01:27, 14.51s/it][WARNING|generation_utils.py:914] 2023-08-29 11:22:23,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:24,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:25,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:25,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:26,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:27,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:28,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:28,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:29,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:30,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:31,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:32,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:32,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:33,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:34,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:35,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:35,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:36,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:37,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:38,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:39,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:39,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:46<01:16, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-29 11:22:40,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:41,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:42,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:42,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:43,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:44,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:44,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:45,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:46,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:46,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:47,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:48,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:49,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:49,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:50,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:51,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:51,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:52,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:53,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:54,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:54,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:55,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:02<01:01, 15.39s/it][WARNING|generation_utils.py:914] 2023-08-29 11:22:56,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:57,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:57,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:58,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:59,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:22:59,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:00,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:01,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:02,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:02,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:03,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:04,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:05,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:05,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:06,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:07,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:07,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:08,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:09,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:10,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:11,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:12,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:18<00:47, 15.83s/it][WARNING|generation_utils.py:914] 2023-08-29 11:23:13,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:13,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:14,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:15,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:16,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:16,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:17,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:18,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:18,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:19,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:20,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:20,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:21,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:22,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:23,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:23,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:24,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:25,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:25,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:26,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:27,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:28,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:34<00:31, 15.78s/it][WARNING|generation_utils.py:914] 2023-08-29 11:23:28,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:29,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:30,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:30,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:31,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:32,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:32,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:33,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:34,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:34,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:35,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:36,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:36,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:37,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:38,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:38,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:39,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:40,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:40,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:41,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:42,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:48<00:15, 15.29s/it][WARNING|generation_utils.py:914] 2023-08-29 11:23:42,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:44,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:44,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:45,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:46,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:47,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:48,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:48,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:49,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:50,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:51,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:52,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:53,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:53,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:54,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:55,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:56,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:56,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:57,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:58,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:59,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 11:23:59,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:06<00:00, 16.07s/it]Generating: 100%|██████████| 20/20 [05:06<00:00, 15.33s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:10,389 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:10,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:10,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:10,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:10,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:24:11,477 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:24:11,478 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:24:12,196 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:24:13,433 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:24:13,493 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:16,797 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:16,856 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:16,856 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:16,856 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:24:16,856 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:24:17,840 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:24:17,841 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:24:18,495 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:24:18,787 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:24:18,787 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 532, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : characters .', 'success_rate': 0.9270833333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : lowest point . Context : Later in the year ( October 18 , 1874 ) he met and married the young John William I , son of Alexander II of Austria , the 1st Baronet of Leipzig . Head Entity : 4th Baronet , Tail Entity : Leipzig .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 486, 'raw': 576}
{'target': 600, 'success': 512, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 514, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.940625, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 575, 'raw': 672}
{'target': 600, 'success': 603, 'raw': 704}
{'prompt': 'Relation : work location .', 'success_rate': 0.8565340909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : follows .', 'success_rate': 0.8636363636363636, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 586, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : has quality .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 563, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8342391304347826, 'errors': {'', "('The White Stripes', 'instrument', '', 'He has performed for Warner Bros. , and the group s The White Stripes among others .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 589, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : league .', 'success_rate': 0.8369565217391305, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9315476190476191, 'errors': {''}}
['Relation : located on astronomical body . Context : The Moon and Neptune are the only two globular clusters ( globular clusters ) at different distances from Earth , the other being the Sun in the southern region of the Solar System . Head Entity : Sun , Tail Entity : Planet .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 514, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9002976190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 439, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9151785714285714, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 493, 'raw': 576}
{'target': 600, 'success': 520, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8536931818181818, 'errors': {'', "('Henry I', 'mother', '', 'In 437 , after his mother died , King George II s son Henry I gave him crown with the title of Prince William of Meuse .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 453, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 625, 'raw': 704}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.8877840909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : residence . Context : Later in the year he moved to Paris , where he studied at Lillehammer at the Royal Academy of Music in 1836 . Head Entity : Louis , Tail Entity : Paris .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8764204545454546, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.8934659090909091, 'errors': {'', "('borderless cooperation', 'shares border with', '', 'The main problem with borderless cooperation is that there are few available trade agreements with other nations , and many people are unable to cross the border with their country of origin through the border .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 504, 'raw': 576}
{'target': 600, 'success': 533, 'raw': 608}
{'target': 600, 'success': 563, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.8778409090909091, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 14631
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14731, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.36it/s]Extractor Estimating: 2it [00:01,  1.47it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.61it/s]Extractor Estimating: 5it [00:03,  1.63it/s]Extractor Estimating: 6it [00:03,  1.60it/s]Extractor Estimating: 7it [00:04,  1.57it/s]Extractor Estimating: 8it [00:05,  1.57it/s]Extractor Estimating: 9it [00:05,  1.65it/s]Extractor Estimating: 10it [00:06,  1.59it/s]Extractor Estimating: 11it [00:06,  1.59it/s]Extractor Estimating: 12it [00:07,  1.58it/s]Extractor Estimating: 13it [00:08,  1.56it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.59it/s]Extractor Estimating: 16it [00:10,  1.49it/s]Extractor Estimating: 17it [00:10,  1.48it/s]Extractor Estimating: 18it [00:11,  1.55it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:12,  1.59it/s]Extractor Estimating: 21it [00:13,  1.58it/s]Extractor Estimating: 22it [00:14,  1.59it/s]Extractor Estimating: 23it [00:14,  1.63it/s]Extractor Estimating: 24it [00:15,  1.66it/s]Extractor Estimating: 25it [00:15,  1.66it/s]Extractor Estimating: 26it [00:16,  1.58it/s]Extractor Estimating: 27it [00:17,  1.50it/s]Extractor Estimating: 28it [00:17,  1.53it/s]Extractor Estimating: 29it [00:18,  1.52it/s]Extractor Estimating: 30it [00:19,  1.55it/s]Extractor Estimating: 31it [00:19,  1.58it/s]Extractor Estimating: 32it [00:20,  1.55it/s]Extractor Estimating: 33it [00:21,  1.57it/s]Extractor Estimating: 34it [00:21,  1.59it/s]Extractor Estimating: 35it [00:22,  1.62it/s]Extractor Estimating: 36it [00:22,  1.66it/s]Extractor Estimating: 37it [00:23,  1.65it/s]Extractor Estimating: 38it [00:24,  1.62it/s]Extractor Estimating: 39it [00:24,  1.65it/s]Extractor Estimating: 40it [00:25,  1.62it/s]Extractor Estimating: 41it [00:25,  1.69it/s]Extractor Estimating: 42it [00:26,  1.62it/s]Extractor Estimating: 43it [00:27,  1.59it/s]Extractor Estimating: 44it [00:27,  1.62it/s]Extractor Estimating: 45it [00:28,  1.67it/s]Extractor Estimating: 46it [00:28,  1.67it/s]Extractor Estimating: 47it [00:29,  1.65it/s]Extractor Estimating: 48it [00:30,  1.69it/s]Extractor Estimating: 49it [00:30,  1.70it/s]Extractor Estimating: 50it [00:31,  1.71it/s]Extractor Estimating: 51it [00:31,  1.69it/s]Extractor Estimating: 52it [00:32,  1.68it/s]Extractor Estimating: 53it [00:33,  1.63it/s]Extractor Estimating: 54it [00:33,  1.59it/s]Extractor Estimating: 55it [00:34,  1.53it/s]Extractor Estimating: 56it [00:35,  1.52it/s]Extractor Estimating: 57it [00:35,  1.47it/s]Extractor Estimating: 58it [00:36,  1.49it/s]Extractor Estimating: 59it [00:37,  1.54it/s]Extractor Estimating: 60it [00:37,  1.59it/s]Extractor Estimating: 61it [00:38,  1.60it/s]Extractor Estimating: 62it [00:38,  1.57it/s]Extractor Estimating: 63it [00:39,  1.47it/s]Extractor Estimating: 64it [00:40,  1.49it/s]Extractor Estimating: 65it [00:41,  1.49it/s]Extractor Estimating: 66it [00:41,  1.49it/s]Extractor Estimating: 67it [00:42,  1.51it/s]Extractor Estimating: 68it [00:43,  1.47it/s]Extractor Estimating: 69it [00:43,  1.51it/s]Extractor Estimating: 70it [00:44,  1.56it/s]Extractor Estimating: 71it [00:44,  1.57it/s]Extractor Estimating: 72it [00:45,  1.54it/s]Extractor Estimating: 73it [00:46,  1.50it/s]Extractor Estimating: 74it [00:46,  1.52it/s]Extractor Estimating: 75it [00:47,  1.48it/s]Extractor Estimating: 76it [00:48,  1.57it/s]Extractor Estimating: 77it [00:48,  1.61it/s]Extractor Estimating: 78it [00:49,  1.62it/s]Extractor Estimating: 79it [00:49,  1.69it/s]Extractor Estimating: 80it [00:50,  1.67it/s]Extractor Estimating: 81it [00:51,  1.65it/s]Extractor Estimating: 82it [00:51,  1.72it/s]Extractor Estimating: 83it [00:52,  1.79it/s]Extractor Estimating: 84it [00:52,  1.81it/s]Extractor Estimating: 85it [00:53,  1.83it/s]Extractor Estimating: 86it [00:53,  1.81it/s]Extractor Estimating: 87it [00:54,  1.76it/s]Extractor Estimating: 88it [00:55,  1.78it/s]Extractor Estimating: 89it [00:55,  1.81it/s]Extractor Estimating: 90it [00:56,  1.79it/s]Extractor Estimating: 91it [00:56,  1.74it/s]Extractor Estimating: 92it [00:57,  1.67it/s]Extractor Estimating: 93it [00:57,  1.68it/s]Extractor Estimating: 94it [00:58,  1.77it/s]Extractor Estimating: 95it [00:58,  1.84it/s]Extractor Estimating: 96it [00:59,  1.84it/s]Extractor Estimating: 97it [01:00,  1.75it/s]Extractor Estimating: 98it [01:00,  1.75it/s]Extractor Estimating: 99it [01:01,  1.79it/s]Extractor Estimating: 100it [01:01,  1.78it/s]Extractor Estimating: 101it [01:02,  1.75it/s]Extractor Estimating: 102it [01:03,  1.69it/s]Extractor Estimating: 103it [01:03,  1.68it/s]Extractor Estimating: 104it [01:04,  1.68it/s]Extractor Estimating: 105it [01:04,  1.64it/s]Extractor Estimating: 106it [01:05,  1.63it/s]Extractor Estimating: 107it [01:06,  1.61it/s]Extractor Estimating: 108it [01:06,  1.64it/s]Extractor Estimating: 109it [01:07,  1.65it/s]Extractor Estimating: 110it [01:07,  1.60it/s]Extractor Estimating: 111it [01:08,  1.65it/s]Extractor Estimating: 112it [01:09,  1.61it/s]Extractor Estimating: 113it [01:09,  1.62it/s]Extractor Estimating: 114it [01:10,  1.56it/s]Extractor Estimating: 115it [01:11,  1.52it/s]Extractor Estimating: 116it [01:11,  1.54it/s]Extractor Estimating: 117it [01:12,  1.39it/s]Extractor Estimating: 118it [01:13,  1.43it/s]Extractor Estimating: 119it [01:14,  1.47it/s]Extractor Estimating: 120it [01:14,  1.52it/s]Extractor Estimating: 121it [01:15,  1.48it/s]Extractor Estimating: 122it [01:15,  1.50it/s]Extractor Estimating: 123it [01:16,  1.46it/s]Extractor Estimating: 124it [01:17,  1.52it/s]Extractor Estimating: 125it [01:18,  1.47it/s]Extractor Estimating: 126it [01:18,  1.48it/s]Extractor Estimating: 127it [01:19,  1.55it/s]Extractor Estimating: 128it [01:19,  1.58it/s]Extractor Estimating: 129it [01:20,  1.58it/s]Extractor Estimating: 130it [01:21,  1.58it/s]Extractor Estimating: 131it [01:21,  1.53it/s]Extractor Estimating: 132it [01:22,  1.57it/s]Extractor Estimating: 133it [01:23,  1.58it/s]Extractor Estimating: 134it [01:23,  1.62it/s]Extractor Estimating: 135it [01:24,  1.61it/s]Extractor Estimating: 136it [01:24,  1.58it/s]Extractor Estimating: 137it [01:25,  1.60it/s]Extractor Estimating: 138it [01:26,  1.61it/s]Extractor Estimating: 139it [01:26,  1.58it/s]Extractor Estimating: 140it [01:27,  1.58it/s]Extractor Estimating: 141it [01:28,  1.58it/s]Extractor Estimating: 142it [01:28,  1.57it/s]Extractor Estimating: 143it [01:29,  1.62it/s]Extractor Estimating: 144it [01:29,  1.61it/s]Extractor Estimating: 145it [01:30,  1.65it/s]Extractor Estimating: 146it [01:31,  1.63it/s]Extractor Estimating: 147it [01:31,  1.66it/s]Extractor Estimating: 148it [01:32,  1.61it/s]Extractor Estimating: 149it [01:32,  1.63it/s]Extractor Estimating: 150it [01:33,  1.62it/s]Extractor Estimating: 151it [01:34,  1.54it/s]Extractor Estimating: 152it [01:34,  1.57it/s]Extractor Estimating: 153it [01:35,  1.56it/s]Extractor Estimating: 154it [01:36,  1.62it/s]Extractor Estimating: 155it [01:36,  1.63it/s]Extractor Estimating: 156it [01:37,  1.57it/s]Extractor Estimating: 157it [01:38,  1.58it/s]Extractor Estimating: 158it [01:38,  1.55it/s]Extractor Estimating: 159it [01:39,  1.55it/s]Extractor Estimating: 160it [01:40,  1.54it/s]Extractor Estimating: 161it [01:40,  1.49it/s]Extractor Estimating: 162it [01:41,  1.50it/s]Extractor Estimating: 163it [01:42,  1.53it/s]Extractor Estimating: 164it [01:42,  1.53it/s]Extractor Estimating: 165it [01:43,  1.53it/s]Extractor Estimating: 166it [01:44,  1.51it/s]Extractor Estimating: 167it [01:44,  1.51it/s]Extractor Estimating: 168it [01:45,  1.48it/s]Extractor Estimating: 169it [01:46,  1.48it/s]Extractor Estimating: 170it [01:46,  1.55it/s]Extractor Estimating: 171it [01:47,  1.54it/s]Extractor Estimating: 172it [01:47,  1.58it/s]Extractor Estimating: 173it [01:48,  1.61it/s]Extractor Estimating: 174it [01:49,  1.56it/s]Extractor Estimating: 175it [01:49,  1.57it/s]Extractor Estimating: 176it [01:50,  1.59it/s]Extractor Estimating: 177it [01:51,  1.57it/s]Extractor Estimating: 178it [01:51,  1.59it/s]Extractor Estimating: 179it [01:53,  1.10s/it]Extractor Estimating: 180it [01:54,  1.05it/s]Extractor Estimating: 181it [01:55,  1.18it/s]Extractor Estimating: 182it [01:55,  1.24it/s]Extractor Estimating: 183it [01:56,  1.35it/s]Extractor Estimating: 184it [01:57,  1.41it/s]Extractor Estimating: 185it [01:57,  1.47it/s]Extractor Estimating: 186it [01:58,  1.50it/s]Extractor Estimating: 187it [01:58,  1.50it/s]Extractor Estimating: 188it [01:59,  1.55it/s]Extractor Estimating: 189it [02:00,  1.50it/s]Extractor Estimating: 190it [02:00,  1.54it/s]Extractor Estimating: 191it [02:01,  1.46it/s]Extractor Estimating: 192it [02:02,  1.48it/s]Extractor Estimating: 193it [02:02,  1.49it/s]Extractor Estimating: 194it [02:03,  1.51it/s]Extractor Estimating: 195it [02:04,  1.51it/s]Extractor Estimating: 196it [02:04,  1.55it/s]Extractor Estimating: 197it [02:05,  1.52it/s]Extractor Estimating: 198it [02:06,  1.57it/s]Extractor Estimating: 199it [02:06,  1.60it/s]Extractor Estimating: 200it [02:07,  1.58it/s]Extractor Estimating: 201it [02:08,  1.54it/s]Extractor Estimating: 202it [02:08,  1.53it/s]Extractor Estimating: 203it [02:09,  1.57it/s]Extractor Estimating: 204it [02:09,  1.62it/s]Extractor Estimating: 205it [02:10,  1.66it/s]Extractor Estimating: 206it [02:11,  1.63it/s]Extractor Estimating: 207it [02:11,  1.51it/s]Extractor Estimating: 208it [02:12,  1.56it/s]Extractor Estimating: 209it [02:13,  1.53it/s]Extractor Estimating: 210it [02:13,  1.54it/s]Extractor Estimating: 211it [02:14,  1.56it/s]Extractor Estimating: 212it [02:15,  1.52it/s]Extractor Estimating: 213it [02:15,  1.52it/s]Extractor Estimating: 214it [02:16,  1.44it/s]Extractor Estimating: 215it [02:17,  1.51it/s]Extractor Estimating: 216it [02:17,  1.52it/s]Extractor Estimating: 217it [02:18,  1.52it/s]Extractor Estimating: 218it [02:19,  1.56it/s]Extractor Estimating: 219it [02:19,  1.60it/s]Extractor Estimating: 220it [02:20,  1.59it/s]Extractor Estimating: 221it [02:20,  1.55it/s]Extractor Estimating: 222it [02:21,  1.63it/s]Extractor Estimating: 223it [02:22,  1.62it/s]Extractor Estimating: 224it [02:22,  1.59it/s]Extractor Estimating: 225it [02:23,  1.56it/s]Extractor Estimating: 226it [02:24,  1.55it/s]Extractor Estimating: 227it [02:24,  1.54it/s]Extractor Estimating: 228it [02:25,  1.51it/s]Extractor Estimating: 229it [02:26,  1.50it/s]Extractor Estimating: 230it [02:26,  1.48it/s]Extractor Estimating: 231it [02:27,  1.52it/s]Extractor Estimating: 232it [02:27,  1.59it/s]Extractor Estimating: 233it [02:28,  1.60it/s]Extractor Estimating: 234it [02:29,  1.54it/s]Extractor Estimating: 235it [02:29,  1.52it/s]Extractor Estimating: 236it [02:30,  1.56it/s]Extractor Estimating: 237it [02:31,  1.52it/s]Extractor Estimating: 238it [02:31,  1.50it/s]Extractor Estimating: 239it [02:32,  1.51it/s]Extractor Estimating: 240it [02:33,  1.49it/s]Extractor Estimating: 241it [02:33,  1.51it/s]Extractor Estimating: 242it [02:34,  1.51it/s]Extractor Estimating: 243it [02:35,  1.51it/s]Extractor Estimating: 244it [02:35,  1.50it/s]Extractor Estimating: 245it [02:36,  1.47it/s]Extractor Estimating: 246it [02:37,  1.50it/s]Extractor Estimating: 247it [02:37,  1.51it/s]Extractor Estimating: 248it [02:38,  1.52it/s]Extractor Estimating: 249it [02:39,  1.55it/s]Extractor Estimating: 250it [02:39,  1.56it/s]Extractor Estimating: 251it [02:40,  1.67it/s]Extractor Estimating: 252it [02:40,  1.68it/s]Extractor Estimating: 253it [02:41,  1.67it/s]Extractor Estimating: 254it [02:42,  1.74it/s]Extractor Estimating: 255it [02:42,  1.74it/s]Extractor Estimating: 256it [02:43,  1.71it/s]Extractor Estimating: 257it [02:43,  1.75it/s]Extractor Estimating: 258it [02:44,  1.72it/s]Extractor Estimating: 259it [02:44,  1.77it/s]Extractor Estimating: 260it [02:45,  1.85it/s]Extractor Estimating: 261it [02:45,  1.89it/s]Extractor Estimating: 262it [02:46,  1.84it/s]Extractor Estimating: 263it [02:47,  1.84it/s]Extractor Estimating: 264it [02:47,  1.87it/s]Extractor Estimating: 265it [02:48,  1.91it/s]Extractor Estimating: 266it [02:48,  1.90it/s]Extractor Estimating: 267it [02:49,  1.92it/s]Extractor Estimating: 268it [02:49,  1.92it/s]Extractor Estimating: 269it [02:50,  1.94it/s]Extractor Estimating: 270it [02:50,  1.92it/s]Extractor Estimating: 271it [02:51,  1.90it/s]Extractor Estimating: 272it [02:51,  1.90it/s]Extractor Estimating: 273it [02:52,  1.91it/s]Extractor Estimating: 274it [02:52,  1.90it/s]Extractor Estimating: 275it [02:53,  1.88it/s]Extractor Estimating: 276it [02:53,  1.79it/s]Extractor Estimating: 277it [02:54,  1.76it/s]Extractor Estimating: 278it [02:55,  1.79it/s]Extractor Estimating: 279it [02:55,  1.81it/s]Extractor Estimating: 280it [02:56,  1.80it/s]Extractor Estimating: 281it [02:56,  1.77it/s]Extractor Estimating: 282it [02:57,  1.70it/s]Extractor Estimating: 283it [02:57,  1.71it/s]Extractor Estimating: 284it [02:58,  1.76it/s]Extractor Estimating: 285it [02:59,  1.64it/s]Extractor Estimating: 286it [02:59,  1.73it/s]Extractor Estimating: 287it [03:00,  1.80it/s]Extractor Estimating: 288it [03:00,  1.69it/s]Extractor Estimating: 289it [03:01,  1.76it/s]Extractor Estimating: 290it [03:01,  1.78it/s]Extractor Estimating: 291it [03:02,  1.75it/s]Extractor Estimating: 292it [03:03,  1.78it/s]Extractor Estimating: 293it [03:03,  1.78it/s]Extractor Estimating: 294it [03:04,  1.78it/s]Extractor Estimating: 295it [03:04,  1.77it/s]Extractor Estimating: 296it [03:05,  1.78it/s]Extractor Estimating: 297it [03:05,  1.80it/s]Extractor Estimating: 298it [03:06,  1.83it/s]Extractor Estimating: 299it [03:06,  1.82it/s]Extractor Estimating: 300it [03:07,  1.80it/s]Extractor Estimating: 301it [03:08,  1.76it/s]Extractor Estimating: 302it [03:08,  1.75it/s]Extractor Estimating: 303it [03:09,  1.70it/s]Extractor Estimating: 304it [03:09,  1.64it/s]Extractor Estimating: 305it [03:10,  1.61it/s]Extractor Estimating: 306it [03:11,  1.62it/s]Extractor Estimating: 307it [03:11,  1.60it/s]Extractor Estimating: 308it [03:12,  1.61it/s]Extractor Estimating: 309it [03:13,  1.62it/s]Extractor Estimating: 310it [03:13,  1.49it/s]Extractor Estimating: 311it [03:14,  1.53it/s]Extractor Estimating: 312it [03:15,  1.48it/s]Extractor Estimating: 313it [03:15,  1.52it/s]Extractor Estimating: 314it [03:16,  1.58it/s]Extractor Estimating: 315it [03:17,  1.55it/s]Extractor Estimating: 316it [03:17,  1.58it/s]Extractor Estimating: 317it [03:18,  1.56it/s]Extractor Estimating: 318it [03:18,  1.57it/s]Extractor Estimating: 319it [03:19,  1.53it/s]Extractor Estimating: 320it [03:20,  1.57it/s]Extractor Estimating: 321it [03:20,  1.58it/s]Extractor Estimating: 322it [03:21,  1.63it/s]Extractor Estimating: 323it [03:22,  1.60it/s]Extractor Estimating: 324it [03:22,  1.61it/s]Extractor Estimating: 325it [03:23,  1.59it/s]Extractor Estimating: 326it [03:23,  1.63it/s]Extractor Estimating: 327it [03:24,  1.66it/s]Extractor Estimating: 328it [03:25,  1.71it/s]Extractor Estimating: 329it [03:25,  1.69it/s]Extractor Estimating: 330it [03:26,  1.61it/s]Extractor Estimating: 331it [03:26,  1.71it/s]Extractor Estimating: 332it [03:27,  1.72it/s]Extractor Estimating: 333it [03:28,  1.72it/s]Extractor Estimating: 334it [03:28,  1.76it/s]Extractor Estimating: 335it [03:29,  1.77it/s]Extractor Estimating: 336it [03:29,  1.72it/s]Extractor Estimating: 337it [03:30,  1.71it/s]Extractor Estimating: 338it [03:30,  1.71it/s]Extractor Estimating: 339it [03:31,  1.73it/s]Extractor Estimating: 340it [03:32,  1.75it/s]Extractor Estimating: 341it [03:32,  1.66it/s]Extractor Estimating: 342it [03:33,  1.67it/s]Extractor Estimating: 343it [03:33,  1.68it/s]Extractor Estimating: 344it [03:34,  1.69it/s]Extractor Estimating: 345it [03:35,  1.71it/s]Extractor Estimating: 346it [03:35,  1.73it/s]Extractor Estimating: 347it [03:36,  1.73it/s]Extractor Estimating: 348it [03:36,  1.73it/s]Extractor Estimating: 349it [03:37,  1.75it/s]Extractor Estimating: 350it [03:37,  1.69it/s]Extractor Estimating: 351it [03:38,  1.60it/s]Extractor Estimating: 352it [03:39,  1.61it/s]Extractor Estimating: 353it [03:39,  1.54it/s]Extractor Estimating: 354it [03:40,  1.57it/s]Extractor Estimating: 355it [03:41,  1.59it/s]Extractor Estimating: 356it [03:41,  1.62it/s]Extractor Estimating: 357it [03:42,  1.57it/s]Extractor Estimating: 358it [03:43,  1.55it/s]Extractor Estimating: 359it [03:43,  1.44it/s]Extractor Estimating: 360it [03:44,  1.50it/s]Extractor Estimating: 361it [03:45,  1.48it/s]Extractor Estimating: 362it [03:45,  1.51it/s]Extractor Estimating: 363it [03:46,  1.52it/s]Extractor Estimating: 364it [03:47,  1.55it/s]Extractor Estimating: 365it [03:47,  1.57it/s]Extractor Estimating: 366it [03:48,  1.58it/s]Extractor Estimating: 367it [03:48,  1.63it/s]Extractor Estimating: 368it [03:49,  1.60it/s]Extractor Estimating: 369it [03:50,  1.57it/s]Extractor Estimating: 370it [03:50,  1.56it/s]Extractor Estimating: 371it [03:51,  1.59it/s]Extractor Estimating: 372it [03:52,  1.54it/s]Extractor Estimating: 373it [03:52,  1.56it/s]Extractor Estimating: 374it [03:53,  1.57it/s]Extractor Estimating: 375it [03:54,  1.57it/s]Extractor Estimating: 376it [03:54,  1.61it/s]Extractor Estimating: 377it [03:55,  1.59it/s]Extractor Estimating: 378it [03:55,  1.60it/s]Extractor Estimating: 379it [03:56,  1.62it/s]Extractor Estimating: 380it [03:57,  1.65it/s]Extractor Estimating: 381it [03:57,  1.65it/s]Extractor Estimating: 382it [03:58,  1.67it/s]Extractor Estimating: 383it [03:58,  1.65it/s]Extractor Estimating: 384it [03:59,  1.67it/s]Extractor Estimating: 385it [04:00,  1.63it/s]Extractor Estimating: 386it [04:00,  1.66it/s]Extractor Estimating: 387it [04:01,  1.64it/s]Extractor Estimating: 388it [04:02,  1.61it/s]Extractor Estimating: 389it [04:02,  1.63it/s]Extractor Estimating: 390it [04:03,  1.65it/s]Extractor Estimating: 391it [04:03,  1.61it/s]Extractor Estimating: 392it [04:04,  1.62it/s]Extractor Estimating: 393it [04:05,  1.61it/s]Extractor Estimating: 394it [04:05,  1.61it/s]Extractor Estimating: 395it [04:06,  1.63it/s]Extractor Estimating: 396it [04:06,  1.65it/s]Extractor Estimating: 397it [04:07,  1.58it/s]Extractor Estimating: 398it [04:08,  1.58it/s]Extractor Estimating: 399it [04:08,  1.62it/s]Extractor Estimating: 400it [04:09,  1.69it/s]Extractor Estimating: 401it [04:09,  1.66it/s]Extractor Estimating: 402it [04:10,  1.60it/s]Extractor Estimating: 403it [04:11,  1.57it/s]Extractor Estimating: 404it [04:11,  1.61it/s]Extractor Estimating: 405it [04:12,  1.62it/s]Extractor Estimating: 406it [04:13,  1.61it/s]Extractor Estimating: 407it [04:13,  1.62it/s]Extractor Estimating: 408it [04:14,  1.60it/s]Extractor Estimating: 409it [04:15,  1.57it/s]Extractor Estimating: 410it [04:15,  1.53it/s]Extractor Estimating: 411it [04:16,  1.55it/s]Extractor Estimating: 412it [04:17,  1.55it/s]Extractor Estimating: 413it [04:17,  1.56it/s]Extractor Estimating: 414it [04:18,  1.62it/s]Extractor Estimating: 415it [04:18,  1.63it/s]Extractor Estimating: 416it [04:19,  1.65it/s]Extractor Estimating: 417it [04:19,  1.68it/s]Extractor Estimating: 418it [04:20,  1.62it/s]Extractor Estimating: 419it [04:21,  1.61it/s]Extractor Estimating: 420it [04:21,  1.61it/s]Extractor Estimating: 421it [04:22,  1.57it/s]Extractor Estimating: 422it [04:23,  1.58it/s]Extractor Estimating: 423it [04:23,  1.59it/s]Extractor Estimating: 424it [04:24,  1.63it/s]Extractor Estimating: 425it [04:24,  1.66it/s]Extractor Estimating: 426it [04:25,  1.71it/s]Extractor Estimating: 427it [04:26,  1.68it/s]Extractor Estimating: 428it [04:26,  1.74it/s]Extractor Estimating: 429it [04:27,  1.74it/s]Extractor Estimating: 430it [04:27,  1.78it/s]Extractor Estimating: 431it [04:28,  1.80it/s]Extractor Estimating: 432it [04:29,  1.63it/s]Extractor Estimating: 433it [04:29,  1.67it/s]Extractor Estimating: 434it [04:30,  1.70it/s]Extractor Estimating: 435it [04:30,  1.71it/s]Extractor Estimating: 436it [04:31,  1.75it/s]Extractor Estimating: 437it [04:31,  1.77it/s]Extractor Estimating: 438it [04:32,  1.79it/s]Extractor Estimating: 439it [04:33,  1.74it/s]Extractor Estimating: 440it [04:33,  1.82it/s]Extractor Estimating: 441it [04:34,  1.82it/s]Extractor Estimating: 442it [04:34,  1.82it/s]Extractor Estimating: 443it [04:35,  1.83it/s]Extractor Estimating: 444it [04:35,  1.77it/s]Extractor Estimating: 445it [04:36,  1.79it/s]Extractor Estimating: 446it [04:36,  1.74it/s]Extractor Estimating: 447it [04:37,  1.83it/s]Extractor Estimating: 448it [04:37,  1.81it/s]Extractor Estimating: 449it [04:38,  1.80it/s]Extractor Estimating: 450it [04:39,  1.66it/s]Extractor Estimating: 451it [04:39,  1.67it/s]Extractor Estimating: 452it [04:40,  1.60it/s]Extractor Estimating: 453it [04:41,  1.63it/s]Extractor Estimating: 454it [04:41,  1.61it/s]Extractor Estimating: 455it [04:42,  1.68it/s]Extractor Estimating: 456it [04:42,  1.69it/s]Extractor Estimating: 457it [04:43,  1.69it/s]Extractor Estimating: 458it [04:44,  1.64it/s]Extractor Estimating: 459it [04:44,  1.65it/s]Extractor Estimating: 460it [04:45,  1.68it/s]Extractor Estimating: 461it [04:45,  1.70it/s]Extractor Estimating: 462it [04:46,  1.67it/s]Extractor Estimating: 463it [04:47,  1.65it/s]Extractor Estimating: 464it [04:47,  1.64it/s]Extractor Estimating: 465it [04:48,  1.67it/s]Extractor Estimating: 466it [04:48,  1.67it/s]Extractor Estimating: 467it [04:49,  1.67it/s]Extractor Estimating: 468it [04:50,  1.68it/s]Extractor Estimating: 469it [04:50,  1.68it/s]Extractor Estimating: 470it [04:51,  1.63it/s]Extractor Estimating: 471it [04:51,  1.58it/s]Extractor Estimating: 472it [04:52,  1.62it/s]Extractor Estimating: 473it [04:53,  1.65it/s]Extractor Estimating: 474it [04:53,  1.61it/s]Extractor Estimating: 475it [04:54,  1.65it/s]Extractor Estimating: 476it [04:54,  1.64it/s]Extractor Estimating: 477it [04:55,  1.66it/s]Extractor Estimating: 478it [04:56,  1.68it/s]Extractor Estimating: 479it [04:56,  1.62it/s]Extractor Estimating: 480it [04:57,  1.60it/s]Extractor Estimating: 481it [04:58,  1.60it/s]Extractor Estimating: 482it [04:58,  1.63it/s]Extractor Estimating: 483it [04:59,  1.63it/s]Extractor Estimating: 484it [04:59,  1.67it/s]Extractor Estimating: 485it [05:00,  1.67it/s]Extractor Estimating: 486it [05:01,  1.68it/s]Extractor Estimating: 487it [05:01,  1.62it/s]Extractor Estimating: 488it [05:02,  1.64it/s]Extractor Estimating: 489it [05:02,  1.68it/s]Extractor Estimating: 490it [05:03,  1.71it/s]Extractor Estimating: 491it [05:04,  1.70it/s]Extractor Estimating: 492it [05:04,  1.74it/s]Extractor Estimating: 493it [05:05,  1.73it/s]Extractor Estimating: 494it [05:05,  1.75it/s]Extractor Estimating: 495it [05:06,  1.74it/s]Extractor Estimating: 496it [05:06,  1.70it/s]Extractor Estimating: 497it [05:07,  1.67it/s]Extractor Estimating: 498it [05:08,  1.69it/s]Extractor Estimating: 499it [05:08,  1.64it/s]Extractor Estimating: 500it [05:09,  1.95it/s]Extractor Estimating: 500it [05:09,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:53,640 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:53,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:53,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:53,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:53,686 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 11:29:54,586 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 11:29:54,587 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:29:55,224 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 11:29:56,374 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:29:56,374 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:59,561 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:59,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:59,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:59,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 11:29:59,584 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 11:30:00,538 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 11:30:00,539 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 11:30:01,163 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 11:30:01,376 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 11:30:01,377 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 14:31:59,825 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 14:32:00,117 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 9936 mean pseudo reward: 0.9326312772392553
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl'}
train vocab size: 25402
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25502, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25502, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.037, loss:699.0868
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.041, loss:686.7073
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.038, loss:709.7240
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.034, loss:672.6726
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 86, avg_time 1.052, loss:681.3585
>> valid entity prec:0.5763, rec:0.5041, f1:0.5378
>> valid relation prec:0.1200, rec:0.0144, f1:0.0258
>> valid relation with NER prec:0.1200, rec:0.0144, f1:0.0258
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 186, avg_time 2.484, loss:704.1093
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 286, avg_time 1.041, loss:661.4620
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 386, avg_time 1.039, loss:704.5307
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 72, avg_time 1.036, loss:678.6499
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 172, avg_time 1.038, loss:690.2107
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5624, rec:0.5589, f1:0.5607
>> valid relation prec:0.1994, rec:0.0161, f1:0.0298
>> valid relation with NER prec:0.1994, rec:0.0161, f1:0.0298
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 272, avg_time 2.491, loss:717.4494
g_step 1200, step 372, avg_time 1.030, loss:690.1968
g_step 1300, step 58, avg_time 1.047, loss:657.5694
g_step 1400, step 158, avg_time 1.037, loss:637.0186
g_step 1500, step 258, avg_time 1.041, loss:671.7145
>> valid entity prec:0.5779, rec:0.4492, f1:0.5055
>> valid relation prec:0.1588, rec:0.0089, f1:0.0168
>> valid relation with NER prec:0.1588, rec:0.0089, f1:0.0168
g_step 1600, step 358, avg_time 2.474, loss:693.5156
g_step 1700, step 44, avg_time 1.041, loss:660.5727
g_step 1800, step 144, avg_time 1.041, loss:631.1595
g_step 1900, step 244, avg_time 1.039, loss:640.1582
g_step 2000, step 344, avg_time 1.035, loss:639.8773
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5375, rec:0.5584, f1:0.5478
>> valid relation prec:0.1415, rec:0.0284, f1:0.0473
>> valid relation with NER prec:0.1415, rec:0.0284, f1:0.0473
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 30, avg_time 2.498, loss:650.3470
g_step 2200, step 130, avg_time 1.050, loss:592.1501
g_step 2300, step 230, avg_time 1.039, loss:609.1136
g_step 2400, step 330, avg_time 1.031, loss:623.1380
g_step 2500, step 16, avg_time 1.038, loss:617.9066
>> valid entity prec:0.6021, rec:0.4497, f1:0.5149
>> valid relation prec:0.1640, rec:0.0149, f1:0.0273
>> valid relation with NER prec:0.1640, rec:0.0149, f1:0.0273
g_step 2600, step 116, avg_time 2.469, loss:561.9073
g_step 2700, step 216, avg_time 1.046, loss:613.9835
g_step 2800, step 316, avg_time 1.042, loss:593.2822
g_step 2900, step 2, avg_time 1.034, loss:613.1064
g_step 3000, step 102, avg_time 1.041, loss:555.4381
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5573, rec:0.4886, f1:0.5207
>> valid relation prec:0.1489, rec:0.0212, f1:0.0371
>> valid relation with NER prec:0.1489, rec:0.0212, f1:0.0371
g_step 3100, step 202, avg_time 2.474, loss:564.7066
g_step 3200, step 302, avg_time 1.038, loss:596.6853
g_step 3300, step 402, avg_time 1.040, loss:581.8226
g_step 3400, step 88, avg_time 1.031, loss:530.9001
g_step 3500, step 188, avg_time 1.046, loss:540.7646
>> valid entity prec:0.5798, rec:0.4800, f1:0.5252
>> valid relation prec:0.1156, rec:0.0204, f1:0.0347
>> valid relation with NER prec:0.1156, rec:0.0204, f1:0.0347
g_step 3600, step 288, avg_time 2.471, loss:562.9510
g_step 3700, step 388, avg_time 1.040, loss:571.8129
g_step 3800, step 74, avg_time 1.052, loss:551.1155
g_step 3900, step 174, avg_time 1.035, loss:530.3727
g_step 4000, step 274, avg_time 1.046, loss:545.9305
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5894, rec:0.4024, f1:0.4783
>> valid relation prec:0.1359, rec:0.0168, f1:0.0300
>> valid relation with NER prec:0.1359, rec:0.0168, f1:0.0300
g_step 4100, step 374, avg_time 2.453, loss:541.7557
g_step 4200, step 60, avg_time 1.054, loss:510.4836
g_step 4300, step 160, avg_time 1.044, loss:498.0542
g_step 4400, step 260, avg_time 1.035, loss:521.0397
g_step 4500, step 360, avg_time 1.043, loss:540.0396
>> valid entity prec:0.5633, rec:0.4852, f1:0.5213
>> valid relation prec:0.1284, rec:0.0207, f1:0.0356
>> valid relation with NER prec:0.1284, rec:0.0207, f1:0.0356
g_step 4600, step 46, avg_time 2.455, loss:517.2329
g_step 4700, step 146, avg_time 1.054, loss:477.5563
g_step 4800, step 246, avg_time 1.040, loss:499.3062
g_step 4900, step 346, avg_time 1.044, loss:514.4763
g_step 5000, step 32, avg_time 1.029, loss:498.2434
learning rate was adjusted to 0.0008
>> valid entity prec:0.5664, rec:0.4390, f1:0.4946
>> valid relation prec:0.1256, rec:0.0180, f1:0.0315
>> valid relation with NER prec:0.1256, rec:0.0180, f1:0.0315
g_step 5100, step 132, avg_time 2.468, loss:483.1755
g_step 5200, step 232, avg_time 1.035, loss:467.8243
g_step 5300, step 332, avg_time 1.042, loss:496.8949
g_step 5400, step 18, avg_time 1.050, loss:497.9357
g_step 5500, step 118, avg_time 1.042, loss:445.2833
>> valid entity prec:0.5469, rec:0.4643, f1:0.5022
>> valid relation prec:0.1479, rec:0.0260, f1:0.0442
>> valid relation with NER prec:0.1479, rec:0.0260, f1:0.0442
g_step 5600, step 218, avg_time 2.481, loss:474.4705
g_step 5700, step 318, avg_time 1.039, loss:493.6996
g_step 5800, step 4, avg_time 1.030, loss:461.6221
g_step 5900, step 104, avg_time 1.056, loss:433.8656
g_step 6000, step 204, avg_time 1.039, loss:445.3073
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5948, rec:0.3581, f1:0.4471
>> valid relation prec:0.1670, rec:0.0219, f1:0.0387
>> valid relation with NER prec:0.1670, rec:0.0219, f1:0.0387
g_step 6100, step 304, avg_time 2.466, loss:464.1263
g_step 6200, step 404, avg_time 1.028, loss:463.0323
g_step 6300, step 90, avg_time 1.039, loss:424.9681
g_step 6400, step 190, avg_time 1.040, loss:437.7976
g_step 6500, step 290, avg_time 1.029, loss:427.9018
>> valid entity prec:0.5698, rec:0.4520, f1:0.5041
>> valid relation prec:0.1350, rec:0.0277, f1:0.0459
>> valid relation with NER prec:0.1350, rec:0.0277, f1:0.0459
g_step 6600, step 390, avg_time 2.471, loss:444.0760
g_step 6700, step 76, avg_time 1.052, loss:411.7870
g_step 6800, step 176, avg_time 1.050, loss:408.5731
g_step 6900, step 276, avg_time 1.027, loss:420.6899
g_step 7000, step 376, avg_time 1.041, loss:435.5659
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5481, rec:0.4706, f1:0.5064
>> valid relation prec:0.1354, rec:0.0281, f1:0.0466
>> valid relation with NER prec:0.1354, rec:0.0281, f1:0.0466
g_step 7100, step 62, avg_time 2.485, loss:412.3198
g_step 7200, step 162, avg_time 1.041, loss:394.4184
g_step 7300, step 262, avg_time 1.037, loss:401.0631
g_step 7400, step 362, avg_time 1.042, loss:412.8019
g_step 7500, step 48, avg_time 1.041, loss:393.2161
>> valid entity prec:0.5560, rec:0.4802, f1:0.5154
>> valid relation prec:0.1120, rec:0.0228, f1:0.0379
>> valid relation with NER prec:0.1120, rec:0.0228, f1:0.0379
g_step 7600, step 148, avg_time 2.461, loss:370.3289
g_step 7700, step 248, avg_time 1.052, loss:397.7742
g_step 7800, step 348, avg_time 1.038, loss:395.5152
g_step 7900, step 34, avg_time 1.037, loss:404.7989
g_step 8000, step 134, avg_time 1.040, loss:367.3837
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6080, rec:0.3480, f1:0.4426
>> valid relation prec:0.1373, rec:0.0188, f1:0.0330
>> valid relation with NER prec:0.1373, rec:0.0188, f1:0.0330
g_step 8100, step 234, avg_time 2.492, loss:382.4733
g_step 8200, step 334, avg_time 1.029, loss:367.0209
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 14:32:00 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 14:32:00 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_14-31-59_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 14:32:01 - WARNING - datasets.builder -   Using custom data configuration default-9eaf8cb9ac224b5d
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-9eaf8cb9ac224b5d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 14:32:03,814 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 14:32:03,851 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 14:32:03,851 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 14:32:03,852 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 14:32:04,023 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:32:04,085 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:32:04,085 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:32:04,085 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:32:04,085 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:32:04,085 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:32:04,085 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 14:32:04,623 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 14:32:22,270 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 14:32:22,299 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-9eaf8cb9ac224b5d/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.11ba/s] 20%|██        | 2/10 [00:00<00:02,  3.95ba/s] 30%|███       | 3/10 [00:00<00:01,  4.32ba/s] 40%|████      | 4/10 [00:00<00:01,  4.50ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.59ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.65ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.69ba/s] 80%|████████  | 8/10 [00:01<00:00,  4.74ba/s] 90%|█████████ | 9/10 [00:01<00:00,  4.72ba/s]100%|██████████| 10/10 [00:02<00:00,  4.74ba/s]100%|██████████| 10/10 [00:02<00:00,  4.55ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.61ba/s] 40%|████      | 2/5 [00:00<00:00,  3.19ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.71ba/s] 80%|████████  | 4/5 [00:01<00:00,  4.01ba/s]100%|██████████| 5/5 [00:01<00:00,  4.44ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  4.95ba/s] 30%|███       | 3/10 [00:00<00:00,  8.21ba/s] 40%|████      | 4/10 [00:00<00:00,  6.22ba/s] 60%|██████    | 6/10 [00:00<00:00,  7.90ba/s] 80%|████████  | 8/10 [00:00<00:00,  8.91ba/s]100%|██████████| 10/10 [00:01<00:00,  9.52ba/s]100%|██████████| 10/10 [00:01<00:00,  8.49ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.54ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.06ba/s]100%|██████████| 5/5 [00:00<00:00, 11.19ba/s]100%|██████████| 5/5 [00:00<00:00,  8.92ba/s]
[INFO|trainer.py:414] 2023-08-29 14:32:29,020 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 14:32:29,092 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 14:32:29,093 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 14:32:29,093 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 14:32:29,093 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 14:32:29,093 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 14:32:29,093 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 14:32:29,093 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:55,  3.30it/s]  0%|          | 2/780 [00:00<03:48,  3.41it/s]  0%|          | 3/780 [00:00<03:45,  3.45it/s]  1%|          | 4/780 [00:01<03:43,  3.48it/s]  1%|          | 5/780 [00:01<03:42,  3.49it/s]  1%|          | 6/780 [00:01<03:41,  3.49it/s]  1%|          | 7/780 [00:02<03:40,  3.50it/s]  1%|          | 8/780 [00:02<03:40,  3.50it/s]  1%|          | 9/780 [00:02<03:53,  3.31it/s]  1%|▏         | 10/780 [00:02<03:48,  3.36it/s]  1%|▏         | 11/780 [00:03<03:45,  3.41it/s]  2%|▏         | 12/780 [00:03<03:43,  3.44it/s]  2%|▏         | 13/780 [00:03<03:41,  3.46it/s]  2%|▏         | 14/780 [00:04<03:40,  3.47it/s]  2%|▏         | 15/780 [00:04<03:39,  3.48it/s]  2%|▏         | 16/780 [00:04<03:38,  3.49it/s]  2%|▏         | 17/780 [00:04<03:38,  3.50it/s]  2%|▏         | 18/780 [00:05<03:37,  3.50it/s]  2%|▏         | 19/780 [00:05<03:37,  3.50it/s]  3%|▎         | 20/780 [00:05<03:36,  3.51it/s]  3%|▎         | 21/780 [00:06<03:48,  3.33it/s]  3%|▎         | 22/780 [00:06<03:44,  3.38it/s]  3%|▎         | 23/780 [00:06<03:41,  3.41it/s]  3%|▎         | 24/780 [00:06<03:39,  3.44it/s]  3%|▎         | 25/780 [00:07<03:38,  3.46it/s]  3%|▎         | 26/780 [00:07<03:37,  3.47it/s]  3%|▎         | 27/780 [00:07<03:36,  3.48it/s]  4%|▎         | 28/780 [00:08<03:35,  3.49it/s]  4%|▎         | 29/780 [00:08<03:34,  3.49it/s]  4%|▍         | 30/780 [00:08<03:34,  3.50it/s]  4%|▍         | 31/780 [00:08<03:33,  3.50it/s]  4%|▍         | 32/780 [00:09<03:50,  3.24it/s]  4%|▍         | 33/780 [00:09<03:45,  3.31it/s]  4%|▍         | 34/780 [00:09<03:41,  3.37it/s]  4%|▍         | 35/780 [00:10<03:38,  3.41it/s]  5%|▍         | 36/780 [00:10<03:36,  3.43it/s]  5%|▍         | 37/780 [00:10<03:35,  3.45it/s]  5%|▍         | 38/780 [00:11<03:34,  3.47it/s]  5%|▌         | 39/780 [00:11<03:33,  3.48it/s]  5%|▌         | 40/780 [00:11<03:32,  3.48it/s]  5%|▌         | 41/780 [00:11<03:31,  3.49it/s]  5%|▌         | 42/780 [00:12<03:31,  3.49it/s]  6%|▌         | 43/780 [00:12<03:49,  3.21it/s]  6%|▌         | 44/780 [00:12<03:43,  3.29it/s]  6%|▌         | 45/780 [00:13<03:39,  3.35it/s]  6%|▌         | 46/780 [00:13<03:36,  3.40it/s]  6%|▌         | 47/780 [00:13<03:34,  3.42it/s]  6%|▌         | 48/780 [00:13<03:32,  3.45it/s]  6%|▋         | 49/780 [00:14<03:31,  3.46it/s]  6%|▋         | 50/780 [00:14<03:30,  3.47it/s]  7%|▋         | 51/780 [00:14<03:29,  3.48it/s]  7%|▋         | 52/780 [00:15<03:28,  3.48it/s]  7%|▋         | 53/780 [00:15<03:28,  3.49it/s]  7%|▋         | 54/780 [00:15<03:42,  3.26it/s]  7%|▋         | 55/780 [00:16<03:37,  3.33it/s]  7%|▋         | 56/780 [00:16<03:34,  3.37it/s]  7%|▋         | 57/780 [00:16<03:31,  3.41it/s]  7%|▋         | 58/780 [00:16<03:30,  3.44it/s]  8%|▊         | 59/780 [00:17<03:28,  3.45it/s]  8%|▊         | 60/780 [00:17<03:27,  3.47it/s]  8%|▊         | 61/780 [00:17<03:27,  3.47it/s]  8%|▊         | 62/780 [00:18<03:26,  3.48it/s]  8%|▊         | 63/780 [00:18<03:26,  3.48it/s]  8%|▊         | 64/780 [00:18<03:25,  3.48it/s]  8%|▊         | 65/780 [00:18<03:40,  3.25it/s]  8%|▊         | 66/780 [00:19<03:35,  3.32it/s]  9%|▊         | 67/780 [00:19<03:31,  3.37it/s]  9%|▊         | 68/780 [00:19<03:29,  3.40it/s]  9%|▉         | 69/780 [00:20<03:27,  3.43it/s]  9%|▉         | 70/780 [00:20<03:26,  3.44it/s]  9%|▉         | 71/780 [00:20<03:25,  3.46it/s]  9%|▉         | 72/780 [00:20<03:24,  3.47it/s]  9%|▉         | 73/780 [00:21<03:23,  3.47it/s]  9%|▉         | 74/780 [00:21<03:22,  3.48it/s] 10%|▉         | 75/780 [00:21<03:22,  3.48it/s] 10%|▉         | 76/780 [00:22<03:26,  3.41it/s] 10%|▉         | 77/780 [00:22<03:24,  3.43it/s] 10%|█         | 78/780 [00:22<03:23,  3.45it/s] 10%|█         | 79/780 [00:23<03:22,  3.46it/s] 10%|█         | 80/780 [00:23<03:21,  3.47it/s] 10%|█         | 81/780 [00:23<03:21,  3.48it/s] 11%|█         | 82/780 [00:23<03:20,  3.48it/s] 11%|█         | 83/780 [00:24<03:20,  3.48it/s] 11%|█         | 84/780 [00:24<03:19,  3.48it/s] 11%|█         | 85/780 [00:24<03:19,  3.48it/s] 11%|█         | 86/780 [00:25<03:19,  3.49it/s] 11%|█         | 87/780 [00:25<03:25,  3.36it/s] 11%|█▏        | 88/780 [00:25<03:23,  3.40it/s] 11%|█▏        | 89/780 [00:25<03:21,  3.42it/s] 12%|█▏        | 90/780 [00:26<03:20,  3.44it/s] 12%|█▏        | 91/780 [00:26<03:19,  3.45it/s] 12%|█▏        | 92/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 93/780 [00:27<03:17,  3.47it/s] 12%|█▏        | 94/780 [00:27<03:17,  3.47it/s] 12%|█▏        | 95/780 [00:27<03:16,  3.48it/s] 12%|█▏        | 96/780 [00:27<03:16,  3.48it/s] 12%|█▏        | 97/780 [00:28<03:16,  3.48it/s] 13%|█▎        | 98/780 [00:28<03:26,  3.30it/s] 13%|█▎        | 99/780 [00:28<03:22,  3.35it/s] 13%|█▎        | 100/780 [00:29<03:20,  3.39it/s] 13%|█▎        | 101/780 [00:29<03:18,  3.42it/s] 13%|█▎        | 102/780 [00:29<03:17,  3.44it/s] 13%|█▎        | 103/780 [00:29<03:15,  3.45it/s] 13%|█▎        | 104/780 [00:30<03:15,  3.46it/s] 13%|█▎        | 105/780 [00:30<03:14,  3.47it/s] 14%|█▎        | 106/780 [00:31<04:22,  2.57it/s] 14%|█▎        | 107/780 [00:31<04:01,  2.79it/s] 14%|█▍        | 108/780 [00:31<03:55,  2.86it/s] 14%|█▍        | 109/780 [00:32<03:42,  3.02it/s] 14%|█▍        | 110/780 [00:32<03:32,  3.15it/s] 14%|█▍        | 111/780 [00:32<03:26,  3.24it/s] 14%|█▍        | 112/780 [00:32<03:21,  3.31it/s] 14%|█▍        | 113/780 [00:33<03:18,  3.36it/s] 15%|█▍        | 114/780 [00:33<03:16,  3.40it/s] 15%|█▍        | 115/780 [00:33<03:14,  3.42it/s] 15%|█▍        | 116/780 [00:34<03:12,  3.44it/s] 15%|█▌        | 117/780 [00:34<03:11,  3.46it/s] 15%|█▌        | 118/780 [00:34<03:11,  3.46it/s] 15%|█▌        | 119/780 [00:34<03:10,  3.47it/s] 15%|█▌        | 120/780 [00:35<03:09,  3.48it/s] 16%|█▌        | 121/780 [00:35<03:09,  3.48it/s] 16%|█▌        | 122/780 [00:35<03:08,  3.48it/s] 16%|█▌        | 123/780 [00:36<03:08,  3.49it/s] 16%|█▌        | 124/780 [00:36<03:08,  3.48it/s] 16%|█▌        | 125/780 [00:36<03:16,  3.34it/s] 16%|█▌        | 126/780 [00:37<03:13,  3.38it/s] 16%|█▋        | 127/780 [00:37<03:11,  3.41it/s] 16%|█▋        | 128/780 [00:37<03:10,  3.43it/s] 17%|█▋        | 129/780 [00:37<03:08,  3.45it/s] 17%|█▋        | 130/780 [00:38<03:07,  3.46it/s] 17%|█▋        | 131/780 [00:38<03:07,  3.47it/s] 17%|█▋        | 132/780 [00:38<03:06,  3.47it/s] 17%|█▋        | 133/780 [00:39<03:06,  3.48it/s] 17%|█▋        | 134/780 [00:39<03:05,  3.48it/s] 17%|█▋        | 135/780 [00:39<03:05,  3.48it/s] 17%|█▋        | 136/780 [00:39<03:09,  3.40it/s] 18%|█▊        | 137/780 [00:40<03:07,  3.43it/s] 18%|█▊        | 138/780 [00:40<03:06,  3.44it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.45it/s] 18%|█▊        | 140/780 [00:41<03:04,  3.46it/s] 18%|█▊        | 141/780 [00:41<03:04,  3.47it/s] 18%|█▊        | 142/780 [00:41<03:03,  3.47it/s] 18%|█▊        | 143/780 [00:41<03:03,  3.48it/s] 18%|█▊        | 144/780 [00:42<03:02,  3.48it/s] 19%|█▊        | 145/780 [00:42<03:02,  3.48it/s] 19%|█▊        | 146/780 [00:42<03:02,  3.48it/s] 19%|█▉        | 147/780 [00:43<03:05,  3.41it/s] 19%|█▉        | 148/780 [00:43<03:04,  3.43it/s] 19%|█▉        | 149/780 [00:43<03:03,  3.44it/s] 19%|█▉        | 150/780 [00:43<03:02,  3.46it/s] 19%|█▉        | 151/780 [00:44<03:01,  3.46it/s] 19%|█▉        | 152/780 [00:44<03:01,  3.47it/s] 20%|█▉        | 153/780 [00:44<03:00,  3.47it/s] 20%|█▉        | 154/780 [00:45<03:00,  3.47it/s] 20%|█▉        | 155/780 [00:45<02:59,  3.48it/s] 20%|██        | 156/780 [00:45<02:59,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 14:33:14,810 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:33:14,810 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 14:33:14,810 >>   Batch size = 8

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 58.06it/s][A
  2%|▏         | 12/521 [00:00<00:10, 47.05it/s][A
  3%|▎         | 17/521 [00:00<00:10, 46.96it/s][A
  4%|▍         | 22/521 [00:00<00:10, 47.17it/s][A
  5%|▌         | 27/521 [00:00<00:10, 47.05it/s][A
  6%|▌         | 32/521 [00:00<00:10, 47.15it/s][A
  7%|▋         | 37/521 [00:00<00:10, 47.20it/s][A
  8%|▊         | 42/521 [00:00<00:10, 47.13it/s][A
  9%|▉         | 47/521 [00:00<00:10, 47.04it/s][A
 10%|▉         | 52/521 [00:01<00:09, 47.11it/s][A
 11%|█         | 57/521 [00:01<00:09, 47.11it/s][A
 12%|█▏        | 62/521 [00:01<00:09, 47.13it/s][A
 13%|█▎        | 67/521 [00:01<00:09, 47.12it/s][A
 14%|█▍        | 72/521 [00:01<00:09, 47.11it/s][A
 15%|█▍        | 77/521 [00:01<00:09, 47.17it/s][A
 16%|█▌        | 82/521 [00:01<00:09, 47.14it/s][A
 17%|█▋        | 87/521 [00:01<00:09, 47.14it/s][A
 18%|█▊        | 92/521 [00:01<00:09, 47.04it/s][A
 19%|█▊        | 97/521 [00:02<00:09, 47.07it/s][A
 20%|█▉        | 102/521 [00:02<00:08, 47.11it/s][A
 21%|██        | 107/521 [00:02<00:08, 47.03it/s][A
 21%|██▏       | 112/521 [00:02<00:08, 47.09it/s][A
 22%|██▏       | 117/521 [00:02<00:08, 47.10it/s][A
 23%|██▎       | 122/521 [00:02<00:08, 47.10it/s][A
 24%|██▍       | 127/521 [00:02<00:08, 47.15it/s][A
 25%|██▌       | 132/521 [00:02<00:08, 47.16it/s][A
 26%|██▋       | 137/521 [00:02<00:08, 47.20it/s][A
 27%|██▋       | 142/521 [00:03<00:08, 47.09it/s][A
 28%|██▊       | 147/521 [00:03<00:07, 47.02it/s][A
 29%|██▉       | 152/521 [00:03<00:08, 45.24it/s][A
 30%|███       | 157/521 [00:03<00:07, 45.88it/s][A
 31%|███       | 162/521 [00:03<00:07, 46.24it/s][A
 32%|███▏      | 167/521 [00:03<00:07, 46.42it/s][A
 33%|███▎      | 172/521 [00:03<00:07, 46.69it/s][A
 34%|███▍      | 177/521 [00:03<00:07, 46.80it/s][A
 35%|███▍      | 182/521 [00:03<00:07, 46.89it/s][A
 36%|███▌      | 187/521 [00:03<00:07, 47.02it/s][A
 37%|███▋      | 192/521 [00:04<00:07, 46.92it/s][A
 38%|███▊      | 197/521 [00:04<00:06, 46.95it/s][A
 39%|███▉      | 202/521 [00:04<00:06, 46.94it/s][A
 40%|███▉      | 207/521 [00:04<00:06, 47.04it/s][A
 41%|████      | 212/521 [00:04<00:06, 46.91it/s][A
 42%|████▏     | 217/521 [00:04<00:06, 46.91it/s][A
 43%|████▎     | 222/521 [00:04<00:06, 47.06it/s][A
 44%|████▎     | 227/521 [00:04<00:06, 47.08it/s][A
 45%|████▍     | 232/521 [00:04<00:06, 47.10it/s][A
 45%|████▌     | 237/521 [00:05<00:06, 47.11it/s][A
 46%|████▋     | 242/521 [00:05<00:05, 46.95it/s][A
 47%|████▋     | 247/521 [00:05<00:05, 47.01it/s][A
 48%|████▊     | 252/521 [00:05<00:05, 47.11it/s][A
 49%|████▉     | 257/521 [00:05<00:05, 47.10it/s][A
 50%|█████     | 262/521 [00:05<00:05, 47.11it/s][A
 51%|█████     | 267/521 [00:05<00:05, 47.06it/s][A
 52%|█████▏    | 272/521 [00:05<00:05, 47.02it/s][A
 53%|█████▎    | 277/521 [00:05<00:05, 46.94it/s][A
 54%|█████▍    | 282/521 [00:05<00:05, 46.98it/s][A
 55%|█████▌    | 287/521 [00:06<00:04, 47.04it/s][A
 56%|█████▌    | 292/521 [00:06<00:04, 47.00it/s][A
 57%|█████▋    | 297/521 [00:06<00:04, 45.75it/s][A
 58%|█████▊    | 302/521 [00:06<00:04, 45.97it/s][A
 59%|█████▉    | 307/521 [00:06<00:04, 46.45it/s][A
 60%|█████▉    | 312/521 [00:06<00:04, 46.58it/s][A
 61%|██████    | 317/521 [00:06<00:04, 46.76it/s][A
 62%|██████▏   | 322/521 [00:06<00:04, 46.93it/s][A
 63%|██████▎   | 327/521 [00:06<00:04, 47.01it/s][A
 64%|██████▎   | 332/521 [00:07<00:04, 46.90it/s][A
 65%|██████▍   | 337/521 [00:07<00:03, 46.98it/s][A
 66%|██████▌   | 342/521 [00:07<00:03, 46.97it/s][A
 67%|██████▋   | 347/521 [00:07<00:03, 46.95it/s][A
 68%|██████▊   | 352/521 [00:07<00:03, 47.05it/s][A
 69%|██████▊   | 357/521 [00:07<00:03, 46.97it/s][A
 69%|██████▉   | 362/521 [00:07<00:03, 47.05it/s][A
 70%|███████   | 367/521 [00:07<00:03, 47.01it/s][A
 71%|███████▏  | 372/521 [00:07<00:03, 47.09it/s][A
 72%|███████▏  | 377/521 [00:08<00:03, 47.04it/s][A
 73%|███████▎  | 382/521 [00:08<00:02, 47.06it/s][A
 74%|███████▍  | 387/521 [00:08<00:02, 47.01it/s][A
 75%|███████▌  | 392/521 [00:08<00:02, 46.91it/s][A
 76%|███████▌  | 397/521 [00:08<00:02, 46.86it/s][A
 77%|███████▋  | 402/521 [00:08<00:02, 47.04it/s][A
 78%|███████▊  | 407/521 [00:08<00:02, 47.04it/s][A
 79%|███████▉  | 412/521 [00:08<00:02, 46.75it/s][A
 80%|████████  | 417/521 [00:08<00:02, 46.89it/s][A
 81%|████████  | 422/521 [00:08<00:02, 46.97it/s][A
 82%|████████▏ | 427/521 [00:09<00:02, 46.84it/s][A
 83%|████████▎ | 432/521 [00:09<00:01, 47.00it/s][A
 84%|████████▍ | 437/521 [00:09<00:01, 46.97it/s][A
 85%|████████▍ | 442/521 [00:09<00:01, 42.41it/s][A
 86%|████████▌ | 447/521 [00:09<00:01, 43.81it/s][A
 87%|████████▋ | 452/521 [00:09<00:01, 44.79it/s][A
 88%|████████▊ | 457/521 [00:09<00:01, 45.31it/s][A
 89%|████████▊ | 462/521 [00:09<00:01, 45.90it/s][A
 90%|████████▉ | 467/521 [00:09<00:01, 46.29it/s][A
 91%|█████████ | 472/521 [00:10<00:01, 46.54it/s][A
 92%|█████████▏| 477/521 [00:10<00:00, 46.72it/s][A
 93%|█████████▎| 482/521 [00:10<00:00, 46.68it/s][A
 93%|█████████▎| 487/521 [00:10<00:00, 46.65it/s][A
 94%|█████████▍| 492/521 [00:10<00:00, 46.79it/s][A
 95%|█████████▌| 497/521 [00:10<00:00, 46.90it/s][A
 96%|█████████▋| 502/521 [00:10<00:00, 46.90it/s][A
 97%|█████████▋| 507/521 [00:10<00:00, 46.91it/s][A
 98%|█████████▊| 512/521 [00:10<00:00, 47.04it/s][A
 99%|█████████▉| 517/521 [00:11<00:00, 47.08it/s][A                                                 
                                                 [A 20%|██        | 156/780 [00:56<02:59,  3.48it/s]
100%|██████████| 521/521 [00:11<00:00, 47.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:33:26,198 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 14:33:26,408 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:33:30,195 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:33:30,446 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:33:30,517 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:09<1:16:09,  7.34s/it] 20%|██        | 158/780 [01:09<54:14,  5.23s/it]   20%|██        | 159/780 [01:10<38:48,  3.75s/it] 21%|██        | 160/780 [01:10<28:00,  2.71s/it] 21%|██        | 161/780 [01:10<20:27,  1.98s/it] 21%|██        | 162/780 [01:10<15:10,  1.47s/it] 21%|██        | 163/780 [01:11<11:29,  1.12s/it] 21%|██        | 164/780 [01:11<08:54,  1.15it/s] 21%|██        | 165/780 [01:11<07:06,  1.44it/s] 21%|██▏       | 166/780 [01:12<05:50,  1.75it/s] 21%|██▏       | 167/780 [01:12<04:58,  2.06it/s] 22%|██▏       | 168/780 [01:12<04:20,  2.35it/s] 22%|██▏       | 169/780 [01:13<04:09,  2.45it/s] 22%|██▏       | 170/780 [01:13<03:46,  2.69it/s] 22%|██▏       | 171/780 [01:13<03:30,  2.89it/s] 22%|██▏       | 172/780 [01:13<03:19,  3.05it/s] 22%|██▏       | 173/780 [01:14<03:11,  3.17it/s] 22%|██▏       | 174/780 [01:14<03:05,  3.26it/s] 22%|██▏       | 175/780 [01:14<03:02,  3.32it/s] 23%|██▎       | 176/780 [01:15<02:59,  3.37it/s] 23%|██▎       | 177/780 [01:15<02:57,  3.40it/s] 23%|██▎       | 178/780 [01:15<02:55,  3.43it/s] 23%|██▎       | 179/780 [01:15<02:54,  3.45it/s] 23%|██▎       | 180/780 [01:16<03:02,  3.28it/s] 23%|██▎       | 181/780 [01:16<02:59,  3.34it/s] 23%|██▎       | 182/780 [01:16<02:56,  3.38it/s] 23%|██▎       | 183/780 [01:17<02:54,  3.41it/s] 24%|██▎       | 184/780 [01:17<02:53,  3.44it/s] 24%|██▎       | 185/780 [01:17<02:52,  3.45it/s] 24%|██▍       | 186/780 [01:17<02:51,  3.46it/s] 24%|██▍       | 187/780 [01:18<02:50,  3.47it/s] 24%|██▍       | 188/780 [01:18<02:50,  3.48it/s] 24%|██▍       | 189/780 [01:18<02:49,  3.48it/s] 24%|██▍       | 190/780 [01:19<02:49,  3.48it/s] 24%|██▍       | 191/780 [01:19<02:54,  3.37it/s] 25%|██▍       | 192/780 [01:19<02:52,  3.41it/s] 25%|██▍       | 193/780 [01:19<02:50,  3.43it/s] 25%|██▍       | 194/780 [01:20<02:50,  3.45it/s] 25%|██▌       | 195/780 [01:20<02:49,  3.46it/s] 25%|██▌       | 196/780 [01:20<02:48,  3.47it/s] 25%|██▌       | 197/780 [01:21<02:47,  3.48it/s] 25%|██▌       | 198/780 [01:21<02:47,  3.48it/s] 26%|██▌       | 199/780 [01:21<02:46,  3.48it/s] 26%|██▌       | 200/780 [01:21<02:46,  3.49it/s] 26%|██▌       | 201/780 [01:22<02:46,  3.48it/s] 26%|██▌       | 202/780 [01:22<02:54,  3.32it/s] 26%|██▌       | 203/780 [01:22<02:51,  3.37it/s] 26%|██▌       | 204/780 [01:23<02:49,  3.40it/s] 26%|██▋       | 205/780 [01:23<02:47,  3.43it/s] 26%|██▋       | 206/780 [01:23<02:46,  3.44it/s] 27%|██▋       | 207/780 [01:24<02:45,  3.46it/s] 27%|██▋       | 208/780 [01:24<02:44,  3.47it/s] 27%|██▋       | 209/780 [01:24<02:44,  3.47it/s] 27%|██▋       | 210/780 [01:24<02:43,  3.48it/s] 27%|██▋       | 211/780 [01:25<02:43,  3.48it/s] 27%|██▋       | 212/780 [01:25<02:43,  3.48it/s] 27%|██▋       | 213/780 [01:25<02:50,  3.32it/s] 27%|██▋       | 214/780 [01:26<02:47,  3.37it/s] 28%|██▊       | 215/780 [01:26<02:45,  3.41it/s] 28%|██▊       | 216/780 [01:26<02:44,  3.43it/s] 28%|██▊       | 217/780 [01:26<02:43,  3.45it/s] 28%|██▊       | 218/780 [01:27<02:42,  3.46it/s] 28%|██▊       | 219/780 [01:27<02:41,  3.47it/s] 28%|██▊       | 220/780 [01:27<02:41,  3.48it/s] 28%|██▊       | 221/780 [01:28<02:40,  3.48it/s] 28%|██▊       | 222/780 [01:28<02:40,  3.48it/s] 29%|██▊       | 223/780 [01:28<02:39,  3.49it/s] 29%|██▊       | 224/780 [01:29<02:53,  3.20it/s] 29%|██▉       | 225/780 [01:29<02:48,  3.28it/s] 29%|██▉       | 226/780 [01:29<02:45,  3.34it/s] 29%|██▉       | 227/780 [01:29<02:43,  3.39it/s] 29%|██▉       | 228/780 [01:30<02:41,  3.42it/s] 29%|██▉       | 229/780 [01:30<02:40,  3.44it/s] 29%|██▉       | 230/780 [01:30<02:39,  3.46it/s] 30%|██▉       | 231/780 [01:31<03:22,  2.71it/s] 30%|██▉       | 232/780 [01:31<03:09,  2.90it/s] 30%|██▉       | 233/780 [01:31<02:59,  3.05it/s] 30%|███       | 234/780 [01:32<02:56,  3.10it/s] 30%|███       | 235/780 [01:32<02:50,  3.21it/s] 30%|███       | 236/780 [01:32<02:45,  3.29it/s] 30%|███       | 237/780 [01:33<02:42,  3.35it/s] 31%|███       | 238/780 [01:33<02:39,  3.39it/s] 31%|███       | 239/780 [01:33<02:38,  3.42it/s] 31%|███       | 240/780 [01:33<02:37,  3.44it/s] 31%|███       | 241/780 [01:34<02:36,  3.45it/s] 31%|███       | 242/780 [01:34<02:35,  3.46it/s] 31%|███       | 243/780 [01:34<02:34,  3.47it/s] 31%|███▏      | 244/780 [01:35<02:42,  3.30it/s] 31%|███▏      | 245/780 [01:35<02:39,  3.35it/s] 32%|███▏      | 246/780 [01:35<02:37,  3.39it/s] 32%|███▏      | 247/780 [01:35<02:35,  3.42it/s] 32%|███▏      | 248/780 [01:36<02:34,  3.44it/s] 32%|███▏      | 249/780 [01:36<02:33,  3.45it/s] 32%|███▏      | 250/780 [01:36<02:33,  3.46it/s] 32%|███▏      | 251/780 [01:37<02:32,  3.46it/s] 32%|███▏      | 252/780 [01:37<02:32,  3.47it/s] 32%|███▏      | 253/780 [01:37<02:31,  3.48it/s] 33%|███▎      | 254/780 [01:37<02:31,  3.48it/s] 33%|███▎      | 255/780 [01:38<02:39,  3.30it/s] 33%|███▎      | 256/780 [01:38<02:36,  3.36it/s] 33%|███▎      | 257/780 [01:38<02:34,  3.39it/s] 33%|███▎      | 258/780 [01:39<02:32,  3.42it/s] 33%|███▎      | 259/780 [01:39<02:31,  3.44it/s] 33%|███▎      | 260/780 [01:39<02:30,  3.45it/s] 33%|███▎      | 261/780 [01:40<02:29,  3.46it/s] 34%|███▎      | 262/780 [01:40<02:29,  3.47it/s] 34%|███▎      | 263/780 [01:40<02:28,  3.48it/s] 34%|███▍      | 264/780 [01:40<02:28,  3.48it/s] 34%|███▍      | 265/780 [01:41<02:27,  3.49it/s] 34%|███▍      | 266/780 [01:41<02:35,  3.30it/s] 34%|███▍      | 267/780 [01:41<02:33,  3.35it/s] 34%|███▍      | 268/780 [01:42<02:31,  3.39it/s] 34%|███▍      | 269/780 [01:42<02:29,  3.42it/s] 35%|███▍      | 270/780 [01:42<02:28,  3.43it/s] 35%|███▍      | 271/780 [01:42<02:27,  3.45it/s] 35%|███▍      | 272/780 [01:43<02:26,  3.46it/s] 35%|███▌      | 273/780 [01:43<02:26,  3.47it/s] 35%|███▌      | 274/780 [01:43<02:25,  3.47it/s] 35%|███▌      | 275/780 [01:44<02:25,  3.48it/s] 35%|███▌      | 276/780 [01:44<02:24,  3.48it/s] 36%|███▌      | 277/780 [01:44<02:30,  3.35it/s] 36%|███▌      | 278/780 [01:45<02:28,  3.39it/s] 36%|███▌      | 279/780 [01:45<02:33,  3.27it/s] 36%|███▌      | 280/780 [01:45<02:29,  3.34it/s] 36%|███▌      | 281/780 [01:45<02:27,  3.38it/s] 36%|███▌      | 282/780 [01:46<02:26,  3.41it/s] 36%|███▋      | 283/780 [01:46<02:24,  3.43it/s] 36%|███▋      | 284/780 [01:46<02:24,  3.44it/s] 37%|███▋      | 285/780 [01:47<02:23,  3.45it/s] 37%|███▋      | 286/780 [01:47<02:22,  3.46it/s] 37%|███▋      | 287/780 [01:47<02:22,  3.47it/s] 37%|███▋      | 288/780 [01:47<02:21,  3.47it/s] 37%|███▋      | 289/780 [01:48<02:21,  3.47it/s] 37%|███▋      | 290/780 [01:48<02:25,  3.36it/s] 37%|███▋      | 291/780 [01:48<02:23,  3.40it/s] 37%|███▋      | 292/780 [01:49<02:22,  3.42it/s] 38%|███▊      | 293/780 [01:49<02:21,  3.44it/s] 38%|███▊      | 294/780 [01:49<02:20,  3.45it/s] 38%|███▊      | 295/780 [01:49<02:20,  3.46it/s] 38%|███▊      | 296/780 [01:50<02:19,  3.47it/s] 38%|███▊      | 297/780 [01:50<02:19,  3.47it/s] 38%|███▊      | 298/780 [01:50<02:18,  3.48it/s] 38%|███▊      | 299/780 [01:51<02:18,  3.47it/s] 38%|███▊      | 300/780 [01:51<02:18,  3.48it/s] 39%|███▊      | 301/780 [01:51<02:17,  3.48it/s] 39%|███▊      | 302/780 [01:51<02:17,  3.48it/s] 39%|███▉      | 303/780 [01:52<02:17,  3.48it/s] 39%|███▉      | 304/780 [01:52<02:16,  3.48it/s] 39%|███▉      | 305/780 [01:52<02:16,  3.48it/s] 39%|███▉      | 306/780 [01:53<02:16,  3.48it/s] 39%|███▉      | 307/780 [01:53<02:21,  3.34it/s] 39%|███▉      | 308/780 [01:53<02:19,  3.38it/s] 40%|███▉      | 309/780 [01:54<02:18,  3.41it/s] 40%|███▉      | 310/780 [01:54<02:17,  3.43it/s] 40%|███▉      | 311/780 [01:54<02:16,  3.44it/s] 40%|████      | 312/780 [01:54<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 14:34:24,033 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:34:24,033 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 14:34:24,033 >>   Batch size = 8
{'eval_loss': 0.9583470225334167, 'eval_runtime': 11.1738, 'eval_samples_per_second': 372.746, 'eval_steps_per_second': 46.627, 'epoch': 1.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.28it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.78it/s][A
  3%|▎         | 18/521 [00:00<00:12, 39.04it/s][A
  4%|▍         | 23/521 [00:00<00:11, 41.55it/s][A
  5%|▌         | 28/521 [00:00<00:11, 43.29it/s][A
  6%|▋         | 33/521 [00:00<00:10, 44.39it/s][A
  7%|▋         | 38/521 [00:00<00:10, 45.26it/s][A
  8%|▊         | 43/521 [00:00<00:10, 45.86it/s][A
  9%|▉         | 48/521 [00:01<00:10, 46.27it/s][A
 10%|█         | 53/521 [00:01<00:10, 46.52it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.51it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.59it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.66it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.78it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.92it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.93it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 47.03it/s][A
 18%|█▊        | 93/521 [00:02<00:09, 47.07it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.99it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.97it/s][A
 21%|██        | 108/521 [00:02<00:08, 47.04it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 47.03it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 47.02it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 47.03it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.94it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 47.04it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 47.09it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 47.08it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 47.08it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 47.11it/s][A
 30%|███       | 158/521 [00:03<00:08, 44.16it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 45.06it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 45.69it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.05it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.28it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.53it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 46.63it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.84it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.83it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.83it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.94it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.99it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 47.04it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.92it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 47.09it/s][A
 45%|████▍     | 233/521 [00:05<00:06, 47.02it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.95it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 47.04it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.97it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.96it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 47.07it/s][A
 50%|█████     | 263/521 [00:05<00:05, 47.03it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 47.08it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 47.07it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 47.12it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.97it/s][A
 55%|█████▌    | 288/521 [00:06<00:05, 42.12it/s][A
 56%|█████▌    | 293/521 [00:06<00:05, 43.66it/s][A
 57%|█████▋    | 298/521 [00:06<00:05, 44.53it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 45.31it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 45.83it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.08it/s][A
 61%|██████    | 318/521 [00:06<00:04, 44.84it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 45.52it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 45.99it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.19it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.49it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.73it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.79it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.87it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.98it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.78it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.90it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 47.02it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.88it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.93it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.99it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 43.00it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 44.16it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 44.97it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 45.55it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.07it/s][A
 80%|████████  | 418/521 [00:09<00:02, 46.36it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.56it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.79it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.67it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.64it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 46.80it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 46.88it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 46.93it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 47.03it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 47.06it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 47.00it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.95it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.91it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.83it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.84it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.97it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.84it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 47.02it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.99it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 47.00it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 47.08it/s][A                                                 
                                                 [A 40%|████      | 312/780 [02:06<02:15,  3.46it/s]
100%|██████████| 521/521 [00:11<00:00, 47.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:34:35,600 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 14:34:35,823 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:34:39,838 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:34:39,991 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:34:40,105 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:22<1:06:15,  8.51s/it] 40%|████      | 314/780 [02:22<47:06,  6.07s/it]   40%|████      | 315/780 [02:23<33:34,  4.33s/it] 41%|████      | 316/780 [02:23<24:06,  3.12s/it] 41%|████      | 317/780 [02:23<17:30,  2.27s/it] 41%|████      | 318/780 [02:24<12:53,  1.67s/it] 41%|████      | 319/780 [02:24<09:39,  1.26s/it] 41%|████      | 320/780 [02:24<07:24,  1.04it/s] 41%|████      | 321/780 [02:24<05:49,  1.31it/s] 41%|████▏     | 322/780 [02:25<04:43,  1.61it/s] 41%|████▏     | 323/780 [02:25<03:57,  1.92it/s] 42%|████▏     | 324/780 [02:25<03:24,  2.22it/s] 42%|████▏     | 325/780 [02:26<03:10,  2.39it/s] 42%|████▏     | 326/780 [02:26<02:52,  2.64it/s] 42%|████▏     | 327/780 [02:26<02:39,  2.85it/s] 42%|████▏     | 328/780 [02:27<02:29,  3.01it/s] 42%|████▏     | 329/780 [02:27<02:23,  3.14it/s] 42%|████▏     | 330/780 [02:27<02:18,  3.24it/s] 42%|████▏     | 331/780 [02:27<02:15,  3.31it/s] 43%|████▎     | 332/780 [02:28<02:13,  3.36it/s] 43%|████▎     | 333/780 [02:28<02:11,  3.40it/s] 43%|████▎     | 334/780 [02:28<02:10,  3.43it/s] 43%|████▎     | 335/780 [02:29<02:09,  3.45it/s] 43%|████▎     | 336/780 [02:29<02:15,  3.27it/s] 43%|████▎     | 337/780 [02:29<02:12,  3.33it/s] 43%|████▎     | 338/780 [02:29<02:10,  3.38it/s] 43%|████▎     | 339/780 [02:30<02:09,  3.41it/s] 44%|████▎     | 340/780 [02:30<02:08,  3.44it/s] 44%|████▎     | 341/780 [02:30<02:07,  3.45it/s] 44%|████▍     | 342/780 [02:31<02:58,  2.45it/s] 44%|████▍     | 343/780 [02:31<02:42,  2.69it/s] 44%|████▍     | 344/780 [02:32<02:31,  2.89it/s] 44%|████▍     | 345/780 [02:32<02:29,  2.91it/s] 44%|████▍     | 346/780 [02:32<02:21,  3.07it/s] 44%|████▍     | 347/780 [02:32<02:16,  3.18it/s] 45%|████▍     | 348/780 [02:33<02:12,  3.27it/s] 45%|████▍     | 349/780 [02:33<02:09,  3.33it/s] 45%|████▍     | 350/780 [02:33<02:07,  3.38it/s] 45%|████▌     | 351/780 [02:34<02:05,  3.41it/s] 45%|████▌     | 352/780 [02:34<02:04,  3.43it/s] 45%|████▌     | 353/780 [02:34<02:03,  3.45it/s] 45%|████▌     | 354/780 [02:34<02:03,  3.46it/s] 46%|████▌     | 355/780 [02:35<02:02,  3.47it/s] 46%|████▌     | 356/780 [02:35<02:09,  3.27it/s] 46%|████▌     | 357/780 [02:35<02:06,  3.33it/s] 46%|████▌     | 358/780 [02:36<02:04,  3.38it/s] 46%|████▌     | 359/780 [02:36<02:11,  3.19it/s] 46%|████▌     | 360/780 [02:36<02:08,  3.27it/s] 46%|████▋     | 361/780 [02:37<02:05,  3.34it/s] 46%|████▋     | 362/780 [02:37<02:03,  3.38it/s] 47%|████▋     | 363/780 [02:37<02:02,  3.41it/s] 47%|████▋     | 364/780 [02:37<02:01,  3.43it/s] 47%|████▋     | 365/780 [02:38<02:00,  3.45it/s] 47%|████▋     | 366/780 [02:38<01:59,  3.46it/s] 47%|████▋     | 367/780 [02:38<01:59,  3.47it/s] 47%|████▋     | 368/780 [02:39<01:58,  3.47it/s] 47%|████▋     | 369/780 [02:39<01:58,  3.48it/s] 47%|████▋     | 370/780 [02:39<02:03,  3.32it/s] 48%|████▊     | 371/780 [02:40<02:01,  3.37it/s] 48%|████▊     | 372/780 [02:40<01:59,  3.40it/s] 48%|████▊     | 373/780 [02:40<01:58,  3.43it/s] 48%|████▊     | 374/780 [02:40<01:57,  3.45it/s] 48%|████▊     | 375/780 [02:41<01:57,  3.46it/s] 48%|████▊     | 376/780 [02:41<01:56,  3.46it/s] 48%|████▊     | 377/780 [02:41<01:56,  3.47it/s] 48%|████▊     | 378/780 [02:42<01:55,  3.48it/s] 49%|████▊     | 379/780 [02:42<01:55,  3.48it/s] 49%|████▊     | 380/780 [02:42<01:54,  3.48it/s] 49%|████▉     | 381/780 [02:42<02:00,  3.30it/s] 49%|████▉     | 382/780 [02:43<01:58,  3.36it/s] 49%|████▉     | 383/780 [02:43<01:56,  3.40it/s] 49%|████▉     | 384/780 [02:43<01:55,  3.42it/s] 49%|████▉     | 385/780 [02:44<01:54,  3.44it/s] 49%|████▉     | 386/780 [02:44<01:54,  3.45it/s] 50%|████▉     | 387/780 [02:44<01:53,  3.47it/s] 50%|████▉     | 388/780 [02:44<01:52,  3.47it/s] 50%|████▉     | 389/780 [02:45<01:52,  3.48it/s] 50%|█████     | 390/780 [02:45<01:52,  3.48it/s] 50%|█████     | 391/780 [02:45<01:51,  3.48it/s] 50%|█████     | 392/780 [02:46<01:57,  3.31it/s] 50%|█████     | 393/780 [02:46<01:55,  3.36it/s] 51%|█████     | 394/780 [02:46<01:53,  3.40it/s] 51%|█████     | 395/780 [02:47<01:52,  3.42it/s] 51%|█████     | 396/780 [02:47<01:51,  3.44it/s] 51%|█████     | 397/780 [02:47<01:50,  3.46it/s] 51%|█████     | 398/780 [02:47<01:50,  3.47it/s] 51%|█████     | 399/780 [02:48<01:49,  3.47it/s] 51%|█████▏    | 400/780 [02:48<01:49,  3.48it/s] 51%|█████▏    | 401/780 [02:48<01:48,  3.48it/s] 52%|█████▏    | 402/780 [02:49<01:48,  3.49it/s] 52%|█████▏    | 403/780 [02:49<01:53,  3.32it/s] 52%|█████▏    | 404/780 [02:49<01:51,  3.37it/s] 52%|█████▏    | 405/780 [02:49<01:50,  3.40it/s] 52%|█████▏    | 406/780 [02:50<01:49,  3.43it/s] 52%|█████▏    | 407/780 [02:50<01:48,  3.45it/s] 52%|█████▏    | 408/780 [02:50<01:47,  3.46it/s] 52%|█████▏    | 409/780 [02:51<01:47,  3.47it/s] 53%|█████▎    | 410/780 [02:51<01:46,  3.47it/s] 53%|█████▎    | 411/780 [02:51<01:46,  3.47it/s] 53%|█████▎    | 412/780 [02:51<01:45,  3.48it/s] 53%|█████▎    | 413/780 [02:52<01:45,  3.48it/s] 53%|█████▎    | 414/780 [02:52<01:45,  3.48it/s] 53%|█████▎    | 415/780 [02:52<01:44,  3.48it/s] 53%|█████▎    | 416/780 [02:53<01:44,  3.48it/s] 53%|█████▎    | 417/780 [02:53<01:44,  3.48it/s] 54%|█████▎    | 418/780 [02:53<01:43,  3.48it/s] 54%|█████▎    | 419/780 [02:53<01:43,  3.48it/s] 54%|█████▍    | 420/780 [02:54<01:43,  3.48it/s] 54%|█████▍    | 421/780 [02:54<01:42,  3.49it/s] 54%|█████▍    | 422/780 [02:54<01:42,  3.48it/s] 54%|█████▍    | 423/780 [02:55<01:46,  3.36it/s] 54%|█████▍    | 424/780 [02:55<01:44,  3.40it/s] 54%|█████▍    | 425/780 [02:55<01:43,  3.42it/s] 55%|█████▍    | 426/780 [02:55<01:42,  3.44it/s] 55%|█████▍    | 427/780 [02:56<01:42,  3.45it/s] 55%|█████▍    | 428/780 [02:56<01:41,  3.46it/s] 55%|█████▌    | 429/780 [02:56<01:41,  3.47it/s] 55%|█████▌    | 430/780 [02:57<01:40,  3.47it/s] 55%|█████▌    | 431/780 [02:57<01:40,  3.48it/s] 55%|█████▌    | 432/780 [02:57<01:39,  3.48it/s] 56%|█████▌    | 433/780 [02:57<01:39,  3.48it/s] 56%|█████▌    | 434/780 [02:58<01:39,  3.48it/s] 56%|█████▌    | 435/780 [02:58<01:39,  3.48it/s] 56%|█████▌    | 436/780 [02:58<01:38,  3.48it/s] 56%|█████▌    | 437/780 [02:59<01:38,  3.48it/s] 56%|█████▌    | 438/780 [02:59<01:38,  3.48it/s] 56%|█████▋    | 439/780 [02:59<01:37,  3.48it/s] 56%|█████▋    | 440/780 [03:00<01:43,  3.27it/s] 57%|█████▋    | 441/780 [03:00<01:41,  3.34it/s] 57%|█████▋    | 442/780 [03:00<01:40,  3.38it/s] 57%|█████▋    | 443/780 [03:00<01:38,  3.41it/s] 57%|█████▋    | 444/780 [03:01<01:43,  3.26it/s] 57%|█████▋    | 445/780 [03:01<01:41,  3.31it/s] 57%|█████▋    | 446/780 [03:01<01:44,  3.18it/s] 57%|█████▋    | 447/780 [03:02<01:42,  3.26it/s] 57%|█████▋    | 448/780 [03:02<01:39,  3.32it/s] 58%|█████▊    | 449/780 [03:02<01:38,  3.37it/s] 58%|█████▊    | 450/780 [03:03<01:36,  3.40it/s] 58%|█████▊    | 451/780 [03:03<01:36,  3.43it/s] 58%|█████▊    | 452/780 [03:03<01:35,  3.44it/s] 58%|█████▊    | 453/780 [03:03<01:34,  3.45it/s] 58%|█████▊    | 454/780 [03:04<01:34,  3.46it/s] 58%|█████▊    | 455/780 [03:04<01:33,  3.47it/s] 58%|█████▊    | 456/780 [03:04<01:33,  3.47it/s] 59%|█████▊    | 457/780 [03:05<01:32,  3.47it/s] 59%|█████▊    | 458/780 [03:05<01:36,  3.35it/s] 59%|█████▉    | 459/780 [03:05<01:34,  3.39it/s] 59%|█████▉    | 460/780 [03:05<01:33,  3.41it/s] 59%|█████▉    | 461/780 [03:06<01:32,  3.43it/s] 59%|█████▉    | 462/780 [03:06<01:32,  3.45it/s] 59%|█████▉    | 463/780 [03:06<01:31,  3.46it/s] 59%|█████▉    | 464/780 [03:07<01:31,  3.47it/s] 60%|█████▉    | 465/780 [03:07<01:30,  3.47it/s] 60%|█████▉    | 466/780 [03:07<01:30,  3.48it/s] 60%|█████▉    | 467/780 [03:07<01:30,  3.48it/s] 60%|██████    | 468/780 [03:08<01:29,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 14:35:37,388 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:35:37,388 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 14:35:37,388 >>   Batch size = 8
{'eval_loss': 0.9714916348457336, 'eval_runtime': 11.2796, 'eval_samples_per_second': 369.251, 'eval_steps_per_second': 46.19, 'epoch': 2.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.79it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.89it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.12it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.44it/s][A
  5%|▌         | 28/521 [00:00<00:10, 48.03it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.72it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.29it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.00it/s][A
  9%|▉         | 48/521 [00:00<00:10, 47.08it/s][A
 10%|█         | 53/521 [00:01<00:09, 47.12it/s][A
 11%|█         | 58/521 [00:01<00:09, 47.04it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 47.12it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 47.05it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 47.11it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 47.14it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 43.90it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 44.89it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 45.56it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 45.91it/s][A
 20%|█▉        | 103/521 [00:02<00:09, 46.32it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.53it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.73it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.86it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.83it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.84it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.77it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.89it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.95it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 47.03it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 47.10it/s][A
 30%|███       | 158/521 [00:03<00:07, 47.13it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.97it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.96it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.94it/s][A
 34%|███▍      | 178/521 [00:03<00:08, 42.48it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 43.77it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 44.64it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 45.39it/s][A
 38%|███▊      | 198/521 [00:04<00:07, 45.86it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 46.31it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.50it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.66it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.57it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.70it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.75it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 46.88it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 47.01it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 47.00it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 47.03it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 47.12it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 47.00it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.94it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.94it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.95it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.87it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.99it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 47.05it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 47.04it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 47.10it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 47.11it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 46.95it/s][A
 60%|██████    | 313/521 [00:06<00:04, 46.96it/s][A
 61%|██████    | 318/521 [00:06<00:05, 37.66it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 40.14it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 41.95it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 43.41it/s][A
 65%|██████▍   | 338/521 [00:07<00:04, 44.45it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 45.24it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 45.78it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.19it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.21it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.31it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.55it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 46.71it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.82it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.91it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 47.01it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.87it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.96it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.89it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.79it/s][A
 79%|███████▉  | 413/521 [00:09<00:02, 37.04it/s][A
 80%|████████  | 418/521 [00:09<00:02, 39.55it/s][A
 81%|████████  | 423/521 [00:09<00:02, 41.51it/s][A
 82%|████████▏ | 428/521 [00:09<00:02, 43.09it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 44.20it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 45.05it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 45.68it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 45.97it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 46.14it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.28it/s][A
 89%|████████▉ | 463/521 [00:10<00:01, 46.58it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 46.76it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.72it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.87it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.91it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 47.01it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.95it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.96it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.84it/s][A
 98%|█████████▊| 508/521 [00:11<00:00, 46.89it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 46.98it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 47.03it/s][A                                                 
                                                 [A 60%|██████    | 468/780 [03:19<01:29,  3.48it/s]
100%|██████████| 521/521 [00:11<00:00, 47.03it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:35:49,408 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 14:35:49,941 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:35:55,110 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:35:55,430 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:35:55,586 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:35<43:25,  8.38s/it] 60%|██████    | 470/780 [03:35<30:49,  5.97s/it] 60%|██████    | 471/780 [03:36<21:57,  4.26s/it] 61%|██████    | 472/780 [03:36<15:45,  3.07s/it] 61%|██████    | 473/780 [03:36<11:26,  2.23s/it] 61%|██████    | 474/780 [03:36<08:24,  1.65s/it] 61%|██████    | 475/780 [03:37<06:18,  1.24s/it] 61%|██████    | 476/780 [03:37<04:50,  1.05it/s] 61%|██████    | 477/780 [03:37<03:48,  1.33it/s] 61%|██████▏   | 478/780 [03:38<03:05,  1.63it/s] 61%|██████▏   | 479/780 [03:38<02:35,  1.94it/s] 62%|██████▏   | 480/780 [03:38<02:14,  2.24it/s] 62%|██████▏   | 481/780 [03:39<02:03,  2.43it/s] 62%|██████▏   | 482/780 [03:39<01:51,  2.68it/s] 62%|██████▏   | 483/780 [03:39<01:43,  2.88it/s] 62%|██████▏   | 484/780 [03:39<01:37,  3.04it/s] 62%|██████▏   | 485/780 [03:40<01:33,  3.16it/s] 62%|██████▏   | 486/780 [03:40<01:30,  3.25it/s] 62%|██████▏   | 487/780 [03:40<01:28,  3.32it/s] 63%|██████▎   | 488/780 [03:41<01:26,  3.37it/s] 63%|██████▎   | 489/780 [03:41<01:25,  3.41it/s] 63%|██████▎   | 490/780 [03:41<01:24,  3.43it/s] 63%|██████▎   | 491/780 [03:41<01:23,  3.45it/s] 63%|██████▎   | 492/780 [03:42<01:29,  3.21it/s] 63%|██████▎   | 493/780 [03:42<01:27,  3.29it/s] 63%|██████▎   | 494/780 [03:42<01:25,  3.35it/s] 63%|██████▎   | 495/780 [03:43<01:24,  3.39it/s] 64%|██████▎   | 496/780 [03:43<01:23,  3.42it/s] 64%|██████▎   | 497/780 [03:43<01:22,  3.44it/s] 64%|██████▍   | 498/780 [03:43<01:21,  3.45it/s] 64%|██████▍   | 499/780 [03:44<01:21,  3.46it/s] 64%|██████▍   | 500/780 [03:44<01:20,  3.47it/s]                                                  64%|██████▍   | 500/780 [03:44<01:20,  3.47it/s] 64%|██████▍   | 501/780 [03:44<01:20,  3.47it/s] 64%|██████▍   | 502/780 [03:45<01:19,  3.48it/s] 64%|██████▍   | 503/780 [03:45<01:23,  3.31it/s] 65%|██████▍   | 504/780 [03:45<01:22,  3.36it/s] 65%|██████▍   | 505/780 [03:46<01:20,  3.40it/s] 65%|██████▍   | 506/780 [03:46<01:20,  3.42it/s] 65%|██████▌   | 507/780 [03:46<01:19,  3.44it/s] 65%|██████▌   | 508/780 [03:46<01:18,  3.45it/s] 65%|██████▌   | 509/780 [03:47<01:18,  3.46it/s] 65%|██████▌   | 510/780 [03:47<01:17,  3.47it/s] 66%|██████▌   | 511/780 [03:47<01:17,  3.48it/s] 66%|██████▌   | 512/780 [03:48<01:17,  3.48it/s] 66%|██████▌   | 513/780 [03:48<01:16,  3.48it/s] 66%|██████▌   | 514/780 [03:48<01:16,  3.48it/s] 66%|██████▌   | 515/780 [03:48<01:16,  3.48it/s] 66%|██████▌   | 516/780 [03:49<01:15,  3.48it/s] 66%|██████▋   | 517/780 [03:49<01:15,  3.48it/s] 66%|██████▋   | 518/780 [03:49<01:15,  3.48it/s] 67%|██████▋   | 519/780 [03:50<01:14,  3.48it/s] 67%|██████▋   | 520/780 [03:50<01:14,  3.48it/s] 67%|██████▋   | 521/780 [03:50<01:14,  3.48it/s] 67%|██████▋   | 522/780 [03:50<01:14,  3.48it/s] 67%|██████▋   | 523/780 [03:51<01:16,  3.35it/s] 67%|██████▋   | 524/780 [03:51<01:15,  3.39it/s] 67%|██████▋   | 525/780 [03:51<01:14,  3.42it/s] 67%|██████▋   | 526/780 [03:52<01:13,  3.44it/s] 68%|██████▊   | 527/780 [03:52<01:13,  3.45it/s] 68%|██████▊   | 528/780 [03:52<01:12,  3.46it/s] 68%|██████▊   | 529/780 [03:52<01:12,  3.47it/s] 68%|██████▊   | 530/780 [03:53<01:12,  3.47it/s] 68%|██████▊   | 531/780 [03:53<01:11,  3.48it/s] 68%|██████▊   | 532/780 [03:53<01:11,  3.48it/s] 68%|██████▊   | 533/780 [03:54<01:10,  3.48it/s] 68%|██████▊   | 534/780 [03:54<01:14,  3.28it/s] 69%|██████▊   | 535/780 [03:54<01:13,  3.34it/s] 69%|██████▊   | 536/780 [03:55<01:12,  3.38it/s] 69%|██████▉   | 537/780 [03:55<01:11,  3.42it/s] 69%|██████▉   | 538/780 [03:55<01:10,  3.44it/s] 69%|██████▉   | 539/780 [03:55<01:09,  3.45it/s] 69%|██████▉   | 540/780 [03:56<01:09,  3.46it/s] 69%|██████▉   | 541/780 [03:56<01:08,  3.47it/s] 69%|██████▉   | 542/780 [03:56<01:08,  3.47it/s] 70%|██████▉   | 543/780 [03:57<01:08,  3.48it/s] 70%|██████▉   | 544/780 [03:57<01:07,  3.48it/s] 70%|██████▉   | 545/780 [03:57<01:10,  3.33it/s] 70%|███████   | 546/780 [03:57<01:09,  3.37it/s] 70%|███████   | 547/780 [03:58<01:08,  3.40it/s] 70%|███████   | 548/780 [03:58<01:07,  3.43it/s] 70%|███████   | 549/780 [03:58<01:07,  3.44it/s] 71%|███████   | 550/780 [03:59<01:06,  3.46it/s] 71%|███████   | 551/780 [03:59<01:06,  3.47it/s] 71%|███████   | 552/780 [03:59<01:05,  3.47it/s] 71%|███████   | 553/780 [03:59<01:05,  3.48it/s] 71%|███████   | 554/780 [04:00<01:04,  3.48it/s] 71%|███████   | 555/780 [04:00<01:04,  3.48it/s] 71%|███████▏  | 556/780 [04:00<01:07,  3.30it/s] 71%|███████▏  | 557/780 [04:01<01:06,  3.35it/s] 72%|███████▏  | 558/780 [04:01<01:08,  3.23it/s] 72%|███████▏  | 559/780 [04:01<01:07,  3.29it/s] 72%|███████▏  | 560/780 [04:02<01:08,  3.22it/s] 72%|███████▏  | 561/780 [04:02<01:06,  3.29it/s] 72%|███████▏  | 562/780 [04:02<01:05,  3.34it/s] 72%|███████▏  | 563/780 [04:02<01:04,  3.39it/s] 72%|███████▏  | 564/780 [04:03<01:03,  3.42it/s] 72%|███████▏  | 565/780 [04:03<01:02,  3.44it/s] 73%|███████▎  | 566/780 [04:03<01:06,  3.24it/s] 73%|███████▎  | 567/780 [04:04<01:04,  3.31it/s] 73%|███████▎  | 568/780 [04:04<01:03,  3.36it/s] 73%|███████▎  | 569/780 [04:04<01:02,  3.40it/s] 73%|███████▎  | 570/780 [04:05<01:01,  3.42it/s] 73%|███████▎  | 571/780 [04:05<01:00,  3.44it/s] 73%|███████▎  | 572/780 [04:05<01:00,  3.45it/s] 73%|███████▎  | 573/780 [04:05<00:59,  3.46it/s] 74%|███████▎  | 574/780 [04:06<00:59,  3.47it/s] 74%|███████▎  | 575/780 [04:06<00:58,  3.47it/s] 74%|███████▍  | 576/780 [04:06<00:58,  3.48it/s] 74%|███████▍  | 577/780 [04:07<01:00,  3.33it/s] 74%|███████▍  | 578/780 [04:07<00:59,  3.38it/s] 74%|███████▍  | 579/780 [04:07<00:58,  3.41it/s] 74%|███████▍  | 580/780 [04:07<00:58,  3.43it/s] 74%|███████▍  | 581/780 [04:08<00:57,  3.45it/s] 75%|███████▍  | 582/780 [04:08<00:57,  3.45it/s] 75%|███████▍  | 583/780 [04:08<00:56,  3.46it/s] 75%|███████▍  | 584/780 [04:09<00:56,  3.47it/s] 75%|███████▌  | 585/780 [04:09<00:56,  3.47it/s] 75%|███████▌  | 586/780 [04:09<00:55,  3.48it/s] 75%|███████▌  | 587/780 [04:09<00:55,  3.48it/s] 75%|███████▌  | 588/780 [04:10<00:58,  3.30it/s] 76%|███████▌  | 589/780 [04:10<00:57,  3.35it/s] 76%|███████▌  | 590/780 [04:10<00:56,  3.38it/s] 76%|███████▌  | 591/780 [04:11<00:55,  3.41it/s] 76%|███████▌  | 592/780 [04:11<00:54,  3.43it/s] 76%|███████▌  | 593/780 [04:11<00:54,  3.45it/s] 76%|███████▌  | 594/780 [04:12<00:53,  3.46it/s] 76%|███████▋  | 595/780 [04:12<00:53,  3.46it/s] 76%|███████▋  | 596/780 [04:12<00:53,  3.47it/s] 77%|███████▋  | 597/780 [04:12<00:52,  3.47it/s] 77%|███████▋  | 598/780 [04:13<00:52,  3.47it/s] 77%|███████▋  | 599/780 [04:13<00:54,  3.30it/s] 77%|███████▋  | 600/780 [04:13<00:53,  3.35it/s] 77%|███████▋  | 601/780 [04:14<00:52,  3.39it/s] 77%|███████▋  | 602/780 [04:14<00:52,  3.42it/s] 77%|███████▋  | 603/780 [04:14<00:51,  3.44it/s] 77%|███████▋  | 604/780 [04:14<00:50,  3.45it/s] 78%|███████▊  | 605/780 [04:15<00:50,  3.46it/s] 78%|███████▊  | 606/780 [04:15<00:50,  3.46it/s] 78%|███████▊  | 607/780 [04:15<00:49,  3.47it/s] 78%|███████▊  | 608/780 [04:16<00:49,  3.47it/s] 78%|███████▊  | 609/780 [04:16<00:49,  3.47it/s] 78%|███████▊  | 610/780 [04:16<00:52,  3.24it/s] 78%|███████▊  | 611/780 [04:17<00:51,  3.31it/s] 78%|███████▊  | 612/780 [04:17<00:50,  3.36it/s] 79%|███████▊  | 613/780 [04:17<00:49,  3.39it/s] 79%|███████▊  | 614/780 [04:17<00:48,  3.42it/s] 79%|███████▉  | 615/780 [04:18<00:47,  3.44it/s] 79%|███████▉  | 616/780 [04:18<00:47,  3.45it/s] 79%|███████▉  | 617/780 [04:18<00:47,  3.46it/s] 79%|███████▉  | 618/780 [04:19<00:46,  3.47it/s] 79%|███████▉  | 619/780 [04:19<00:46,  3.47it/s] 79%|███████▉  | 620/780 [04:19<00:46,  3.47it/s] 80%|███████▉  | 621/780 [04:19<00:45,  3.47it/s] 80%|███████▉  | 622/780 [04:20<00:45,  3.48it/s] 80%|███████▉  | 623/780 [04:20<00:45,  3.48it/s] 80%|████████  | 624/780 [04:20<00:44,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 14:36:49,915 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:36:49,915 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 14:36:49,915 >>   Batch size = 8
{'eval_loss': 0.9848144054412842, 'eval_runtime': 11.3518, 'eval_samples_per_second': 366.903, 'eval_steps_per_second': 45.896, 'epoch': 3.0}
{'loss': 0.5904, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.61it/s][A
  2%|▏         | 12/521 [00:00<00:09, 50.92it/s][A
  3%|▎         | 18/521 [00:00<00:10, 48.97it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.35it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.82it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.56it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.45it/s][A
  8%|▊         | 43/521 [00:00<00:12, 39.24it/s][A
  9%|▉         | 48/521 [00:01<00:11, 41.40it/s][A
 10%|█         | 53/521 [00:01<00:10, 42.93it/s][A
 11%|█         | 58/521 [00:01<00:10, 44.12it/s][A
 12%|█▏        | 63/521 [00:01<00:10, 44.93it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 45.56it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.01it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.33it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.53it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.71it/s][A
 18%|█▊        | 93/521 [00:02<00:09, 46.82it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.83it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.92it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.97it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 47.02it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 47.04it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.92it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.95it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.93it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.93it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 47.06it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 47.03it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.99it/s][A
 30%|███       | 158/521 [00:03<00:07, 47.05it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.96it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.97it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.99it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 47.07it/s][A
 35%|███▌      | 183/521 [00:03<00:08, 40.99it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 42.62it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 43.82it/s][A
 38%|███▊      | 198/521 [00:04<00:07, 44.69it/s][A
 39%|███▉      | 203/521 [00:04<00:07, 45.40it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 45.92it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.27it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.47it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.51it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.67it/s][A
 45%|████▍     | 233/521 [00:05<00:06, 46.77it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.86it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.96it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.97it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 47.02it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.99it/s][A
 50%|█████     | 263/521 [00:05<00:05, 47.02it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 47.02it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 47.02it/s][A
 53%|█████▎    | 278/521 [00:06<00:05, 47.05it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 47.03it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.98it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 47.05it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 47.08it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 47.01it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 47.02it/s][A
 60%|██████    | 313/521 [00:06<00:04, 47.04it/s][A
 61%|██████    | 318/521 [00:06<00:04, 46.99it/s][A
 62%|██████▏   | 323/521 [00:07<00:04, 42.84it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 44.02it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 44.92it/s][A
 65%|██████▍   | 338/521 [00:07<00:04, 45.51it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 45.97it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.24it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.47it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.67it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.76it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.83it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 46.93it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.87it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.93it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.97it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.96it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 47.00it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.99it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.89it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.91it/s][A
 80%|████████  | 418/521 [00:09<00:02, 46.99it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.94it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.96it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.98it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.89it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 46.93it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 47.01it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 47.03it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.94it/s][A
 89%|████████▉ | 463/521 [00:10<00:01, 43.05it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 44.17it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 45.00it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 45.62it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.01it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.34it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.56it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.67it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.76it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.76it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 46.76it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.96it/s][A                                                 
                                                 [A 80%|████████  | 624/780 [04:32<00:44,  3.48it/s]
100%|██████████| 521/521 [00:11<00:00, 46.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:37:01,501 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 14:37:01,824 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:37:06,283 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:37:06,502 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:37:06,622 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:47<20:59,  8.13s/it] 80%|████████  | 626/780 [04:47<14:51,  5.79s/it] 80%|████████  | 627/780 [04:47<10:32,  4.14s/it] 81%|████████  | 628/780 [04:48<07:33,  2.98s/it] 81%|████████  | 629/780 [04:48<05:28,  2.17s/it] 81%|████████  | 630/780 [04:48<04:01,  1.61s/it] 81%|████████  | 631/780 [04:48<03:00,  1.21s/it] 81%|████████  | 632/780 [04:49<02:18,  1.07it/s] 81%|████████  | 633/780 [04:49<01:48,  1.35it/s] 81%|████████▏ | 634/780 [04:49<01:28,  1.66it/s] 81%|████████▏ | 635/780 [04:50<01:13,  1.97it/s] 82%|████████▏ | 636/780 [04:50<01:03,  2.26it/s] 82%|████████▏ | 637/780 [04:50<00:56,  2.53it/s] 82%|████████▏ | 638/780 [04:50<00:51,  2.76it/s] 82%|████████▏ | 639/780 [04:51<00:47,  2.94it/s] 82%|████████▏ | 640/780 [04:51<00:45,  3.09it/s] 82%|████████▏ | 641/780 [04:51<00:43,  3.20it/s] 82%|████████▏ | 642/780 [04:52<00:42,  3.28it/s] 82%|████████▏ | 643/780 [04:52<00:40,  3.34it/s] 83%|████████▎ | 644/780 [04:52<00:40,  3.32it/s] 83%|████████▎ | 645/780 [04:52<00:40,  3.37it/s] 83%|████████▎ | 646/780 [04:53<00:39,  3.40it/s] 83%|████████▎ | 647/780 [04:53<00:38,  3.43it/s] 83%|████████▎ | 648/780 [04:53<00:38,  3.45it/s] 83%|████████▎ | 649/780 [04:54<00:37,  3.46it/s] 83%|████████▎ | 650/780 [04:54<00:37,  3.47it/s] 83%|████████▎ | 651/780 [04:54<00:37,  3.48it/s] 84%|████████▎ | 652/780 [04:54<00:36,  3.48it/s] 84%|████████▎ | 653/780 [04:55<00:36,  3.48it/s] 84%|████████▍ | 654/780 [04:55<00:36,  3.49it/s] 84%|████████▍ | 655/780 [04:55<00:36,  3.44it/s] 84%|████████▍ | 656/780 [04:56<00:35,  3.45it/s] 84%|████████▍ | 657/780 [04:56<00:35,  3.46it/s] 84%|████████▍ | 658/780 [04:56<00:35,  3.47it/s] 84%|████████▍ | 659/780 [04:57<00:34,  3.47it/s] 85%|████████▍ | 660/780 [04:57<00:34,  3.48it/s] 85%|████████▍ | 661/780 [04:57<00:34,  3.48it/s] 85%|████████▍ | 662/780 [04:57<00:33,  3.49it/s] 85%|████████▌ | 663/780 [04:58<00:33,  3.49it/s] 85%|████████▌ | 664/780 [04:58<00:33,  3.49it/s] 85%|████████▌ | 665/780 [04:58<00:32,  3.49it/s] 85%|████████▌ | 666/780 [04:59<00:33,  3.37it/s] 86%|████████▌ | 667/780 [04:59<00:33,  3.40it/s] 86%|████████▌ | 668/780 [04:59<00:32,  3.43it/s] 86%|████████▌ | 669/780 [04:59<00:32,  3.45it/s] 86%|████████▌ | 670/780 [05:00<00:31,  3.46it/s] 86%|████████▌ | 671/780 [05:00<00:31,  3.46it/s] 86%|████████▌ | 672/780 [05:00<00:31,  3.47it/s] 86%|████████▋ | 673/780 [05:01<00:30,  3.48it/s] 86%|████████▋ | 674/780 [05:01<00:30,  3.48it/s] 87%|████████▋ | 675/780 [05:01<00:31,  3.38it/s] 87%|████████▋ | 676/780 [05:01<00:30,  3.40it/s] 87%|████████▋ | 677/780 [05:02<00:33,  3.12it/s] 87%|████████▋ | 678/780 [05:02<00:31,  3.21it/s] 87%|████████▋ | 679/780 [05:02<00:30,  3.29it/s] 87%|████████▋ | 680/780 [05:03<00:29,  3.35it/s] 87%|████████▋ | 681/780 [05:03<00:29,  3.39it/s] 87%|████████▋ | 682/780 [05:03<00:28,  3.42it/s] 88%|████████▊ | 683/780 [05:04<00:28,  3.44it/s] 88%|████████▊ | 684/780 [05:04<00:27,  3.45it/s] 88%|████████▊ | 685/780 [05:04<00:27,  3.47it/s] 88%|████████▊ | 686/780 [05:04<00:27,  3.47it/s] 88%|████████▊ | 687/780 [05:05<00:26,  3.48it/s] 88%|████████▊ | 688/780 [05:05<00:27,  3.36it/s] 88%|████████▊ | 689/780 [05:05<00:26,  3.40it/s] 88%|████████▊ | 690/780 [05:06<00:26,  3.42it/s] 89%|████████▊ | 691/780 [05:06<00:25,  3.45it/s] 89%|████████▊ | 692/780 [05:06<00:25,  3.46it/s] 89%|████████▉ | 693/780 [05:06<00:25,  3.47it/s] 89%|████████▉ | 694/780 [05:07<00:24,  3.48it/s] 89%|████████▉ | 695/780 [05:07<00:24,  3.48it/s] 89%|████████▉ | 696/780 [05:07<00:24,  3.48it/s] 89%|████████▉ | 697/780 [05:08<00:23,  3.48it/s] 89%|████████▉ | 698/780 [05:08<00:23,  3.49it/s] 90%|████████▉ | 699/780 [05:08<00:24,  3.31it/s] 90%|████████▉ | 700/780 [05:09<00:23,  3.36it/s] 90%|████████▉ | 701/780 [05:09<00:23,  3.40it/s] 90%|█████████ | 702/780 [05:09<00:22,  3.43it/s] 90%|█████████ | 703/780 [05:09<00:22,  3.45it/s] 90%|█████████ | 704/780 [05:10<00:21,  3.46it/s] 90%|█████████ | 705/780 [05:10<00:21,  3.47it/s] 91%|█████████ | 706/780 [05:10<00:21,  3.47it/s] 91%|█████████ | 707/780 [05:11<00:20,  3.48it/s] 91%|█████████ | 708/780 [05:11<00:20,  3.48it/s] 91%|█████████ | 709/780 [05:11<00:20,  3.49it/s] 91%|█████████ | 710/780 [05:11<00:20,  3.36it/s] 91%|█████████ | 711/780 [05:12<00:20,  3.40it/s] 91%|█████████▏| 712/780 [05:12<00:19,  3.43it/s] 91%|█████████▏| 713/780 [05:12<00:19,  3.45it/s] 92%|█████████▏| 714/780 [05:13<00:19,  3.46it/s] 92%|█████████▏| 715/780 [05:13<00:18,  3.47it/s] 92%|█████████▏| 716/780 [05:13<00:18,  3.48it/s] 92%|█████████▏| 717/780 [05:13<00:18,  3.48it/s] 92%|█████████▏| 718/780 [05:14<00:17,  3.48it/s] 92%|█████████▏| 719/780 [05:14<00:17,  3.48it/s] 92%|█████████▏| 720/780 [05:14<00:17,  3.49it/s] 92%|█████████▏| 721/780 [05:15<00:18,  3.24it/s] 93%|█████████▎| 722/780 [05:15<00:17,  3.31it/s] 93%|█████████▎| 723/780 [05:15<00:16,  3.36it/s] 93%|█████████▎| 724/780 [05:15<00:16,  3.40it/s] 93%|█████████▎| 725/780 [05:16<00:16,  3.43it/s] 93%|█████████▎| 726/780 [05:16<00:15,  3.44it/s] 93%|█████████▎| 727/780 [05:16<00:15,  3.46it/s] 93%|█████████▎| 728/780 [05:17<00:14,  3.47it/s] 93%|█████████▎| 729/780 [05:17<00:14,  3.47it/s] 94%|█████████▎| 730/780 [05:17<00:14,  3.48it/s] 94%|█████████▎| 731/780 [05:17<00:14,  3.48it/s] 94%|█████████▍| 732/780 [05:18<00:14,  3.21it/s] 94%|█████████▍| 733/780 [05:18<00:14,  3.29it/s] 94%|█████████▍| 734/780 [05:18<00:13,  3.34it/s] 94%|█████████▍| 735/780 [05:19<00:13,  3.39it/s] 94%|█████████▍| 736/780 [05:19<00:12,  3.41it/s] 94%|█████████▍| 737/780 [05:19<00:12,  3.44it/s] 95%|█████████▍| 738/780 [05:20<00:12,  3.45it/s] 95%|█████████▍| 739/780 [05:20<00:12,  3.16it/s] 95%|█████████▍| 740/780 [05:20<00:12,  3.25it/s] 95%|█████████▌| 741/780 [05:21<00:11,  3.32it/s] 95%|█████████▌| 742/780 [05:21<00:11,  3.36it/s] 95%|█████████▌| 743/780 [05:21<00:10,  3.40it/s] 95%|█████████▌| 744/780 [05:22<00:12,  2.81it/s] 96%|█████████▌| 745/780 [05:25<00:39,  1.13s/it] 96%|█████████▌| 746/780 [05:25<00:30,  1.11it/s] 96%|█████████▌| 747/780 [05:25<00:23,  1.40it/s] 96%|█████████▌| 748/780 [05:25<00:18,  1.71it/s] 96%|█████████▌| 749/780 [05:26<00:15,  2.02it/s] 96%|█████████▌| 750/780 [05:26<00:12,  2.31it/s] 96%|█████████▋| 751/780 [05:26<00:11,  2.57it/s] 96%|█████████▋| 752/780 [05:27<00:10,  2.80it/s] 97%|█████████▋| 753/780 [05:27<00:09,  2.97it/s] 97%|█████████▋| 754/780 [05:27<00:08,  3.11it/s] 97%|█████████▋| 755/780 [05:27<00:07,  3.22it/s] 97%|█████████▋| 756/780 [05:28<00:07,  3.29it/s] 97%|█████████▋| 757/780 [05:28<00:07,  3.17it/s] 97%|█████████▋| 758/780 [05:28<00:06,  3.26it/s] 97%|█████████▋| 759/780 [05:29<00:06,  3.33it/s] 97%|█████████▋| 760/780 [05:29<00:05,  3.37it/s] 98%|█████████▊| 761/780 [05:29<00:05,  3.41it/s] 98%|█████████▊| 762/780 [05:30<00:05,  3.43it/s] 98%|█████████▊| 763/780 [05:30<00:04,  3.45it/s] 98%|█████████▊| 764/780 [05:30<00:04,  3.46it/s] 98%|█████████▊| 765/780 [05:30<00:04,  3.47it/s] 98%|█████████▊| 766/780 [05:31<00:04,  3.48it/s] 98%|█████████▊| 767/780 [05:31<00:03,  3.48it/s] 98%|█████████▊| 768/780 [05:31<00:03,  3.34it/s] 99%|█████████▊| 769/780 [05:32<00:03,  3.38it/s] 99%|█████████▊| 770/780 [05:32<00:02,  3.42it/s] 99%|█████████▉| 771/780 [05:32<00:02,  3.44it/s] 99%|█████████▉| 772/780 [05:32<00:02,  3.45it/s] 99%|█████████▉| 773/780 [05:33<00:02,  3.46it/s] 99%|█████████▉| 774/780 [05:33<00:01,  3.47it/s] 99%|█████████▉| 775/780 [05:33<00:01,  3.47it/s] 99%|█████████▉| 776/780 [05:34<00:01,  3.48it/s]100%|█████████▉| 777/780 [05:34<00:00,  3.48it/s]100%|█████████▉| 778/780 [05:34<00:00,  3.49it/s]100%|█████████▉| 779/780 [05:35<00:00,  3.29it/s]100%|██████████| 780/780 [05:35<00:00,  2.63it/s][INFO|trainer.py:2140] 2023-08-29 14:38:04,667 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:38:04,667 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 14:38:04,667 >>   Batch size = 8
{'eval_loss': 0.9917645454406738, 'eval_runtime': 11.2911, 'eval_samples_per_second': 368.876, 'eval_steps_per_second': 46.143, 'epoch': 4.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.92it/s][A
  2%|▏         | 12/521 [00:00<00:09, 51.07it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.20it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.43it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.98it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.76it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.57it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.30it/s][A
  9%|▉         | 48/521 [00:00<00:10, 47.21it/s][A
 10%|█         | 53/521 [00:01<00:09, 47.15it/s][A
 11%|█         | 58/521 [00:01<00:09, 47.16it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 47.24it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 47.09it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 47.16it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 47.07it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 47.16it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 47.07it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 47.07it/s][A
 19%|█▉        | 98/521 [00:02<00:08, 47.12it/s][A
 20%|█▉        | 103/521 [00:02<00:09, 43.35it/s][A
 21%|██        | 108/521 [00:02<00:10, 40.20it/s][A
 22%|██▏       | 113/521 [00:02<00:09, 42.07it/s][A
 23%|██▎       | 118/521 [00:02<00:09, 43.48it/s][A
 24%|██▎       | 123/521 [00:02<00:12, 33.10it/s][A
 25%|██▍       | 128/521 [00:02<00:10, 36.68it/s][A
 26%|██▌       | 133/521 [00:02<00:09, 39.30it/s][A
 26%|██▋       | 138/521 [00:03<00:09, 41.39it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 42.99it/s][A
 28%|██▊       | 148/521 [00:03<00:08, 44.07it/s][A
 29%|██▉       | 153/521 [00:03<00:08, 45.04it/s][A
 30%|███       | 158/521 [00:03<00:07, 45.68it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 46.14it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 46.17it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 46.44it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 46.64it/s][A
 35%|███▌      | 183/521 [00:04<00:07, 46.84it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 46.92it/s][A
 37%|███▋      | 193/521 [00:04<00:06, 46.92it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 47.05it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 47.00it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 47.06it/s][A
 41%|████      | 213/521 [00:04<00:06, 47.06it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.92it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 47.05it/s][A
 44%|████▍     | 228/521 [00:05<00:06, 47.01it/s][A
 45%|████▍     | 233/521 [00:05<00:06, 47.06it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 47.11it/s][A
 47%|████▋     | 243/521 [00:05<00:06, 42.73it/s][A
 48%|████▊     | 248/521 [00:05<00:06, 44.01it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 44.91it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 45.59it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.01it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.37it/s][A
 52%|█████▏    | 273/521 [00:06<00:05, 46.64it/s][A
 53%|█████▎    | 278/521 [00:06<00:05, 46.76it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.78it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.83it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.81it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.94it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 47.01it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 47.09it/s][A
 60%|██████    | 313/521 [00:06<00:04, 47.14it/s][A
 61%|██████    | 318/521 [00:06<00:04, 47.04it/s][A
 62%|██████▏   | 323/521 [00:07<00:04, 47.13it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 47.09it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.97it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.94it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 47.01it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 47.06it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 47.11it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 47.10it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 47.07it/s][A
 71%|███████   | 368/521 [00:08<00:03, 47.12it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 47.08it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 47.06it/s][A
 74%|███████▎  | 383/521 [00:08<00:03, 42.85it/s][A
 74%|███████▍  | 388/521 [00:08<00:03, 43.99it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 44.92it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 45.60it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.08it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.37it/s][A
 79%|███████▉  | 413/521 [00:09<00:02, 46.57it/s][A
 80%|████████  | 418/521 [00:09<00:02, 46.74it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.66it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.79it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.87it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.96it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 47.02it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 46.97it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 47.08it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 46.97it/s][A
 89%|████████▉ | 463/521 [00:10<00:01, 47.04it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 47.05it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.95it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 47.04it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 47.07it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 47.07it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 47.09it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 47.07it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 47.09it/s][A
 98%|█████████▊| 508/521 [00:11<00:00, 47.10it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 47.14it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 47.02it/s][A                                                 
                                                 [A100%|██████████| 780/780 [05:46<00:00,  2.63it/s]
100%|██████████| 521/521 [00:11<00:00, 47.02it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 14:38:16,455 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 14:38:16,722 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:38:20,956 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:38:21,081 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:38:21,162 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 14:38:30,434 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 14:38:30,467 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156 (score: 0.9583470225334167).
                                                 100%|██████████| 780/780 [06:14<00:00,  2.63it/s]100%|██████████| 780/780 [06:14<00:00,  2.09it/s]
[INFO|trainer.py:1894] 2023-08-29 14:38:43,229 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 14:38:43,386 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 14:38:48,360 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 14:38:48,672 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 14:38:48,826 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 14:38:49,667 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:38:49,668 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:38:49,668 >>   train_loss               =     0.5796
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:38:49,668 >>   train_runtime            = 0:06:14.02
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:38:49,668 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:38:49,668 >>   train_samples_per_second =     133.68
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:38:49,668 >>   train_steps_per_second   =      2.085
{'eval_loss': 0.9969673752784729, 'eval_runtime': 11.3476, 'eval_samples_per_second': 367.037, 'eval_steps_per_second': 45.913, 'epoch': 5.0}
{'train_runtime': 374.0285, 'train_samples_per_second': 133.68, 'train_steps_per_second': 2.085, 'train_loss': 0.5796065697303185, 'epoch': 5.0}
08/29/2023 14:38:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 14:38:50,180 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 14:38:50,180 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 14:38:50,180 >>   Batch size = 8
  0%|          | 0/521 [00:00<?, ?it/s]  1%|          | 6/521 [00:00<00:08, 58.97it/s]  2%|▏         | 12/521 [00:00<00:09, 52.02it/s]  3%|▎         | 18/521 [00:00<00:10, 50.11it/s]  5%|▍         | 24/521 [00:00<00:17, 28.47it/s]  6%|▌         | 29/521 [00:00<00:15, 32.72it/s]  7%|▋         | 34/521 [00:00<00:13, 36.34it/s]  7%|▋         | 39/521 [00:01<00:12, 39.21it/s]  8%|▊         | 44/521 [00:01<00:11, 41.52it/s]  9%|▉         | 49/521 [00:01<00:10, 43.25it/s] 10%|█         | 54/521 [00:01<00:10, 44.54it/s] 11%|█▏        | 59/521 [00:01<00:10, 45.50it/s] 12%|█▏        | 64/521 [00:01<00:09, 45.91it/s] 13%|█▎        | 69/521 [00:01<00:09, 46.27it/s] 14%|█▍        | 74/521 [00:01<00:09, 46.67it/s] 15%|█▌        | 79/521 [00:01<00:09, 46.96it/s] 16%|█▌        | 84/521 [00:01<00:09, 47.13it/s] 17%|█▋        | 89/521 [00:02<00:09, 47.35it/s] 18%|█▊        | 94/521 [00:02<00:08, 47.49it/s] 19%|█▉        | 99/521 [00:02<00:08, 47.45it/s] 20%|█▉        | 104/521 [00:02<00:08, 47.50it/s] 21%|██        | 109/521 [00:02<00:08, 47.60it/s] 22%|██▏       | 114/521 [00:02<00:08, 47.57it/s] 23%|██▎       | 119/521 [00:02<00:08, 47.55it/s] 24%|██▍       | 124/521 [00:02<00:08, 47.48it/s] 25%|██▍       | 129/521 [00:02<00:08, 47.44it/s] 26%|██▌       | 134/521 [00:03<00:08, 47.49it/s] 27%|██▋       | 139/521 [00:03<00:08, 47.53it/s] 28%|██▊       | 144/521 [00:03<00:07, 47.52it/s] 29%|██▊       | 149/521 [00:03<00:07, 47.64it/s] 30%|██▉       | 154/521 [00:03<00:07, 47.55it/s] 31%|███       | 159/521 [00:03<00:09, 39.23it/s] 31%|███▏      | 164/521 [00:03<00:08, 41.34it/s] 32%|███▏      | 169/521 [00:03<00:08, 43.10it/s] 33%|███▎      | 174/521 [00:03<00:07, 44.37it/s] 34%|███▍      | 179/521 [00:04<00:07, 45.27it/s] 35%|███▌      | 184/521 [00:04<00:07, 45.89it/s] 36%|███▋      | 189/521 [00:04<00:07, 46.43it/s] 37%|███▋      | 194/521 [00:04<00:06, 46.77it/s] 38%|███▊      | 199/521 [00:04<00:06, 46.91it/s] 39%|███▉      | 204/521 [00:04<00:06, 47.12it/s] 40%|████      | 209/521 [00:04<00:06, 47.25it/s] 41%|████      | 214/521 [00:04<00:06, 47.30it/s] 42%|████▏     | 219/521 [00:04<00:06, 47.36it/s] 43%|████▎     | 224/521 [00:04<00:06, 47.44it/s] 44%|████▍     | 229/521 [00:05<00:06, 47.50it/s] 45%|████▍     | 234/521 [00:05<00:06, 47.54it/s] 46%|████▌     | 239/521 [00:05<00:05, 47.48it/s] 47%|████▋     | 244/521 [00:05<00:05, 47.47it/s] 48%|████▊     | 249/521 [00:05<00:05, 47.52it/s] 49%|████▉     | 254/521 [00:05<00:05, 47.53it/s] 50%|████▉     | 259/521 [00:05<00:05, 47.51it/s] 51%|█████     | 264/521 [00:05<00:05, 47.57it/s] 52%|█████▏    | 269/521 [00:05<00:05, 47.52it/s] 53%|█████▎    | 274/521 [00:06<00:05, 47.50it/s] 54%|█████▎    | 279/521 [00:06<00:05, 47.54it/s] 55%|█████▍    | 284/521 [00:06<00:04, 47.57it/s] 55%|█████▌    | 289/521 [00:06<00:04, 47.58it/s] 56%|█████▋    | 294/521 [00:06<00:04, 47.50it/s] 57%|█████▋    | 299/521 [00:06<00:05, 42.07it/s] 58%|█████▊    | 304/521 [00:06<00:04, 43.60it/s] 59%|█████▉    | 309/521 [00:06<00:04, 44.71it/s] 60%|██████    | 314/521 [00:06<00:04, 45.51it/s] 61%|██████    | 319/521 [00:07<00:04, 46.05it/s] 62%|██████▏   | 324/521 [00:07<00:04, 46.51it/s] 63%|██████▎   | 329/521 [00:07<00:04, 46.78it/s] 64%|██████▍   | 334/521 [00:07<00:03, 47.07it/s] 65%|██████▌   | 339/521 [00:07<00:03, 47.13it/s] 66%|██████▌   | 344/521 [00:07<00:03, 47.26it/s] 67%|██████▋   | 349/521 [00:07<00:03, 47.28it/s] 68%|██████▊   | 354/521 [00:07<00:03, 47.33it/s] 69%|██████▉   | 359/521 [00:07<00:03, 47.46it/s] 70%|██████▉   | 364/521 [00:07<00:03, 47.50it/s] 71%|███████   | 369/521 [00:08<00:03, 47.35it/s] 72%|███████▏  | 374/521 [00:08<00:03, 47.52it/s] 73%|███████▎  | 379/521 [00:08<00:02, 47.54it/s] 74%|███████▎  | 384/521 [00:08<00:02, 47.51it/s] 75%|███████▍  | 389/521 [00:08<00:02, 47.40it/s] 76%|███████▌  | 394/521 [00:08<00:02, 47.48it/s] 77%|███████▋  | 399/521 [00:08<00:02, 47.41it/s] 78%|███████▊  | 404/521 [00:08<00:02, 47.26it/s] 79%|███████▊  | 409/521 [00:08<00:02, 47.33it/s] 79%|███████▉  | 414/521 [00:09<00:02, 47.19it/s] 80%|████████  | 419/521 [00:09<00:02, 47.41it/s] 81%|████████▏ | 424/521 [00:09<00:02, 47.41it/s] 82%|████████▏ | 429/521 [00:09<00:01, 47.46it/s] 83%|████████▎ | 434/521 [00:09<00:01, 47.42it/s] 84%|████████▍ | 439/521 [00:09<00:01, 47.37it/s] 85%|████████▌ | 444/521 [00:09<00:01, 42.26it/s] 86%|████████▌ | 449/521 [00:09<00:01, 43.68it/s] 87%|████████▋ | 454/521 [00:09<00:01, 44.66it/s] 88%|████████▊ | 459/521 [00:10<00:01, 45.34it/s] 89%|████████▉ | 464/521 [00:10<00:01, 45.99it/s] 90%|█████████ | 469/521 [00:10<00:01, 46.35it/s] 91%|█████████ | 474/521 [00:10<00:01, 46.66it/s] 92%|█████████▏| 479/521 [00:10<00:00, 46.79it/s] 93%|█████████▎| 484/521 [00:10<00:00, 46.99it/s] 94%|█████████▍| 489/521 [00:10<00:00, 47.14it/s] 95%|█████████▍| 494/521 [00:10<00:00, 47.26it/s] 96%|█████████▌| 499/521 [00:10<00:00, 47.34it/s] 97%|█████████▋| 504/521 [00:10<00:00, 47.39it/s] 98%|█████████▊| 509/521 [00:11<00:00, 47.31it/s] 99%|█████████▊| 514/521 [00:11<00:00, 47.39it/s]100%|█████████▉| 519/521 [00:11<00:00, 47.33it/s]100%|██████████| 521/521 [00:11<00:00, 45.98it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 14:39:01,532 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:39:01,533 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:39:01,533 >>   eval_loss               =     0.9583
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:39:01,533 >>   eval_runtime            = 0:00:11.35
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:39:01,533 >>   eval_samples            =       4165
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:39:01,533 >>   eval_samples_per_second =    366.877
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:39:01,533 >>   eval_steps_per_second   =     45.893
[INFO|trainer_pt_utils.py:913] 2023-08-29 14:39:01,533 >>   perplexity              =     2.6074
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:14,563 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:14,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:14,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:14,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:14,586 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:39:15,386 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:39:15,387 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:39:16,116 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:39:17,290 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:39:17,290 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:18,888 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:18,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:18,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:18,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:39:18,916 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:39:19,749 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:39:19,750 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:39:20,422 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:39:20,736 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:39:20,737 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-468
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'lowest point', 'manufacturer', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12876
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12976, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.82it/s]Extractor Predicting: 2it [00:01,  1.76it/s]Extractor Predicting: 3it [00:01,  1.70it/s]Extractor Predicting: 4it [00:02,  1.73it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.73it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:09,  1.57it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.50it/s]Extractor Predicting: 19it [00:11,  1.50it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.52it/s]Extractor Predicting: 22it [00:13,  1.51it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.48it/s]Extractor Predicting: 25it [00:15,  1.51it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:16,  1.55it/s]Extractor Predicting: 28it [00:17,  1.55it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:20,  1.52it/s]Extractor Predicting: 34it [00:21,  1.49it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:23,  1.50it/s]Extractor Predicting: 38it [00:24,  1.47it/s]Extractor Predicting: 39it [00:24,  1.49it/s]Extractor Predicting: 40it [00:25,  1.51it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:26,  1.47it/s]Extractor Predicting: 43it [00:27,  1.47it/s]Extractor Predicting: 44it [00:28,  1.47it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:29,  1.53it/s]Extractor Predicting: 47it [00:30,  1.48it/s]Extractor Predicting: 48it [00:30,  1.51it/s]Extractor Predicting: 49it [00:31,  1.53it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.54it/s]Extractor Predicting: 53it [00:34,  1.58it/s]Extractor Predicting: 54it [00:34,  1.55it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.44it/s]Extractor Predicting: 57it [00:36,  1.41it/s]Extractor Predicting: 58it [00:37,  1.44it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:38,  1.49it/s]Extractor Predicting: 61it [00:39,  1.54it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:40,  1.54it/s]Extractor Predicting: 64it [00:41,  1.55it/s]Extractor Predicting: 65it [00:42,  1.56it/s]Extractor Predicting: 66it [00:42,  1.54it/s]Extractor Predicting: 67it [00:43,  1.53it/s]Extractor Predicting: 68it [00:44,  1.50it/s]Extractor Predicting: 69it [00:44,  1.52it/s]Extractor Predicting: 70it [00:45,  1.55it/s]Extractor Predicting: 71it [00:45,  1.56it/s]Extractor Predicting: 72it [00:46,  1.51it/s]Extractor Predicting: 73it [00:47,  1.51it/s]Extractor Predicting: 74it [00:47,  1.50it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:49,  1.49it/s]Extractor Predicting: 77it [00:50,  1.46it/s]Extractor Predicting: 78it [00:50,  1.46it/s]Extractor Predicting: 79it [00:51,  1.47it/s]Extractor Predicting: 80it [00:52,  1.47it/s]Extractor Predicting: 81it [00:52,  1.44it/s]Extractor Predicting: 82it [00:53,  1.45it/s]Extractor Predicting: 83it [00:54,  1.51it/s]Extractor Predicting: 84it [00:54,  1.52it/s]Extractor Predicting: 85it [00:55,  1.52it/s]Extractor Predicting: 86it [00:55,  1.54it/s]Extractor Predicting: 87it [00:56,  1.50it/s]Extractor Predicting: 88it [00:57,  1.54it/s]Extractor Predicting: 89it [00:57,  1.52it/s]Extractor Predicting: 90it [00:58,  1.51it/s]Extractor Predicting: 91it [00:59,  1.53it/s]Extractor Predicting: 92it [00:59,  1.56it/s]Extractor Predicting: 93it [01:00,  1.49it/s]Extractor Predicting: 94it [01:01,  1.48it/s]Extractor Predicting: 95it [01:02,  1.47it/s]Extractor Predicting: 96it [01:02,  1.51it/s]Extractor Predicting: 97it [01:03,  1.53it/s]Extractor Predicting: 98it [01:03,  1.53it/s]Extractor Predicting: 99it [01:04,  1.51it/s]Extractor Predicting: 100it [01:05,  1.54it/s]Extractor Predicting: 101it [01:05,  1.52it/s]Extractor Predicting: 102it [01:06,  1.50it/s]Extractor Predicting: 103it [01:07,  1.49it/s]Extractor Predicting: 104it [01:07,  1.52it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:09,  1.55it/s]Extractor Predicting: 108it [01:10,  1.49it/s]Extractor Predicting: 109it [01:11,  1.50it/s]Extractor Predicting: 110it [01:11,  1.51it/s]Extractor Predicting: 111it [01:12,  1.52it/s]Extractor Predicting: 112it [01:13,  1.51it/s]Extractor Predicting: 113it [01:13,  1.49it/s]Extractor Predicting: 114it [01:14,  1.49it/s]Extractor Predicting: 115it [01:15,  1.52it/s]Extractor Predicting: 116it [01:15,  1.54it/s]Extractor Predicting: 117it [01:16,  1.56it/s]Extractor Predicting: 118it [01:17,  1.41it/s]Extractor Predicting: 119it [01:17,  1.41it/s]Extractor Predicting: 120it [01:18,  1.47it/s]Extractor Predicting: 121it [01:19,  1.48it/s]Extractor Predicting: 122it [01:19,  1.48it/s]Extractor Predicting: 123it [01:20,  1.44it/s]Extractor Predicting: 124it [01:21,  1.45it/s]Extractor Predicting: 125it [01:22,  1.45it/s]Extractor Predicting: 126it [01:22,  1.46it/s]Extractor Predicting: 127it [01:23,  1.46it/s]Extractor Predicting: 128it [01:24,  1.43it/s]Extractor Predicting: 129it [01:24,  1.44it/s]Extractor Predicting: 130it [01:25,  1.47it/s]Extractor Predicting: 131it [01:26,  1.45it/s]Extractor Predicting: 132it [01:26,  1.47it/s]Extractor Predicting: 133it [01:27,  1.47it/s]Extractor Predicting: 134it [01:28,  1.49it/s]Extractor Predicting: 135it [01:28,  1.50it/s]Extractor Predicting: 136it [01:29,  1.51it/s]Extractor Predicting: 137it [01:30,  1.49it/s]Extractor Predicting: 138it [01:30,  1.49it/s]Extractor Predicting: 139it [01:31,  1.47it/s]Extractor Predicting: 140it [01:32,  1.49it/s]Extractor Predicting: 141it [01:32,  1.52it/s]Extractor Predicting: 142it [01:33,  1.50it/s]Extractor Predicting: 143it [01:34,  1.52it/s]Extractor Predicting: 144it [01:34,  1.47it/s]Extractor Predicting: 145it [01:35,  1.49it/s]Extractor Predicting: 146it [01:36,  1.47it/s]Extractor Predicting: 147it [01:36,  1.48it/s]Extractor Predicting: 148it [01:37,  1.48it/s]Extractor Predicting: 149it [01:38,  1.43it/s]Extractor Predicting: 150it [01:38,  1.46it/s]Extractor Predicting: 151it [01:39,  1.47it/s]Extractor Predicting: 152it [01:40,  1.43it/s]Extractor Predicting: 153it [01:41,  1.45it/s]Extractor Predicting: 154it [01:41,  1.55it/s]Extractor Predicting: 154it [01:41,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:22,723 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:22,795 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:22,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:22,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:22,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:41:23,718 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:41:23,719 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:41:24,401 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:41:25,515 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:41:25,515 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:28,848 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:28,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:28,905 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:28,905 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:41:28,905 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:41:29,963 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:41:29,965 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:41:30,641 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:41:30,875 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:41:30,875 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.36726546906187624,
  "recall": 0.04417767106842737,
  "score": 0.07886840977282468,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27443
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27543, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.65it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.63it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.55it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:17,  1.55it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.55it/s]Extractor Predicting: 34it [00:21,  1.56it/s]Extractor Predicting: 35it [00:22,  1.58it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.49it/s]Extractor Predicting: 39it [00:24,  1.56it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.48it/s]Extractor Predicting: 44it [00:28,  1.48it/s]Extractor Predicting: 45it [00:28,  1.50it/s]Extractor Predicting: 46it [00:29,  1.48it/s]Extractor Predicting: 47it [00:30,  1.47it/s]Extractor Predicting: 48it [00:30,  1.42it/s]Extractor Predicting: 49it [00:31,  1.45it/s]Extractor Predicting: 50it [00:32,  1.45it/s]Extractor Predicting: 51it [00:32,  1.47it/s]Extractor Predicting: 52it [00:33,  1.48it/s]Extractor Predicting: 53it [00:34,  1.45it/s]Extractor Predicting: 54it [00:35,  1.45it/s]Extractor Predicting: 55it [00:35,  1.42it/s]Extractor Predicting: 56it [00:36,  1.47it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:37,  1.49it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:39,  1.45it/s]Extractor Predicting: 61it [00:39,  1.45it/s]Extractor Predicting: 62it [00:40,  1.49it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:41,  1.44it/s]Extractor Predicting: 65it [00:42,  1.45it/s]Extractor Predicting: 66it [00:43,  1.46it/s]Extractor Predicting: 67it [00:43,  1.45it/s]Extractor Predicting: 68it [00:44,  1.44it/s]Extractor Predicting: 69it [00:45,  1.44it/s]Extractor Predicting: 70it [00:46,  1.44it/s]Extractor Predicting: 71it [00:46,  1.46it/s]Extractor Predicting: 72it [00:47,  1.46it/s]Extractor Predicting: 73it [00:48,  1.43it/s]Extractor Predicting: 74it [00:48,  1.42it/s]Extractor Predicting: 75it [00:49,  1.43it/s]Extractor Predicting: 76it [00:50,  1.41it/s]Extractor Predicting: 77it [00:50,  1.46it/s]Extractor Predicting: 78it [00:51,  1.42it/s]Extractor Predicting: 79it [00:52,  1.44it/s]Extractor Predicting: 80it [00:53,  1.44it/s]Extractor Predicting: 81it [00:53,  1.40it/s]Extractor Predicting: 82it [00:54,  1.42it/s]Extractor Predicting: 83it [00:55,  1.44it/s]Extractor Predicting: 84it [00:55,  1.48it/s]Extractor Predicting: 85it [00:56,  1.45it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:57,  1.48it/s]Extractor Predicting: 88it [00:58,  1.45it/s]Extractor Predicting: 89it [00:59,  1.43it/s]Extractor Predicting: 90it [00:59,  1.42it/s]Extractor Predicting: 91it [01:00,  1.44it/s]Extractor Predicting: 92it [01:01,  1.47it/s]Extractor Predicting: 93it [01:01,  1.47it/s]Extractor Predicting: 94it [01:02,  1.49it/s]Extractor Predicting: 95it [01:03,  1.51it/s]Extractor Predicting: 96it [01:03,  1.51it/s]Extractor Predicting: 97it [01:04,  1.43it/s]Extractor Predicting: 98it [01:05,  1.42it/s]Extractor Predicting: 99it [01:06,  1.46it/s]Extractor Predicting: 100it [01:06,  1.45it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:08,  1.49it/s]Extractor Predicting: 103it [01:08,  1.48it/s]Extractor Predicting: 104it [01:09,  1.50it/s]Extractor Predicting: 105it [01:10,  1.49it/s]Extractor Predicting: 106it [01:10,  1.48it/s]Extractor Predicting: 107it [01:11,  1.50it/s]Extractor Predicting: 108it [01:12,  1.48it/s]Extractor Predicting: 109it [01:12,  1.47it/s]Extractor Predicting: 110it [01:13,  1.39it/s]Extractor Predicting: 111it [01:14,  1.28it/s]Extractor Predicting: 112it [01:15,  1.30it/s]Extractor Predicting: 113it [01:15,  1.38it/s]Extractor Predicting: 114it [01:16,  1.43it/s]Extractor Predicting: 115it [01:17,  1.41it/s]Extractor Predicting: 116it [01:17,  1.43it/s]Extractor Predicting: 117it [01:18,  1.46it/s]Extractor Predicting: 118it [01:19,  1.40it/s]Extractor Predicting: 119it [01:20,  1.42it/s]Extractor Predicting: 120it [01:20,  1.45it/s]Extractor Predicting: 121it [01:21,  1.44it/s]Extractor Predicting: 122it [01:22,  1.47it/s]Extractor Predicting: 123it [01:22,  1.45it/s]Extractor Predicting: 124it [01:23,  1.48it/s]Extractor Predicting: 125it [01:24,  1.48it/s]Extractor Predicting: 126it [01:24,  1.50it/s]Extractor Predicting: 127it [01:25,  1.51it/s]Extractor Predicting: 128it [01:26,  1.43it/s]Extractor Predicting: 129it [01:26,  1.48it/s]Extractor Predicting: 130it [01:27,  1.45it/s]Extractor Predicting: 131it [01:28,  1.52it/s]Extractor Predicting: 132it [01:28,  1.54it/s]Extractor Predicting: 133it [01:29,  1.52it/s]Extractor Predicting: 134it [01:30,  1.51it/s]Extractor Predicting: 135it [01:30,  1.49it/s]Extractor Predicting: 136it [01:31,  1.49it/s]Extractor Predicting: 137it [01:32,  1.49it/s]Extractor Predicting: 138it [01:32,  1.48it/s]Extractor Predicting: 139it [01:33,  1.49it/s]Extractor Predicting: 140it [01:34,  1.48it/s]Extractor Predicting: 141it [01:34,  1.53it/s]Extractor Predicting: 142it [01:35,  1.48it/s]Extractor Predicting: 143it [01:36,  1.40it/s]Extractor Predicting: 144it [01:36,  1.44it/s]Extractor Predicting: 145it [01:37,  1.47it/s]Extractor Predicting: 146it [01:38,  1.42it/s]Extractor Predicting: 147it [01:39,  1.44it/s]Extractor Predicting: 148it [01:39,  1.44it/s]Extractor Predicting: 149it [01:40,  1.54it/s]Extractor Predicting: 150it [01:40,  1.62it/s]Extractor Predicting: 151it [01:41,  1.60it/s]Extractor Predicting: 152it [01:41,  1.69it/s]Extractor Predicting: 153it [01:42,  1.74it/s]Extractor Predicting: 154it [01:43,  1.72it/s]Extractor Predicting: 155it [01:43,  1.75it/s]Extractor Predicting: 156it [01:44,  1.72it/s]Extractor Predicting: 157it [01:44,  1.75it/s]Extractor Predicting: 158it [01:45,  1.78it/s]Extractor Predicting: 159it [01:45,  1.81it/s]Extractor Predicting: 160it [01:46,  1.76it/s]Extractor Predicting: 161it [01:47,  1.73it/s]Extractor Predicting: 162it [01:47,  1.75it/s]Extractor Predicting: 163it [01:48,  1.71it/s]Extractor Predicting: 164it [01:48,  1.71it/s]Extractor Predicting: 165it [01:49,  1.75it/s]Extractor Predicting: 166it [01:49,  1.77it/s]Extractor Predicting: 167it [01:50,  1.75it/s]Extractor Predicting: 168it [01:51,  1.69it/s]Extractor Predicting: 169it [01:51,  1.64it/s]Extractor Predicting: 170it [01:52,  1.56it/s]Extractor Predicting: 171it [01:53,  1.53it/s]Extractor Predicting: 172it [01:53,  1.50it/s]Extractor Predicting: 173it [01:54,  1.48it/s]Extractor Predicting: 174it [01:55,  1.47it/s]Extractor Predicting: 175it [01:55,  1.52it/s]Extractor Predicting: 176it [01:56,  1.51it/s]Extractor Predicting: 177it [01:57,  1.53it/s]Extractor Predicting: 178it [01:57,  1.52it/s]Extractor Predicting: 179it [01:58,  1.57it/s]Extractor Predicting: 180it [01:59,  1.58it/s]Extractor Predicting: 181it [01:59,  1.57it/s]Extractor Predicting: 182it [02:00,  1.56it/s]Extractor Predicting: 183it [02:01,  1.53it/s]Extractor Predicting: 184it [02:01,  1.50it/s]Extractor Predicting: 185it [02:02,  1.53it/s]Extractor Predicting: 186it [02:03,  1.50it/s]Extractor Predicting: 187it [02:03,  1.50it/s]Extractor Predicting: 188it [02:04,  1.56it/s]Extractor Predicting: 189it [02:04,  1.54it/s]Extractor Predicting: 190it [02:05,  1.54it/s]Extractor Predicting: 191it [02:06,  1.53it/s]Extractor Predicting: 192it [02:06,  1.55it/s]Extractor Predicting: 193it [02:07,  1.54it/s]Extractor Predicting: 194it [02:08,  1.59it/s]Extractor Predicting: 195it [02:08,  1.57it/s]Extractor Predicting: 196it [02:09,  1.54it/s]Extractor Predicting: 197it [02:10,  1.56it/s]Extractor Predicting: 198it [02:10,  1.55it/s]Extractor Predicting: 199it [02:11,  1.54it/s]Extractor Predicting: 200it [02:12,  1.52it/s]Extractor Predicting: 201it [02:12,  1.50it/s]Extractor Predicting: 202it [02:13,  1.48it/s]Extractor Predicting: 203it [02:14,  1.45it/s]Extractor Predicting: 204it [02:14,  1.46it/s]Extractor Predicting: 205it [02:15,  1.47it/s]Extractor Predicting: 206it [02:16,  1.45it/s]Extractor Predicting: 207it [02:16,  1.47it/s]Extractor Predicting: 208it [02:17,  1.48it/s]Extractor Predicting: 209it [02:18,  1.50it/s]Extractor Predicting: 210it [02:18,  1.51it/s]Extractor Predicting: 211it [02:19,  1.46it/s]Extractor Predicting: 212it [02:20,  1.51it/s]Extractor Predicting: 213it [02:20,  1.49it/s]Extractor Predicting: 214it [02:21,  1.51it/s]Extractor Predicting: 215it [02:22,  1.53it/s]Extractor Predicting: 216it [02:22,  1.53it/s]Extractor Predicting: 217it [02:23,  1.52it/s]Extractor Predicting: 218it [02:24,  1.52it/s]Extractor Predicting: 219it [02:24,  1.45it/s]Extractor Predicting: 220it [02:25,  1.51it/s]Extractor Predicting: 221it [02:26,  1.50it/s]Extractor Predicting: 222it [02:26,  1.53it/s]Extractor Predicting: 223it [02:27,  1.54it/s]Extractor Predicting: 224it [02:28,  1.54it/s]Extractor Predicting: 225it [02:28,  1.58it/s]Extractor Predicting: 226it [02:29,  1.60it/s]Extractor Predicting: 227it [02:29,  1.63it/s]Extractor Predicting: 228it [02:30,  1.53it/s]Extractor Predicting: 229it [02:31,  1.55it/s]Extractor Predicting: 230it [02:31,  1.53it/s]Extractor Predicting: 231it [02:32,  1.53it/s]Extractor Predicting: 232it [02:33,  1.58it/s]Extractor Predicting: 233it [02:33,  1.55it/s]Extractor Predicting: 234it [02:34,  1.53it/s]Extractor Predicting: 235it [02:35,  1.55it/s]Extractor Predicting: 236it [02:35,  1.55it/s]Extractor Predicting: 237it [02:36,  1.43it/s]Extractor Predicting: 238it [02:37,  1.44it/s]Extractor Predicting: 239it [02:37,  1.47it/s]Extractor Predicting: 240it [02:38,  1.47it/s]Extractor Predicting: 241it [02:39,  1.48it/s]Extractor Predicting: 242it [02:39,  1.49it/s]Extractor Predicting: 243it [02:40,  1.51it/s]Extractor Predicting: 244it [02:41,  1.51it/s]Extractor Predicting: 245it [02:41,  1.50it/s]Extractor Predicting: 246it [02:42,  1.50it/s]Extractor Predicting: 247it [02:43,  1.48it/s]Extractor Predicting: 248it [02:44,  1.44it/s]Extractor Predicting: 249it [02:44,  1.46it/s]Extractor Predicting: 250it [02:45,  1.48it/s]Extractor Predicting: 251it [02:46,  1.52it/s]Extractor Predicting: 252it [02:46,  1.54it/s]Extractor Predicting: 253it [02:47,  1.49it/s]Extractor Predicting: 254it [02:47,  1.52it/s]Extractor Predicting: 255it [02:48,  1.52it/s]Extractor Predicting: 256it [02:49,  1.53it/s]Extractor Predicting: 257it [02:49,  1.53it/s]Extractor Predicting: 258it [02:50,  1.50it/s]Extractor Predicting: 259it [02:51,  1.47it/s]Extractor Predicting: 260it [02:52,  1.49it/s]Extractor Predicting: 261it [02:52,  1.50it/s]Extractor Predicting: 262it [02:53,  1.52it/s]Extractor Predicting: 263it [02:53,  1.51it/s]Extractor Predicting: 264it [02:54,  1.49it/s]Extractor Predicting: 265it [02:55,  1.49it/s]Extractor Predicting: 266it [02:55,  1.52it/s]Extractor Predicting: 267it [02:56,  1.52it/s]Extractor Predicting: 268it [02:57,  1.51it/s]Extractor Predicting: 269it [02:57,  1.50it/s]Extractor Predicting: 270it [02:58,  1.51it/s]Extractor Predicting: 271it [02:59,  1.52it/s]Extractor Predicting: 272it [02:59,  1.50it/s]Extractor Predicting: 273it [03:00,  1.47it/s]Extractor Predicting: 274it [03:01,  1.48it/s]Extractor Predicting: 275it [03:01,  1.50it/s]Extractor Predicting: 276it [03:02,  1.52it/s]Extractor Predicting: 277it [03:03,  1.51it/s]Extractor Predicting: 278it [03:03,  1.52it/s]Extractor Predicting: 279it [03:04,  1.48it/s]Extractor Predicting: 280it [03:05,  1.50it/s]Extractor Predicting: 281it [03:06,  1.45it/s]Extractor Predicting: 282it [03:06,  1.47it/s]Extractor Predicting: 283it [03:07,  1.46it/s]Extractor Predicting: 284it [03:08,  1.45it/s]Extractor Predicting: 285it [03:08,  1.42it/s]Extractor Predicting: 286it [03:09,  1.39it/s]Extractor Predicting: 287it [03:10,  1.41it/s]Extractor Predicting: 288it [03:10,  1.44it/s]Extractor Predicting: 289it [03:11,  1.41it/s]Extractor Predicting: 290it [03:12,  1.43it/s]Extractor Predicting: 291it [03:13,  1.44it/s]Extractor Predicting: 292it [03:13,  1.46it/s]Extractor Predicting: 293it [03:14,  1.47it/s]Extractor Predicting: 294it [03:15,  1.47it/s]Extractor Predicting: 295it [03:15,  1.50it/s]Extractor Predicting: 296it [03:16,  1.52it/s]Extractor Predicting: 297it [03:16,  1.54it/s]Extractor Predicting: 298it [03:17,  1.59it/s]Extractor Predicting: 299it [03:18,  1.54it/s]Extractor Predicting: 300it [03:18,  1.59it/s]Extractor Predicting: 301it [03:19,  1.58it/s]Extractor Predicting: 302it [03:20,  1.54it/s]Extractor Predicting: 303it [03:20,  1.51it/s]Extractor Predicting: 304it [03:21,  1.50it/s]Extractor Predicting: 305it [03:22,  1.52it/s]Extractor Predicting: 306it [03:22,  1.52it/s]Extractor Predicting: 307it [03:23,  1.54it/s]Extractor Predicting: 308it [03:24,  1.53it/s]Extractor Predicting: 309it [03:24,  1.51it/s]Extractor Predicting: 310it [03:25,  1.48it/s]Extractor Predicting: 311it [03:26,  1.48it/s]Extractor Predicting: 312it [03:26,  1.51it/s]Extractor Predicting: 313it [03:27,  1.49it/s]Extractor Predicting: 314it [03:28,  1.50it/s]Extractor Predicting: 315it [03:28,  1.52it/s]Extractor Predicting: 316it [03:29,  1.51it/s]Extractor Predicting: 317it [03:30,  1.49it/s]Extractor Predicting: 318it [03:30,  1.52it/s]Extractor Predicting: 319it [03:31,  1.48it/s]Extractor Predicting: 320it [03:32,  1.48it/s]Extractor Predicting: 321it [03:32,  1.49it/s]Extractor Predicting: 322it [03:33,  1.51it/s]Extractor Predicting: 323it [03:34,  1.52it/s]Extractor Predicting: 324it [03:34,  1.53it/s]Extractor Predicting: 325it [03:35,  1.54it/s]Extractor Predicting: 326it [03:36,  1.54it/s]Extractor Predicting: 327it [03:36,  1.56it/s]Extractor Predicting: 328it [03:37,  1.54it/s]Extractor Predicting: 329it [03:37,  1.55it/s]Extractor Predicting: 330it [03:38,  1.55it/s]Extractor Predicting: 331it [03:39,  1.53it/s]Extractor Predicting: 332it [03:39,  1.52it/s]Extractor Predicting: 333it [03:40,  1.50it/s]Extractor Predicting: 334it [03:41,  1.47it/s]Extractor Predicting: 335it [03:42,  1.47it/s]Extractor Predicting: 336it [03:42,  1.50it/s]Extractor Predicting: 337it [03:43,  1.49it/s]Extractor Predicting: 338it [03:44,  1.49it/s]Extractor Predicting: 339it [03:44,  1.50it/s]Extractor Predicting: 340it [03:45,  1.54it/s]Extractor Predicting: 341it [03:45,  1.51it/s]Extractor Predicting: 342it [03:46,  1.54it/s]Extractor Predicting: 343it [03:47,  1.57it/s]Extractor Predicting: 344it [03:47,  1.61it/s]Extractor Predicting: 345it [03:48,  1.60it/s]Extractor Predicting: 346it [03:48,  1.65it/s]Extractor Predicting: 347it [03:49,  1.69it/s]Extractor Predicting: 348it [03:50,  1.67it/s]Extractor Predicting: 349it [03:50,  1.68it/s]Extractor Predicting: 350it [03:51,  1.63it/s]Extractor Predicting: 351it [03:52,  1.42it/s]Extractor Predicting: 352it [03:52,  1.46it/s]Extractor Predicting: 353it [03:53,  1.49it/s]Extractor Predicting: 354it [03:54,  1.46it/s]Extractor Predicting: 355it [03:54,  1.51it/s]Extractor Predicting: 356it [03:55,  1.50it/s]Extractor Predicting: 357it [03:56,  1.49it/s]Extractor Predicting: 358it [03:56,  1.49it/s]Extractor Predicting: 359it [03:57,  1.48it/s]Extractor Predicting: 360it [03:58,  1.49it/s]Extractor Predicting: 361it [03:58,  1.49it/s]Extractor Predicting: 362it [03:59,  1.48it/s]Extractor Predicting: 363it [04:00,  1.50it/s]Extractor Predicting: 364it [04:00,  1.52it/s]Extractor Predicting: 365it [04:01,  1.50it/s]Extractor Predicting: 366it [04:02,  1.49it/s]Extractor Predicting: 367it [04:03,  1.47it/s]Extractor Predicting: 368it [04:03,  1.48it/s]Extractor Predicting: 369it [04:04,  1.48it/s]Extractor Predicting: 370it [04:05,  1.50it/s]Extractor Predicting: 371it [04:05,  1.50it/s]Extractor Predicting: 372it [04:06,  1.45it/s]Extractor Predicting: 373it [04:07,  1.46it/s]Extractor Predicting: 374it [04:07,  1.49it/s]Extractor Predicting: 375it [04:08,  1.52it/s]Extractor Predicting: 376it [04:08,  1.56it/s]Extractor Predicting: 377it [04:09,  1.60it/s]Extractor Predicting: 378it [04:10,  1.59it/s]Extractor Predicting: 379it [04:10,  1.61it/s]Extractor Predicting: 380it [04:11,  1.61it/s]Extractor Predicting: 381it [04:12,  1.62it/s]Extractor Predicting: 382it [04:12,  1.58it/s]Extractor Predicting: 383it [04:13,  1.58it/s]Extractor Predicting: 384it [04:13,  1.58it/s]Extractor Predicting: 385it [04:14,  1.60it/s]Extractor Predicting: 386it [04:15,  1.63it/s]Extractor Predicting: 387it [04:15,  1.61it/s]Extractor Predicting: 388it [04:16,  1.61it/s]Extractor Predicting: 389it [04:17,  1.62it/s]Extractor Predicting: 390it [04:17,  1.63it/s]Extractor Predicting: 391it [04:18,  1.62it/s]Extractor Predicting: 392it [04:18,  1.63it/s]Extractor Predicting: 393it [04:19,  1.62it/s]Extractor Predicting: 394it [04:20,  1.64it/s]Extractor Predicting: 395it [04:20,  1.62it/s]Extractor Predicting: 396it [04:21,  1.61it/s]Extractor Predicting: 397it [04:22,  1.53it/s]Extractor Predicting: 398it [04:22,  1.52it/s]Extractor Predicting: 399it [04:23,  1.51it/s]Extractor Predicting: 400it [04:24,  1.51it/s]Extractor Predicting: 401it [04:24,  1.52it/s]Extractor Predicting: 402it [04:25,  1.54it/s]Extractor Predicting: 403it [04:25,  1.58it/s]Extractor Predicting: 404it [04:26,  1.49it/s]Extractor Predicting: 405it [04:27,  1.49it/s]Extractor Predicting: 406it [04:27,  1.51it/s]Extractor Predicting: 407it [04:28,  1.51it/s]Extractor Predicting: 408it [04:29,  1.52it/s]Extractor Predicting: 409it [04:30,  1.46it/s]Extractor Predicting: 410it [04:30,  1.47it/s]Extractor Predicting: 411it [04:31,  1.49it/s]Extractor Predicting: 412it [04:31,  1.53it/s]Extractor Predicting: 413it [04:32,  1.51it/s]Extractor Predicting: 414it [04:33,  1.49it/s]Extractor Predicting: 415it [04:34,  1.50it/s]Extractor Predicting: 416it [04:34,  1.51it/s]Extractor Predicting: 417it [04:35,  1.52it/s]Extractor Predicting: 418it [04:35,  1.54it/s]Extractor Predicting: 419it [04:36,  1.52it/s]Extractor Predicting: 420it [04:37,  1.52it/s]Extractor Predicting: 421it [04:37,  1.53it/s]Extractor Predicting: 422it [04:38,  1.51it/s]Extractor Predicting: 423it [04:39,  1.50it/s]Extractor Predicting: 424it [04:39,  1.52it/s]Extractor Predicting: 425it [04:40,  1.51it/s]Extractor Predicting: 426it [04:41,  1.56it/s]Extractor Predicting: 427it [04:41,  1.60it/s]Extractor Predicting: 428it [04:42,  1.60it/s]Extractor Predicting: 429it [04:43,  1.59it/s]Extractor Predicting: 430it [04:43,  1.61it/s]Extractor Predicting: 431it [04:44,  1.66it/s]Extractor Predicting: 432it [04:44,  1.63it/s]Extractor Predicting: 433it [04:45,  1.61it/s]Extractor Predicting: 434it [04:46,  1.60it/s]Extractor Predicting: 435it [04:46,  1.59it/s]Extractor Predicting: 436it [04:47,  1.58it/s]Extractor Predicting: 437it [04:47,  1.63it/s]Extractor Predicting: 438it [04:48,  1.65it/s]Extractor Predicting: 439it [04:49,  1.62it/s]Extractor Predicting: 440it [04:49,  1.59it/s]Extractor Predicting: 441it [04:50,  1.60it/s]Extractor Predicting: 442it [04:51,  1.63it/s]Extractor Predicting: 443it [04:51,  1.59it/s]Extractor Predicting: 444it [04:52,  1.59it/s]Extractor Predicting: 445it [04:52,  1.61it/s]Extractor Predicting: 446it [04:53,  1.64it/s]Extractor Predicting: 447it [04:54,  1.62it/s]Extractor Predicting: 448it [04:54,  1.64it/s]Extractor Predicting: 449it [04:55,  1.65it/s]Extractor Predicting: 450it [04:55,  1.64it/s]Extractor Predicting: 451it [04:56,  1.64it/s]Extractor Predicting: 452it [04:57,  1.55it/s]Extractor Predicting: 453it [04:57,  1.56it/s]Extractor Predicting: 454it [04:58,  1.51it/s]Extractor Predicting: 455it [04:59,  1.56it/s]Extractor Predicting: 456it [04:59,  1.60it/s]Extractor Predicting: 457it [05:00,  1.58it/s]Extractor Predicting: 458it [05:01,  1.61it/s]Extractor Predicting: 459it [05:01,  1.59it/s]Extractor Predicting: 460it [05:02,  1.54it/s]Extractor Predicting: 461it [05:03,  1.53it/s]Extractor Predicting: 462it [05:03,  1.47it/s]Extractor Predicting: 463it [05:04,  1.47it/s]Extractor Predicting: 464it [05:05,  1.51it/s]Extractor Predicting: 465it [05:05,  1.52it/s]Extractor Predicting: 466it [05:06,  1.52it/s]Extractor Predicting: 467it [05:07,  1.46it/s]Extractor Predicting: 468it [05:07,  1.48it/s]Extractor Predicting: 469it [05:08,  1.51it/s]Extractor Predicting: 470it [05:09,  1.49it/s]Extractor Predicting: 471it [05:09,  1.46it/s]Extractor Predicting: 472it [05:10,  1.44it/s]Extractor Predicting: 473it [05:11,  1.48it/s]Extractor Predicting: 474it [05:11,  1.52it/s]Extractor Predicting: 475it [05:12,  1.52it/s]Extractor Predicting: 476it [05:13,  1.52it/s]Extractor Predicting: 477it [05:13,  1.40it/s]Extractor Predicting: 478it [05:14,  1.41it/s]Extractor Predicting: 479it [05:15,  1.41it/s]Extractor Predicting: 480it [05:16,  1.43it/s]Extractor Predicting: 481it [05:16,  1.46it/s]Extractor Predicting: 482it [05:17,  1.43it/s]Extractor Predicting: 483it [05:18,  1.47it/s]Extractor Predicting: 484it [05:18,  1.47it/s]Extractor Predicting: 485it [05:19,  1.34it/s]Extractor Predicting: 486it [05:20,  1.41it/s]Extractor Predicting: 487it [05:21,  1.40it/s]Extractor Predicting: 488it [05:21,  1.41it/s]Extractor Predicting: 489it [05:22,  1.44it/s]Extractor Predicting: 490it [05:23,  1.44it/s]Extractor Predicting: 491it [05:23,  1.45it/s]Extractor Predicting: 492it [05:24,  1.36it/s]Extractor Predicting: 493it [05:25,  1.40it/s]Extractor Predicting: 494it [05:25,  1.43it/s]Extractor Predicting: 495it [05:26,  1.50it/s]Extractor Predicting: 496it [05:27,  1.51it/s]Extractor Predicting: 496it [05:27,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:18,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:18,917 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:18,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:18,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:18,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:47:19,540 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:47:19,541 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:47:19,896 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:47:21,050 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:47:21,050 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:22,708 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:22,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:22,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:22,756 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:47:22,757 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:47:23,303 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:47:23,304 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:47:24,063 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:47:24,298 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:47:24,298 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.35344367325146825,
  "recall": 0.055611559139784945,
  "score": 0.09610219931770342,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 10529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:01,  1.51it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.45it/s]Extractor Predicting: 16it [00:10,  1.44it/s]Extractor Predicting: 17it [00:11,  1.42it/s]Extractor Predicting: 18it [00:12,  1.40it/s]Extractor Predicting: 19it [00:13,  1.37it/s]Extractor Predicting: 20it [00:14,  1.32it/s]Extractor Predicting: 21it [00:14,  1.31it/s]Extractor Predicting: 22it [00:15,  1.30it/s]Extractor Predicting: 23it [00:16,  1.30it/s]Extractor Predicting: 24it [00:17,  1.35it/s]Extractor Predicting: 25it [00:17,  1.38it/s]Extractor Predicting: 26it [00:18,  1.40it/s]Extractor Predicting: 27it [00:19,  1.39it/s]Extractor Predicting: 28it [00:19,  1.40it/s]Extractor Predicting: 29it [00:20,  1.38it/s]Extractor Predicting: 30it [00:21,  1.38it/s]Extractor Predicting: 31it [00:21,  1.39it/s]Extractor Predicting: 32it [00:22,  1.41it/s]Extractor Predicting: 33it [00:23,  1.45it/s]Extractor Predicting: 34it [00:23,  1.48it/s]Extractor Predicting: 35it [00:24,  1.43it/s]Extractor Predicting: 36it [00:25,  1.44it/s]Extractor Predicting: 37it [00:26,  1.46it/s]Extractor Predicting: 38it [00:26,  1.46it/s]Extractor Predicting: 39it [00:27,  1.48it/s]Extractor Predicting: 40it [00:28,  1.48it/s]Extractor Predicting: 41it [00:28,  1.48it/s]Extractor Predicting: 42it [00:29,  1.50it/s]Extractor Predicting: 43it [00:30,  1.50it/s]Extractor Predicting: 44it [00:30,  1.51it/s]Extractor Predicting: 45it [00:31,  1.48it/s]Extractor Predicting: 46it [00:32,  1.45it/s]Extractor Predicting: 47it [00:32,  1.45it/s]Extractor Predicting: 48it [00:33,  1.47it/s]Extractor Predicting: 49it [00:34,  1.49it/s]Extractor Predicting: 50it [00:34,  1.45it/s]Extractor Predicting: 51it [00:35,  1.46it/s]Extractor Predicting: 52it [00:36,  1.47it/s]Extractor Predicting: 53it [00:36,  1.48it/s]Extractor Predicting: 54it [00:37,  1.48it/s]Extractor Predicting: 55it [00:38,  1.47it/s]Extractor Predicting: 56it [00:38,  1.49it/s]Extractor Predicting: 57it [00:39,  1.52it/s]Extractor Predicting: 58it [00:40,  1.52it/s]Extractor Predicting: 59it [00:40,  1.52it/s]Extractor Predicting: 60it [00:41,  1.36it/s]Extractor Predicting: 61it [00:42,  1.30it/s]Extractor Predicting: 62it [00:43,  1.36it/s]Extractor Predicting: 63it [00:44,  1.36it/s]Extractor Predicting: 64it [00:44,  1.34it/s]Extractor Predicting: 65it [00:45,  1.35it/s]Extractor Predicting: 66it [00:46,  1.43it/s]Extractor Predicting: 67it [00:46,  1.52it/s]Extractor Predicting: 68it [00:47,  1.59it/s]Extractor Predicting: 69it [00:47,  1.67it/s]Extractor Predicting: 70it [00:48,  1.75it/s]Extractor Predicting: 71it [00:48,  1.80it/s]Extractor Predicting: 72it [00:49,  1.81it/s]Extractor Predicting: 73it [00:49,  1.84it/s]Extractor Predicting: 74it [00:50,  1.83it/s]Extractor Predicting: 75it [00:51,  1.76it/s]Extractor Predicting: 76it [00:51,  1.78it/s]Extractor Predicting: 77it [00:52,  1.78it/s]Extractor Predicting: 78it [00:52,  1.80it/s]Extractor Predicting: 79it [00:53,  1.84it/s]Extractor Predicting: 80it [00:53,  1.81it/s]Extractor Predicting: 81it [00:54,  1.77it/s]Extractor Predicting: 82it [00:54,  1.80it/s]Extractor Predicting: 83it [00:55,  1.84it/s]Extractor Predicting: 84it [00:55,  1.87it/s]Extractor Predicting: 85it [00:56,  1.86it/s]Extractor Predicting: 86it [00:57,  1.85it/s]Extractor Predicting: 87it [00:57,  1.87it/s]Extractor Predicting: 88it [00:58,  1.83it/s]Extractor Predicting: 89it [00:58,  1.82it/s]Extractor Predicting: 90it [00:59,  1.81it/s]Extractor Predicting: 91it [00:59,  1.81it/s]Extractor Predicting: 92it [01:00,  1.83it/s]Extractor Predicting: 93it [01:00,  1.83it/s]Extractor Predicting: 94it [01:01,  1.83it/s]Extractor Predicting: 95it [01:01,  1.85it/s]Extractor Predicting: 96it [01:02,  1.71it/s]Extractor Predicting: 97it [01:03,  1.64it/s]Extractor Predicting: 98it [01:03,  1.59it/s]Extractor Predicting: 99it [01:04,  1.50it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:06,  1.46it/s]Extractor Predicting: 102it [01:06,  1.45it/s]Extractor Predicting: 103it [01:07,  1.46it/s]Extractor Predicting: 104it [01:08,  1.44it/s]Extractor Predicting: 105it [01:08,  1.43it/s]Extractor Predicting: 106it [01:09,  1.44it/s]Extractor Predicting: 107it [01:10,  1.43it/s]Extractor Predicting: 108it [01:11,  1.42it/s]Extractor Predicting: 109it [01:11,  1.41it/s]Extractor Predicting: 110it [01:12,  1.41it/s]Extractor Predicting: 111it [01:13,  1.42it/s]Extractor Predicting: 112it [01:13,  1.44it/s]Extractor Predicting: 113it [01:14,  1.47it/s]Extractor Predicting: 114it [01:15,  1.52it/s]Extractor Predicting: 115it [01:15,  1.51it/s]Extractor Predicting: 116it [01:16,  1.54it/s]Extractor Predicting: 117it [01:17,  1.55it/s]Extractor Predicting: 118it [01:17,  1.55it/s]Extractor Predicting: 119it [01:18,  1.55it/s]Extractor Predicting: 120it [01:18,  1.57it/s]Extractor Predicting: 121it [01:19,  1.57it/s]Extractor Predicting: 122it [01:20,  1.59it/s]Extractor Predicting: 123it [01:20,  1.58it/s]Extractor Predicting: 124it [01:21,  1.54it/s]Extractor Predicting: 125it [01:22,  1.52it/s]Extractor Predicting: 126it [01:22,  1.54it/s]Extractor Predicting: 127it [01:23,  1.53it/s]Extractor Predicting: 128it [01:24,  1.50it/s]Extractor Predicting: 129it [01:24,  1.50it/s]Extractor Predicting: 130it [01:25,  1.48it/s]Extractor Predicting: 131it [01:26,  1.46it/s]Extractor Predicting: 132it [01:26,  1.44it/s]Extractor Predicting: 133it [01:27,  1.51it/s]Extractor Predicting: 133it [01:27,  1.52it/s]
[INFO|configuration_utils.py:515] 2023-08-29 14:48:57,214 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 14:48:57,270 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 14:48:57,366 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 14:48:57,367 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 14:48:57,431 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 14:49:16,459 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 14:49:16,536 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 14:49:16,831 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 14:49:16,832 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 14:49:17,037 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:49:17,166 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:49:17,166 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:49:17,166 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:49:17,166 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:49:17,166 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 14:49:17,166 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6824175824175824,
  "recall": 0.08230616302186879,
  "score": 0.14689532820816087,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 14:49:17,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:18,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:19,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:20,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:20,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:21,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:22,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:23,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:23,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:24,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:25,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:25,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:26,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:27,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:27,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:28,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:29,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:29,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:30,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:31,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:26, 14.05s/it][WARNING|generation_utils.py:914] 2023-08-29 14:49:31,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:32,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:33,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:33,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:34,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:35,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:35,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:36,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:36,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:37,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:38,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:38,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:39,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:40,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:40,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:41,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:42,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:42,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:43,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:44,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:44,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:27<04:08, 13.82s/it][WARNING|generation_utils.py:914] 2023-08-29 14:49:45,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:46,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:46,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:47,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:48,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:49,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:50,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:50,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:51,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:52,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:53,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:53,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:54,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:55,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:56,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:57,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:57,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:58,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:49:59,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:00,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:00,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:43<04:10, 14.75s/it][WARNING|generation_utils.py:914] 2023-08-29 14:50:01,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:02,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:02,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:03,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:04,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:05,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:05,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:06,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:07,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:07,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:08,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:09,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:09,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:10,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:11,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:12,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:12,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:13,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:14,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:14,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:15,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:58<03:55, 14.69s/it][WARNING|generation_utils.py:914] 2023-08-29 14:50:16,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:16,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:17,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:18,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:18,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:19,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:20,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:20,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:21,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:22,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:23,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:23,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:24,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:24,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:25,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:26,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:27,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:27,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:28,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:29,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:29,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:30,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:31,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:14<03:47, 15.17s/it][WARNING|generation_utils.py:914] 2023-08-29 14:50:32,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:33,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:33,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:34,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:34,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:35,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:36,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:36,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:37,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:38,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:38,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:39,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:40,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:40,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:41,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:42,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:43,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:43,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:44,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:45,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:45,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:28<03:29, 14.93s/it][WARNING|generation_utils.py:914] 2023-08-29 14:50:46,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:47,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:47,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:48,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:49,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:50,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:51,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:51,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:52,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:53,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:54,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:54,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:55,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:56,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:56,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:57,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:58,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:58,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:50:59,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:00,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:01,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:43<03:15, 15.06s/it][WARNING|generation_utils.py:914] 2023-08-29 14:51:01,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:02,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:03,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:04,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:04,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:05,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:06,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:06,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:07,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:08,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:08,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:09,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:10,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:11,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:11,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:12,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:13,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:14,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:14,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:15,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:15,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:16,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:59<03:01, 15.16s/it][WARNING|generation_utils.py:914] 2023-08-29 14:51:17,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:18,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:18,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:19,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:20,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:20,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:21,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:22,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:23,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:24,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:24,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:25,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:26,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:26,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:27,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:28,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:28,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:29,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:30,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:31,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:32,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:32,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:33,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:16<02:53, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-29 14:51:34,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:34,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:35,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:36,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:36,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:37,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:38,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:39,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:39,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:40,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:41,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:41,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:42,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:43,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:43,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:44,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:45,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:45,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:46,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:47,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:48,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:30<02:33, 15.38s/it][WARNING|generation_utils.py:914] 2023-08-29 14:51:48,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:49,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:50,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:50,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:51,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:52,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:52,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:53,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:54,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:54,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:55,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:55,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:56,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:56,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:57,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:58,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:58,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:51:59,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:00,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:00,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:43<02:10, 14.51s/it][WARNING|generation_utils.py:914] 2023-08-29 14:52:01,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:02,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:02,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:03,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:04,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:04,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:05,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:05,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:06,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:06,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:07,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:08,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:08,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:09,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:09,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:10,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:11,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:11,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:12,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:12,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:55<01:50, 13.76s/it][WARNING|generation_utils.py:914] 2023-08-29 14:52:13,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:14,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:14,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:15,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:16,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:16,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:17,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:18,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:19,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:19,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:20,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:21,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:21,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:22,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:23,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:23,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:24,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:25,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:26,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:26,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:27,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:28,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:10<01:39, 14.25s/it][WARNING|generation_utils.py:914] 2023-08-29 14:52:28,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:29,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:30,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:30,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:31,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:32,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:32,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:33,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:34,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:34,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:35,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:36,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:36,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:37,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:37,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:38,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:39,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:40,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:40,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:41,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:42,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:24<01:24, 14.13s/it][WARNING|generation_utils.py:914] 2023-08-29 14:52:42,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:43,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:44,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:44,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:45,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:46,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:47,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:47,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:48,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:49,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:50,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:50,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:51,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:52,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:53,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:53,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:54,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:55,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:56,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:56,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:57,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:52:58,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:41<01:14, 14.93s/it][WARNING|generation_utils.py:914] 2023-08-29 14:52:59,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:00,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:01,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:01,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:02,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:03,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:03,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:04,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:05,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:06,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:06,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:07,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:08,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:09,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:09,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:10,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:11,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:12,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:12,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:13,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:14,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:56<01:00, 15.05s/it][WARNING|generation_utils.py:914] 2023-08-29 14:53:14,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:15,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:16,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:16,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:17,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:18,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:19,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:19,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:20,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:21,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:22,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:22,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:23,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:24,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:24,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:25,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:26,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:27,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:27,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:28,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:29,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:12<00:45, 15.17s/it][WARNING|generation_utils.py:914] 2023-08-29 14:53:30,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:31,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:31,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:32,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:32,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:33,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:34,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:34,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:35,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:35,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:36,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:37,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:37,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:38,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:39,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:39,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:40,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:41,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:41,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:42,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:42,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:25<00:29, 14.65s/it][WARNING|generation_utils.py:914] 2023-08-29 14:53:43,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:44,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:44,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:45,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:46,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:46,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:47,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:48,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:48,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:49,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:50,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:50,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:51,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:52,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:53,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:53,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:54,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:55,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:55,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:56,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:57,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:39<00:14, 14.51s/it][WARNING|generation_utils.py:914] 2023-08-29 14:53:57,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:58,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:53:59,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:00,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:01,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:02,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:02,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:03,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:04,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:05,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:06,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:07,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:08,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:08,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:09,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:10,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:11,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:12,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:13,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:13,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 14:54:14,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:57<00:00, 15.40s/it]Generating: 100%|██████████| 20/20 [04:57<00:00, 14.87s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:24,973 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:25,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:25,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:25,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:25,000 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 14:54:25,823 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 14:54:25,825 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:54:26,434 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 14:54:27,597 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:54:27,597 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:31,038 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:31,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:31,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:31,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 14:54:31,080 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 14:54:31,981 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 14:54:31,982 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 14:54:32,645 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 14:54:32,882 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 14:54:32,882 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 335, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 459, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 562, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : work location .', 'success_rate': 0.8301630434782609, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 621, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9241071428571429, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9092261904761905, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 426, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : has quality .', 'success_rate': 0.890625, 'errors': {'', "('Hugo Award for Best Original Song', 'has quality', '', 'A short story , originally published in 1937 , was nominated for the Hugo Award for Best Original Song in 1958 , won by composer John Carpenter .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 623, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.8464673913043478, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 520, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 489, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 518, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.9453125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 405, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : member of .', 'success_rate': 0.890625, 'errors': {'', "('George I of England', 'member of', '', 'Following the death of his father in June 1917 , he was appointed Minister under George I of England as Minister of Finance , and later as President of the Treasury .')"}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 592, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9211309523809523, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8849431818181818, 'errors': {''}}
['Relation : opposite of . Context : Later in the year ( 1177 ) , he purchased the lands of Lüthold in Bavaria , near Eger , now the seat of the Einsatzgruppen . Head Entity : Nørland , Tail Entity : Eger .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 442, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 588, 'raw': 640}
{'target': 600, 'success': 617, 'raw': 672}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.9181547619047619, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 517, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.9151785714285714, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 493, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9122023809523809, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 13007
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13107, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.29it/s]Extractor Estimating: 2it [00:01,  1.45it/s]Extractor Estimating: 3it [00:01,  1.56it/s]Extractor Estimating: 4it [00:02,  1.63it/s]Extractor Estimating: 5it [00:03,  1.64it/s]Extractor Estimating: 6it [00:03,  1.58it/s]Extractor Estimating: 7it [00:04,  1.56it/s]Extractor Estimating: 8it [00:05,  1.60it/s]Extractor Estimating: 9it [00:05,  1.64it/s]Extractor Estimating: 10it [00:06,  1.68it/s]Extractor Estimating: 11it [00:06,  1.62it/s]Extractor Estimating: 12it [00:07,  1.59it/s]Extractor Estimating: 13it [00:08,  1.63it/s]Extractor Estimating: 14it [00:08,  1.63it/s]Extractor Estimating: 15it [00:09,  1.66it/s]Extractor Estimating: 16it [00:09,  1.61it/s]Extractor Estimating: 17it [00:10,  1.63it/s]Extractor Estimating: 18it [00:11,  1.65it/s]Extractor Estimating: 19it [00:11,  1.67it/s]Extractor Estimating: 20it [00:12,  1.67it/s]Extractor Estimating: 21it [00:12,  1.68it/s]Extractor Estimating: 22it [00:13,  1.69it/s]Extractor Estimating: 23it [00:14,  1.64it/s]Extractor Estimating: 24it [00:14,  1.65it/s]Extractor Estimating: 25it [00:15,  1.65it/s]Extractor Estimating: 26it [00:15,  1.71it/s]Extractor Estimating: 27it [00:16,  1.71it/s]Extractor Estimating: 28it [00:17,  1.74it/s]Extractor Estimating: 29it [00:17,  1.76it/s]Extractor Estimating: 30it [00:18,  1.71it/s]Extractor Estimating: 31it [00:18,  1.70it/s]Extractor Estimating: 32it [00:19,  1.65it/s]Extractor Estimating: 33it [00:20,  1.68it/s]Extractor Estimating: 34it [00:20,  1.72it/s]Extractor Estimating: 35it [00:21,  1.72it/s]Extractor Estimating: 36it [00:21,  1.65it/s]Extractor Estimating: 37it [00:22,  1.69it/s]Extractor Estimating: 38it [00:23,  1.65it/s]Extractor Estimating: 39it [00:23,  1.70it/s]Extractor Estimating: 40it [00:24,  1.62it/s]Extractor Estimating: 41it [00:24,  1.63it/s]Extractor Estimating: 42it [00:25,  1.69it/s]Extractor Estimating: 43it [00:26,  1.65it/s]Extractor Estimating: 44it [00:26,  1.66it/s]Extractor Estimating: 45it [00:27,  1.68it/s]Extractor Estimating: 46it [00:27,  1.67it/s]Extractor Estimating: 47it [00:28,  1.63it/s]Extractor Estimating: 48it [00:29,  1.60it/s]Extractor Estimating: 49it [00:29,  1.59it/s]Extractor Estimating: 50it [00:30,  1.63it/s]Extractor Estimating: 51it [00:30,  1.66it/s]Extractor Estimating: 52it [00:31,  1.56it/s]Extractor Estimating: 53it [00:32,  1.57it/s]Extractor Estimating: 54it [00:32,  1.56it/s]Extractor Estimating: 55it [00:33,  1.54it/s]Extractor Estimating: 56it [00:34,  1.57it/s]Extractor Estimating: 57it [00:34,  1.53it/s]Extractor Estimating: 58it [00:35,  1.51it/s]Extractor Estimating: 59it [00:36,  1.54it/s]Extractor Estimating: 60it [00:36,  1.50it/s]Extractor Estimating: 61it [00:37,  1.51it/s]Extractor Estimating: 62it [00:38,  1.49it/s]Extractor Estimating: 63it [00:38,  1.48it/s]Extractor Estimating: 64it [00:39,  1.50it/s]Extractor Estimating: 65it [00:40,  1.51it/s]Extractor Estimating: 66it [00:40,  1.53it/s]Extractor Estimating: 67it [00:41,  1.44it/s]Extractor Estimating: 68it [00:42,  1.41it/s]Extractor Estimating: 69it [00:43,  1.47it/s]Extractor Estimating: 70it [00:43,  1.50it/s]Extractor Estimating: 71it [00:44,  1.53it/s]Extractor Estimating: 72it [00:44,  1.56it/s]Extractor Estimating: 73it [00:45,  1.47it/s]Extractor Estimating: 74it [00:46,  1.49it/s]Extractor Estimating: 75it [00:46,  1.48it/s]Extractor Estimating: 76it [00:47,  1.54it/s]Extractor Estimating: 77it [00:48,  1.62it/s]Extractor Estimating: 78it [00:48,  1.65it/s]Extractor Estimating: 79it [00:49,  1.59it/s]Extractor Estimating: 80it [00:49,  1.65it/s]Extractor Estimating: 81it [00:50,  1.59it/s]Extractor Estimating: 82it [00:51,  1.61it/s]Extractor Estimating: 83it [00:51,  1.60it/s]Extractor Estimating: 84it [00:52,  1.67it/s]Extractor Estimating: 85it [00:52,  1.73it/s]Extractor Estimating: 86it [00:53,  1.77it/s]Extractor Estimating: 87it [00:54,  1.73it/s]Extractor Estimating: 88it [00:54,  1.77it/s]Extractor Estimating: 89it [00:55,  1.66it/s]Extractor Estimating: 90it [00:55,  1.66it/s]Extractor Estimating: 91it [00:56,  1.70it/s]Extractor Estimating: 92it [00:57,  1.69it/s]Extractor Estimating: 93it [00:57,  1.72it/s]Extractor Estimating: 94it [00:58,  1.73it/s]Extractor Estimating: 95it [00:58,  1.70it/s]Extractor Estimating: 96it [00:59,  1.78it/s]Extractor Estimating: 97it [00:59,  1.86it/s]Extractor Estimating: 98it [01:00,  1.78it/s]Extractor Estimating: 99it [01:00,  1.82it/s]Extractor Estimating: 100it [01:01,  1.78it/s]Extractor Estimating: 101it [01:02,  1.73it/s]Extractor Estimating: 102it [01:02,  1.68it/s]Extractor Estimating: 103it [01:03,  1.65it/s]Extractor Estimating: 104it [01:03,  1.64it/s]Extractor Estimating: 105it [01:04,  1.62it/s]Extractor Estimating: 106it [01:05,  1.61it/s]Extractor Estimating: 107it [01:05,  1.62it/s]Extractor Estimating: 108it [01:06,  1.63it/s]Extractor Estimating: 109it [01:07,  1.61it/s]Extractor Estimating: 110it [01:07,  1.60it/s]Extractor Estimating: 111it [01:08,  1.60it/s]Extractor Estimating: 112it [01:09,  1.58it/s]Extractor Estimating: 113it [01:09,  1.59it/s]Extractor Estimating: 114it [01:10,  1.58it/s]Extractor Estimating: 115it [01:10,  1.59it/s]Extractor Estimating: 116it [01:11,  1.61it/s]Extractor Estimating: 117it [01:12,  1.63it/s]Extractor Estimating: 118it [01:12,  1.63it/s]Extractor Estimating: 119it [01:13,  1.59it/s]Extractor Estimating: 120it [01:13,  1.60it/s]Extractor Estimating: 121it [01:14,  1.57it/s]Extractor Estimating: 122it [01:15,  1.54it/s]Extractor Estimating: 123it [01:15,  1.54it/s]Extractor Estimating: 124it [01:16,  1.56it/s]Extractor Estimating: 125it [01:17,  1.49it/s]Extractor Estimating: 126it [01:18,  1.39it/s]Extractor Estimating: 127it [01:18,  1.46it/s]Extractor Estimating: 128it [01:19,  1.48it/s]Extractor Estimating: 129it [01:20,  1.56it/s]Extractor Estimating: 130it [01:20,  1.64it/s]Extractor Estimating: 131it [01:21,  1.65it/s]Extractor Estimating: 132it [01:21,  1.62it/s]Extractor Estimating: 133it [01:22,  1.63it/s]Extractor Estimating: 134it [01:23,  1.57it/s]Extractor Estimating: 135it [01:23,  1.65it/s]Extractor Estimating: 136it [01:24,  1.69it/s]Extractor Estimating: 137it [01:24,  1.69it/s]Extractor Estimating: 138it [01:25,  1.68it/s]Extractor Estimating: 139it [01:26,  1.64it/s]Extractor Estimating: 140it [01:26,  1.61it/s]Extractor Estimating: 141it [01:27,  1.60it/s]Extractor Estimating: 142it [01:27,  1.61it/s]Extractor Estimating: 143it [01:28,  1.61it/s]Extractor Estimating: 144it [01:29,  1.62it/s]Extractor Estimating: 145it [01:29,  1.58it/s]Extractor Estimating: 146it [01:30,  1.61it/s]Extractor Estimating: 147it [01:30,  1.63it/s]Extractor Estimating: 148it [01:31,  1.66it/s]Extractor Estimating: 149it [01:32,  1.69it/s]Extractor Estimating: 150it [01:32,  1.64it/s]Extractor Estimating: 151it [01:33,  1.65it/s]Extractor Estimating: 152it [01:34,  1.58it/s]Extractor Estimating: 153it [01:34,  1.60it/s]Extractor Estimating: 154it [01:35,  1.56it/s]Extractor Estimating: 155it [01:36,  1.55it/s]Extractor Estimating: 156it [01:36,  1.60it/s]Extractor Estimating: 157it [01:37,  1.61it/s]Extractor Estimating: 158it [01:37,  1.64it/s]Extractor Estimating: 159it [01:38,  1.65it/s]Extractor Estimating: 160it [01:39,  1.52it/s]Extractor Estimating: 161it [01:39,  1.55it/s]Extractor Estimating: 162it [01:40,  1.61it/s]Extractor Estimating: 163it [01:40,  1.61it/s]Extractor Estimating: 164it [01:41,  1.45it/s]Extractor Estimating: 165it [01:42,  1.43it/s]Extractor Estimating: 166it [01:43,  1.47it/s]Extractor Estimating: 167it [01:43,  1.56it/s]Extractor Estimating: 168it [01:44,  1.61it/s]Extractor Estimating: 169it [01:44,  1.60it/s]Extractor Estimating: 170it [01:45,  1.57it/s]Extractor Estimating: 171it [01:46,  1.56it/s]Extractor Estimating: 172it [01:46,  1.52it/s]Extractor Estimating: 173it [01:47,  1.55it/s]Extractor Estimating: 174it [01:48,  1.58it/s]Extractor Estimating: 175it [01:48,  1.59it/s]Extractor Estimating: 176it [01:49,  1.60it/s]Extractor Estimating: 177it [01:50,  1.57it/s]Extractor Estimating: 178it [01:50,  1.56it/s]Extractor Estimating: 179it [01:51,  1.60it/s]Extractor Estimating: 180it [01:51,  1.63it/s]Extractor Estimating: 181it [01:52,  1.65it/s]Extractor Estimating: 182it [01:53,  1.62it/s]Extractor Estimating: 183it [01:53,  1.64it/s]Extractor Estimating: 184it [01:54,  1.64it/s]Extractor Estimating: 185it [01:54,  1.62it/s]Extractor Estimating: 186it [01:55,  1.62it/s]Extractor Estimating: 187it [01:56,  1.59it/s]Extractor Estimating: 188it [01:56,  1.58it/s]Extractor Estimating: 189it [01:57,  1.58it/s]Extractor Estimating: 190it [01:58,  1.59it/s]Extractor Estimating: 191it [01:58,  1.63it/s]Extractor Estimating: 192it [01:59,  1.62it/s]Extractor Estimating: 193it [01:59,  1.64it/s]Extractor Estimating: 194it [02:00,  1.61it/s]Extractor Estimating: 195it [02:01,  1.67it/s]Extractor Estimating: 196it [02:01,  1.66it/s]Extractor Estimating: 197it [02:02,  1.65it/s]Extractor Estimating: 198it [02:02,  1.69it/s]Extractor Estimating: 199it [02:03,  1.74it/s]Extractor Estimating: 200it [02:04,  1.68it/s]Extractor Estimating: 201it [02:04,  1.63it/s]Extractor Estimating: 202it [02:05,  1.57it/s]Extractor Estimating: 203it [02:06,  1.56it/s]Extractor Estimating: 204it [02:06,  1.60it/s]Extractor Estimating: 205it [02:07,  1.60it/s]Extractor Estimating: 206it [02:07,  1.61it/s]Extractor Estimating: 207it [02:08,  1.57it/s]Extractor Estimating: 208it [02:09,  1.61it/s]Extractor Estimating: 209it [02:09,  1.60it/s]Extractor Estimating: 210it [02:10,  1.61it/s]Extractor Estimating: 211it [02:11,  1.59it/s]Extractor Estimating: 212it [02:11,  1.53it/s]Extractor Estimating: 213it [02:12,  1.58it/s]Extractor Estimating: 214it [02:12,  1.61it/s]Extractor Estimating: 215it [02:13,  1.58it/s]Extractor Estimating: 216it [02:14,  1.62it/s]Extractor Estimating: 217it [02:14,  1.64it/s]Extractor Estimating: 218it [02:15,  1.60it/s]Extractor Estimating: 219it [02:16,  1.58it/s]Extractor Estimating: 220it [02:16,  1.62it/s]Extractor Estimating: 221it [02:17,  1.61it/s]Extractor Estimating: 222it [02:17,  1.58it/s]Extractor Estimating: 223it [02:18,  1.58it/s]Extractor Estimating: 224it [02:19,  1.54it/s]Extractor Estimating: 225it [02:19,  1.56it/s]Extractor Estimating: 226it [02:20,  1.56it/s]Extractor Estimating: 227it [02:21,  1.51it/s]Extractor Estimating: 228it [02:21,  1.53it/s]Extractor Estimating: 229it [02:22,  1.52it/s]Extractor Estimating: 230it [02:23,  1.51it/s]Extractor Estimating: 231it [02:23,  1.47it/s]Extractor Estimating: 232it [02:24,  1.46it/s]Extractor Estimating: 233it [02:25,  1.51it/s]Extractor Estimating: 234it [02:25,  1.54it/s]Extractor Estimating: 235it [02:26,  1.56it/s]Extractor Estimating: 236it [02:27,  1.53it/s]Extractor Estimating: 237it [02:27,  1.53it/s]Extractor Estimating: 238it [02:28,  1.52it/s]Extractor Estimating: 239it [02:29,  1.37it/s]Extractor Estimating: 240it [02:30,  1.40it/s]Extractor Estimating: 241it [02:30,  1.41it/s]Extractor Estimating: 242it [02:31,  1.46it/s]Extractor Estimating: 243it [02:32,  1.43it/s]Extractor Estimating: 244it [02:32,  1.47it/s]Extractor Estimating: 245it [02:33,  1.45it/s]Extractor Estimating: 246it [02:34,  1.48it/s]Extractor Estimating: 247it [02:34,  1.45it/s]Extractor Estimating: 248it [02:35,  1.50it/s]Extractor Estimating: 249it [02:36,  1.50it/s]Extractor Estimating: 250it [02:36,  1.53it/s]Extractor Estimating: 251it [02:37,  1.60it/s]Extractor Estimating: 252it [02:37,  1.73it/s]Extractor Estimating: 253it [02:38,  1.80it/s]Extractor Estimating: 254it [02:38,  1.84it/s]Extractor Estimating: 255it [02:39,  1.87it/s]Extractor Estimating: 256it [02:39,  1.90it/s]Extractor Estimating: 257it [02:40,  1.78it/s]Extractor Estimating: 258it [02:41,  1.79it/s]Extractor Estimating: 259it [02:41,  1.85it/s]Extractor Estimating: 260it [02:42,  1.85it/s]Extractor Estimating: 261it [02:42,  1.96it/s]Extractor Estimating: 262it [02:42,  1.99it/s]Extractor Estimating: 263it [02:43,  1.94it/s]Extractor Estimating: 264it [02:44,  1.88it/s]Extractor Estimating: 265it [02:44,  1.88it/s]Extractor Estimating: 266it [02:45,  1.97it/s]Extractor Estimating: 267it [02:45,  1.93it/s]Extractor Estimating: 268it [02:46,  1.91it/s]Extractor Estimating: 269it [02:46,  1.96it/s]Extractor Estimating: 270it [02:47,  2.00it/s]Extractor Estimating: 271it [02:47,  2.02it/s]Extractor Estimating: 272it [02:48,  1.95it/s]Extractor Estimating: 273it [02:48,  2.04it/s]Extractor Estimating: 274it [02:49,  2.01it/s]Extractor Estimating: 275it [02:49,  1.99it/s]Extractor Estimating: 276it [02:50,  1.94it/s]Extractor Estimating: 277it [02:50,  1.94it/s]Extractor Estimating: 278it [02:51,  1.87it/s]Extractor Estimating: 279it [02:51,  1.90it/s]Extractor Estimating: 280it [02:52,  1.82it/s]Extractor Estimating: 281it [02:52,  1.89it/s]Extractor Estimating: 282it [02:53,  1.92it/s]Extractor Estimating: 283it [02:53,  1.94it/s]Extractor Estimating: 284it [02:54,  1.90it/s]Extractor Estimating: 285it [02:54,  1.88it/s]Extractor Estimating: 286it [02:55,  1.86it/s]Extractor Estimating: 287it [02:56,  1.77it/s]Extractor Estimating: 288it [02:56,  1.80it/s]Extractor Estimating: 289it [02:57,  1.90it/s]Extractor Estimating: 290it [02:57,  1.88it/s]Extractor Estimating: 291it [02:58,  1.85it/s]Extractor Estimating: 292it [02:58,  1.82it/s]Extractor Estimating: 293it [02:59,  1.82it/s]Extractor Estimating: 294it [02:59,  1.89it/s]Extractor Estimating: 295it [03:00,  1.89it/s]Extractor Estimating: 296it [03:00,  1.83it/s]Extractor Estimating: 297it [03:01,  1.85it/s]Extractor Estimating: 298it [03:01,  1.85it/s]Extractor Estimating: 299it [03:02,  1.90it/s]Extractor Estimating: 300it [03:03,  1.92it/s]Extractor Estimating: 301it [03:03,  1.85it/s]Extractor Estimating: 302it [03:04,  1.70it/s]Extractor Estimating: 303it [03:04,  1.64it/s]Extractor Estimating: 304it [03:05,  1.66it/s]Extractor Estimating: 305it [03:06,  1.61it/s]Extractor Estimating: 306it [03:06,  1.63it/s]Extractor Estimating: 307it [03:07,  1.58it/s]Extractor Estimating: 308it [03:08,  1.59it/s]Extractor Estimating: 309it [03:08,  1.60it/s]Extractor Estimating: 310it [03:09,  1.59it/s]Extractor Estimating: 311it [03:09,  1.64it/s]Extractor Estimating: 312it [03:10,  1.58it/s]Extractor Estimating: 313it [03:11,  1.58it/s]Extractor Estimating: 314it [03:11,  1.60it/s]Extractor Estimating: 315it [03:12,  1.61it/s]Extractor Estimating: 316it [03:13,  1.57it/s]Extractor Estimating: 317it [03:13,  1.50it/s]Extractor Estimating: 318it [03:14,  1.52it/s]Extractor Estimating: 319it [03:15,  1.54it/s]Extractor Estimating: 320it [03:15,  1.56it/s]Extractor Estimating: 321it [03:16,  1.58it/s]Extractor Estimating: 322it [03:16,  1.62it/s]Extractor Estimating: 323it [03:17,  1.57it/s]Extractor Estimating: 324it [03:18,  1.56it/s]Extractor Estimating: 325it [03:18,  1.61it/s]Extractor Estimating: 326it [03:19,  1.64it/s]Extractor Estimating: 327it [03:20,  1.65it/s]Extractor Estimating: 328it [03:20,  1.67it/s]Extractor Estimating: 329it [03:21,  1.50it/s]Extractor Estimating: 330it [03:22,  1.55it/s]Extractor Estimating: 331it [03:22,  1.63it/s]Extractor Estimating: 332it [03:23,  1.69it/s]Extractor Estimating: 333it [03:23,  1.70it/s]Extractor Estimating: 334it [03:24,  1.69it/s]Extractor Estimating: 335it [03:24,  1.69it/s]Extractor Estimating: 336it [03:25,  1.66it/s]Extractor Estimating: 337it [03:26,  1.65it/s]Extractor Estimating: 338it [03:26,  1.67it/s]Extractor Estimating: 339it [03:27,  1.65it/s]Extractor Estimating: 340it [03:27,  1.65it/s]Extractor Estimating: 341it [03:28,  1.68it/s]Extractor Estimating: 342it [03:29,  1.68it/s]Extractor Estimating: 343it [03:29,  1.72it/s]Extractor Estimating: 344it [03:30,  1.70it/s]Extractor Estimating: 345it [03:30,  1.71it/s]Extractor Estimating: 346it [03:31,  1.71it/s]Extractor Estimating: 347it [03:32,  1.66it/s]Extractor Estimating: 348it [03:32,  1.70it/s]Extractor Estimating: 349it [03:33,  1.73it/s]Extractor Estimating: 350it [03:33,  1.72it/s]Extractor Estimating: 351it [03:34,  1.65it/s]Extractor Estimating: 352it [03:35,  1.61it/s]Extractor Estimating: 353it [03:35,  1.60it/s]Extractor Estimating: 354it [03:36,  1.59it/s]Extractor Estimating: 355it [03:36,  1.62it/s]Extractor Estimating: 356it [03:37,  1.65it/s]Extractor Estimating: 357it [03:38,  1.59it/s]Extractor Estimating: 358it [03:38,  1.66it/s]Extractor Estimating: 359it [03:39,  1.67it/s]Extractor Estimating: 360it [03:39,  1.63it/s]Extractor Estimating: 361it [03:40,  1.64it/s]Extractor Estimating: 362it [03:41,  1.56it/s]Extractor Estimating: 363it [03:41,  1.58it/s]Extractor Estimating: 364it [03:42,  1.57it/s]Extractor Estimating: 365it [03:43,  1.61it/s]Extractor Estimating: 366it [03:43,  1.63it/s]Extractor Estimating: 367it [03:44,  1.58it/s]Extractor Estimating: 368it [03:44,  1.61it/s]Extractor Estimating: 369it [03:45,  1.54it/s]Extractor Estimating: 370it [03:46,  1.53it/s]Extractor Estimating: 371it [03:46,  1.58it/s]Extractor Estimating: 372it [03:47,  1.64it/s]Extractor Estimating: 373it [03:48,  1.62it/s]Extractor Estimating: 374it [03:48,  1.63it/s]Extractor Estimating: 375it [03:49,  1.62it/s]Extractor Estimating: 376it [03:50,  1.53it/s]Extractor Estimating: 377it [03:50,  1.55it/s]Extractor Estimating: 378it [03:51,  1.59it/s]Extractor Estimating: 379it [03:51,  1.59it/s]Extractor Estimating: 380it [03:52,  1.62it/s]Extractor Estimating: 381it [03:53,  1.61it/s]Extractor Estimating: 382it [03:53,  1.66it/s]Extractor Estimating: 383it [03:54,  1.69it/s]Extractor Estimating: 384it [03:54,  1.62it/s]Extractor Estimating: 385it [03:55,  1.60it/s]Extractor Estimating: 386it [03:56,  1.55it/s]Extractor Estimating: 387it [03:56,  1.59it/s]Extractor Estimating: 388it [03:57,  1.59it/s]Extractor Estimating: 389it [03:58,  1.59it/s]Extractor Estimating: 390it [03:58,  1.59it/s]Extractor Estimating: 391it [03:59,  1.57it/s]Extractor Estimating: 392it [04:00,  1.56it/s]Extractor Estimating: 393it [04:00,  1.57it/s]Extractor Estimating: 394it [04:01,  1.57it/s]Extractor Estimating: 395it [04:01,  1.63it/s]Extractor Estimating: 396it [04:02,  1.63it/s]Extractor Estimating: 397it [04:03,  1.64it/s]Extractor Estimating: 398it [04:03,  1.58it/s]Extractor Estimating: 399it [04:04,  1.63it/s]Extractor Estimating: 400it [04:04,  1.64it/s]Extractor Estimating: 401it [04:05,  1.58it/s]Extractor Estimating: 402it [04:06,  1.59it/s]Extractor Estimating: 403it [04:06,  1.61it/s]Extractor Estimating: 404it [04:07,  1.63it/s]Extractor Estimating: 405it [04:08,  1.61it/s]Extractor Estimating: 406it [04:08,  1.62it/s]Extractor Estimating: 407it [04:09,  1.61it/s]Extractor Estimating: 408it [04:10,  1.59it/s]Extractor Estimating: 409it [04:10,  1.56it/s]Extractor Estimating: 410it [04:11,  1.55it/s]Extractor Estimating: 411it [04:11,  1.55it/s]Extractor Estimating: 412it [04:12,  1.59it/s]Extractor Estimating: 413it [04:13,  1.59it/s]Extractor Estimating: 414it [04:13,  1.56it/s]Extractor Estimating: 415it [04:14,  1.55it/s]Extractor Estimating: 416it [04:15,  1.52it/s]Extractor Estimating: 417it [04:15,  1.56it/s]Extractor Estimating: 418it [04:16,  1.55it/s]Extractor Estimating: 419it [04:17,  1.54it/s]Extractor Estimating: 420it [04:17,  1.41it/s]Extractor Estimating: 421it [04:18,  1.49it/s]Extractor Estimating: 422it [04:19,  1.50it/s]Extractor Estimating: 423it [04:19,  1.56it/s]Extractor Estimating: 424it [04:20,  1.57it/s]Extractor Estimating: 425it [04:20,  1.68it/s]Extractor Estimating: 426it [04:21,  1.72it/s]Extractor Estimating: 427it [04:22,  1.76it/s]Extractor Estimating: 428it [04:22,  1.88it/s]Extractor Estimating: 429it [04:22,  1.90it/s]Extractor Estimating: 430it [04:23,  1.88it/s]Extractor Estimating: 431it [04:24,  1.87it/s]Extractor Estimating: 432it [04:24,  1.88it/s]Extractor Estimating: 433it [04:25,  1.95it/s]Extractor Estimating: 434it [04:25,  1.93it/s]Extractor Estimating: 435it [04:26,  1.98it/s]Extractor Estimating: 436it [04:26,  1.95it/s]Extractor Estimating: 437it [04:27,  1.96it/s]Extractor Estimating: 438it [04:27,  1.98it/s]Extractor Estimating: 439it [04:28,  1.99it/s]Extractor Estimating: 440it [04:28,  1.96it/s]Extractor Estimating: 441it [04:29,  1.94it/s]Extractor Estimating: 442it [04:29,  1.92it/s]Extractor Estimating: 443it [04:30,  1.95it/s]Extractor Estimating: 444it [04:30,  2.03it/s]Extractor Estimating: 445it [04:31,  2.00it/s]Extractor Estimating: 446it [04:31,  2.05it/s]Extractor Estimating: 447it [04:32,  2.08it/s]Extractor Estimating: 448it [04:32,  2.02it/s]Extractor Estimating: 449it [04:33,  1.88it/s]Extractor Estimating: 450it [04:33,  1.83it/s]Extractor Estimating: 451it [04:34,  1.80it/s]Extractor Estimating: 452it [04:34,  1.76it/s]Extractor Estimating: 453it [04:35,  1.81it/s]Extractor Estimating: 454it [04:36,  1.78it/s]Extractor Estimating: 455it [04:36,  1.71it/s]Extractor Estimating: 456it [04:37,  1.65it/s]Extractor Estimating: 457it [04:37,  1.68it/s]Extractor Estimating: 458it [04:38,  1.69it/s]Extractor Estimating: 459it [04:39,  1.60it/s]Extractor Estimating: 460it [04:39,  1.59it/s]Extractor Estimating: 461it [04:40,  1.64it/s]Extractor Estimating: 462it [04:40,  1.68it/s]Extractor Estimating: 463it [04:41,  1.69it/s]Extractor Estimating: 464it [04:42,  1.71it/s]Extractor Estimating: 465it [04:42,  1.68it/s]Extractor Estimating: 466it [04:43,  1.60it/s]Extractor Estimating: 467it [04:43,  1.67it/s]Extractor Estimating: 468it [04:44,  1.67it/s]Extractor Estimating: 469it [04:45,  1.72it/s]Extractor Estimating: 470it [04:45,  1.62it/s]Extractor Estimating: 471it [04:46,  1.59it/s]Extractor Estimating: 472it [04:47,  1.58it/s]Extractor Estimating: 473it [04:47,  1.58it/s]Extractor Estimating: 474it [04:48,  1.67it/s]Extractor Estimating: 475it [04:48,  1.68it/s]Extractor Estimating: 476it [04:49,  1.71it/s]Extractor Estimating: 477it [04:49,  1.71it/s]Extractor Estimating: 478it [04:50,  1.71it/s]Extractor Estimating: 479it [04:51,  1.68it/s]Extractor Estimating: 480it [04:51,  1.69it/s]Extractor Estimating: 481it [04:52,  1.71it/s]Extractor Estimating: 482it [04:52,  1.69it/s]Extractor Estimating: 483it [04:53,  1.66it/s]Extractor Estimating: 484it [04:54,  1.64it/s]Extractor Estimating: 485it [04:54,  1.68it/s]Extractor Estimating: 486it [04:55,  1.68it/s]Extractor Estimating: 487it [04:55,  1.76it/s]Extractor Estimating: 488it [04:56,  1.74it/s]Extractor Estimating: 489it [04:56,  1.79it/s]Extractor Estimating: 490it [04:57,  1.75it/s]Extractor Estimating: 491it [04:58,  1.80it/s]Extractor Estimating: 492it [04:58,  1.77it/s]Extractor Estimating: 493it [04:59,  1.77it/s]Extractor Estimating: 494it [04:59,  1.81it/s]Extractor Estimating: 495it [05:00,  1.83it/s]Extractor Estimating: 496it [05:00,  1.75it/s]Extractor Estimating: 497it [05:01,  1.70it/s]Extractor Estimating: 498it [05:02,  1.71it/s]Extractor Estimating: 499it [05:02,  1.74it/s]Extractor Estimating: 500it [05:03,  1.98it/s]Extractor Estimating: 500it [05:03,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:01,197 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:01,232 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:01,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:01,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:01,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 15:00:02,141 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 15:00:02,142 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:00:02,773 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 15:00:03,895 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:00:03,895 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:07,049 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:07,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:07,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:07,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 15:00:07,077 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 15:00:07,911 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 15:00:07,912 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 15:00:08,528 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 15:00:08,783 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 15:00:08,783 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 18:03:12,337 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 18:03:12,533 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 9977 mean pseudo reward: 0.949971306552467
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl'}
train vocab size: 22026
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22126, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22126, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.024, loss:626.0241
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.031, loss:626.0357
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.040, loss:595.6523
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.053, loss:614.7574
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.046, loss:593.4303
>> valid entity prec:0.5695, rec:0.4877, f1:0.5255
>> valid relation prec:0.1240, rec:0.0077, f1:0.0145
>> valid relation with NER prec:0.1240, rec:0.0077, f1:0.0145
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.507, loss:611.0077
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.039, loss:608.3622
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.041, loss:581.1557
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.042, loss:572.9479
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.035, loss:592.9267
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5960, rec:0.4103, f1:0.4860
>> valid relation prec:0.1014, rec:0.0070, f1:0.0130
>> valid relation with NER prec:0.1014, rec:0.0070, f1:0.0130
g_step 1100, step 268, avg_time 2.464, loss:587.0127
g_step 1200, step 368, avg_time 1.052, loss:629.3386
g_step 1300, step 52, avg_time 1.041, loss:600.6699
g_step 1400, step 152, avg_time 1.041, loss:567.7808
g_step 1500, step 252, avg_time 1.036, loss:556.4953
>> valid entity prec:0.5649, rec:0.5044, f1:0.5329
>> valid relation prec:0.1257, rec:0.0156, f1:0.0278
>> valid relation with NER prec:0.1257, rec:0.0156, f1:0.0278
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.497, loss:576.2902
g_step 1700, step 36, avg_time 1.041, loss:589.3045
g_step 1800, step 136, avg_time 1.041, loss:530.4649
g_step 1900, step 236, avg_time 1.043, loss:552.9390
g_step 2000, step 336, avg_time 1.032, loss:568.1024
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5567, rec:0.5169, f1:0.5360
>> valid relation prec:0.1635, rec:0.0166, f1:0.0301
>> valid relation with NER prec:0.1635, rec:0.0166, f1:0.0301
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.506, loss:555.9080
g_step 2200, step 120, avg_time 1.052, loss:516.6783
g_step 2300, step 220, avg_time 1.039, loss:534.8939
g_step 2400, step 320, avg_time 1.041, loss:551.8664
g_step 2500, step 4, avg_time 1.046, loss:532.1511
>> valid entity prec:0.5742, rec:0.4596, f1:0.5106
>> valid relation prec:0.1406, rec:0.0130, f1:0.0238
>> valid relation with NER prec:0.1406, rec:0.0130, f1:0.0238
g_step 2600, step 104, avg_time 2.477, loss:483.2965
g_step 2700, step 204, avg_time 1.044, loss:521.0279
g_step 2800, step 304, avg_time 1.047, loss:515.8875
g_step 2900, step 404, avg_time 1.041, loss:522.3128
g_step 3000, step 88, avg_time 1.044, loss:474.2939
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5710, rec:0.4482, f1:0.5022
>> valid relation prec:0.1594, rec:0.0221, f1:0.0389
>> valid relation with NER prec:0.1594, rec:0.0221, f1:0.0389
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3100, step 188, avg_time 2.483, loss:503.9981
g_step 3200, step 288, avg_time 1.035, loss:494.6770
g_step 3300, step 388, avg_time 1.121, loss:506.2774
g_step 3400, step 72, avg_time 1.034, loss:474.4346
g_step 3500, step 172, avg_time 1.039, loss:466.8414
>> valid entity prec:0.5804, rec:0.4529, f1:0.5088
>> valid relation prec:0.1215, rec:0.0156, f1:0.0277
>> valid relation with NER prec:0.1215, rec:0.0156, f1:0.0277
g_step 3600, step 272, avg_time 2.470, loss:492.3141
g_step 3700, step 372, avg_time 1.043, loss:468.5271
g_step 3800, step 56, avg_time 1.047, loss:466.1848
g_step 3900, step 156, avg_time 1.040, loss:429.7749
g_step 4000, step 256, avg_time 1.042, loss:466.2105
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5524, rec:0.4542, f1:0.4985
>> valid relation prec:0.1746, rec:0.0202, f1:0.0362
>> valid relation with NER prec:0.1746, rec:0.0202, f1:0.0362
g_step 4100, step 356, avg_time 2.540, loss:468.4833
g_step 4200, step 40, avg_time 1.038, loss:446.2329
g_step 4300, step 140, avg_time 1.029, loss:420.9328
g_step 4400, step 240, avg_time 1.245, loss:458.7564
g_step 4500, step 340, avg_time 1.027, loss:457.9371
>> valid entity prec:0.5597, rec:0.4570, f1:0.5032
>> valid relation prec:0.1444, rec:0.0166, f1:0.0298
>> valid relation with NER prec:0.1444, rec:0.0166, f1:0.0298
g_step 4600, step 24, avg_time 2.483, loss:451.3557
g_step 4700, step 124, avg_time 1.031, loss:416.1609
g_step 4800, step 224, avg_time 1.036, loss:417.3033
g_step 4900, step 324, avg_time 1.029, loss:443.9255
g_step 5000, step 8, avg_time 1.023, loss:438.7990
learning rate was adjusted to 0.0008
>> valid entity prec:0.5693, rec:0.4483, f1:0.5016
>> valid relation prec:0.1218, rec:0.0149, f1:0.0266
>> valid relation with NER prec:0.1218, rec:0.0149, f1:0.0266
g_step 5100, step 108, avg_time 2.449, loss:408.9860
g_step 5200, step 208, avg_time 1.033, loss:406.1915
g_step 5300, step 308, avg_time 1.038, loss:422.7993
g_step 5400, step 408, avg_time 1.034, loss:415.7097
g_step 5500, step 92, avg_time 1.023, loss:381.6168
>> valid entity prec:0.5455, rec:0.4918, f1:0.5173
>> valid relation prec:0.1157, rec:0.0284, f1:0.0456
>> valid relation with NER prec:0.1157, rec:0.0284, f1:0.0456
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 192, avg_time 2.620, loss:395.8373
g_step 5700, step 292, avg_time 1.037, loss:430.3780
g_step 5800, step 392, avg_time 1.035, loss:416.6205
g_step 5900, step 76, avg_time 1.035, loss:375.5143
g_step 6000, step 176, avg_time 1.025, loss:386.0801
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5407, rec:0.4979, f1:0.5184
>> valid relation prec:0.0837, rec:0.0127, f1:0.0221
>> valid relation with NER prec:0.0837, rec:0.0127, f1:0.0221
g_step 6100, step 276, avg_time 2.466, loss:396.6210
g_step 6200, step 376, avg_time 1.026, loss:401.4578
g_step 6300, step 60, avg_time 1.035, loss:382.1882
g_step 6400, step 160, avg_time 1.031, loss:362.5514
g_step 6500, step 260, avg_time 1.031, loss:383.6782
>> valid entity prec:0.5434, rec:0.4842, f1:0.5121
>> valid relation prec:0.1284, rec:0.0298, f1:0.0484
>> valid relation with NER prec:0.1284, rec:0.0298, f1:0.0484
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 6600, step 360, avg_time 2.450, loss:384.2487
g_step 6700, step 44, avg_time 1.038, loss:380.5315
g_step 6800, step 144, avg_time 1.029, loss:369.3644
g_step 6900, step 244, avg_time 1.033, loss:363.7262
g_step 7000, step 344, avg_time 1.038, loss:378.0981
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5572, rec:0.4080, f1:0.4711
>> valid relation prec:0.1381, rec:0.0190, f1:0.0334
>> valid relation with NER prec:0.1381, rec:0.0190, f1:0.0334
g_step 7100, step 28, avg_time 2.468, loss:353.3834
g_step 7200, step 128, avg_time 1.035, loss:333.0009
g_step 7300, step 228, avg_time 1.043, loss:356.0341
g_step 7400, step 328, avg_time 1.037, loss:366.6544
g_step 7500, step 12, avg_time 1.030, loss:364.7041
>> valid entity prec:0.5497, rec:0.4749, f1:0.5096
>> valid relation prec:0.1179, rec:0.0226, f1:0.0379
>> valid relation with NER prec:0.1179, rec:0.0226, f1:0.0379
g_step 7600, step 112, avg_time 2.448, loss:315.9428
g_step 7700, step 212, avg_time 1.038, loss:337.9899
g_step 7800, step 312, avg_time 1.024, loss:346.4214
g_step 7900, step 412, avg_time 1.040, loss:353.2320
g_step 8000, step 96, avg_time 1.026, loss:319.6228
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5591, rec:0.4482, f1:0.4975
>> valid relation prec:0.1137, rec:0.0224, f1:0.0374
>> valid relation with NER prec:0.1137, rec:0.0224, f1:0.0374
g_step 8100, step 196, avg_time 2.446, loss:317.6613
g_step 8200, step 296, avg_time 1.048, loss:333.6802
g_step 8300, step 396, avg_time 1.037, loss:321.3097
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 18:03:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 18:03:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_18-03-12_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 18:03:13 - WARNING - datasets.builder -   Using custom data configuration default-6db58572ec935d4c
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-6db58572ec935d4c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:01,  1.68s/ tables]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 18:03:18,832 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 18:03:18,833 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 18:03:18,833 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 18:03:18,834 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 18:03:18,986 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:03:19,049 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:03:19,049 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:03:19,049 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:03:19,049 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:03:19,049 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:03:19,049 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 18:03:19,947 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 18:03:23,383 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 18:03:23,425 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-6db58572ec935d4c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:05,  1.54ba/s] 20%|██        | 2/10 [00:00<00:03,  2.60ba/s] 30%|███       | 3/10 [00:01<00:02,  3.34ba/s] 40%|████      | 4/10 [00:01<00:01,  3.86ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.21ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.44ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.58ba/s] 80%|████████  | 8/10 [00:02<00:00,  4.71ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.79ba/s]100%|██████████| 10/10 [00:02<00:00,  4.83ba/s]100%|██████████| 10/10 [00:02<00:00,  4.08ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.79ba/s] 40%|████      | 2/5 [00:00<00:00,  3.63ba/s] 60%|██████    | 3/5 [00:00<00:00,  3.18ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.62ba/s]100%|██████████| 5/5 [00:01<00:00,  4.19ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:03,  2.49ba/s] 30%|███       | 3/10 [00:00<00:01,  5.84ba/s] 50%|█████     | 5/10 [00:00<00:00,  7.64ba/s] 70%|███████   | 7/10 [00:00<00:00,  8.72ba/s] 90%|█████████ | 9/10 [00:01<00:00,  9.48ba/s]100%|██████████| 10/10 [00:01<00:00,  8.19ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.10ba/s] 60%|██████    | 3/5 [00:00<00:00,  6.67ba/s]100%|██████████| 5/5 [00:00<00:00,  9.90ba/s]100%|██████████| 5/5 [00:00<00:00,  8.18ba/s]
[INFO|trainer.py:414] 2023-08-29 18:03:31,133 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 18:03:31,259 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 18:03:31,259 >>   Num examples = 9999
[INFO|trainer.py:1149] 2023-08-29 18:03:31,259 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 18:03:31,259 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 18:03:31,259 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 18:03:31,259 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 18:03:31,259 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<07:48,  1.66it/s]  0%|          | 2/780 [00:00<05:23,  2.40it/s]  0%|          | 3/780 [00:01<04:59,  2.60it/s]  1%|          | 4/780 [00:01<04:29,  2.87it/s]  1%|          | 5/780 [00:01<04:12,  3.07it/s]  1%|          | 6/780 [00:02<04:01,  3.21it/s]  1%|          | 7/780 [00:02<03:53,  3.31it/s]  1%|          | 8/780 [00:02<03:49,  3.37it/s]  1%|          | 9/780 [00:02<03:45,  3.42it/s]  1%|▏         | 10/780 [00:03<03:43,  3.45it/s]  1%|▏         | 11/780 [00:03<03:41,  3.47it/s]  2%|▏         | 12/780 [00:03<03:40,  3.49it/s]  2%|▏         | 13/780 [00:04<03:39,  3.50it/s]  2%|▏         | 14/780 [00:04<03:38,  3.51it/s]  2%|▏         | 15/780 [00:04<03:56,  3.23it/s]  2%|▏         | 16/780 [00:05<03:50,  3.31it/s]  2%|▏         | 17/780 [00:05<03:46,  3.37it/s]  2%|▏         | 18/780 [00:05<03:43,  3.42it/s]  2%|▏         | 19/780 [00:05<03:40,  3.45it/s]  3%|▎         | 20/780 [00:06<03:38,  3.47it/s]  3%|▎         | 21/780 [00:06<03:37,  3.49it/s]  3%|▎         | 22/780 [00:06<03:36,  3.50it/s]  3%|▎         | 23/780 [00:07<03:35,  3.50it/s]  3%|▎         | 24/780 [00:07<03:35,  3.51it/s]  3%|▎         | 25/780 [00:07<03:35,  3.51it/s]  3%|▎         | 26/780 [00:07<03:34,  3.51it/s]  3%|▎         | 27/780 [00:08<03:34,  3.51it/s]  4%|▎         | 28/780 [00:08<03:34,  3.51it/s]  4%|▎         | 29/780 [00:08<03:33,  3.51it/s]  4%|▍         | 30/780 [00:08<03:33,  3.51it/s]  4%|▍         | 31/780 [00:09<03:33,  3.51it/s]  4%|▍         | 32/780 [00:09<03:32,  3.51it/s]  4%|▍         | 33/780 [00:09<03:32,  3.51it/s]  4%|▍         | 34/780 [00:10<03:32,  3.51it/s]  4%|▍         | 35/780 [00:10<03:32,  3.51it/s]  5%|▍         | 36/780 [00:10<03:31,  3.51it/s]  5%|▍         | 37/780 [00:10<03:31,  3.52it/s]  5%|▍         | 38/780 [00:11<03:31,  3.52it/s]  5%|▌         | 39/780 [00:11<03:30,  3.52it/s]  5%|▌         | 40/780 [00:11<03:30,  3.51it/s]  5%|▌         | 41/780 [00:12<03:30,  3.52it/s]  5%|▌         | 42/780 [00:12<03:29,  3.51it/s]  6%|▌         | 43/780 [00:12<03:29,  3.52it/s]  6%|▌         | 44/780 [00:12<03:29,  3.51it/s]  6%|▌         | 45/780 [00:13<03:29,  3.51it/s]  6%|▌         | 46/780 [00:13<03:29,  3.51it/s]  6%|▌         | 47/780 [00:13<03:34,  3.41it/s]  6%|▌         | 48/780 [00:14<03:32,  3.44it/s]  6%|▋         | 49/780 [00:14<03:31,  3.46it/s]  6%|▋         | 50/780 [00:14<03:30,  3.47it/s]  7%|▋         | 51/780 [00:15<03:29,  3.48it/s]  7%|▋         | 52/780 [00:15<03:28,  3.49it/s]  7%|▋         | 53/780 [00:15<03:27,  3.50it/s]  7%|▋         | 54/780 [00:15<03:27,  3.50it/s]  7%|▋         | 55/780 [00:16<03:27,  3.50it/s]  7%|▋         | 56/780 [00:16<03:26,  3.50it/s]  7%|▋         | 57/780 [00:16<03:26,  3.50it/s]  7%|▋         | 58/780 [00:16<03:26,  3.50it/s]  8%|▊         | 59/780 [00:17<03:25,  3.50it/s]  8%|▊         | 60/780 [00:17<03:25,  3.50it/s]  8%|▊         | 61/780 [00:17<03:25,  3.50it/s]  8%|▊         | 62/780 [00:18<03:24,  3.50it/s]  8%|▊         | 63/780 [00:18<03:24,  3.50it/s]  8%|▊         | 64/780 [00:18<03:24,  3.50it/s]  8%|▊         | 65/780 [00:18<03:24,  3.50it/s]  8%|▊         | 66/780 [00:19<03:23,  3.50it/s]  9%|▊         | 67/780 [00:19<03:23,  3.50it/s]  9%|▊         | 68/780 [00:19<03:23,  3.50it/s]  9%|▉         | 69/780 [00:20<03:23,  3.50it/s]  9%|▉         | 70/780 [00:20<03:22,  3.50it/s]  9%|▉         | 71/780 [00:20<03:22,  3.50it/s]  9%|▉         | 72/780 [00:20<03:22,  3.50it/s]  9%|▉         | 73/780 [00:21<03:21,  3.50it/s]  9%|▉         | 74/780 [00:21<03:21,  3.50it/s] 10%|▉         | 75/780 [00:21<03:21,  3.50it/s] 10%|▉         | 76/780 [00:22<03:20,  3.50it/s] 10%|▉         | 77/780 [00:22<03:20,  3.50it/s] 10%|█         | 78/780 [00:22<03:20,  3.50it/s] 10%|█         | 79/780 [00:22<03:20,  3.50it/s] 10%|█         | 80/780 [00:23<03:19,  3.50it/s] 10%|█         | 81/780 [00:23<03:19,  3.50it/s] 11%|█         | 82/780 [00:23<03:19,  3.50it/s] 11%|█         | 83/780 [00:24<03:19,  3.50it/s] 11%|█         | 84/780 [00:24<03:18,  3.50it/s] 11%|█         | 85/780 [00:24<03:18,  3.50it/s] 11%|█         | 86/780 [00:24<03:18,  3.50it/s] 11%|█         | 87/780 [00:25<03:18,  3.50it/s] 11%|█▏        | 88/780 [00:25<03:17,  3.50it/s] 11%|█▏        | 89/780 [00:25<03:17,  3.50it/s] 12%|█▏        | 90/780 [00:26<03:17,  3.50it/s] 12%|█▏        | 91/780 [00:26<03:16,  3.50it/s] 12%|█▏        | 92/780 [00:26<03:16,  3.50it/s] 12%|█▏        | 93/780 [00:26<03:16,  3.50it/s] 12%|█▏        | 94/780 [00:27<03:16,  3.50it/s] 12%|█▏        | 95/780 [00:27<03:15,  3.50it/s] 12%|█▏        | 96/780 [00:27<03:15,  3.50it/s] 12%|█▏        | 97/780 [00:28<03:15,  3.50it/s] 13%|█▎        | 98/780 [00:28<03:15,  3.50it/s] 13%|█▎        | 99/780 [00:28<03:14,  3.50it/s] 13%|█▎        | 100/780 [00:28<03:14,  3.50it/s] 13%|█▎        | 101/780 [00:29<03:14,  3.50it/s] 13%|█▎        | 102/780 [00:29<03:13,  3.50it/s] 13%|█▎        | 103/780 [00:29<03:13,  3.50it/s] 13%|█▎        | 104/780 [00:30<03:13,  3.50it/s] 13%|█▎        | 105/780 [00:30<03:12,  3.50it/s] 14%|█▎        | 106/780 [00:30<03:12,  3.50it/s] 14%|█▎        | 107/780 [00:30<03:12,  3.50it/s] 14%|█▍        | 108/780 [00:31<03:12,  3.50it/s] 14%|█▍        | 109/780 [00:31<03:11,  3.50it/s] 14%|█▍        | 110/780 [00:31<03:11,  3.50it/s] 14%|█▍        | 111/780 [00:32<03:11,  3.50it/s] 14%|█▍        | 112/780 [00:32<03:11,  3.49it/s] 14%|█▍        | 113/780 [00:32<03:11,  3.49it/s] 15%|█▍        | 114/780 [00:33<03:10,  3.49it/s] 15%|█▍        | 115/780 [00:33<03:10,  3.49it/s] 15%|█▍        | 116/780 [00:33<03:10,  3.49it/s] 15%|█▌        | 117/780 [00:33<03:09,  3.49it/s] 15%|█▌        | 118/780 [00:34<03:09,  3.49it/s] 15%|█▌        | 119/780 [00:34<03:09,  3.49it/s] 15%|█▌        | 120/780 [00:34<03:08,  3.49it/s] 16%|█▌        | 121/780 [00:35<03:08,  3.49it/s] 16%|█▌        | 122/780 [00:35<03:08,  3.49it/s] 16%|█▌        | 123/780 [00:35<03:08,  3.49it/s] 16%|█▌        | 124/780 [00:35<03:07,  3.50it/s] 16%|█▌        | 125/780 [00:36<03:07,  3.49it/s] 16%|█▌        | 126/780 [00:36<03:07,  3.49it/s] 16%|█▋        | 127/780 [00:36<03:13,  3.37it/s] 16%|█▋        | 128/780 [00:37<03:23,  3.20it/s] 17%|█▋        | 129/780 [00:37<03:18,  3.28it/s] 17%|█▋        | 130/780 [00:37<03:14,  3.34it/s] 17%|█▋        | 131/780 [00:38<03:23,  3.19it/s] 17%|█▋        | 132/780 [00:38<03:17,  3.28it/s] 17%|█▋        | 133/780 [00:38<03:13,  3.34it/s] 17%|█▋        | 134/780 [00:38<03:16,  3.29it/s] 17%|█▋        | 135/780 [00:39<03:13,  3.34it/s] 17%|█▋        | 136/780 [00:39<03:10,  3.39it/s] 18%|█▊        | 137/780 [00:39<03:08,  3.42it/s] 18%|█▊        | 138/780 [00:40<03:06,  3.44it/s] 18%|█▊        | 139/780 [00:40<03:05,  3.46it/s] 18%|█▊        | 140/780 [00:40<03:04,  3.47it/s] 18%|█▊        | 141/780 [00:40<03:03,  3.48it/s] 18%|█▊        | 142/780 [00:41<03:48,  2.79it/s] 18%|█▊        | 143/780 [00:41<03:34,  2.97it/s] 18%|█▊        | 144/780 [00:42<03:24,  3.11it/s] 19%|█▊        | 145/780 [00:42<03:17,  3.21it/s] 19%|█▊        | 146/780 [00:42<03:12,  3.29it/s] 19%|█▉        | 147/780 [00:42<03:08,  3.35it/s] 19%|█▉        | 148/780 [00:43<03:06,  3.39it/s] 19%|█▉        | 149/780 [00:43<03:04,  3.42it/s] 19%|█▉        | 150/780 [00:43<03:02,  3.45it/s] 19%|█▉        | 151/780 [00:44<03:01,  3.46it/s] 19%|█▉        | 152/780 [00:44<03:05,  3.39it/s] 20%|█▉        | 153/780 [00:44<03:03,  3.42it/s] 20%|█▉        | 154/780 [00:44<03:01,  3.44it/s] 20%|█▉        | 155/780 [00:45<03:00,  3.46it/s] 20%|██        | 156/780 [00:45<02:59,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 18:04:16,775 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 18:04:16,776 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 18:04:16,776 >>   Batch size = 8

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|▏         | 7/521 [00:00<00:08, 58.49it/s][A
  2%|▏         | 13/521 [00:00<00:09, 51.70it/s][A
  4%|▎         | 19/521 [00:00<00:10, 49.62it/s][A
  5%|▍         | 24/521 [00:00<00:10, 48.74it/s][A
  6%|▌         | 29/521 [00:00<00:10, 48.24it/s][A
  7%|▋         | 34/521 [00:00<00:10, 47.94it/s][A
  7%|▋         | 39/521 [00:00<00:10, 47.63it/s][A
  8%|▊         | 44/521 [00:00<00:10, 47.38it/s][A
  9%|▉         | 49/521 [00:01<00:10, 47.18it/s][A
 10%|█         | 54/521 [00:01<00:09, 47.19it/s][A
 11%|█▏        | 59/521 [00:01<00:09, 47.23it/s][A
 12%|█▏        | 64/521 [00:01<00:09, 47.25it/s][A
 13%|█▎        | 69/521 [00:01<00:09, 47.15it/s][A
 14%|█▍        | 74/521 [00:01<00:09, 47.18it/s][A
 15%|█▌        | 79/521 [00:01<00:09, 44.42it/s][A
 16%|█▌        | 84/521 [00:01<00:09, 45.19it/s][A
 17%|█▋        | 89/521 [00:01<00:09, 45.85it/s][A
 18%|█▊        | 94/521 [00:01<00:09, 46.18it/s][A
 19%|█▉        | 99/521 [00:02<00:09, 46.43it/s][A
 20%|█▉        | 104/521 [00:02<00:08, 46.70it/s][A
 21%|██        | 109/521 [00:02<00:08, 46.87it/s][A
 22%|██▏       | 114/521 [00:02<00:08, 46.92it/s][A
 23%|██▎       | 119/521 [00:02<00:08, 46.72it/s][A
 24%|██▍       | 124/521 [00:02<00:08, 46.72it/s][A
 25%|██▍       | 129/521 [00:02<00:08, 46.87it/s][A
 26%|██▌       | 134/521 [00:02<00:08, 46.97it/s][A
 27%|██▋       | 139/521 [00:02<00:08, 47.07it/s][A
 28%|██▊       | 144/521 [00:03<00:08, 47.07it/s][A
 29%|██▊       | 149/521 [00:03<00:07, 47.14it/s][A
 30%|██▉       | 154/521 [00:03<00:07, 47.18it/s][A
 31%|███       | 159/521 [00:03<00:07, 47.20it/s][A
 31%|███▏      | 164/521 [00:03<00:07, 47.10it/s][A
 32%|███▏      | 169/521 [00:03<00:07, 47.04it/s][A
 33%|███▎      | 174/521 [00:03<00:07, 47.02it/s][A
 34%|███▍      | 179/521 [00:03<00:07, 47.03it/s][A
 35%|███▌      | 184/521 [00:03<00:07, 47.11it/s][A
 36%|███▋      | 189/521 [00:04<00:07, 47.13it/s][A
 37%|███▋      | 194/521 [00:04<00:06, 47.16it/s][A
 38%|███▊      | 199/521 [00:04<00:06, 47.14it/s][A
 39%|███▉      | 204/521 [00:04<00:06, 47.20it/s][A
 40%|████      | 209/521 [00:04<00:06, 47.14it/s][A
 41%|████      | 214/521 [00:04<00:06, 47.02it/s][A
 42%|████▏     | 219/521 [00:04<00:06, 47.03it/s][A
 43%|████▎     | 224/521 [00:04<00:06, 42.71it/s][A
 44%|████▍     | 229/521 [00:04<00:06, 43.95it/s][A
 45%|████▍     | 234/521 [00:04<00:06, 44.90it/s][A
 46%|████▌     | 239/521 [00:05<00:06, 45.57it/s][A
 47%|████▋     | 244/521 [00:05<00:06, 46.07it/s][A
 48%|████▊     | 249/521 [00:05<00:05, 46.39it/s][A
 49%|████▉     | 254/521 [00:05<00:05, 46.67it/s][A
 50%|████▉     | 259/521 [00:05<00:05, 46.82it/s][A
 51%|█████     | 264/521 [00:05<00:05, 46.68it/s][A
 52%|█████▏    | 269/521 [00:05<00:05, 46.77it/s][A
 53%|█████▎    | 274/521 [00:05<00:05, 46.90it/s][A
 54%|█████▎    | 279/521 [00:05<00:05, 46.93it/s][A
 55%|█████▍    | 284/521 [00:06<00:05, 46.99it/s][A
 55%|█████▌    | 289/521 [00:06<00:04, 47.07it/s][A
 56%|█████▋    | 294/521 [00:06<00:04, 47.10it/s][A
 57%|█████▋    | 299/521 [00:06<00:04, 47.14it/s][A
 58%|█████▊    | 304/521 [00:06<00:04, 47.19it/s][A
 59%|█████▉    | 309/521 [00:06<00:04, 47.06it/s][A
 60%|██████    | 314/521 [00:06<00:04, 46.99it/s][A
 61%|██████    | 319/521 [00:06<00:04, 47.07it/s][A
 62%|██████▏   | 324/521 [00:06<00:04, 47.08it/s][A
 63%|██████▎   | 329/521 [00:07<00:04, 47.05it/s][A
 64%|██████▍   | 334/521 [00:07<00:03, 47.11it/s][A
 65%|██████▌   | 339/521 [00:07<00:03, 46.89it/s][A
 66%|██████▌   | 344/521 [00:07<00:03, 46.97it/s][A
 67%|██████▋   | 349/521 [00:07<00:03, 47.07it/s][A
 68%|██████▊   | 354/521 [00:07<00:03, 47.05it/s][A
 69%|██████▉   | 359/521 [00:07<00:03, 47.01it/s][A
 70%|██████▉   | 364/521 [00:07<00:03, 42.67it/s][A
 71%|███████   | 369/521 [00:07<00:03, 43.97it/s][A
 72%|███████▏  | 374/521 [00:08<00:03, 44.90it/s][A
 73%|███████▎  | 379/521 [00:08<00:03, 45.55it/s][A
 74%|███████▎  | 384/521 [00:08<00:02, 46.06it/s][A
 75%|███████▍  | 389/521 [00:08<00:02, 46.41it/s][A
 76%|███████▌  | 394/521 [00:08<00:02, 46.64it/s][A
 77%|███████▋  | 399/521 [00:08<00:02, 46.80it/s][A
 78%|███████▊  | 404/521 [00:08<00:02, 46.65it/s][A
 79%|███████▊  | 409/521 [00:08<00:02, 46.67it/s][A
 79%|███████▉  | 414/521 [00:08<00:02, 46.83it/s][A
 80%|████████  | 419/521 [00:08<00:02, 46.94it/s][A
 81%|████████▏ | 424/521 [00:09<00:02, 47.01it/s][A
 82%|████████▏ | 429/521 [00:09<00:01, 47.04it/s][A
 83%|████████▎ | 434/521 [00:09<00:01, 47.11it/s][A
 84%|████████▍ | 439/521 [00:09<00:01, 47.14it/s][A
 85%|████████▌ | 444/521 [00:09<00:01, 47.12it/s][A
 86%|████████▌ | 449/521 [00:09<00:01, 47.03it/s][A
 87%|████████▋ | 454/521 [00:09<00:01, 46.96it/s][A
 88%|████████▊ | 459/521 [00:09<00:01, 46.96it/s][A
 89%|████████▉ | 464/521 [00:09<00:01, 47.03it/s][A
 90%|█████████ | 469/521 [00:10<00:01, 47.07it/s][A
 91%|█████████ | 474/521 [00:10<00:01, 46.75it/s][A
 92%|█████████▏| 479/521 [00:10<00:00, 46.93it/s][A
 93%|█████████▎| 484/521 [00:10<00:00, 47.01it/s][A
 94%|█████████▍| 489/521 [00:10<00:00, 47.01it/s][A
 95%|█████████▍| 494/521 [00:10<00:00, 46.99it/s][A
 96%|█████████▌| 499/521 [00:10<00:00, 46.91it/s][A
 97%|█████████▋| 504/521 [00:10<00:00, 44.04it/s][A
 98%|█████████▊| 509/521 [00:10<00:00, 44.96it/s][A
 99%|█████████▊| 514/521 [00:11<00:00, 45.63it/s][A
100%|█████████▉| 519/521 [00:11<00:00, 46.08it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 46.08it/s][A 20%|██        | 156/780 [00:56<02:59,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 18:04:28,331 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 18:04:28,567 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 18:04:34,876 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 18:04:35,128 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 18:04:35,244 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:17<1:43:13,  9.94s/it] 20%|██        | 158/780 [01:18<1:13:16,  7.07s/it] 20%|██        | 159/780 [01:18<52:05,  5.03s/it]   21%|██        | 160/780 [01:18<37:17,  3.61s/it] 21%|██        | 161/780 [01:19<26:56,  2.61s/it] 21%|██        | 162/780 [01:19<19:42,  1.91s/it] 21%|██        | 163/780 [01:19<14:39,  1.43s/it] 21%|██        | 164/780 [01:20<11:07,  1.08s/it] 21%|██        | 165/780 [01:20<08:38,  1.19it/s] 21%|██▏       | 166/780 [01:20<06:55,  1.48it/s] 21%|██▏       | 167/780 [01:20<05:42,  1.79it/s] 22%|██▏       | 168/780 [01:21<04:51,  2.10it/s] 22%|██▏       | 169/780 [01:21<04:34,  2.23it/s] 22%|██▏       | 170/780 [01:21<04:04,  2.50it/s] 22%|██▏       | 171/780 [01:22<03:42,  2.73it/s] 22%|██▏       | 172/780 [01:22<03:27,  2.93it/s] 22%|██▏       | 173/780 [01:22<03:17,  3.08it/s] 22%|██▏       | 174/780 [01:22<03:09,  3.20it/s] 22%|██▏       | 175/780 [01:23<03:04,  3.28it/s] 23%|██▎       | 176/780 [01:23<03:00,  3.35it/s] 23%|██▎       | 177/780 [01:23<02:57,  3.39it/s] 23%|██▎       | 178/780 [01:24<02:55,  3.43it/s] 23%|██▎       | 179/780 [01:24<02:54,  3.45it/s] 23%|██▎       | 180/780 [01:24<03:02,  3.29it/s] 23%|██▎       | 181/780 [01:25<02:58,  3.35it/s] 23%|██▎       | 182/780 [01:25<02:56,  3.40it/s] 23%|██▎       | 183/780 [01:25<02:54,  3.43it/s] 24%|██▎       | 184/780 [01:25<02:52,  3.45it/s] 24%|██▎       | 185/780 [01:26<02:51,  3.47it/s] 24%|██▍       | 186/780 [01:26<02:50,  3.48it/s] 24%|██▍       | 187/780 [01:26<02:50,  3.49it/s] 24%|██▍       | 188/780 [01:27<02:49,  3.49it/s] 24%|██▍       | 189/780 [01:27<02:49,  3.50it/s] 24%|██▍       | 190/780 [01:27<02:48,  3.50it/s] 24%|██▍       | 191/780 [01:27<02:58,  3.29it/s] 25%|██▍       | 192/780 [01:28<02:55,  3.35it/s] 25%|██▍       | 193/780 [01:28<02:52,  3.40it/s] 25%|██▍       | 194/780 [01:28<02:50,  3.43it/s] 25%|██▌       | 195/780 [01:29<02:49,  3.45it/s] 25%|██▌       | 196/780 [01:29<02:48,  3.47it/s] 25%|██▌       | 197/780 [01:29<02:47,  3.48it/s] 25%|██▌       | 198/780 [01:29<02:46,  3.49it/s] 26%|██▌       | 199/780 [01:30<02:46,  3.49it/s] 26%|██▌       | 200/780 [01:30<02:45,  3.49it/s] 26%|██▌       | 201/780 [01:30<02:45,  3.50it/s] 26%|██▌       | 202/780 [01:31<02:54,  3.31it/s] 26%|██▌       | 203/780 [01:31<02:51,  3.36it/s] 26%|██▌       | 204/780 [01:31<02:49,  3.40it/s] 26%|██▋       | 205/780 [01:31<02:47,  3.43it/s] 26%|██▋       | 206/780 [01:32<02:46,  3.45it/s] 27%|██▋       | 207/780 [01:32<02:45,  3.47it/s] 27%|██▋       | 208/780 [01:32<02:44,  3.48it/s] 27%|██▋       | 209/780 [01:33<02:43,  3.49it/s] 27%|██▋       | 210/780 [01:33<02:43,  3.49it/s] 27%|██▋       | 211/780 [01:33<02:42,  3.49it/s] 27%|██▋       | 212/780 [01:33<02:42,  3.50it/s] 27%|██▋       | 213/780 [01:34<02:52,  3.29it/s] 27%|██▋       | 214/780 [01:34<02:48,  3.35it/s] 28%|██▊       | 215/780 [01:34<02:46,  3.39it/s] 28%|██▊       | 216/780 [01:35<02:44,  3.43it/s] 28%|██▊       | 217/780 [01:35<02:43,  3.45it/s] 28%|██▊       | 218/780 [01:35<02:42,  3.46it/s] 28%|██▊       | 219/780 [01:36<02:41,  3.47it/s] 28%|██▊       | 220/780 [01:36<02:41,  3.48it/s] 28%|██▊       | 221/780 [01:36<02:40,  3.48it/s] 28%|██▊       | 222/780 [01:36<02:44,  3.39it/s] 29%|██▊       | 223/780 [01:37<02:59,  3.11it/s] 29%|██▊       | 224/780 [01:37<02:59,  3.10it/s] 29%|██▉       | 225/780 [01:37<02:52,  3.21it/s] 29%|██▉       | 226/780 [01:38<02:48,  3.29it/s] 29%|██▉       | 227/780 [01:38<02:44,  3.35it/s] 29%|██▉       | 228/780 [01:38<02:42,  3.40it/s] 29%|██▉       | 229/780 [01:39<02:49,  3.26it/s] 29%|██▉       | 230/780 [01:39<02:45,  3.32it/s] 30%|██▉       | 231/780 [01:39<02:42,  3.37it/s] 30%|██▉       | 232/780 [01:39<02:40,  3.41it/s] 30%|██▉       | 233/780 [01:40<02:39,  3.44it/s] 30%|███       | 234/780 [01:40<02:37,  3.46it/s] 30%|███       | 235/780 [01:40<02:43,  3.34it/s] 30%|███       | 236/780 [01:41<02:40,  3.38it/s] 30%|███       | 237/780 [01:41<03:12,  2.82it/s] 31%|███       | 238/780 [01:41<03:01,  2.99it/s] 31%|███       | 239/780 [01:42<02:53,  3.12it/s] 31%|███       | 240/780 [01:42<02:47,  3.23it/s] 31%|███       | 241/780 [01:42<02:43,  3.30it/s] 31%|███       | 242/780 [01:43<02:40,  3.36it/s] 31%|███       | 243/780 [01:43<02:38,  3.40it/s] 31%|███▏      | 244/780 [01:43<02:36,  3.43it/s] 31%|███▏      | 245/780 [01:44<02:46,  3.21it/s] 32%|███▏      | 246/780 [01:44<02:42,  3.28it/s] 32%|███▏      | 247/780 [01:44<02:39,  3.34it/s] 32%|███▏      | 248/780 [01:44<02:37,  3.39it/s] 32%|███▏      | 249/780 [01:45<02:35,  3.42it/s] 32%|███▏      | 250/780 [01:45<02:33,  3.44it/s] 32%|███▏      | 251/780 [01:45<02:32,  3.46it/s] 32%|███▏      | 252/780 [01:46<02:32,  3.47it/s] 32%|███▏      | 253/780 [01:46<02:31,  3.48it/s] 33%|███▎      | 254/780 [01:46<02:30,  3.48it/s] 33%|███▎      | 255/780 [01:46<02:30,  3.49it/s] 33%|███▎      | 256/780 [01:47<02:30,  3.49it/s] 33%|███▎      | 257/780 [01:47<02:29,  3.49it/s] 33%|███▎      | 258/780 [01:47<02:29,  3.49it/s] 33%|███▎      | 259/780 [01:48<02:29,  3.49it/s] 33%|███▎      | 260/780 [01:48<02:28,  3.49it/s] 33%|███▎      | 261/780 [01:48<02:28,  3.49it/s] 34%|███▎      | 262/780 [01:48<02:39,  3.25it/s] 34%|███▎      | 263/780 [01:49<02:35,  3.32it/s] 34%|███▍      | 264/780 [01:49<02:33,  3.37it/s] 34%|███▍      | 265/780 [01:49<02:31,  3.41it/s] 34%|███▍      | 266/780 [01:50<02:29,  3.43it/s] 34%|███▍      | 267/780 [01:50<02:28,  3.45it/s] 34%|███▍      | 268/780 [01:50<02:28,  3.46it/s] 34%|███▍      | 269/780 [01:50<02:27,  3.47it/s] 35%|███▍      | 270/780 [01:51<02:26,  3.47it/s] 35%|███▍      | 271/780 [01:51<02:26,  3.48it/s] 35%|███▍      | 272/780 [01:51<02:25,  3.48it/s] 35%|███▌      | 273/780 [01:52<02:36,  3.25it/s] 35%|███▌      | 274/780 [01:52<02:32,  3.32it/s] 35%|███▌      | 275/780 [01:52<02:29,  3.37it/s] 35%|███▌      | 276/780 [01:53<02:28,  3.41it/s] 36%|███▌      | 277/780 [01:53<02:26,  3.43it/s] 36%|███▌      | 278/780 [01:53<02:25,  3.45it/s] 36%|███▌      | 279/780 [01:53<02:24,  3.46it/s] 36%|███▌      | 280/780 [01:54<02:24,  3.47it/s] 36%|███▌      | 281/780 [01:54<02:23,  3.48it/s] 36%|███▌      | 282/780 [01:54<02:23,  3.48it/s] 36%|███▋      | 283/780 [01:55<02:22,  3.48it/s] 36%|███▋      | 284/780 [01:55<02:31,  3.28it/s] 37%|███▋      | 285/780 [01:55<02:28,  3.34it/s] 37%|███▋      | 286/780 [01:55<02:25,  3.38it/s] 37%|███▋      | 287/780 [01:56<02:24,  3.42it/s] 37%|███▋      | 288/780 [01:56<02:23,  3.44it/s] 37%|███▋      | 289/780 [01:56<02:22,  3.46it/s] 37%|███▋      | 290/780 [01:57<02:21,  3.47it/s] 37%|███▋      | 291/780 [01:57<02:20,  3.48it/s] 37%|███▋      | 292/780 [01:57<02:20,  3.48it/s] 38%|███▊      | 293/780 [01:57<02:19,  3.48it/s] 38%|███▊      | 294/780 [01:58<02:19,  3.49it/s] 38%|███▊      | 295/780 [01:58<02:27,  3.28it/s] 38%|███▊      | 296/780 [01:58<02:24,  3.34it/s] 38%|███▊      | 297/780 [01:59<02:22,  3.39it/s] 38%|███▊      | 298/780 [01:59<02:21,  3.42it/s] 38%|███▊      | 299/780 [01:59<02:19,  3.44it/s] 38%|███▊      | 300/780 [02:00<02:18,  3.45it/s] 39%|███▊      | 301/780 [02:00<02:18,  3.47it/s] 39%|███▊      | 302/780 [02:00<02:17,  3.47it/s] 39%|███▉      | 303/780 [02:00<02:17,  3.48it/s] 39%|███▉      | 304/780 [02:01<02:16,  3.49it/s] 39%|███▉      | 305/780 [02:01<02:35,  3.05it/s] 39%|███▉      | 306/780 [02:01<02:37,  3.00it/s] 39%|███▉      | 307/780 [02:02<02:30,  3.14it/s] 39%|███▉      | 308/780 [02:02<02:25,  3.24it/s] 40%|███▉      | 309/780 [02:02<02:22,  3.31it/s] 40%|███▉      | 310/780 [02:03<02:19,  3.36it/s] 40%|███▉      | 311/780 [02:03<02:17,  3.40it/s] 40%|████      | 312/780 [02:03<02:16,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 18:05:34,949 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 18:05:34,949 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 18:05:34,949 >>   Batch size = 8
{'eval_loss': 0.9803659319877625, 'eval_runtime': 11.2542, 'eval_samples_per_second': 370.084, 'eval_steps_per_second': 46.294, 'epoch': 1.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.94it/s][A
  2%|▏         | 12/521 [00:00<00:09, 51.05it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.19it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.51it/s][A
  5%|▌         | 28/521 [00:00<00:10, 48.09it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.72it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.48it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.09it/s][A
  9%|▉         | 48/521 [00:01<00:10, 44.33it/s][A
 10%|█         | 53/521 [00:01<00:10, 45.21it/s][A
 11%|█         | 58/521 [00:01<00:10, 45.78it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.21it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.52it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.75it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.84it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 47.01it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.84it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 46.82it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.91it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 47.02it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.85it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.94it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.98it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 47.05it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 46.99it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 46.97it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.87it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.90it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.96it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 47.02it/s][A
 30%|███       | 158/521 [00:03<00:07, 47.02it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 47.07it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 47.08it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 47.02it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 47.00it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.92it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 46.86it/s][A
 37%|███▋      | 193/521 [00:04<00:08, 36.88it/s][A
 38%|███▊      | 198/521 [00:04<00:08, 39.47it/s][A
 39%|███▉      | 203/521 [00:04<00:07, 41.50it/s][A
 40%|███▉      | 208/521 [00:04<00:07, 43.05it/s][A
 41%|████      | 213/521 [00:04<00:06, 44.24it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 45.10it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 45.70it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.16it/s][A
 45%|████▍     | 233/521 [00:05<00:06, 46.13it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.30it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.56it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.76it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.90it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.97it/s][A
 50%|█████     | 263/521 [00:05<00:05, 47.06it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 47.10it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 47.09it/s][A
 53%|█████▎    | 278/521 [00:06<00:05, 46.98it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.92it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.96it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 47.04it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 47.07it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 47.06it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 47.14it/s][A
 60%|██████    | 313/521 [00:06<00:04, 47.16it/s][A
 61%|██████    | 318/521 [00:06<00:04, 47.08it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 47.07it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 46.99it/s][A
 64%|██████▍   | 333/521 [00:07<00:05, 36.10it/s][A
 65%|██████▍   | 338/521 [00:07<00:04, 38.78it/s][A
 66%|██████▌   | 343/521 [00:07<00:04, 40.91it/s][A
 67%|██████▋   | 348/521 [00:07<00:04, 42.66it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 43.94it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 44.86it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 45.43it/s][A
 71%|███████   | 368/521 [00:08<00:03, 45.98it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 46.02it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.28it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.51it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.65it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.79it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.92it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 47.01it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 47.03it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 47.06it/s][A
 80%|████████  | 418/521 [00:09<00:02, 46.94it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.86it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.92it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.93it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.94it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 47.02it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 47.08it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 47.09it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 47.14it/s][A
 89%|████████▉ | 463/521 [00:10<00:01, 47.06it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 46.94it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 34.63it/s][A
 92%|█████████▏| 478/521 [00:10<00:01, 37.59it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 40.04it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 41.93it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 43.38it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 44.44it/s][A
 97%|█████████▋| 503/521 [00:11<00:00, 45.25it/s][A
 98%|█████████▊| 508/521 [00:11<00:00, 45.82it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 45.90it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.20it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 46.20it/s][A 40%|████      | 312/780 [02:15<02:16,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 18:05:47,206 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 18:05:47,792 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 18:05:54,522 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 18:05:54,750 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 18:05:54,881 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:37<1:20:48, 10.38s/it] 40%|████      | 314/780 [02:37<57:17,  7.38s/it]   40%|████      | 315/780 [02:38<40:40,  5.25s/it] 41%|████      | 316/780 [02:38<29:04,  3.76s/it] 41%|████      | 317/780 [02:38<20:58,  2.72s/it] 41%|████      | 318/780 [02:39<15:26,  2.00s/it] 41%|████      | 319/780 [02:39<11:26,  1.49s/it] 41%|████      | 320/780 [02:39<08:39,  1.13s/it] 41%|████      | 321/780 [02:39<06:41,  1.14it/s] 41%|████▏     | 322/780 [02:40<05:19,  1.43it/s] 41%|████▏     | 323/780 [02:40<04:22,  1.74it/s] 42%|████▏     | 324/780 [02:40<03:42,  2.05it/s] 42%|████▏     | 325/780 [02:41<03:19,  2.28it/s] 42%|████▏     | 326/780 [02:41<03:08,  2.41it/s] 42%|████▏     | 327/780 [02:41<03:07,  2.41it/s] 42%|████▏     | 328/780 [02:42<02:50,  2.65it/s] 42%|████▏     | 329/780 [02:42<02:37,  2.86it/s] 42%|████▏     | 330/780 [02:42<02:28,  3.03it/s] 42%|████▏     | 331/780 [02:43<02:22,  3.16it/s] 43%|████▎     | 332/780 [02:43<02:17,  3.25it/s] 43%|████▎     | 333/780 [02:43<02:14,  3.33it/s] 43%|████▎     | 334/780 [02:43<02:12,  3.38it/s] 43%|████▎     | 335/780 [02:44<02:17,  3.24it/s] 43%|████▎     | 336/780 [02:44<02:14,  3.31it/s] 43%|████▎     | 337/780 [02:44<02:11,  3.37it/s] 43%|████▎     | 338/780 [02:45<02:09,  3.41it/s] 43%|████▎     | 339/780 [02:45<02:08,  3.43it/s] 44%|████▎     | 340/780 [02:45<02:07,  3.45it/s] 44%|████▎     | 341/780 [02:45<02:06,  3.47it/s] 44%|████▍     | 342/780 [02:46<02:05,  3.48it/s] 44%|████▍     | 343/780 [02:46<02:05,  3.49it/s] 44%|████▍     | 344/780 [02:46<02:04,  3.49it/s] 44%|████▍     | 345/780 [02:47<02:04,  3.49it/s] 44%|████▍     | 346/780 [02:47<02:04,  3.50it/s] 44%|████▍     | 347/780 [02:47<02:03,  3.50it/s] 45%|████▍     | 348/780 [02:47<02:03,  3.50it/s] 45%|████▍     | 349/780 [02:48<02:03,  3.50it/s] 45%|████▍     | 350/780 [02:48<02:02,  3.50it/s] 45%|████▌     | 351/780 [02:48<02:02,  3.50it/s] 45%|████▌     | 352/780 [02:49<02:02,  3.50it/s] 45%|████▌     | 353/780 [02:49<02:01,  3.50it/s] 45%|████▌     | 354/780 [02:49<02:01,  3.50it/s] 46%|████▌     | 355/780 [02:49<02:01,  3.50it/s] 46%|████▌     | 356/780 [02:50<02:01,  3.50it/s] 46%|████▌     | 357/780 [02:50<02:06,  3.33it/s] 46%|████▌     | 358/780 [02:50<02:04,  3.38it/s] 46%|████▌     | 359/780 [02:51<02:03,  3.42it/s] 46%|████▌     | 360/780 [02:51<02:02,  3.44it/s] 46%|████▋     | 361/780 [02:51<02:01,  3.46it/s] 46%|████▋     | 362/780 [02:52<02:00,  3.47it/s] 47%|████▋     | 363/780 [02:52<01:59,  3.48it/s] 47%|████▋     | 364/780 [02:52<01:59,  3.49it/s] 47%|████▋     | 365/780 [02:52<01:58,  3.49it/s] 47%|████▋     | 366/780 [02:53<01:58,  3.49it/s] 47%|████▋     | 367/780 [02:53<01:58,  3.49it/s] 47%|████▋     | 368/780 [02:53<02:05,  3.29it/s] 47%|████▋     | 369/780 [02:54<02:02,  3.35it/s] 47%|████▋     | 370/780 [02:54<02:00,  3.39it/s] 48%|████▊     | 371/780 [02:54<01:59,  3.42it/s] 48%|████▊     | 372/780 [02:54<01:58,  3.45it/s] 48%|████▊     | 373/780 [02:55<01:57,  3.46it/s] 48%|████▊     | 374/780 [02:55<01:56,  3.47it/s] 48%|████▊     | 375/780 [02:55<01:56,  3.48it/s] 48%|████▊     | 376/780 [02:56<01:55,  3.49it/s] 48%|████▊     | 377/780 [02:56<01:55,  3.49it/s] 48%|████▊     | 378/780 [02:56<01:55,  3.49it/s] 49%|████▊     | 379/780 [02:57<01:59,  3.36it/s] 49%|████▊     | 380/780 [02:57<01:57,  3.40it/s] 49%|████▉     | 381/780 [02:57<01:56,  3.43it/s] 49%|████▉     | 382/780 [02:57<01:55,  3.45it/s] 49%|████▉     | 383/780 [02:58<01:54,  3.46it/s] 49%|████▉     | 384/780 [02:58<01:54,  3.47it/s] 49%|████▉     | 385/780 [02:58<01:53,  3.48it/s] 49%|████▉     | 386/780 [02:59<01:53,  3.48it/s] 50%|████▉     | 387/780 [02:59<01:52,  3.49it/s] 50%|████▉     | 388/780 [02:59<01:52,  3.49it/s] 50%|████▉     | 389/780 [02:59<01:51,  3.49it/s] 50%|█████     | 390/780 [03:00<01:57,  3.32it/s] 50%|█████     | 391/780 [03:00<01:55,  3.37it/s] 50%|█████     | 392/780 [03:00<01:53,  3.41it/s] 50%|█████     | 393/780 [03:01<01:52,  3.43it/s] 51%|█████     | 394/780 [03:01<01:51,  3.45it/s] 51%|█████     | 395/780 [03:01<02:00,  3.20it/s] 51%|█████     | 396/780 [03:01<01:57,  3.28it/s] 51%|█████     | 397/780 [03:02<01:54,  3.34it/s] 51%|█████     | 398/780 [03:02<01:52,  3.39it/s] 51%|█████     | 399/780 [03:02<01:51,  3.42it/s] 51%|█████▏    | 400/780 [03:03<01:50,  3.44it/s] 51%|█████▏    | 401/780 [03:03<01:51,  3.39it/s] 52%|█████▏    | 402/780 [03:03<01:50,  3.42it/s] 52%|█████▏    | 403/780 [03:04<01:49,  3.44it/s] 52%|█████▏    | 404/780 [03:04<01:48,  3.46it/s] 52%|█████▏    | 405/780 [03:04<01:48,  3.47it/s] 52%|█████▏    | 406/780 [03:04<01:47,  3.48it/s] 52%|█████▏    | 407/780 [03:05<01:47,  3.48it/s] 52%|█████▏    | 408/780 [03:05<01:46,  3.49it/s] 52%|█████▏    | 409/780 [03:05<01:46,  3.49it/s] 53%|█████▎    | 410/780 [03:06<01:45,  3.49it/s] 53%|█████▎    | 411/780 [03:06<01:45,  3.49it/s] 53%|█████▎    | 412/780 [03:06<01:50,  3.32it/s] 53%|█████▎    | 413/780 [03:06<01:49,  3.37it/s] 53%|█████▎    | 414/780 [03:07<01:47,  3.40it/s] 53%|█████▎    | 415/780 [03:07<01:46,  3.43it/s] 53%|█████▎    | 416/780 [03:07<01:45,  3.45it/s] 53%|█████▎    | 417/780 [03:08<01:44,  3.46it/s] 54%|█████▎    | 418/780 [03:08<01:44,  3.48it/s] 54%|█████▎    | 419/780 [03:08<01:43,  3.48it/s] 54%|█████▍    | 420/780 [03:08<01:43,  3.48it/s] 54%|█████▍    | 421/780 [03:09<01:42,  3.49it/s] 54%|█████▍    | 422/780 [03:09<01:42,  3.49it/s] 54%|█████▍    | 423/780 [03:09<01:49,  3.26it/s] 54%|█████▍    | 424/780 [03:10<01:47,  3.33it/s] 54%|█████▍    | 425/780 [03:10<01:45,  3.37it/s] 55%|█████▍    | 426/780 [03:10<01:43,  3.41it/s] 55%|█████▍    | 427/780 [03:11<01:42,  3.43it/s] 55%|█████▍    | 428/780 [03:11<01:41,  3.45it/s] 55%|█████▌    | 429/780 [03:11<01:41,  3.46it/s] 55%|█████▌    | 430/780 [03:11<01:40,  3.47it/s] 55%|█████▌    | 431/780 [03:12<01:40,  3.48it/s] 55%|█████▌    | 432/780 [03:12<01:39,  3.48it/s] 56%|█████▌    | 433/780 [03:12<01:39,  3.49it/s] 56%|█████▌    | 434/780 [03:13<01:46,  3.26it/s] 56%|█████▌    | 435/780 [03:13<01:43,  3.32it/s] 56%|█████▌    | 436/780 [03:13<01:41,  3.37it/s] 56%|█████▌    | 437/780 [03:13<01:40,  3.41it/s] 56%|█████▌    | 438/780 [03:14<01:39,  3.43it/s] 56%|█████▋    | 439/780 [03:14<01:38,  3.45it/s] 56%|█████▋    | 440/780 [03:14<01:38,  3.46it/s] 57%|█████▋    | 441/780 [03:15<01:37,  3.47it/s] 57%|█████▋    | 442/780 [03:15<01:37,  3.48it/s] 57%|█████▋    | 443/780 [03:15<01:36,  3.48it/s] 57%|█████▋    | 444/780 [03:15<01:36,  3.49it/s] 57%|█████▋    | 445/780 [03:16<01:44,  3.20it/s] 57%|█████▋    | 446/780 [03:16<01:41,  3.29it/s] 57%|█████▋    | 447/780 [03:16<01:39,  3.35it/s] 57%|█████▋    | 448/780 [03:17<01:37,  3.39it/s] 58%|█████▊    | 449/780 [03:17<01:36,  3.42it/s] 58%|█████▊    | 450/780 [03:17<01:35,  3.44it/s] 58%|█████▊    | 451/780 [03:18<01:35,  3.46it/s] 58%|█████▊    | 452/780 [03:18<01:34,  3.47it/s] 58%|█████▊    | 453/780 [03:18<01:34,  3.48it/s] 58%|█████▊    | 454/780 [03:18<01:33,  3.48it/s] 58%|█████▊    | 455/780 [03:19<01:33,  3.48it/s] 58%|█████▊    | 456/780 [03:19<01:32,  3.49it/s] 59%|█████▊    | 457/780 [03:19<01:32,  3.49it/s] 59%|█████▊    | 458/780 [03:20<01:32,  3.49it/s] 59%|█████▉    | 459/780 [03:20<01:31,  3.49it/s] 59%|█████▉    | 460/780 [03:20<01:31,  3.49it/s] 59%|█████▉    | 461/780 [03:20<01:31,  3.49it/s] 59%|█████▉    | 462/780 [03:21<01:32,  3.43it/s] 59%|█████▉    | 463/780 [03:21<01:32,  3.45it/s] 59%|█████▉    | 464/780 [03:21<01:31,  3.46it/s] 60%|█████▉    | 465/780 [03:22<01:30,  3.47it/s] 60%|█████▉    | 466/780 [03:22<01:30,  3.47it/s] 60%|█████▉    | 467/780 [03:22<01:29,  3.48it/s] 60%|██████    | 468/780 [03:22<01:29,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 18:06:54,215 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 18:06:54,215 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 18:06:54,215 >>   Batch size = 8
{'eval_loss': 0.9943628907203674, 'eval_runtime': 11.4479, 'eval_samples_per_second': 363.823, 'eval_steps_per_second': 45.511, 'epoch': 2.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.58it/s][A
  2%|▏         | 12/521 [00:00<00:10, 50.89it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.10it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.35it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.94it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.66it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.43it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.36it/s][A
  9%|▉         | 48/521 [00:01<00:10, 47.22it/s][A
 10%|█         | 53/521 [00:01<00:11, 41.07it/s][A
 11%|█         | 58/521 [00:01<00:10, 42.73it/s][A
 12%|█▏        | 63/521 [00:01<00:10, 44.00it/s][A
 13%|█▎        | 68/521 [00:01<00:10, 44.94it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 45.57it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.05it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.40it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.64it/s][A
 18%|█▊        | 93/521 [00:02<00:09, 46.56it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.72it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.80it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.85it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.93it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 47.00it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 47.01it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 47.04it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 47.08it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 46.96it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 46.92it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 46.92it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 46.95it/s][A
 30%|███       | 158/521 [00:03<00:07, 46.99it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 47.04it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 47.09it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 47.13it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 47.19it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 47.13it/s][A
 36%|███▌      | 188/521 [00:04<00:07, 47.06it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 43.63it/s][A
 38%|███▊      | 198/521 [00:04<00:07, 44.63it/s][A
 39%|███▉      | 203/521 [00:04<00:07, 45.41it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 45.92it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.30it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 46.57it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.77it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.90it/s][A
 45%|████▍     | 233/521 [00:05<00:06, 46.69it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.76it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.82it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.87it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.99it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 47.05it/s][A
 50%|█████     | 263/521 [00:05<00:05, 47.07it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 47.11it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 47.10it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 46.91it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.95it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.95it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.95it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 47.01it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 47.07it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 47.06it/s][A
 60%|██████    | 313/521 [00:06<00:04, 47.07it/s][A
 61%|██████    | 318/521 [00:06<00:04, 47.07it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 46.97it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 46.95it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 41.67it/s][A
 65%|██████▍   | 338/521 [00:07<00:04, 43.15it/s][A
 66%|██████▌   | 343/521 [00:07<00:04, 44.29it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 45.11it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 45.69it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.13it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.45it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.67it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 46.55it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.58it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.71it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.86it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.96it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.99it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 47.01it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 47.07it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 47.06it/s][A
 80%|████████  | 418/521 [00:08<00:02, 46.96it/s][A
 81%|████████  | 423/521 [00:09<00:02, 46.87it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 46.90it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 46.97it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 47.04it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 47.02it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 47.01it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 47.08it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 47.05it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 46.94it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 46.92it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 43.75it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 44.74it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 45.45it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 45.94it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.29it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.55it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.74it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.82it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 46.64it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.64it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 46.64it/s][A 60%|██████    | 468/780 [03:34<01:29,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 18:07:05,760 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 18:07:06,015 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 18:07:09,991 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 18:07:10,245 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 18:07:10,379 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:48<41:26,  7.99s/it] 60%|██████    | 470/780 [03:49<29:21,  5.68s/it] 60%|██████    | 471/780 [03:49<20:55,  4.06s/it] 61%|██████    | 472/780 [03:49<15:02,  2.93s/it] 61%|██████    | 473/780 [03:50<10:55,  2.14s/it] 61%|██████    | 474/780 [03:50<08:03,  1.58s/it] 61%|██████    | 475/780 [03:50<06:03,  1.19s/it] 61%|██████    | 476/780 [03:50<04:39,  1.09it/s] 61%|██████    | 477/780 [03:51<03:41,  1.37it/s] 61%|██████▏   | 478/780 [03:51<03:00,  1.68it/s] 61%|██████▏   | 479/780 [03:51<02:31,  1.99it/s] 62%|██████▏   | 480/780 [03:52<02:16,  2.20it/s] 62%|██████▏   | 481/780 [03:52<02:00,  2.47it/s] 62%|██████▏   | 482/780 [03:52<01:49,  2.71it/s] 62%|██████▏   | 483/780 [03:52<01:42,  2.91it/s] 62%|██████▏   | 484/780 [03:53<01:36,  3.06it/s] 62%|██████▏   | 485/780 [03:53<01:32,  3.18it/s] 62%|██████▏   | 486/780 [03:53<01:29,  3.27it/s] 62%|██████▏   | 487/780 [03:54<01:27,  3.34it/s] 63%|██████▎   | 488/780 [03:54<01:26,  3.39it/s] 63%|██████▎   | 489/780 [03:54<01:25,  3.42it/s] 63%|██████▎   | 490/780 [03:54<01:24,  3.44it/s] 63%|██████▎   | 491/780 [03:55<01:29,  3.25it/s] 63%|██████▎   | 492/780 [03:55<01:26,  3.32it/s] 63%|██████▎   | 493/780 [03:55<01:25,  3.37it/s] 63%|██████▎   | 494/780 [03:56<01:23,  3.41it/s] 63%|██████▎   | 495/780 [03:56<01:22,  3.44it/s] 64%|██████▎   | 496/780 [03:56<01:22,  3.45it/s] 64%|██████▎   | 497/780 [03:57<01:21,  3.46it/s] 64%|██████▍   | 498/780 [03:57<01:21,  3.47it/s] 64%|██████▍   | 499/780 [03:57<01:20,  3.48it/s] 64%|██████▍   | 500/780 [03:57<01:20,  3.49it/s]                                                  64%|██████▍   | 500/780 [03:57<01:20,  3.49it/s] 64%|██████▍   | 501/780 [03:58<01:19,  3.49it/s] 64%|██████▍   | 502/780 [03:58<01:24,  3.29it/s] 64%|██████▍   | 503/780 [03:58<01:22,  3.35it/s] 65%|██████▍   | 504/780 [03:59<01:21,  3.39it/s] 65%|██████▍   | 505/780 [03:59<01:20,  3.42it/s] 65%|██████▍   | 506/780 [03:59<01:19,  3.45it/s] 65%|██████▌   | 507/780 [03:59<01:18,  3.46it/s] 65%|██████▌   | 508/780 [04:00<01:18,  3.48it/s] 65%|██████▌   | 509/780 [04:00<01:17,  3.48it/s] 65%|██████▌   | 510/780 [04:00<01:17,  3.49it/s] 66%|██████▌   | 511/780 [04:01<01:17,  3.49it/s] 66%|██████▌   | 512/780 [04:01<01:16,  3.50it/s] 66%|██████▌   | 513/780 [04:01<01:23,  3.21it/s] 66%|██████▌   | 514/780 [04:02<01:20,  3.29it/s] 66%|██████▌   | 515/780 [04:02<01:19,  3.35it/s] 66%|██████▌   | 516/780 [04:02<01:17,  3.39it/s] 66%|██████▋   | 517/780 [04:02<01:16,  3.42it/s] 66%|██████▋   | 518/780 [04:03<01:16,  3.45it/s] 67%|██████▋   | 519/780 [04:03<01:15,  3.46it/s] 67%|██████▋   | 520/780 [04:03<01:14,  3.47it/s] 67%|██████▋   | 521/780 [04:04<01:14,  3.48it/s] 67%|██████▋   | 522/780 [04:04<01:13,  3.49it/s] 67%|██████▋   | 523/780 [04:04<01:13,  3.49it/s] 67%|██████▋   | 524/780 [04:04<01:16,  3.36it/s] 67%|██████▋   | 525/780 [04:05<01:14,  3.40it/s] 67%|██████▋   | 526/780 [04:05<01:14,  3.43it/s] 68%|██████▊   | 527/780 [04:05<01:13,  3.45it/s] 68%|██████▊   | 528/780 [04:06<01:12,  3.47it/s] 68%|██████▊   | 529/780 [04:06<01:12,  3.48it/s] 68%|██████▊   | 530/780 [04:06<01:11,  3.48it/s] 68%|██████▊   | 531/780 [04:06<01:11,  3.49it/s] 68%|██████▊   | 532/780 [04:07<01:11,  3.49it/s] 68%|██████▊   | 533/780 [04:07<01:10,  3.50it/s] 68%|██████▊   | 534/780 [04:07<01:10,  3.50it/s] 69%|██████▊   | 535/780 [04:08<01:15,  3.22it/s] 69%|██████▊   | 536/780 [04:08<01:13,  3.30it/s] 69%|██████▉   | 537/780 [04:08<01:12,  3.36it/s] 69%|██████▉   | 538/780 [04:08<01:11,  3.40it/s] 69%|██████▉   | 539/780 [04:09<01:10,  3.42it/s] 69%|██████▉   | 540/780 [04:09<01:09,  3.44it/s] 69%|██████▉   | 541/780 [04:09<01:09,  3.46it/s] 69%|██████▉   | 542/780 [04:10<01:08,  3.47it/s] 70%|██████▉   | 543/780 [04:10<01:08,  3.48it/s] 70%|██████▉   | 544/780 [04:10<01:07,  3.49it/s] 70%|██████▉   | 545/780 [04:10<01:07,  3.49it/s] 70%|███████   | 546/780 [04:11<01:14,  3.16it/s] 70%|███████   | 547/780 [04:11<01:11,  3.25it/s] 70%|███████   | 548/780 [04:11<01:09,  3.33it/s] 70%|███████   | 549/780 [04:12<01:08,  3.38it/s] 71%|███████   | 550/780 [04:12<01:07,  3.42it/s] 71%|███████   | 551/780 [04:12<01:06,  3.44it/s] 71%|███████   | 552/780 [04:13<01:05,  3.46it/s] 71%|███████   | 553/780 [04:13<01:05,  3.48it/s] 71%|███████   | 554/780 [04:13<01:04,  3.48it/s] 71%|███████   | 555/780 [04:13<01:04,  3.49it/s] 71%|███████▏  | 556/780 [04:14<01:04,  3.49it/s] 71%|███████▏  | 557/780 [04:14<01:10,  3.18it/s] 72%|███████▏  | 558/780 [04:14<01:07,  3.27it/s] 72%|███████▏  | 559/780 [04:15<01:06,  3.34it/s] 72%|███████▏  | 560/780 [04:15<01:04,  3.39it/s] 72%|███████▏  | 561/780 [04:15<01:04,  3.42it/s] 72%|███████▏  | 562/780 [04:16<01:03,  3.44it/s] 72%|███████▏  | 563/780 [04:16<01:02,  3.46it/s] 72%|███████▏  | 564/780 [04:16<01:02,  3.47it/s] 72%|███████▏  | 565/780 [04:16<01:01,  3.48it/s] 73%|███████▎  | 566/780 [04:17<01:01,  3.49it/s] 73%|███████▎  | 567/780 [04:17<01:01,  3.48it/s] 73%|███████▎  | 568/780 [04:17<01:05,  3.24it/s] 73%|███████▎  | 569/780 [04:18<01:03,  3.30it/s] 73%|███████▎  | 570/780 [04:18<01:02,  3.36it/s] 73%|███████▎  | 571/780 [04:18<01:01,  3.40it/s] 73%|███████▎  | 572/780 [04:18<01:00,  3.43it/s] 73%|███████▎  | 573/780 [04:19<00:59,  3.45it/s] 74%|███████▎  | 574/780 [04:19<01:03,  3.24it/s] 74%|███████▎  | 575/780 [04:19<01:01,  3.32it/s] 74%|███████▍  | 576/780 [04:20<01:00,  3.37it/s] 74%|███████▍  | 577/780 [04:20<00:59,  3.41it/s] 74%|███████▍  | 578/780 [04:20<00:58,  3.44it/s] 74%|███████▍  | 579/780 [04:21<00:58,  3.46it/s] 74%|███████▍  | 580/780 [04:21<00:57,  3.47it/s] 74%|███████▍  | 581/780 [04:21<00:57,  3.48it/s] 75%|███████▍  | 582/780 [04:21<00:56,  3.49it/s] 75%|███████▍  | 583/780 [04:22<00:56,  3.49it/s] 75%|███████▍  | 584/780 [04:22<00:56,  3.50it/s] 75%|███████▌  | 585/780 [04:22<01:00,  3.21it/s] 75%|███████▌  | 586/780 [04:23<00:58,  3.29it/s] 75%|███████▌  | 587/780 [04:23<00:57,  3.35it/s] 75%|███████▌  | 588/780 [04:23<00:56,  3.40it/s] 76%|███████▌  | 589/780 [04:23<00:55,  3.43it/s] 76%|███████▌  | 590/780 [04:24<00:55,  3.45it/s] 76%|███████▌  | 591/780 [04:24<00:54,  3.47it/s] 76%|███████▌  | 592/780 [04:24<00:54,  3.48it/s] 76%|███████▌  | 593/780 [04:25<00:53,  3.49it/s] 76%|███████▌  | 594/780 [04:25<00:53,  3.49it/s] 76%|███████▋  | 595/780 [04:25<00:52,  3.49it/s] 76%|███████▋  | 596/780 [04:26<00:58,  3.16it/s] 77%|███████▋  | 597/780 [04:26<00:56,  3.25it/s] 77%|███████▋  | 598/780 [04:26<00:54,  3.32it/s] 77%|███████▋  | 599/780 [04:26<00:53,  3.37it/s] 77%|███████▋  | 600/780 [04:27<00:52,  3.41it/s] 77%|███████▋  | 601/780 [04:27<00:52,  3.44it/s] 77%|███████▋  | 602/780 [04:27<00:51,  3.45it/s] 77%|███████▋  | 603/780 [04:28<00:51,  3.46it/s] 77%|███████▋  | 604/780 [04:28<00:50,  3.47it/s] 78%|███████▊  | 605/780 [04:28<00:50,  3.48it/s] 78%|███████▊  | 606/780 [04:28<00:49,  3.48it/s] 78%|███████▊  | 607/780 [04:29<00:52,  3.31it/s] 78%|███████▊  | 608/780 [04:29<00:51,  3.36it/s] 78%|███████▊  | 609/780 [04:29<00:50,  3.40it/s] 78%|███████▊  | 610/780 [04:30<00:49,  3.42it/s] 78%|███████▊  | 611/780 [04:30<00:49,  3.44it/s] 78%|███████▊  | 612/780 [04:30<00:48,  3.46it/s] 79%|███████▊  | 613/780 [04:30<00:48,  3.47it/s] 79%|███████▊  | 614/780 [04:31<00:47,  3.47it/s] 79%|███████▉  | 615/780 [04:31<00:47,  3.48it/s] 79%|███████▉  | 616/780 [04:31<00:47,  3.48it/s] 79%|███████▉  | 617/780 [04:32<00:46,  3.48it/s] 79%|███████▉  | 618/780 [04:32<00:49,  3.26it/s] 79%|███████▉  | 619/780 [04:32<00:48,  3.33it/s] 79%|███████▉  | 620/780 [04:33<00:47,  3.38it/s] 80%|███████▉  | 621/780 [04:33<00:46,  3.41it/s] 80%|███████▉  | 622/780 [04:33<00:46,  3.43it/s] 80%|███████▉  | 623/780 [04:33<00:45,  3.45it/s] 80%|████████  | 624/780 [04:34<00:45,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 18:08:05,519 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 18:08:05,520 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 18:08:05,520 >>   Batch size = 8
{'eval_loss': 1.0029276609420776, 'eval_runtime': 11.246, 'eval_samples_per_second': 370.353, 'eval_steps_per_second': 46.328, 'epoch': 3.0}
{'loss': 0.5031, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.58it/s][A
  2%|▏         | 12/521 [00:00<00:09, 50.93it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.08it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.39it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.94it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.62it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.50it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.25it/s][A
  9%|▉         | 48/521 [00:01<00:11, 40.97it/s][A
 10%|█         | 53/521 [00:01<00:10, 42.63it/s][A
 11%|█         | 58/521 [00:01<00:10, 43.88it/s][A
 12%|█▏        | 63/521 [00:01<00:10, 44.78it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 45.45it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 45.90it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.24it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.50it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 46.56it/s][A
 18%|█▊        | 93/521 [00:02<00:09, 46.71it/s][A
 19%|█▉        | 98/521 [00:02<00:09, 46.80it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 46.85it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.93it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.96it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.96it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.99it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 47.00it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 47.01it/s][A
 26%|██▋       | 138/521 [00:03<00:09, 40.74it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 42.88it/s][A
 28%|██▊       | 148/521 [00:03<00:08, 44.04it/s][A
 29%|██▉       | 153/521 [00:03<00:10, 34.97it/s][A
 30%|███       | 158/521 [00:03<00:09, 38.27it/s][A
 31%|███▏      | 163/521 [00:03<00:08, 40.56it/s][A
 32%|███▏      | 168/521 [00:03<00:08, 42.31it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 43.65it/s][A
 34%|███▍      | 178/521 [00:04<00:07, 44.64it/s][A
 35%|███▌      | 183/521 [00:04<00:10, 32.24it/s][A
 36%|███▌      | 188/521 [00:04<00:09, 35.61it/s][A
 37%|███▋      | 193/521 [00:04<00:08, 38.40it/s][A
 38%|███▊      | 198/521 [00:04<00:07, 40.63it/s][A
 39%|███▉      | 203/521 [00:04<00:07, 42.40it/s][A
 40%|███▉      | 208/521 [00:04<00:07, 43.70it/s][A
 41%|████      | 213/521 [00:04<00:06, 44.64it/s][A
 42%|████▏     | 218/521 [00:05<00:07, 38.71it/s][A
 43%|████▎     | 223/521 [00:05<00:07, 40.85it/s][A
 44%|████▍     | 228/521 [00:05<00:06, 42.52it/s][A
 45%|████▍     | 233/521 [00:05<00:06, 43.79it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 44.73it/s][A
 47%|████▋     | 243/521 [00:05<00:06, 45.38it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 45.90it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.25it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.43it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.49it/s][A
 51%|█████▏    | 268/521 [00:06<00:05, 46.77it/s][A
 52%|█████▏    | 273/521 [00:06<00:05, 46.83it/s][A
 53%|█████▎    | 278/521 [00:06<00:05, 46.93it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 46.97it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 46.99it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 47.01it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 47.03it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 46.99it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 47.00it/s][A
 60%|██████    | 313/521 [00:07<00:04, 47.01it/s][A
 61%|██████    | 318/521 [00:07<00:04, 42.69it/s][A
 62%|██████▏   | 323/521 [00:07<00:04, 43.89it/s][A
 63%|██████▎   | 328/521 [00:07<00:04, 44.81it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 45.46it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 45.94it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.27it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.50it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.66it/s][A
 69%|██████▊   | 358/521 [00:08<00:03, 46.69it/s][A
 70%|██████▉   | 363/521 [00:08<00:03, 46.81it/s][A
 71%|███████   | 368/521 [00:08<00:03, 46.88it/s][A
 72%|███████▏  | 373/521 [00:08<00:03, 46.91it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.95it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 46.99it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 47.00it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 47.00it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 47.00it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.96it/s][A
 78%|███████▊  | 408/521 [00:09<00:02, 46.98it/s][A
 79%|███████▉  | 413/521 [00:09<00:02, 47.01it/s][A
 80%|████████  | 418/521 [00:09<00:02, 46.98it/s][A
 81%|████████  | 423/521 [00:09<00:02, 47.02it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 47.03it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 47.01it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 47.03it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 42.86it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 44.12it/s][A
 87%|████████▋ | 453/521 [00:10<00:01, 44.96it/s][A
 88%|████████▊ | 458/521 [00:10<00:01, 40.98it/s][A
 89%|████████▉ | 463/521 [00:10<00:01, 42.64it/s][A
 90%|████████▉ | 468/521 [00:10<00:01, 43.88it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 44.78it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 45.43it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 45.90it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.25it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.49it/s][A
 96%|█████████▌| 498/521 [00:11<00:00, 46.49it/s][A
 97%|█████████▋| 503/521 [00:11<00:00, 46.64it/s][A
 98%|█████████▊| 508/521 [00:11<00:00, 46.77it/s][A
 98%|█████████▊| 513/521 [00:11<00:00, 46.86it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.88it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 46.88it/s][A 80%|████████  | 624/780 [04:45<00:45,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 18:08:17,657 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 18:08:18,168 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 18:08:23,346 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 18:08:23,551 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 18:08:23,666 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [05:03<23:13,  8.99s/it] 80%|████████  | 626/780 [05:03<16:24,  6.39s/it] 80%|████████  | 627/780 [05:04<11:37,  4.56s/it] 81%|████████  | 628/780 [05:04<08:18,  3.28s/it] 81%|████████  | 629/780 [05:04<05:59,  2.38s/it] 81%|████████  | 630/780 [05:04<04:22,  1.75s/it] 81%|████████  | 631/780 [05:05<03:15,  1.31s/it] 81%|████████  | 632/780 [05:05<02:28,  1.00s/it] 81%|████████  | 633/780 [05:05<01:55,  1.27it/s] 81%|████████▏ | 634/780 [05:06<01:33,  1.57it/s] 81%|████████▏ | 635/780 [05:06<01:17,  1.88it/s] 82%|████████▏ | 636/780 [05:06<01:05,  2.18it/s] 82%|████████▏ | 637/780 [05:07<01:00,  2.36it/s] 82%|████████▏ | 638/780 [05:07<00:54,  2.61it/s] 82%|████████▏ | 639/780 [05:07<00:49,  2.83it/s] 82%|████████▏ | 640/780 [05:07<00:46,  3.00it/s] 82%|████████▏ | 641/780 [05:08<00:44,  3.14it/s] 82%|████████▏ | 642/780 [05:08<00:42,  3.24it/s] 82%|████████▏ | 643/780 [05:08<00:41,  3.31it/s] 83%|████████▎ | 644/780 [05:09<00:40,  3.37it/s] 83%|████████▎ | 645/780 [05:09<00:39,  3.41it/s] 83%|████████▎ | 646/780 [05:09<00:39,  3.44it/s] 83%|████████▎ | 647/780 [05:09<00:38,  3.45it/s] 83%|████████▎ | 648/780 [05:10<00:42,  3.10it/s] 83%|████████▎ | 649/780 [05:10<00:40,  3.21it/s] 83%|████████▎ | 650/780 [05:10<00:39,  3.29it/s] 83%|████████▎ | 651/780 [05:11<00:38,  3.35it/s] 84%|████████▎ | 652/780 [05:11<00:37,  3.40it/s] 84%|████████▎ | 653/780 [05:11<00:37,  3.43it/s] 84%|████████▍ | 654/780 [05:12<00:36,  3.45it/s] 84%|████████▍ | 655/780 [05:12<00:36,  3.47it/s] 84%|████████▍ | 656/780 [05:12<00:35,  3.48it/s] 84%|████████▍ | 657/780 [05:12<00:35,  3.48it/s] 84%|████████▍ | 658/780 [05:13<00:34,  3.49it/s] 84%|████████▍ | 659/780 [05:13<00:37,  3.22it/s] 85%|████████▍ | 660/780 [05:13<00:36,  3.30it/s] 85%|████████▍ | 661/780 [05:14<00:35,  3.36it/s] 85%|████████▍ | 662/780 [05:14<00:34,  3.40it/s] 85%|████████▌ | 663/780 [05:14<00:34,  3.43it/s] 85%|████████▌ | 664/780 [05:14<00:33,  3.45it/s] 85%|████████▌ | 665/780 [05:15<00:33,  3.47it/s] 85%|████████▌ | 666/780 [05:15<00:32,  3.48it/s] 86%|████████▌ | 667/780 [05:15<00:32,  3.48it/s] 86%|████████▌ | 668/780 [05:16<00:32,  3.49it/s] 86%|████████▌ | 669/780 [05:16<00:31,  3.49it/s] 86%|████████▌ | 670/780 [05:16<00:33,  3.25it/s] 86%|████████▌ | 671/780 [05:17<00:32,  3.32it/s] 86%|████████▌ | 672/780 [05:17<00:32,  3.37it/s] 86%|████████▋ | 673/780 [05:17<00:31,  3.41it/s] 86%|████████▋ | 674/780 [05:17<00:30,  3.44it/s] 87%|████████▋ | 675/780 [05:18<00:30,  3.46it/s] 87%|████████▋ | 676/780 [05:18<00:29,  3.47it/s] 87%|████████▋ | 677/780 [05:18<00:29,  3.48it/s] 87%|████████▋ | 678/780 [05:19<00:29,  3.49it/s] 87%|████████▋ | 679/780 [05:19<00:28,  3.49it/s] 87%|████████▋ | 680/780 [05:19<00:28,  3.50it/s] 87%|████████▋ | 681/780 [05:19<00:30,  3.28it/s] 87%|████████▋ | 682/780 [05:20<00:29,  3.34it/s] 88%|████████▊ | 683/780 [05:20<00:28,  3.39it/s] 88%|████████▊ | 684/780 [05:20<00:28,  3.42it/s] 88%|████████▊ | 685/780 [05:21<00:30,  3.16it/s] 88%|████████▊ | 686/780 [05:21<00:28,  3.26it/s] 88%|████████▊ | 687/780 [05:21<00:27,  3.32it/s] 88%|████████▊ | 688/780 [05:22<00:27,  3.38it/s] 88%|████████▊ | 689/780 [05:22<00:26,  3.41it/s] 88%|████████▊ | 690/780 [05:22<00:26,  3.44it/s] 89%|████████▊ | 691/780 [05:22<00:25,  3.45it/s] 89%|████████▊ | 692/780 [05:23<00:25,  3.47it/s] 89%|████████▉ | 693/780 [05:23<00:25,  3.48it/s] 89%|████████▉ | 694/780 [05:23<00:24,  3.48it/s] 89%|████████▉ | 695/780 [05:24<00:26,  3.27it/s] 89%|████████▉ | 696/780 [05:24<00:25,  3.34it/s] 89%|████████▉ | 697/780 [05:24<00:24,  3.38it/s] 89%|████████▉ | 698/780 [05:24<00:23,  3.42it/s] 90%|████████▉ | 699/780 [05:25<00:23,  3.44it/s] 90%|████████▉ | 700/780 [05:25<00:23,  3.46it/s] 90%|████████▉ | 701/780 [05:25<00:22,  3.47it/s] 90%|█████████ | 702/780 [05:26<00:22,  3.48it/s] 90%|█████████ | 703/780 [05:26<00:22,  3.48it/s] 90%|█████████ | 704/780 [05:26<00:21,  3.49it/s] 90%|█████████ | 705/780 [05:26<00:21,  3.49it/s] 91%|█████████ | 706/780 [05:27<00:22,  3.31it/s] 91%|█████████ | 707/780 [05:27<00:21,  3.37it/s] 91%|█████████ | 708/780 [05:27<00:21,  3.41it/s] 91%|█████████ | 709/780 [05:28<00:20,  3.43it/s] 91%|█████████ | 710/780 [05:28<00:20,  3.44it/s] 91%|█████████ | 711/780 [05:28<00:19,  3.46it/s] 91%|█████████▏| 712/780 [05:29<00:19,  3.47it/s] 91%|█████████▏| 713/780 [05:29<00:19,  3.48it/s] 92%|█████████▏| 714/780 [05:29<00:18,  3.48it/s] 92%|█████████▏| 715/780 [05:29<00:18,  3.49it/s] 92%|█████████▏| 716/780 [05:30<00:18,  3.49it/s] 92%|█████████▏| 717/780 [05:30<00:18,  3.35it/s] 92%|█████████▏| 718/780 [05:30<00:18,  3.40it/s] 92%|█████████▏| 719/780 [05:31<00:17,  3.42it/s] 92%|█████████▏| 720/780 [05:31<00:17,  3.44it/s] 92%|█████████▏| 721/780 [05:31<00:17,  3.46it/s] 93%|█████████▎| 722/780 [05:31<00:16,  3.47it/s] 93%|█████████▎| 723/780 [05:32<00:16,  3.48it/s] 93%|█████████▎| 724/780 [05:32<00:16,  3.48it/s] 93%|█████████▎| 725/780 [05:32<00:15,  3.48it/s] 93%|█████████▎| 726/780 [05:33<00:15,  3.49it/s] 93%|█████████▎| 727/780 [05:33<00:15,  3.49it/s] 93%|█████████▎| 728/780 [05:33<00:16,  3.10it/s] 93%|█████████▎| 729/780 [05:34<00:15,  3.21it/s] 94%|█████████▎| 730/780 [05:34<00:15,  3.29it/s] 94%|█████████▎| 731/780 [05:34<00:14,  3.35it/s] 94%|█████████▍| 732/780 [05:34<00:14,  3.39it/s] 94%|█████████▍| 733/780 [05:35<00:13,  3.42it/s] 94%|█████████▍| 734/780 [05:35<00:13,  3.44it/s] 94%|█████████▍| 735/780 [05:35<00:13,  3.46it/s] 94%|█████████▍| 736/780 [05:36<00:12,  3.47it/s] 94%|█████████▍| 737/780 [05:36<00:12,  3.48it/s] 95%|█████████▍| 738/780 [05:36<00:12,  3.48it/s] 95%|█████████▍| 739/780 [05:36<00:12,  3.37it/s] 95%|█████████▍| 740/780 [05:37<00:11,  3.40it/s] 95%|█████████▌| 741/780 [05:37<00:11,  3.43it/s] 95%|█████████▌| 742/780 [05:37<00:12,  3.16it/s] 95%|█████████▌| 743/780 [05:38<00:11,  3.24it/s] 95%|█████████▌| 744/780 [05:38<00:10,  3.31it/s] 96%|█████████▌| 745/780 [05:38<00:10,  3.37it/s] 96%|█████████▌| 746/780 [05:39<00:09,  3.40it/s] 96%|█████████▌| 747/780 [05:39<00:10,  3.26it/s] 96%|█████████▌| 748/780 [05:39<00:09,  3.32it/s] 96%|█████████▌| 749/780 [05:39<00:09,  3.28it/s] 96%|█████████▌| 750/780 [05:40<00:08,  3.34it/s] 96%|█████████▋| 751/780 [05:40<00:08,  3.38it/s] 96%|█████████▋| 752/780 [05:40<00:08,  3.41it/s] 97%|█████████▋| 753/780 [05:41<00:07,  3.44it/s] 97%|█████████▋| 754/780 [05:41<00:07,  3.45it/s] 97%|█████████▋| 755/780 [05:41<00:07,  3.46it/s] 97%|█████████▋| 756/780 [05:41<00:06,  3.47it/s] 97%|█████████▋| 757/780 [05:42<00:06,  3.48it/s] 97%|█████████▋| 758/780 [05:42<00:06,  3.48it/s] 97%|█████████▋| 759/780 [05:42<00:06,  3.48it/s] 97%|█████████▋| 760/780 [05:43<00:06,  3.33it/s] 98%|█████████▊| 761/780 [05:43<00:05,  3.37it/s] 98%|█████████▊| 762/780 [05:43<00:05,  3.41it/s] 98%|█████████▊| 763/780 [05:44<00:04,  3.43it/s] 98%|█████████▊| 764/780 [05:44<00:04,  3.45it/s] 98%|█████████▊| 765/780 [05:44<00:04,  3.46it/s] 98%|█████████▊| 766/780 [05:44<00:04,  3.47it/s] 98%|█████████▊| 767/780 [05:45<00:03,  3.47it/s] 98%|█████████▊| 768/780 [05:45<00:03,  3.48it/s] 99%|█████████▊| 769/780 [05:45<00:03,  3.48it/s] 99%|█████████▊| 770/780 [05:46<00:02,  3.48it/s] 99%|█████████▉| 771/780 [05:46<00:02,  3.30it/s] 99%|█████████▉| 772/780 [05:46<00:02,  3.35it/s] 99%|█████████▉| 773/780 [05:46<00:02,  3.39it/s] 99%|█████████▉| 774/780 [05:47<00:01,  3.42it/s] 99%|█████████▉| 775/780 [05:47<00:01,  3.44it/s] 99%|█████████▉| 776/780 [05:47<00:01,  3.45it/s]100%|█████████▉| 777/780 [05:48<00:00,  3.47it/s]100%|█████████▉| 778/780 [05:48<00:00,  3.47it/s]100%|█████████▉| 779/780 [05:48<00:00,  3.48it/s]100%|██████████| 780/780 [05:48<00:00,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 18:09:20,204 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 18:09:20,204 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 18:09:20,204 >>   Batch size = 8
{'eval_loss': 1.0108811855316162, 'eval_runtime': 11.6246, 'eval_samples_per_second': 358.291, 'eval_steps_per_second': 44.819, 'epoch': 4.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.92it/s][A
  2%|▏         | 12/521 [00:00<00:12, 39.53it/s][A
  3%|▎         | 17/521 [00:00<00:11, 42.34it/s][A
  4%|▍         | 22/521 [00:00<00:11, 44.01it/s][A
  5%|▌         | 27/521 [00:00<00:10, 45.03it/s][A
  6%|▌         | 32/521 [00:00<00:10, 45.67it/s][A
  7%|▋         | 37/521 [00:00<00:10, 46.16it/s][A
  8%|▊         | 42/521 [00:00<00:10, 46.46it/s][A
  9%|▉         | 47/521 [00:01<00:10, 46.66it/s][A
 10%|▉         | 52/521 [00:01<00:10, 46.66it/s][A
 11%|█         | 57/521 [00:01<00:09, 46.80it/s][A
 12%|█▏        | 62/521 [00:01<00:09, 46.88it/s][A
 13%|█▎        | 67/521 [00:01<00:09, 47.00it/s][A
 14%|█▍        | 72/521 [00:01<00:09, 47.03it/s][A
 15%|█▍        | 77/521 [00:01<00:09, 47.06it/s][A
 16%|█▌        | 82/521 [00:01<00:09, 47.08it/s][A
 17%|█▋        | 87/521 [00:01<00:09, 47.10it/s][A
 18%|█▊        | 92/521 [00:01<00:09, 47.09it/s][A
 19%|█▊        | 97/521 [00:02<00:09, 46.98it/s][A
 20%|█▉        | 102/521 [00:02<00:08, 47.02it/s][A
 21%|██        | 107/521 [00:02<00:08, 47.04it/s][A
 21%|██▏       | 112/521 [00:02<00:08, 47.04it/s][A
 22%|██▏       | 117/521 [00:02<00:08, 46.64it/s][A
 23%|██▎       | 122/521 [00:02<00:08, 46.78it/s][A
 24%|██▍       | 127/521 [00:02<00:08, 46.88it/s][A
 25%|██▌       | 132/521 [00:02<00:08, 46.95it/s][A
 26%|██▋       | 137/521 [00:02<00:08, 46.98it/s][A
 27%|██▋       | 142/521 [00:03<00:08, 46.99it/s][A
 28%|██▊       | 147/521 [00:03<00:07, 47.05it/s][A
 29%|██▉       | 152/521 [00:03<00:07, 47.04it/s][A
 30%|███       | 157/521 [00:03<00:07, 47.02it/s][A
 31%|███       | 162/521 [00:03<00:07, 47.06it/s][A
 32%|███▏      | 167/521 [00:03<00:07, 47.06it/s][A
 33%|███▎      | 172/521 [00:03<00:07, 47.06it/s][A
 34%|███▍      | 177/521 [00:03<00:07, 47.05it/s][A
 35%|███▍      | 182/521 [00:03<00:07, 47.06it/s][A
 36%|███▌      | 187/521 [00:04<00:07, 47.04it/s][A
 37%|███▋      | 192/521 [00:04<00:06, 47.05it/s][A
 38%|███▊      | 197/521 [00:04<00:06, 47.04it/s][A
 39%|███▉      | 202/521 [00:04<00:06, 47.04it/s][A
 40%|███▉      | 207/521 [00:04<00:06, 47.07it/s][A
 41%|████      | 212/521 [00:04<00:06, 47.08it/s][A
 42%|████▏     | 217/521 [00:04<00:09, 30.60it/s][A
 43%|████▎     | 222/521 [00:04<00:08, 34.17it/s][A
 44%|████▎     | 227/521 [00:05<00:07, 37.23it/s][A
 45%|████▍     | 232/521 [00:05<00:07, 39.74it/s][A
 45%|████▌     | 237/521 [00:05<00:06, 41.73it/s][A
 46%|████▋     | 242/521 [00:05<00:06, 43.24it/s][A
 47%|████▋     | 247/521 [00:05<00:06, 44.35it/s][A
 48%|████▊     | 252/521 [00:05<00:07, 38.33it/s][A
 49%|████▉     | 257/521 [00:05<00:06, 40.57it/s][A
 50%|█████     | 262/521 [00:05<00:06, 42.37it/s][A
 51%|█████     | 267/521 [00:05<00:05, 43.72it/s][A
 52%|█████▏    | 272/521 [00:06<00:05, 44.73it/s][A
 53%|█████▎    | 277/521 [00:06<00:05, 45.43it/s][A
 54%|█████▍    | 282/521 [00:06<00:05, 45.95it/s][A
 55%|█████▌    | 287/521 [00:06<00:05, 46.35it/s][A
 56%|█████▌    | 292/521 [00:06<00:04, 46.18it/s][A
 57%|█████▋    | 297/521 [00:06<00:04, 46.33it/s][A
 58%|█████▊    | 302/521 [00:06<00:04, 46.53it/s][A
 59%|█████▉    | 307/521 [00:06<00:04, 46.68it/s][A
 60%|█████▉    | 312/521 [00:06<00:04, 46.84it/s][A
 61%|██████    | 317/521 [00:07<00:04, 46.96it/s][A
 62%|██████▏   | 322/521 [00:07<00:04, 47.00it/s][A
 63%|██████▎   | 327/521 [00:07<00:04, 47.08it/s][A
 64%|██████▎   | 332/521 [00:07<00:04, 47.09it/s][A
 65%|██████▍   | 337/521 [00:07<00:03, 46.97it/s][A
 66%|██████▌   | 342/521 [00:07<00:03, 46.91it/s][A
 67%|██████▋   | 347/521 [00:07<00:03, 46.85it/s][A
 68%|██████▊   | 352/521 [00:07<00:03, 46.90it/s][A
 69%|██████▊   | 357/521 [00:07<00:03, 46.98it/s][A
 69%|██████▉   | 362/521 [00:07<00:03, 47.05it/s][A
 70%|███████   | 367/521 [00:08<00:03, 47.05it/s][A
 71%|███████▏  | 372/521 [00:08<00:03, 47.09it/s][A
 72%|███████▏  | 377/521 [00:08<00:03, 47.07it/s][A
 73%|███████▎  | 382/521 [00:08<00:02, 47.02it/s][A
 74%|███████▍  | 387/521 [00:08<00:02, 46.96it/s][A
 75%|███████▌  | 392/521 [00:08<00:03, 41.17it/s][A
 76%|███████▌  | 397/521 [00:08<00:02, 42.82it/s][A
 77%|███████▋  | 402/521 [00:08<00:02, 44.05it/s][A
 78%|███████▊  | 407/521 [00:08<00:02, 44.93it/s][A
 79%|███████▉  | 412/521 [00:09<00:02, 45.58it/s][A
 80%|████████  | 417/521 [00:09<00:02, 46.04it/s][A
 81%|████████  | 422/521 [00:09<00:02, 46.37it/s][A
 82%|████████▏ | 427/521 [00:09<00:02, 46.62it/s][A
 83%|████████▎ | 432/521 [00:09<00:01, 46.45it/s][A
 84%|████████▍ | 437/521 [00:09<00:01, 46.49it/s][A
 85%|████████▍ | 442/521 [00:09<00:01, 46.72it/s][A
 86%|████████▌ | 447/521 [00:09<00:01, 46.86it/s][A
 87%|████████▋ | 452/521 [00:09<00:01, 46.91it/s][A
 88%|████████▊ | 457/521 [00:10<00:01, 47.00it/s][A
 89%|████████▊ | 462/521 [00:10<00:01, 47.06it/s][A
 90%|████████▉ | 467/521 [00:10<00:01, 47.04it/s][A
 91%|█████████ | 472/521 [00:10<00:01, 47.04it/s][A
 92%|█████████▏| 477/521 [00:10<00:00, 46.92it/s][A
 93%|█████████▎| 482/521 [00:10<00:00, 46.84it/s][A
 93%|█████████▎| 487/521 [00:10<00:00, 46.86it/s][A
 94%|█████████▍| 492/521 [00:10<00:00, 46.96it/s][A
 95%|█████████▌| 497/521 [00:10<00:00, 46.97it/s][A
 96%|█████████▋| 502/521 [00:11<00:00, 47.02it/s][A
 97%|█████████▋| 507/521 [00:11<00:00, 47.05it/s][A
 98%|█████████▊| 512/521 [00:11<00:00, 47.10it/s][A
 99%|█████████▉| 517/521 [00:11<00:00, 47.02it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 47.02it/s][A100%|██████████| 780/780 [06:00<00:00,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 18:09:32,078 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 18:09:32,502 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 18:09:38,095 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 18:09:38,432 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 18:09:38,629 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 18:09:52,094 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 18:09:52,198 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156 (score: 0.9803659319877625).
                                                 100%|██████████| 780/780 [06:40<00:00,  3.48it/s]100%|██████████| 780/780 [06:40<00:00,  1.95it/s]
[INFO|trainer.py:1894] 2023-08-29 18:10:11,585 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 18:10:11,805 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 18:10:16,701 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 18:10:16,977 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 18:10:17,110 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 18:10:17,960 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:17,961 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:17,961 >>   train_loss               =     0.4936
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:17,961 >>   train_runtime            = 0:06:40.22
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:17,961 >>   train_samples            =       9999
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:17,961 >>   train_samples_per_second =    124.918
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:17,961 >>   train_steps_per_second   =      1.949
{'eval_loss': 1.0183043479919434, 'eval_runtime': 11.4354, 'eval_samples_per_second': 364.219, 'eval_steps_per_second': 45.56, 'epoch': 5.0}
{'train_runtime': 400.2215, 'train_samples_per_second': 124.918, 'train_steps_per_second': 1.949, 'train_loss': 0.49364490998096955, 'epoch': 5.0}
08/29/2023 18:10:18 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 18:10:18,409 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 18:10:18,409 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 18:10:18,410 >>   Batch size = 8
  0%|          | 0/521 [00:00<?, ?it/s]  1%|          | 6/521 [00:00<00:08, 59.12it/s]  2%|▏         | 12/521 [00:00<00:09, 51.88it/s]  3%|▎         | 18/521 [00:00<00:10, 50.04it/s]  5%|▍         | 24/521 [00:00<00:10, 49.21it/s]  6%|▌         | 29/521 [00:00<00:10, 48.79it/s]  7%|▋         | 34/521 [00:00<00:10, 48.54it/s]  7%|▋         | 39/521 [00:00<00:09, 48.37it/s]  8%|▊         | 44/521 [00:00<00:09, 48.18it/s]  9%|▉         | 49/521 [00:00<00:09, 47.94it/s] 10%|█         | 54/521 [00:01<00:09, 47.93it/s] 11%|█▏        | 59/521 [00:01<00:09, 47.86it/s] 12%|█▏        | 64/521 [00:01<00:09, 47.82it/s] 13%|█▎        | 69/521 [00:01<00:10, 44.34it/s] 14%|█▍        | 74/521 [00:01<00:09, 45.38it/s] 15%|█▌        | 79/521 [00:01<00:09, 46.13it/s] 16%|█▌        | 84/521 [00:01<00:09, 46.68it/s] 17%|█▋        | 89/521 [00:01<00:09, 47.04it/s] 18%|█▊        | 94/521 [00:01<00:09, 47.32it/s] 19%|█▉        | 99/521 [00:02<00:08, 47.52it/s] 20%|█▉        | 104/521 [00:02<00:08, 47.51it/s] 21%|██        | 109/521 [00:02<00:08, 47.52it/s] 22%|██▏       | 114/521 [00:02<00:08, 47.50it/s] 23%|██▎       | 119/521 [00:02<00:08, 47.58it/s] 24%|██▍       | 124/521 [00:02<00:08, 47.63it/s] 25%|██▍       | 129/521 [00:02<00:08, 47.67it/s] 26%|██▌       | 134/521 [00:02<00:08, 47.77it/s] 27%|██▋       | 139/521 [00:02<00:07, 47.82it/s] 28%|██▊       | 144/521 [00:03<00:07, 47.88it/s] 29%|██▊       | 149/521 [00:03<00:07, 47.90it/s] 30%|██▉       | 154/521 [00:03<00:07, 47.79it/s] 31%|███       | 159/521 [00:03<00:07, 47.69it/s] 31%|███▏      | 164/521 [00:03<00:07, 47.63it/s] 32%|███▏      | 169/521 [00:03<00:07, 47.62it/s] 33%|███▎      | 174/521 [00:03<00:07, 47.71it/s] 34%|███▍      | 179/521 [00:03<00:07, 47.73it/s] 35%|███▌      | 184/521 [00:03<00:07, 47.74it/s] 36%|███▋      | 189/521 [00:03<00:06, 47.74it/s] 37%|███▋      | 194/521 [00:04<00:06, 47.79it/s] 38%|███▊      | 199/521 [00:04<00:06, 47.80it/s] 39%|███▉      | 204/521 [00:04<00:06, 47.79it/s] 40%|████      | 209/521 [00:04<00:06, 47.70it/s] 41%|████      | 214/521 [00:04<00:06, 46.02it/s] 42%|████▏     | 219/521 [00:04<00:06, 46.57it/s] 43%|████▎     | 224/521 [00:04<00:06, 46.97it/s] 44%|████▍     | 229/521 [00:04<00:06, 47.25it/s] 45%|████▍     | 234/521 [00:04<00:06, 47.46it/s] 46%|████▌     | 239/521 [00:05<00:05, 47.53it/s] 47%|████▋     | 244/521 [00:05<00:05, 47.61it/s] 48%|████▊     | 249/521 [00:05<00:05, 47.68it/s] 49%|████▉     | 254/521 [00:05<00:05, 47.55it/s] 50%|████▉     | 259/521 [00:05<00:05, 47.45it/s] 51%|█████     | 264/521 [00:05<00:05, 47.52it/s] 52%|█████▏    | 269/521 [00:05<00:05, 47.60it/s] 53%|█████▎    | 274/521 [00:05<00:05, 47.67it/s] 54%|█████▎    | 279/521 [00:05<00:05, 47.69it/s] 55%|█████▍    | 284/521 [00:05<00:04, 47.67it/s] 55%|█████▌    | 289/521 [00:06<00:04, 47.68it/s] 56%|█████▋    | 294/521 [00:06<00:04, 47.65it/s] 57%|█████▋    | 299/521 [00:06<00:04, 47.56it/s] 58%|█████▊    | 304/521 [00:06<00:07, 30.78it/s] 59%|█████▉    | 309/521 [00:06<00:06, 34.64it/s] 60%|██████    | 314/521 [00:06<00:05, 37.69it/s] 61%|██████    | 319/521 [00:06<00:05, 40.22it/s] 62%|██████▏   | 324/521 [00:06<00:04, 42.19it/s] 63%|██████▎   | 329/521 [00:07<00:04, 43.63it/s] 64%|██████▍   | 334/521 [00:07<00:04, 44.81it/s] 65%|██████▌   | 339/521 [00:07<00:03, 45.74it/s] 66%|██████▌   | 344/521 [00:07<00:03, 46.34it/s] 67%|██████▋   | 349/521 [00:07<00:03, 43.02it/s] 68%|██████▊   | 354/521 [00:07<00:03, 44.36it/s] 69%|██████▉   | 359/521 [00:07<00:03, 45.34it/s] 70%|██████▉   | 364/521 [00:07<00:03, 46.02it/s] 71%|███████   | 369/521 [00:07<00:03, 46.52it/s] 72%|███████▏  | 374/521 [00:08<00:03, 46.90it/s] 73%|███████▎  | 379/521 [00:08<00:03, 47.10it/s] 74%|███████▎  | 384/521 [00:08<00:02, 47.22it/s] 75%|███████▍  | 389/521 [00:08<00:02, 47.13it/s] 76%|███████▌  | 394/521 [00:08<00:02, 47.12it/s] 77%|███████▋  | 399/521 [00:08<00:02, 47.20it/s] 78%|███████▊  | 404/521 [00:08<00:02, 47.32it/s] 79%|███████▊  | 409/521 [00:08<00:02, 47.41it/s] 79%|███████▉  | 414/521 [00:08<00:02, 47.43it/s] 80%|████████  | 419/521 [00:09<00:02, 47.44it/s] 81%|████████▏ | 424/521 [00:09<00:02, 47.49it/s] 82%|████████▏ | 429/521 [00:09<00:01, 47.59it/s] 83%|████████▎ | 434/521 [00:09<00:01, 47.59it/s] 84%|████████▍ | 439/521 [00:09<00:01, 47.54it/s] 85%|████████▌ | 444/521 [00:09<00:01, 47.54it/s] 86%|████████▌ | 449/521 [00:09<00:01, 47.52it/s] 87%|████████▋ | 454/521 [00:09<00:01, 47.49it/s] 88%|████████▊ | 459/521 [00:09<00:01, 47.50it/s] 89%|████████▉ | 464/521 [00:09<00:01, 47.55it/s] 90%|█████████ | 469/521 [00:10<00:01, 47.53it/s] 91%|█████████ | 474/521 [00:10<00:00, 47.52it/s] 92%|█████████▏| 479/521 [00:10<00:00, 47.52it/s] 93%|█████████▎| 484/521 [00:10<00:00, 47.57it/s] 94%|█████████▍| 489/521 [00:10<00:00, 47.54it/s] 95%|█████████▍| 494/521 [00:10<00:00, 41.88it/s] 96%|█████████▌| 499/521 [00:10<00:00, 43.42it/s] 97%|█████████▋| 504/521 [00:10<00:00, 44.57it/s] 98%|█████████▊| 509/521 [00:10<00:00, 45.39it/s] 99%|█████████▊| 514/521 [00:11<00:00, 46.06it/s]100%|█████████▉| 519/521 [00:11<00:00, 46.47it/s]100%|██████████| 521/521 [00:11<00:00, 46.47it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 18:10:29,646 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:29,646 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:29,646 >>   eval_loss               =     0.9804
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:29,646 >>   eval_runtime            = 0:00:11.23
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:29,646 >>   eval_samples            =       4165
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:29,646 >>   eval_samples_per_second =    370.672
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:29,646 >>   eval_steps_per_second   =     46.367
[INFO|trainer_pt_utils.py:913] 2023-08-29 18:10:29,646 >>   perplexity              =     2.6654
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:43,152 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:43,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:43,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:43,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:43,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 18:10:44,308 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 18:10:44,309 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:10:45,145 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 18:10:46,361 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:10:46,446 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:49,971 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:50,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:50,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:50,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:10:50,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 18:10:51,157 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 18:10:51,158 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:10:51,900 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 18:10:52,211 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:10:52,211 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'lowest point', 'manufacturer', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12876
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12976, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.85it/s]Extractor Predicting: 2it [00:01,  1.77it/s]Extractor Predicting: 3it [00:01,  1.71it/s]Extractor Predicting: 4it [00:02,  1.74it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.64it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.70it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.72it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:09,  1.56it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:11,  1.43it/s]Extractor Predicting: 20it [00:12,  1.48it/s]Extractor Predicting: 21it [00:13,  1.48it/s]Extractor Predicting: 22it [00:13,  1.48it/s]Extractor Predicting: 23it [00:14,  1.49it/s]Extractor Predicting: 24it [00:15,  1.46it/s]Extractor Predicting: 25it [00:15,  1.49it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:19,  1.51it/s]Extractor Predicting: 32it [00:20,  1.54it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:21,  1.48it/s]Extractor Predicting: 35it [00:22,  1.52it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:23,  1.49it/s]Extractor Predicting: 38it [00:24,  1.50it/s]Extractor Predicting: 39it [00:25,  1.48it/s]Extractor Predicting: 40it [00:25,  1.50it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:26,  1.56it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:29,  1.54it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:30,  1.55it/s]Extractor Predicting: 49it [00:31,  1.53it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:32,  1.59it/s]Extractor Predicting: 52it [00:33,  1.57it/s]Extractor Predicting: 53it [00:34,  1.60it/s]Extractor Predicting: 54it [00:34,  1.54it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:36,  1.55it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:37,  1.53it/s]Extractor Predicting: 60it [00:38,  1.53it/s]Extractor Predicting: 61it [00:39,  1.57it/s]Extractor Predicting: 62it [00:39,  1.57it/s]Extractor Predicting: 63it [00:40,  1.58it/s]Extractor Predicting: 64it [00:41,  1.58it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:42,  1.55it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:43,  1.53it/s]Extractor Predicting: 69it [00:44,  1.54it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:45,  1.55it/s]Extractor Predicting: 72it [00:46,  1.54it/s]Extractor Predicting: 73it [00:46,  1.53it/s]Extractor Predicting: 74it [00:47,  1.52it/s]Extractor Predicting: 75it [00:48,  1.56it/s]Extractor Predicting: 76it [00:49,  1.46it/s]Extractor Predicting: 77it [00:49,  1.46it/s]Extractor Predicting: 78it [00:50,  1.46it/s]Extractor Predicting: 79it [00:51,  1.47it/s]Extractor Predicting: 80it [00:51,  1.48it/s]Extractor Predicting: 81it [00:52,  1.41it/s]Extractor Predicting: 82it [00:53,  1.46it/s]Extractor Predicting: 83it [00:53,  1.52it/s]Extractor Predicting: 84it [00:54,  1.53it/s]Extractor Predicting: 85it [00:55,  1.53it/s]Extractor Predicting: 86it [00:55,  1.51it/s]Extractor Predicting: 87it [00:56,  1.51it/s]Extractor Predicting: 88it [00:57,  1.54it/s]Extractor Predicting: 89it [00:57,  1.52it/s]Extractor Predicting: 90it [00:58,  1.52it/s]Extractor Predicting: 91it [00:59,  1.52it/s]Extractor Predicting: 92it [00:59,  1.54it/s]Extractor Predicting: 93it [01:00,  1.52it/s]Extractor Predicting: 94it [01:00,  1.50it/s]Extractor Predicting: 95it [01:01,  1.36it/s]Extractor Predicting: 96it [01:02,  1.40it/s]Extractor Predicting: 97it [01:03,  1.45it/s]Extractor Predicting: 98it [01:03,  1.48it/s]Extractor Predicting: 99it [01:04,  1.48it/s]Extractor Predicting: 100it [01:05,  1.51it/s]Extractor Predicting: 101it [01:05,  1.47it/s]Extractor Predicting: 102it [01:06,  1.47it/s]Extractor Predicting: 103it [01:07,  1.48it/s]Extractor Predicting: 104it [01:07,  1.51it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.48it/s]Extractor Predicting: 107it [01:09,  1.48it/s]Extractor Predicting: 108it [01:10,  1.47it/s]Extractor Predicting: 109it [01:11,  1.49it/s]Extractor Predicting: 110it [01:11,  1.49it/s]Extractor Predicting: 111it [01:12,  1.51it/s]Extractor Predicting: 112it [01:13,  1.47it/s]Extractor Predicting: 113it [01:13,  1.50it/s]Extractor Predicting: 114it [01:14,  1.50it/s]Extractor Predicting: 115it [01:15,  1.53it/s]Extractor Predicting: 116it [01:15,  1.54it/s]Extractor Predicting: 117it [01:16,  1.54it/s]Extractor Predicting: 118it [01:17,  1.52it/s]Extractor Predicting: 119it [01:17,  1.50it/s]Extractor Predicting: 120it [01:18,  1.53it/s]Extractor Predicting: 121it [01:19,  1.53it/s]Extractor Predicting: 122it [01:19,  1.50it/s]Extractor Predicting: 123it [01:20,  1.48it/s]Extractor Predicting: 124it [01:21,  1.49it/s]Extractor Predicting: 125it [01:21,  1.48it/s]Extractor Predicting: 126it [01:22,  1.49it/s]Extractor Predicting: 127it [01:23,  1.47it/s]Extractor Predicting: 128it [01:23,  1.47it/s]Extractor Predicting: 129it [01:24,  1.46it/s]Extractor Predicting: 130it [01:25,  1.49it/s]Extractor Predicting: 131it [01:25,  1.47it/s]Extractor Predicting: 132it [01:26,  1.47it/s]Extractor Predicting: 133it [01:27,  1.49it/s]Extractor Predicting: 134it [01:27,  1.51it/s]Extractor Predicting: 135it [01:28,  1.52it/s]Extractor Predicting: 136it [01:29,  1.52it/s]Extractor Predicting: 137it [01:29,  1.48it/s]Extractor Predicting: 138it [01:30,  1.48it/s]Extractor Predicting: 139it [01:31,  1.49it/s]Extractor Predicting: 140it [01:31,  1.50it/s]Extractor Predicting: 141it [01:32,  1.52it/s]Extractor Predicting: 142it [01:33,  1.48it/s]Extractor Predicting: 143it [01:33,  1.51it/s]Extractor Predicting: 144it [01:34,  1.49it/s]Extractor Predicting: 145it [01:35,  1.50it/s]Extractor Predicting: 146it [01:35,  1.48it/s]Extractor Predicting: 147it [01:36,  1.46it/s]Extractor Predicting: 148it [01:37,  1.47it/s]Extractor Predicting: 149it [01:38,  1.48it/s]Extractor Predicting: 150it [01:38,  1.50it/s]Extractor Predicting: 151it [01:39,  1.50it/s]Extractor Predicting: 152it [01:40,  1.43it/s]Extractor Predicting: 153it [01:40,  1.45it/s]Extractor Predicting: 154it [01:41,  1.30it/s]Extractor Predicting: 154it [01:41,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:52,648 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:52,724 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:52,724 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:52,724 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:52,724 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 18:12:53,837 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 18:12:53,838 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:12:54,536 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 18:12:55,652 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:12:55,711 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:58,725 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:58,760 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:58,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:58,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:12:58,761 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 18:12:59,563 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 18:12:59,564 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:13:00,193 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 18:13:00,415 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:13:00,415 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.42,
  "recall": 0.025210084033613446,
  "score": 0.04756511891279728,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27443
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27543, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.57it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.55it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:08,  1.58it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.55it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:19,  1.59it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:20,  1.58it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:23,  1.56it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:26,  1.54it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:28,  1.50it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:29,  1.37it/s]Extractor Predicting: 47it [00:30,  1.39it/s]Extractor Predicting: 48it [00:31,  1.40it/s]Extractor Predicting: 49it [00:31,  1.43it/s]Extractor Predicting: 50it [00:32,  1.44it/s]Extractor Predicting: 51it [00:33,  1.44it/s]Extractor Predicting: 52it [00:33,  1.46it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:35,  1.45it/s]Extractor Predicting: 55it [00:35,  1.42it/s]Extractor Predicting: 56it [00:36,  1.45it/s]Extractor Predicting: 57it [00:37,  1.49it/s]Extractor Predicting: 58it [00:37,  1.49it/s]Extractor Predicting: 59it [00:38,  1.48it/s]Extractor Predicting: 60it [00:39,  1.46it/s]Extractor Predicting: 61it [00:39,  1.45it/s]Extractor Predicting: 62it [00:40,  1.49it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:41,  1.43it/s]Extractor Predicting: 65it [00:42,  1.45it/s]Extractor Predicting: 66it [00:43,  1.46it/s]Extractor Predicting: 67it [00:44,  1.44it/s]Extractor Predicting: 68it [00:44,  1.43it/s]Extractor Predicting: 69it [00:45,  1.43it/s]Extractor Predicting: 70it [00:46,  1.44it/s]Extractor Predicting: 71it [00:46,  1.45it/s]Extractor Predicting: 72it [00:47,  1.46it/s]Extractor Predicting: 73it [00:48,  1.39it/s]Extractor Predicting: 74it [00:49,  1.39it/s]Extractor Predicting: 75it [00:49,  1.40it/s]Extractor Predicting: 76it [00:50,  1.39it/s]Extractor Predicting: 77it [00:51,  1.45it/s]Extractor Predicting: 78it [00:51,  1.39it/s]Extractor Predicting: 79it [00:52,  1.42it/s]Extractor Predicting: 80it [00:53,  1.42it/s]Extractor Predicting: 81it [00:53,  1.39it/s]Extractor Predicting: 82it [00:54,  1.42it/s]Extractor Predicting: 83it [00:55,  1.44it/s]Extractor Predicting: 84it [00:55,  1.48it/s]Extractor Predicting: 85it [00:56,  1.44it/s]Extractor Predicting: 86it [00:57,  1.43it/s]Extractor Predicting: 87it [00:58,  1.48it/s]Extractor Predicting: 88it [00:58,  1.45it/s]Extractor Predicting: 89it [00:59,  1.43it/s]Extractor Predicting: 90it [01:00,  1.42it/s]Extractor Predicting: 91it [01:00,  1.44it/s]Extractor Predicting: 92it [01:01,  1.47it/s]Extractor Predicting: 93it [01:02,  1.48it/s]Extractor Predicting: 94it [01:02,  1.49it/s]Extractor Predicting: 95it [01:03,  1.51it/s]Extractor Predicting: 96it [01:04,  1.51it/s]Extractor Predicting: 97it [01:04,  1.43it/s]Extractor Predicting: 98it [01:05,  1.42it/s]Extractor Predicting: 99it [01:06,  1.46it/s]Extractor Predicting: 100it [01:06,  1.45it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:08,  1.48it/s]Extractor Predicting: 103it [01:08,  1.50it/s]Extractor Predicting: 104it [01:09,  1.51it/s]Extractor Predicting: 105it [01:10,  1.49it/s]Extractor Predicting: 106it [01:10,  1.48it/s]Extractor Predicting: 107it [01:11,  1.50it/s]Extractor Predicting: 108it [01:12,  1.50it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:13,  1.40it/s]Extractor Predicting: 111it [01:14,  1.44it/s]Extractor Predicting: 112it [01:15,  1.45it/s]Extractor Predicting: 113it [01:15,  1.49it/s]Extractor Predicting: 114it [01:16,  1.52it/s]Extractor Predicting: 115it [01:17,  1.48it/s]Extractor Predicting: 116it [01:17,  1.48it/s]Extractor Predicting: 117it [01:18,  1.52it/s]Extractor Predicting: 118it [01:19,  1.42it/s]Extractor Predicting: 119it [01:19,  1.45it/s]Extractor Predicting: 120it [01:20,  1.46it/s]Extractor Predicting: 121it [01:21,  1.46it/s]Extractor Predicting: 122it [01:21,  1.47it/s]Extractor Predicting: 123it [01:22,  1.48it/s]Extractor Predicting: 124it [01:23,  1.51it/s]Extractor Predicting: 125it [01:23,  1.51it/s]Extractor Predicting: 126it [01:24,  1.52it/s]Extractor Predicting: 127it [01:25,  1.50it/s]Extractor Predicting: 128it [01:25,  1.53it/s]Extractor Predicting: 129it [01:26,  1.56it/s]Extractor Predicting: 130it [01:27,  1.50it/s]Extractor Predicting: 131it [01:27,  1.55it/s]Extractor Predicting: 132it [01:28,  1.55it/s]Extractor Predicting: 133it [01:29,  1.55it/s]Extractor Predicting: 134it [01:29,  1.53it/s]Extractor Predicting: 135it [01:30,  1.51it/s]Extractor Predicting: 136it [01:31,  1.50it/s]Extractor Predicting: 137it [01:31,  1.47it/s]Extractor Predicting: 138it [01:32,  1.49it/s]Extractor Predicting: 139it [01:33,  1.49it/s]Extractor Predicting: 140it [01:33,  1.48it/s]Extractor Predicting: 141it [01:34,  1.53it/s]Extractor Predicting: 142it [01:35,  1.46it/s]Extractor Predicting: 143it [01:35,  1.42it/s]Extractor Predicting: 144it [01:36,  1.45it/s]Extractor Predicting: 145it [01:37,  1.48it/s]Extractor Predicting: 146it [01:37,  1.43it/s]Extractor Predicting: 147it [01:38,  1.43it/s]Extractor Predicting: 148it [01:39,  1.46it/s]Extractor Predicting: 149it [01:39,  1.55it/s]Extractor Predicting: 150it [01:40,  1.68it/s]Extractor Predicting: 151it [01:40,  1.68it/s]Extractor Predicting: 152it [01:41,  1.79it/s]Extractor Predicting: 153it [01:41,  1.87it/s]Extractor Predicting: 154it [01:42,  1.88it/s]Extractor Predicting: 155it [01:42,  1.86it/s]Extractor Predicting: 156it [01:43,  1.85it/s]Extractor Predicting: 157it [01:44,  1.84it/s]Extractor Predicting: 158it [01:44,  1.90it/s]Extractor Predicting: 159it [01:44,  1.95it/s]Extractor Predicting: 160it [01:45,  1.83it/s]Extractor Predicting: 161it [01:46,  1.83it/s]Extractor Predicting: 162it [01:46,  1.88it/s]Extractor Predicting: 163it [01:47,  1.84it/s]Extractor Predicting: 164it [01:47,  1.86it/s]Extractor Predicting: 165it [01:48,  1.91it/s]Extractor Predicting: 166it [01:48,  1.86it/s]Extractor Predicting: 167it [01:49,  1.81it/s]Extractor Predicting: 168it [01:50,  1.74it/s]Extractor Predicting: 169it [01:50,  1.68it/s]Extractor Predicting: 170it [01:51,  1.58it/s]Extractor Predicting: 171it [01:52,  1.54it/s]Extractor Predicting: 172it [01:52,  1.50it/s]Extractor Predicting: 173it [01:53,  1.48it/s]Extractor Predicting: 174it [01:54,  1.48it/s]Extractor Predicting: 175it [01:54,  1.53it/s]Extractor Predicting: 176it [01:55,  1.51it/s]Extractor Predicting: 177it [01:56,  1.53it/s]Extractor Predicting: 178it [01:56,  1.52it/s]Extractor Predicting: 179it [01:57,  1.57it/s]Extractor Predicting: 180it [01:58,  1.39it/s]Extractor Predicting: 181it [01:58,  1.44it/s]Extractor Predicting: 182it [01:59,  1.47it/s]Extractor Predicting: 183it [02:00,  1.46it/s]Extractor Predicting: 184it [02:00,  1.45it/s]Extractor Predicting: 185it [02:01,  1.49it/s]Extractor Predicting: 186it [02:02,  1.48it/s]Extractor Predicting: 187it [02:02,  1.48it/s]Extractor Predicting: 188it [02:03,  1.54it/s]Extractor Predicting: 189it [02:04,  1.53it/s]Extractor Predicting: 190it [02:04,  1.53it/s]Extractor Predicting: 191it [02:05,  1.53it/s]Extractor Predicting: 192it [02:06,  1.55it/s]Extractor Predicting: 193it [02:06,  1.54it/s]Extractor Predicting: 194it [02:07,  1.59it/s]Extractor Predicting: 195it [02:08,  1.57it/s]Extractor Predicting: 196it [02:08,  1.54it/s]Extractor Predicting: 197it [02:09,  1.56it/s]Extractor Predicting: 198it [02:09,  1.56it/s]Extractor Predicting: 199it [02:10,  1.54it/s]Extractor Predicting: 200it [02:11,  1.52it/s]Extractor Predicting: 201it [02:11,  1.52it/s]Extractor Predicting: 202it [02:12,  1.50it/s]Extractor Predicting: 203it [02:13,  1.45it/s]Extractor Predicting: 204it [02:14,  1.47it/s]Extractor Predicting: 205it [02:14,  1.47it/s]Extractor Predicting: 206it [02:15,  1.48it/s]Extractor Predicting: 207it [02:16,  1.50it/s]Extractor Predicting: 208it [02:16,  1.48it/s]Extractor Predicting: 209it [02:17,  1.50it/s]Extractor Predicting: 210it [02:18,  1.50it/s]Extractor Predicting: 211it [02:18,  1.52it/s]Extractor Predicting: 212it [02:19,  1.56it/s]Extractor Predicting: 213it [02:19,  1.51it/s]Extractor Predicting: 214it [02:20,  1.52it/s]Extractor Predicting: 215it [02:21,  1.54it/s]Extractor Predicting: 216it [02:21,  1.55it/s]Extractor Predicting: 217it [02:22,  1.54it/s]Extractor Predicting: 218it [02:23,  1.56it/s]Extractor Predicting: 219it [02:23,  1.48it/s]Extractor Predicting: 220it [02:24,  1.54it/s]Extractor Predicting: 221it [02:25,  1.52it/s]Extractor Predicting: 222it [02:25,  1.55it/s]Extractor Predicting: 223it [02:26,  1.58it/s]Extractor Predicting: 224it [02:27,  1.57it/s]Extractor Predicting: 225it [02:27,  1.60it/s]Extractor Predicting: 226it [02:28,  1.62it/s]Extractor Predicting: 227it [02:28,  1.64it/s]Extractor Predicting: 228it [02:29,  1.57it/s]Extractor Predicting: 229it [02:30,  1.58it/s]Extractor Predicting: 230it [02:30,  1.55it/s]Extractor Predicting: 231it [02:31,  1.54it/s]Extractor Predicting: 232it [02:32,  1.59it/s]Extractor Predicting: 233it [02:32,  1.55it/s]Extractor Predicting: 234it [02:33,  1.53it/s]Extractor Predicting: 235it [02:34,  1.55it/s]Extractor Predicting: 236it [02:34,  1.54it/s]Extractor Predicting: 237it [02:35,  1.63it/s]Extractor Predicting: 238it [02:35,  1.57it/s]Extractor Predicting: 239it [02:36,  1.57it/s]Extractor Predicting: 240it [02:37,  1.54it/s]Extractor Predicting: 241it [02:37,  1.53it/s]Extractor Predicting: 242it [02:38,  1.53it/s]Extractor Predicting: 243it [02:39,  1.55it/s]Extractor Predicting: 244it [02:39,  1.55it/s]Extractor Predicting: 245it [02:40,  1.53it/s]Extractor Predicting: 246it [02:41,  1.52it/s]Extractor Predicting: 247it [02:41,  1.49it/s]Extractor Predicting: 248it [02:42,  1.47it/s]Extractor Predicting: 249it [02:43,  1.48it/s]Extractor Predicting: 250it [02:43,  1.48it/s]Extractor Predicting: 251it [02:44,  1.52it/s]Extractor Predicting: 252it [02:45,  1.54it/s]Extractor Predicting: 253it [02:45,  1.52it/s]Extractor Predicting: 254it [02:46,  1.54it/s]Extractor Predicting: 255it [02:47,  1.50it/s]Extractor Predicting: 256it [02:47,  1.51it/s]Extractor Predicting: 257it [02:48,  1.52it/s]Extractor Predicting: 258it [02:49,  1.50it/s]Extractor Predicting: 259it [02:49,  1.52it/s]Extractor Predicting: 260it [02:50,  1.48it/s]Extractor Predicting: 261it [02:51,  1.50it/s]Extractor Predicting: 262it [02:51,  1.52it/s]Extractor Predicting: 263it [02:52,  1.52it/s]Extractor Predicting: 264it [02:53,  1.53it/s]Extractor Predicting: 265it [02:53,  1.48it/s]Extractor Predicting: 266it [02:54,  1.51it/s]Extractor Predicting: 267it [02:55,  1.52it/s]Extractor Predicting: 268it [02:55,  1.51it/s]Extractor Predicting: 269it [02:56,  1.52it/s]Extractor Predicting: 270it [02:57,  1.45it/s]Extractor Predicting: 271it [02:57,  1.49it/s]Extractor Predicting: 272it [02:58,  1.48it/s]Extractor Predicting: 273it [02:59,  1.46it/s]Extractor Predicting: 274it [02:59,  1.51it/s]Extractor Predicting: 275it [03:00,  1.49it/s]Extractor Predicting: 276it [03:01,  1.51it/s]Extractor Predicting: 277it [03:01,  1.50it/s]Extractor Predicting: 278it [03:02,  1.51it/s]Extractor Predicting: 279it [03:03,  1.50it/s]Extractor Predicting: 280it [03:03,  1.48it/s]Extractor Predicting: 281it [03:04,  1.44it/s]Extractor Predicting: 282it [03:05,  1.46it/s]Extractor Predicting: 283it [03:05,  1.45it/s]Extractor Predicting: 284it [03:06,  1.47it/s]Extractor Predicting: 285it [03:07,  1.41it/s]Extractor Predicting: 286it [03:08,  1.38it/s]Extractor Predicting: 287it [03:08,  1.40it/s]Extractor Predicting: 288it [03:09,  1.44it/s]Extractor Predicting: 289it [03:10,  1.44it/s]Extractor Predicting: 290it [03:11,  1.28it/s]Extractor Predicting: 291it [03:11,  1.33it/s]Extractor Predicting: 292it [03:12,  1.38it/s]Extractor Predicting: 293it [03:13,  1.27it/s]Extractor Predicting: 294it [03:14,  1.33it/s]Extractor Predicting: 295it [03:14,  1.38it/s]Extractor Predicting: 296it [03:15,  1.43it/s]Extractor Predicting: 297it [03:16,  1.47it/s]Extractor Predicting: 298it [03:16,  1.55it/s]Extractor Predicting: 299it [03:17,  1.56it/s]Extractor Predicting: 300it [03:17,  1.57it/s]Extractor Predicting: 301it [03:18,  1.57it/s]Extractor Predicting: 302it [03:19,  1.53it/s]Extractor Predicting: 303it [03:19,  1.51it/s]Extractor Predicting: 304it [03:20,  1.53it/s]Extractor Predicting: 305it [03:21,  1.52it/s]Extractor Predicting: 306it [03:21,  1.52it/s]Extractor Predicting: 307it [03:22,  1.53it/s]Extractor Predicting: 308it [03:23,  1.53it/s]Extractor Predicting: 309it [03:23,  1.52it/s]Extractor Predicting: 310it [03:24,  1.49it/s]Extractor Predicting: 311it [03:25,  1.50it/s]Extractor Predicting: 312it [03:25,  1.51it/s]Extractor Predicting: 313it [03:26,  1.50it/s]Extractor Predicting: 314it [03:27,  1.50it/s]Extractor Predicting: 315it [03:27,  1.51it/s]Extractor Predicting: 316it [03:28,  1.50it/s]Extractor Predicting: 317it [03:29,  1.49it/s]Extractor Predicting: 318it [03:29,  1.52it/s]Extractor Predicting: 319it [03:30,  1.48it/s]Extractor Predicting: 320it [03:31,  1.47it/s]Extractor Predicting: 321it [03:31,  1.49it/s]Extractor Predicting: 322it [03:32,  1.51it/s]Extractor Predicting: 323it [03:33,  1.52it/s]Extractor Predicting: 324it [03:33,  1.53it/s]Extractor Predicting: 325it [03:34,  1.55it/s]Extractor Predicting: 326it [03:35,  1.54it/s]Extractor Predicting: 327it [03:35,  1.57it/s]Extractor Predicting: 328it [03:36,  1.54it/s]Extractor Predicting: 329it [03:37,  1.55it/s]Extractor Predicting: 330it [03:37,  1.55it/s]Extractor Predicting: 331it [03:38,  1.53it/s]Extractor Predicting: 332it [03:39,  1.52it/s]Extractor Predicting: 333it [03:39,  1.51it/s]Extractor Predicting: 334it [03:40,  1.48it/s]Extractor Predicting: 335it [03:41,  1.47it/s]Extractor Predicting: 336it [03:41,  1.50it/s]Extractor Predicting: 337it [03:42,  1.50it/s]Extractor Predicting: 338it [03:43,  1.49it/s]Extractor Predicting: 339it [03:43,  1.51it/s]Extractor Predicting: 340it [03:44,  1.56it/s]Extractor Predicting: 341it [03:44,  1.53it/s]Extractor Predicting: 342it [03:45,  1.55it/s]Extractor Predicting: 343it [03:46,  1.57it/s]Extractor Predicting: 344it [03:46,  1.61it/s]Extractor Predicting: 345it [03:47,  1.63it/s]Extractor Predicting: 346it [03:47,  1.67it/s]Extractor Predicting: 347it [03:48,  1.70it/s]Extractor Predicting: 348it [03:49,  1.62it/s]Extractor Predicting: 349it [03:49,  1.64it/s]Extractor Predicting: 350it [03:50,  1.60it/s]Extractor Predicting: 351it [03:51,  1.63it/s]Extractor Predicting: 352it [03:51,  1.61it/s]Extractor Predicting: 353it [03:52,  1.56it/s]Extractor Predicting: 354it [03:53,  1.51it/s]Extractor Predicting: 355it [03:53,  1.55it/s]Extractor Predicting: 356it [03:54,  1.54it/s]Extractor Predicting: 357it [03:55,  1.54it/s]Extractor Predicting: 358it [03:55,  1.48it/s]Extractor Predicting: 359it [03:56,  1.48it/s]Extractor Predicting: 360it [03:57,  1.49it/s]Extractor Predicting: 361it [03:57,  1.49it/s]Extractor Predicting: 362it [03:58,  1.50it/s]Extractor Predicting: 363it [03:59,  1.50it/s]Extractor Predicting: 364it [03:59,  1.52it/s]Extractor Predicting: 365it [04:00,  1.50it/s]Extractor Predicting: 366it [04:01,  1.49it/s]Extractor Predicting: 367it [04:01,  1.50it/s]Extractor Predicting: 368it [04:02,  1.48it/s]Extractor Predicting: 369it [04:03,  1.47it/s]Extractor Predicting: 370it [04:03,  1.50it/s]Extractor Predicting: 371it [04:04,  1.50it/s]Extractor Predicting: 372it [04:05,  1.48it/s]Extractor Predicting: 373it [04:05,  1.46it/s]Extractor Predicting: 374it [04:06,  1.50it/s]Extractor Predicting: 375it [04:07,  1.52it/s]Extractor Predicting: 376it [04:07,  1.56it/s]Extractor Predicting: 377it [04:08,  1.62it/s]Extractor Predicting: 378it [04:08,  1.59it/s]Extractor Predicting: 379it [04:09,  1.61it/s]Extractor Predicting: 380it [04:10,  1.62it/s]Extractor Predicting: 381it [04:10,  1.62it/s]Extractor Predicting: 382it [04:11,  1.61it/s]Extractor Predicting: 383it [04:12,  1.58it/s]Extractor Predicting: 384it [04:12,  1.58it/s]Extractor Predicting: 385it [04:13,  1.60it/s]Extractor Predicting: 386it [04:13,  1.63it/s]Extractor Predicting: 387it [04:14,  1.63it/s]Extractor Predicting: 388it [04:15,  1.62it/s]Extractor Predicting: 389it [04:15,  1.62it/s]Extractor Predicting: 390it [04:16,  1.63it/s]Extractor Predicting: 391it [04:16,  1.62it/s]Extractor Predicting: 392it [04:17,  1.66it/s]Extractor Predicting: 393it [04:18,  1.65it/s]Extractor Predicting: 394it [04:18,  1.64it/s]Extractor Predicting: 395it [04:19,  1.62it/s]Extractor Predicting: 396it [04:20,  1.62it/s]Extractor Predicting: 397it [04:20,  1.59it/s]Extractor Predicting: 398it [04:21,  1.57it/s]Extractor Predicting: 399it [04:22,  1.52it/s]Extractor Predicting: 400it [04:22,  1.52it/s]Extractor Predicting: 401it [04:23,  1.52it/s]Extractor Predicting: 402it [04:23,  1.55it/s]Extractor Predicting: 403it [04:24,  1.58it/s]Extractor Predicting: 404it [04:25,  1.51it/s]Extractor Predicting: 405it [04:25,  1.51it/s]Extractor Predicting: 406it [04:26,  1.52it/s]Extractor Predicting: 407it [04:27,  1.52it/s]Extractor Predicting: 408it [04:27,  1.53it/s]Extractor Predicting: 409it [04:28,  1.48it/s]Extractor Predicting: 410it [04:29,  1.48it/s]Extractor Predicting: 411it [04:29,  1.50it/s]Extractor Predicting: 412it [04:30,  1.54it/s]Extractor Predicting: 413it [04:31,  1.51it/s]Extractor Predicting: 414it [04:31,  1.50it/s]Extractor Predicting: 415it [04:32,  1.51it/s]Extractor Predicting: 416it [04:33,  1.52it/s]Extractor Predicting: 417it [04:33,  1.52it/s]Extractor Predicting: 418it [04:34,  1.54it/s]Extractor Predicting: 419it [04:35,  1.53it/s]Extractor Predicting: 420it [04:35,  1.53it/s]Extractor Predicting: 421it [04:36,  1.54it/s]Extractor Predicting: 422it [04:37,  1.51it/s]Extractor Predicting: 423it [04:37,  1.51it/s]Extractor Predicting: 424it [04:38,  1.33it/s]Extractor Predicting: 425it [04:39,  1.37it/s]Extractor Predicting: 426it [04:40,  1.45it/s]Extractor Predicting: 427it [04:40,  1.52it/s]Extractor Predicting: 428it [04:41,  1.54it/s]Extractor Predicting: 429it [04:41,  1.56it/s]Extractor Predicting: 430it [04:42,  1.58it/s]Extractor Predicting: 431it [04:43,  1.64it/s]Extractor Predicting: 432it [04:43,  1.61it/s]Extractor Predicting: 433it [04:44,  1.59it/s]Extractor Predicting: 434it [04:44,  1.61it/s]Extractor Predicting: 435it [04:45,  1.60it/s]Extractor Predicting: 436it [04:46,  1.59it/s]Extractor Predicting: 437it [04:46,  1.62it/s]Extractor Predicting: 438it [04:47,  1.63it/s]Extractor Predicting: 439it [04:48,  1.63it/s]Extractor Predicting: 440it [04:48,  1.59it/s]Extractor Predicting: 441it [04:49,  1.61it/s]Extractor Predicting: 442it [04:49,  1.59it/s]Extractor Predicting: 443it [04:50,  1.58it/s]Extractor Predicting: 444it [04:51,  1.61it/s]Extractor Predicting: 445it [04:51,  1.62it/s]Extractor Predicting: 446it [04:52,  1.65it/s]Extractor Predicting: 447it [04:53,  1.62it/s]Extractor Predicting: 448it [04:53,  1.64it/s]Extractor Predicting: 449it [04:54,  1.65it/s]Extractor Predicting: 450it [04:54,  1.65it/s]Extractor Predicting: 451it [04:55,  1.64it/s]Extractor Predicting: 452it [04:56,  1.59it/s]Extractor Predicting: 453it [04:56,  1.59it/s]Extractor Predicting: 454it [04:57,  1.53it/s]Extractor Predicting: 455it [04:58,  1.58it/s]Extractor Predicting: 456it [04:58,  1.61it/s]Extractor Predicting: 457it [04:59,  1.61it/s]Extractor Predicting: 458it [04:59,  1.63it/s]Extractor Predicting: 459it [05:00,  1.61it/s]Extractor Predicting: 460it [05:01,  1.55it/s]Extractor Predicting: 461it [05:01,  1.53it/s]Extractor Predicting: 462it [05:02,  1.47it/s]Extractor Predicting: 463it [05:03,  1.48it/s]Extractor Predicting: 464it [05:03,  1.51it/s]Extractor Predicting: 465it [05:04,  1.52it/s]Extractor Predicting: 466it [05:05,  1.53it/s]Extractor Predicting: 467it [05:05,  1.47it/s]Extractor Predicting: 468it [05:06,  1.49it/s]Extractor Predicting: 469it [05:07,  1.51it/s]Extractor Predicting: 470it [05:07,  1.49it/s]Extractor Predicting: 471it [05:08,  1.46it/s]Extractor Predicting: 472it [05:09,  1.43it/s]Extractor Predicting: 473it [05:10,  1.47it/s]Extractor Predicting: 474it [05:10,  1.51it/s]Extractor Predicting: 475it [05:11,  1.52it/s]Extractor Predicting: 476it [05:11,  1.52it/s]Extractor Predicting: 477it [05:12,  1.46it/s]Extractor Predicting: 478it [05:13,  1.45it/s]Extractor Predicting: 479it [05:14,  1.44it/s]Extractor Predicting: 480it [05:14,  1.45it/s]Extractor Predicting: 481it [05:15,  1.47it/s]Extractor Predicting: 482it [05:16,  1.46it/s]Extractor Predicting: 483it [05:16,  1.50it/s]Extractor Predicting: 484it [05:17,  1.47it/s]Extractor Predicting: 485it [05:18,  1.52it/s]Extractor Predicting: 486it [05:18,  1.55it/s]Extractor Predicting: 487it [05:19,  1.53it/s]Extractor Predicting: 488it [05:20,  1.51it/s]Extractor Predicting: 489it [05:20,  1.49it/s]Extractor Predicting: 490it [05:21,  1.49it/s]Extractor Predicting: 491it [05:22,  1.48it/s]Extractor Predicting: 492it [05:22,  1.47it/s]Extractor Predicting: 493it [05:23,  1.48it/s]Extractor Predicting: 494it [05:24,  1.48it/s]Extractor Predicting: 495it [05:24,  1.53it/s]Extractor Predicting: 496it [05:25,  1.53it/s]Extractor Predicting: 496it [05:25,  1.52it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:43,252 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:43,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:43,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:43,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:43,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 18:18:44,258 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 18:18:44,259 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:18:44,912 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 18:18:46,039 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:18:46,039 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:49,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:49,204 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:49,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:49,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:18:49,205 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 18:18:50,224 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 18:18:50,225 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:18:50,882 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 18:18:51,153 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:18:51,154 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4421052631578947,
  "recall": 0.04939516129032258,
  "score": 0.08886202206437963,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 10529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.50it/s]Extractor Predicting: 5it [00:03,  1.47it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.42it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.45it/s]Extractor Predicting: 12it [00:08,  1.41it/s]Extractor Predicting: 13it [00:08,  1.44it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.44it/s]Extractor Predicting: 16it [00:11,  1.45it/s]Extractor Predicting: 17it [00:11,  1.36it/s]Extractor Predicting: 18it [00:12,  1.36it/s]Extractor Predicting: 19it [00:13,  1.34it/s]Extractor Predicting: 20it [00:14,  1.29it/s]Extractor Predicting: 21it [00:15,  1.21it/s]Extractor Predicting: 22it [00:15,  1.22it/s]Extractor Predicting: 23it [00:16,  1.24it/s]Extractor Predicting: 24it [00:17,  1.30it/s]Extractor Predicting: 25it [00:18,  1.35it/s]Extractor Predicting: 26it [00:18,  1.37it/s]Extractor Predicting: 27it [00:19,  1.37it/s]Extractor Predicting: 28it [00:20,  1.38it/s]Extractor Predicting: 29it [00:20,  1.37it/s]Extractor Predicting: 30it [00:21,  1.37it/s]Extractor Predicting: 31it [00:22,  1.38it/s]Extractor Predicting: 32it [00:23,  1.40it/s]Extractor Predicting: 33it [00:23,  1.44it/s]Extractor Predicting: 34it [00:24,  1.46it/s]Extractor Predicting: 35it [00:25,  1.43it/s]Extractor Predicting: 36it [00:25,  1.43it/s]Extractor Predicting: 37it [00:26,  1.45it/s]Extractor Predicting: 38it [00:27,  1.45it/s]Extractor Predicting: 39it [00:27,  1.47it/s]Extractor Predicting: 40it [00:28,  1.49it/s]Extractor Predicting: 41it [00:29,  1.48it/s]Extractor Predicting: 42it [00:29,  1.50it/s]Extractor Predicting: 43it [00:30,  1.50it/s]Extractor Predicting: 44it [00:31,  1.51it/s]Extractor Predicting: 45it [00:31,  1.49it/s]Extractor Predicting: 46it [00:32,  1.45it/s]Extractor Predicting: 47it [00:33,  1.46it/s]Extractor Predicting: 48it [00:33,  1.48it/s]Extractor Predicting: 49it [00:34,  1.49it/s]Extractor Predicting: 50it [00:35,  1.48it/s]Extractor Predicting: 51it [00:35,  1.48it/s]Extractor Predicting: 52it [00:36,  1.48it/s]Extractor Predicting: 53it [00:37,  1.50it/s]Extractor Predicting: 54it [00:37,  1.49it/s]Extractor Predicting: 55it [00:38,  1.50it/s]Extractor Predicting: 56it [00:39,  1.51it/s]Extractor Predicting: 57it [00:39,  1.53it/s]Extractor Predicting: 58it [00:40,  1.54it/s]Extractor Predicting: 59it [00:41,  1.53it/s]Extractor Predicting: 60it [00:42,  1.43it/s]Extractor Predicting: 61it [00:42,  1.48it/s]Extractor Predicting: 62it [00:43,  1.50it/s]Extractor Predicting: 63it [00:44,  1.46it/s]Extractor Predicting: 64it [00:44,  1.47it/s]Extractor Predicting: 65it [00:45,  1.41it/s]Extractor Predicting: 66it [00:46,  1.48it/s]Extractor Predicting: 67it [00:46,  1.56it/s]Extractor Predicting: 68it [00:47,  1.63it/s]Extractor Predicting: 69it [00:47,  1.71it/s]Extractor Predicting: 70it [00:48,  1.79it/s]Extractor Predicting: 71it [00:48,  1.81it/s]Extractor Predicting: 72it [00:49,  1.81it/s]Extractor Predicting: 73it [00:49,  1.84it/s]Extractor Predicting: 74it [00:50,  1.83it/s]Extractor Predicting: 75it [00:50,  1.82it/s]Extractor Predicting: 76it [00:51,  1.82it/s]Extractor Predicting: 77it [00:52,  1.79it/s]Extractor Predicting: 78it [00:52,  1.81it/s]Extractor Predicting: 79it [00:53,  1.85it/s]Extractor Predicting: 80it [00:53,  1.82it/s]Extractor Predicting: 81it [00:54,  1.82it/s]Extractor Predicting: 82it [00:54,  1.83it/s]Extractor Predicting: 83it [00:55,  1.85it/s]Extractor Predicting: 84it [00:55,  1.88it/s]Extractor Predicting: 85it [00:56,  1.88it/s]Extractor Predicting: 86it [00:56,  1.85it/s]Extractor Predicting: 87it [00:57,  1.91it/s]Extractor Predicting: 88it [00:57,  1.83it/s]Extractor Predicting: 89it [00:58,  1.80it/s]Extractor Predicting: 90it [00:59,  1.75it/s]Extractor Predicting: 91it [00:59,  1.77it/s]Extractor Predicting: 92it [01:00,  1.80it/s]Extractor Predicting: 93it [01:00,  1.82it/s]Extractor Predicting: 94it [01:01,  1.83it/s]Extractor Predicting: 95it [01:01,  1.84it/s]Extractor Predicting: 96it [01:02,  1.68it/s]Extractor Predicting: 97it [01:03,  1.63it/s]Extractor Predicting: 98it [01:03,  1.58it/s]Extractor Predicting: 99it [01:04,  1.52it/s]Extractor Predicting: 100it [01:05,  1.48it/s]Extractor Predicting: 101it [01:06,  1.44it/s]Extractor Predicting: 102it [01:06,  1.43it/s]Extractor Predicting: 103it [01:07,  1.44it/s]Extractor Predicting: 104it [01:08,  1.45it/s]Extractor Predicting: 105it [01:08,  1.44it/s]Extractor Predicting: 106it [01:09,  1.39it/s]Extractor Predicting: 107it [01:10,  1.41it/s]Extractor Predicting: 108it [01:11,  1.29it/s]Extractor Predicting: 109it [01:11,  1.33it/s]Extractor Predicting: 110it [01:12,  1.31it/s]Extractor Predicting: 111it [01:13,  1.35it/s]Extractor Predicting: 112it [01:14,  1.39it/s]Extractor Predicting: 113it [01:14,  1.44it/s]Extractor Predicting: 114it [01:15,  1.50it/s]Extractor Predicting: 115it [01:16,  1.48it/s]Extractor Predicting: 116it [01:16,  1.52it/s]Extractor Predicting: 117it [01:17,  1.54it/s]Extractor Predicting: 118it [01:17,  1.55it/s]Extractor Predicting: 119it [01:18,  1.54it/s]Extractor Predicting: 120it [01:19,  1.58it/s]Extractor Predicting: 121it [01:19,  1.58it/s]Extractor Predicting: 122it [01:20,  1.61it/s]Extractor Predicting: 123it [01:21,  1.59it/s]Extractor Predicting: 124it [01:21,  1.55it/s]Extractor Predicting: 125it [01:22,  1.54it/s]Extractor Predicting: 126it [01:22,  1.56it/s]Extractor Predicting: 127it [01:23,  1.54it/s]Extractor Predicting: 128it [01:24,  1.51it/s]Extractor Predicting: 129it [01:25,  1.50it/s]Extractor Predicting: 130it [01:25,  1.48it/s]Extractor Predicting: 131it [01:26,  1.47it/s]Extractor Predicting: 132it [01:27,  1.43it/s]Extractor Predicting: 133it [01:27,  1.50it/s]Extractor Predicting: 133it [01:27,  1.52it/s]
[INFO|configuration_utils.py:515] 2023-08-29 18:20:23,432 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 18:20:23,433 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 18:20:23,488 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 18:20:23,489 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 18:20:23,519 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 18:20:39,886 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 18:20:39,915 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 18:20:40,071 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 18:20:40,071 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 18:20:40,175 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:20:40,264 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:20:40,264 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:20:40,264 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:20:40,264 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:20:40,264 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 18:20:40,264 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8365217391304348,
  "recall": 0.0637508283631544,
  "score": 0.11847290640394088,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 18:20:40,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:41,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:42,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:42,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:43,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:43,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:44,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:45,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:46,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:46,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:47,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:48,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:48,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:49,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:50,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:50,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:51,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:52,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:52,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:53,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:13<04:17, 13.54s/it][WARNING|generation_utils.py:914] 2023-08-29 18:20:54,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:54,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:55,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:56,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:56,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:57,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:57,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:58,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:59,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:20:59,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:00,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:01,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:01,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:02,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:02,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:03,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:04,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:05,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:05,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:06,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:06,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:26<04:02, 13.47s/it][WARNING|generation_utils.py:914] 2023-08-29 18:21:07,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:08,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:09,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:10,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:11,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:11,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:12,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:13,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:14,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:15,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:15,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:16,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:17,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:18,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:18,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:19,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:20,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:21,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:21,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:22,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:23,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:43<04:11, 14.80s/it][WARNING|generation_utils.py:914] 2023-08-29 18:21:24,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:24,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:25,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:26,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:26,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:27,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:28,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:28,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:29,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:30,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:31,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:31,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:32,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:32,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:33,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:34,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:34,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:35,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:36,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:37,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:57<03:50, 14.38s/it][WARNING|generation_utils.py:914] 2023-08-29 18:21:37,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:38,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:39,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:39,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:40,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:41,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:42,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:42,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:43,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:44,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:44,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:45,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:46,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:47,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:47,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:48,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:49,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:50,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:50,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:51,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:52,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:52,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:12<03:42, 14.85s/it][WARNING|generation_utils.py:914] 2023-08-29 18:21:53,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:54,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:54,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:55,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:56,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:56,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:57,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:58,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:59,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:21:59,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:00,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:01,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:01,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:02,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:03,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:03,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:04,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:05,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:06,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:06,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:07,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:27<03:25, 14.70s/it][WARNING|generation_utils.py:914] 2023-08-29 18:22:07,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:08,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:09,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:09,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:10,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:11,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:12,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:12,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:13,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:14,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:14,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:15,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:16,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:16,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:17,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:18,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:18,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:19,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:20,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:20,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:40<03:07, 14.39s/it][WARNING|generation_utils.py:914] 2023-08-29 18:22:21,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:22,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:23,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:23,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:24,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:25,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:25,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:26,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:27,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:28,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:28,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:29,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:30,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:30,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:31,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:32,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:32,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:33,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:34,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:34,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:35,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:55<02:53, 14.46s/it][WARNING|generation_utils.py:914] 2023-08-29 18:22:36,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:37,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:37,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:38,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:39,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:40,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:40,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:41,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:41,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:42,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:43,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:44,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:44,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:45,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:46,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:47,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:47,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:48,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:49,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:49,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:50,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:51,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:52,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:12<02:46, 15.12s/it][WARNING|generation_utils.py:914] 2023-08-29 18:22:52,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:53,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:54,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:54,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:55,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:56,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:56,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:57,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:58,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:58,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:22:59,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:00,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:01,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:01,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:02,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:03,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:04,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:04,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:05,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:06,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:07,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:27<02:30, 15.06s/it][WARNING|generation_utils.py:914] 2023-08-29 18:23:07,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:08,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:08,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:09,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:10,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:10,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:11,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:12,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:12,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:13,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:13,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:14,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:15,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:15,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:16,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:16,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:17,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:17,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:18,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:19,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:19,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:39<02:08, 14.31s/it][WARNING|generation_utils.py:914] 2023-08-29 18:23:20,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:21,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:21,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:22,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:23,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:23,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:24,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:24,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:25,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:26,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:26,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:27,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:28,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:28,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:29,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:29,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:30,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:30,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:31,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:32,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:51<01:49, 13.72s/it][WARNING|generation_utils.py:914] 2023-08-29 18:23:32,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:33,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:34,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:34,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:35,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:36,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:36,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:37,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:38,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:38,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:39,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:39,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:40,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:41,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:42,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:42,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:43,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:44,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:44,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:45,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:46,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:06<01:36, 13.82s/it][WARNING|generation_utils.py:914] 2023-08-29 18:23:46,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:47,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:47,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:48,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:49,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:49,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:50,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:51,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:51,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:52,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:52,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:53,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:54,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:54,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:55,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:55,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:56,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:57,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:57,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:58,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:23:59,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:19<01:21, 13.57s/it][WARNING|generation_utils.py:914] 2023-08-29 18:23:59,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:00,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:01,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:02,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:02,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:03,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:04,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:05,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:05,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:06,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:07,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:08,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:08,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:09,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:10,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:11,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:11,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:12,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:13,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:13,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:14,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:34<01:10, 14.15s/it][WARNING|generation_utils.py:914] 2023-08-29 18:24:15,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:16,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:17,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:18,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:18,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:19,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:20,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:21,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:21,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:22,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:23,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:23,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:24,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:25,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:26,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:26,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:27,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:28,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:29,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:30,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:30,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:50<00:59, 14.82s/it][WARNING|generation_utils.py:914] 2023-08-29 18:24:31,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:32,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:33,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:33,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:34,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:35,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:36,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:36,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:37,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:38,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:38,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:39,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:40,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:40,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:41,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:42,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:43,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:43,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:44,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:44,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:45,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:05<00:44, 14.82s/it][WARNING|generation_utils.py:914] 2023-08-29 18:24:46,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:47,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:47,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:48,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:48,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:49,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:49,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:50,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:51,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:51,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:52,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:52,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:53,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:54,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:54,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:55,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:56,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:56,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:57,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:57,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:58,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:18<00:28, 14.16s/it][WARNING|generation_utils.py:914] 2023-08-29 18:24:59,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:24:59,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:01,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:01,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:02,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:03,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:03,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:04,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:05,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:05,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:06,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:07,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:07,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:08,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:09,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:09,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:10,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:11,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:11,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:12,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:32<00:14, 14.11s/it][WARNING|generation_utils.py:914] 2023-08-29 18:25:13,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:14,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:15,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:15,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:16,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:17,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:18,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:19,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:20,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:21,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:21,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:22,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:23,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:24,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:25,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:26,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:27,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:28,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:28,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:29,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 18:25:30,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:50<00:00, 15.44s/it]Generating: 100%|██████████| 20/20 [04:50<00:00, 14.55s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:40,948 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:40,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:40,976 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:40,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:40,977 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 18:25:41,815 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 18:25:41,816 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:25:42,649 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 18:25:43,763 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:25:43,763 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:46,822 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:46,850 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:46,851 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:46,851 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:25:46,851 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 18:25:47,681 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 18:25:47,683 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:25:48,422 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 18:25:48,610 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:25:48,611 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 395, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 486, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 607, 'raw': 640}
{'prompt': 'Relation : characters .', 'success_rate': 0.9484375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 534, 'raw': 576}
{'target': 600, 'success': 561, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 619, 'raw': 672}
{'prompt': 'Relation : from narrative universe .', 'success_rate': 0.9211309523809523, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : lowest point .', 'success_rate': 0.9360119047619048, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 366, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 484, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 604, 'raw': 640}
{'prompt': 'Relation : manufacturer .', 'success_rate': 0.94375, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 435, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 541, 'raw': 608}
{'target': 600, 'success': 570, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 627, 'raw': 704}
{'prompt': 'Relation : work location .', 'success_rate': 0.890625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 558, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : cast member .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.946875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : has quality .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : instrument . Context : Later in the year he recorded his first solo opera , The Phantom of the Opera at the Royal Academy of Music in London . Head Entity : The Phantom of the Opera , Tail Entity : piano .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 543, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : instrument .', 'success_rate': 0.845108695652174, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 468, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9151785714285714, 'errors': {'', "('2 . 5 m', 'located in or next to body of water', '', 'It has a diameter of approximately 2 . 5 m ( 5 . 7 ft ) , and a maximum depth of up to 1 . 25 meters ( 12 . 6 m ) .')"}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 493, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : located on astronomical body .', 'success_rate': 0.959375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 603, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.8973214285714286, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.8943452380952381, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : opposite of .', 'success_rate': 0.9330357142857143, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 504, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : shares border with .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 515, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : subsidiary .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : twinned administrative body .', 'success_rate': 0.9360119047619048, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 10863
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10963, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.56it/s]Extractor Estimating: 2it [00:01,  1.49it/s]Extractor Estimating: 3it [00:01,  1.60it/s]Extractor Estimating: 4it [00:02,  1.67it/s]Extractor Estimating: 5it [00:03,  1.67it/s]Extractor Estimating: 6it [00:03,  1.65it/s]Extractor Estimating: 7it [00:04,  1.62it/s]Extractor Estimating: 8it [00:04,  1.64it/s]Extractor Estimating: 9it [00:05,  1.66it/s]Extractor Estimating: 10it [00:06,  1.66it/s]Extractor Estimating: 11it [00:06,  1.55it/s]Extractor Estimating: 12it [00:07,  1.65it/s]Extractor Estimating: 13it [00:08,  1.61it/s]Extractor Estimating: 14it [00:08,  1.66it/s]Extractor Estimating: 15it [00:09,  1.67it/s]Extractor Estimating: 16it [00:09,  1.70it/s]Extractor Estimating: 17it [00:10,  1.67it/s]Extractor Estimating: 18it [00:10,  1.69it/s]Extractor Estimating: 19it [00:11,  1.72it/s]Extractor Estimating: 20it [00:12,  1.69it/s]Extractor Estimating: 21it [00:12,  1.65it/s]Extractor Estimating: 22it [00:13,  1.60it/s]Extractor Estimating: 23it [00:13,  1.66it/s]Extractor Estimating: 24it [00:14,  1.66it/s]Extractor Estimating: 25it [00:15,  1.64it/s]Extractor Estimating: 26it [00:15,  1.60it/s]Extractor Estimating: 27it [00:16,  1.65it/s]Extractor Estimating: 28it [00:17,  1.60it/s]Extractor Estimating: 29it [00:17,  1.69it/s]Extractor Estimating: 30it [00:18,  1.70it/s]Extractor Estimating: 31it [00:18,  1.66it/s]Extractor Estimating: 32it [00:19,  1.62it/s]Extractor Estimating: 33it [00:20,  1.66it/s]Extractor Estimating: 34it [00:20,  1.69it/s]Extractor Estimating: 35it [00:21,  1.70it/s]Extractor Estimating: 36it [00:21,  1.68it/s]Extractor Estimating: 37it [00:22,  1.71it/s]Extractor Estimating: 38it [00:22,  1.67it/s]Extractor Estimating: 39it [00:23,  1.67it/s]Extractor Estimating: 40it [00:24,  1.69it/s]Extractor Estimating: 41it [00:24,  1.73it/s]Extractor Estimating: 42it [00:25,  1.73it/s]Extractor Estimating: 43it [00:25,  1.62it/s]Extractor Estimating: 44it [00:26,  1.61it/s]Extractor Estimating: 45it [00:27,  1.66it/s]Extractor Estimating: 46it [00:27,  1.72it/s]Extractor Estimating: 47it [00:28,  1.76it/s]Extractor Estimating: 48it [00:28,  1.74it/s]Extractor Estimating: 49it [00:29,  1.74it/s]Extractor Estimating: 50it [00:30,  1.66it/s]Extractor Estimating: 51it [00:30,  1.66it/s]Extractor Estimating: 52it [00:31,  1.61it/s]Extractor Estimating: 53it [00:31,  1.63it/s]Extractor Estimating: 54it [00:32,  1.61it/s]Extractor Estimating: 55it [00:33,  1.54it/s]Extractor Estimating: 56it [00:33,  1.58it/s]Extractor Estimating: 57it [00:34,  1.59it/s]Extractor Estimating: 58it [00:35,  1.54it/s]Extractor Estimating: 59it [00:35,  1.54it/s]Extractor Estimating: 60it [00:36,  1.41it/s]Extractor Estimating: 61it [00:37,  1.49it/s]Extractor Estimating: 62it [00:37,  1.51it/s]Extractor Estimating: 63it [00:38,  1.54it/s]Extractor Estimating: 64it [00:39,  1.57it/s]Extractor Estimating: 65it [00:39,  1.50it/s]Extractor Estimating: 66it [00:40,  1.49it/s]Extractor Estimating: 67it [00:41,  1.57it/s]Extractor Estimating: 68it [00:41,  1.56it/s]Extractor Estimating: 69it [00:42,  1.62it/s]Extractor Estimating: 70it [00:42,  1.61it/s]Extractor Estimating: 71it [00:43,  1.54it/s]Extractor Estimating: 72it [00:44,  1.55it/s]Extractor Estimating: 73it [00:44,  1.55it/s]Extractor Estimating: 74it [00:45,  1.57it/s]Extractor Estimating: 75it [00:46,  1.54it/s]Extractor Estimating: 76it [00:46,  1.55it/s]Extractor Estimating: 77it [00:47,  1.54it/s]Extractor Estimating: 78it [00:48,  1.62it/s]Extractor Estimating: 79it [00:48,  1.71it/s]Extractor Estimating: 80it [00:49,  1.77it/s]Extractor Estimating: 81it [00:49,  1.79it/s]Extractor Estimating: 82it [00:50,  1.71it/s]Extractor Estimating: 83it [00:50,  1.71it/s]Extractor Estimating: 84it [00:51,  1.75it/s]Extractor Estimating: 85it [00:52,  1.70it/s]Extractor Estimating: 86it [00:52,  1.68it/s]Extractor Estimating: 87it [00:53,  1.62it/s]Extractor Estimating: 88it [00:53,  1.71it/s]Extractor Estimating: 89it [00:54,  1.79it/s]Extractor Estimating: 90it [00:54,  1.83it/s]Extractor Estimating: 91it [00:55,  1.82it/s]Extractor Estimating: 92it [00:56,  1.76it/s]Extractor Estimating: 93it [00:56,  1.74it/s]Extractor Estimating: 94it [00:57,  1.74it/s]Extractor Estimating: 95it [00:57,  1.79it/s]Extractor Estimating: 96it [00:58,  1.79it/s]Extractor Estimating: 97it [00:58,  1.69it/s]Extractor Estimating: 98it [00:59,  1.72it/s]Extractor Estimating: 99it [01:00,  1.72it/s]Extractor Estimating: 100it [01:00,  1.66it/s]Extractor Estimating: 101it [01:01,  1.67it/s]Extractor Estimating: 102it [01:02,  1.58it/s]Extractor Estimating: 103it [01:02,  1.56it/s]Extractor Estimating: 104it [01:03,  1.56it/s]Extractor Estimating: 105it [01:03,  1.61it/s]Extractor Estimating: 106it [01:04,  1.62it/s]Extractor Estimating: 107it [01:05,  1.59it/s]Extractor Estimating: 108it [01:05,  1.61it/s]Extractor Estimating: 109it [01:06,  1.52it/s]Extractor Estimating: 110it [01:07,  1.53it/s]Extractor Estimating: 111it [01:07,  1.58it/s]Extractor Estimating: 112it [01:08,  1.59it/s]Extractor Estimating: 113it [01:08,  1.60it/s]Extractor Estimating: 114it [01:09,  1.54it/s]Extractor Estimating: 115it [01:10,  1.52it/s]Extractor Estimating: 116it [01:10,  1.58it/s]Extractor Estimating: 117it [01:11,  1.57it/s]Extractor Estimating: 118it [01:12,  1.58it/s]Extractor Estimating: 119it [01:12,  1.55it/s]Extractor Estimating: 120it [01:13,  1.56it/s]Extractor Estimating: 121it [01:14,  1.41it/s]Extractor Estimating: 122it [01:14,  1.51it/s]Extractor Estimating: 123it [01:15,  1.57it/s]Extractor Estimating: 124it [01:16,  1.60it/s]Extractor Estimating: 125it [01:16,  1.62it/s]Extractor Estimating: 126it [01:17,  1.58it/s]Extractor Estimating: 127it [01:17,  1.61it/s]Extractor Estimating: 128it [01:18,  1.63it/s]Extractor Estimating: 129it [01:19,  1.68it/s]Extractor Estimating: 130it [01:19,  1.65it/s]Extractor Estimating: 131it [01:20,  1.60it/s]Extractor Estimating: 132it [01:21,  1.63it/s]Extractor Estimating: 133it [01:21,  1.61it/s]Extractor Estimating: 134it [01:22,  1.61it/s]Extractor Estimating: 135it [01:22,  1.65it/s]Extractor Estimating: 136it [01:23,  1.59it/s]Extractor Estimating: 137it [01:24,  1.60it/s]Extractor Estimating: 138it [01:24,  1.60it/s]Extractor Estimating: 139it [01:25,  1.67it/s]Extractor Estimating: 140it [01:25,  1.69it/s]Extractor Estimating: 141it [01:26,  1.66it/s]Extractor Estimating: 142it [01:27,  1.65it/s]Extractor Estimating: 143it [01:27,  1.68it/s]Extractor Estimating: 144it [01:28,  1.68it/s]Extractor Estimating: 145it [01:28,  1.63it/s]Extractor Estimating: 146it [01:29,  1.63it/s]Extractor Estimating: 147it [01:30,  1.66it/s]Extractor Estimating: 148it [01:30,  1.66it/s]Extractor Estimating: 149it [01:31,  1.68it/s]Extractor Estimating: 150it [01:31,  1.69it/s]Extractor Estimating: 151it [01:32,  1.64it/s]Extractor Estimating: 152it [01:33,  1.65it/s]Extractor Estimating: 153it [01:33,  1.67it/s]Extractor Estimating: 154it [01:34,  1.68it/s]Extractor Estimating: 155it [01:34,  1.64it/s]Extractor Estimating: 156it [01:35,  1.65it/s]Extractor Estimating: 157it [01:36,  1.61it/s]Extractor Estimating: 158it [01:36,  1.61it/s]Extractor Estimating: 159it [01:37,  1.65it/s]Extractor Estimating: 160it [01:38,  1.64it/s]Extractor Estimating: 161it [01:38,  1.64it/s]Extractor Estimating: 162it [01:39,  1.64it/s]Extractor Estimating: 163it [01:39,  1.65it/s]Extractor Estimating: 164it [01:40,  1.64it/s]Extractor Estimating: 165it [01:41,  1.62it/s]Extractor Estimating: 166it [01:41,  1.63it/s]Extractor Estimating: 167it [01:42,  1.67it/s]Extractor Estimating: 168it [01:42,  1.67it/s]Extractor Estimating: 169it [01:43,  1.66it/s]Extractor Estimating: 170it [01:44,  1.69it/s]Extractor Estimating: 171it [01:44,  1.63it/s]Extractor Estimating: 172it [01:45,  1.61it/s]Extractor Estimating: 173it [01:46,  1.58it/s]Extractor Estimating: 174it [01:46,  1.61it/s]Extractor Estimating: 175it [01:47,  1.62it/s]Extractor Estimating: 176it [01:47,  1.62it/s]Extractor Estimating: 177it [01:48,  1.65it/s]Extractor Estimating: 178it [01:49,  1.65it/s]Extractor Estimating: 179it [01:49,  1.63it/s]Extractor Estimating: 180it [01:50,  1.62it/s]Extractor Estimating: 181it [01:50,  1.61it/s]Extractor Estimating: 182it [01:51,  1.59it/s]Extractor Estimating: 183it [01:52,  1.56it/s]Extractor Estimating: 184it [01:52,  1.58it/s]Extractor Estimating: 185it [01:53,  1.58it/s]Extractor Estimating: 186it [01:54,  1.56it/s]Extractor Estimating: 187it [01:54,  1.59it/s]Extractor Estimating: 188it [01:55,  1.60it/s]Extractor Estimating: 189it [01:55,  1.61it/s]Extractor Estimating: 190it [01:56,  1.61it/s]Extractor Estimating: 191it [01:57,  1.43it/s]Extractor Estimating: 192it [01:58,  1.44it/s]Extractor Estimating: 193it [01:58,  1.46it/s]Extractor Estimating: 194it [01:59,  1.52it/s]Extractor Estimating: 195it [02:00,  1.55it/s]Extractor Estimating: 196it [02:00,  1.55it/s]Extractor Estimating: 197it [02:01,  1.59it/s]Extractor Estimating: 198it [02:01,  1.60it/s]Extractor Estimating: 199it [02:02,  1.63it/s]Extractor Estimating: 200it [02:03,  1.61it/s]Extractor Estimating: 201it [02:03,  1.51it/s]Extractor Estimating: 202it [02:04,  1.58it/s]Extractor Estimating: 203it [02:05,  1.52it/s]Extractor Estimating: 204it [02:05,  1.53it/s]Extractor Estimating: 205it [02:06,  1.54it/s]Extractor Estimating: 206it [02:07,  1.50it/s]Extractor Estimating: 207it [02:07,  1.54it/s]Extractor Estimating: 208it [02:08,  1.58it/s]Extractor Estimating: 209it [02:08,  1.57it/s]Extractor Estimating: 210it [02:09,  1.57it/s]Extractor Estimating: 211it [02:10,  1.58it/s]Extractor Estimating: 212it [02:10,  1.58it/s]Extractor Estimating: 213it [02:11,  1.59it/s]Extractor Estimating: 214it [02:12,  1.56it/s]Extractor Estimating: 215it [02:12,  1.52it/s]Extractor Estimating: 216it [02:13,  1.53it/s]Extractor Estimating: 217it [02:14,  1.54it/s]Extractor Estimating: 218it [02:14,  1.57it/s]Extractor Estimating: 219it [02:15,  1.57it/s]Extractor Estimating: 220it [02:16,  1.53it/s]Extractor Estimating: 221it [02:16,  1.52it/s]Extractor Estimating: 222it [02:17,  1.54it/s]Extractor Estimating: 223it [02:17,  1.59it/s]Extractor Estimating: 224it [02:18,  1.58it/s]Extractor Estimating: 225it [02:19,  1.50it/s]Extractor Estimating: 226it [02:20,  1.44it/s]Extractor Estimating: 227it [02:20,  1.43it/s]Extractor Estimating: 228it [02:21,  1.44it/s]Extractor Estimating: 229it [02:22,  1.44it/s]Extractor Estimating: 230it [02:23,  1.35it/s]Extractor Estimating: 231it [02:23,  1.38it/s]Extractor Estimating: 232it [02:24,  1.40it/s]Extractor Estimating: 233it [02:25,  1.42it/s]Extractor Estimating: 234it [02:25,  1.45it/s]Extractor Estimating: 235it [02:26,  1.41it/s]Extractor Estimating: 236it [02:27,  1.39it/s]Extractor Estimating: 237it [02:27,  1.40it/s]Extractor Estimating: 238it [02:28,  1.37it/s]Extractor Estimating: 239it [02:29,  1.42it/s]Extractor Estimating: 240it [02:30,  1.34it/s]Extractor Estimating: 241it [02:30,  1.35it/s]Extractor Estimating: 242it [02:31,  1.40it/s]Extractor Estimating: 243it [02:32,  1.43it/s]Extractor Estimating: 244it [02:32,  1.40it/s]Extractor Estimating: 245it [02:33,  1.38it/s]Extractor Estimating: 246it [02:34,  1.38it/s]Extractor Estimating: 247it [02:35,  1.35it/s]Extractor Estimating: 248it [02:35,  1.43it/s]Extractor Estimating: 249it [02:36,  1.43it/s]Extractor Estimating: 250it [02:37,  1.47it/s]Extractor Estimating: 251it [02:37,  1.65it/s]Extractor Estimating: 252it [02:38,  1.75it/s]Extractor Estimating: 253it [02:38,  1.83it/s]Extractor Estimating: 254it [02:39,  1.88it/s]Extractor Estimating: 255it [02:39,  1.89it/s]Extractor Estimating: 256it [02:40,  1.84it/s]Extractor Estimating: 257it [02:40,  1.91it/s]Extractor Estimating: 258it [02:41,  1.89it/s]Extractor Estimating: 259it [02:41,  1.93it/s]Extractor Estimating: 260it [02:42,  1.98it/s]Extractor Estimating: 261it [02:42,  2.01it/s]Extractor Estimating: 262it [02:43,  2.02it/s]Extractor Estimating: 263it [02:43,  1.97it/s]Extractor Estimating: 264it [02:44,  2.01it/s]Extractor Estimating: 265it [02:44,  2.10it/s]Extractor Estimating: 266it [02:45,  2.05it/s]Extractor Estimating: 267it [02:45,  1.99it/s]Extractor Estimating: 268it [02:46,  1.89it/s]Extractor Estimating: 269it [02:46,  1.97it/s]Extractor Estimating: 270it [02:47,  2.02it/s]Extractor Estimating: 271it [02:47,  2.08it/s]Extractor Estimating: 272it [02:48,  2.01it/s]Extractor Estimating: 273it [02:48,  1.83it/s]Extractor Estimating: 274it [02:49,  1.80it/s]Extractor Estimating: 275it [02:50,  1.70it/s]Extractor Estimating: 276it [02:50,  1.50it/s]Extractor Estimating: 277it [02:51,  1.63it/s]Extractor Estimating: 278it [02:51,  1.68it/s]Extractor Estimating: 279it [02:52,  1.67it/s]Extractor Estimating: 280it [02:53,  1.67it/s]Extractor Estimating: 281it [02:53,  1.71it/s]Extractor Estimating: 282it [02:54,  1.71it/s]Extractor Estimating: 283it [02:54,  1.69it/s]Extractor Estimating: 284it [02:55,  1.78it/s]Extractor Estimating: 285it [02:55,  1.74it/s]Extractor Estimating: 286it [02:56,  1.79it/s]Extractor Estimating: 287it [02:56,  1.85it/s]Extractor Estimating: 288it [02:57,  1.81it/s]Extractor Estimating: 289it [02:58,  1.78it/s]Extractor Estimating: 290it [02:58,  1.86it/s]Extractor Estimating: 291it [02:59,  1.85it/s]Extractor Estimating: 292it [02:59,  1.92it/s]Extractor Estimating: 293it [03:00,  1.91it/s]Extractor Estimating: 294it [03:00,  1.91it/s]Extractor Estimating: 295it [03:01,  1.89it/s]Extractor Estimating: 296it [03:01,  1.92it/s]Extractor Estimating: 297it [03:02,  1.85it/s]Extractor Estimating: 298it [03:02,  1.75it/s]Extractor Estimating: 299it [03:03,  1.73it/s]Extractor Estimating: 300it [03:04,  1.71it/s]Extractor Estimating: 301it [03:04,  1.68it/s]Extractor Estimating: 302it [03:05,  1.60it/s]Extractor Estimating: 303it [03:06,  1.66it/s]Extractor Estimating: 304it [03:06,  1.66it/s]Extractor Estimating: 305it [03:07,  1.63it/s]Extractor Estimating: 306it [03:07,  1.66it/s]Extractor Estimating: 307it [03:08,  1.63it/s]Extractor Estimating: 308it [03:09,  1.63it/s]Extractor Estimating: 309it [03:09,  1.63it/s]Extractor Estimating: 310it [03:10,  1.66it/s]Extractor Estimating: 311it [03:10,  1.65it/s]Extractor Estimating: 312it [03:11,  1.62it/s]Extractor Estimating: 313it [03:12,  1.67it/s]Extractor Estimating: 314it [03:12,  1.63it/s]Extractor Estimating: 315it [03:13,  1.63it/s]Extractor Estimating: 316it [03:13,  1.64it/s]Extractor Estimating: 317it [03:14,  1.67it/s]Extractor Estimating: 318it [03:15,  1.67it/s]Extractor Estimating: 319it [03:15,  1.68it/s]Extractor Estimating: 320it [03:16,  1.70it/s]Extractor Estimating: 321it [03:16,  1.65it/s]Extractor Estimating: 322it [03:17,  1.65it/s]Extractor Estimating: 323it [03:18,  1.64it/s]Extractor Estimating: 324it [03:18,  1.66it/s]Extractor Estimating: 325it [03:19,  1.68it/s]Extractor Estimating: 326it [03:19,  1.65it/s]Extractor Estimating: 327it [03:20,  1.74it/s]Extractor Estimating: 328it [03:21,  1.70it/s]Extractor Estimating: 329it [03:21,  1.73it/s]Extractor Estimating: 330it [03:22,  1.73it/s]Extractor Estimating: 331it [03:22,  1.75it/s]Extractor Estimating: 332it [03:23,  1.70it/s]Extractor Estimating: 333it [03:23,  1.73it/s]Extractor Estimating: 334it [03:24,  1.74it/s]Extractor Estimating: 335it [03:25,  1.70it/s]Extractor Estimating: 336it [03:25,  1.72it/s]Extractor Estimating: 337it [03:26,  1.78it/s]Extractor Estimating: 338it [03:26,  1.73it/s]Extractor Estimating: 339it [03:27,  1.74it/s]Extractor Estimating: 340it [03:27,  1.74it/s]Extractor Estimating: 341it [03:28,  1.74it/s]Extractor Estimating: 342it [03:29,  1.80it/s]Extractor Estimating: 343it [03:29,  1.81it/s]Extractor Estimating: 344it [03:30,  1.70it/s]Extractor Estimating: 345it [03:30,  1.72it/s]Extractor Estimating: 346it [03:31,  1.75it/s]Extractor Estimating: 347it [03:32,  1.70it/s]Extractor Estimating: 348it [03:32,  1.65it/s]Extractor Estimating: 349it [03:33,  1.66it/s]Extractor Estimating: 350it [03:33,  1.69it/s]Extractor Estimating: 351it [03:34,  1.63it/s]Extractor Estimating: 352it [03:35,  1.67it/s]Extractor Estimating: 353it [03:35,  1.73it/s]Extractor Estimating: 354it [03:36,  1.68it/s]Extractor Estimating: 355it [03:36,  1.67it/s]Extractor Estimating: 356it [03:37,  1.67it/s]Extractor Estimating: 357it [03:38,  1.64it/s]Extractor Estimating: 358it [03:38,  1.69it/s]Extractor Estimating: 359it [03:39,  1.67it/s]Extractor Estimating: 360it [03:39,  1.64it/s]Extractor Estimating: 361it [03:40,  1.65it/s]Extractor Estimating: 362it [03:41,  1.67it/s]Extractor Estimating: 363it [03:41,  1.52it/s]Extractor Estimating: 364it [03:42,  1.58it/s]Extractor Estimating: 365it [03:43,  1.53it/s]Extractor Estimating: 366it [03:43,  1.60it/s]Extractor Estimating: 367it [03:44,  1.59it/s]Extractor Estimating: 368it [03:44,  1.63it/s]Extractor Estimating: 369it [03:45,  1.61it/s]Extractor Estimating: 370it [03:46,  1.61it/s]Extractor Estimating: 371it [03:46,  1.65it/s]Extractor Estimating: 372it [03:47,  1.65it/s]Extractor Estimating: 373it [03:47,  1.65it/s]Extractor Estimating: 374it [03:48,  1.69it/s]Extractor Estimating: 375it [03:49,  1.73it/s]Extractor Estimating: 376it [03:49,  1.74it/s]Extractor Estimating: 377it [03:50,  1.75it/s]Extractor Estimating: 378it [03:50,  1.78it/s]Extractor Estimating: 379it [03:51,  1.73it/s]Extractor Estimating: 380it [03:51,  1.74it/s]Extractor Estimating: 381it [03:52,  1.79it/s]Extractor Estimating: 382it [03:52,  1.79it/s]Extractor Estimating: 383it [03:53,  1.76it/s]Extractor Estimating: 384it [03:54,  1.76it/s]Extractor Estimating: 385it [03:54,  1.76it/s]Extractor Estimating: 386it [03:55,  1.76it/s]Extractor Estimating: 387it [03:55,  1.76it/s]Extractor Estimating: 388it [03:56,  1.71it/s]Extractor Estimating: 389it [03:57,  1.70it/s]Extractor Estimating: 390it [03:57,  1.68it/s]Extractor Estimating: 391it [03:58,  1.63it/s]Extractor Estimating: 392it [03:58,  1.68it/s]Extractor Estimating: 393it [03:59,  1.76it/s]Extractor Estimating: 394it [04:00,  1.60it/s]Extractor Estimating: 395it [04:00,  1.64it/s]Extractor Estimating: 396it [04:01,  1.65it/s]Extractor Estimating: 397it [04:01,  1.64it/s]Extractor Estimating: 398it [04:02,  1.63it/s]Extractor Estimating: 399it [04:03,  1.64it/s]Extractor Estimating: 400it [04:03,  1.59it/s]Extractor Estimating: 401it [04:04,  1.51it/s]Extractor Estimating: 402it [04:05,  1.55it/s]Extractor Estimating: 403it [04:05,  1.63it/s]Extractor Estimating: 404it [04:06,  1.63it/s]Extractor Estimating: 405it [04:06,  1.63it/s]Extractor Estimating: 406it [04:07,  1.64it/s]Extractor Estimating: 407it [04:08,  1.59it/s]Extractor Estimating: 408it [04:08,  1.63it/s]Extractor Estimating: 409it [04:09,  1.64it/s]Extractor Estimating: 410it [04:10,  1.63it/s]Extractor Estimating: 411it [04:10,  1.66it/s]Extractor Estimating: 412it [04:11,  1.70it/s]Extractor Estimating: 413it [04:11,  1.72it/s]Extractor Estimating: 414it [04:12,  1.73it/s]Extractor Estimating: 415it [04:12,  1.68it/s]Extractor Estimating: 416it [04:13,  1.68it/s]Extractor Estimating: 417it [04:14,  1.61it/s]Extractor Estimating: 418it [04:14,  1.62it/s]Extractor Estimating: 419it [04:15,  1.64it/s]Extractor Estimating: 420it [04:15,  1.70it/s]Extractor Estimating: 421it [04:16,  1.68it/s]Extractor Estimating: 422it [04:17,  1.67it/s]Extractor Estimating: 423it [04:17,  1.61it/s]Extractor Estimating: 424it [04:18,  1.71it/s]Extractor Estimating: 425it [04:18,  1.82it/s]Extractor Estimating: 426it [04:19,  1.87it/s]Extractor Estimating: 427it [04:19,  1.97it/s]Extractor Estimating: 428it [04:20,  2.04it/s]Extractor Estimating: 429it [04:20,  2.11it/s]Extractor Estimating: 430it [04:21,  2.16it/s]Extractor Estimating: 431it [04:21,  1.99it/s]Extractor Estimating: 432it [04:22,  2.00it/s]Extractor Estimating: 433it [04:22,  2.07it/s]Extractor Estimating: 434it [04:23,  2.03it/s]Extractor Estimating: 435it [04:23,  2.11it/s]Extractor Estimating: 436it [04:24,  2.09it/s]Extractor Estimating: 437it [04:24,  2.16it/s]Extractor Estimating: 438it [04:25,  1.99it/s]Extractor Estimating: 439it [04:25,  1.99it/s]Extractor Estimating: 440it [04:26,  1.98it/s]Extractor Estimating: 441it [04:26,  2.03it/s]Extractor Estimating: 442it [04:27,  1.95it/s]Extractor Estimating: 443it [04:27,  2.00it/s]Extractor Estimating: 444it [04:28,  1.97it/s]Extractor Estimating: 445it [04:28,  1.99it/s]Extractor Estimating: 446it [04:29,  2.03it/s]Extractor Estimating: 447it [04:29,  2.08it/s]Extractor Estimating: 448it [04:29,  2.11it/s]Extractor Estimating: 449it [04:30,  1.96it/s]Extractor Estimating: 450it [04:31,  1.86it/s]Extractor Estimating: 451it [04:32,  1.46it/s]Extractor Estimating: 452it [04:32,  1.54it/s]Extractor Estimating: 453it [04:33,  1.60it/s]Extractor Estimating: 454it [04:33,  1.63it/s]Extractor Estimating: 455it [04:34,  1.64it/s]Extractor Estimating: 456it [04:35,  1.47it/s]Extractor Estimating: 457it [04:36,  1.42it/s]Extractor Estimating: 458it [04:36,  1.50it/s]Extractor Estimating: 459it [04:37,  1.59it/s]Extractor Estimating: 460it [04:37,  1.57it/s]Extractor Estimating: 461it [04:38,  1.59it/s]Extractor Estimating: 462it [04:39,  1.59it/s]Extractor Estimating: 463it [04:39,  1.57it/s]Extractor Estimating: 464it [04:40,  1.56it/s]Extractor Estimating: 465it [04:41,  1.57it/s]Extractor Estimating: 466it [04:41,  1.57it/s]Extractor Estimating: 467it [04:42,  1.65it/s]Extractor Estimating: 468it [04:42,  1.66it/s]Extractor Estimating: 469it [04:43,  1.69it/s]Extractor Estimating: 470it [04:43,  1.75it/s]Extractor Estimating: 471it [04:44,  1.77it/s]Extractor Estimating: 472it [04:45,  1.69it/s]Extractor Estimating: 473it [04:45,  1.70it/s]Extractor Estimating: 474it [04:46,  1.66it/s]Extractor Estimating: 475it [04:46,  1.69it/s]Extractor Estimating: 476it [04:47,  1.68it/s]Extractor Estimating: 477it [04:48,  1.73it/s]Extractor Estimating: 478it [04:48,  1.74it/s]Extractor Estimating: 479it [04:49,  1.76it/s]Extractor Estimating: 480it [04:49,  1.76it/s]Extractor Estimating: 481it [04:50,  1.73it/s]Extractor Estimating: 482it [04:50,  1.74it/s]Extractor Estimating: 483it [04:51,  1.74it/s]Extractor Estimating: 484it [04:52,  1.73it/s]Extractor Estimating: 485it [04:52,  1.71it/s]Extractor Estimating: 486it [04:53,  1.77it/s]Extractor Estimating: 487it [04:53,  1.78it/s]Extractor Estimating: 488it [04:54,  1.70it/s]Extractor Estimating: 489it [04:54,  1.79it/s]Extractor Estimating: 490it [04:55,  1.78it/s]Extractor Estimating: 491it [04:56,  1.76it/s]Extractor Estimating: 492it [04:56,  1.74it/s]Extractor Estimating: 493it [04:57,  1.75it/s]Extractor Estimating: 494it [04:57,  1.74it/s]Extractor Estimating: 495it [04:58,  1.77it/s]Extractor Estimating: 496it [04:58,  1.77it/s]Extractor Estimating: 497it [04:59,  1.72it/s]Extractor Estimating: 498it [05:00,  1.72it/s]Extractor Estimating: 499it [05:00,  2.13it/s]Extractor Estimating: 499it [05:00,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:16,853 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:16,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:16,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:16,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:16,935 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 18:31:18,335 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 18:31:18,336 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:31:19,031 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 18:31:20,174 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:31:20,196 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:23,443 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:23,491 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:23,491 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:23,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 18:31:23,492 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 18:31:24,392 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 18:31:24,393 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 18:31:25,041 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 18:31:25,265 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 18:31:25,266 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 21:35:45,714 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 21:35:46,074 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9961 mean pseudo reward: 0.9281930960862222
fit {'path_train': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl'}
train vocab size: 17888
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17988, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17988, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.056, loss:567.7113
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.299, loss:546.4221
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.055, loss:531.8021
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.095, loss:527.0269
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 1.109, loss:512.4244
>> valid entity prec:0.5883, rec:0.4788, f1:0.5279
>> valid relation prec:0.1244, rec:0.0130, f1:0.0235
>> valid relation with NER prec:0.1244, rec:0.0130, f1:0.0235
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.497, loss:496.0519
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 1.058, loss:526.8307
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 1.048, loss:521.4923
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 1.050, loss:495.1059
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 1.042, loss:501.3262
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4902, rec:0.4389, f1:0.4631
>> valid relation prec:0.1054, rec:0.0108, f1:0.0196
>> valid relation with NER prec:0.1054, rec:0.0108, f1:0.0196
g_step 1100, step 268, avg_time 2.507, loss:524.2036
g_step 1200, step 368, avg_time 1.073, loss:505.3398
g_step 1300, step 52, avg_time 1.044, loss:487.9957
g_step 1400, step 152, avg_time 1.048, loss:476.7775
g_step 1500, step 252, avg_time 1.071, loss:475.6340
>> valid entity prec:0.5427, rec:0.4887, f1:0.5143
>> valid relation prec:0.1612, rec:0.0286, f1:0.0486
>> valid relation with NER prec:0.1612, rec:0.0286, f1:0.0486
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 352, avg_time 2.494, loss:507.9048
g_step 1700, step 36, avg_time 1.047, loss:466.8795
g_step 1800, step 136, avg_time 1.056, loss:461.4126
g_step 1900, step 236, avg_time 1.069, loss:472.8947
g_step 2000, step 336, avg_time 1.066, loss:474.8669
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5693, rec:0.4880, f1:0.5255
>> valid relation prec:0.1602, rec:0.0286, f1:0.0486
>> valid relation with NER prec:0.1602, rec:0.0286, f1:0.0486
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.488, loss:467.9015
g_step 2200, step 120, avg_time 1.067, loss:444.0828
g_step 2300, step 220, avg_time 1.046, loss:461.0595
g_step 2400, step 320, avg_time 1.057, loss:449.8519
g_step 2500, step 4, avg_time 1.045, loss:451.5159
>> valid entity prec:0.5740, rec:0.4679, f1:0.5156
>> valid relation prec:0.1125, rec:0.0190, f1:0.0325
>> valid relation with NER prec:0.1125, rec:0.0190, f1:0.0325
g_step 2600, step 104, avg_time 2.489, loss:444.2309
g_step 2700, step 204, avg_time 1.051, loss:427.9194
g_step 2800, step 304, avg_time 1.049, loss:436.3780
g_step 2900, step 404, avg_time 1.067, loss:455.0157
g_step 3000, step 88, avg_time 1.039, loss:416.4458
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5573, rec:0.4831, f1:0.5176
>> valid relation prec:0.1335, rec:0.0255, f1:0.0428
>> valid relation with NER prec:0.1335, rec:0.0255, f1:0.0428
g_step 3100, step 188, avg_time 2.476, loss:419.5182
g_step 3200, step 288, avg_time 1.059, loss:418.7702
g_step 3300, step 388, avg_time 1.043, loss:447.6721
g_step 3400, step 72, avg_time 1.062, loss:409.7754
g_step 3500, step 172, avg_time 1.046, loss:400.9671
>> valid entity prec:0.5486, rec:0.5286, f1:0.5384
>> valid relation prec:0.1097, rec:0.0243, f1:0.0398
>> valid relation with NER prec:0.1097, rec:0.0243, f1:0.0398
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 272, avg_time 2.498, loss:426.2467
g_step 3700, step 372, avg_time 1.048, loss:408.5938
g_step 3800, step 56, avg_time 1.038, loss:391.9408
g_step 3900, step 156, avg_time 1.050, loss:394.2660
g_step 4000, step 256, avg_time 1.060, loss:390.3404
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5602, rec:0.4688, f1:0.5104
>> valid relation prec:0.1226, rec:0.0281, f1:0.0458
>> valid relation with NER prec:0.1226, rec:0.0281, f1:0.0458
g_step 4100, step 356, avg_time 2.465, loss:413.2952
g_step 4200, step 40, avg_time 1.053, loss:385.2329
g_step 4300, step 140, avg_time 1.053, loss:357.8010
g_step 4400, step 240, avg_time 1.036, loss:383.1933
g_step 4500, step 340, avg_time 1.060, loss:397.6471
>> valid entity prec:0.5235, rec:0.5133, f1:0.5184
>> valid relation prec:0.1474, rec:0.0332, f1:0.0542
>> valid relation with NER prec:0.1474, rec:0.0332, f1:0.0542
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 24, avg_time 2.477, loss:377.6328
g_step 4700, step 124, avg_time 1.061, loss:345.7128
g_step 4800, step 224, avg_time 1.059, loss:379.4068
g_step 4900, step 324, avg_time 1.044, loss:381.4337
g_step 5000, step 8, avg_time 1.034, loss:380.4905
learning rate was adjusted to 0.0008
>> valid entity prec:0.5510, rec:0.4692, f1:0.5068
>> valid relation prec:0.0924, rec:0.0137, f1:0.0239
>> valid relation with NER prec:0.0924, rec:0.0137, f1:0.0239
g_step 5100, step 108, avg_time 2.457, loss:351.3057
g_step 5200, step 208, avg_time 1.071, loss:360.3340
g_step 5300, step 308, avg_time 1.033, loss:368.8694
g_step 5400, step 408, avg_time 1.046, loss:352.7678
g_step 5500, step 92, avg_time 1.040, loss:349.7751
>> valid entity prec:0.5388, rec:0.4894, f1:0.5129
>> valid relation prec:0.0944, rec:0.0226, f1:0.0365
>> valid relation with NER prec:0.0944, rec:0.0226, f1:0.0365
g_step 5600, step 192, avg_time 2.458, loss:334.8241
g_step 5700, step 292, avg_time 1.039, loss:358.0239
g_step 5800, step 392, avg_time 1.050, loss:360.6141
g_step 5900, step 76, avg_time 1.043, loss:332.0443
g_step 6000, step 176, avg_time 1.062, loss:335.7751
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5489, rec:0.4745, f1:0.5090
>> valid relation prec:0.1011, rec:0.0231, f1:0.0376
>> valid relation with NER prec:0.1011, rec:0.0231, f1:0.0376
g_step 6100, step 276, avg_time 2.464, loss:346.3470
g_step 6200, step 376, avg_time 1.052, loss:339.0607
g_step 6300, step 60, avg_time 1.062, loss:327.7362
g_step 6400, step 160, avg_time 1.042, loss:322.1508
g_step 6500, step 260, avg_time 1.037, loss:345.0406
>> valid entity prec:0.5535, rec:0.4758, f1:0.5117
>> valid relation prec:0.1066, rec:0.0296, f1:0.0463
>> valid relation with NER prec:0.1066, rec:0.0296, f1:0.0463
g_step 6600, step 360, avg_time 2.456, loss:334.0426
g_step 6700, step 44, avg_time 1.038, loss:320.1933
g_step 6800, step 144, avg_time 1.049, loss:311.7855
g_step 6900, step 244, avg_time 1.052, loss:321.4253
g_step 7000, step 344, avg_time 1.057, loss:330.6029
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5505, rec:0.4755, f1:0.5103
>> valid relation prec:0.1262, rec:0.0361, f1:0.0561
>> valid relation with NER prec:0.1262, rec:0.0361, f1:0.0561
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 7100, step 28, avg_time 2.437, loss:314.9478
g_step 7200, step 128, avg_time 1.058, loss:297.6891
g_step 7300, step 228, avg_time 1.036, loss:313.3429
g_step 7400, step 328, avg_time 1.047, loss:306.1886
g_step 7500, step 12, avg_time 1.040, loss:314.2946
>> valid entity prec:0.5607, rec:0.4762, f1:0.5150
>> valid relation prec:0.1250, rec:0.0286, f1:0.0466
>> valid relation with NER prec:0.1250, rec:0.0286, f1:0.0466
g_step 7600, step 112, avg_time 2.455, loss:286.5909
g_step 7700, step 212, avg_time 1.065, loss:297.8681
g_step 7800, step 312, avg_time 1.042, loss:293.2838
g_step 7900, step 412, avg_time 1.032, loss:321.7981
g_step 8000, step 96, avg_time 1.033, loss:282.3024
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5315, rec:0.4989, f1:0.5147
>> valid relation prec:0.1209, rec:0.0337, f1:0.0527
>> valid relation with NER prec:0.1209, rec:0.0337, f1:0.0527
g_step 8100, step 196, avg_time 2.480, loss:285.2680
g_step 8200, step 296, avg_time 1.036, loss:284.4804
g_step 8300, step 396, avg_time 1.050, loss:296.6476
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model', data_dir='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/data', model_name='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 21:35:46 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 21:35:46 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_21-35-45_ctolab06.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 21:35:47 - WARNING - datasets.builder -   Using custom data configuration default-85b023bfdc87bb53
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-85b023bfdc87bb53/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 21:35:49,944 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 21:35:49,945 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 21:35:49,946 >> loading configuration file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 21:35:49,947 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 21:35:50,085 >> Didn't find file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 21:35:50,161 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 21:35:50,161 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 21:35:50,161 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 21:35:50,161 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 21:35:50,161 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 21:35:50,161 >> loading file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 21:35:50,629 >> loading weights file outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 21:35:53,797 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 21:35:53,797 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-85b023bfdc87bb53/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:03,  2.59ba/s] 20%|██        | 2/10 [00:00<00:02,  3.67ba/s] 30%|███       | 3/10 [00:00<00:01,  4.27ba/s] 40%|████      | 4/10 [00:00<00:01,  4.61ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.54ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.79ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.94ba/s] 80%|████████  | 8/10 [00:01<00:00,  5.06ba/s] 90%|█████████ | 9/10 [00:02<00:00,  4.19ba/s]100%|██████████| 10/10 [00:02<00:00,  4.46ba/s]100%|██████████| 10/10 [00:02<00:00,  4.41ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.15ba/s] 40%|████      | 2/5 [00:00<00:00,  3.93ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.26ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.43ba/s]100%|██████████| 5/5 [00:00<00:00,  5.06ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:01,  4.89ba/s] 30%|███       | 3/10 [00:00<00:00,  8.35ba/s] 50%|█████     | 5/10 [00:00<00:00,  9.56ba/s] 70%|███████   | 7/10 [00:00<00:00, 10.10ba/s] 90%|█████████ | 9/10 [00:00<00:00, 10.50ba/s]100%|██████████| 10/10 [00:01<00:00,  9.86ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  4.65ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.13ba/s]100%|██████████| 5/5 [00:00<00:00, 11.50ba/s]100%|██████████| 5/5 [00:00<00:00,  9.95ba/s]
[INFO|trainer.py:414] 2023-08-29 21:35:59,910 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 21:36:00,048 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 21:36:00,048 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 21:36:00,048 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 21:36:00,048 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 21:36:00,048 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 21:36:00,048 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 21:36:00,048 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:01<21:12,  1.63s/it]  0%|          | 2/780 [00:01<10:52,  1.19it/s]  0%|          | 3/780 [00:02<07:33,  1.71it/s]  1%|          | 4/780 [00:02<06:00,  2.15it/s]  1%|          | 5/780 [00:02<05:08,  2.51it/s]  1%|          | 6/780 [00:03<04:37,  2.79it/s]  1%|          | 7/780 [00:03<04:17,  3.00it/s]  1%|          | 8/780 [00:03<04:04,  3.16it/s]  1%|          | 9/780 [00:03<03:55,  3.27it/s]  1%|▏         | 10/780 [00:04<03:49,  3.35it/s]  1%|▏         | 11/780 [00:04<03:45,  3.41it/s]  2%|▏         | 12/780 [00:04<03:42,  3.45it/s]  2%|▏         | 13/780 [00:05<03:40,  3.48it/s]  2%|▏         | 14/780 [00:05<03:38,  3.50it/s]  2%|▏         | 15/780 [00:05<03:37,  3.52it/s]  2%|▏         | 16/780 [00:05<03:36,  3.53it/s]  2%|▏         | 17/780 [00:06<03:35,  3.53it/s]  2%|▏         | 18/780 [00:06<03:35,  3.54it/s]  2%|▏         | 19/780 [00:06<03:34,  3.54it/s]  3%|▎         | 20/780 [00:06<03:34,  3.54it/s]  3%|▎         | 21/780 [00:07<03:46,  3.36it/s]  3%|▎         | 22/780 [00:07<03:42,  3.41it/s]  3%|▎         | 23/780 [00:07<03:39,  3.45it/s]  3%|▎         | 24/780 [00:08<03:37,  3.47it/s]  3%|▎         | 25/780 [00:08<03:36,  3.49it/s]  3%|▎         | 26/780 [00:08<03:34,  3.51it/s]  3%|▎         | 27/780 [00:09<03:33,  3.52it/s]  4%|▎         | 28/780 [00:09<03:33,  3.53it/s]  4%|▎         | 29/780 [00:09<03:32,  3.53it/s]  4%|▍         | 30/780 [00:09<03:32,  3.54it/s]  4%|▍         | 31/780 [00:10<03:31,  3.54it/s]  4%|▍         | 32/780 [00:10<03:31,  3.54it/s]  4%|▍         | 33/780 [00:10<03:30,  3.54it/s]  4%|▍         | 34/780 [00:10<03:30,  3.54it/s]  4%|▍         | 35/780 [00:11<03:30,  3.54it/s]  5%|▍         | 36/780 [00:11<03:29,  3.54it/s]  5%|▍         | 37/780 [00:11<03:29,  3.54it/s]  5%|▍         | 38/780 [00:12<03:29,  3.54it/s]  5%|▌         | 39/780 [00:12<03:29,  3.54it/s]  5%|▌         | 40/780 [00:12<03:28,  3.54it/s]  5%|▌         | 41/780 [00:12<03:28,  3.55it/s]  5%|▌         | 42/780 [00:13<03:28,  3.55it/s]  6%|▌         | 43/780 [00:13<03:27,  3.54it/s]  6%|▌         | 44/780 [00:13<03:27,  3.55it/s]  6%|▌         | 45/780 [00:14<03:27,  3.55it/s]  6%|▌         | 46/780 [00:14<03:27,  3.54it/s]  6%|▌         | 47/780 [00:14<03:26,  3.54it/s]  6%|▌         | 48/780 [00:14<03:26,  3.54it/s]  6%|▋         | 49/780 [00:15<03:26,  3.54it/s]  6%|▋         | 50/780 [00:15<03:26,  3.54it/s]  7%|▋         | 51/780 [00:15<03:25,  3.54it/s]  7%|▋         | 52/780 [00:16<03:25,  3.54it/s]  7%|▋         | 53/780 [00:16<03:25,  3.54it/s]  7%|▋         | 54/780 [00:16<03:24,  3.54it/s]  7%|▋         | 55/780 [00:16<03:24,  3.54it/s]  7%|▋         | 56/780 [00:17<03:24,  3.54it/s]  7%|▋         | 57/780 [00:17<03:24,  3.54it/s]  7%|▋         | 58/780 [00:17<03:23,  3.54it/s]  8%|▊         | 59/780 [00:18<03:23,  3.54it/s]  8%|▊         | 60/780 [00:18<03:23,  3.54it/s]  8%|▊         | 61/780 [00:18<03:23,  3.54it/s]  8%|▊         | 62/780 [00:18<03:22,  3.54it/s]  8%|▊         | 63/780 [00:19<03:22,  3.54it/s]  8%|▊         | 64/780 [00:19<03:22,  3.54it/s]  8%|▊         | 65/780 [00:19<03:21,  3.54it/s]  8%|▊         | 66/780 [00:20<03:21,  3.54it/s]  9%|▊         | 67/780 [00:20<03:21,  3.54it/s]  9%|▊         | 68/780 [00:20<03:21,  3.54it/s]  9%|▉         | 69/780 [00:20<03:20,  3.54it/s]  9%|▉         | 70/780 [00:21<03:20,  3.54it/s]  9%|▉         | 71/780 [00:21<03:20,  3.54it/s]  9%|▉         | 72/780 [00:21<03:20,  3.54it/s]  9%|▉         | 73/780 [00:21<03:19,  3.54it/s]  9%|▉         | 74/780 [00:22<03:19,  3.54it/s] 10%|▉         | 75/780 [00:22<03:19,  3.54it/s] 10%|▉         | 76/780 [00:22<03:19,  3.54it/s] 10%|▉         | 77/780 [00:23<03:18,  3.54it/s] 10%|█         | 78/780 [00:23<03:18,  3.54it/s] 10%|█         | 79/780 [00:23<03:18,  3.54it/s] 10%|█         | 80/780 [00:23<03:17,  3.54it/s] 10%|█         | 81/780 [00:24<03:17,  3.54it/s] 11%|█         | 82/780 [00:24<03:17,  3.54it/s] 11%|█         | 83/780 [00:24<03:16,  3.54it/s] 11%|█         | 84/780 [00:25<03:16,  3.54it/s] 11%|█         | 85/780 [00:25<03:16,  3.54it/s] 11%|█         | 86/780 [00:25<03:16,  3.54it/s] 11%|█         | 87/780 [00:25<03:15,  3.54it/s] 11%|█▏        | 88/780 [00:26<03:15,  3.54it/s] 11%|█▏        | 89/780 [00:26<03:15,  3.54it/s] 12%|█▏        | 90/780 [00:26<03:15,  3.54it/s] 12%|█▏        | 91/780 [00:27<03:14,  3.54it/s] 12%|█▏        | 92/780 [00:27<03:14,  3.54it/s] 12%|█▏        | 93/780 [00:27<03:14,  3.54it/s] 12%|█▏        | 94/780 [00:27<03:13,  3.54it/s] 12%|█▏        | 95/780 [00:28<03:13,  3.54it/s] 12%|█▏        | 96/780 [00:28<03:13,  3.54it/s] 12%|█▏        | 97/780 [00:28<03:13,  3.54it/s] 13%|█▎        | 98/780 [00:29<03:12,  3.54it/s] 13%|█▎        | 99/780 [00:29<03:12,  3.54it/s] 13%|█▎        | 100/780 [00:29<03:12,  3.54it/s] 13%|█▎        | 101/780 [00:29<03:12,  3.54it/s] 13%|█▎        | 102/780 [00:30<03:11,  3.54it/s] 13%|█▎        | 103/780 [00:30<03:11,  3.54it/s] 13%|█▎        | 104/780 [00:30<03:11,  3.54it/s] 13%|█▎        | 105/780 [00:31<03:10,  3.54it/s] 14%|█▎        | 106/780 [00:31<03:10,  3.54it/s] 14%|█▎        | 107/780 [00:31<03:10,  3.54it/s] 14%|█▍        | 108/780 [00:31<03:10,  3.53it/s] 14%|█▍        | 109/780 [00:32<03:09,  3.54it/s] 14%|█▍        | 110/780 [00:32<03:09,  3.54it/s] 14%|█▍        | 111/780 [00:32<03:09,  3.54it/s] 14%|█▍        | 112/780 [00:33<03:08,  3.53it/s] 14%|█▍        | 113/780 [00:33<03:08,  3.53it/s] 15%|█▍        | 114/780 [00:33<03:08,  3.53it/s] 15%|█▍        | 115/780 [00:33<03:08,  3.53it/s] 15%|█▍        | 116/780 [00:34<03:08,  3.53it/s] 15%|█▌        | 117/780 [00:34<03:07,  3.53it/s] 15%|█▌        | 118/780 [00:34<03:07,  3.53it/s] 15%|█▌        | 119/780 [00:35<03:07,  3.53it/s] 15%|█▌        | 120/780 [00:35<03:06,  3.53it/s] 16%|█▌        | 121/780 [00:35<03:06,  3.53it/s] 16%|█▌        | 122/780 [00:35<03:06,  3.53it/s] 16%|█▌        | 123/780 [00:36<03:06,  3.53it/s] 16%|█▌        | 124/780 [00:36<03:05,  3.53it/s] 16%|█▌        | 125/780 [00:36<03:05,  3.53it/s] 16%|█▌        | 126/780 [00:36<03:05,  3.53it/s] 16%|█▋        | 127/780 [00:37<03:04,  3.53it/s] 16%|█▋        | 128/780 [00:37<03:04,  3.53it/s] 17%|█▋        | 129/780 [00:37<03:04,  3.53it/s] 17%|█▋        | 130/780 [00:38<03:04,  3.53it/s] 17%|█▋        | 131/780 [00:38<03:03,  3.53it/s] 17%|█▋        | 132/780 [00:38<03:03,  3.53it/s] 17%|█▋        | 133/780 [00:38<03:03,  3.53it/s] 17%|█▋        | 134/780 [00:39<03:02,  3.53it/s] 17%|█▋        | 135/780 [00:39<03:02,  3.53it/s] 17%|█▋        | 136/780 [00:39<03:02,  3.53it/s] 18%|█▊        | 137/780 [00:40<03:02,  3.53it/s] 18%|█▊        | 138/780 [00:40<03:01,  3.53it/s] 18%|█▊        | 139/780 [00:40<03:01,  3.53it/s] 18%|█▊        | 140/780 [00:40<03:01,  3.53it/s] 18%|█▊        | 141/780 [00:41<03:00,  3.53it/s] 18%|█▊        | 142/780 [00:41<03:00,  3.53it/s] 18%|█▊        | 143/780 [00:41<03:00,  3.53it/s] 18%|█▊        | 144/780 [00:42<03:13,  3.28it/s] 19%|█▊        | 145/780 [00:42<03:09,  3.35it/s] 19%|█▊        | 146/780 [00:42<03:06,  3.40it/s] 19%|█▉        | 147/780 [00:43<03:04,  3.44it/s] 19%|█▉        | 148/780 [00:43<03:02,  3.47it/s] 19%|█▉        | 149/780 [00:43<03:10,  3.31it/s] 19%|█▉        | 150/780 [00:43<03:07,  3.36it/s] 19%|█▉        | 151/780 [00:44<03:04,  3.41it/s] 19%|█▉        | 152/780 [00:44<03:02,  3.45it/s] 20%|█▉        | 153/780 [00:44<03:00,  3.47it/s] 20%|█▉        | 154/780 [00:45<02:59,  3.49it/s] 20%|█▉        | 155/780 [00:45<03:12,  3.24it/s] 20%|██        | 156/780 [00:45<03:07,  3.32it/s][INFO|trainer.py:2140] 2023-08-29 21:36:46,025 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 21:36:46,025 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 21:36:46,025 >>   Batch size = 8

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 59.36it/s][A
  2%|▏         | 12/521 [00:00<00:09, 51.74it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.62it/s][A
  5%|▍         | 24/521 [00:00<00:10, 48.63it/s][A
  6%|▌         | 29/521 [00:00<00:10, 48.26it/s][A
  7%|▋         | 34/521 [00:00<00:10, 47.95it/s][A
  7%|▋         | 39/521 [00:00<00:10, 47.72it/s][A
  8%|▊         | 44/521 [00:00<00:10, 47.50it/s][A
  9%|▉         | 49/521 [00:01<00:09, 47.30it/s][A
 10%|█         | 54/521 [00:01<00:09, 47.28it/s][A
 11%|█▏        | 59/521 [00:01<00:09, 47.30it/s][A
 12%|█▏        | 64/521 [00:01<00:09, 47.29it/s][A
 13%|█▎        | 69/521 [00:01<00:09, 47.30it/s][A
 14%|█▍        | 74/521 [00:01<00:09, 47.27it/s][A
 15%|█▌        | 79/521 [00:01<00:09, 47.33it/s][A
 16%|█▌        | 84/521 [00:01<00:09, 47.33it/s][A
 17%|█▋        | 89/521 [00:01<00:09, 47.23it/s][A
 18%|█▊        | 94/521 [00:01<00:09, 47.20it/s][A
 19%|█▉        | 99/521 [00:02<00:08, 47.17it/s][A
 20%|█▉        | 104/521 [00:02<00:09, 44.92it/s][A
 21%|██        | 109/521 [00:02<00:09, 45.62it/s][A
 22%|██▏       | 114/521 [00:02<00:08, 46.12it/s][A
 23%|██▎       | 119/521 [00:02<00:08, 46.52it/s][A
 24%|██▍       | 124/521 [00:02<00:09, 43.88it/s][A
 25%|██▍       | 129/521 [00:02<00:08, 44.88it/s][A
 26%|██▌       | 134/521 [00:02<00:08, 45.57it/s][A
 27%|██▋       | 139/521 [00:02<00:08, 46.05it/s][A
 28%|██▊       | 144/521 [00:03<00:08, 46.43it/s][A
 29%|██▊       | 149/521 [00:03<00:07, 46.70it/s][A
 30%|██▉       | 154/521 [00:03<00:07, 46.87it/s][A
 31%|███       | 159/521 [00:03<00:07, 47.00it/s][A
 31%|███▏      | 164/521 [00:03<00:07, 46.95it/s][A
 32%|███▏      | 169/521 [00:03<00:07, 46.96it/s][A
 33%|███▎      | 174/521 [00:03<00:07, 47.03it/s][A
 34%|███▍      | 179/521 [00:03<00:07, 47.15it/s][A
 35%|███▌      | 184/521 [00:03<00:07, 47.20it/s][A
 36%|███▋      | 189/521 [00:04<00:07, 47.21it/s][A
 37%|███▋      | 194/521 [00:04<00:06, 47.23it/s][A
 38%|███▊      | 199/521 [00:04<00:06, 47.26it/s][A
 39%|███▉      | 204/521 [00:04<00:06, 47.26it/s][A
 40%|████      | 209/521 [00:04<00:06, 47.20it/s][A
 41%|████      | 214/521 [00:04<00:06, 47.16it/s][A
 42%|████▏     | 219/521 [00:04<00:06, 47.10it/s][A
 43%|████▎     | 224/521 [00:04<00:06, 47.13it/s][A
 44%|████▍     | 229/521 [00:04<00:06, 47.18it/s][A
 45%|████▍     | 234/521 [00:04<00:06, 47.08it/s][A
 46%|████▌     | 239/521 [00:05<00:05, 47.12it/s][A
 47%|████▋     | 244/521 [00:05<00:05, 47.19it/s][A
 48%|████▊     | 249/521 [00:05<00:05, 47.23it/s][A
 49%|████▉     | 254/521 [00:05<00:05, 47.20it/s][A
 50%|████▉     | 259/521 [00:05<00:05, 47.15it/s][A
 51%|█████     | 264/521 [00:05<00:05, 45.01it/s][A
 52%|█████▏    | 269/521 [00:05<00:05, 45.71it/s][A
 53%|█████▎    | 274/521 [00:05<00:05, 46.15it/s][A
 54%|█████▎    | 279/521 [00:05<00:05, 46.48it/s][A
 55%|█████▍    | 284/521 [00:06<00:05, 46.74it/s][A
 55%|█████▌    | 289/521 [00:06<00:04, 46.89it/s][A
 56%|█████▋    | 294/521 [00:06<00:04, 47.01it/s][A
 57%|█████▋    | 299/521 [00:06<00:04, 47.11it/s][A
 58%|█████▊    | 304/521 [00:06<00:04, 46.98it/s][A
 59%|█████▉    | 309/521 [00:06<00:04, 46.96it/s][A
 60%|██████    | 314/521 [00:06<00:04, 47.06it/s][A
 61%|██████    | 319/521 [00:06<00:04, 47.16it/s][A
 62%|██████▏   | 324/521 [00:06<00:04, 47.18it/s][A
 63%|██████▎   | 329/521 [00:06<00:04, 47.20it/s][A
 64%|██████▍   | 334/521 [00:07<00:03, 47.27it/s][A
 65%|██████▌   | 339/521 [00:07<00:03, 47.28it/s][A
 66%|██████▌   | 344/521 [00:07<00:03, 47.28it/s][A
 67%|██████▋   | 349/521 [00:07<00:03, 47.14it/s][A
 68%|██████▊   | 354/521 [00:07<00:03, 47.10it/s][A
 69%|██████▉   | 359/521 [00:07<00:03, 47.09it/s][A
 70%|██████▉   | 364/521 [00:07<00:03, 47.09it/s][A
 71%|███████   | 369/521 [00:07<00:03, 47.18it/s][A
 72%|███████▏  | 374/521 [00:07<00:03, 47.18it/s][A
 73%|███████▎  | 379/521 [00:08<00:03, 47.20it/s][A
 74%|███████▎  | 384/521 [00:08<00:02, 47.27it/s][A
 75%|███████▍  | 389/521 [00:08<00:02, 47.27it/s][A
 76%|███████▌  | 394/521 [00:08<00:02, 47.19it/s][A
 77%|███████▋  | 399/521 [00:08<00:02, 47.14it/s][A
 78%|███████▊  | 404/521 [00:08<00:02, 47.12it/s][A
 79%|███████▊  | 409/521 [00:08<00:02, 44.39it/s][A
 79%|███████▉  | 414/521 [00:08<00:02, 45.26it/s][A
 80%|████████  | 419/521 [00:08<00:02, 45.86it/s][A
 81%|████████▏ | 424/521 [00:09<00:02, 46.27it/s][A
 82%|████████▏ | 429/521 [00:09<00:01, 46.53it/s][A
 83%|████████▎ | 434/521 [00:09<00:01, 46.77it/s][A
 84%|████████▍ | 439/521 [00:09<00:01, 46.92it/s][A
 85%|████████▌ | 444/521 [00:09<00:01, 47.01it/s][A
 86%|████████▌ | 449/521 [00:09<00:01, 46.96it/s][A
 87%|████████▋ | 454/521 [00:09<00:01, 46.95it/s][A
 88%|████████▊ | 459/521 [00:09<00:01, 47.00it/s][A
 89%|████████▉ | 464/521 [00:09<00:01, 47.09it/s][A
 90%|█████████ | 469/521 [00:09<00:01, 47.12it/s][A
 91%|█████████ | 474/521 [00:10<00:00, 47.12it/s][A
 92%|█████████▏| 479/521 [00:10<00:00, 47.14it/s][A
 93%|█████████▎| 484/521 [00:10<00:00, 47.20it/s][A
 94%|█████████▍| 489/521 [00:10<00:00, 47.19it/s][A
 95%|█████████▍| 494/521 [00:10<00:00, 47.17it/s][A
 96%|█████████▌| 499/521 [00:10<00:00, 47.20it/s][A
 97%|█████████▋| 504/521 [00:10<00:00, 47.18it/s][A
 98%|█████████▊| 509/521 [00:10<00:00, 47.16it/s][A
 99%|█████████▊| 514/521 [00:10<00:00, 47.19it/s][A
100%|█████████▉| 519/521 [00:11<00:00, 47.19it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 47.19it/s][A 20%|██        | 156/780 [00:57<03:07,  3.32it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 21:36:57,610 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 21:36:57,909 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 21:37:01,939 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 21:37:02,131 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 21:37:02,222 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:10<1:20:05,  7.71s/it] 20%|██        | 158/780 [01:11<56:58,  5.50s/it]   20%|██        | 159/780 [01:11<40:41,  3.93s/it] 21%|██        | 160/780 [01:11<29:18,  2.84s/it] 21%|██        | 161/780 [01:11<21:21,  2.07s/it] 21%|██        | 162/780 [01:12<15:48,  1.53s/it] 21%|██        | 163/780 [01:12<11:54,  1.16s/it] 21%|██        | 164/780 [01:12<09:11,  1.12it/s] 21%|██        | 165/780 [01:12<07:17,  1.41it/s] 21%|██▏       | 166/780 [01:13<05:58,  1.72it/s] 21%|██▏       | 167/780 [01:13<05:02,  2.03it/s] 22%|██▏       | 168/780 [01:13<04:22,  2.33it/s] 22%|██▏       | 169/780 [01:14<04:02,  2.52it/s] 22%|██▏       | 170/780 [01:14<03:41,  2.76it/s] 22%|██▏       | 171/780 [01:14<03:26,  2.95it/s] 22%|██▏       | 172/780 [01:15<03:15,  3.11it/s] 22%|██▏       | 173/780 [01:15<03:08,  3.23it/s] 22%|██▏       | 174/780 [01:15<03:02,  3.32it/s] 22%|██▏       | 175/780 [01:15<02:59,  3.38it/s] 23%|██▎       | 176/780 [01:16<02:56,  3.43it/s] 23%|██▎       | 177/780 [01:16<02:54,  3.46it/s] 23%|██▎       | 178/780 [01:16<02:52,  3.48it/s] 23%|██▎       | 179/780 [01:16<02:51,  3.50it/s] 23%|██▎       | 180/780 [01:17<02:56,  3.40it/s] 23%|██▎       | 181/780 [01:17<02:54,  3.44it/s] 23%|██▎       | 182/780 [01:17<02:52,  3.47it/s] 23%|██▎       | 183/780 [01:18<02:51,  3.49it/s] 24%|██▎       | 184/780 [01:18<02:50,  3.50it/s] 24%|██▎       | 185/780 [01:18<02:49,  3.51it/s] 24%|██▍       | 186/780 [01:18<02:48,  3.52it/s] 24%|██▍       | 187/780 [01:19<02:48,  3.53it/s] 24%|██▍       | 188/780 [01:19<02:49,  3.50it/s] 24%|██▍       | 189/780 [01:19<02:48,  3.51it/s] 24%|██▍       | 190/780 [01:20<02:47,  3.52it/s] 24%|██▍       | 191/780 [01:20<02:47,  3.53it/s] 25%|██▍       | 192/780 [01:20<02:46,  3.53it/s] 25%|██▍       | 193/780 [01:20<02:46,  3.53it/s] 25%|██▍       | 194/780 [01:21<02:45,  3.53it/s] 25%|██▌       | 195/780 [01:21<02:45,  3.54it/s] 25%|██▌       | 196/780 [01:21<02:45,  3.54it/s] 25%|██▌       | 197/780 [01:22<02:44,  3.54it/s] 25%|██▌       | 198/780 [01:22<02:44,  3.54it/s] 26%|██▌       | 199/780 [01:22<02:54,  3.33it/s] 26%|██▌       | 200/780 [01:23<02:51,  3.39it/s] 26%|██▌       | 201/780 [01:23<02:48,  3.43it/s] 26%|██▌       | 202/780 [01:23<02:47,  3.46it/s] 26%|██▌       | 203/780 [01:23<02:45,  3.48it/s] 26%|██▌       | 204/780 [01:24<02:51,  3.36it/s] 26%|██▋       | 205/780 [01:24<02:49,  3.40it/s] 26%|██▋       | 206/780 [01:24<02:46,  3.44it/s] 27%|██▋       | 207/780 [01:25<02:45,  3.47it/s] 27%|██▋       | 208/780 [01:25<02:43,  3.49it/s] 27%|██▋       | 209/780 [01:25<02:42,  3.50it/s] 27%|██▋       | 210/780 [01:25<02:47,  3.40it/s] 27%|██▋       | 211/780 [01:26<02:45,  3.44it/s] 27%|██▋       | 212/780 [01:26<02:43,  3.47it/s] 27%|██▋       | 213/780 [01:26<02:42,  3.49it/s] 27%|██▋       | 214/780 [01:27<02:41,  3.50it/s] 28%|██▊       | 215/780 [01:27<02:40,  3.51it/s] 28%|██▊       | 216/780 [01:27<02:40,  3.52it/s] 28%|██▊       | 217/780 [01:27<02:39,  3.53it/s] 28%|██▊       | 218/780 [01:28<02:39,  3.53it/s] 28%|██▊       | 219/780 [01:28<02:38,  3.53it/s] 28%|██▊       | 220/780 [01:28<02:38,  3.53it/s] 28%|██▊       | 221/780 [01:29<02:47,  3.34it/s] 28%|██▊       | 222/780 [01:29<02:44,  3.40it/s] 29%|██▊       | 223/780 [01:29<02:41,  3.44it/s] 29%|██▊       | 224/780 [01:29<02:40,  3.47it/s] 29%|██▉       | 225/780 [01:30<02:39,  3.49it/s] 29%|██▉       | 226/780 [01:30<02:38,  3.50it/s] 29%|██▉       | 227/780 [01:30<02:37,  3.51it/s] 29%|██▉       | 228/780 [01:31<02:36,  3.52it/s] 29%|██▉       | 229/780 [01:31<02:36,  3.52it/s] 29%|██▉       | 230/780 [01:31<02:35,  3.53it/s] 30%|██▉       | 231/780 [01:31<02:35,  3.53it/s] 30%|██▉       | 232/780 [01:32<02:43,  3.36it/s] 30%|██▉       | 233/780 [01:32<02:40,  3.41it/s] 30%|███       | 234/780 [01:32<02:38,  3.45it/s] 30%|███       | 235/780 [01:33<02:36,  3.47it/s] 30%|███       | 236/780 [01:33<02:35,  3.49it/s] 30%|███       | 237/780 [01:33<02:34,  3.51it/s] 31%|███       | 238/780 [01:33<02:34,  3.51it/s] 31%|███       | 239/780 [01:34<02:33,  3.52it/s] 31%|███       | 240/780 [01:34<02:33,  3.52it/s] 31%|███       | 241/780 [01:34<02:32,  3.52it/s] 31%|███       | 242/780 [01:35<02:32,  3.53it/s] 31%|███       | 243/780 [01:35<02:40,  3.35it/s] 31%|███▏      | 244/780 [01:35<02:37,  3.41it/s] 31%|███▏      | 245/780 [01:35<02:35,  3.45it/s] 32%|███▏      | 246/780 [01:36<02:33,  3.47it/s] 32%|███▏      | 247/780 [01:36<02:32,  3.49it/s] 32%|███▏      | 248/780 [01:36<02:31,  3.51it/s] 32%|███▏      | 249/780 [01:37<02:31,  3.51it/s] 32%|███▏      | 250/780 [01:37<02:30,  3.52it/s] 32%|███▏      | 251/780 [01:37<02:30,  3.52it/s] 32%|███▏      | 252/780 [01:37<02:29,  3.53it/s] 32%|███▏      | 253/780 [01:38<02:29,  3.53it/s] 33%|███▎      | 254/780 [01:38<02:34,  3.40it/s] 33%|███▎      | 255/780 [01:38<02:32,  3.44it/s] 33%|███▎      | 256/780 [01:39<02:31,  3.47it/s] 33%|███▎      | 257/780 [01:39<02:30,  3.49it/s] 33%|███▎      | 258/780 [01:39<02:29,  3.50it/s] 33%|███▎      | 259/780 [01:39<02:28,  3.51it/s] 33%|███▎      | 260/780 [01:40<02:27,  3.52it/s] 33%|███▎      | 261/780 [01:40<02:27,  3.52it/s] 34%|███▎      | 262/780 [01:40<02:26,  3.53it/s] 34%|███▎      | 263/780 [01:41<02:26,  3.53it/s] 34%|███▍      | 264/780 [01:41<02:26,  3.53it/s] 34%|███▍      | 265/780 [01:41<02:37,  3.27it/s] 34%|███▍      | 266/780 [01:42<02:33,  3.34it/s] 34%|███▍      | 267/780 [01:42<02:30,  3.40it/s] 34%|███▍      | 268/780 [01:42<02:28,  3.44it/s] 34%|███▍      | 269/780 [01:42<02:27,  3.47it/s] 35%|███▍      | 270/780 [01:43<02:26,  3.49it/s] 35%|███▍      | 271/780 [01:43<02:25,  3.50it/s] 35%|███▍      | 272/780 [01:43<02:33,  3.30it/s] 35%|███▌      | 273/780 [01:44<02:30,  3.36it/s] 35%|███▌      | 274/780 [01:44<02:28,  3.41it/s] 35%|███▌      | 275/780 [01:44<02:26,  3.45it/s] 35%|███▌      | 276/780 [01:44<02:35,  3.25it/s] 36%|███▌      | 277/780 [01:45<02:31,  3.33it/s] 36%|███▌      | 278/780 [01:45<02:28,  3.39it/s] 36%|███▌      | 279/780 [01:46<02:57,  2.83it/s] 36%|███▌      | 280/780 [01:46<02:46,  3.00it/s] 36%|███▌      | 281/780 [01:46<02:38,  3.14it/s] 36%|███▌      | 282/780 [01:46<02:33,  3.25it/s] 36%|███▋      | 283/780 [01:47<02:29,  3.33it/s] 36%|███▋      | 284/780 [01:47<02:26,  3.39it/s] 37%|███▋      | 285/780 [01:47<02:24,  3.43it/s] 37%|███▋      | 286/780 [01:48<02:27,  3.36it/s] 37%|███▋      | 287/780 [01:48<02:24,  3.41it/s] 37%|███▋      | 288/780 [01:48<02:22,  3.45it/s] 37%|███▋      | 289/780 [01:48<02:21,  3.47it/s] 37%|███▋      | 290/780 [01:49<02:20,  3.49it/s] 37%|███▋      | 291/780 [01:49<02:19,  3.50it/s] 37%|███▋      | 292/780 [01:49<02:18,  3.51it/s] 38%|███▊      | 293/780 [01:50<02:18,  3.52it/s] 38%|███▊      | 294/780 [01:50<02:17,  3.52it/s] 38%|███▊      | 295/780 [01:50<02:17,  3.53it/s] 38%|███▊      | 296/780 [01:50<02:17,  3.53it/s] 38%|███▊      | 297/780 [01:51<02:16,  3.53it/s] 38%|███▊      | 298/780 [01:51<02:16,  3.53it/s] 38%|███▊      | 299/780 [01:51<02:16,  3.53it/s] 38%|███▊      | 300/780 [01:52<02:15,  3.53it/s] 39%|███▊      | 301/780 [01:52<02:15,  3.53it/s] 39%|███▊      | 302/780 [01:52<02:15,  3.53it/s] 39%|███▉      | 303/780 [01:52<02:15,  3.53it/s] 39%|███▉      | 304/780 [01:53<02:14,  3.54it/s] 39%|███▉      | 305/780 [01:53<02:20,  3.37it/s] 39%|███▉      | 306/780 [01:53<02:18,  3.42it/s] 39%|███▉      | 307/780 [01:54<02:17,  3.45it/s] 39%|███▉      | 308/780 [01:54<02:15,  3.47it/s] 40%|███▉      | 309/780 [01:54<02:14,  3.49it/s] 40%|███▉      | 310/780 [01:54<02:14,  3.50it/s] 40%|███▉      | 311/780 [01:55<02:13,  3.51it/s] 40%|████      | 312/780 [01:55<02:13,  3.52it/s][INFO|trainer.py:2140] 2023-08-29 21:37:55,554 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 21:37:55,554 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 21:37:55,554 >>   Batch size = 8
{'eval_loss': 1.0246262550354004, 'eval_runtime': 11.1411, 'eval_samples_per_second': 373.84, 'eval_steps_per_second': 46.764, 'epoch': 1.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.85it/s][A
  2%|▏         | 12/521 [00:00<00:09, 51.12it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.33it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.57it/s][A
  5%|▌         | 28/521 [00:00<00:10, 48.13it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.85it/s][A
  7%|▋         | 38/521 [00:00<00:11, 43.71it/s][A
  8%|▊         | 43/521 [00:00<00:10, 44.79it/s][A
  9%|▉         | 48/521 [00:01<00:10, 45.53it/s][A
 10%|█         | 53/521 [00:01<00:10, 46.05it/s][A
 11%|█         | 58/521 [00:01<00:09, 46.42it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.70it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.89it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.98it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 46.89it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 46.97it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 47.03it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 47.06it/s][A
 19%|█▉        | 98/521 [00:02<00:08, 47.15it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 47.23it/s][A
 21%|██        | 108/521 [00:02<00:08, 47.16it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 47.21it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 47.23it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 47.10it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 47.08it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 47.11it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 47.09it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 47.15it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 47.22it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 47.13it/s][A
 30%|███       | 158/521 [00:03<00:07, 47.16it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 47.25it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 47.22it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 47.14it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 47.16it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.02it/s][A
 36%|███▌      | 188/521 [00:03<00:07, 46.43it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.70it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.87it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 47.00it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 47.12it/s][A
 41%|████      | 213/521 [00:04<00:06, 47.13it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 47.08it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 47.07it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 47.10it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 47.15it/s][A
 46%|████▌     | 238/521 [00:05<00:05, 47.22it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 47.25it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 47.26it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 47.25it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 47.23it/s][A
 50%|█████     | 263/521 [00:05<00:05, 47.20it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 47.13it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 47.14it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 47.17it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 47.19it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 47.23it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 47.27it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 47.28it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 47.25it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 47.23it/s][A
 60%|██████    | 313/521 [00:06<00:04, 47.16it/s][A
 61%|██████    | 318/521 [00:06<00:04, 47.09it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 47.16it/s][A
 63%|██████▎   | 328/521 [00:06<00:04, 45.83it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.31it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.60it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.78it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 46.94it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 47.05it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 47.08it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 47.06it/s][A
 71%|███████   | 368/521 [00:07<00:03, 47.07it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 47.07it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 47.11it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 47.20it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 47.23it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 47.23it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 47.23it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 47.26it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 47.19it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 47.11it/s][A
 80%|████████  | 418/521 [00:08<00:02, 47.15it/s][A
 81%|████████  | 423/521 [00:08<00:02, 47.20it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 47.20it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 47.23it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 47.23it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 47.19it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 47.21it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 47.23it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 47.15it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 47.11it/s][A
 90%|████████▉ | 468/521 [00:09<00:01, 47.19it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 44.84it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 45.54it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 46.05it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 46.42it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 46.66it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.85it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 47.02it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 47.10it/s][A
 98%|█████████▊| 513/521 [00:10<00:00, 46.95it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.96it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 46.96it/s][A 40%|████      | 312/780 [02:06<02:13,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 21:38:06,900 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 21:38:07,054 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 21:38:10,790 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 21:38:10,953 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 21:38:11,029 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:19<58:26,  7.51s/it] 40%|████      | 314/780 [02:20<41:31,  5.35s/it] 40%|████      | 315/780 [02:20<29:40,  3.83s/it] 41%|████      | 316/780 [02:20<21:22,  2.76s/it] 41%|████      | 317/780 [02:21<15:38,  2.03s/it] 41%|████      | 318/780 [02:21<11:34,  1.50s/it] 41%|████      | 319/780 [02:21<08:44,  1.14s/it] 41%|████      | 320/780 [02:21<06:45,  1.13it/s] 41%|████      | 321/780 [02:22<05:22,  1.42it/s] 41%|████▏     | 322/780 [02:22<04:23,  1.74it/s] 41%|████▏     | 323/780 [02:22<03:43,  2.05it/s] 42%|████▏     | 324/780 [02:22<03:14,  2.34it/s] 42%|████▏     | 325/780 [02:23<02:54,  2.61it/s] 42%|████▏     | 326/780 [02:23<02:40,  2.83it/s] 42%|████▏     | 327/780 [02:23<02:30,  3.01it/s] 42%|████▏     | 328/780 [02:24<02:26,  3.08it/s] 42%|████▏     | 329/780 [02:24<02:20,  3.20it/s] 42%|████▏     | 330/780 [02:24<02:16,  3.30it/s] 42%|████▏     | 331/780 [02:24<02:13,  3.36it/s] 43%|████▎     | 332/780 [02:25<02:11,  3.41it/s] 43%|████▎     | 333/780 [02:25<02:09,  3.45it/s] 43%|████▎     | 334/780 [02:25<02:08,  3.47it/s] 43%|████▎     | 335/780 [02:26<02:07,  3.49it/s] 43%|████▎     | 336/780 [02:26<02:06,  3.50it/s] 43%|████▎     | 337/780 [02:26<02:06,  3.51it/s] 43%|████▎     | 338/780 [02:26<02:05,  3.52it/s] 43%|████▎     | 339/780 [02:27<02:09,  3.41it/s] 44%|████▎     | 340/780 [02:27<02:07,  3.44it/s] 44%|████▎     | 341/780 [02:27<02:06,  3.47it/s] 44%|████▍     | 342/780 [02:28<02:05,  3.49it/s] 44%|████▍     | 343/780 [02:28<02:04,  3.50it/s] 44%|████▍     | 344/780 [02:28<02:04,  3.51it/s] 44%|████▍     | 345/780 [02:28<02:03,  3.52it/s] 44%|████▍     | 346/780 [02:29<02:03,  3.52it/s] 44%|████▍     | 347/780 [02:29<02:02,  3.53it/s] 45%|████▍     | 348/780 [02:29<02:02,  3.53it/s] 45%|████▍     | 349/780 [02:30<02:02,  3.53it/s] 45%|████▍     | 350/780 [02:30<02:06,  3.40it/s] 45%|████▌     | 351/780 [02:30<02:04,  3.44it/s] 45%|████▌     | 352/780 [02:31<02:03,  3.47it/s] 45%|████▌     | 353/780 [02:31<02:02,  3.49it/s] 45%|████▌     | 354/780 [02:31<02:01,  3.50it/s] 46%|████▌     | 355/780 [02:31<02:01,  3.51it/s] 46%|████▌     | 356/780 [02:32<02:00,  3.52it/s] 46%|████▌     | 357/780 [02:32<02:00,  3.52it/s] 46%|████▌     | 358/780 [02:32<01:59,  3.53it/s] 46%|████▌     | 359/780 [02:32<01:59,  3.53it/s] 46%|████▌     | 360/780 [02:33<01:59,  3.53it/s] 46%|████▋     | 361/780 [02:33<02:01,  3.44it/s] 46%|████▋     | 362/780 [02:33<02:00,  3.47it/s] 47%|████▋     | 363/780 [02:34<01:59,  3.49it/s] 47%|████▋     | 364/780 [02:34<01:58,  3.50it/s] 47%|████▋     | 365/780 [02:34<01:58,  3.51it/s] 47%|████▋     | 366/780 [02:34<01:57,  3.52it/s] 47%|████▋     | 367/780 [02:35<01:57,  3.52it/s] 47%|████▋     | 368/780 [02:35<01:56,  3.52it/s] 47%|████▋     | 369/780 [02:35<01:56,  3.53it/s] 47%|████▋     | 370/780 [02:36<01:56,  3.53it/s] 48%|████▊     | 371/780 [02:36<01:55,  3.53it/s] 48%|████▊     | 372/780 [02:36<02:03,  3.30it/s] 48%|████▊     | 373/780 [02:37<02:00,  3.37it/s] 48%|████▊     | 374/780 [02:37<01:58,  3.41it/s] 48%|████▊     | 375/780 [02:37<01:57,  3.45it/s] 48%|████▊     | 376/780 [02:37<01:56,  3.47it/s] 48%|████▊     | 377/780 [02:38<01:55,  3.49it/s] 48%|████▊     | 378/780 [02:38<01:54,  3.50it/s] 49%|████▊     | 379/780 [02:38<01:54,  3.51it/s] 49%|████▊     | 380/780 [02:39<01:53,  3.52it/s] 49%|████▉     | 381/780 [02:39<01:53,  3.52it/s] 49%|████▉     | 382/780 [02:39<01:52,  3.53it/s] 49%|████▉     | 383/780 [02:39<02:00,  3.29it/s] 49%|████▉     | 384/780 [02:40<01:57,  3.36it/s] 49%|████▉     | 385/780 [02:40<01:55,  3.41it/s] 49%|████▉     | 386/780 [02:40<01:54,  3.44it/s] 50%|████▉     | 387/780 [02:41<01:53,  3.47it/s] 50%|████▉     | 388/780 [02:41<01:52,  3.49it/s] 50%|████▉     | 389/780 [02:41<01:51,  3.50it/s] 50%|█████     | 390/780 [02:41<01:51,  3.51it/s] 50%|█████     | 391/780 [02:42<01:50,  3.52it/s] 50%|█████     | 392/780 [02:42<01:50,  3.52it/s] 50%|█████     | 393/780 [02:42<01:49,  3.52it/s] 51%|█████     | 394/780 [02:43<01:58,  3.27it/s] 51%|█████     | 395/780 [02:43<01:55,  3.34it/s] 51%|█████     | 396/780 [02:43<01:53,  3.40it/s] 51%|█████     | 397/780 [02:44<01:56,  3.29it/s] 51%|█████     | 398/780 [02:44<01:54,  3.35it/s] 51%|█████     | 399/780 [02:44<01:52,  3.40it/s] 51%|█████▏    | 400/780 [02:44<01:50,  3.44it/s] 51%|█████▏    | 401/780 [02:45<01:49,  3.47it/s] 52%|█████▏    | 402/780 [02:45<01:48,  3.49it/s] 52%|█████▏    | 403/780 [02:45<01:47,  3.50it/s] 52%|█████▏    | 404/780 [02:46<02:35,  2.41it/s] 52%|█████▏    | 405/780 [02:46<02:23,  2.61it/s] 52%|█████▏    | 406/780 [02:47<02:12,  2.83it/s] 52%|█████▏    | 407/780 [02:47<02:03,  3.01it/s] 52%|█████▏    | 408/780 [02:47<01:58,  3.15it/s] 52%|█████▏    | 409/780 [02:47<01:53,  3.26it/s] 53%|█████▎    | 410/780 [02:48<01:50,  3.33it/s] 53%|█████▎    | 411/780 [02:48<01:48,  3.39it/s] 53%|█████▎    | 412/780 [02:48<01:47,  3.43it/s] 53%|█████▎    | 413/780 [02:49<01:46,  3.46it/s] 53%|█████▎    | 414/780 [02:49<01:45,  3.48it/s] 53%|█████▎    | 415/780 [02:49<01:44,  3.50it/s] 53%|█████▎    | 416/780 [02:49<01:46,  3.40it/s] 53%|█████▎    | 417/780 [02:50<01:45,  3.44it/s] 54%|█████▎    | 418/780 [02:50<01:44,  3.47it/s] 54%|█████▎    | 419/780 [02:50<01:43,  3.49it/s] 54%|█████▍    | 420/780 [02:51<01:42,  3.50it/s] 54%|█████▍    | 421/780 [02:51<01:42,  3.51it/s] 54%|█████▍    | 422/780 [02:51<01:41,  3.52it/s] 54%|█████▍    | 423/780 [02:51<01:41,  3.52it/s] 54%|█████▍    | 424/780 [02:52<01:41,  3.52it/s] 54%|█████▍    | 425/780 [02:52<01:40,  3.53it/s] 55%|█████▍    | 426/780 [02:52<01:40,  3.53it/s] 55%|█████▍    | 427/780 [02:53<01:40,  3.53it/s] 55%|█████▍    | 428/780 [02:53<01:39,  3.53it/s] 55%|█████▌    | 429/780 [02:53<01:39,  3.53it/s] 55%|█████▌    | 430/780 [02:53<01:39,  3.53it/s] 55%|█████▌    | 431/780 [02:54<01:38,  3.53it/s] 55%|█████▌    | 432/780 [02:54<01:38,  3.53it/s] 56%|█████▌    | 433/780 [02:54<01:42,  3.40it/s] 56%|█████▌    | 434/780 [02:55<01:40,  3.44it/s] 56%|█████▌    | 435/780 [02:55<01:39,  3.46it/s] 56%|█████▌    | 436/780 [02:55<01:38,  3.48it/s] 56%|█████▌    | 437/780 [02:55<01:38,  3.50it/s] 56%|█████▌    | 438/780 [02:56<01:37,  3.51it/s] 56%|█████▋    | 439/780 [02:56<01:36,  3.52it/s] 56%|█████▋    | 440/780 [02:56<01:36,  3.52it/s] 57%|█████▋    | 441/780 [02:57<01:36,  3.52it/s] 57%|█████▋    | 442/780 [02:57<01:35,  3.52it/s] 57%|█████▋    | 443/780 [02:57<01:35,  3.53it/s] 57%|█████▋    | 444/780 [02:57<01:39,  3.37it/s] 57%|█████▋    | 445/780 [02:58<01:38,  3.42it/s] 57%|█████▋    | 446/780 [02:58<01:36,  3.45it/s] 57%|█████▋    | 447/780 [02:58<01:35,  3.47it/s] 57%|█████▋    | 448/780 [02:59<01:35,  3.49it/s] 58%|█████▊    | 449/780 [02:59<01:34,  3.50it/s] 58%|█████▊    | 450/780 [02:59<01:34,  3.51it/s] 58%|█████▊    | 451/780 [02:59<01:33,  3.52it/s] 58%|█████▊    | 452/780 [03:00<01:33,  3.52it/s] 58%|█████▊    | 453/780 [03:00<01:32,  3.52it/s] 58%|█████▊    | 454/780 [03:00<01:32,  3.52it/s] 58%|█████▊    | 455/780 [03:01<01:35,  3.39it/s] 58%|█████▊    | 456/780 [03:01<01:34,  3.43it/s] 59%|█████▊    | 457/780 [03:01<01:33,  3.46it/s] 59%|█████▊    | 458/780 [03:01<01:32,  3.48it/s] 59%|█████▉    | 459/780 [03:02<01:31,  3.49it/s] 59%|█████▉    | 460/780 [03:02<01:31,  3.50it/s] 59%|█████▉    | 461/780 [03:02<01:30,  3.51it/s] 59%|█████▉    | 462/780 [03:03<01:30,  3.51it/s] 59%|█████▉    | 463/780 [03:03<01:30,  3.52it/s] 59%|█████▉    | 464/780 [03:03<01:29,  3.52it/s] 60%|█████▉    | 465/780 [03:03<01:29,  3.52it/s] 60%|█████▉    | 466/780 [03:04<01:32,  3.39it/s] 60%|█████▉    | 467/780 [03:04<01:31,  3.43it/s] 60%|██████    | 468/780 [03:04<01:30,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 21:39:04,872 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 21:39:04,872 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 21:39:04,872 >>   Batch size = 8
{'eval_loss': 1.0417149066925049, 'eval_runtime': 11.1268, 'eval_samples_per_second': 374.32, 'eval_steps_per_second': 46.824, 'epoch': 2.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 58.01it/s][A
  2%|▏         | 12/521 [00:00<00:09, 51.17it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.36it/s][A
  4%|▍         | 23/521 [00:00<00:10, 48.63it/s][A
  5%|▌         | 28/521 [00:00<00:10, 48.16it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.87it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.63it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.30it/s][A
  9%|▉         | 48/521 [00:00<00:10, 47.23it/s][A
 10%|█         | 53/521 [00:01<00:09, 47.30it/s][A
 11%|█         | 58/521 [00:01<00:09, 47.30it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 47.26it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 47.29it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 47.34it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 47.33it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 47.29it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 47.17it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 47.12it/s][A
 19%|█▉        | 98/521 [00:02<00:08, 47.13it/s][A
 20%|█▉        | 103/521 [00:02<00:09, 45.89it/s][A
 21%|██        | 108/521 [00:02<00:08, 46.29it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 46.63it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 46.83it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 46.95it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 47.07it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 47.16it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 47.18it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 47.07it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 47.09it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 47.14it/s][A
 30%|███       | 158/521 [00:03<00:07, 47.19it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 47.18it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 47.23it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 47.22it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 47.25it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 47.31it/s][A
 36%|███▌      | 188/521 [00:03<00:07, 47.19it/s][A
 37%|███▋      | 193/521 [00:04<00:06, 47.14it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 47.20it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 47.23it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 47.24it/s][A
 41%|████      | 213/521 [00:04<00:06, 47.20it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 47.25it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 47.23it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 47.21it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 47.24it/s][A
 46%|████▌     | 238/521 [00:05<00:05, 47.17it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 47.14it/s][A
 48%|████▊     | 248/521 [00:05<00:06, 45.13it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 45.78it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 46.22it/s][A
 50%|█████     | 263/521 [00:05<00:05, 46.52it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 46.79it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 46.95it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 47.03it/s][A
 54%|█████▍    | 283/521 [00:05<00:05, 47.13it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 47.04it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 46.99it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 46.98it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 47.12it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 47.14it/s][A
 60%|██████    | 313/521 [00:06<00:04, 47.15it/s][A
 61%|██████    | 318/521 [00:06<00:04, 47.21it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 47.23it/s][A
 63%|██████▎   | 328/521 [00:06<00:04, 47.19it/s][A
 64%|██████▍   | 333/521 [00:07<00:03, 47.10it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 47.08it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 47.08it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 47.07it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 47.10it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 47.09it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 47.14it/s][A
 71%|███████   | 368/521 [00:07<00:03, 47.24it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 47.20it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 47.15it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 47.15it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 47.12it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 45.97it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.29it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 46.58it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 46.76it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 46.93it/s][A
 80%|████████  | 418/521 [00:08<00:02, 47.03it/s][A
 81%|████████  | 423/521 [00:08<00:02, 47.06it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 47.14it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 47.06it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 47.03it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 47.12it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 47.15it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 47.11it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 47.18it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 47.23it/s][A
 90%|████████▉ | 468/521 [00:09<00:01, 47.18it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 47.14it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 47.14it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 47.10it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 47.08it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 47.14it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 47.14it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 47.13it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 47.19it/s][A
 98%|█████████▊| 513/521 [00:10<00:00, 47.22it/s][A
 99%|█████████▉| 518/521 [00:10<00:00, 47.22it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 47.22it/s][A 60%|██████    | 468/780 [03:15<01:30,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 21:39:16,176 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 21:39:16,332 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 21:39:19,666 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 21:39:19,836 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 21:39:19,902 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:29<39:18,  7.58s/it] 60%|██████    | 470/780 [03:29<27:55,  5.41s/it] 60%|██████    | 471/780 [03:29<19:55,  3.87s/it] 61%|██████    | 472/780 [03:30<14:20,  2.79s/it] 61%|██████    | 473/780 [03:30<10:26,  2.04s/it] 61%|██████    | 474/780 [03:30<07:42,  1.51s/it] 61%|██████    | 475/780 [03:31<05:48,  1.14s/it] 61%|██████    | 476/780 [03:31<04:29,  1.13it/s] 61%|██████    | 477/780 [03:31<03:33,  1.42it/s] 61%|██████▏   | 478/780 [03:31<02:54,  1.73it/s] 61%|██████▏   | 479/780 [03:32<02:27,  2.04it/s] 62%|██████▏   | 480/780 [03:32<02:08,  2.34it/s] 62%|██████▏   | 481/780 [03:32<01:57,  2.54it/s] 62%|██████▏   | 482/780 [03:33<01:47,  2.77it/s] 62%|██████▏   | 483/780 [03:33<01:40,  2.96it/s] 62%|██████▏   | 484/780 [03:33<01:35,  3.12it/s] 62%|██████▏   | 485/780 [03:33<01:31,  3.23it/s] 62%|██████▏   | 486/780 [03:34<01:28,  3.32it/s] 62%|██████▏   | 487/780 [03:34<01:26,  3.38it/s] 63%|██████▎   | 488/780 [03:34<01:25,  3.43it/s] 63%|██████▎   | 489/780 [03:35<01:24,  3.46it/s] 63%|██████▎   | 490/780 [03:35<01:23,  3.48it/s] 63%|██████▎   | 491/780 [03:35<01:22,  3.50it/s] 63%|██████▎   | 492/780 [03:36<01:26,  3.32it/s] 63%|██████▎   | 493/780 [03:36<01:24,  3.39it/s] 63%|██████▎   | 494/780 [03:36<01:23,  3.43it/s] 63%|██████▎   | 495/780 [03:36<01:22,  3.46it/s] 64%|██████▎   | 496/780 [03:37<01:21,  3.48it/s] 64%|██████▎   | 497/780 [03:37<01:20,  3.50it/s] 64%|██████▍   | 498/780 [03:37<01:20,  3.51it/s] 64%|██████▍   | 499/780 [03:37<01:19,  3.51it/s] 64%|██████▍   | 500/780 [03:38<01:19,  3.52it/s]                                                  64%|██████▍   | 500/780 [03:38<01:19,  3.52it/s] 64%|██████▍   | 501/780 [03:38<01:19,  3.52it/s] 64%|██████▍   | 502/780 [03:38<01:18,  3.53it/s] 64%|██████▍   | 503/780 [03:39<01:23,  3.31it/s] 65%|██████▍   | 504/780 [03:39<01:21,  3.37it/s] 65%|██████▍   | 505/780 [03:39<01:20,  3.42it/s] 65%|██████▍   | 506/780 [03:40<01:19,  3.45it/s] 65%|██████▌   | 507/780 [03:40<01:18,  3.48it/s] 65%|██████▌   | 508/780 [03:40<01:17,  3.50it/s] 65%|██████▌   | 509/780 [03:40<01:17,  3.51it/s] 65%|██████▌   | 510/780 [03:41<01:16,  3.52it/s] 66%|██████▌   | 511/780 [03:41<01:16,  3.52it/s] 66%|██████▌   | 512/780 [03:41<01:15,  3.53it/s] 66%|██████▌   | 513/780 [03:42<01:15,  3.53it/s] 66%|██████▌   | 514/780 [03:42<01:19,  3.34it/s] 66%|██████▌   | 515/780 [03:42<01:17,  3.40it/s] 66%|██████▌   | 516/780 [03:42<01:16,  3.44it/s] 66%|██████▋   | 517/780 [03:43<01:15,  3.47it/s] 66%|██████▋   | 518/780 [03:43<01:15,  3.49it/s] 67%|██████▋   | 519/780 [03:43<01:14,  3.50it/s] 67%|██████▋   | 520/780 [03:44<01:18,  3.32it/s] 67%|██████▋   | 521/780 [03:44<01:16,  3.37it/s] 67%|██████▋   | 522/780 [03:44<01:15,  3.42it/s] 67%|██████▋   | 523/780 [03:44<01:14,  3.45it/s] 67%|██████▋   | 524/780 [03:45<01:13,  3.48it/s] 67%|██████▋   | 525/780 [03:45<01:17,  3.27it/s] 67%|██████▋   | 526/780 [03:45<01:15,  3.35it/s] 68%|██████▊   | 527/780 [03:46<01:53,  2.23it/s] 68%|██████▊   | 528/780 [03:46<01:40,  2.50it/s] 68%|██████▊   | 529/780 [03:47<01:31,  2.74it/s] 68%|██████▊   | 530/780 [03:47<01:25,  2.94it/s] 68%|██████▊   | 531/780 [03:47<01:20,  3.09it/s] 68%|██████▊   | 532/780 [03:48<01:17,  3.22it/s] 68%|██████▊   | 533/780 [03:48<01:14,  3.31it/s] 68%|██████▊   | 534/780 [03:48<01:14,  3.28it/s] 69%|██████▊   | 535/780 [03:48<01:13,  3.35it/s] 69%|██████▊   | 536/780 [03:49<01:11,  3.41it/s] 69%|██████▉   | 537/780 [03:49<01:10,  3.44it/s] 69%|██████▉   | 538/780 [03:49<01:09,  3.47it/s] 69%|██████▉   | 539/780 [03:50<01:09,  3.49it/s] 69%|██████▉   | 540/780 [03:50<01:08,  3.50it/s] 69%|██████▉   | 541/780 [03:50<01:08,  3.51it/s] 69%|██████▉   | 542/780 [03:50<01:07,  3.52it/s] 70%|██████▉   | 543/780 [03:51<01:07,  3.52it/s] 70%|██████▉   | 544/780 [03:51<01:06,  3.53it/s] 70%|██████▉   | 545/780 [03:51<01:07,  3.46it/s] 70%|███████   | 546/780 [03:52<01:07,  3.48it/s] 70%|███████   | 547/780 [03:52<01:06,  3.50it/s] 70%|███████   | 548/780 [03:52<01:06,  3.51it/s] 70%|███████   | 549/780 [03:52<01:05,  3.52it/s] 71%|███████   | 550/780 [03:53<01:05,  3.52it/s] 71%|███████   | 551/780 [03:53<01:04,  3.53it/s] 71%|███████   | 552/780 [03:53<01:04,  3.53it/s] 71%|███████   | 553/780 [03:54<01:04,  3.53it/s] 71%|███████   | 554/780 [03:54<01:03,  3.53it/s] 71%|███████   | 555/780 [03:54<01:03,  3.53it/s] 71%|███████▏  | 556/780 [03:54<01:03,  3.53it/s] 71%|███████▏  | 557/780 [03:55<01:03,  3.53it/s] 72%|███████▏  | 558/780 [03:55<01:02,  3.53it/s] 72%|███████▏  | 559/780 [03:55<01:02,  3.53it/s] 72%|███████▏  | 560/780 [03:56<01:02,  3.53it/s] 72%|███████▏  | 561/780 [03:56<01:03,  3.43it/s] 72%|███████▏  | 562/780 [03:56<01:02,  3.46it/s] 72%|███████▏  | 563/780 [03:56<01:02,  3.49it/s] 72%|███████▏  | 564/780 [03:57<01:01,  3.50it/s] 72%|███████▏  | 565/780 [03:57<01:01,  3.51it/s] 73%|███████▎  | 566/780 [03:57<01:00,  3.52it/s] 73%|███████▎  | 567/780 [03:58<01:00,  3.52it/s] 73%|███████▎  | 568/780 [03:58<01:00,  3.53it/s] 73%|███████▎  | 569/780 [03:58<00:59,  3.53it/s] 73%|███████▎  | 570/780 [03:58<00:59,  3.53it/s] 73%|███████▎  | 571/780 [03:59<00:59,  3.53it/s] 73%|███████▎  | 572/780 [03:59<01:00,  3.43it/s] 73%|███████▎  | 573/780 [03:59<00:59,  3.46it/s] 74%|███████▎  | 574/780 [04:00<00:59,  3.48it/s] 74%|███████▎  | 575/780 [04:00<00:58,  3.50it/s] 74%|███████▍  | 576/780 [04:00<00:58,  3.51it/s] 74%|███████▍  | 577/780 [04:00<00:57,  3.52it/s] 74%|███████▍  | 578/780 [04:01<00:57,  3.52it/s] 74%|███████▍  | 579/780 [04:01<00:57,  3.52it/s] 74%|███████▍  | 580/780 [04:01<00:56,  3.53it/s] 74%|███████▍  | 581/780 [04:02<00:56,  3.53it/s] 75%|███████▍  | 582/780 [04:02<00:56,  3.53it/s] 75%|███████▍  | 583/780 [04:02<00:58,  3.39it/s] 75%|███████▍  | 584/780 [04:02<00:57,  3.43it/s] 75%|███████▌  | 585/780 [04:03<00:56,  3.46it/s] 75%|███████▌  | 586/780 [04:03<00:55,  3.48it/s] 75%|███████▌  | 587/780 [04:03<00:55,  3.50it/s] 75%|███████▌  | 588/780 [04:04<00:54,  3.51it/s] 76%|███████▌  | 589/780 [04:04<00:54,  3.52it/s] 76%|███████▌  | 590/780 [04:04<00:53,  3.52it/s] 76%|███████▌  | 591/780 [04:04<00:53,  3.53it/s] 76%|███████▌  | 592/780 [04:05<00:53,  3.53it/s] 76%|███████▌  | 593/780 [04:05<00:52,  3.53it/s] 76%|███████▌  | 594/780 [04:05<00:55,  3.38it/s] 76%|███████▋  | 595/780 [04:06<00:54,  3.42it/s] 76%|███████▋  | 596/780 [04:06<00:53,  3.46it/s] 77%|███████▋  | 597/780 [04:06<00:52,  3.48it/s] 77%|███████▋  | 598/780 [04:06<00:52,  3.49it/s] 77%|███████▋  | 599/780 [04:07<00:51,  3.51it/s] 77%|███████▋  | 600/780 [04:07<00:51,  3.52it/s] 77%|███████▋  | 601/780 [04:07<00:50,  3.52it/s] 77%|███████▋  | 602/780 [04:08<00:50,  3.53it/s] 77%|███████▋  | 603/780 [04:08<00:50,  3.53it/s] 77%|███████▋  | 604/780 [04:08<00:49,  3.53it/s] 78%|███████▊  | 605/780 [04:08<00:51,  3.41it/s] 78%|███████▊  | 606/780 [04:09<00:50,  3.44it/s] 78%|███████▊  | 607/780 [04:09<00:49,  3.47it/s] 78%|███████▊  | 608/780 [04:09<00:49,  3.49it/s] 78%|███████▊  | 609/780 [04:10<00:48,  3.50it/s] 78%|███████▊  | 610/780 [04:10<00:48,  3.51it/s] 78%|███████▊  | 611/780 [04:10<00:48,  3.52it/s] 78%|███████▊  | 612/780 [04:10<00:47,  3.52it/s] 79%|███████▊  | 613/780 [04:11<00:47,  3.53it/s] 79%|███████▊  | 614/780 [04:11<00:47,  3.53it/s] 79%|███████▉  | 615/780 [04:11<00:46,  3.53it/s] 79%|███████▉  | 616/780 [04:12<00:47,  3.45it/s] 79%|███████▉  | 617/780 [04:12<00:47,  3.47it/s] 79%|███████▉  | 618/780 [04:12<00:46,  3.49it/s] 79%|███████▉  | 619/780 [04:12<00:46,  3.50it/s] 79%|███████▉  | 620/780 [04:13<00:45,  3.51it/s] 80%|███████▉  | 621/780 [04:13<00:45,  3.52it/s] 80%|███████▉  | 622/780 [04:13<00:44,  3.52it/s] 80%|███████▉  | 623/780 [04:14<00:44,  3.52it/s] 80%|████████  | 624/780 [04:14<00:44,  3.52it/s][INFO|trainer.py:2140] 2023-08-29 21:40:14,455 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 21:40:14,455 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 21:40:14,455 >>   Batch size = 8
{'eval_loss': 1.0592352151870728, 'eval_runtime': 11.0975, 'eval_samples_per_second': 375.31, 'eval_steps_per_second': 46.948, 'epoch': 3.0}
{'loss': 0.4085, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.89it/s][A
  2%|▏         | 12/521 [00:00<00:09, 51.13it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.39it/s][A
  4%|▍         | 23/521 [00:00<00:10, 47.47it/s][A
  5%|▌         | 28/521 [00:00<00:10, 47.46it/s][A
  6%|▋         | 33/521 [00:00<00:10, 47.45it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.31it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.08it/s][A
  9%|▉         | 48/521 [00:01<00:10, 47.12it/s][A
 10%|█         | 53/521 [00:01<00:09, 47.17it/s][A
 11%|█         | 58/521 [00:01<00:09, 47.21it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 47.25it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 47.29it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 47.25it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 47.26it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 47.21it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 47.08it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 47.13it/s][A
 19%|█▉        | 98/521 [00:02<00:08, 47.13it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 47.21it/s][A
 21%|██        | 108/521 [00:02<00:08, 47.18it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 47.26it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 47.27it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 47.20it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 47.23it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 47.16it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 47.12it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 47.14it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 47.18it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 47.21it/s][A
 30%|███       | 158/521 [00:03<00:07, 47.17it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 47.25it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 44.31it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 45.16it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 45.80it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 46.23it/s][A
 36%|███▌      | 188/521 [00:03<00:07, 46.54it/s][A
 37%|███▋      | 193/521 [00:04<00:07, 46.80it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 46.94it/s][A
 39%|███▉      | 203/521 [00:04<00:06, 47.04it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 46.87it/s][A
 41%|████      | 213/521 [00:04<00:06, 46.95it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 47.03it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 47.08it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 47.16it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 47.20it/s][A
 46%|████▌     | 238/521 [00:05<00:05, 47.23it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 47.26it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 47.28it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 47.10it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 47.06it/s][A
 50%|█████     | 263/521 [00:05<00:05, 47.12it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 47.16it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 47.19it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 47.24it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 47.25it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 47.25it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 47.30it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 47.20it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 47.13it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 47.12it/s][A
 60%|██████    | 313/521 [00:06<00:04, 44.12it/s][A
 61%|██████    | 318/521 [00:06<00:04, 45.01it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 45.72it/s][A
 63%|██████▎   | 328/521 [00:06<00:04, 46.16it/s][A
 64%|██████▍   | 333/521 [00:07<00:04, 46.48it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 46.74it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 46.92it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 47.03it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 46.87it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 46.96it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 47.06it/s][A
 71%|███████   | 368/521 [00:07<00:03, 47.13it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 47.19it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 47.21it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 47.21it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 47.25it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 47.28it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 47.19it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 47.11it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 47.17it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 47.20it/s][A
 80%|████████  | 418/521 [00:08<00:02, 47.22it/s][A
 81%|████████  | 423/521 [00:08<00:02, 47.26it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 47.28it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 47.27it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 47.25it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 47.21it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 47.14it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 47.07it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 47.18it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 46.02it/s][A
 90%|████████▉ | 468/521 [00:09<00:01, 46.51it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 46.75it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 46.89it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 47.03it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 47.09it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 47.05it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 47.04it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 47.12it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 47.07it/s][A
 98%|█████████▊| 513/521 [00:10<00:00, 47.10it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 47.19it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 47.19it/s][A 80%|████████  | 624/780 [04:25<00:44,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 21:40:25,893 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 21:40:26,140 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 21:40:30,042 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 21:40:30,260 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 21:40:30,362 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:38<19:22,  7.50s/it] 80%|████████  | 626/780 [04:39<13:43,  5.35s/it] 80%|████████  | 627/780 [04:39<09:45,  3.83s/it] 81%|████████  | 628/780 [04:39<07:00,  2.76s/it] 81%|████████  | 629/780 [04:39<05:04,  2.02s/it] 81%|████████  | 630/780 [04:40<03:44,  1.50s/it] 81%|████████  | 631/780 [04:40<02:48,  1.13s/it] 81%|████████  | 632/780 [04:40<02:09,  1.14it/s] 81%|████████  | 633/780 [04:40<01:42,  1.43it/s] 81%|████████▏ | 634/780 [04:41<01:23,  1.74it/s] 81%|████████▏ | 635/780 [04:41<01:10,  2.05it/s] 82%|████████▏ | 636/780 [04:41<01:01,  2.35it/s] 82%|████████▏ | 637/780 [04:42<00:58,  2.45it/s] 82%|████████▏ | 638/780 [04:42<00:52,  2.70it/s] 82%|████████▏ | 639/780 [04:42<00:48,  2.91it/s] 82%|████████▏ | 640/780 [04:43<00:45,  3.07it/s] 82%|████████▏ | 641/780 [04:43<00:43,  3.20it/s] 82%|████████▏ | 642/780 [04:43<00:41,  3.29it/s] 82%|████████▏ | 643/780 [04:43<00:40,  3.36it/s] 83%|████████▎ | 644/780 [04:44<00:42,  3.23it/s] 83%|████████▎ | 645/780 [04:44<00:40,  3.30it/s] 83%|████████▎ | 646/780 [04:44<00:39,  3.37it/s] 83%|████████▎ | 647/780 [04:45<00:38,  3.42it/s] 83%|████████▎ | 648/780 [04:45<00:41,  3.22it/s] 83%|████████▎ | 649/780 [04:45<00:39,  3.31it/s] 83%|████████▎ | 650/780 [04:46<00:38,  3.37it/s] 83%|████████▎ | 651/780 [04:46<01:02,  2.08it/s] 84%|████████▎ | 652/780 [04:47<00:54,  2.36it/s] 84%|████████▎ | 653/780 [04:47<00:48,  2.63it/s] 84%|████████▍ | 654/780 [04:47<00:44,  2.85it/s] 84%|████████▍ | 655/780 [04:48<00:41,  3.02it/s] 84%|████████▍ | 656/780 [04:48<00:39,  3.16it/s] 84%|████████▍ | 657/780 [04:48<00:39,  3.11it/s] 84%|████████▍ | 658/780 [04:48<00:37,  3.23it/s] 84%|████████▍ | 659/780 [04:49<00:36,  3.31it/s] 85%|████████▍ | 660/780 [04:49<00:35,  3.38it/s] 85%|████████▍ | 661/780 [04:49<00:34,  3.42it/s] 85%|████████▍ | 662/780 [04:50<00:34,  3.46it/s] 85%|████████▌ | 663/780 [04:50<00:33,  3.48it/s] 85%|████████▌ | 664/780 [04:50<00:33,  3.50it/s] 85%|████████▌ | 665/780 [04:50<00:32,  3.51it/s] 85%|████████▌ | 666/780 [04:51<00:32,  3.52it/s] 86%|████████▌ | 667/780 [04:51<00:32,  3.52it/s] 86%|████████▌ | 668/780 [04:51<00:32,  3.42it/s] 86%|████████▌ | 669/780 [04:52<00:32,  3.45it/s] 86%|████████▌ | 670/780 [04:52<00:31,  3.48it/s] 86%|████████▌ | 671/780 [04:52<00:31,  3.49it/s] 86%|████████▌ | 672/780 [04:52<00:30,  3.51it/s] 86%|████████▋ | 673/780 [04:53<00:30,  3.52it/s] 86%|████████▋ | 674/780 [04:53<00:30,  3.52it/s] 87%|████████▋ | 675/780 [04:53<00:29,  3.53it/s] 87%|████████▋ | 676/780 [04:54<00:29,  3.53it/s] 87%|████████▋ | 677/780 [04:54<00:29,  3.53it/s] 87%|████████▋ | 678/780 [04:54<00:28,  3.53it/s] 87%|████████▋ | 679/780 [04:54<00:28,  3.53it/s] 87%|████████▋ | 680/780 [04:55<00:28,  3.53it/s] 87%|████████▋ | 681/780 [04:55<00:28,  3.53it/s] 87%|████████▋ | 682/780 [04:55<00:27,  3.54it/s] 88%|████████▊ | 683/780 [04:56<00:27,  3.54it/s] 88%|████████▊ | 684/780 [04:56<00:27,  3.53it/s] 88%|████████▊ | 685/780 [04:56<00:26,  3.53it/s] 88%|████████▊ | 686/780 [04:56<00:26,  3.54it/s] 88%|████████▊ | 687/780 [04:57<00:26,  3.54it/s] 88%|████████▊ | 688/780 [04:57<00:26,  3.54it/s] 88%|████████▊ | 689/780 [04:57<00:26,  3.41it/s] 88%|████████▊ | 690/780 [04:58<00:26,  3.45it/s] 89%|████████▊ | 691/780 [04:58<00:25,  3.47it/s] 89%|████████▊ | 692/780 [04:58<00:25,  3.49it/s] 89%|████████▉ | 693/780 [04:58<00:24,  3.51it/s] 89%|████████▉ | 694/780 [04:59<00:24,  3.51it/s] 89%|████████▉ | 695/780 [04:59<00:24,  3.52it/s] 89%|████████▉ | 696/780 [04:59<00:23,  3.53it/s] 89%|████████▉ | 697/780 [05:00<00:23,  3.53it/s] 89%|████████▉ | 698/780 [05:00<00:23,  3.53it/s] 90%|████████▉ | 699/780 [05:00<00:22,  3.53it/s] 90%|████████▉ | 700/780 [05:00<00:23,  3.42it/s] 90%|████████▉ | 701/780 [05:01<00:22,  3.46it/s] 90%|█████████ | 702/780 [05:01<00:22,  3.48it/s] 90%|█████████ | 703/780 [05:01<00:22,  3.50it/s] 90%|█████████ | 704/780 [05:02<00:21,  3.51it/s] 90%|█████████ | 705/780 [05:02<00:21,  3.52it/s] 91%|█████████ | 706/780 [05:02<00:21,  3.52it/s] 91%|█████████ | 707/780 [05:02<00:20,  3.53it/s] 91%|█████████ | 708/780 [05:03<00:20,  3.53it/s] 91%|█████████ | 709/780 [05:03<00:20,  3.53it/s] 91%|█████████ | 710/780 [05:03<00:19,  3.53it/s] 91%|█████████ | 711/780 [05:04<00:20,  3.40it/s] 91%|█████████▏| 712/780 [05:04<00:19,  3.44it/s] 91%|█████████▏| 713/780 [05:04<00:19,  3.47it/s] 92%|█████████▏| 714/780 [05:04<00:18,  3.49it/s] 92%|█████████▏| 715/780 [05:05<00:18,  3.50it/s] 92%|█████████▏| 716/780 [05:05<00:18,  3.51it/s] 92%|█████████▏| 717/780 [05:05<00:17,  3.52it/s] 92%|█████████▏| 718/780 [05:06<00:17,  3.52it/s] 92%|█████████▏| 719/780 [05:06<00:17,  3.53it/s] 92%|█████████▏| 720/780 [05:06<00:17,  3.53it/s] 92%|█████████▏| 721/780 [05:06<00:16,  3.53it/s] 93%|█████████▎| 722/780 [05:07<00:16,  3.41it/s] 93%|█████████▎| 723/780 [05:07<00:16,  3.45it/s] 93%|█████████▎| 724/780 [05:07<00:16,  3.48it/s] 93%|█████████▎| 725/780 [05:08<00:15,  3.49it/s] 93%|█████████▎| 726/780 [05:08<00:15,  3.51it/s] 93%|█████████▎| 727/780 [05:08<00:15,  3.52it/s] 93%|█████████▎| 728/780 [05:08<00:14,  3.52it/s] 93%|█████████▎| 729/780 [05:09<00:14,  3.53it/s] 94%|█████████▎| 730/780 [05:09<00:14,  3.53it/s] 94%|█████████▎| 731/780 [05:09<00:13,  3.53it/s] 94%|█████████▍| 732/780 [05:10<00:13,  3.53it/s] 94%|█████████▍| 733/780 [05:10<00:13,  3.38it/s] 94%|█████████▍| 734/780 [05:10<00:13,  3.43it/s] 94%|█████████▍| 735/780 [05:10<00:13,  3.46it/s] 94%|█████████▍| 736/780 [05:11<00:12,  3.48it/s] 94%|█████████▍| 737/780 [05:11<00:12,  3.50it/s] 95%|█████████▍| 738/780 [05:11<00:11,  3.51it/s] 95%|█████████▍| 739/780 [05:12<00:11,  3.52it/s] 95%|█████████▍| 740/780 [05:12<00:11,  3.52it/s] 95%|█████████▌| 741/780 [05:12<00:11,  3.53it/s] 95%|█████████▌| 742/780 [05:12<00:10,  3.53it/s] 95%|█████████▌| 743/780 [05:13<00:10,  3.53it/s] 95%|█████████▌| 744/780 [05:13<00:10,  3.45it/s] 96%|█████████▌| 745/780 [05:13<00:10,  3.48it/s] 96%|█████████▌| 746/780 [05:14<00:09,  3.49it/s] 96%|█████████▌| 747/780 [05:14<00:09,  3.51it/s] 96%|█████████▌| 748/780 [05:14<00:09,  3.51it/s] 96%|█████████▌| 749/780 [05:14<00:08,  3.52it/s] 96%|█████████▌| 750/780 [05:15<00:08,  3.52it/s] 96%|█████████▋| 751/780 [05:15<00:08,  3.53it/s] 96%|█████████▋| 752/780 [05:15<00:07,  3.53it/s] 97%|█████████▋| 753/780 [05:16<00:07,  3.53it/s] 97%|█████████▋| 754/780 [05:16<00:07,  3.53it/s] 97%|█████████▋| 755/780 [05:16<00:07,  3.44it/s] 97%|█████████▋| 756/780 [05:16<00:06,  3.47it/s] 97%|█████████▋| 757/780 [05:17<00:06,  3.49it/s] 97%|█████████▋| 758/780 [05:17<00:06,  3.50it/s] 97%|█████████▋| 759/780 [05:17<00:05,  3.51it/s] 97%|█████████▋| 760/780 [05:18<00:05,  3.52it/s] 98%|█████████▊| 761/780 [05:18<00:05,  3.52it/s] 98%|█████████▊| 762/780 [05:18<00:05,  3.52it/s] 98%|█████████▊| 763/780 [05:18<00:04,  3.53it/s] 98%|█████████▊| 764/780 [05:19<00:04,  3.53it/s] 98%|█████████▊| 765/780 [05:19<00:04,  3.53it/s] 98%|█████████▊| 766/780 [05:19<00:04,  3.38it/s] 98%|█████████▊| 767/780 [05:20<00:03,  3.43it/s] 98%|█████████▊| 768/780 [05:20<00:03,  3.46it/s] 99%|█████████▊| 769/780 [05:20<00:03,  3.48it/s] 99%|█████████▊| 770/780 [05:20<00:02,  3.49it/s] 99%|█████████▉| 771/780 [05:21<00:02,  3.50it/s] 99%|█████████▉| 772/780 [05:21<00:02,  3.51it/s] 99%|█████████▉| 773/780 [05:21<00:01,  3.52it/s] 99%|█████████▉| 774/780 [05:22<00:01,  3.52it/s] 99%|█████████▉| 775/780 [05:22<00:01,  3.53it/s] 99%|█████████▉| 776/780 [05:22<00:01,  3.53it/s]100%|█████████▉| 777/780 [05:22<00:00,  3.43it/s]100%|█████████▉| 778/780 [05:23<00:00,  3.46it/s]100%|█████████▉| 779/780 [05:23<00:00,  3.48it/s]100%|██████████| 780/780 [05:23<00:00,  3.49it/s][INFO|trainer.py:2140] 2023-08-29 21:41:23,850 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 21:41:23,850 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 21:41:23,850 >>   Batch size = 8
{'eval_loss': 1.0688855648040771, 'eval_runtime': 11.1219, 'eval_samples_per_second': 374.485, 'eval_steps_per_second': 46.844, 'epoch': 4.0}

  0%|          | 0/521 [00:00<?, ?it/s][A
  1%|          | 6/521 [00:00<00:08, 57.51it/s][A
  2%|▏         | 12/521 [00:00<00:09, 51.18it/s][A
  3%|▎         | 18/521 [00:00<00:10, 49.41it/s][A
  4%|▍         | 23/521 [00:00<00:10, 45.55it/s][A
  5%|▌         | 28/521 [00:00<00:10, 46.74it/s][A
  6%|▋         | 33/521 [00:00<00:10, 46.94it/s][A
  7%|▋         | 38/521 [00:00<00:10, 47.04it/s][A
  8%|▊         | 43/521 [00:00<00:10, 47.07it/s][A
  9%|▉         | 48/521 [00:01<00:10, 47.14it/s][A
 10%|█         | 53/521 [00:01<00:09, 47.20it/s][A
 11%|█         | 58/521 [00:01<00:09, 47.20it/s][A
 12%|█▏        | 63/521 [00:01<00:09, 46.85it/s][A
 13%|█▎        | 68/521 [00:01<00:09, 46.86it/s][A
 14%|█▍        | 73/521 [00:01<00:09, 46.96it/s][A
 15%|█▍        | 78/521 [00:01<00:09, 47.09it/s][A
 16%|█▌        | 83/521 [00:01<00:09, 47.13it/s][A
 17%|█▋        | 88/521 [00:01<00:09, 47.11it/s][A
 18%|█▊        | 93/521 [00:01<00:09, 47.10it/s][A
 19%|█▉        | 98/521 [00:02<00:08, 47.18it/s][A
 20%|█▉        | 103/521 [00:02<00:08, 47.14it/s][A
 21%|██        | 108/521 [00:02<00:08, 47.11it/s][A
 22%|██▏       | 113/521 [00:02<00:08, 47.16it/s][A
 23%|██▎       | 118/521 [00:02<00:08, 47.10it/s][A
 24%|██▎       | 123/521 [00:02<00:08, 47.14it/s][A
 25%|██▍       | 128/521 [00:02<00:08, 47.20it/s][A
 26%|██▌       | 133/521 [00:02<00:08, 47.17it/s][A
 26%|██▋       | 138/521 [00:02<00:08, 47.16it/s][A
 27%|██▋       | 143/521 [00:03<00:08, 47.17it/s][A
 28%|██▊       | 148/521 [00:03<00:07, 47.17it/s][A
 29%|██▉       | 153/521 [00:03<00:07, 47.14it/s][A
 30%|███       | 158/521 [00:03<00:07, 47.08it/s][A
 31%|███▏      | 163/521 [00:03<00:07, 47.17it/s][A
 32%|███▏      | 168/521 [00:03<00:07, 47.19it/s][A
 33%|███▎      | 173/521 [00:03<00:07, 47.22it/s][A
 34%|███▍      | 178/521 [00:03<00:07, 47.21it/s][A
 35%|███▌      | 183/521 [00:03<00:07, 47.22it/s][A
 36%|███▌      | 188/521 [00:03<00:07, 47.26it/s][A
 37%|███▋      | 193/521 [00:04<00:06, 47.19it/s][A
 38%|███▊      | 198/521 [00:04<00:06, 47.17it/s][A
 39%|███▉      | 203/521 [00:04<00:07, 43.69it/s][A
 40%|███▉      | 208/521 [00:04<00:06, 44.75it/s][A
 41%|████      | 213/521 [00:04<00:06, 45.48it/s][A
 42%|████▏     | 218/521 [00:04<00:06, 45.98it/s][A
 43%|████▎     | 223/521 [00:04<00:06, 46.38it/s][A
 44%|████▍     | 228/521 [00:04<00:06, 46.63it/s][A
 45%|████▍     | 233/521 [00:04<00:06, 46.82it/s][A
 46%|████▌     | 238/521 [00:05<00:06, 46.99it/s][A
 47%|████▋     | 243/521 [00:05<00:05, 46.82it/s][A
 48%|████▊     | 248/521 [00:05<00:05, 46.84it/s][A
 49%|████▊     | 253/521 [00:05<00:05, 46.98it/s][A
 50%|████▉     | 258/521 [00:05<00:05, 47.09it/s][A
 50%|█████     | 263/521 [00:05<00:05, 47.16it/s][A
 51%|█████▏    | 268/521 [00:05<00:05, 47.18it/s][A
 52%|█████▏    | 273/521 [00:05<00:05, 47.26it/s][A
 53%|█████▎    | 278/521 [00:05<00:05, 47.26it/s][A
 54%|█████▍    | 283/521 [00:06<00:05, 47.26it/s][A
 55%|█████▌    | 288/521 [00:06<00:04, 47.16it/s][A
 56%|█████▌    | 293/521 [00:06<00:04, 47.11it/s][A
 57%|█████▋    | 298/521 [00:06<00:04, 47.09it/s][A
 58%|█████▊    | 303/521 [00:06<00:04, 47.11it/s][A
 59%|█████▉    | 308/521 [00:06<00:04, 47.21it/s][A
 60%|██████    | 313/521 [00:06<00:04, 47.19it/s][A
 61%|██████    | 318/521 [00:06<00:04, 47.13it/s][A
 62%|██████▏   | 323/521 [00:06<00:04, 47.24it/s][A
 63%|██████▎   | 328/521 [00:06<00:04, 47.22it/s][A
 64%|██████▍   | 333/521 [00:07<00:03, 47.16it/s][A
 65%|██████▍   | 338/521 [00:07<00:03, 47.08it/s][A
 66%|██████▌   | 343/521 [00:07<00:03, 47.05it/s][A
 67%|██████▋   | 348/521 [00:07<00:03, 44.62it/s][A
 68%|██████▊   | 353/521 [00:07<00:03, 45.40it/s][A
 69%|██████▊   | 358/521 [00:07<00:03, 45.99it/s][A
 70%|██████▉   | 363/521 [00:07<00:03, 46.35it/s][A
 71%|███████   | 368/521 [00:07<00:03, 46.60it/s][A
 72%|███████▏  | 373/521 [00:07<00:03, 46.84it/s][A
 73%|███████▎  | 378/521 [00:08<00:03, 46.96it/s][A
 74%|███████▎  | 383/521 [00:08<00:02, 47.04it/s][A
 74%|███████▍  | 388/521 [00:08<00:02, 46.92it/s][A
 75%|███████▌  | 393/521 [00:08<00:02, 46.90it/s][A
 76%|███████▋  | 398/521 [00:08<00:02, 46.99it/s][A
 77%|███████▋  | 403/521 [00:08<00:02, 47.07it/s][A
 78%|███████▊  | 408/521 [00:08<00:02, 47.17it/s][A
 79%|███████▉  | 413/521 [00:08<00:02, 47.14it/s][A
 80%|████████  | 418/521 [00:08<00:02, 47.11it/s][A
 81%|████████  | 423/521 [00:09<00:02, 47.21it/s][A
 82%|████████▏ | 428/521 [00:09<00:01, 47.20it/s][A
 83%|████████▎ | 433/521 [00:09<00:01, 47.07it/s][A
 84%|████████▍ | 438/521 [00:09<00:01, 46.99it/s][A
 85%|████████▌ | 443/521 [00:09<00:01, 47.04it/s][A
 86%|████████▌ | 448/521 [00:09<00:01, 47.11it/s][A
 87%|████████▋ | 453/521 [00:09<00:01, 47.20it/s][A
 88%|████████▊ | 458/521 [00:09<00:01, 47.21it/s][A
 89%|████████▉ | 463/521 [00:09<00:01, 47.18it/s][A
 90%|████████▉ | 468/521 [00:09<00:01, 47.22it/s][A
 91%|█████████ | 473/521 [00:10<00:01, 47.26it/s][A
 92%|█████████▏| 478/521 [00:10<00:00, 47.17it/s][A
 93%|█████████▎| 483/521 [00:10<00:00, 47.09it/s][A
 94%|█████████▎| 488/521 [00:10<00:00, 47.10it/s][A
 95%|█████████▍| 493/521 [00:10<00:00, 45.52it/s][A
 96%|█████████▌| 498/521 [00:10<00:00, 46.02it/s][A
 97%|█████████▋| 503/521 [00:10<00:00, 46.38it/s][A
 98%|█████████▊| 508/521 [00:10<00:00, 46.65it/s][A
 98%|█████████▊| 513/521 [00:10<00:00, 46.86it/s][A
 99%|█████████▉| 518/521 [00:11<00:00, 46.98it/s][A
                                                 [A                                                 
100%|██████████| 521/521 [00:11<00:00, 46.98it/s][A100%|██████████| 780/780 [05:34<00:00,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 21:41:35,190 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 21:41:35,370 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 21:41:39,363 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 21:41:39,527 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 21:41:39,600 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 21:41:50,346 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 21:41:50,411 >> Loading best model from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156 (score: 1.0246262550354004).
                                                 100%|██████████| 780/780 [06:02<00:00,  3.49it/s]100%|██████████| 780/780 [06:02<00:00,  2.15it/s]
[INFO|trainer.py:1894] 2023-08-29 21:42:02,614 >> Saving model checkpoint to outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 21:42:02,774 >> Configuration saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 21:42:06,682 >> Model weights saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 21:42:06,879 >> tokenizer config file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 21:42:06,955 >> Special tokens file saved in outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 21:42:07,554 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:07,554 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:07,554 >>   train_loss               =     0.4004
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:07,554 >>   train_runtime            = 0:06:02.50
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:07,554 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:07,554 >>   train_samples_per_second =    137.929
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:07,554 >>   train_steps_per_second   =      2.152
{'eval_loss': 1.0768579244613647, 'eval_runtime': 11.1133, 'eval_samples_per_second': 374.776, 'eval_steps_per_second': 46.881, 'epoch': 5.0}
{'train_runtime': 362.5056, 'train_samples_per_second': 137.929, 'train_steps_per_second': 2.152, 'train_loss': 0.40038565611227966, 'epoch': 5.0}
08/29/2023 21:42:07 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 21:42:07,844 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 21:42:07,845 >>   Num examples = 4165
[INFO|trainer.py:2145] 2023-08-29 21:42:07,845 >>   Batch size = 8
  0%|          | 0/521 [00:00<?, ?it/s]  1%|          | 6/521 [00:00<00:08, 58.97it/s]  2%|▏         | 12/521 [00:00<00:09, 51.92it/s]  3%|▎         | 18/521 [00:00<00:10, 50.00it/s]  5%|▍         | 24/521 [00:00<00:10, 49.14it/s]  6%|▌         | 29/521 [00:00<00:10, 48.72it/s]  7%|▋         | 34/521 [00:00<00:10, 48.46it/s]  7%|▋         | 39/521 [00:00<00:09, 48.25it/s]  8%|▊         | 44/521 [00:00<00:09, 48.11it/s]  9%|▉         | 49/521 [00:01<00:09, 47.94it/s] 10%|█         | 54/521 [00:01<00:09, 47.84it/s] 11%|█▏        | 59/521 [00:01<00:09, 47.85it/s] 12%|█▏        | 64/521 [00:01<00:09, 47.86it/s] 13%|█▎        | 69/521 [00:01<00:09, 47.88it/s] 14%|█▍        | 74/521 [00:01<00:09, 47.86it/s] 15%|█▌        | 79/521 [00:01<00:09, 47.84it/s] 16%|█▌        | 84/521 [00:01<00:09, 47.85it/s] 17%|█▋        | 89/521 [00:01<00:09, 47.85it/s] 18%|█▊        | 94/521 [00:01<00:09, 45.60it/s] 19%|█▉        | 99/521 [00:02<00:09, 46.18it/s] 20%|█▉        | 104/521 [00:02<00:08, 46.66it/s] 21%|██        | 109/521 [00:02<00:08, 47.01it/s] 22%|██▏       | 114/521 [00:02<00:08, 47.27it/s] 23%|██▎       | 119/521 [00:02<00:08, 47.46it/s] 24%|██▍       | 124/521 [00:02<00:08, 47.55it/s] 25%|██▍       | 129/521 [00:02<00:08, 47.66it/s] 26%|██▌       | 134/521 [00:02<00:08, 47.61it/s] 27%|██▋       | 139/521 [00:02<00:08, 47.63it/s] 28%|██▊       | 144/521 [00:03<00:07, 47.62it/s] 29%|██▊       | 149/521 [00:03<00:07, 47.62it/s] 30%|██▉       | 154/521 [00:03<00:07, 47.62it/s] 31%|███       | 159/521 [00:03<00:07, 47.62it/s] 31%|███▏      | 164/521 [00:03<00:07, 47.65it/s] 32%|███▏      | 169/521 [00:03<00:07, 47.71it/s] 33%|███▎      | 174/521 [00:03<00:07, 47.67it/s] 34%|███▍      | 179/521 [00:03<00:07, 47.62it/s] 35%|███▌      | 184/521 [00:03<00:07, 47.57it/s] 36%|███▋      | 189/521 [00:03<00:06, 47.61it/s] 37%|███▋      | 194/521 [00:04<00:06, 47.61it/s] 38%|███▊      | 199/521 [00:04<00:06, 47.59it/s] 39%|███▉      | 204/521 [00:04<00:06, 47.58it/s] 40%|████      | 209/521 [00:04<00:06, 47.61it/s] 41%|████      | 214/521 [00:04<00:06, 47.62it/s] 42%|████▏     | 219/521 [00:04<00:06, 47.61it/s] 43%|████▎     | 224/521 [00:04<00:06, 47.57it/s] 44%|████▍     | 229/521 [00:04<00:06, 47.50it/s] 45%|████▍     | 234/521 [00:04<00:06, 47.49it/s] 46%|████▌     | 239/521 [00:05<00:06, 46.19it/s] 47%|████▋     | 244/521 [00:05<00:05, 46.68it/s] 48%|████▊     | 249/521 [00:05<00:05, 46.93it/s] 49%|████▉     | 254/521 [00:05<00:05, 47.18it/s] 50%|████▉     | 259/521 [00:05<00:05, 47.32it/s] 51%|█████     | 264/521 [00:05<00:05, 47.41it/s] 52%|█████▏    | 269/521 [00:05<00:05, 47.47it/s] 53%|█████▎    | 274/521 [00:05<00:05, 47.47it/s] 54%|█████▎    | 279/521 [00:05<00:05, 47.40it/s] 55%|█████▍    | 284/521 [00:05<00:05, 47.36it/s] 55%|█████▌    | 289/521 [00:06<00:04, 47.44it/s] 56%|█████▋    | 294/521 [00:06<00:04, 47.55it/s] 57%|█████▋    | 299/521 [00:06<00:04, 47.56it/s] 58%|█████▊    | 304/521 [00:06<00:04, 47.57it/s] 59%|█████▉    | 309/521 [00:06<00:04, 47.56it/s] 60%|██████    | 314/521 [00:06<00:04, 47.62it/s] 61%|██████    | 319/521 [00:06<00:04, 47.62it/s] 62%|██████▏   | 324/521 [00:06<00:04, 47.53it/s] 63%|██████▎   | 329/521 [00:06<00:04, 47.45it/s] 64%|██████▍   | 334/521 [00:07<00:03, 47.47it/s] 65%|██████▌   | 339/521 [00:07<00:03, 47.50it/s] 66%|██████▌   | 344/521 [00:07<00:03, 47.57it/s] 67%|██████▋   | 349/521 [00:07<00:03, 47.60it/s] 68%|██████▊   | 354/521 [00:07<00:03, 47.67it/s] 69%|██████▉   | 359/521 [00:07<00:03, 47.69it/s] 70%|██████▉   | 364/521 [00:07<00:03, 47.72it/s] 71%|███████   | 369/521 [00:07<00:03, 47.62it/s] 72%|███████▏  | 374/521 [00:07<00:03, 47.60it/s] 73%|███████▎  | 379/521 [00:07<00:02, 47.57it/s] 74%|███████▎  | 384/521 [00:08<00:02, 46.40it/s] 75%|███████▍  | 389/521 [00:08<00:02, 46.82it/s] 76%|███████▌  | 394/521 [00:08<00:02, 47.10it/s] 77%|███████▋  | 399/521 [00:08<00:02, 47.26it/s] 78%|███████▊  | 404/521 [00:08<00:02, 47.43it/s] 79%|███████▊  | 409/521 [00:08<00:02, 47.53it/s] 79%|███████▉  | 414/521 [00:08<00:02, 47.55it/s] 80%|████████  | 419/521 [00:08<00:02, 47.53it/s] 81%|████████▏ | 424/521 [00:08<00:02, 47.47it/s] 82%|████████▏ | 429/521 [00:09<00:01, 47.43it/s] 83%|████████▎ | 434/521 [00:09<00:01, 47.45it/s] 84%|████████▍ | 439/521 [00:09<00:01, 47.51it/s] 85%|████████▌ | 444/521 [00:09<00:01, 47.62it/s] 86%|████████▌ | 449/521 [00:09<00:01, 47.58it/s] 87%|████████▋ | 454/521 [00:09<00:01, 47.57it/s] 88%|████████▊ | 459/521 [00:09<00:01, 47.55it/s] 89%|████████▉ | 464/521 [00:09<00:01, 47.60it/s] 90%|█████████ | 469/521 [00:09<00:01, 47.57it/s] 91%|█████████ | 474/521 [00:09<00:00, 47.50it/s] 92%|█████████▏| 479/521 [00:10<00:00, 47.47it/s] 93%|█████████▎| 484/521 [00:10<00:00, 47.49it/s] 94%|█████████▍| 489/521 [00:10<00:00, 47.50it/s] 95%|█████████▍| 494/521 [00:10<00:00, 47.50it/s] 96%|█████████▌| 499/521 [00:10<00:00, 47.56it/s] 97%|█████████▋| 504/521 [00:10<00:00, 47.56it/s] 98%|█████████▊| 509/521 [00:10<00:00, 47.54it/s] 99%|█████████▊| 514/521 [00:10<00:00, 47.54it/s]100%|█████████▉| 519/521 [00:10<00:00, 47.59it/s]100%|██████████| 521/521 [00:10<00:00, 47.57it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 21:42:18,819 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:18,819 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:18,819 >>   eval_loss               =     1.0246
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:18,819 >>   eval_runtime            = 0:00:10.97
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:18,819 >>   eval_samples            =       4165
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:18,819 >>   eval_samples_per_second =    379.517
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:18,819 >>   eval_steps_per_second   =     47.474
[INFO|trainer_pt_utils.py:913] 2023-08-29 21:42:18,819 >>   perplexity              =     2.7861
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:28,950 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:28,952 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:28,952 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:28,952 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:28,952 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 21:42:29,844 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 21:42:29,880 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 21:42:30,492 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 21:42:31,559 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 21:42:31,559 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:34,721 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:34,747 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:34,747 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:34,747 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:42:34,747 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 21:42:35,499 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 21:42:35,500 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 21:42:36,089 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 21:42:36,286 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 21:42:36,287 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-624
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-156
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-780
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-312
outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/generator/iter5/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'lowest point', 'manufacturer', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12876
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12976, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.67it/s]Extractor Predicting: 6it [00:03,  1.61it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.73it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.63it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:09,  1.58it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:11,  1.52it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:13,  1.52it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.49it/s]Extractor Predicting: 25it [00:15,  1.53it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:16,  1.56it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:20,  1.57it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:21,  1.50it/s]Extractor Predicting: 35it [00:22,  1.54it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.51it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:24,  1.50it/s]Extractor Predicting: 40it [00:25,  1.52it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:29,  1.55it/s]Extractor Predicting: 47it [00:29,  1.53it/s]Extractor Predicting: 48it [00:30,  1.55it/s]Extractor Predicting: 49it [00:31,  1.54it/s]Extractor Predicting: 50it [00:31,  1.58it/s]Extractor Predicting: 51it [00:32,  1.60it/s]Extractor Predicting: 52it [00:33,  1.58it/s]Extractor Predicting: 53it [00:33,  1.60it/s]Extractor Predicting: 54it [00:34,  1.54it/s]Extractor Predicting: 55it [00:35,  1.57it/s]Extractor Predicting: 56it [00:35,  1.55it/s]Extractor Predicting: 57it [00:36,  1.54it/s]Extractor Predicting: 58it [00:36,  1.54it/s]Extractor Predicting: 59it [00:37,  1.54it/s]Extractor Predicting: 60it [00:38,  1.54it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.57it/s]Extractor Predicting: 63it [00:40,  1.58it/s]Extractor Predicting: 64it [00:40,  1.56it/s]Extractor Predicting: 65it [00:41,  1.57it/s]Extractor Predicting: 66it [00:42,  1.56it/s]Extractor Predicting: 67it [00:42,  1.57it/s]Extractor Predicting: 68it [00:43,  1.54it/s]Extractor Predicting: 69it [00:44,  1.54it/s]Extractor Predicting: 70it [00:44,  1.57it/s]Extractor Predicting: 71it [00:45,  1.57it/s]Extractor Predicting: 72it [00:45,  1.56it/s]Extractor Predicting: 73it [00:46,  1.54it/s]Extractor Predicting: 74it [00:47,  1.52it/s]Extractor Predicting: 75it [00:47,  1.56it/s]Extractor Predicting: 76it [00:48,  1.51it/s]Extractor Predicting: 77it [00:49,  1.49it/s]Extractor Predicting: 78it [00:49,  1.49it/s]Extractor Predicting: 79it [00:50,  1.50it/s]Extractor Predicting: 80it [00:51,  1.49it/s]Extractor Predicting: 81it [00:52,  1.33it/s]Extractor Predicting: 82it [00:52,  1.39it/s]Extractor Predicting: 83it [00:53,  1.47it/s]Extractor Predicting: 84it [00:54,  1.49it/s]Extractor Predicting: 85it [00:54,  1.50it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:56,  1.51it/s]Extractor Predicting: 88it [00:56,  1.55it/s]Extractor Predicting: 89it [00:57,  1.53it/s]Extractor Predicting: 90it [00:58,  1.52it/s]Extractor Predicting: 91it [00:58,  1.53it/s]Extractor Predicting: 92it [00:59,  1.56it/s]Extractor Predicting: 93it [00:59,  1.52it/s]Extractor Predicting: 94it [01:00,  1.51it/s]Extractor Predicting: 95it [01:01,  1.48it/s]Extractor Predicting: 96it [01:02,  1.50it/s]Extractor Predicting: 97it [01:02,  1.53it/s]Extractor Predicting: 98it [01:03,  1.55it/s]Extractor Predicting: 99it [01:03,  1.53it/s]Extractor Predicting: 100it [01:04,  1.55it/s]Extractor Predicting: 101it [01:05,  1.49it/s]Extractor Predicting: 102it [01:05,  1.48it/s]Extractor Predicting: 103it [01:06,  1.49it/s]Extractor Predicting: 104it [01:07,  1.52it/s]Extractor Predicting: 105it [01:07,  1.51it/s]Extractor Predicting: 106it [01:08,  1.51it/s]Extractor Predicting: 107it [01:09,  1.53it/s]Extractor Predicting: 108it [01:09,  1.50it/s]Extractor Predicting: 109it [01:10,  1.51it/s]Extractor Predicting: 110it [01:11,  1.51it/s]Extractor Predicting: 111it [01:11,  1.50it/s]Extractor Predicting: 112it [01:12,  1.49it/s]Extractor Predicting: 113it [01:13,  1.52it/s]Extractor Predicting: 114it [01:13,  1.51it/s]Extractor Predicting: 115it [01:14,  1.54it/s]Extractor Predicting: 116it [01:15,  1.52it/s]Extractor Predicting: 117it [01:15,  1.55it/s]Extractor Predicting: 118it [01:16,  1.53it/s]Extractor Predicting: 119it [01:17,  1.50it/s]Extractor Predicting: 120it [01:17,  1.54it/s]Extractor Predicting: 121it [01:18,  1.52it/s]Extractor Predicting: 122it [01:19,  1.51it/s]Extractor Predicting: 123it [01:19,  1.49it/s]Extractor Predicting: 124it [01:20,  1.49it/s]Extractor Predicting: 125it [01:21,  1.48it/s]Extractor Predicting: 126it [01:21,  1.49it/s]Extractor Predicting: 127it [01:22,  1.48it/s]Extractor Predicting: 128it [01:23,  1.47it/s]Extractor Predicting: 129it [01:23,  1.47it/s]Extractor Predicting: 130it [01:24,  1.49it/s]Extractor Predicting: 131it [01:25,  1.47it/s]Extractor Predicting: 132it [01:25,  1.49it/s]Extractor Predicting: 133it [01:26,  1.49it/s]Extractor Predicting: 134it [01:27,  1.51it/s]Extractor Predicting: 135it [01:27,  1.52it/s]Extractor Predicting: 136it [01:28,  1.52it/s]Extractor Predicting: 137it [01:29,  1.50it/s]Extractor Predicting: 138it [01:29,  1.48it/s]Extractor Predicting: 139it [01:30,  1.50it/s]Extractor Predicting: 140it [01:31,  1.51it/s]Extractor Predicting: 141it [01:31,  1.53it/s]Extractor Predicting: 142it [01:32,  1.51it/s]Extractor Predicting: 143it [01:33,  1.52it/s]Extractor Predicting: 144it [01:33,  1.50it/s]Extractor Predicting: 145it [01:34,  1.51it/s]Extractor Predicting: 146it [01:35,  1.49it/s]Extractor Predicting: 147it [01:35,  1.49it/s]Extractor Predicting: 148it [01:36,  1.49it/s]Extractor Predicting: 149it [01:37,  1.49it/s]Extractor Predicting: 150it [01:37,  1.51it/s]Extractor Predicting: 151it [01:38,  1.51it/s]Extractor Predicting: 152it [01:39,  1.47it/s]Extractor Predicting: 153it [01:39,  1.46it/s]Extractor Predicting: 154it [01:40,  1.48it/s]Extractor Predicting: 154it [01:40,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:31,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:31,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:31,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:31,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:31,128 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 21:44:32,038 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 21:44:32,039 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 21:44:32,658 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 21:44:33,778 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 21:44:33,778 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:36,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:36,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:36,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:36,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:44:36,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 21:44:37,620 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 21:44:37,621 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 21:44:38,252 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 21:44:38,461 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 21:44:38,461 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.344106463878327,
  "recall": 0.04345738295318127,
  "score": 0.07716904711149009,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27443
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27543, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.62it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.55it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:13,  1.58it/s]Extractor Predicting: 22it [00:13,  1.56it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.59it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:18,  1.59it/s]Extractor Predicting: 30it [00:18,  1.60it/s]Extractor Predicting: 31it [00:19,  1.60it/s]Extractor Predicting: 32it [00:20,  1.58it/s]Extractor Predicting: 33it [00:20,  1.58it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:22,  1.60it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:23,  1.57it/s]Extractor Predicting: 38it [00:24,  1.56it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.60it/s]Extractor Predicting: 41it [00:25,  1.57it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:27,  1.50it/s]Extractor Predicting: 44it [00:27,  1.50it/s]Extractor Predicting: 45it [00:28,  1.51it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.34it/s]Extractor Predicting: 48it [00:30,  1.36it/s]Extractor Predicting: 49it [00:31,  1.42it/s]Extractor Predicting: 50it [00:32,  1.43it/s]Extractor Predicting: 51it [00:32,  1.45it/s]Extractor Predicting: 52it [00:33,  1.45it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:34,  1.46it/s]Extractor Predicting: 55it [00:35,  1.43it/s]Extractor Predicting: 56it [00:36,  1.47it/s]Extractor Predicting: 57it [00:36,  1.50it/s]Extractor Predicting: 58it [00:37,  1.50it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:39,  1.46it/s]Extractor Predicting: 61it [00:39,  1.46it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:41,  1.44it/s]Extractor Predicting: 65it [00:42,  1.45it/s]Extractor Predicting: 66it [00:43,  1.46it/s]Extractor Predicting: 67it [00:43,  1.42it/s]Extractor Predicting: 68it [00:44,  1.44it/s]Extractor Predicting: 69it [00:45,  1.43it/s]Extractor Predicting: 70it [00:45,  1.44it/s]Extractor Predicting: 71it [00:46,  1.45it/s]Extractor Predicting: 72it [00:47,  1.44it/s]Extractor Predicting: 73it [00:48,  1.42it/s]Extractor Predicting: 74it [00:48,  1.42it/s]Extractor Predicting: 75it [00:49,  1.42it/s]Extractor Predicting: 76it [00:50,  1.41it/s]Extractor Predicting: 77it [00:50,  1.45it/s]Extractor Predicting: 78it [00:51,  1.43it/s]Extractor Predicting: 79it [00:52,  1.44it/s]Extractor Predicting: 80it [00:52,  1.44it/s]Extractor Predicting: 81it [00:53,  1.40it/s]Extractor Predicting: 82it [00:54,  1.41it/s]Extractor Predicting: 83it [00:55,  1.45it/s]Extractor Predicting: 84it [00:55,  1.49it/s]Extractor Predicting: 85it [00:56,  1.45it/s]Extractor Predicting: 86it [00:57,  1.44it/s]Extractor Predicting: 87it [00:57,  1.46it/s]Extractor Predicting: 88it [00:58,  1.44it/s]Extractor Predicting: 89it [00:59,  1.43it/s]Extractor Predicting: 90it [00:59,  1.42it/s]Extractor Predicting: 91it [01:00,  1.44it/s]Extractor Predicting: 92it [01:01,  1.46it/s]Extractor Predicting: 93it [01:01,  1.50it/s]Extractor Predicting: 94it [01:02,  1.50it/s]Extractor Predicting: 95it [01:03,  1.53it/s]Extractor Predicting: 96it [01:03,  1.51it/s]Extractor Predicting: 97it [01:04,  1.40it/s]Extractor Predicting: 98it [01:05,  1.42it/s]Extractor Predicting: 99it [01:06,  1.46it/s]Extractor Predicting: 100it [01:06,  1.45it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:08,  1.46it/s]Extractor Predicting: 103it [01:08,  1.48it/s]Extractor Predicting: 104it [01:09,  1.49it/s]Extractor Predicting: 105it [01:10,  1.49it/s]Extractor Predicting: 106it [01:10,  1.48it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:12,  1.50it/s]Extractor Predicting: 109it [01:12,  1.48it/s]Extractor Predicting: 110it [01:13,  1.40it/s]Extractor Predicting: 111it [01:14,  1.44it/s]Extractor Predicting: 112it [01:14,  1.44it/s]Extractor Predicting: 113it [01:15,  1.49it/s]Extractor Predicting: 114it [01:16,  1.51it/s]Extractor Predicting: 115it [01:16,  1.48it/s]Extractor Predicting: 116it [01:17,  1.49it/s]Extractor Predicting: 117it [01:18,  1.51it/s]Extractor Predicting: 118it [01:18,  1.42it/s]Extractor Predicting: 119it [01:19,  1.45it/s]Extractor Predicting: 120it [01:20,  1.47it/s]Extractor Predicting: 121it [01:20,  1.47it/s]Extractor Predicting: 122it [01:21,  1.47it/s]Extractor Predicting: 123it [01:22,  1.48it/s]Extractor Predicting: 124it [01:22,  1.51it/s]Extractor Predicting: 125it [01:23,  1.51it/s]Extractor Predicting: 126it [01:24,  1.53it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:25,  1.54it/s]Extractor Predicting: 129it [01:26,  1.57it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:27,  1.56it/s]Extractor Predicting: 132it [01:28,  1.56it/s]Extractor Predicting: 133it [01:28,  1.56it/s]Extractor Predicting: 134it [01:29,  1.54it/s]Extractor Predicting: 135it [01:30,  1.51it/s]Extractor Predicting: 136it [01:30,  1.51it/s]Extractor Predicting: 137it [01:31,  1.49it/s]Extractor Predicting: 138it [01:32,  1.50it/s]Extractor Predicting: 139it [01:32,  1.51it/s]Extractor Predicting: 140it [01:33,  1.49it/s]Extractor Predicting: 141it [01:34,  1.55it/s]Extractor Predicting: 142it [01:34,  1.48it/s]Extractor Predicting: 143it [01:35,  1.44it/s]Extractor Predicting: 144it [01:36,  1.47it/s]Extractor Predicting: 145it [01:36,  1.49it/s]Extractor Predicting: 146it [01:37,  1.44it/s]Extractor Predicting: 147it [01:38,  1.45it/s]Extractor Predicting: 148it [01:38,  1.47it/s]Extractor Predicting: 149it [01:39,  1.57it/s]Extractor Predicting: 150it [01:39,  1.64it/s]Extractor Predicting: 151it [01:40,  1.62it/s]Extractor Predicting: 152it [01:41,  1.47it/s]Extractor Predicting: 153it [01:41,  1.58it/s]Extractor Predicting: 154it [01:42,  1.65it/s]Extractor Predicting: 155it [01:43,  1.69it/s]Extractor Predicting: 156it [01:43,  1.68it/s]Extractor Predicting: 157it [01:44,  1.71it/s]Extractor Predicting: 158it [01:44,  1.75it/s]Extractor Predicting: 159it [01:45,  1.79it/s]Extractor Predicting: 160it [01:45,  1.77it/s]Extractor Predicting: 161it [01:46,  1.74it/s]Extractor Predicting: 162it [01:47,  1.76it/s]Extractor Predicting: 163it [01:47,  1.70it/s]Extractor Predicting: 164it [01:48,  1.70it/s]Extractor Predicting: 165it [01:48,  1.74it/s]Extractor Predicting: 166it [01:49,  1.76it/s]Extractor Predicting: 167it [01:49,  1.74it/s]Extractor Predicting: 168it [01:50,  1.69it/s]Extractor Predicting: 169it [01:51,  1.64it/s]Extractor Predicting: 170it [01:51,  1.56it/s]Extractor Predicting: 171it [01:52,  1.54it/s]Extractor Predicting: 172it [01:53,  1.51it/s]Extractor Predicting: 173it [01:54,  1.48it/s]Extractor Predicting: 174it [01:54,  1.48it/s]Extractor Predicting: 175it [01:55,  1.53it/s]Extractor Predicting: 176it [01:55,  1.51it/s]Extractor Predicting: 177it [01:56,  1.53it/s]Extractor Predicting: 178it [01:57,  1.52it/s]Extractor Predicting: 179it [01:57,  1.56it/s]Extractor Predicting: 180it [01:58,  1.58it/s]Extractor Predicting: 181it [01:59,  1.57it/s]Extractor Predicting: 182it [01:59,  1.56it/s]Extractor Predicting: 183it [02:00,  1.53it/s]Extractor Predicting: 184it [02:01,  1.50it/s]Extractor Predicting: 185it [02:01,  1.53it/s]Extractor Predicting: 186it [02:02,  1.50it/s]Extractor Predicting: 187it [02:03,  1.50it/s]Extractor Predicting: 188it [02:03,  1.56it/s]Extractor Predicting: 189it [02:04,  1.54it/s]Extractor Predicting: 190it [02:05,  1.54it/s]Extractor Predicting: 191it [02:05,  1.54it/s]Extractor Predicting: 192it [02:06,  1.55it/s]Extractor Predicting: 193it [02:06,  1.54it/s]Extractor Predicting: 194it [02:07,  1.59it/s]Extractor Predicting: 195it [02:08,  1.56it/s]Extractor Predicting: 196it [02:08,  1.53it/s]Extractor Predicting: 197it [02:09,  1.55it/s]Extractor Predicting: 198it [02:10,  1.54it/s]Extractor Predicting: 199it [02:10,  1.53it/s]Extractor Predicting: 200it [02:11,  1.52it/s]Extractor Predicting: 201it [02:12,  1.49it/s]Extractor Predicting: 202it [02:12,  1.47it/s]Extractor Predicting: 203it [02:13,  1.45it/s]Extractor Predicting: 204it [02:14,  1.47it/s]Extractor Predicting: 205it [02:14,  1.47it/s]Extractor Predicting: 206it [02:15,  1.46it/s]Extractor Predicting: 207it [02:16,  1.48it/s]Extractor Predicting: 208it [02:17,  1.49it/s]Extractor Predicting: 209it [02:17,  1.51it/s]Extractor Predicting: 210it [02:18,  1.51it/s]Extractor Predicting: 211it [02:18,  1.51it/s]Extractor Predicting: 212it [02:19,  1.56it/s]Extractor Predicting: 213it [02:20,  1.53it/s]Extractor Predicting: 214it [02:20,  1.54it/s]Extractor Predicting: 215it [02:21,  1.56it/s]Extractor Predicting: 216it [02:22,  1.57it/s]Extractor Predicting: 217it [02:22,  1.55it/s]Extractor Predicting: 218it [02:23,  1.56it/s]Extractor Predicting: 219it [02:24,  1.48it/s]Extractor Predicting: 220it [02:24,  1.54it/s]Extractor Predicting: 221it [02:25,  1.52it/s]Extractor Predicting: 222it [02:26,  1.56it/s]Extractor Predicting: 223it [02:26,  1.59it/s]Extractor Predicting: 224it [02:27,  1.58it/s]Extractor Predicting: 225it [02:27,  1.61it/s]Extractor Predicting: 226it [02:28,  1.63it/s]Extractor Predicting: 227it [02:29,  1.65it/s]Extractor Predicting: 228it [02:29,  1.59it/s]Extractor Predicting: 229it [02:30,  1.59it/s]Extractor Predicting: 230it [02:31,  1.56it/s]Extractor Predicting: 231it [02:31,  1.56it/s]Extractor Predicting: 232it [02:32,  1.61it/s]Extractor Predicting: 233it [02:32,  1.58it/s]Extractor Predicting: 234it [02:33,  1.55it/s]Extractor Predicting: 235it [02:34,  1.57it/s]Extractor Predicting: 236it [02:34,  1.56it/s]Extractor Predicting: 237it [02:35,  1.65it/s]Extractor Predicting: 238it [02:36,  1.60it/s]Extractor Predicting: 239it [02:36,  1.60it/s]Extractor Predicting: 240it [02:37,  1.56it/s]Extractor Predicting: 241it [02:38,  1.55it/s]Extractor Predicting: 242it [02:38,  1.55it/s]Extractor Predicting: 243it [02:39,  1.57it/s]Extractor Predicting: 244it [02:39,  1.56it/s]Extractor Predicting: 245it [02:40,  1.54it/s]Extractor Predicting: 246it [02:41,  1.53it/s]Extractor Predicting: 247it [02:41,  1.51it/s]Extractor Predicting: 248it [02:42,  1.47it/s]Extractor Predicting: 249it [02:43,  1.49it/s]Extractor Predicting: 250it [02:43,  1.51it/s]Extractor Predicting: 251it [02:44,  1.54it/s]Extractor Predicting: 252it [02:45,  1.56it/s]Extractor Predicting: 253it [02:45,  1.52it/s]Extractor Predicting: 254it [02:46,  1.54it/s]Extractor Predicting: 255it [02:47,  1.54it/s]Extractor Predicting: 256it [02:47,  1.54it/s]Extractor Predicting: 257it [02:48,  1.54it/s]Extractor Predicting: 258it [02:49,  1.50it/s]Extractor Predicting: 259it [02:49,  1.52it/s]Extractor Predicting: 260it [02:50,  1.53it/s]Extractor Predicting: 261it [02:51,  1.53it/s]Extractor Predicting: 262it [02:51,  1.54it/s]Extractor Predicting: 263it [02:52,  1.53it/s]Extractor Predicting: 264it [02:53,  1.54it/s]Extractor Predicting: 265it [02:53,  1.52it/s]Extractor Predicting: 266it [02:54,  1.55it/s]Extractor Predicting: 267it [02:55,  1.55it/s]Extractor Predicting: 268it [02:55,  1.36it/s]Extractor Predicting: 269it [02:56,  1.41it/s]Extractor Predicting: 270it [02:57,  1.43it/s]Extractor Predicting: 271it [02:57,  1.47it/s]Extractor Predicting: 272it [02:58,  1.46it/s]Extractor Predicting: 273it [02:59,  1.45it/s]Extractor Predicting: 274it [02:59,  1.50it/s]Extractor Predicting: 275it [03:00,  1.51it/s]Extractor Predicting: 276it [03:01,  1.53it/s]Extractor Predicting: 277it [03:01,  1.51it/s]Extractor Predicting: 278it [03:02,  1.51it/s]Extractor Predicting: 279it [03:03,  1.50it/s]Extractor Predicting: 280it [03:03,  1.50it/s]Extractor Predicting: 281it [03:04,  1.44it/s]Extractor Predicting: 282it [03:05,  1.47it/s]Extractor Predicting: 283it [03:06,  1.45it/s]Extractor Predicting: 284it [03:06,  1.46it/s]Extractor Predicting: 285it [03:07,  1.40it/s]Extractor Predicting: 286it [03:08,  1.36it/s]Extractor Predicting: 287it [03:08,  1.39it/s]Extractor Predicting: 288it [03:09,  1.43it/s]Extractor Predicting: 289it [03:10,  1.43it/s]Extractor Predicting: 290it [03:11,  1.40it/s]Extractor Predicting: 291it [03:11,  1.42it/s]Extractor Predicting: 292it [03:12,  1.45it/s]Extractor Predicting: 293it [03:13,  1.46it/s]Extractor Predicting: 294it [03:13,  1.48it/s]Extractor Predicting: 295it [03:14,  1.47it/s]Extractor Predicting: 296it [03:15,  1.50it/s]Extractor Predicting: 297it [03:15,  1.53it/s]Extractor Predicting: 298it [03:16,  1.60it/s]Extractor Predicting: 299it [03:16,  1.59it/s]Extractor Predicting: 300it [03:17,  1.59it/s]Extractor Predicting: 301it [03:18,  1.59it/s]Extractor Predicting: 302it [03:18,  1.55it/s]Extractor Predicting: 303it [03:19,  1.52it/s]Extractor Predicting: 304it [03:20,  1.54it/s]Extractor Predicting: 305it [03:20,  1.54it/s]Extractor Predicting: 306it [03:21,  1.53it/s]Extractor Predicting: 307it [03:22,  1.55it/s]Extractor Predicting: 308it [03:22,  1.54it/s]Extractor Predicting: 309it [03:23,  1.53it/s]Extractor Predicting: 310it [03:24,  1.53it/s]Extractor Predicting: 311it [03:24,  1.51it/s]Extractor Predicting: 312it [03:25,  1.52it/s]Extractor Predicting: 313it [03:26,  1.50it/s]Extractor Predicting: 314it [03:26,  1.51it/s]Extractor Predicting: 315it [03:27,  1.55it/s]Extractor Predicting: 316it [03:28,  1.51it/s]Extractor Predicting: 317it [03:28,  1.50it/s]Extractor Predicting: 318it [03:29,  1.53it/s]Extractor Predicting: 319it [03:30,  1.49it/s]Extractor Predicting: 320it [03:30,  1.50it/s]Extractor Predicting: 321it [03:31,  1.50it/s]Extractor Predicting: 322it [03:31,  1.52it/s]Extractor Predicting: 323it [03:32,  1.53it/s]Extractor Predicting: 324it [03:33,  1.54it/s]Extractor Predicting: 325it [03:33,  1.57it/s]Extractor Predicting: 326it [03:34,  1.54it/s]Extractor Predicting: 327it [03:35,  1.57it/s]Extractor Predicting: 328it [03:35,  1.54it/s]Extractor Predicting: 329it [03:36,  1.55it/s]Extractor Predicting: 330it [03:37,  1.58it/s]Extractor Predicting: 331it [03:37,  1.54it/s]Extractor Predicting: 332it [03:38,  1.52it/s]Extractor Predicting: 333it [03:39,  1.51it/s]Extractor Predicting: 334it [03:39,  1.48it/s]Extractor Predicting: 335it [03:40,  1.50it/s]Extractor Predicting: 336it [03:41,  1.51it/s]Extractor Predicting: 337it [03:41,  1.50it/s]Extractor Predicting: 338it [03:42,  1.50it/s]Extractor Predicting: 339it [03:43,  1.52it/s]Extractor Predicting: 340it [03:43,  1.57it/s]Extractor Predicting: 341it [03:44,  1.53it/s]Extractor Predicting: 342it [03:44,  1.56it/s]Extractor Predicting: 343it [03:45,  1.58it/s]Extractor Predicting: 344it [03:46,  1.62it/s]Extractor Predicting: 345it [03:46,  1.64it/s]Extractor Predicting: 346it [03:47,  1.65it/s]Extractor Predicting: 347it [03:47,  1.68it/s]Extractor Predicting: 348it [03:48,  1.67it/s]Extractor Predicting: 349it [03:49,  1.68it/s]Extractor Predicting: 350it [03:49,  1.63it/s]Extractor Predicting: 351it [03:50,  1.64it/s]Extractor Predicting: 352it [03:51,  1.62it/s]Extractor Predicting: 353it [03:51,  1.62it/s]Extractor Predicting: 354it [03:52,  1.54it/s]Extractor Predicting: 355it [03:52,  1.58it/s]Extractor Predicting: 356it [03:53,  1.56it/s]Extractor Predicting: 357it [03:54,  1.55it/s]Extractor Predicting: 358it [03:54,  1.53it/s]Extractor Predicting: 359it [03:55,  1.49it/s]Extractor Predicting: 360it [03:56,  1.50it/s]Extractor Predicting: 361it [03:56,  1.50it/s]Extractor Predicting: 362it [03:57,  1.51it/s]Extractor Predicting: 363it [03:58,  1.53it/s]Extractor Predicting: 364it [03:58,  1.53it/s]Extractor Predicting: 365it [03:59,  1.51it/s]Extractor Predicting: 366it [04:00,  1.50it/s]Extractor Predicting: 367it [04:00,  1.50it/s]Extractor Predicting: 368it [04:01,  1.51it/s]Extractor Predicting: 369it [04:02,  1.48it/s]Extractor Predicting: 370it [04:02,  1.51it/s]Extractor Predicting: 371it [04:03,  1.50it/s]Extractor Predicting: 372it [04:04,  1.49it/s]Extractor Predicting: 373it [04:04,  1.49it/s]Extractor Predicting: 374it [04:05,  1.51it/s]Extractor Predicting: 375it [04:06,  1.54it/s]Extractor Predicting: 376it [04:06,  1.57it/s]Extractor Predicting: 377it [04:07,  1.63it/s]Extractor Predicting: 378it [04:08,  1.62it/s]Extractor Predicting: 379it [04:08,  1.61it/s]Extractor Predicting: 380it [04:09,  1.61it/s]Extractor Predicting: 381it [04:09,  1.62it/s]Extractor Predicting: 382it [04:10,  1.62it/s]Extractor Predicting: 383it [04:11,  1.61it/s]Extractor Predicting: 384it [04:11,  1.57it/s]Extractor Predicting: 385it [04:12,  1.61it/s]Extractor Predicting: 386it [04:12,  1.64it/s]Extractor Predicting: 387it [04:13,  1.64it/s]Extractor Predicting: 388it [04:14,  1.64it/s]Extractor Predicting: 389it [04:14,  1.62it/s]Extractor Predicting: 390it [04:15,  1.63it/s]Extractor Predicting: 391it [04:16,  1.63it/s]Extractor Predicting: 392it [04:16,  1.68it/s]Extractor Predicting: 393it [04:17,  1.66it/s]Extractor Predicting: 394it [04:17,  1.66it/s]Extractor Predicting: 395it [04:18,  1.64it/s]Extractor Predicting: 396it [04:19,  1.63it/s]Extractor Predicting: 397it [04:19,  1.61it/s]Extractor Predicting: 398it [04:20,  1.58it/s]Extractor Predicting: 399it [04:21,  1.54it/s]Extractor Predicting: 400it [04:21,  1.53it/s]Extractor Predicting: 401it [04:22,  1.54it/s]Extractor Predicting: 402it [04:22,  1.55it/s]Extractor Predicting: 403it [04:23,  1.59it/s]Extractor Predicting: 404it [04:24,  1.53it/s]Extractor Predicting: 405it [04:24,  1.53it/s]Extractor Predicting: 406it [04:25,  1.53it/s]Extractor Predicting: 407it [04:26,  1.52it/s]Extractor Predicting: 408it [04:26,  1.53it/s]Extractor Predicting: 409it [04:27,  1.50it/s]Extractor Predicting: 410it [04:28,  1.34it/s]Extractor Predicting: 411it [04:29,  1.40it/s]Extractor Predicting: 412it [04:29,  1.44it/s]Extractor Predicting: 413it [04:30,  1.45it/s]Extractor Predicting: 414it [04:31,  1.46it/s]Extractor Predicting: 415it [04:31,  1.49it/s]Extractor Predicting: 416it [04:32,  1.50it/s]Extractor Predicting: 417it [04:33,  1.48it/s]Extractor Predicting: 418it [04:33,  1.52it/s]Extractor Predicting: 419it [04:34,  1.53it/s]Extractor Predicting: 420it [04:35,  1.52it/s]Extractor Predicting: 421it [04:35,  1.53it/s]Extractor Predicting: 422it [04:36,  1.49it/s]Extractor Predicting: 423it [04:37,  1.49it/s]Extractor Predicting: 424it [04:37,  1.52it/s]Extractor Predicting: 425it [04:38,  1.51it/s]Extractor Predicting: 426it [04:39,  1.57it/s]Extractor Predicting: 427it [04:39,  1.59it/s]Extractor Predicting: 428it [04:40,  1.60it/s]Extractor Predicting: 429it [04:40,  1.63it/s]Extractor Predicting: 430it [04:41,  1.64it/s]Extractor Predicting: 431it [04:41,  1.69it/s]Extractor Predicting: 432it [04:42,  1.65it/s]Extractor Predicting: 433it [04:43,  1.61it/s]Extractor Predicting: 434it [04:43,  1.63it/s]Extractor Predicting: 435it [04:44,  1.61it/s]Extractor Predicting: 436it [04:45,  1.60it/s]Extractor Predicting: 437it [04:45,  1.64it/s]Extractor Predicting: 438it [04:46,  1.64it/s]Extractor Predicting: 439it [04:46,  1.64it/s]Extractor Predicting: 440it [04:47,  1.60it/s]Extractor Predicting: 441it [04:48,  1.61it/s]Extractor Predicting: 442it [04:48,  1.64it/s]Extractor Predicting: 443it [04:49,  1.59it/s]Extractor Predicting: 444it [04:50,  1.62it/s]Extractor Predicting: 445it [04:50,  1.63it/s]Extractor Predicting: 446it [04:51,  1.65it/s]Extractor Predicting: 447it [04:51,  1.67it/s]Extractor Predicting: 448it [04:52,  1.66it/s]Extractor Predicting: 449it [04:53,  1.67it/s]Extractor Predicting: 450it [04:53,  1.65it/s]Extractor Predicting: 451it [04:54,  1.65it/s]Extractor Predicting: 452it [04:54,  1.61it/s]Extractor Predicting: 453it [04:55,  1.60it/s]Extractor Predicting: 454it [04:56,  1.54it/s]Extractor Predicting: 455it [04:56,  1.58it/s]Extractor Predicting: 456it [04:57,  1.62it/s]Extractor Predicting: 457it [04:58,  1.63it/s]Extractor Predicting: 458it [04:58,  1.65it/s]Extractor Predicting: 459it [04:59,  1.62it/s]Extractor Predicting: 460it [04:59,  1.55it/s]Extractor Predicting: 461it [05:00,  1.54it/s]Extractor Predicting: 462it [05:01,  1.49it/s]Extractor Predicting: 463it [05:02,  1.49it/s]Extractor Predicting: 464it [05:02,  1.53it/s]Extractor Predicting: 465it [05:03,  1.52it/s]Extractor Predicting: 466it [05:03,  1.53it/s]Extractor Predicting: 467it [05:04,  1.49it/s]Extractor Predicting: 468it [05:05,  1.50it/s]Extractor Predicting: 469it [05:05,  1.52it/s]Extractor Predicting: 470it [05:06,  1.48it/s]Extractor Predicting: 471it [05:07,  1.45it/s]Extractor Predicting: 472it [05:08,  1.46it/s]Extractor Predicting: 473it [05:08,  1.49it/s]Extractor Predicting: 474it [05:09,  1.54it/s]Extractor Predicting: 475it [05:09,  1.52it/s]Extractor Predicting: 476it [05:10,  1.52it/s]Extractor Predicting: 477it [05:11,  1.48it/s]Extractor Predicting: 478it [05:12,  1.46it/s]Extractor Predicting: 479it [05:12,  1.46it/s]Extractor Predicting: 480it [05:13,  1.43it/s]Extractor Predicting: 481it [05:14,  1.46it/s]Extractor Predicting: 482it [05:14,  1.46it/s]Extractor Predicting: 483it [05:15,  1.49it/s]Extractor Predicting: 484it [05:16,  1.49it/s]Extractor Predicting: 485it [05:16,  1.49it/s]Extractor Predicting: 486it [05:17,  1.53it/s]Extractor Predicting: 487it [05:18,  1.52it/s]Extractor Predicting: 488it [05:18,  1.50it/s]Extractor Predicting: 489it [05:19,  1.51it/s]Extractor Predicting: 490it [05:20,  1.48it/s]Extractor Predicting: 491it [05:20,  1.48it/s]Extractor Predicting: 492it [05:21,  1.47it/s]Extractor Predicting: 493it [05:22,  1.48it/s]Extractor Predicting: 494it [05:22,  1.50it/s]Extractor Predicting: 495it [05:23,  1.52it/s]Extractor Predicting: 496it [05:24,  1.53it/s]Extractor Predicting: 496it [05:24,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:18,949 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:18,990 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:18,991 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:18,991 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:18,991 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 21:50:19,851 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 21:50:19,852 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 21:50:20,592 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 21:50:21,688 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 21:50:21,688 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:24,981 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:25,027 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:25,027 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:25,028 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 21:50:25,028 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 21:50:25,926 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 21:50:25,927 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 21:50:26,589 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 21:50:26,837 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 21:50:26,837 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.2424812030075188,
  "recall": 0.054183467741935484,
  "score": 0.08857456742653116,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 10529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.32it/s]Extractor Predicting: 2it [00:01,  1.39it/s]Extractor Predicting: 3it [00:02,  1.45it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.44it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:06,  1.46it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.45it/s]Extractor Predicting: 13it [00:08,  1.46it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.44it/s]Extractor Predicting: 16it [00:11,  1.46it/s]Extractor Predicting: 17it [00:11,  1.41it/s]Extractor Predicting: 18it [00:12,  1.40it/s]Extractor Predicting: 19it [00:13,  1.37it/s]Extractor Predicting: 20it [00:14,  1.31it/s]Extractor Predicting: 21it [00:14,  1.32it/s]Extractor Predicting: 22it [00:15,  1.29it/s]Extractor Predicting: 23it [00:16,  1.30it/s]Extractor Predicting: 24it [00:17,  1.34it/s]Extractor Predicting: 25it [00:17,  1.38it/s]Extractor Predicting: 26it [00:18,  1.39it/s]Extractor Predicting: 27it [00:19,  1.38it/s]Extractor Predicting: 28it [00:19,  1.39it/s]Extractor Predicting: 29it [00:20,  1.37it/s]Extractor Predicting: 30it [00:21,  1.38it/s]Extractor Predicting: 31it [00:22,  1.39it/s]Extractor Predicting: 32it [00:22,  1.40it/s]Extractor Predicting: 33it [00:23,  1.44it/s]Extractor Predicting: 34it [00:24,  1.47it/s]Extractor Predicting: 35it [00:24,  1.43it/s]Extractor Predicting: 36it [00:25,  1.44it/s]Extractor Predicting: 37it [00:26,  1.45it/s]Extractor Predicting: 38it [00:26,  1.45it/s]Extractor Predicting: 39it [00:27,  1.47it/s]Extractor Predicting: 40it [00:28,  1.47it/s]Extractor Predicting: 41it [00:28,  1.46it/s]Extractor Predicting: 42it [00:29,  1.49it/s]Extractor Predicting: 43it [00:30,  1.49it/s]Extractor Predicting: 44it [00:30,  1.50it/s]Extractor Predicting: 45it [00:31,  1.48it/s]Extractor Predicting: 46it [00:32,  1.44it/s]Extractor Predicting: 47it [00:33,  1.45it/s]Extractor Predicting: 48it [00:33,  1.48it/s]Extractor Predicting: 49it [00:34,  1.50it/s]Extractor Predicting: 50it [00:35,  1.47it/s]Extractor Predicting: 51it [00:35,  1.47it/s]Extractor Predicting: 52it [00:36,  1.48it/s]Extractor Predicting: 53it [00:37,  1.49it/s]Extractor Predicting: 54it [00:37,  1.48it/s]Extractor Predicting: 55it [00:38,  1.49it/s]Extractor Predicting: 56it [00:39,  1.51it/s]Extractor Predicting: 57it [00:39,  1.54it/s]Extractor Predicting: 58it [00:40,  1.55it/s]Extractor Predicting: 59it [00:40,  1.53it/s]Extractor Predicting: 60it [00:41,  1.46it/s]Extractor Predicting: 61it [00:42,  1.48it/s]Extractor Predicting: 62it [00:43,  1.51it/s]Extractor Predicting: 63it [00:43,  1.46it/s]Extractor Predicting: 64it [00:44,  1.48it/s]Extractor Predicting: 65it [00:45,  1.45it/s]Extractor Predicting: 66it [00:45,  1.51it/s]Extractor Predicting: 67it [00:46,  1.43it/s]Extractor Predicting: 68it [00:47,  1.53it/s]Extractor Predicting: 69it [00:47,  1.63it/s]Extractor Predicting: 70it [00:48,  1.72it/s]Extractor Predicting: 71it [00:48,  1.78it/s]Extractor Predicting: 72it [00:49,  1.77it/s]Extractor Predicting: 73it [00:49,  1.81it/s]Extractor Predicting: 74it [00:50,  1.81it/s]Extractor Predicting: 75it [00:50,  1.80it/s]Extractor Predicting: 76it [00:51,  1.81it/s]Extractor Predicting: 77it [00:51,  1.80it/s]Extractor Predicting: 78it [00:52,  1.80it/s]Extractor Predicting: 79it [00:52,  1.85it/s]Extractor Predicting: 80it [00:53,  1.82it/s]Extractor Predicting: 81it [00:54,  1.82it/s]Extractor Predicting: 82it [00:54,  1.83it/s]Extractor Predicting: 83it [00:55,  1.87it/s]Extractor Predicting: 84it [00:55,  1.87it/s]Extractor Predicting: 85it [00:56,  1.87it/s]Extractor Predicting: 86it [00:56,  1.84it/s]Extractor Predicting: 87it [00:57,  1.90it/s]Extractor Predicting: 88it [00:57,  1.84it/s]Extractor Predicting: 89it [00:58,  1.82it/s]Extractor Predicting: 90it [00:58,  1.80it/s]Extractor Predicting: 91it [00:59,  1.80it/s]Extractor Predicting: 92it [01:00,  1.82it/s]Extractor Predicting: 93it [01:00,  1.85it/s]Extractor Predicting: 94it [01:01,  1.85it/s]Extractor Predicting: 95it [01:01,  1.86it/s]Extractor Predicting: 96it [01:02,  1.71it/s]Extractor Predicting: 97it [01:03,  1.65it/s]Extractor Predicting: 98it [01:03,  1.59it/s]Extractor Predicting: 99it [01:04,  1.53it/s]Extractor Predicting: 100it [01:05,  1.48it/s]Extractor Predicting: 101it [01:05,  1.45it/s]Extractor Predicting: 102it [01:06,  1.45it/s]Extractor Predicting: 103it [01:07,  1.46it/s]Extractor Predicting: 104it [01:07,  1.46it/s]Extractor Predicting: 105it [01:08,  1.44it/s]Extractor Predicting: 106it [01:09,  1.43it/s]Extractor Predicting: 107it [01:10,  1.43it/s]Extractor Predicting: 108it [01:10,  1.41it/s]Extractor Predicting: 109it [01:11,  1.43it/s]Extractor Predicting: 110it [01:12,  1.42it/s]Extractor Predicting: 111it [01:12,  1.41it/s]Extractor Predicting: 112it [01:13,  1.44it/s]Extractor Predicting: 113it [01:14,  1.48it/s]Extractor Predicting: 114it [01:14,  1.53it/s]Extractor Predicting: 115it [01:15,  1.55it/s]Extractor Predicting: 116it [01:16,  1.56it/s]Extractor Predicting: 117it [01:16,  1.57it/s]Extractor Predicting: 118it [01:17,  1.58it/s]Extractor Predicting: 119it [01:17,  1.57it/s]Extractor Predicting: 120it [01:18,  1.62it/s]Extractor Predicting: 121it [01:19,  1.60it/s]Extractor Predicting: 122it [01:19,  1.62it/s]Extractor Predicting: 123it [01:20,  1.61it/s]Extractor Predicting: 124it [01:21,  1.57it/s]Extractor Predicting: 125it [01:21,  1.57it/s]Extractor Predicting: 126it [01:22,  1.56it/s]Extractor Predicting: 127it [01:23,  1.54it/s]Extractor Predicting: 128it [01:23,  1.51it/s]Extractor Predicting: 129it [01:24,  1.51it/s]Extractor Predicting: 130it [01:25,  1.51it/s]Extractor Predicting: 131it [01:25,  1.45it/s]Extractor Predicting: 132it [01:26,  1.43it/s]Extractor Predicting: 133it [01:27,  1.49it/s]Extractor Predicting: 133it [01:27,  1.53it/s]
{
  "path_pred": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6441005802707931,
  "recall": 0.08827037773359842,
  "score": 0.15526285114815244,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/wiki_rl_all_rsFalse_nbrel_synthetic_large/unseen_15_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
