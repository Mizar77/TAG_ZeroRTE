Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_0', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:57<13:25, 57.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [01:15<07:24, 34.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [01:33<05:22, 26.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:49<04:07, 22.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [02:08<03:31, 21.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [02:28<03:07, 20.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:45<02:37, 19.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [03:03<02:14, 19.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [03:20<01:50, 18.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [03:36<01:27, 17.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:52<01:09, 17.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [04:08<00:50, 16.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [04:28<00:35, 17.64s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:50<00:19, 19.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [05:14<00:00, 20.50s/it]Generating: 100%|██████████| 15/15 [05:14<00:00, 20.96s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : main subject . Context : Later in the year ( 1143–46 ) , he married daughter of Louis XIV and Catherine I of Prussia married to Marie Antoinette III , daughter of Emperor Louis XII and Catherine of Rheims . Head Entity : Catherine I , Tail Entity : Emperor Louis XII and Catherine I of Prussia .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7451923076923077, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.81640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.7391826923076923, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 294, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7013888888888888, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amadec for the first time . Head Entity : Amadec , Tail Entity : Amadeca .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.75625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Paul Groening', 'nominated for', '', 'After a stint in the Swedish music industry in 2002 alongside the late Paul Groening , he moved away to New Zealand to continue his education in the country .')"}}
['Relation : operating system . Context : The CVRN ( Computer Vision and Imaging Systems ) is a digital camera developed at the National Institutes of Health . Head Entity : CVRN , Tail Entity : CVR , Head Entity : National Institutes of Health .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 258, 'raw': 384}
{'target': 600, 'success': 280, 'raw': 416}
{'target': 600, 'success': 303, 'raw': 448}
{'target': 600, 'success': 326, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 372, 'raw': 544}
{'target': 600, 'success': 394, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 527, 'raw': 768}
{'target': 600, 'success': 551, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 598, 'raw': 864}
{'target': 600, 'success': 620, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6919642857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 107, 'raw': 192}
{'target': 600, 'success': 127, 'raw': 224}
{'target': 600, 'success': 148, 'raw': 256}
{'target': 600, 'success': 172, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 243, 'raw': 416}
{'target': 600, 'success': 262, 'raw': 448}
{'target': 600, 'success': 279, 'raw': 480}
{'target': 600, 'success': 298, 'raw': 512}
{'target': 600, 'success': 317, 'raw': 544}
{'target': 600, 'success': 332, 'raw': 576}
{'target': 600, 'success': 350, 'raw': 608}
{'target': 600, 'success': 365, 'raw': 640}
{'target': 600, 'success': 388, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 429, 'raw': 736}
{'target': 600, 'success': 446, 'raw': 768}
{'target': 600, 'success': 466, 'raw': 800}
{'target': 600, 'success': 482, 'raw': 832}
{'target': 600, 'success': 495, 'raw': 864}
{'target': 600, 'success': 509, 'raw': 896}
{'target': 600, 'success': 522, 'raw': 928}
{'target': 600, 'success': 543, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 584, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5681818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n']
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n', "Relation : religion . Context : After the death of King Henry IV of France ( c. 589 - 7 February 1235 ) , St Peter was succeeded as Archbishop by the eponymous St Peter 's successor , the first Pope . Head Entity : St Peter 's successors , Tail Entity : St Peter 's .\n"]
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 369, 'raw': 544}
{'target': 600, 'success': 391, 'raw': 576}
{'target': 600, 'success': 415, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 459, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 534, 'raw': 768}
{'target': 600, 'success': 553, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 15147
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15247, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:17, 17.62s/it]Extractor Estimating: 2it [00:20,  8.76s/it]Extractor Estimating: 3it [00:20,  5.08s/it]Extractor Estimating: 4it [00:21,  3.32s/it]Extractor Estimating: 5it [00:22,  2.38s/it]Extractor Estimating: 6it [00:22,  1.77s/it]Extractor Estimating: 7it [00:23,  1.42s/it]Extractor Estimating: 8it [00:24,  1.16s/it]Extractor Estimating: 9it [00:24,  1.02it/s]Extractor Estimating: 10it [00:25,  1.09it/s]Extractor Estimating: 11it [00:26,  1.21it/s]Extractor Estimating: 12it [00:27,  1.13s/it]Extractor Estimating: 13it [00:28,  1.02s/it]Extractor Estimating: 14it [00:29,  1.09it/s]Extractor Estimating: 15it [00:29,  1.21it/s]Extractor Estimating: 16it [00:30,  1.29it/s]Extractor Estimating: 17it [00:31,  1.36it/s]Extractor Estimating: 18it [00:31,  1.43it/s]Extractor Estimating: 19it [00:32,  1.41it/s]Extractor Estimating: 20it [00:33,  1.43it/s]Extractor Estimating: 21it [00:33,  1.47it/s]Extractor Estimating: 22it [00:35,  1.18it/s]Extractor Estimating: 23it [00:35,  1.26it/s]Extractor Estimating: 24it [00:36,  1.25it/s]Extractor Estimating: 25it [00:37,  1.30it/s]Extractor Estimating: 26it [00:37,  1.36it/s]Extractor Estimating: 27it [00:38,  1.37it/s]Extractor Estimating: 28it [00:39,  1.36it/s]Extractor Estimating: 29it [00:40,  1.42it/s]Extractor Estimating: 30it [00:40,  1.40it/s]Extractor Estimating: 31it [00:41,  1.44it/s]Extractor Estimating: 32it [00:42,  1.43it/s]Extractor Estimating: 33it [00:42,  1.40it/s]Extractor Estimating: 34it [00:43,  1.41it/s]Extractor Estimating: 35it [00:44,  1.43it/s]Extractor Estimating: 36it [00:44,  1.44it/s]Extractor Estimating: 37it [00:45,  1.47it/s]Extractor Estimating: 38it [00:46,  1.41it/s]Extractor Estimating: 39it [00:47,  1.45it/s]Extractor Estimating: 40it [00:47,  1.45it/s]Extractor Estimating: 41it [00:48,  1.41it/s]Extractor Estimating: 42it [00:49,  1.40it/s]Extractor Estimating: 43it [00:49,  1.43it/s]Extractor Estimating: 44it [00:50,  1.44it/s]Extractor Estimating: 45it [00:51,  1.45it/s]Extractor Estimating: 46it [00:51,  1.48it/s]Extractor Estimating: 47it [00:52,  1.48it/s]Extractor Estimating: 48it [00:53,  1.46it/s]Extractor Estimating: 49it [00:53,  1.51it/s]Extractor Estimating: 50it [00:54,  1.50it/s]Extractor Estimating: 51it [00:55,  1.46it/s]Extractor Estimating: 52it [00:55,  1.48it/s]Extractor Estimating: 53it [00:56,  1.46it/s]Extractor Estimating: 54it [00:57,  1.48it/s]Extractor Estimating: 55it [00:57,  1.50it/s]Extractor Estimating: 56it [00:58,  1.50it/s]Extractor Estimating: 57it [00:59,  1.50it/s]Extractor Estimating: 58it [01:00,  1.43it/s]Extractor Estimating: 59it [01:00,  1.48it/s]Extractor Estimating: 60it [01:01,  1.53it/s]Extractor Estimating: 61it [01:01,  1.57it/s]Extractor Estimating: 62it [01:02,  1.60it/s]Extractor Estimating: 63it [01:03,  1.55it/s]Extractor Estimating: 64it [01:03,  1.56it/s]Extractor Estimating: 65it [01:04,  1.56it/s]Extractor Estimating: 66it [01:05,  1.56it/s]Extractor Estimating: 67it [01:05,  1.53it/s]Extractor Estimating: 68it [01:06,  1.45it/s]Extractor Estimating: 69it [01:07,  1.46it/s]Extractor Estimating: 70it [01:07,  1.48it/s]Extractor Estimating: 71it [01:08,  1.50it/s]Extractor Estimating: 72it [01:09,  1.51it/s]Extractor Estimating: 73it [01:09,  1.50it/s]Extractor Estimating: 74it [01:10,  1.56it/s]Extractor Estimating: 75it [01:11,  1.59it/s]Extractor Estimating: 76it [01:14,  1.38s/it]Extractor Estimating: 77it [01:14,  1.18s/it]Extractor Estimating: 78it [01:15,  1.01s/it]Extractor Estimating: 79it [01:16,  1.14it/s]Extractor Estimating: 80it [01:16,  1.28it/s]Extractor Estimating: 81it [01:17,  1.38it/s]Extractor Estimating: 82it [01:17,  1.40it/s]Extractor Estimating: 83it [01:18,  1.48it/s]Extractor Estimating: 84it [01:19,  1.40it/s]Extractor Estimating: 85it [01:19,  1.46it/s]Extractor Estimating: 86it [01:20,  1.38it/s]Extractor Estimating: 87it [01:21,  1.42it/s]Extractor Estimating: 88it [01:22,  1.45it/s]Extractor Estimating: 89it [01:22,  1.44it/s]Extractor Estimating: 90it [01:23,  1.42it/s]Extractor Estimating: 91it [01:24,  1.47it/s]Extractor Estimating: 92it [01:24,  1.48it/s]Extractor Estimating: 93it [01:25,  1.51it/s]Extractor Estimating: 94it [01:26,  1.50it/s]Extractor Estimating: 95it [01:26,  1.48it/s]Extractor Estimating: 96it [01:27,  1.48it/s]Extractor Estimating: 97it [01:28,  1.52it/s]Extractor Estimating: 98it [01:28,  1.54it/s]Extractor Estimating: 99it [01:29,  1.55it/s]Extractor Estimating: 100it [01:29,  1.53it/s]Extractor Estimating: 101it [01:30,  1.52it/s]Extractor Estimating: 102it [01:31,  1.48it/s]Extractor Estimating: 103it [01:32,  1.47it/s]Extractor Estimating: 104it [01:32,  1.55it/s]Extractor Estimating: 105it [01:33,  1.53it/s]Extractor Estimating: 106it [01:33,  1.56it/s]Extractor Estimating: 107it [01:34,  1.59it/s]Extractor Estimating: 108it [01:35,  1.57it/s]Extractor Estimating: 109it [01:35,  1.61it/s]Extractor Estimating: 110it [01:36,  1.52it/s]Extractor Estimating: 111it [01:37,  1.53it/s]Extractor Estimating: 112it [01:37,  1.56it/s]Extractor Estimating: 113it [01:38,  1.59it/s]Extractor Estimating: 114it [01:38,  1.62it/s]Extractor Estimating: 115it [01:39,  1.52it/s]Extractor Estimating: 116it [01:40,  1.50it/s]Extractor Estimating: 117it [01:40,  1.55it/s]Extractor Estimating: 118it [01:41,  1.53it/s]Extractor Estimating: 119it [01:42,  1.55it/s]Extractor Estimating: 120it [01:43,  1.45it/s]Extractor Estimating: 121it [01:43,  1.51it/s]Extractor Estimating: 122it [01:44,  1.53it/s]Extractor Estimating: 123it [01:44,  1.56it/s]Extractor Estimating: 124it [01:45,  1.58it/s]Extractor Estimating: 125it [01:46,  1.59it/s]Extractor Estimating: 126it [01:46,  1.62it/s]Extractor Estimating: 127it [01:47,  1.59it/s]Extractor Estimating: 128it [01:47,  1.64it/s]Extractor Estimating: 129it [01:48,  1.65it/s]Extractor Estimating: 130it [01:49,  1.56it/s]Extractor Estimating: 131it [01:49,  1.53it/s]Extractor Estimating: 132it [01:50,  1.53it/s]Extractor Estimating: 133it [01:51,  1.54it/s]Extractor Estimating: 134it [01:51,  1.57it/s]Extractor Estimating: 135it [01:52,  1.64it/s]Extractor Estimating: 136it [01:52,  1.67it/s]Extractor Estimating: 137it [01:53,  1.60it/s]Extractor Estimating: 138it [01:54,  1.58it/s]Extractor Estimating: 139it [01:54,  1.63it/s]Extractor Estimating: 140it [01:55,  1.63it/s]Extractor Estimating: 141it [01:56,  1.67it/s]Extractor Estimating: 142it [01:56,  1.72it/s]Extractor Estimating: 143it [01:57,  1.71it/s]Extractor Estimating: 144it [01:57,  1.62it/s]Extractor Estimating: 145it [01:58,  1.65it/s]Extractor Estimating: 146it [01:59,  1.52it/s]Extractor Estimating: 147it [01:59,  1.59it/s]Extractor Estimating: 148it [02:00,  1.63it/s]Extractor Estimating: 149it [02:01,  1.54it/s]Extractor Estimating: 150it [02:01,  1.49it/s]Extractor Estimating: 151it [02:02,  1.52it/s]Extractor Estimating: 152it [02:03,  1.53it/s]Extractor Estimating: 153it [02:03,  1.52it/s]Extractor Estimating: 154it [02:04,  1.44it/s]Extractor Estimating: 155it [02:05,  1.44it/s]Extractor Estimating: 156it [02:05,  1.44it/s]Extractor Estimating: 157it [02:06,  1.48it/s]Extractor Estimating: 158it [02:07,  1.51it/s]Extractor Estimating: 159it [02:07,  1.49it/s]Extractor Estimating: 160it [02:08,  1.53it/s]Extractor Estimating: 161it [02:09,  1.51it/s]Extractor Estimating: 162it [02:09,  1.48it/s]Extractor Estimating: 163it [02:10,  1.48it/s]Extractor Estimating: 164it [02:11,  1.47it/s]Extractor Estimating: 165it [02:11,  1.47it/s]Extractor Estimating: 166it [02:12,  1.50it/s]Extractor Estimating: 167it [02:13,  1.51it/s]Extractor Estimating: 168it [02:13,  1.48it/s]Extractor Estimating: 169it [02:14,  1.48it/s]Extractor Estimating: 170it [02:15,  1.44it/s]Extractor Estimating: 171it [02:16,  1.43it/s]Extractor Estimating: 172it [02:16,  1.48it/s]Extractor Estimating: 173it [02:17,  1.52it/s]Extractor Estimating: 174it [02:18,  1.48it/s]Extractor Estimating: 175it [02:18,  1.40it/s]Extractor Estimating: 176it [02:19,  1.41it/s]Extractor Estimating: 177it [02:20,  1.46it/s]Extractor Estimating: 178it [02:20,  1.50it/s]Extractor Estimating: 179it [02:21,  1.50it/s]Extractor Estimating: 180it [02:22,  1.52it/s]Extractor Estimating: 181it [02:22,  1.54it/s]Extractor Estimating: 182it [02:23,  1.54it/s]Extractor Estimating: 183it [02:23,  1.56it/s]Extractor Estimating: 184it [02:24,  1.62it/s]Extractor Estimating: 185it [02:25,  1.55it/s]Extractor Estimating: 186it [02:25,  1.58it/s]Extractor Estimating: 187it [02:26,  1.53it/s]Extractor Estimating: 188it [02:27,  1.57it/s]Extractor Estimating: 189it [02:27,  1.59it/s]Extractor Estimating: 190it [02:28,  1.51it/s]Extractor Estimating: 191it [02:29,  1.53it/s]Extractor Estimating: 192it [02:29,  1.58it/s]Extractor Estimating: 193it [02:30,  1.60it/s]Extractor Estimating: 194it [02:30,  1.62it/s]Extractor Estimating: 195it [02:31,  1.59it/s]Extractor Estimating: 196it [02:32,  1.52it/s]Extractor Estimating: 197it [02:32,  1.52it/s]Extractor Estimating: 198it [02:33,  1.48it/s]Extractor Estimating: 199it [02:34,  1.50it/s]Extractor Estimating: 200it [02:34,  1.52it/s]Extractor Estimating: 201it [02:35,  1.50it/s]Extractor Estimating: 202it [02:36,  1.51it/s]Extractor Estimating: 203it [02:37,  1.47it/s]Extractor Estimating: 204it [02:37,  1.47it/s]Extractor Estimating: 205it [02:38,  1.41it/s]Extractor Estimating: 206it [02:39,  1.39it/s]Extractor Estimating: 207it [02:39,  1.40it/s]Extractor Estimating: 208it [02:40,  1.41it/s]Extractor Estimating: 209it [02:41,  1.44it/s]Extractor Estimating: 210it [02:42,  1.39it/s]Extractor Estimating: 211it [02:42,  1.42it/s]Extractor Estimating: 212it [02:43,  1.46it/s]Extractor Estimating: 213it [02:44,  1.50it/s]Extractor Estimating: 214it [02:44,  1.48it/s]Extractor Estimating: 215it [02:45,  1.44it/s]Extractor Estimating: 216it [02:46,  1.45it/s]Extractor Estimating: 217it [02:46,  1.46it/s]Extractor Estimating: 218it [02:47,  1.44it/s]Extractor Estimating: 219it [02:48,  1.47it/s]Extractor Estimating: 220it [02:48,  1.43it/s]Extractor Estimating: 221it [02:49,  1.36it/s]Extractor Estimating: 222it [02:50,  1.40it/s]Extractor Estimating: 223it [02:51,  1.40it/s]Extractor Estimating: 224it [02:51,  1.40it/s]Extractor Estimating: 225it [02:52,  1.33it/s]Extractor Estimating: 226it [02:53,  1.47it/s]Extractor Estimating: 227it [02:53,  1.53it/s]Extractor Estimating: 228it [02:54,  1.54it/s]Extractor Estimating: 229it [02:54,  1.58it/s]Extractor Estimating: 230it [02:55,  1.55it/s]Extractor Estimating: 231it [02:56,  1.51it/s]Extractor Estimating: 232it [02:56,  1.58it/s]Extractor Estimating: 233it [02:57,  1.68it/s]Extractor Estimating: 234it [02:57,  1.73it/s]Extractor Estimating: 235it [02:58,  1.74it/s]Extractor Estimating: 236it [02:59,  1.66it/s]Extractor Estimating: 237it [02:59,  1.70it/s]Extractor Estimating: 238it [03:00,  1.63it/s]Extractor Estimating: 239it [03:01,  1.59it/s]Extractor Estimating: 240it [03:01,  1.64it/s]Extractor Estimating: 241it [03:02,  1.58it/s]Extractor Estimating: 242it [03:02,  1.61it/s]Extractor Estimating: 243it [03:03,  1.60it/s]Extractor Estimating: 244it [03:04,  1.63it/s]Extractor Estimating: 245it [03:04,  1.65it/s]Extractor Estimating: 246it [03:05,  1.59it/s]Extractor Estimating: 247it [03:06,  1.60it/s]Extractor Estimating: 248it [03:06,  1.59it/s]Extractor Estimating: 249it [03:07,  1.61it/s]Extractor Estimating: 250it [03:07,  1.62it/s]Extractor Estimating: 251it [03:08,  1.45it/s]Extractor Estimating: 252it [03:09,  1.52it/s]Extractor Estimating: 253it [03:10,  1.40it/s]Extractor Estimating: 254it [03:10,  1.38it/s]Extractor Estimating: 255it [03:11,  1.37it/s]Extractor Estimating: 256it [03:12,  1.45it/s]Extractor Estimating: 257it [03:12,  1.48it/s]Extractor Estimating: 258it [03:13,  1.51it/s]Extractor Estimating: 259it [03:14,  1.56it/s]Extractor Estimating: 260it [03:14,  1.49it/s]Extractor Estimating: 261it [03:15,  1.51it/s]Extractor Estimating: 262it [03:16,  1.48it/s]Extractor Estimating: 263it [03:16,  1.47it/s]Extractor Estimating: 264it [03:17,  1.49it/s]Extractor Estimating: 265it [03:18,  1.46it/s]Extractor Estimating: 266it [03:19,  1.39it/s]Extractor Estimating: 267it [03:19,  1.42it/s]Extractor Estimating: 268it [03:20,  1.45it/s]Extractor Estimating: 269it [03:21,  1.43it/s]Extractor Estimating: 270it [03:21,  1.40it/s]Extractor Estimating: 271it [03:22,  1.38it/s]Extractor Estimating: 272it [03:23,  1.31it/s]Extractor Estimating: 273it [03:24,  1.36it/s]Extractor Estimating: 274it [03:24,  1.44it/s]Extractor Estimating: 275it [03:25,  1.51it/s]Extractor Estimating: 276it [03:25,  1.55it/s]Extractor Estimating: 277it [03:26,  1.50it/s]Extractor Estimating: 278it [03:27,  1.57it/s]Extractor Estimating: 279it [03:27,  1.66it/s]Extractor Estimating: 280it [03:28,  1.63it/s]Extractor Estimating: 281it [03:29,  1.64it/s]Extractor Estimating: 282it [03:29,  1.62it/s]Extractor Estimating: 283it [03:30,  1.64it/s]Extractor Estimating: 284it [03:30,  1.63it/s]Extractor Estimating: 285it [03:31,  1.65it/s]Extractor Estimating: 286it [03:32,  1.63it/s]Extractor Estimating: 287it [03:32,  1.48it/s]Extractor Estimating: 288it [03:33,  1.47it/s]Extractor Estimating: 289it [03:34,  1.49it/s]Extractor Estimating: 290it [03:34,  1.57it/s]Extractor Estimating: 291it [03:35,  1.55it/s]Extractor Estimating: 292it [03:36,  1.51it/s]Extractor Estimating: 293it [03:36,  1.52it/s]Extractor Estimating: 294it [03:37,  1.52it/s]Extractor Estimating: 295it [03:38,  1.55it/s]Extractor Estimating: 296it [03:38,  1.58it/s]Extractor Estimating: 297it [03:39,  1.51it/s]Extractor Estimating: 298it [03:40,  1.54it/s]Extractor Estimating: 299it [03:40,  1.56it/s]Extractor Estimating: 300it [03:41,  1.56it/s]Extractor Estimating: 301it [03:41,  1.55it/s]Extractor Estimating: 302it [03:42,  1.55it/s]Extractor Estimating: 303it [03:43,  1.58it/s]Extractor Estimating: 304it [03:43,  1.55it/s]Extractor Estimating: 305it [03:44,  1.54it/s]Extractor Estimating: 306it [03:45,  1.60it/s]Extractor Estimating: 307it [03:45,  1.58it/s]Extractor Estimating: 308it [03:46,  1.54it/s]Extractor Estimating: 309it [03:46,  1.65it/s]Extractor Estimating: 310it [03:47,  1.62it/s]Extractor Estimating: 311it [03:48,  1.59it/s]Extractor Estimating: 312it [03:49,  1.48it/s]Extractor Estimating: 313it [03:49,  1.49it/s]Extractor Estimating: 314it [03:50,  1.49it/s]Extractor Estimating: 315it [03:50,  1.54it/s]Extractor Estimating: 316it [03:51,  1.56it/s]Extractor Estimating: 317it [03:52,  1.51it/s]Extractor Estimating: 318it [03:52,  1.55it/s]Extractor Estimating: 319it [03:53,  1.55it/s]Extractor Estimating: 320it [03:54,  1.51it/s]Extractor Estimating: 321it [03:54,  1.56it/s]Extractor Estimating: 322it [03:55,  1.56it/s]Extractor Estimating: 323it [03:56,  1.54it/s]Extractor Estimating: 324it [03:56,  1.53it/s]Extractor Estimating: 325it [03:57,  1.49it/s]Extractor Estimating: 326it [03:58,  1.54it/s]Extractor Estimating: 327it [03:58,  1.58it/s]Extractor Estimating: 328it [03:59,  1.46it/s]Extractor Estimating: 329it [04:00,  1.57it/s]Extractor Estimating: 330it [04:01,  1.34it/s]Extractor Estimating: 331it [04:01,  1.46it/s]Extractor Estimating: 332it [04:02,  1.53it/s]Extractor Estimating: 333it [04:02,  1.63it/s]Extractor Estimating: 334it [04:03,  1.65it/s]Extractor Estimating: 335it [04:03,  1.56it/s]Extractor Estimating: 336it [04:04,  1.55it/s]Extractor Estimating: 337it [04:05,  1.61it/s]Extractor Estimating: 338it [04:05,  1.64it/s]Extractor Estimating: 339it [04:06,  1.62it/s]Extractor Estimating: 340it [04:07,  1.56it/s]Extractor Estimating: 341it [04:07,  1.63it/s]Extractor Estimating: 342it [04:08,  1.67it/s]Extractor Estimating: 343it [04:08,  1.71it/s]Extractor Estimating: 344it [04:09,  1.74it/s]Extractor Estimating: 345it [04:09,  1.74it/s]Extractor Estimating: 346it [04:10,  1.61it/s]Extractor Estimating: 347it [04:11,  1.65it/s]Extractor Estimating: 348it [04:11,  1.62it/s]Extractor Estimating: 349it [04:12,  1.63it/s]Extractor Estimating: 350it [04:13,  1.64it/s]Extractor Estimating: 351it [04:13,  1.44it/s]Extractor Estimating: 352it [04:14,  1.41it/s]Extractor Estimating: 353it [04:15,  1.42it/s]Extractor Estimating: 354it [04:16,  1.45it/s]Extractor Estimating: 355it [04:16,  1.40it/s]Extractor Estimating: 356it [04:17,  1.37it/s]Extractor Estimating: 357it [04:18,  1.42it/s]Extractor Estimating: 358it [04:18,  1.48it/s]Extractor Estimating: 359it [04:19,  1.45it/s]Extractor Estimating: 360it [04:20,  1.47it/s]Extractor Estimating: 361it [04:21,  1.40it/s]Extractor Estimating: 362it [04:21,  1.44it/s]Extractor Estimating: 363it [04:22,  1.53it/s]Extractor Estimating: 364it [04:22,  1.53it/s]Extractor Estimating: 365it [04:23,  1.51it/s]Extractor Estimating: 366it [04:24,  1.42it/s]Extractor Estimating: 367it [04:25,  1.39it/s]Extractor Estimating: 368it [04:25,  1.45it/s]Extractor Estimating: 369it [04:26,  1.43it/s]Extractor Estimating: 370it [04:27,  1.48it/s]Extractor Estimating: 371it [04:27,  1.50it/s]Extractor Estimating: 372it [04:28,  1.40it/s]Extractor Estimating: 373it [04:29,  1.40it/s]Extractor Estimating: 374it [04:29,  1.45it/s]Extractor Estimating: 375it [04:30,  1.49it/s]Extractor Estimating: 375it [04:30,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7486 mean pseudo reward: 0.9646102559875417
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 27097
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27197, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_10_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27197, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.243, loss:3265.8106
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.017, loss:2322.4460
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.990, loss:1864.1932
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.983, loss:1691.1565
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 0.978, loss:1620.9381
>> valid entity prec:0.5419, rec:0.6176, f1:0.5773
>> valid relation prec:0.6300, rec:0.0951, f1:0.1653
>> valid relation with NER prec:0.6300, rec:0.0951, f1:0.1653
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.341, loss:1511.3086
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.975, loss:1419.7564
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.980, loss:1317.0504
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 0.981, loss:1254.8871
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 0.992, loss:1182.7072
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5990, rec:0.5829, f1:0.5908
>> valid relation prec:0.5196, rec:0.1255, f1:0.2022
>> valid relation with NER prec:0.5196, rec:0.1255, f1:0.2022
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 164, avg_time 2.317, loss:1170.4138
g_step 1200, step 264, avg_time 0.991, loss:1118.1223
g_step 1300, step 52, avg_time 0.990, loss:1042.4878
g_step 1400, step 152, avg_time 0.987, loss:1037.1697
g_step 1500, step 252, avg_time 0.984, loss:1034.3167
>> valid entity prec:0.5009, rec:0.5790, f1:0.5372
>> valid relation prec:0.4027, rec:0.1126, f1:0.1760
>> valid relation with NER prec:0.4027, rec:0.1126, f1:0.1760
g_step 1600, step 40, avg_time 2.278, loss:992.9419
g_step 1700, step 140, avg_time 0.983, loss:959.1672
g_step 1800, step 240, avg_time 1.157, loss:959.8077
g_step 1900, step 28, avg_time 0.985, loss:948.7322
g_step 2000, step 128, avg_time 0.987, loss:846.4551
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5894, rec:0.5558, f1:0.5721
>> valid relation prec:0.3815, rec:0.1315, f1:0.1956
>> valid relation with NER prec:0.3815, rec:0.1315, f1:0.1956
g_step 2100, step 228, avg_time 2.272, loss:923.8794
g_step 2200, step 16, avg_time 0.979, loss:904.7099
g_step 2300, step 116, avg_time 0.985, loss:837.6570
g_step 2400, step 216, avg_time 0.989, loss:874.4194
g_step 2500, step 4, avg_time 0.986, loss:850.2029
>> valid entity prec:0.6098, rec:0.5151, f1:0.5584
>> valid relation prec:0.3786, rec:0.1166, f1:0.1783
>> valid relation with NER prec:0.3786, rec:0.1166, f1:0.1783
g_step 2600, step 104, avg_time 2.276, loss:780.3882
g_step 2700, step 204, avg_time 0.985, loss:820.9918
g_step 2800, step 304, avg_time 0.989, loss:812.6844
g_step 2900, step 92, avg_time 0.992, loss:767.0604
g_step 3000, step 192, avg_time 0.979, loss:790.2718
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5732, rec:0.5659, f1:0.5695
>> valid relation prec:0.3099, rec:0.1198, f1:0.1728
>> valid relation with NER prec:0.3099, rec:0.1198, f1:0.1728
g_step 3100, step 292, avg_time 2.274, loss:761.6838
g_step 3200, step 80, avg_time 0.976, loss:714.5633
g_step 3300, step 180, avg_time 0.983, loss:733.1393
g_step 3400, step 280, avg_time 0.986, loss:742.1962
g_step 3500, step 68, avg_time 0.989, loss:685.0415
>> valid entity prec:0.5793, rec:0.5511, f1:0.5648
>> valid relation prec:0.3042, rec:0.1066, f1:0.1579
>> valid relation with NER prec:0.3042, rec:0.1066, f1:0.1579
g_step 3600, step 168, avg_time 2.280, loss:711.8012
g_step 3700, step 268, avg_time 0.989, loss:711.9334
g_step 3800, step 56, avg_time 0.983, loss:689.7725
g_step 3900, step 156, avg_time 0.991, loss:685.1469
g_step 4000, step 256, avg_time 0.975, loss:695.7125
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5656, rec:0.5669, f1:0.5663
>> valid relation prec:0.2530, rec:0.1261, f1:0.1683
>> valid relation with NER prec:0.2530, rec:0.1261, f1:0.1683
g_step 4100, step 44, avg_time 2.255, loss:650.0611
g_step 4200, step 144, avg_time 0.981, loss:641.6591
g_step 4300, step 244, avg_time 0.987, loss:653.5654
g_step 4400, step 32, avg_time 0.967, loss:659.4924
g_step 4500, step 132, avg_time 0.992, loss:629.0700
>> valid entity prec:0.5798, rec:0.5439, f1:0.5613
>> valid relation prec:0.2881, rec:0.1266, f1:0.1760
>> valid relation with NER prec:0.2881, rec:0.1266, f1:0.1760
g_step 4600, step 232, avg_time 2.247, loss:618.9104
g_step 4700, step 20, avg_time 0.974, loss:617.6672
g_step 4800, step 120, avg_time 0.976, loss:596.9537
g_step 4900, step 220, avg_time 0.974, loss:604.5307
g_step 5000, step 8, avg_time 0.965, loss:605.8474
learning rate was adjusted to 0.0008
>> valid entity prec:0.6041, rec:0.5403, f1:0.5704
>> valid relation prec:0.2615, rec:0.1221, f1:0.1664
>> valid relation with NER prec:0.2615, rec:0.1221, f1:0.1664
g_step 5100, step 108, avg_time 2.245, loss:551.8538
g_step 5200, step 208, avg_time 0.969, loss:585.2578
g_step 5300, step 308, avg_time 0.972, loss:606.9528
g_step 5400, step 96, avg_time 0.967, loss:517.8618
g_step 5500, step 196, avg_time 0.955, loss:563.1487
>> valid entity prec:0.5979, rec:0.5492, f1:0.5725
>> valid relation prec:0.2615, rec:0.1223, f1:0.1667
>> valid relation with NER prec:0.2615, rec:0.1223, f1:0.1667
g_step 5600, step 296, avg_time 2.247, loss:577.2624
g_step 5700, step 84, avg_time 0.970, loss:539.7965
g_step 5800, step 184, avg_time 0.984, loss:528.5782
g_step 5900, step 284, avg_time 0.980, loss:534.7153
g_step 6000, step 72, avg_time 0.988, loss:502.7604
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5672, rec:0.5728, f1:0.5700
>> valid relation prec:0.2194, rec:0.1192, f1:0.1545
>> valid relation with NER prec:0.2194, rec:0.1192, f1:0.1545
g_step 6100, step 172, avg_time 2.245, loss:510.9094
g_step 6200, step 272, avg_time 0.989, loss:511.1564
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 23:31:30 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 23:31:30 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_23-31-30_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 23:31:32 - WARNING - datasets.builder -   Using custom data configuration default-2064f7bca534d016
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2064f7bca534d016/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 23:31:38,348 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:31:38,349 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 23:31:38,350 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 23:31:38,351 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 23:31:38,845 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:31:39,011 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:31:39,011 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:31:39,011 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:31:39,012 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:31:39,012 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 23:31:39,012 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 23:31:39,861 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 23:31:43,180 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 23:31:43,244 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2064f7bca534d016/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 23:31:43 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x1543e1c2be60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:06,  1.04ba/s] 25%|██▌       | 2/8 [00:01<00:03,  1.93ba/s] 38%|███▊      | 3/8 [00:01<00:01,  2.63ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.16ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.57ba/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.87ba/s] 88%|████████▊ | 7/8 [00:02<00:00,  4.07ba/s]100%|██████████| 8/8 [00:02<00:00,  4.91ba/s]100%|██████████| 8/8 [00:02<00:00,  3.38ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.50ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.49ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.15ba/s]100%|██████████| 4/4 [00:01<00:00,  4.26ba/s]100%|██████████| 4/4 [00:01<00:00,  3.31ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.82ba/s] 38%|███▊      | 3/8 [00:00<00:00,  6.26ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  8.02ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.05ba/s]100%|██████████| 8/8 [00:00<00:00,  8.40ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  3.56ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.08ba/s]100%|██████████| 4/4 [00:00<00:00,  7.82ba/s]
[INFO|trainer.py:414] 2023-08-27 23:31:53,142 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 23:31:53,249 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 23:31:53,249 >>   Num examples = 7514
[INFO|trainer.py:1149] 2023-08-27 23:31:53,249 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 23:31:53,249 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 23:31:53,249 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 23:31:53,249 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 23:31:53,249 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<03:00,  3.24it/s]  0%|          | 2/585 [00:00<02:54,  3.35it/s]  1%|          | 3/585 [00:00<02:52,  3.38it/s]  1%|          | 4/585 [00:01<02:51,  3.40it/s]  1%|          | 5/585 [00:01<02:50,  3.41it/s]  1%|          | 6/585 [00:01<02:49,  3.42it/s]  1%|          | 7/585 [00:02<02:47,  3.44it/s]  1%|▏         | 8/585 [00:02<03:04,  3.12it/s]  2%|▏         | 9/585 [00:02<02:58,  3.22it/s]  2%|▏         | 10/585 [00:03<02:54,  3.30it/s]  2%|▏         | 11/585 [00:03<02:51,  3.35it/s]  2%|▏         | 12/585 [00:03<02:49,  3.38it/s]  2%|▏         | 13/585 [00:03<02:47,  3.41it/s]  2%|▏         | 14/585 [00:04<02:46,  3.43it/s]  3%|▎         | 15/585 [00:04<02:45,  3.44it/s]  3%|▎         | 16/585 [00:04<02:44,  3.45it/s]  3%|▎         | 17/585 [00:05<02:44,  3.46it/s]  3%|▎         | 18/585 [00:05<02:43,  3.46it/s]  3%|▎         | 19/585 [00:05<02:43,  3.46it/s]  3%|▎         | 20/585 [00:05<02:42,  3.47it/s]  4%|▎         | 21/585 [00:06<02:42,  3.47it/s]  4%|▍         | 22/585 [00:06<02:42,  3.47it/s]  4%|▍         | 23/585 [00:06<02:41,  3.47it/s]  4%|▍         | 24/585 [00:07<02:41,  3.47it/s]  4%|▍         | 25/585 [00:07<02:41,  3.47it/s]  4%|▍         | 26/585 [00:07<02:40,  3.47it/s]  5%|▍         | 27/585 [00:07<02:40,  3.47it/s]  5%|▍         | 28/585 [00:08<02:40,  3.47it/s]  5%|▍         | 29/585 [00:08<02:40,  3.47it/s]  5%|▌         | 30/585 [00:08<02:39,  3.47it/s]  5%|▌         | 31/585 [00:09<02:39,  3.47it/s]  5%|▌         | 32/585 [00:09<02:39,  3.47it/s]  6%|▌         | 33/585 [00:09<02:39,  3.47it/s]  6%|▌         | 34/585 [00:09<02:38,  3.47it/s]  6%|▌         | 35/585 [00:10<02:38,  3.47it/s]  6%|▌         | 36/585 [00:10<02:38,  3.47it/s]  6%|▋         | 37/585 [00:10<02:37,  3.47it/s]  6%|▋         | 38/585 [00:11<02:37,  3.47it/s]  7%|▋         | 39/585 [00:11<02:37,  3.47it/s]  7%|▋         | 40/585 [00:11<02:36,  3.47it/s]  7%|▋         | 41/585 [00:11<02:36,  3.48it/s]  7%|▋         | 42/585 [00:12<02:36,  3.47it/s]  7%|▋         | 43/585 [00:12<02:45,  3.28it/s]  8%|▊         | 44/585 [00:12<02:42,  3.33it/s]  8%|▊         | 45/585 [00:13<02:40,  3.37it/s]  8%|▊         | 46/585 [00:13<02:38,  3.40it/s]  8%|▊         | 47/585 [00:13<02:37,  3.42it/s]  8%|▊         | 48/585 [00:14<02:36,  3.43it/s]  8%|▊         | 49/585 [00:14<02:43,  3.28it/s]  9%|▊         | 50/585 [00:14<02:40,  3.34it/s]  9%|▊         | 51/585 [00:14<02:38,  3.38it/s]  9%|▉         | 52/585 [00:15<02:36,  3.40it/s]  9%|▉         | 53/585 [00:15<02:35,  3.42it/s]  9%|▉         | 54/585 [00:15<02:34,  3.43it/s]  9%|▉         | 55/585 [00:16<02:33,  3.44it/s] 10%|▉         | 56/585 [00:16<02:33,  3.45it/s] 10%|▉         | 57/585 [00:16<02:32,  3.45it/s] 10%|▉         | 58/585 [00:16<02:32,  3.46it/s] 10%|█         | 59/585 [00:17<02:31,  3.46it/s] 10%|█         | 60/585 [00:17<02:31,  3.46it/s] 10%|█         | 61/585 [00:17<02:31,  3.46it/s] 11%|█         | 62/585 [00:18<02:30,  3.47it/s] 11%|█         | 63/585 [00:18<02:30,  3.46it/s] 11%|█         | 64/585 [00:18<02:30,  3.46it/s] 11%|█         | 65/585 [00:18<02:29,  3.47it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.47it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 68/585 [00:19<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 71/585 [00:20<02:28,  3.46it/s] 12%|█▏        | 72/585 [00:20<02:27,  3.47it/s] 12%|█▏        | 73/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 74/585 [00:21<02:27,  3.46it/s] 13%|█▎        | 75/585 [00:21<02:27,  3.47it/s] 13%|█▎        | 76/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.46it/s] 13%|█▎        | 78/585 [00:22<02:26,  3.46it/s] 14%|█▎        | 79/585 [00:23<02:26,  3.47it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 81/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 82/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 83/585 [00:24<02:24,  3.47it/s] 14%|█▍        | 84/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 85/585 [00:24<02:24,  3.46it/s] 15%|█▍        | 86/585 [00:25<02:23,  3.47it/s] 15%|█▍        | 87/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 88/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:25<02:23,  3.46it/s] 15%|█▌        | 90/585 [00:26<02:23,  3.46it/s] 16%|█▌        | 91/585 [00:26<02:22,  3.47it/s] 16%|█▌        | 92/585 [00:26<02:22,  3.46it/s] 16%|█▌        | 93/585 [00:27<02:22,  3.46it/s] 16%|█▌        | 94/585 [00:27<02:21,  3.46it/s] 16%|█▌        | 95/585 [00:27<02:21,  3.46it/s] 16%|█▋        | 96/585 [00:27<02:21,  3.46it/s] 17%|█▋        | 97/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 98/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:28<02:20,  3.46it/s] 17%|█▋        | 100/585 [00:29<02:20,  3.46it/s] 17%|█▋        | 101/585 [00:29<02:19,  3.46it/s] 17%|█▋        | 102/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 103/585 [00:29<02:19,  3.46it/s] 18%|█▊        | 104/585 [00:30<02:18,  3.47it/s] 18%|█▊        | 105/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 106/585 [00:30<02:18,  3.46it/s] 18%|█▊        | 107/585 [00:31<02:18,  3.46it/s] 18%|█▊        | 108/585 [00:31<02:17,  3.46it/s] 19%|█▊        | 109/585 [00:31<02:26,  3.26it/s] 19%|█▉        | 110/585 [00:32<02:54,  2.72it/s] 19%|█▉        | 111/585 [00:32<02:43,  2.90it/s] 19%|█▉        | 112/585 [00:32<02:35,  3.05it/s] 19%|█▉        | 113/585 [00:33<02:29,  3.17it/s] 19%|█▉        | 114/585 [00:33<02:25,  3.25it/s] 20%|█▉        | 115/585 [00:33<02:22,  3.31it/s] 20%|█▉        | 116/585 [00:33<02:19,  3.35it/s] 20%|██        | 117/585 [00:34<02:18,  3.39it/s][INFO|trainer.py:2140] 2023-08-27 23:32:27,628 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:32:27,628 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-27 23:32:27,628 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.54it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.03it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.24it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.64it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.16it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.94it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.66it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.56it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.56it/s][A
 12%|█▏        | 52/437 [00:01<00:09, 38.87it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 40.60it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 41.83it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 42.73it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.43it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.87it/s][A
 19%|█▉        | 82/437 [00:01<00:09, 38.18it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 39.79it/s][A
 21%|██        | 92/437 [00:02<00:08, 41.16it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 42.23it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.04it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.62it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.04it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.13it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.93it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.81it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.83it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.23it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.43it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.58it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.70it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.79it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.17it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.06it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.08it/s][A
 41%|████      | 177/437 [00:04<00:07, 35.58it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 37.57it/s][A
 43%|████▎     | 187/437 [00:04<00:06, 39.71it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 41.19it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 42.20it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 42.96it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.60it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 38.12it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 40.04it/s][A
 51%|█████     | 222/437 [00:05<00:05, 41.30it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 42.34it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.09it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.53it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.01it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.15it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.84it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 43.66it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 43.93it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.06it/s][A
 62%|██████▏   | 272/437 [00:06<00:05, 32.36it/s][A
 63%|██████▎   | 277/437 [00:06<00:04, 36.16it/s][A
 65%|██████▍   | 282/437 [00:06<00:04, 38.47it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 40.25it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 41.19it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 42.40it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 37.49it/s][A
 70%|███████   | 307/437 [00:07<00:03, 39.48it/s][A
 71%|███████▏  | 312/437 [00:07<00:03, 40.77it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 41.82it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 42.70it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.35it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.78it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 39.51it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 40.76it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 41.88it/s][A
 81%|████████  | 352/437 [00:08<00:01, 42.76it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.29it/s][A
 83%|████████▎ | 362/437 [00:08<00:02, 36.54it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 38.63it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 40.31it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 41.66it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 42.58it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.33it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.72it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.02it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.66it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.65it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.63it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.93it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 44.07it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.44it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.61it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.69it/s][A                                                 
                                                 [A 20%|██        | 117/585 [00:44<02:18,  3.39it/s]
100%|██████████| 437/437 [00:10<00:00, 44.69it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:32:38,707 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-27 23:32:39,038 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:33:01,074 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:33:01,548 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:33:01,789 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [01:59<3:19:54, 25.68s/it] 20%|██        | 119/585 [01:59<2:20:28, 18.09s/it] 21%|██        | 120/585 [01:59<1:38:47, 12.75s/it] 21%|██        | 121/585 [02:00<1:09:52,  9.04s/it] 21%|██        | 122/585 [02:00<49:29,  6.41s/it]   21%|██        | 123/585 [02:00<35:14,  4.58s/it] 21%|██        | 124/585 [02:01<25:17,  3.29s/it] 21%|██▏       | 125/585 [02:01<18:20,  2.39s/it] 22%|██▏       | 126/585 [02:01<13:28,  1.76s/it] 22%|██▏       | 127/585 [02:01<10:05,  1.32s/it] 22%|██▏       | 128/585 [02:02<07:42,  1.01s/it] 22%|██▏       | 129/585 [02:02<06:13,  1.22it/s] 22%|██▏       | 130/585 [02:02<05:00,  1.51it/s] 22%|██▏       | 131/585 [02:03<04:10,  1.82it/s] 23%|██▎       | 132/585 [02:03<03:34,  2.11it/s] 23%|██▎       | 133/585 [02:03<03:09,  2.39it/s] 23%|██▎       | 134/585 [02:04<02:51,  2.62it/s] 23%|██▎       | 135/585 [02:04<02:39,  2.82it/s] 23%|██▎       | 136/585 [02:04<02:30,  2.97it/s] 23%|██▎       | 137/585 [02:04<02:24,  3.09it/s] 24%|██▎       | 138/585 [02:05<02:20,  3.18it/s] 24%|██▍       | 139/585 [02:05<02:26,  3.05it/s] 24%|██▍       | 140/585 [02:05<02:21,  3.15it/s] 24%|██▍       | 141/585 [02:06<02:17,  3.23it/s] 24%|██▍       | 142/585 [02:06<02:14,  3.28it/s] 24%|██▍       | 143/585 [02:06<02:12,  3.32it/s] 25%|██▍       | 144/585 [02:07<02:11,  3.35it/s] 25%|██▍       | 145/585 [02:07<02:10,  3.37it/s] 25%|██▍       | 146/585 [02:07<02:09,  3.38it/s] 25%|██▌       | 147/585 [02:07<02:09,  3.39it/s] 25%|██▌       | 148/585 [02:08<02:08,  3.40it/s] 25%|██▌       | 149/585 [02:08<02:08,  3.40it/s] 26%|██▌       | 150/585 [02:08<02:14,  3.23it/s] 26%|██▌       | 151/585 [02:09<02:12,  3.28it/s] 26%|██▌       | 152/585 [02:09<02:10,  3.32it/s] 26%|██▌       | 153/585 [02:09<02:08,  3.35it/s] 26%|██▋       | 154/585 [02:10<02:17,  3.14it/s] 26%|██▋       | 155/585 [02:10<02:13,  3.23it/s] 27%|██▋       | 156/585 [02:10<02:10,  3.29it/s] 27%|██▋       | 157/585 [02:11<02:08,  3.34it/s] 27%|██▋       | 158/585 [02:11<02:06,  3.38it/s] 27%|██▋       | 159/585 [02:11<02:05,  3.41it/s] 27%|██▋       | 160/585 [02:11<02:12,  3.21it/s] 28%|██▊       | 161/585 [02:12<02:09,  3.28it/s] 28%|██▊       | 162/585 [02:12<02:06,  3.33it/s] 28%|██▊       | 163/585 [02:12<02:05,  3.37it/s] 28%|██▊       | 164/585 [02:13<02:03,  3.40it/s] 28%|██▊       | 165/585 [02:13<02:03,  3.41it/s] 28%|██▊       | 166/585 [02:13<02:02,  3.43it/s] 29%|██▊       | 167/585 [02:13<02:01,  3.44it/s] 29%|██▊       | 168/585 [02:14<02:00,  3.45it/s] 29%|██▉       | 169/585 [02:14<02:00,  3.45it/s] 29%|██▉       | 170/585 [02:14<02:00,  3.46it/s] 29%|██▉       | 171/585 [02:15<02:10,  3.18it/s] 29%|██▉       | 172/585 [02:15<02:06,  3.26it/s] 30%|██▉       | 173/585 [02:15<02:04,  3.32it/s] 30%|██▉       | 174/585 [02:16<02:02,  3.36it/s] 30%|██▉       | 175/585 [02:16<02:00,  3.39it/s] 30%|███       | 176/585 [02:16<01:59,  3.41it/s] 30%|███       | 177/585 [02:16<01:59,  3.43it/s] 30%|███       | 178/585 [02:17<01:58,  3.44it/s] 31%|███       | 179/585 [02:17<01:57,  3.44it/s] 31%|███       | 180/585 [02:17<01:57,  3.45it/s] 31%|███       | 181/585 [02:18<01:57,  3.45it/s] 31%|███       | 182/585 [02:18<02:04,  3.24it/s] 31%|███▏      | 183/585 [02:18<02:01,  3.30it/s] 31%|███▏      | 184/585 [02:19<01:59,  3.35it/s] 32%|███▏      | 185/585 [02:19<01:58,  3.38it/s] 32%|███▏      | 186/585 [02:19<01:57,  3.40it/s] 32%|███▏      | 187/585 [02:19<01:56,  3.42it/s] 32%|███▏      | 188/585 [02:20<01:55,  3.43it/s] 32%|███▏      | 189/585 [02:20<01:55,  3.44it/s] 32%|███▏      | 190/585 [02:20<01:54,  3.44it/s] 33%|███▎      | 191/585 [02:21<01:54,  3.45it/s] 33%|███▎      | 192/585 [02:21<01:53,  3.45it/s] 33%|███▎      | 193/585 [02:21<02:01,  3.23it/s] 33%|███▎      | 194/585 [02:21<01:58,  3.30it/s] 33%|███▎      | 195/585 [02:22<01:56,  3.35it/s] 34%|███▎      | 196/585 [02:22<01:55,  3.38it/s] 34%|███▎      | 197/585 [02:22<01:53,  3.41it/s] 34%|███▍      | 198/585 [02:23<01:53,  3.42it/s] 34%|███▍      | 199/585 [02:23<01:52,  3.43it/s] 34%|███▍      | 200/585 [02:23<01:51,  3.44it/s] 34%|███▍      | 201/585 [02:24<01:51,  3.44it/s] 35%|███▍      | 202/585 [02:24<01:51,  3.45it/s] 35%|███▍      | 203/585 [02:24<01:50,  3.45it/s] 35%|███▍      | 204/585 [02:24<01:58,  3.23it/s] 35%|███▌      | 205/585 [02:25<01:55,  3.29it/s] 35%|███▌      | 206/585 [02:25<01:53,  3.34it/s] 35%|███▌      | 207/585 [02:25<01:51,  3.38it/s] 36%|███▌      | 208/585 [02:26<01:50,  3.40it/s] 36%|███▌      | 209/585 [02:26<01:50,  3.42it/s] 36%|███▌      | 210/585 [02:26<01:49,  3.43it/s] 36%|███▌      | 211/585 [02:26<01:48,  3.44it/s] 36%|███▌      | 212/585 [02:27<01:48,  3.44it/s] 36%|███▋      | 213/585 [02:27<01:47,  3.45it/s] 37%|███▋      | 214/585 [02:27<01:47,  3.45it/s] 37%|███▋      | 215/585 [02:28<01:54,  3.24it/s] 37%|███▋      | 216/585 [02:28<01:51,  3.30it/s] 37%|███▋      | 217/585 [02:28<01:49,  3.35it/s] 37%|███▋      | 218/585 [02:29<01:48,  3.38it/s] 37%|███▋      | 219/585 [02:29<01:47,  3.40it/s] 38%|███▊      | 220/585 [02:29<01:46,  3.42it/s] 38%|███▊      | 221/585 [02:29<01:46,  3.43it/s] 38%|███▊      | 222/585 [02:30<01:45,  3.44it/s] 38%|███▊      | 223/585 [02:30<01:45,  3.44it/s] 38%|███▊      | 224/585 [02:30<01:49,  3.29it/s] 38%|███▊      | 225/585 [02:31<01:47,  3.34it/s] 39%|███▊      | 226/585 [02:31<01:46,  3.37it/s] 39%|███▉      | 227/585 [02:31<01:45,  3.39it/s] 39%|███▉      | 228/585 [02:31<01:44,  3.41it/s] 39%|███▉      | 229/585 [02:32<01:59,  2.98it/s] 39%|███▉      | 230/585 [02:32<01:54,  3.11it/s] 39%|███▉      | 231/585 [02:33<01:50,  3.21it/s] 40%|███▉      | 232/585 [02:33<01:47,  3.28it/s] 40%|███▉      | 233/585 [02:33<01:45,  3.33it/s] 40%|████      | 234/585 [02:33<01:54,  3.08it/s][INFO|trainer.py:2140] 2023-08-27 23:34:27,332 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:34:27,332 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-27 23:34:27,332 >>   Batch size = 8
{'eval_loss': 1.025233268737793, 'eval_runtime': 10.3585, 'eval_samples_per_second': 337.113, 'eval_steps_per_second': 42.187, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.41it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.24it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.62it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.36it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.27it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.60it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.18it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.10it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.17it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.42it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.62it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.84it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.81it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.64it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.33it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.16it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.21it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.26it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.36it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.54it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.65it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.75it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 40.51it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 41.72it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 42.50it/s][A
 30%|███       | 132/437 [00:02<00:07, 42.97it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.39it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.83it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.17it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.24it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.00it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.08it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.23it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.27it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.32it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.31it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.47it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.61it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.54it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.30it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.17it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.40it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.42it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.45it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.48it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.42it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.47it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.34it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.34it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 38.60it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 40.27it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 41.56it/s][A
 61%|██████    | 267/437 [00:06<00:03, 42.58it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.30it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.80it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.15it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.18it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.84it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.62it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.76it/s][A
 70%|███████   | 307/437 [00:06<00:02, 43.96it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.32it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.46it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.63it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.73it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.45it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.21it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.06it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.05it/s][A
 81%|████████  | 352/437 [00:07<00:01, 44.11it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.35it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.55it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.70it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.66it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.42it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.25it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 38.29it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 40.10it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 41.45it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 42.45it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.12it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.67it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.10it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.03it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.66it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.54it/s][A
100%|██████████| 437/437 [00:09<00:00, 43.86it/s][A                                                 
                                                 [A 40%|████      | 234/585 [02:44<01:54,  3.08it/s]
100%|██████████| 437/437 [00:09<00:00, 43.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:34:37,497 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-27 23:34:38,273 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:35:06,752 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:35:07,503 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:35:07,925 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [03:53<2:19:46, 23.96s/it] 40%|████      | 236/585 [03:53<1:38:14, 16.89s/it] 41%|████      | 237/585 [03:53<1:09:04, 11.91s/it] 41%|████      | 238/585 [03:54<48:43,  8.42s/it]   41%|████      | 239/585 [03:54<34:30,  5.98s/it] 41%|████      | 240/585 [03:54<24:35,  4.28s/it] 41%|████      | 241/585 [03:54<17:39,  3.08s/it] 41%|████▏     | 242/585 [03:55<12:49,  2.24s/it] 42%|████▏     | 243/585 [03:55<09:26,  1.66s/it] 42%|████▏     | 244/585 [03:55<07:05,  1.25s/it] 42%|████▏     | 245/585 [03:56<05:25,  1.04it/s] 42%|████▏     | 246/585 [03:56<04:16,  1.32it/s] 42%|████▏     | 247/585 [03:56<03:36,  1.56it/s] 42%|████▏     | 248/585 [03:57<03:18,  1.70it/s] 43%|████▎     | 249/585 [03:57<02:47,  2.01it/s] 43%|████▎     | 250/585 [03:57<02:25,  2.30it/s] 43%|████▎     | 251/585 [03:58<02:10,  2.56it/s] 43%|████▎     | 252/585 [03:58<01:59,  2.78it/s] 43%|████▎     | 253/585 [03:58<01:52,  2.96it/s] 43%|████▎     | 254/585 [03:58<01:46,  3.10it/s] 44%|████▎     | 255/585 [03:59<01:42,  3.20it/s] 44%|████▍     | 256/585 [03:59<01:40,  3.28it/s] 44%|████▍     | 257/585 [03:59<01:57,  2.78it/s] 44%|████▍     | 258/585 [04:00<01:50,  2.96it/s] 44%|████▍     | 259/585 [04:00<01:45,  3.10it/s] 44%|████▍     | 260/585 [04:00<01:41,  3.20it/s] 45%|████▍     | 261/585 [04:01<01:38,  3.28it/s] 45%|████▍     | 262/585 [04:01<01:36,  3.33it/s] 45%|████▍     | 263/585 [04:01<01:35,  3.37it/s] 45%|████▌     | 264/585 [04:02<01:34,  3.40it/s] 45%|████▌     | 265/585 [04:02<01:33,  3.42it/s] 45%|████▌     | 266/585 [04:02<01:32,  3.44it/s] 46%|████▌     | 267/585 [04:02<01:38,  3.24it/s] 46%|████▌     | 268/585 [04:03<01:35,  3.31it/s] 46%|████▌     | 269/585 [04:03<01:34,  3.35it/s] 46%|████▌     | 270/585 [04:03<01:32,  3.39it/s] 46%|████▋     | 271/585 [04:04<01:31,  3.41it/s] 46%|████▋     | 272/585 [04:04<01:31,  3.43it/s] 47%|████▋     | 273/585 [04:04<01:30,  3.44it/s] 47%|████▋     | 274/585 [04:04<01:30,  3.45it/s] 47%|████▋     | 275/585 [04:05<01:29,  3.46it/s] 47%|████▋     | 276/585 [04:05<01:29,  3.46it/s] 47%|████▋     | 277/585 [04:05<01:29,  3.46it/s] 48%|████▊     | 278/585 [04:06<01:33,  3.29it/s] 48%|████▊     | 279/585 [04:06<01:31,  3.35it/s] 48%|████▊     | 280/585 [04:06<01:30,  3.38it/s] 48%|████▊     | 281/585 [04:07<01:29,  3.41it/s] 48%|████▊     | 282/585 [04:07<01:28,  3.43it/s] 48%|████▊     | 283/585 [04:07<01:27,  3.44it/s] 49%|████▊     | 284/585 [04:07<01:27,  3.45it/s] 49%|████▊     | 285/585 [04:08<01:26,  3.45it/s] 49%|████▉     | 286/585 [04:08<01:26,  3.46it/s] 49%|████▉     | 287/585 [04:08<01:26,  3.46it/s] 49%|████▉     | 288/585 [04:09<01:25,  3.46it/s] 49%|████▉     | 289/585 [04:09<01:31,  3.24it/s] 50%|████▉     | 290/585 [04:09<01:29,  3.31it/s] 50%|████▉     | 291/585 [04:09<01:27,  3.35it/s] 50%|████▉     | 292/585 [04:10<01:28,  3.30it/s] 50%|█████     | 293/585 [04:10<01:27,  3.35it/s] 50%|█████     | 294/585 [04:10<01:26,  3.38it/s] 50%|█████     | 295/585 [04:11<01:25,  3.40it/s] 51%|█████     | 296/585 [04:11<01:24,  3.42it/s] 51%|█████     | 297/585 [04:11<01:23,  3.43it/s] 51%|█████     | 298/585 [04:12<01:23,  3.45it/s] 51%|█████     | 299/585 [04:12<01:23,  3.44it/s] 51%|█████▏    | 300/585 [04:12<01:28,  3.21it/s] 51%|█████▏    | 301/585 [04:12<01:26,  3.28it/s] 52%|█████▏    | 302/585 [04:13<01:24,  3.34it/s] 52%|█████▏    | 303/585 [04:13<01:23,  3.37it/s] 52%|█████▏    | 304/585 [04:13<01:22,  3.40it/s] 52%|█████▏    | 305/585 [04:14<01:21,  3.42it/s] 52%|█████▏    | 306/585 [04:14<01:21,  3.43it/s] 52%|█████▏    | 307/585 [04:14<01:20,  3.44it/s] 53%|█████▎    | 308/585 [04:14<01:20,  3.45it/s] 53%|█████▎    | 309/585 [04:15<01:24,  3.27it/s] 53%|█████▎    | 310/585 [04:15<01:27,  3.13it/s] 53%|█████▎    | 311/585 [04:15<01:24,  3.22it/s] 53%|█████▎    | 312/585 [04:16<01:22,  3.29it/s] 54%|█████▎    | 313/585 [04:16<01:21,  3.35it/s] 54%|█████▎    | 314/585 [04:16<01:20,  3.38it/s] 54%|█████▍    | 315/585 [04:17<01:19,  3.41it/s] 54%|█████▍    | 316/585 [04:17<01:18,  3.42it/s] 54%|█████▍    | 317/585 [04:17<01:18,  3.43it/s] 54%|█████▍    | 318/585 [04:17<01:17,  3.44it/s] 55%|█████▍    | 319/585 [04:18<01:17,  3.45it/s] 55%|█████▍    | 320/585 [04:18<01:16,  3.45it/s] 55%|█████▍    | 321/585 [04:18<01:25,  3.10it/s] 55%|█████▌    | 322/585 [04:19<01:22,  3.20it/s] 55%|█████▌    | 323/585 [04:19<01:19,  3.28it/s] 55%|█████▌    | 324/585 [04:19<01:18,  3.33it/s] 56%|█████▌    | 325/585 [04:20<01:17,  3.37it/s] 56%|█████▌    | 326/585 [04:20<01:16,  3.40it/s] 56%|█████▌    | 327/585 [04:20<01:15,  3.41it/s] 56%|█████▌    | 328/585 [04:20<01:14,  3.43it/s] 56%|█████▌    | 329/585 [04:21<01:14,  3.44it/s] 56%|█████▋    | 330/585 [04:21<01:13,  3.45it/s] 57%|█████▋    | 331/585 [04:21<01:13,  3.45it/s] 57%|█████▋    | 332/585 [04:22<01:20,  3.13it/s] 57%|█████▋    | 333/585 [04:22<01:18,  3.23it/s] 57%|█████▋    | 334/585 [04:22<01:16,  3.29it/s] 57%|█████▋    | 335/585 [04:23<01:14,  3.34it/s] 57%|█████▋    | 336/585 [04:23<01:13,  3.38it/s] 58%|█████▊    | 337/585 [04:23<01:12,  3.40it/s] 58%|█████▊    | 338/585 [04:23<01:12,  3.42it/s] 58%|█████▊    | 339/585 [04:24<01:11,  3.43it/s] 58%|█████▊    | 340/585 [04:24<01:11,  3.44it/s] 58%|█████▊    | 341/585 [04:24<01:10,  3.45it/s] 58%|█████▊    | 342/585 [04:25<01:10,  3.45it/s] 59%|█████▊    | 343/585 [04:25<01:14,  3.24it/s] 59%|█████▉    | 344/585 [04:25<01:13,  3.30it/s] 59%|█████▉    | 345/585 [04:26<01:11,  3.35it/s] 59%|█████▉    | 346/585 [04:26<01:10,  3.38it/s] 59%|█████▉    | 347/585 [04:26<01:09,  3.41it/s] 59%|█████▉    | 348/585 [04:26<01:09,  3.42it/s] 60%|█████▉    | 349/585 [04:27<01:08,  3.44it/s] 60%|█████▉    | 350/585 [04:27<01:08,  3.44it/s] 60%|██████    | 351/585 [04:27<01:07,  3.45it/s][INFO|trainer.py:2140] 2023-08-27 23:36:21,145 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:36:21,145 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-27 23:36:21,145 >>   Batch size = 8
{'eval_loss': 1.021533489227295, 'eval_runtime': 9.9771, 'eval_samples_per_second': 350.002, 'eval_steps_per_second': 43.8, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.64it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.02it/s][A
  4%|▍         | 17/437 [00:16<00:13, 30.81it/s][A
  4%|▍         | 17/437 [00:16<00:13, 30.81it/s][A
  4%|▍         | 18/437 [00:16<11:46,  1.69s/it][A
  5%|▌         | 23/437 [00:16<06:42,  1.03it/s][A
  6%|▋         | 28/437 [00:16<04:09,  1.64it/s][A
  8%|▊         | 33/437 [00:16<02:43,  2.48it/s][A
  9%|▊         | 38/437 [00:16<01:50,  3.62it/s][A
 10%|▉         | 43/437 [00:16<01:16,  5.16it/s][A
 11%|█         | 48/437 [00:16<00:54,  7.16it/s][A
 12%|█▏        | 53/437 [00:17<00:39,  9.70it/s][A
 13%|█▎        | 58/437 [00:17<00:29, 12.76it/s][A
 14%|█▍        | 63/437 [00:17<00:22, 16.27it/s][A
 16%|█▌        | 68/437 [00:17<00:18, 20.10it/s][A
 17%|█▋        | 73/437 [00:17<00:15, 24.09it/s][A
 18%|█▊        | 78/437 [00:17<00:12, 27.96it/s][A
 19%|█▉        | 83/437 [00:17<00:11, 31.59it/s][A
 20%|██        | 88/437 [00:17<00:10, 34.75it/s][A
 21%|██▏       | 93/437 [00:17<00:09, 37.34it/s][A
 22%|██▏       | 98/437 [00:18<00:08, 39.30it/s][A
 24%|██▎       | 103/437 [00:18<00:08, 40.58it/s][A
 25%|██▍       | 108/437 [00:18<00:07, 41.46it/s][A
 26%|██▌       | 113/437 [00:18<00:07, 42.12it/s][A
 27%|██▋       | 118/437 [00:18<00:07, 42.75it/s][A
 28%|██▊       | 123/437 [00:18<00:07, 43.30it/s][A
 29%|██▉       | 128/437 [00:18<00:07, 43.80it/s][A
 30%|███       | 133/437 [00:18<00:06, 44.23it/s][A
 32%|███▏      | 138/437 [00:18<00:06, 44.55it/s][A
 33%|███▎      | 143/437 [00:19<00:06, 44.56it/s][A
 34%|███▍      | 148/437 [00:19<00:06, 44.37it/s][A
 35%|███▌      | 153/437 [00:19<00:06, 44.16it/s][A
 36%|███▌      | 158/437 [00:19<00:06, 44.09it/s][A
 37%|███▋      | 163/437 [00:19<00:06, 44.02it/s][A
 38%|███▊      | 168/437 [00:19<00:06, 44.28it/s][A
 40%|███▉      | 173/437 [00:19<00:05, 44.47it/s][A
 41%|████      | 178/437 [00:19<00:05, 44.73it/s][A
 42%|████▏     | 183/437 [00:19<00:05, 44.87it/s][A
 43%|████▎     | 188/437 [00:20<00:05, 44.69it/s][A
 44%|████▍     | 193/437 [00:20<00:05, 44.41it/s][A
 45%|████▌     | 198/437 [00:20<00:05, 44.27it/s][A
 46%|████▋     | 203/437 [00:20<00:05, 44.16it/s][A
 48%|████▊     | 208/437 [00:20<00:05, 43.97it/s][A
 49%|████▊     | 213/437 [00:20<00:05, 44.35it/s][A
 50%|████▉     | 218/437 [00:20<00:04, 44.53it/s][A
 51%|█████     | 223/437 [00:20<00:04, 44.76it/s][A
 52%|█████▏    | 228/437 [00:20<00:04, 44.81it/s][A
 53%|█████▎    | 233/437 [00:21<00:04, 44.61it/s][A
 54%|█████▍    | 238/437 [00:21<00:04, 44.47it/s][A
 56%|█████▌    | 243/437 [00:21<00:04, 44.15it/s][A
 57%|█████▋    | 248/437 [00:21<00:04, 44.00it/s][A
 58%|█████▊    | 253/437 [00:21<00:04, 44.13it/s][A
 59%|█████▉    | 258/437 [00:21<00:04, 44.46it/s][A
 60%|██████    | 263/437 [00:21<00:03, 44.54it/s][A
 61%|██████▏   | 268/437 [00:21<00:03, 44.78it/s][A
 62%|██████▏   | 273/437 [00:21<00:03, 44.75it/s][A
 64%|██████▎   | 278/437 [00:22<00:03, 44.63it/s][A
 65%|██████▍   | 283/437 [00:22<00:03, 44.43it/s][A
 66%|██████▌   | 288/437 [00:22<00:03, 41.07it/s][A
 67%|██████▋   | 293/437 [00:22<00:03, 42.08it/s][A
 68%|██████▊   | 298/437 [00:22<00:03, 42.82it/s][A
 69%|██████▉   | 303/437 [00:22<00:03, 43.49it/s][A
 70%|███████   | 308/437 [00:22<00:02, 43.89it/s][A
 72%|███████▏  | 313/437 [00:22<00:02, 44.25it/s][A
 73%|███████▎  | 318/437 [00:23<00:02, 44.18it/s][A
 74%|███████▍  | 323/437 [00:23<00:02, 44.30it/s][A
 75%|███████▌  | 328/437 [00:23<00:02, 44.00it/s][A
 76%|███████▌  | 333/437 [00:23<00:02, 43.82it/s][A
 77%|███████▋  | 338/437 [00:23<00:02, 44.07it/s][A
 78%|███████▊  | 343/437 [00:23<00:02, 44.21it/s][A
 80%|███████▉  | 348/437 [00:23<00:01, 44.56it/s][A
 81%|████████  | 353/437 [00:23<00:01, 44.68it/s][A
 82%|████████▏ | 358/437 [00:23<00:01, 44.73it/s][A
 83%|████████▎ | 363/437 [00:24<00:01, 44.74it/s][A
 84%|████████▍ | 368/437 [00:24<00:01, 44.58it/s][A
 85%|████████▌ | 373/437 [00:24<00:01, 44.27it/s][A
 86%|████████▋ | 378/437 [00:24<00:01, 44.18it/s][A
 88%|████████▊ | 383/437 [00:24<00:01, 44.31it/s][A
 89%|████████▉ | 388/437 [00:24<00:01, 44.57it/s][A
 90%|████████▉ | 393/437 [00:24<00:00, 44.70it/s][A
 91%|█████████ | 398/437 [00:24<00:00, 44.85it/s][A
 92%|█████████▏| 403/437 [00:24<00:00, 44.92it/s][A
 93%|█████████▎| 408/437 [00:25<00:00, 44.78it/s][A
 95%|█████████▍| 413/437 [00:25<00:00, 44.58it/s][A
 96%|█████████▌| 418/437 [00:25<00:00, 44.33it/s][A
 97%|█████████▋| 423/437 [00:25<00:00, 43.46it/s][A
 98%|█████████▊| 428/437 [00:25<00:00, 43.82it/s][A
 99%|█████████▉| 433/437 [00:25<00:00, 44.23it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [04:53<01:07,  3.45it/s]
100%|██████████| 437/437 [00:25<00:00, 44.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:36:47,161 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-27 23:36:47,412 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:36:49,689 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:36:49,854 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:36:49,938 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [05:28<1:11:10, 18.33s/it] 60%|██████    | 353/585 [05:28<50:00, 12.93s/it]   61%|██████    | 354/585 [05:28<35:11,  9.14s/it] 61%|██████    | 355/585 [05:29<24:51,  6.49s/it] 61%|██████    | 356/585 [05:29<17:39,  4.63s/it] 61%|██████    | 357/585 [05:29<12:38,  3.33s/it] 61%|██████    | 358/585 [05:30<09:08,  2.42s/it] 61%|██████▏   | 359/585 [05:30<06:41,  1.78s/it] 62%|██████▏   | 360/585 [05:30<04:59,  1.33s/it] 62%|██████▏   | 361/585 [05:30<03:48,  1.02s/it] 62%|██████▏   | 362/585 [05:31<02:58,  1.25it/s] 62%|██████▏   | 363/585 [05:31<02:24,  1.54it/s] 62%|██████▏   | 364/585 [05:31<02:04,  1.78it/s] 62%|██████▏   | 365/585 [05:32<01:45,  2.08it/s] 63%|██████▎   | 366/585 [05:32<01:39,  2.20it/s] 63%|██████▎   | 367/585 [05:32<01:36,  2.27it/s] 63%|██████▎   | 368/585 [05:33<01:25,  2.53it/s] 63%|██████▎   | 369/585 [05:33<01:18,  2.74it/s] 63%|██████▎   | 370/585 [05:33<01:13,  2.92it/s] 63%|██████▎   | 371/585 [05:34<01:10,  3.05it/s] 64%|██████▎   | 372/585 [05:34<01:07,  3.15it/s] 64%|██████▍   | 373/585 [05:34<01:05,  3.23it/s] 64%|██████▍   | 374/585 [05:35<01:08,  3.10it/s] 64%|██████▍   | 375/585 [05:35<01:05,  3.19it/s] 64%|██████▍   | 376/585 [05:35<01:04,  3.26it/s] 64%|██████▍   | 377/585 [05:35<01:02,  3.31it/s] 65%|██████▍   | 378/585 [05:36<01:01,  3.34it/s] 65%|██████▍   | 379/585 [05:36<01:01,  3.37it/s] 65%|██████▍   | 380/585 [05:36<01:00,  3.39it/s] 65%|██████▌   | 381/585 [05:37<01:00,  3.39it/s] 65%|██████▌   | 382/585 [05:37<00:59,  3.40it/s] 65%|██████▌   | 383/585 [05:37<00:59,  3.41it/s] 66%|██████▌   | 384/585 [05:37<00:58,  3.41it/s] 66%|██████▌   | 385/585 [05:38<01:05,  3.05it/s] 66%|██████▌   | 386/585 [05:38<01:03,  3.15it/s] 66%|██████▌   | 387/585 [05:38<01:01,  3.22it/s] 66%|██████▋   | 388/585 [05:39<01:00,  3.28it/s] 66%|██████▋   | 389/585 [05:39<00:59,  3.32it/s] 67%|██████▋   | 390/585 [05:39<00:58,  3.35it/s] 67%|██████▋   | 391/585 [05:40<00:57,  3.37it/s] 67%|██████▋   | 392/585 [05:40<00:57,  3.39it/s] 67%|██████▋   | 393/585 [05:40<00:56,  3.40it/s] 67%|██████▋   | 394/585 [05:40<00:56,  3.40it/s] 68%|██████▊   | 395/585 [05:41<00:59,  3.21it/s] 68%|██████▊   | 396/585 [05:41<00:57,  3.27it/s] 68%|██████▊   | 397/585 [05:41<00:56,  3.32it/s] 68%|██████▊   | 398/585 [05:42<00:55,  3.35it/s] 68%|██████▊   | 399/585 [05:42<00:55,  3.37it/s] 68%|██████▊   | 400/585 [05:42<00:54,  3.38it/s] 69%|██████▊   | 401/585 [05:43<00:54,  3.40it/s] 69%|██████▊   | 402/585 [05:43<00:53,  3.40it/s] 69%|██████▉   | 403/585 [05:43<00:53,  3.40it/s] 69%|██████▉   | 404/585 [05:43<00:53,  3.41it/s] 69%|██████▉   | 405/585 [05:44<00:52,  3.41it/s] 69%|██████▉   | 406/585 [05:44<00:57,  3.09it/s] 70%|██████▉   | 407/585 [05:44<00:55,  3.18it/s] 70%|██████▉   | 408/585 [05:45<00:54,  3.25it/s] 70%|██████▉   | 409/585 [05:45<01:04,  2.72it/s] 70%|███████   | 410/585 [05:46<01:00,  2.90it/s] 70%|███████   | 411/585 [05:46<00:57,  3.04it/s] 70%|███████   | 412/585 [05:46<00:55,  3.14it/s] 71%|███████   | 413/585 [05:46<00:53,  3.22it/s] 71%|███████   | 414/585 [05:47<00:52,  3.28it/s] 71%|███████   | 415/585 [05:47<00:51,  3.32it/s] 71%|███████   | 416/585 [05:47<00:50,  3.35it/s] 71%|███████▏  | 417/585 [05:48<00:49,  3.37it/s] 71%|███████▏  | 418/585 [05:48<00:49,  3.38it/s] 72%|███████▏  | 419/585 [05:48<00:51,  3.24it/s] 72%|███████▏  | 420/585 [05:49<00:50,  3.29it/s] 72%|███████▏  | 421/585 [05:49<00:49,  3.32it/s] 72%|███████▏  | 422/585 [05:49<00:48,  3.35it/s] 72%|███████▏  | 423/585 [05:49<00:48,  3.37it/s] 72%|███████▏  | 424/585 [05:50<00:47,  3.38it/s] 73%|███████▎  | 425/585 [05:50<00:47,  3.39it/s] 73%|███████▎  | 426/585 [05:50<00:46,  3.40it/s] 73%|███████▎  | 427/585 [05:51<00:46,  3.40it/s] 73%|███████▎  | 428/585 [05:51<00:46,  3.41it/s] 73%|███████▎  | 429/585 [05:51<00:45,  3.41it/s] 74%|███████▎  | 430/585 [05:51<00:48,  3.22it/s] 74%|███████▎  | 431/585 [05:52<00:47,  3.27it/s] 74%|███████▍  | 432/585 [05:52<00:46,  3.32it/s] 74%|███████▍  | 433/585 [05:52<00:45,  3.34it/s] 74%|███████▍  | 434/585 [05:53<00:44,  3.37it/s] 74%|███████▍  | 435/585 [05:53<00:44,  3.38it/s] 75%|███████▍  | 436/585 [05:53<00:43,  3.39it/s] 75%|███████▍  | 437/585 [05:54<00:43,  3.39it/s] 75%|███████▍  | 438/585 [05:54<00:43,  3.40it/s] 75%|███████▌  | 439/585 [05:54<00:42,  3.40it/s] 75%|███████▌  | 440/585 [05:54<00:42,  3.41it/s] 75%|███████▌  | 441/585 [05:55<00:45,  3.18it/s] 76%|███████▌  | 442/585 [05:55<00:44,  3.25it/s] 76%|███████▌  | 443/585 [05:55<00:43,  3.29it/s] 76%|███████▌  | 444/585 [05:56<00:42,  3.32it/s] 76%|███████▌  | 445/585 [05:56<00:41,  3.35it/s] 76%|███████▌  | 446/585 [05:56<00:41,  3.37it/s] 76%|███████▋  | 447/585 [05:57<00:44,  3.11it/s] 77%|███████▋  | 448/585 [05:57<00:45,  2.99it/s] 77%|███████▋  | 449/585 [05:57<00:43,  3.11it/s] 77%|███████▋  | 450/585 [05:58<00:42,  3.19it/s] 77%|███████▋  | 451/585 [05:58<00:44,  3.02it/s] 77%|███████▋  | 452/585 [05:58<00:42,  3.12it/s] 77%|███████▋  | 453/585 [05:59<00:41,  3.20it/s] 78%|███████▊  | 454/585 [05:59<00:40,  3.27it/s] 78%|███████▊  | 455/585 [05:59<00:39,  3.31it/s] 78%|███████▊  | 456/585 [05:59<00:38,  3.34it/s] 78%|███████▊  | 457/585 [06:00<00:38,  3.36it/s] 78%|███████▊  | 458/585 [06:00<00:37,  3.37it/s] 78%|███████▊  | 459/585 [06:00<00:37,  3.39it/s] 79%|███████▊  | 460/585 [06:01<00:36,  3.39it/s] 79%|███████▉  | 461/585 [06:01<00:39,  3.10it/s] 79%|███████▉  | 462/585 [06:01<00:38,  3.19it/s] 79%|███████▉  | 463/585 [06:02<00:37,  3.25it/s] 79%|███████▉  | 464/585 [06:02<00:36,  3.30it/s] 79%|███████▉  | 465/585 [06:02<00:36,  3.33it/s] 80%|███████▉  | 466/585 [06:03<00:42,  2.80it/s] 80%|███████▉  | 467/585 [06:03<00:39,  2.96it/s] 80%|████████  | 468/585 [06:03<00:37,  3.09it/s][INFO|trainer.py:2140] 2023-08-27 23:37:57,091 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:37:57,092 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-27 23:37:57,092 >>   Batch size = 8
{'eval_loss': 1.023051142692566, 'eval_runtime': 25.6964, 'eval_samples_per_second': 135.895, 'eval_steps_per_second': 17.006, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.98it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.04it/s][A
  4%|▍         | 17/437 [00:00<00:11, 36.86it/s][A
  5%|▌         | 22/437 [00:00<00:10, 39.44it/s][A
  6%|▌         | 27/437 [00:00<00:09, 41.21it/s][A
  7%|▋         | 32/437 [00:00<00:09, 42.53it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.27it/s][A
 10%|▉         | 42/437 [00:00<00:09, 43.87it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.19it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.17it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.91it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.78it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.85it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.20it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.38it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.63it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.79it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.87it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.69it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.30it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.01it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.09it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.37it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.55it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.71it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.82it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.75it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.46it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.07it/s][A
 35%|███▍      | 152/437 [00:03<00:07, 37.62it/s][A
 36%|███▌      | 157/437 [00:03<00:07, 39.64it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 41.11it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 42.24it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.00it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.54it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.96it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.10it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.83it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.74it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.98it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.18it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.44it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.61it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.66it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.83it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.55it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.34it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.14it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.15it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.32it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.52it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.65it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.76it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.84it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.53it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.47it/s][A
 66%|██████▌   | 287/437 [00:06<00:04, 34.04it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 36.83it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 38.98it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 40.66it/s][A
 70%|███████   | 307/437 [00:07<00:03, 41.74it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 42.62it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.36it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.66it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.52it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.51it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.73it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.99it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.37it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.41it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.67it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.61it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.36it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.14it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.03it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.03it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.24it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.35it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.49it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.70it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.79it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.62it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 38.00it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 39.92it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 41.38it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 42.39it/s][A
100%|██████████| 437/437 [00:10<00:00, 42.88it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [06:13<00:37,  3.09it/s]
100%|██████████| 437/437 [00:10<00:00, 42.88it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:38:07,695 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-27 23:38:08,210 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:38:28,133 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:38:28,405 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:38:28,547 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [07:04<35:28, 18.35s/it] 80%|████████  | 470/585 [07:04<24:50, 12.96s/it] 81%|████████  | 471/585 [07:04<17:24,  9.16s/it] 81%|████████  | 472/585 [07:05<12:14,  6.50s/it] 81%|████████  | 473/585 [07:05<08:39,  4.64s/it] 81%|████████  | 474/585 [07:05<06:10,  3.33s/it] 81%|████████  | 475/585 [07:05<04:26,  2.42s/it] 81%|████████▏ | 476/585 [07:06<03:14,  1.78s/it] 82%|████████▏ | 477/585 [07:06<02:24,  1.34s/it] 82%|████████▏ | 478/585 [07:06<01:49,  1.02s/it] 82%|████████▏ | 479/585 [07:07<01:25,  1.24it/s] 82%|████████▏ | 480/585 [07:07<01:11,  1.48it/s] 82%|████████▏ | 481/585 [07:07<00:58,  1.78it/s] 82%|████████▏ | 482/585 [07:08<00:49,  2.08it/s] 83%|████████▎ | 483/585 [07:08<00:43,  2.36it/s] 83%|████████▎ | 484/585 [07:08<00:38,  2.60it/s] 83%|████████▎ | 485/585 [07:08<00:35,  2.80it/s] 83%|████████▎ | 486/585 [07:09<00:33,  2.96it/s] 83%|████████▎ | 487/585 [07:09<00:31,  3.09it/s] 83%|████████▎ | 488/585 [07:09<00:30,  3.18it/s] 84%|████████▎ | 489/585 [07:10<00:29,  3.25it/s] 84%|████████▍ | 490/585 [07:10<00:32,  2.93it/s] 84%|████████▍ | 491/585 [07:10<00:30,  3.06it/s] 84%|████████▍ | 492/585 [07:11<00:29,  3.16it/s] 84%|████████▍ | 493/585 [07:11<00:28,  3.24it/s] 84%|████████▍ | 494/585 [07:11<00:27,  3.29it/s] 85%|████████▍ | 495/585 [07:12<00:27,  3.33it/s] 85%|████████▍ | 496/585 [07:12<00:26,  3.36it/s] 85%|████████▍ | 497/585 [07:12<00:26,  3.38it/s] 85%|████████▌ | 498/585 [07:12<00:25,  3.39it/s] 85%|████████▌ | 499/585 [07:13<00:25,  3.40it/s] 85%|████████▌ | 500/585 [07:13<00:27,  3.09it/s]                                                  85%|████████▌ | 500/585 [07:13<00:27,  3.09it/s] 86%|████████▌ | 501/585 [07:13<00:26,  3.18it/s] 86%|████████▌ | 502/585 [07:14<00:25,  3.25it/s] 86%|████████▌ | 503/585 [07:14<00:24,  3.30it/s] 86%|████████▌ | 504/585 [07:14<00:27,  2.99it/s] 86%|████████▋ | 505/585 [07:15<00:25,  3.11it/s] 86%|████████▋ | 506/585 [07:15<00:24,  3.20it/s] 87%|████████▋ | 507/585 [07:15<00:23,  3.26it/s] 87%|████████▋ | 508/585 [07:16<00:23,  3.31it/s] 87%|████████▋ | 509/585 [07:16<00:22,  3.34it/s] 87%|████████▋ | 510/585 [07:16<00:24,  3.07it/s] 87%|████████▋ | 511/585 [07:17<00:23,  3.17it/s] 88%|████████▊ | 512/585 [07:17<00:22,  3.24it/s] 88%|████████▊ | 513/585 [07:17<00:21,  3.29it/s] 88%|████████▊ | 514/585 [07:17<00:21,  3.33it/s] 88%|████████▊ | 515/585 [07:18<00:20,  3.36it/s] 88%|████████▊ | 516/585 [07:18<00:20,  3.38it/s] 88%|████████▊ | 517/585 [07:18<00:20,  3.39it/s] 89%|████████▊ | 518/585 [07:19<00:19,  3.40it/s] 89%|████████▊ | 519/585 [07:19<00:19,  3.40it/s] 89%|████████▉ | 520/585 [07:19<00:19,  3.41it/s] 89%|████████▉ | 521/585 [07:19<00:18,  3.41it/s] 89%|████████▉ | 522/585 [07:20<00:18,  3.41it/s] 89%|████████▉ | 523/585 [07:20<00:18,  3.41it/s] 90%|████████▉ | 524/585 [07:20<00:19,  3.18it/s] 90%|████████▉ | 525/585 [07:21<00:18,  3.25it/s] 90%|████████▉ | 526/585 [07:21<00:17,  3.30it/s] 90%|█████████ | 527/585 [07:21<00:17,  3.33it/s] 90%|█████████ | 528/585 [07:22<00:16,  3.36it/s] 90%|█████████ | 529/585 [07:22<00:16,  3.38it/s] 91%|█████████ | 530/585 [07:22<00:16,  3.39it/s] 91%|█████████ | 531/585 [07:22<00:15,  3.40it/s] 91%|█████████ | 532/585 [07:23<00:15,  3.40it/s] 91%|█████████ | 533/585 [07:23<00:15,  3.40it/s] 91%|█████████▏| 534/585 [07:23<00:14,  3.41it/s] 91%|█████████▏| 535/585 [07:24<00:16,  3.02it/s] 92%|█████████▏| 536/585 [07:24<00:15,  3.13it/s] 92%|█████████▏| 537/585 [07:24<00:14,  3.21it/s] 92%|█████████▏| 538/585 [07:25<00:14,  3.27it/s] 92%|█████████▏| 539/585 [07:25<00:13,  3.32it/s] 92%|█████████▏| 540/585 [07:25<00:13,  3.37it/s] 92%|█████████▏| 541/585 [07:25<00:12,  3.39it/s] 93%|█████████▎| 542/585 [07:26<00:12,  3.41it/s] 93%|█████████▎| 543/585 [07:26<00:12,  3.43it/s] 93%|█████████▎| 544/585 [07:26<00:11,  3.44it/s] 93%|█████████▎| 545/585 [07:27<00:12,  3.16it/s] 93%|█████████▎| 546/585 [07:27<00:12,  3.24it/s] 94%|█████████▎| 547/585 [07:27<00:11,  3.30it/s] 94%|█████████▎| 548/585 [07:28<00:11,  3.35it/s] 94%|█████████▍| 549/585 [07:28<00:10,  3.39it/s] 94%|█████████▍| 550/585 [07:28<00:10,  3.41it/s] 94%|█████████▍| 551/585 [07:28<00:09,  3.42it/s] 94%|█████████▍| 552/585 [07:29<00:09,  3.44it/s] 95%|█████████▍| 553/585 [07:29<00:09,  3.44it/s] 95%|█████████▍| 554/585 [07:29<00:08,  3.45it/s] 95%|█████████▍| 555/585 [07:30<00:08,  3.45it/s] 95%|█████████▌| 556/585 [07:30<00:09,  3.03it/s] 95%|█████████▌| 557/585 [07:30<00:08,  3.15it/s] 95%|█████████▌| 558/585 [07:31<00:08,  3.24it/s] 96%|█████████▌| 559/585 [07:31<00:07,  3.30it/s] 96%|█████████▌| 560/585 [07:31<00:07,  3.35it/s] 96%|█████████▌| 561/585 [07:32<00:07,  3.14it/s] 96%|█████████▌| 562/585 [07:32<00:07,  3.23it/s] 96%|█████████▌| 563/585 [07:33<00:09,  2.41it/s] 96%|█████████▋| 564/585 [07:33<00:07,  2.66it/s] 97%|█████████▋| 565/585 [07:33<00:07,  2.66it/s] 97%|█████████▋| 566/585 [07:33<00:06,  2.86it/s] 97%|█████████▋| 567/585 [07:34<00:05,  3.02it/s] 97%|█████████▋| 568/585 [07:34<00:05,  3.14it/s] 97%|█████████▋| 569/585 [07:34<00:04,  3.23it/s] 97%|█████████▋| 570/585 [07:35<00:04,  3.29it/s] 98%|█████████▊| 571/585 [07:35<00:04,  3.34it/s] 98%|█████████▊| 572/585 [07:35<00:03,  3.38it/s] 98%|█████████▊| 573/585 [07:35<00:03,  3.40it/s] 98%|█████████▊| 574/585 [07:36<00:03,  3.42it/s] 98%|█████████▊| 575/585 [07:36<00:02,  3.43it/s] 98%|█████████▊| 576/585 [07:36<00:02,  3.21it/s] 99%|█████████▊| 577/585 [07:37<00:02,  3.28it/s] 99%|█████████▉| 578/585 [07:37<00:02,  3.33it/s] 99%|█████████▉| 579/585 [07:37<00:01,  3.37it/s] 99%|█████████▉| 580/585 [07:38<00:01,  3.39it/s] 99%|█████████▉| 581/585 [07:38<00:01,  3.41it/s] 99%|█████████▉| 582/585 [07:38<00:00,  3.43it/s]100%|█████████▉| 583/585 [07:38<00:00,  3.43it/s]100%|█████████▉| 584/585 [07:39<00:00,  3.44it/s]100%|██████████| 585/585 [07:39<00:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-27 23:39:32,788 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:39:32,788 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-27 23:39:32,788 >>   Batch size = 8
{'eval_loss': 1.032050371170044, 'eval_runtime': 10.1125, 'eval_samples_per_second': 345.314, 'eval_steps_per_second': 43.214, 'epoch': 4.0}
{'loss': 0.821, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.31it/s][A
  3%|▎         | 12/437 [00:00<00:10, 41.63it/s][A
  4%|▍         | 17/437 [00:00<00:09, 42.75it/s][A
  5%|▌         | 22/437 [00:00<00:09, 43.35it/s][A
  6%|▌         | 27/437 [00:00<00:09, 43.74it/s][A
  7%|▋         | 32/437 [00:00<00:09, 43.96it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.95it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.00it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.14it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.84it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.09it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.22it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.29it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.32it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.16it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.21it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.25it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.06it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.95it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.09it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.22it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.42it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.45it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.27it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.24it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.13it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.06it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.97it/s][A
 34%|███▎      | 147/437 [00:03<00:07, 41.37it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 42.31it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.10it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.58it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.76it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.89it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.95it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.85it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.69it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.68it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.97it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.19it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.37it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.44it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.27it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.33it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.12it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.91it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.96it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.07it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.23it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.43it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.47it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.40it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.35it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.06it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.93it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 39.22it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 40.80it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 41.89it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 42.70it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.26it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.72it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.91it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.86it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.55it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.43it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.76it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.02it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.18it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.16it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.53it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.52it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.19it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.92it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.78it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.90it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.16it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.32it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.42it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.48it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.37it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.11it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.00it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.87it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.93it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.12it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.28it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.32it/s][A                                                 
                                                 [A100%|██████████| 585/585 [07:49<00:00,  3.45it/s]
100%|██████████| 437/437 [00:09<00:00, 44.32it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 23:39:43,229 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-27 23:39:43,617 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:40:05,388 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:40:06,043 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:40:06,340 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 23:40:53,089 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 23:40:53,151 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234 (score: 1.021533489227295).
                                                 100%|██████████| 585/585 [09:23<00:00,  3.45it/s]100%|██████████| 585/585 [09:23<00:00,  1.04it/s]
[INFO|trainer.py:1894] 2023-08-27 23:41:16,535 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 23:41:16,758 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 23:41:35,305 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 23:41:35,727 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 23:41:35,901 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 23:41:37,207 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:37,207 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:37,207 >>   train_loss               =     0.8153
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:37,207 >>   train_runtime            = 0:09:23.22
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:37,207 >>   train_samples            =       7514
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:37,207 >>   train_samples_per_second =     66.705
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:37,207 >>   train_steps_per_second   =      1.039
{'eval_loss': 1.0369045734405518, 'eval_runtime': 9.9758, 'eval_samples_per_second': 350.048, 'eval_steps_per_second': 43.806, 'epoch': 5.0}
{'train_runtime': 563.2279, 'train_samples_per_second': 66.705, 'train_steps_per_second': 1.039, 'train_loss': 0.8153288360334869, 'epoch': 5.0}
08/27/2023 23:41:37 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 23:41:37,679 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 23:41:37,679 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-27 23:41:37,679 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:08, 50.08it/s]  3%|▎         | 12/437 [00:00<00:08, 47.57it/s]  4%|▍         | 17/437 [00:00<00:08, 46.89it/s]  5%|▌         | 22/437 [00:00<00:08, 46.54it/s]  6%|▌         | 27/437 [00:00<00:08, 46.13it/s]  7%|▋         | 32/437 [00:00<00:08, 46.18it/s]  8%|▊         | 37/437 [00:00<00:10, 39.04it/s] 10%|▉         | 42/437 [00:00<00:09, 40.59it/s] 11%|█         | 47/437 [00:01<00:09, 41.54it/s] 12%|█▏        | 52/437 [00:01<00:09, 42.10it/s] 13%|█▎        | 57/437 [00:01<00:08, 42.69it/s] 14%|█▍        | 62/437 [00:01<00:08, 43.26it/s] 15%|█▌        | 67/437 [00:01<00:08, 43.94it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.38it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.53it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.80it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.97it/s] 21%|██        | 92/437 [00:02<00:07, 44.98it/s] 22%|██▏       | 97/437 [00:02<00:07, 44.87it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.70it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.60it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.75it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.85it/s] 28%|██▊       | 122/437 [00:02<00:06, 45.01it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.96it/s] 30%|███       | 132/437 [00:02<00:06, 45.00it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.92it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.71it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.68it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.60it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.81it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.96it/s] 38%|███▊      | 167/437 [00:03<00:05, 45.00it/s] 39%|███▉      | 172/437 [00:03<00:06, 40.87it/s] 41%|████      | 177/437 [00:04<00:06, 42.22it/s] 42%|████▏     | 182/437 [00:04<00:05, 43.14it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.56it/s] 44%|████▍     | 192/437 [00:04<00:05, 43.78it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.02it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.42it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.38it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.32it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.38it/s] 51%|█████     | 222/437 [00:05<00:04, 44.64it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.91it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.96it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.89it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.87it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.93it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.84it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.67it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.64it/s] 61%|██████    | 267/437 [00:06<00:03, 44.73it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.92it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.91it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.91it/s] 66%|██████▌   | 287/437 [00:06<00:03, 44.91it/s] 67%|██████▋   | 292/437 [00:06<00:03, 44.85it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.86it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.66it/s] 70%|███████   | 307/437 [00:06<00:03, 39.30it/s] 71%|███████▏  | 312/437 [00:07<00:03, 40.93it/s] 73%|███████▎  | 317/437 [00:07<00:02, 42.22it/s] 74%|███████▎  | 322/437 [00:07<00:02, 43.17it/s] 75%|███████▍  | 327/437 [00:07<00:02, 43.82it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.16it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.67it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.54it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.21it/s] 81%|████████  | 352/437 [00:07<00:01, 44.12it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.13it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.52it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.63it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.91it/s] 86%|████████▋ | 377/437 [00:08<00:01, 45.01it/s] 87%|████████▋ | 382/437 [00:08<00:01, 45.23it/s] 89%|████████▊ | 387/437 [00:08<00:01, 44.98it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.57it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.36it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.40it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.50it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.70it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.88it/s] 97%|█████████▋| 422/437 [00:09<00:00, 45.05it/s] 98%|█████████▊| 427/437 [00:09<00:00, 45.09it/s] 99%|█████████▉| 432/437 [00:09<00:00, 45.00it/s]100%|██████████| 437/437 [00:09<00:00, 44.57it/s]100%|██████████| 437/437 [00:09<00:00, 43.99it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 23:41:47,652 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:47,652 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:47,652 >>   eval_loss               =     1.0215
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:47,652 >>   eval_runtime            = 0:00:09.97
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:47,652 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:47,653 >>   eval_samples_per_second =    350.149
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:47,653 >>   eval_steps_per_second   =     43.819
[INFO|trainer_pt_utils.py:913] 2023-08-27 23:41:47,653 >>   perplexity              =     2.7775
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_0', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:17<04:07, 17.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:35<03:50, 17.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:53<03:33, 17.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:09<03:06, 16.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:28<02:57, 17.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:47<02:45, 18.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:04<02:24, 18.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:22<02:05, 17.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:39<01:45, 17.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:54<01:24, 16.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:11<01:07, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:27<00:49, 16.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:47<00:35, 17.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:09<00:18, 18.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:32<00:00, 20.27s/it]Generating: 100%|██████████| 15/15 [04:32<00:00, 18.18s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : main subject . Context : Later in the year ( 1143–46 ) , he married daughter of Louis XIV and Catherine I of Prussia married to Marie Antoinette III , daughter of Emperor Louis XII and Catherine of Rheims . Head Entity : Catherine I , Tail Entity : Emperor Louis XII and Catherine I of Prussia .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7451923076923077, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.81640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.7391826923076923, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 294, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7013888888888888, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amadec for the first time . Head Entity : Amadec , Tail Entity : Amadeca .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.75625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Paul Groening', 'nominated for', '', 'After a stint in the Swedish music industry in 2002 alongside the late Paul Groening , he moved away to New Zealand to continue his education in the country .')"}}
['Relation : operating system . Context : The CVRN ( Computer Vision and Imaging Systems ) is a digital camera developed at the National Institutes of Health . Head Entity : CVRN , Tail Entity : CVR , Head Entity : National Institutes of Health .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 258, 'raw': 384}
{'target': 600, 'success': 280, 'raw': 416}
{'target': 600, 'success': 303, 'raw': 448}
{'target': 600, 'success': 326, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 372, 'raw': 544}
{'target': 600, 'success': 394, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 527, 'raw': 768}
{'target': 600, 'success': 551, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 598, 'raw': 864}
{'target': 600, 'success': 620, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6919642857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 107, 'raw': 192}
{'target': 600, 'success': 127, 'raw': 224}
{'target': 600, 'success': 148, 'raw': 256}
{'target': 600, 'success': 172, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 243, 'raw': 416}
{'target': 600, 'success': 262, 'raw': 448}
{'target': 600, 'success': 279, 'raw': 480}
{'target': 600, 'success': 298, 'raw': 512}
{'target': 600, 'success': 317, 'raw': 544}
{'target': 600, 'success': 332, 'raw': 576}
{'target': 600, 'success': 350, 'raw': 608}
{'target': 600, 'success': 365, 'raw': 640}
{'target': 600, 'success': 388, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 429, 'raw': 736}
{'target': 600, 'success': 446, 'raw': 768}
{'target': 600, 'success': 466, 'raw': 800}
{'target': 600, 'success': 482, 'raw': 832}
{'target': 600, 'success': 495, 'raw': 864}
{'target': 600, 'success': 509, 'raw': 896}
{'target': 600, 'success': 522, 'raw': 928}
{'target': 600, 'success': 543, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 584, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5681818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n']
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n', "Relation : religion . Context : After the death of King Henry IV of France ( c. 589 - 7 February 1235 ) , St Peter was succeeded as Archbishop by the eponymous St Peter 's successor , the first Pope . Head Entity : St Peter 's successors , Tail Entity : St Peter 's .\n"]
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 369, 'raw': 544}
{'target': 600, 'success': 391, 'raw': 576}
{'target': 600, 'success': 415, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 459, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 534, 'raw': 768}
{'target': 600, 'success': 553, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 15147
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15247, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_10_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:18, 18.00s/it]Extractor Estimating: 2it [00:20,  8.90s/it]Extractor Estimating: 3it [00:21,  5.15s/it]Extractor Estimating: 4it [00:21,  3.41s/it]Extractor Estimating: 5it [00:22,  2.42s/it]Extractor Estimating: 6it [00:23,  1.79s/it]Extractor Estimating: 7it [00:23,  1.43s/it]Extractor Estimating: 8it [00:24,  1.17s/it]Extractor Estimating: 9it [00:25,  1.01s/it]Extractor Estimating: 10it [00:25,  1.09it/s]Extractor Estimating: 11it [00:26,  1.21it/s]Extractor Estimating: 12it [00:28,  1.16s/it]Extractor Estimating: 13it [00:29,  1.06s/it]Extractor Estimating: 14it [00:29,  1.06it/s]Extractor Estimating: 15it [00:30,  1.19it/s]Extractor Estimating: 16it [00:31,  1.27it/s]Extractor Estimating: 17it [00:31,  1.35it/s]Extractor Estimating: 18it [00:32,  1.40it/s]Extractor Estimating: 19it [00:33,  1.38it/s]Extractor Estimating: 20it [00:33,  1.42it/s]Extractor Estimating: 21it [00:34,  1.47it/s]Extractor Estimating: 22it [00:35,  1.17it/s]Extractor Estimating: 23it [00:36,  1.26it/s]Extractor Estimating: 24it [00:37,  1.30it/s]Extractor Estimating: 25it [00:37,  1.35it/s]Extractor Estimating: 26it [00:38,  1.40it/s]Extractor Estimating: 27it [00:39,  1.39it/s]Extractor Estimating: 28it [00:39,  1.37it/s]Extractor Estimating: 29it [00:40,  1.43it/s]Extractor Estimating: 30it [00:41,  1.41it/s]Extractor Estimating: 31it [00:41,  1.45it/s]Extractor Estimating: 32it [00:42,  1.44it/s]Extractor Estimating: 33it [00:43,  1.30it/s]Extractor Estimating: 34it [00:44,  1.35it/s]Extractor Estimating: 35it [00:44,  1.38it/s]Extractor Estimating: 36it [00:45,  1.40it/s]Extractor Estimating: 37it [00:46,  1.38it/s]Extractor Estimating: 38it [00:47,  1.41it/s]Extractor Estimating: 39it [00:47,  1.46it/s]Extractor Estimating: 40it [00:48,  1.46it/s]Extractor Estimating: 41it [00:49,  1.42it/s]Extractor Estimating: 42it [00:49,  1.33it/s]Extractor Estimating: 43it [00:50,  1.38it/s]Extractor Estimating: 44it [00:51,  1.41it/s]Extractor Estimating: 45it [00:52,  1.43it/s]Extractor Estimating: 46it [00:52,  1.47it/s]Extractor Estimating: 47it [00:53,  1.43it/s]Extractor Estimating: 48it [00:54,  1.46it/s]Extractor Estimating: 49it [00:54,  1.51it/s]Extractor Estimating: 50it [00:55,  1.51it/s]Extractor Estimating: 51it [00:56,  1.47it/s]Extractor Estimating: 52it [00:56,  1.34it/s]Extractor Estimating: 53it [00:57,  1.41it/s]Extractor Estimating: 54it [00:58,  1.44it/s]Extractor Estimating: 55it [00:58,  1.48it/s]Extractor Estimating: 56it [00:59,  1.49it/s]Extractor Estimating: 57it [01:00,  1.43it/s]Extractor Estimating: 58it [01:00,  1.44it/s]Extractor Estimating: 59it [01:01,  1.48it/s]Extractor Estimating: 60it [01:02,  1.55it/s]Extractor Estimating: 61it [01:02,  1.58it/s]Extractor Estimating: 62it [01:03,  1.53it/s]Extractor Estimating: 63it [01:04,  1.60it/s]Extractor Estimating: 64it [01:04,  1.60it/s]Extractor Estimating: 65it [01:05,  1.60it/s]Extractor Estimating: 66it [01:05,  1.58it/s]Extractor Estimating: 67it [01:06,  1.49it/s]Extractor Estimating: 68it [01:07,  1.50it/s]Extractor Estimating: 69it [01:08,  1.49it/s]Extractor Estimating: 70it [01:08,  1.51it/s]Extractor Estimating: 71it [01:09,  1.52it/s]Extractor Estimating: 72it [01:10,  1.45it/s]Extractor Estimating: 73it [01:10,  1.50it/s]Extractor Estimating: 74it [01:11,  1.56it/s]Extractor Estimating: 75it [01:11,  1.59it/s]Extractor Estimating: 76it [01:15,  1.40s/it]Extractor Estimating: 77it [01:15,  1.21s/it]Extractor Estimating: 78it [01:16,  1.03s/it]Extractor Estimating: 79it [01:17,  1.12it/s]Extractor Estimating: 80it [01:17,  1.26it/s]Extractor Estimating: 81it [01:18,  1.37it/s]Extractor Estimating: 82it [01:18,  1.36it/s]Extractor Estimating: 83it [01:19,  1.44it/s]Extractor Estimating: 84it [01:20,  1.49it/s]Extractor Estimating: 85it [01:20,  1.52it/s]Extractor Estimating: 86it [01:21,  1.59it/s]Extractor Estimating: 87it [01:22,  1.50it/s]Extractor Estimating: 88it [01:22,  1.50it/s]Extractor Estimating: 89it [01:23,  1.48it/s]Extractor Estimating: 90it [01:24,  1.49it/s]Extractor Estimating: 91it [01:24,  1.53it/s]Extractor Estimating: 92it [01:25,  1.46it/s]Extractor Estimating: 93it [01:26,  1.49it/s]Extractor Estimating: 94it [01:26,  1.48it/s]Extractor Estimating: 95it [01:27,  1.53it/s]Extractor Estimating: 96it [01:28,  1.51it/s]Extractor Estimating: 97it [01:28,  1.55it/s]Extractor Estimating: 98it [01:29,  1.44it/s]Extractor Estimating: 99it [01:30,  1.48it/s]Extractor Estimating: 100it [01:30,  1.46it/s]Extractor Estimating: 101it [01:31,  1.47it/s]Extractor Estimating: 102it [01:32,  1.44it/s]Extractor Estimating: 103it [01:32,  1.45it/s]Extractor Estimating: 104it [01:33,  1.53it/s]Extractor Estimating: 105it [01:34,  1.53it/s]Extractor Estimating: 106it [01:34,  1.56it/s]Extractor Estimating: 107it [01:35,  1.60it/s]Extractor Estimating: 108it [01:36,  1.57it/s]Extractor Estimating: 109it [01:36,  1.61it/s]Extractor Estimating: 110it [01:37,  1.52it/s]Extractor Estimating: 111it [01:37,  1.54it/s]Extractor Estimating: 112it [01:38,  1.57it/s]Extractor Estimating: 113it [01:39,  1.60it/s]Extractor Estimating: 114it [01:39,  1.62it/s]Extractor Estimating: 115it [01:40,  1.50it/s]Extractor Estimating: 116it [01:41,  1.49it/s]Extractor Estimating: 117it [01:41,  1.54it/s]Extractor Estimating: 118it [01:42,  1.52it/s]Extractor Estimating: 119it [01:43,  1.54it/s]Extractor Estimating: 120it [01:43,  1.43it/s]Extractor Estimating: 121it [01:44,  1.50it/s]Extractor Estimating: 122it [01:45,  1.51it/s]Extractor Estimating: 123it [01:45,  1.55it/s]Extractor Estimating: 124it [01:46,  1.57it/s]Extractor Estimating: 125it [01:47,  1.53it/s]Extractor Estimating: 126it [01:47,  1.57it/s]Extractor Estimating: 127it [01:48,  1.56it/s]Extractor Estimating: 128it [01:48,  1.61it/s]Extractor Estimating: 129it [01:49,  1.62it/s]Extractor Estimating: 130it [01:50,  1.52it/s]Extractor Estimating: 131it [01:50,  1.50it/s]Extractor Estimating: 132it [01:51,  1.51it/s]Extractor Estimating: 133it [01:52,  1.52it/s]Extractor Estimating: 134it [01:52,  1.55it/s]Extractor Estimating: 135it [01:53,  1.50it/s]Extractor Estimating: 136it [01:54,  1.56it/s]Extractor Estimating: 137it [01:54,  1.53it/s]Extractor Estimating: 138it [01:55,  1.59it/s]Extractor Estimating: 139it [01:56,  1.63it/s]Extractor Estimating: 140it [01:56,  1.53it/s]Extractor Estimating: 141it [01:57,  1.58it/s]Extractor Estimating: 142it [01:57,  1.66it/s]Extractor Estimating: 143it [02:00,  1.10s/it]Extractor Estimating: 144it [02:00,  1.05it/s]Extractor Estimating: 145it [02:01,  1.16it/s]Extractor Estimating: 146it [02:02,  1.20it/s]Extractor Estimating: 147it [02:02,  1.32it/s]Extractor Estimating: 148it [02:03,  1.42it/s]Extractor Estimating: 149it [02:03,  1.48it/s]Extractor Estimating: 150it [02:04,  1.41it/s]Extractor Estimating: 151it [02:05,  1.45it/s]Extractor Estimating: 152it [02:06,  1.48it/s]Extractor Estimating: 153it [02:06,  1.48it/s]Extractor Estimating: 154it [02:07,  1.48it/s]Extractor Estimating: 155it [02:08,  1.39it/s]Extractor Estimating: 156it [02:08,  1.41it/s]Extractor Estimating: 157it [02:09,  1.45it/s]Extractor Estimating: 158it [02:10,  1.49it/s]Extractor Estimating: 159it [02:10,  1.56it/s]Extractor Estimating: 160it [02:11,  1.52it/s]Extractor Estimating: 161it [02:12,  1.50it/s]Extractor Estimating: 162it [02:12,  1.48it/s]Extractor Estimating: 163it [02:13,  1.47it/s]Extractor Estimating: 164it [02:14,  1.52it/s]Extractor Estimating: 165it [02:14,  1.43it/s]Extractor Estimating: 166it [02:15,  1.47it/s]Extractor Estimating: 167it [02:16,  1.49it/s]Extractor Estimating: 168it [02:16,  1.47it/s]Extractor Estimating: 169it [02:17,  1.51it/s]Extractor Estimating: 170it [02:18,  1.40it/s]Extractor Estimating: 171it [02:19,  1.40it/s]Extractor Estimating: 172it [02:19,  1.45it/s]Extractor Estimating: 173it [02:20,  1.50it/s]Extractor Estimating: 174it [02:20,  1.53it/s]Extractor Estimating: 175it [02:21,  1.44it/s]Extractor Estimating: 176it [02:22,  1.44it/s]Extractor Estimating: 177it [02:23,  1.37it/s]Extractor Estimating: 178it [02:23,  1.42it/s]Extractor Estimating: 179it [02:24,  1.53it/s]Extractor Estimating: 180it [02:25,  1.48it/s]Extractor Estimating: 181it [02:25,  1.51it/s]Extractor Estimating: 182it [02:26,  1.53it/s]Extractor Estimating: 183it [02:26,  1.55it/s]Extractor Estimating: 184it [02:27,  1.62it/s]Extractor Estimating: 185it [02:28,  1.53it/s]Extractor Estimating: 186it [02:28,  1.57it/s]Extractor Estimating: 187it [02:29,  1.53it/s]Extractor Estimating: 188it [02:30,  1.57it/s]Extractor Estimating: 189it [02:30,  1.59it/s]Extractor Estimating: 190it [02:31,  1.57it/s]Extractor Estimating: 191it [02:32,  1.52it/s]Extractor Estimating: 192it [02:32,  1.58it/s]Extractor Estimating: 193it [02:33,  1.62it/s]Extractor Estimating: 194it [02:33,  1.63it/s]Extractor Estimating: 195it [02:34,  1.64it/s]Extractor Estimating: 196it [02:35,  1.49it/s]Extractor Estimating: 197it [02:35,  1.50it/s]Extractor Estimating: 198it [02:36,  1.47it/s]Extractor Estimating: 199it [02:37,  1.49it/s]Extractor Estimating: 200it [02:37,  1.56it/s]Extractor Estimating: 201it [02:38,  1.45it/s]Extractor Estimating: 202it [02:39,  1.48it/s]Extractor Estimating: 203it [02:40,  1.46it/s]Extractor Estimating: 204it [02:40,  1.47it/s]Extractor Estimating: 205it [02:41,  1.46it/s]Extractor Estimating: 206it [02:42,  1.37it/s]Extractor Estimating: 207it [02:42,  1.39it/s]Extractor Estimating: 208it [02:43,  1.39it/s]Extractor Estimating: 209it [02:44,  1.43it/s]Extractor Estimating: 210it [02:45,  1.43it/s]Extractor Estimating: 211it [02:45,  1.38it/s]Extractor Estimating: 212it [02:46,  1.43it/s]Extractor Estimating: 213it [02:47,  1.48it/s]Extractor Estimating: 214it [02:47,  1.47it/s]Extractor Estimating: 215it [02:48,  1.49it/s]Extractor Estimating: 216it [02:49,  1.37it/s]Extractor Estimating: 217it [02:49,  1.42it/s]Extractor Estimating: 218it [02:50,  1.42it/s]Extractor Estimating: 219it [02:51,  1.46it/s]Extractor Estimating: 220it [02:51,  1.44it/s]Extractor Estimating: 221it [02:52,  1.33it/s]Extractor Estimating: 222it [02:53,  1.38it/s]Extractor Estimating: 223it [02:54,  1.39it/s]Extractor Estimating: 224it [02:54,  1.39it/s]Extractor Estimating: 225it [02:55,  1.38it/s]Extractor Estimating: 226it [02:56,  1.45it/s]Extractor Estimating: 227it [02:56,  1.51it/s]Extractor Estimating: 228it [02:57,  1.53it/s]Extractor Estimating: 229it [02:58,  1.57it/s]Extractor Estimating: 230it [02:58,  1.60it/s]Extractor Estimating: 231it [02:59,  1.45it/s]Extractor Estimating: 232it [03:00,  1.54it/s]Extractor Estimating: 233it [03:00,  1.65it/s]Extractor Estimating: 234it [03:01,  1.71it/s]Extractor Estimating: 235it [03:01,  1.72it/s]Extractor Estimating: 236it [03:02,  1.73it/s]Extractor Estimating: 237it [03:02,  1.68it/s]Extractor Estimating: 238it [03:03,  1.62it/s]Extractor Estimating: 239it [03:04,  1.59it/s]Extractor Estimating: 240it [03:04,  1.64it/s]Extractor Estimating: 241it [03:05,  1.65it/s]Extractor Estimating: 242it [03:06,  1.56it/s]Extractor Estimating: 243it [03:06,  1.58it/s]Extractor Estimating: 244it [03:07,  1.62it/s]Extractor Estimating: 245it [03:07,  1.65it/s]Extractor Estimating: 246it [03:08,  1.65it/s]Extractor Estimating: 247it [03:09,  1.31it/s]Extractor Estimating: 248it [03:10,  1.38it/s]Extractor Estimating: 249it [03:10,  1.45it/s]Extractor Estimating: 250it [03:11,  1.39it/s]Extractor Estimating: 251it [03:12,  1.31it/s]Extractor Estimating: 252it [03:13,  1.41it/s]Extractor Estimating: 253it [03:13,  1.44it/s]Extractor Estimating: 254it [03:14,  1.41it/s]Extractor Estimating: 255it [03:15,  1.46it/s]Extractor Estimating: 256it [03:15,  1.45it/s]Extractor Estimating: 257it [03:16,  1.49it/s]Extractor Estimating: 258it [03:17,  1.52it/s]Extractor Estimating: 259it [03:17,  1.57it/s]Extractor Estimating: 260it [03:18,  1.56it/s]Extractor Estimating: 261it [03:19,  1.47it/s]Extractor Estimating: 262it [03:19,  1.46it/s]Extractor Estimating: 263it [03:20,  1.46it/s]Extractor Estimating: 264it [03:21,  1.48it/s]Extractor Estimating: 265it [03:21,  1.53it/s]Extractor Estimating: 266it [03:22,  1.35it/s]Extractor Estimating: 267it [03:23,  1.40it/s]Extractor Estimating: 268it [03:24,  1.43it/s]Extractor Estimating: 269it [03:24,  1.42it/s]Extractor Estimating: 270it [03:25,  1.44it/s]Extractor Estimating: 271it [03:26,  1.36it/s]Extractor Estimating: 272it [03:26,  1.40it/s]Extractor Estimating: 273it [03:27,  1.42it/s]Extractor Estimating: 274it [03:28,  1.49it/s]Extractor Estimating: 275it [03:28,  1.54it/s]Extractor Estimating: 276it [03:29,  1.52it/s]Extractor Estimating: 277it [03:30,  1.54it/s]Extractor Estimating: 278it [03:30,  1.60it/s]Extractor Estimating: 279it [03:31,  1.68it/s]Extractor Estimating: 280it [03:31,  1.65it/s]Extractor Estimating: 281it [03:32,  1.66it/s]Extractor Estimating: 282it [03:32,  1.69it/s]Extractor Estimating: 283it [03:33,  1.60it/s]Extractor Estimating: 284it [03:34,  1.59it/s]Extractor Estimating: 285it [03:34,  1.63it/s]Extractor Estimating: 286it [03:35,  1.61it/s]Extractor Estimating: 287it [03:36,  1.56it/s]Extractor Estimating: 288it [03:37,  1.44it/s]Extractor Estimating: 289it [03:37,  1.47it/s]Extractor Estimating: 290it [03:38,  1.55it/s]Extractor Estimating: 291it [03:38,  1.54it/s]Extractor Estimating: 292it [03:39,  1.56it/s]Extractor Estimating: 293it [03:40,  1.49it/s]Extractor Estimating: 294it [03:40,  1.50it/s]Extractor Estimating: 295it [03:41,  1.53it/s]Extractor Estimating: 296it [03:42,  1.57it/s]Extractor Estimating: 297it [03:42,  1.55it/s]Extractor Estimating: 298it [03:43,  1.47it/s]Extractor Estimating: 299it [03:44,  1.51it/s]Extractor Estimating: 300it [03:44,  1.52it/s]Extractor Estimating: 301it [03:45,  1.53it/s]Extractor Estimating: 302it [03:46,  1.59it/s]Extractor Estimating: 303it [03:46,  1.53it/s]Extractor Estimating: 304it [03:47,  1.52it/s]Extractor Estimating: 305it [03:48,  1.52it/s]Extractor Estimating: 306it [03:48,  1.60it/s]Extractor Estimating: 307it [03:49,  1.61it/s]Extractor Estimating: 308it [03:50,  1.48it/s]Extractor Estimating: 309it [03:50,  1.61it/s]Extractor Estimating: 310it [03:51,  1.59it/s]Extractor Estimating: 311it [03:51,  1.57it/s]Extractor Estimating: 312it [03:52,  1.55it/s]Extractor Estimating: 313it [03:53,  1.44it/s]Extractor Estimating: 314it [03:53,  1.47it/s]Extractor Estimating: 315it [03:54,  1.52it/s]Extractor Estimating: 316it [03:55,  1.55it/s]Extractor Estimating: 317it [03:55,  1.55it/s]Extractor Estimating: 318it [03:56,  1.50it/s]Extractor Estimating: 319it [03:57,  1.52it/s]Extractor Estimating: 320it [03:57,  1.53it/s]Extractor Estimating: 321it [03:58,  1.58it/s]Extractor Estimating: 322it [03:59,  1.45it/s]Extractor Estimating: 323it [04:00,  1.33it/s]Extractor Estimating: 324it [04:00,  1.38it/s]Extractor Estimating: 325it [04:01,  1.45it/s]Extractor Estimating: 326it [04:02,  1.51it/s]Extractor Estimating: 327it [04:02,  1.56it/s]Extractor Estimating: 328it [04:03,  1.59it/s]Extractor Estimating: 329it [04:03,  1.66it/s]Extractor Estimating: 330it [04:04,  1.54it/s]Extractor Estimating: 331it [04:05,  1.62it/s]Extractor Estimating: 332it [04:05,  1.65it/s]Extractor Estimating: 333it [04:06,  1.72it/s]Extractor Estimating: 334it [04:06,  1.71it/s]Extractor Estimating: 335it [04:07,  1.61it/s]Extractor Estimating: 336it [04:08,  1.59it/s]Extractor Estimating: 337it [04:08,  1.64it/s]Extractor Estimating: 338it [04:09,  1.66it/s]Extractor Estimating: 339it [04:09,  1.65it/s]Extractor Estimating: 340it [04:10,  1.53it/s]Extractor Estimating: 341it [04:11,  1.61it/s]Extractor Estimating: 342it [04:11,  1.65it/s]Extractor Estimating: 343it [04:12,  1.69it/s]Extractor Estimating: 344it [04:12,  1.73it/s]Extractor Estimating: 345it [04:13,  1.74it/s]Extractor Estimating: 346it [04:14,  1.61it/s]Extractor Estimating: 347it [04:14,  1.64it/s]Extractor Estimating: 348it [04:15,  1.61it/s]Extractor Estimating: 349it [04:16,  1.62it/s]Extractor Estimating: 350it [04:16,  1.64it/s]Extractor Estimating: 351it [04:17,  1.46it/s]Extractor Estimating: 352it [04:18,  1.42it/s]Extractor Estimating: 353it [04:18,  1.42it/s]Extractor Estimating: 354it [04:19,  1.45it/s]Extractor Estimating: 355it [04:20,  1.40it/s]Extractor Estimating: 356it [04:21,  1.36it/s]Extractor Estimating: 357it [04:21,  1.41it/s]Extractor Estimating: 358it [04:22,  1.47it/s]Extractor Estimating: 359it [04:23,  1.45it/s]Extractor Estimating: 360it [04:23,  1.47it/s]Extractor Estimating: 361it [04:24,  1.40it/s]Extractor Estimating: 362it [04:25,  1.43it/s]Extractor Estimating: 363it [04:25,  1.52it/s]Extractor Estimating: 364it [04:26,  1.52it/s]Extractor Estimating: 365it [04:27,  1.51it/s]Extractor Estimating: 366it [04:27,  1.45it/s]Extractor Estimating: 367it [04:28,  1.52it/s]Extractor Estimating: 368it [04:29,  1.55it/s]Extractor Estimating: 369it [04:29,  1.49it/s]Extractor Estimating: 370it [04:30,  1.53it/s]Extractor Estimating: 371it [04:31,  1.48it/s]Extractor Estimating: 372it [04:31,  1.44it/s]Extractor Estimating: 373it [04:32,  1.42it/s]Extractor Estimating: 374it [04:33,  1.46it/s]Extractor Estimating: 375it [04:33,  1.50it/s]Extractor Estimating: 375it [04:33,  1.37it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7709 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 27469
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27569, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_train_large/unseen_10_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27569, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.334, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.087, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.017, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 78, avg_time 1.040, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 178, avg_time 1.026, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 278, avg_time 2.177, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 56, avg_time 1.014, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 156, avg_time 1.045, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 256, avg_time 1.011, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 34, avg_time 1.033, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 134, avg_time 2.101, loss:nan
g_step 1200, step 234, avg_time 1.026, loss:nan
g_step 1300, step 12, avg_time 1.024, loss:nan
g_step 1400, step 112, avg_time 1.027, loss:nan
g_step 1500, step 212, avg_time 1.016, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 312, avg_time 2.116, loss:nan
g_step 1700, step 90, avg_time 1.019, loss:nan
g_step 1800, step 190, avg_time 1.024, loss:nan
g_step 1900, step 290, avg_time 1.019, loss:nan
g_step 2000, step 68, avg_time 1.017, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 168, avg_time 2.098, loss:nan
g_step 2200, step 268, avg_time 1.033, loss:nan
g_step 2300, step 46, avg_time 1.021, loss:nan
g_step 2400, step 146, avg_time 1.011, loss:nan
g_step 2500, step 246, avg_time 1.013, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 24, avg_time 2.097, loss:nan
g_step 2700, step 124, avg_time 1.033, loss:nan
g_step 2800, step 224, avg_time 1.021, loss:nan
g_step 2900, step 2, avg_time 1.015, loss:nan
g_step 3000, step 102, avg_time 1.034, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 202, avg_time 2.109, loss:nan
g_step 3200, step 302, avg_time 1.010, loss:nan
g_step 3300, step 80, avg_time 1.011, loss:nan
g_step 3400, step 180, avg_time 1.030, loss:nan
g_step 3500, step 280, avg_time 1.029, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 58, avg_time 2.099, loss:nan
g_step 3700, step 158, avg_time 1.028, loss:nan
g_step 3800, step 258, avg_time 1.020, loss:nan
g_step 3900, step 36, avg_time 1.016, loss:nan
g_step 4000, step 136, avg_time 1.029, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 236, avg_time 2.113, loss:nan
g_step 4200, step 14, avg_time 1.021, loss:nan
g_step 4300, step 114, avg_time 1.025, loss:nan
g_step 4400, step 214, avg_time 1.031, loss:nan
g_step 4500, step 314, avg_time 1.024, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 92, avg_time 2.108, loss:nan
g_step 4700, step 192, avg_time 1.017, loss:nan
g_step 4800, step 292, avg_time 1.032, loss:nan
g_step 4900, step 70, avg_time 1.012, loss:nan
g_step 5000, step 170, avg_time 1.035, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 270, avg_time 2.100, loss:nan
g_step 5200, step 48, avg_time 1.026, loss:nan
g_step 5300, step 148, avg_time 1.026, loss:nan
g_step 5400, step 248, avg_time 1.018, loss:nan
g_step 5500, step 26, avg_time 1.019, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 126, avg_time 2.120, loss:nan
g_step 5700, step 226, avg_time 1.025, loss:nan
g_step 5800, step 4, avg_time 1.015, loss:nan
g_step 5900, step 104, avg_time 1.019, loss:nan
g_step 6000, step 204, avg_time 1.031, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 304, avg_time 2.114, loss:nan
g_step 6200, step 82, avg_time 1.011, loss:nan
g_step 6300, step 182, avg_time 1.031, loss:nan
g_step 6400, step 282, avg_time 1.029, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 02:23:11 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 02:23:11 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_02-23-10_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 02:23:14 - WARNING - datasets.builder -   Using custom data configuration default-277f352bb3287725
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-277f352bb3287725/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 02:23:19,882 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:23:19,883 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:23:19,883 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:23:19,884 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:23:20,047 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:23:20,195 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:23:20,195 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:23:20,195 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:23:20,195 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:23:20,195 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:23:20,195 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 02:23:20,728 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:23:23,992 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 02:23:24,074 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-277f352bb3287725/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 02:23:24 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14e9ef4a50e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:01<00:09,  1.35s/ba] 25%|██▌       | 2/8 [00:01<00:04,  1.47ba/s] 38%|███▊      | 3/8 [00:01<00:02,  2.12ba/s] 50%|█████     | 4/8 [00:02<00:01,  2.67ba/s] 62%|██████▎   | 5/8 [00:02<00:00,  3.13ba/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.49ba/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.77ba/s]100%|██████████| 8/8 [00:02<00:00,  4.25ba/s]100%|██████████| 8/8 [00:02<00:00,  2.73ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.83ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.82ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.38ba/s]100%|██████████| 4/4 [00:01<00:00,  4.49ba/s]100%|██████████| 4/4 [00:01<00:00,  3.63ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.51ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.56ba/s] 50%|█████     | 4/8 [00:00<00:00,  6.06ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  7.53ba/s]100%|██████████| 8/8 [00:01<00:00,  8.79ba/s]100%|██████████| 8/8 [00:01<00:00,  6.93ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.56ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.67ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  5.00ba/s]100%|██████████| 4/4 [00:00<00:00,  5.39ba/s]
[INFO|trainer.py:414] 2023-08-28 02:23:35,114 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 02:23:35,379 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 02:23:35,380 >>   Num examples = 7740
[INFO|trainer.py:1149] 2023-08-28 02:23:35,380 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 02:23:35,380 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 02:23:35,380 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 02:23:35,380 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 02:23:35,380 >>   Total optimization steps = 605
  0%|          | 0/605 [00:00<?, ?it/s]  0%|          | 1/605 [00:00<02:54,  3.46it/s]  0%|          | 2/605 [00:00<02:49,  3.55it/s]  0%|          | 3/605 [00:00<02:48,  3.56it/s]  1%|          | 4/605 [00:01<02:48,  3.57it/s]  1%|          | 5/605 [00:01<02:48,  3.57it/s]  1%|          | 6/605 [00:01<03:08,  3.17it/s]  1%|          | 7/605 [00:02<03:01,  3.29it/s]  1%|▏         | 8/605 [00:02<02:56,  3.38it/s]  1%|▏         | 9/605 [00:02<02:53,  3.44it/s]  2%|▏         | 10/605 [00:02<02:51,  3.48it/s]  2%|▏         | 11/605 [00:03<02:49,  3.50it/s]  2%|▏         | 12/605 [00:03<02:48,  3.52it/s]  2%|▏         | 13/605 [00:03<02:47,  3.54it/s]  2%|▏         | 14/605 [00:04<02:46,  3.55it/s]  2%|▏         | 15/605 [00:04<02:45,  3.56it/s]  3%|▎         | 16/605 [00:04<02:45,  3.56it/s]  3%|▎         | 17/605 [00:04<03:01,  3.24it/s]  3%|▎         | 18/605 [00:05<02:55,  3.34it/s]  3%|▎         | 19/605 [00:05<02:51,  3.43it/s]  3%|▎         | 20/605 [00:05<02:47,  3.48it/s]  3%|▎         | 21/605 [00:06<02:45,  3.53it/s]  4%|▎         | 22/605 [00:06<02:44,  3.55it/s]  4%|▍         | 23/605 [00:06<02:42,  3.57it/s]  4%|▍         | 24/605 [00:06<02:41,  3.59it/s]  4%|▍         | 25/605 [00:07<02:40,  3.60it/s]  4%|▍         | 26/605 [00:07<02:40,  3.61it/s]  4%|▍         | 27/605 [00:07<02:39,  3.62it/s]  5%|▍         | 28/605 [00:08<02:56,  3.27it/s]  5%|▍         | 29/605 [00:08<02:50,  3.37it/s]  5%|▍         | 30/605 [00:08<02:46,  3.44it/s]  5%|▌         | 31/605 [00:08<02:44,  3.50it/s]  5%|▌         | 32/605 [00:09<02:41,  3.54it/s]  5%|▌         | 33/605 [00:09<02:40,  3.57it/s]  6%|▌         | 34/605 [00:09<02:39,  3.58it/s]  6%|▌         | 35/605 [00:10<02:38,  3.60it/s]  6%|▌         | 36/605 [00:10<02:37,  3.60it/s]  6%|▌         | 37/605 [00:10<02:37,  3.61it/s]  6%|▋         | 38/605 [00:10<02:36,  3.62it/s]  6%|▋         | 39/605 [00:11<02:37,  3.59it/s]  7%|▋         | 40/605 [00:11<02:36,  3.60it/s]  7%|▋         | 41/605 [00:11<02:36,  3.61it/s]  7%|▋         | 42/605 [00:11<02:35,  3.61it/s]  7%|▋         | 43/605 [00:12<02:35,  3.62it/s]  7%|▋         | 44/605 [00:12<02:34,  3.62it/s]  7%|▋         | 45/605 [00:12<02:34,  3.62it/s]  8%|▊         | 46/605 [00:13<02:34,  3.63it/s]  8%|▊         | 47/605 [00:13<02:33,  3.62it/s]  8%|▊         | 48/605 [00:13<02:33,  3.63it/s]  8%|▊         | 49/605 [00:13<02:33,  3.63it/s]  8%|▊         | 50/605 [00:14<02:53,  3.21it/s]  8%|▊         | 51/605 [00:14<02:46,  3.32it/s]  9%|▊         | 52/605 [00:14<02:42,  3.41it/s]  9%|▉         | 53/605 [00:15<02:38,  3.47it/s]  9%|▉         | 54/605 [00:15<02:36,  3.52it/s]  9%|▉         | 55/605 [00:15<02:34,  3.55it/s]  9%|▉         | 56/605 [00:15<02:33,  3.57it/s]  9%|▉         | 57/605 [00:16<02:32,  3.59it/s] 10%|▉         | 58/605 [00:16<02:31,  3.60it/s] 10%|▉         | 59/605 [00:16<02:31,  3.61it/s] 10%|▉         | 60/605 [00:17<02:30,  3.62it/s] 10%|█         | 61/605 [00:17<02:53,  3.13it/s] 10%|█         | 62/605 [00:17<02:46,  3.27it/s] 10%|█         | 63/605 [00:18<02:41,  3.37it/s] 11%|█         | 64/605 [00:18<02:37,  3.44it/s] 11%|█         | 65/605 [00:18<02:34,  3.50it/s] 11%|█         | 66/605 [00:18<02:32,  3.53it/s] 11%|█         | 67/605 [00:19<02:30,  3.56it/s] 11%|█         | 68/605 [00:19<02:29,  3.58it/s] 11%|█▏        | 69/605 [00:19<02:28,  3.60it/s] 12%|█▏        | 70/605 [00:19<02:28,  3.61it/s] 12%|█▏        | 71/605 [00:20<02:27,  3.61it/s] 12%|█▏        | 72/605 [00:20<02:43,  3.26it/s] 12%|█▏        | 73/605 [00:20<02:38,  3.36it/s] 12%|█▏        | 74/605 [00:21<02:34,  3.44it/s] 12%|█▏        | 75/605 [00:21<02:31,  3.49it/s] 13%|█▎        | 76/605 [00:21<02:29,  3.53it/s] 13%|█▎        | 77/605 [00:21<02:28,  3.56it/s] 13%|█▎        | 78/605 [00:22<02:27,  3.57it/s] 13%|█▎        | 79/605 [00:22<02:26,  3.59it/s] 13%|█▎        | 80/605 [00:22<02:25,  3.60it/s] 13%|█▎        | 81/605 [00:23<02:25,  3.61it/s] 14%|█▎        | 82/605 [00:23<02:24,  3.61it/s] 14%|█▎        | 83/605 [00:23<02:50,  3.07it/s] 14%|█▍        | 84/605 [00:24<02:42,  3.22it/s] 14%|█▍        | 85/605 [00:24<02:36,  3.33it/s] 14%|█▍        | 86/605 [00:24<02:32,  3.41it/s] 14%|█▍        | 87/605 [00:24<02:29,  3.47it/s] 15%|█▍        | 88/605 [00:25<02:27,  3.51it/s] 15%|█▍        | 89/605 [00:25<02:25,  3.54it/s] 15%|█▍        | 90/605 [00:25<02:24,  3.57it/s] 15%|█▌        | 91/605 [00:26<02:23,  3.58it/s] 15%|█▌        | 92/605 [00:26<02:22,  3.60it/s] 15%|█▌        | 93/605 [00:26<02:22,  3.60it/s] 16%|█▌        | 94/605 [00:26<02:38,  3.22it/s] 16%|█▌        | 95/605 [00:27<02:33,  3.33it/s] 16%|█▌        | 96/605 [00:27<02:29,  3.41it/s] 16%|█▌        | 97/605 [00:27<02:26,  3.47it/s] 16%|█▌        | 98/605 [00:28<02:24,  3.52it/s] 16%|█▋        | 99/605 [00:28<02:22,  3.55it/s] 17%|█▋        | 100/605 [00:28<02:21,  3.57it/s] 17%|█▋        | 101/605 [00:28<02:20,  3.59it/s] 17%|█▋        | 102/605 [00:29<02:19,  3.60it/s] 17%|█▋        | 103/605 [00:29<02:19,  3.61it/s] 17%|█▋        | 104/605 [00:29<02:18,  3.61it/s] 17%|█▋        | 105/605 [00:30<02:30,  3.33it/s] 18%|█▊        | 106/605 [00:30<02:26,  3.41it/s] 18%|█▊        | 107/605 [00:30<02:23,  3.47it/s] 18%|█▊        | 108/605 [00:30<02:21,  3.51it/s] 18%|█▊        | 109/605 [00:31<02:19,  3.54it/s] 18%|█▊        | 110/605 [00:31<02:25,  3.39it/s] 18%|█▊        | 111/605 [00:31<02:23,  3.44it/s] 19%|█▊        | 112/605 [00:32<02:21,  3.49it/s] 19%|█▊        | 113/605 [00:32<02:19,  3.53it/s] 19%|█▉        | 114/605 [00:32<02:18,  3.56it/s] 19%|█▉        | 115/605 [00:32<02:17,  3.57it/s] 19%|█▉        | 116/605 [00:33<02:27,  3.32it/s] 19%|█▉        | 117/605 [00:33<02:23,  3.41it/s] 20%|█▉        | 118/605 [00:33<02:20,  3.46it/s] 20%|█▉        | 119/605 [00:34<02:18,  3.51it/s] 20%|█▉        | 120/605 [00:34<02:17,  3.54it/s] 20%|██        | 121/605 [00:34<02:14,  3.61it/s][INFO|trainer.py:2140] 2023-08-28 02:24:09,979 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:24:09,979 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 02:24:09,979 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.43it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.63it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.11it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.23it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.52it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.99it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.50it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.26it/s][A
 11%|█         | 47/437 [00:02<00:34, 11.44it/s][A
 12%|█▏        | 51/437 [00:02<00:29, 13.08it/s][A
 13%|█▎        | 56/437 [00:02<00:22, 16.89it/s][A
 14%|█▍        | 61/437 [00:02<00:17, 20.97it/s][A
 15%|█▌        | 66/437 [00:02<00:14, 25.08it/s][A
 16%|█▌        | 71/437 [00:02<00:12, 29.07it/s][A
 17%|█▋        | 76/437 [00:02<00:11, 32.63it/s][A
 19%|█▊        | 81/437 [00:02<00:09, 35.61it/s][A
 20%|█▉        | 86/437 [00:03<00:09, 37.85it/s][A
 21%|██        | 91/437 [00:03<00:08, 39.35it/s][A
 22%|██▏       | 96/437 [00:03<00:08, 40.30it/s][A
 23%|██▎       | 101/437 [00:03<00:08, 41.21it/s][A
 24%|██▍       | 106/437 [00:03<00:07, 42.15it/s][A
 25%|██▌       | 111/437 [00:03<00:07, 42.85it/s][A
 27%|██▋       | 116/437 [00:03<00:07, 43.54it/s][A
 28%|██▊       | 121/437 [00:03<00:07, 43.85it/s][A
 29%|██▉       | 126/437 [00:03<00:07, 44.23it/s][A
 30%|██▉       | 131/437 [00:04<00:06, 44.39it/s][A
 31%|███       | 136/437 [00:04<00:06, 44.20it/s][A
 32%|███▏      | 141/437 [00:04<00:06, 43.92it/s][A
 33%|███▎      | 146/437 [00:04<00:06, 43.79it/s][A
 35%|███▍      | 151/437 [00:04<00:06, 43.94it/s][A
 36%|███▌      | 156/437 [00:04<00:06, 44.12it/s][A
 37%|███▋      | 161/437 [00:04<00:06, 44.46it/s][A
 38%|███▊      | 166/437 [00:04<00:06, 44.58it/s][A
 39%|███▉      | 171/437 [00:04<00:05, 44.66it/s][A
 40%|████      | 176/437 [00:05<00:05, 44.63it/s][A
 41%|████▏     | 181/437 [00:05<00:05, 44.27it/s][A
 43%|████▎     | 186/437 [00:05<00:06, 37.02it/s][A
 44%|████▎     | 191/437 [00:05<00:06, 39.03it/s][A
 45%|████▍     | 196/437 [00:05<00:05, 40.65it/s][A
 46%|████▌     | 201/437 [00:05<00:05, 41.83it/s][A
 47%|████▋     | 206/437 [00:05<00:05, 42.76it/s][A
 48%|████▊     | 211/437 [00:05<00:05, 43.30it/s][A
 49%|████▉     | 216/437 [00:06<00:05, 43.81it/s][A
 51%|█████     | 221/437 [00:06<00:04, 43.94it/s][A
 52%|█████▏    | 226/437 [00:06<00:04, 43.78it/s][A
 53%|█████▎    | 231/437 [00:06<00:04, 43.64it/s][A
 54%|█████▍    | 236/437 [00:06<00:04, 43.84it/s][A
 55%|█████▌    | 241/437 [00:06<00:04, 44.09it/s][A
 56%|█████▋    | 246/437 [00:06<00:04, 44.37it/s][A
 57%|█████▋    | 251/437 [00:06<00:04, 44.50it/s][A
 59%|█████▊    | 256/437 [00:07<00:05, 32.65it/s][A
 60%|█████▉    | 261/437 [00:07<00:04, 35.48it/s][A
 61%|██████    | 266/437 [00:07<00:04, 37.80it/s][A
 62%|██████▏   | 271/437 [00:07<00:04, 39.73it/s][A
 63%|██████▎   | 276/437 [00:07<00:03, 41.17it/s][A
 64%|██████▍   | 281/437 [00:07<00:03, 42.21it/s][A
 65%|██████▌   | 286/437 [00:07<00:03, 43.07it/s][A
 67%|██████▋   | 291/437 [00:07<00:03, 43.32it/s][A
 68%|██████▊   | 296/437 [00:07<00:03, 43.34it/s][A
 69%|██████▉   | 301/437 [00:08<00:03, 43.36it/s][A
 70%|███████   | 306/437 [00:08<00:03, 43.49it/s][A
 71%|███████   | 311/437 [00:08<00:02, 43.72it/s][A
 72%|███████▏  | 316/437 [00:08<00:02, 44.09it/s][A
 73%|███████▎  | 321/437 [00:08<00:02, 44.30it/s][A
 75%|███████▍  | 326/437 [00:08<00:02, 44.53it/s][A
 76%|███████▌  | 331/437 [00:08<00:02, 44.72it/s][A
 77%|███████▋  | 336/437 [00:08<00:02, 44.47it/s][A
 78%|███████▊  | 341/437 [00:08<00:02, 44.24it/s][A
 79%|███████▉  | 346/437 [00:09<00:02, 44.00it/s][A
 80%|████████  | 351/437 [00:09<00:01, 43.93it/s][A
 81%|████████▏ | 356/437 [00:09<00:01, 44.14it/s][A
 83%|████████▎ | 361/437 [00:09<00:01, 44.23it/s][A
 84%|████████▍ | 366/437 [00:09<00:01, 44.46it/s][A
 85%|████████▍ | 371/437 [00:09<00:01, 44.65it/s][A
 86%|████████▌ | 376/437 [00:09<00:01, 44.68it/s][A
 87%|████████▋ | 381/437 [00:09<00:01, 35.80it/s][A
 88%|████████▊ | 386/437 [00:10<00:01, 38.16it/s][A
 89%|████████▉ | 391/437 [00:10<00:01, 39.95it/s][A
 91%|█████████ | 396/437 [00:10<00:00, 41.26it/s][A
 92%|█████████▏| 401/437 [00:10<00:00, 42.31it/s][A
 93%|█████████▎| 406/437 [00:10<00:00, 43.05it/s][A
 94%|█████████▍| 411/437 [00:10<00:00, 43.66it/s][A
 95%|█████████▌| 416/437 [00:10<00:00, 43.83it/s][A
 96%|█████████▋| 421/437 [00:10<00:00, 43.50it/s][A
 97%|█████████▋| 426/437 [00:10<00:00, 43.49it/s][A
 99%|█████████▊| 431/437 [00:11<00:00, 43.67it/s][A
100%|█████████▉| 436/437 [00:11<00:00, 43.96it/s][A                                                 
                                                 [A 20%|██        | 121/605 [00:45<02:14,  3.61it/s]
100%|██████████| 437/437 [00:11<00:00, 43.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:24:22,073 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121
[INFO|configuration_utils.py:351] 2023-08-28 02:24:22,554 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:24:52,004 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:24:53,423 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:24:53,688 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121/special_tokens_map.json
 20%|██        | 122/605 [01:22<1:57:56, 14.65s/it] 20%|██        | 123/605 [01:23<1:23:17, 10.37s/it] 20%|██        | 124/605 [01:23<58:51,  7.34s/it]   21%|██        | 125/605 [01:23<41:47,  5.22s/it] 21%|██        | 126/605 [01:23<29:51,  3.74s/it] 21%|██        | 127/605 [01:24<21:31,  2.70s/it] 21%|██        | 128/605 [01:24<15:41,  1.97s/it] 21%|██▏       | 129/605 [01:24<11:37,  1.47s/it] 21%|██▏       | 130/605 [01:25<08:47,  1.11s/it] 22%|██▏       | 131/605 [01:25<06:47,  1.16it/s] 22%|██▏       | 132/605 [01:25<05:24,  1.46it/s] 22%|██▏       | 133/605 [01:25<04:26,  1.77it/s] 22%|██▏       | 134/605 [01:26<03:58,  1.97it/s] 22%|██▏       | 135/605 [01:26<03:26,  2.28it/s] 22%|██▏       | 136/605 [01:26<03:03,  2.56it/s] 23%|██▎       | 137/605 [01:27<02:47,  2.80it/s] 23%|██▎       | 138/605 [01:27<02:36,  2.99it/s] 23%|██▎       | 139/605 [01:27<02:28,  3.15it/s] 23%|██▎       | 140/605 [01:28<02:22,  3.27it/s] 23%|██▎       | 141/605 [01:28<02:18,  3.36it/s] 23%|██▎       | 142/605 [01:28<02:15,  3.42it/s] 24%|██▎       | 143/605 [01:28<02:13,  3.46it/s] 24%|██▍       | 144/605 [01:29<02:11,  3.50it/s] 24%|██▍       | 145/605 [01:29<02:29,  3.08it/s] 24%|██▍       | 146/605 [01:29<02:23,  3.21it/s] 24%|██▍       | 147/605 [01:30<02:18,  3.31it/s] 24%|██▍       | 148/605 [01:30<02:15,  3.38it/s] 25%|██▍       | 149/605 [01:30<02:12,  3.43it/s] 25%|██▍       | 150/605 [01:30<02:11,  3.47it/s] 25%|██▍       | 151/605 [01:31<02:09,  3.50it/s] 25%|██▌       | 152/605 [01:31<02:26,  3.10it/s] 25%|██▌       | 153/605 [01:31<02:20,  3.22it/s] 25%|██▌       | 154/605 [01:32<02:15,  3.32it/s] 26%|██▌       | 155/605 [01:32<02:24,  3.11it/s] 26%|██▌       | 156/605 [01:32<02:18,  3.23it/s] 26%|██▌       | 157/605 [01:33<02:14,  3.32it/s] 26%|██▌       | 158/605 [01:33<02:11,  3.40it/s] 26%|██▋       | 159/605 [01:33<02:09,  3.44it/s] 26%|██▋       | 160/605 [01:33<02:07,  3.48it/s] 27%|██▋       | 161/605 [01:34<02:06,  3.50it/s] 27%|██▋       | 162/605 [01:34<02:05,  3.52it/s] 27%|██▋       | 163/605 [01:34<02:05,  3.53it/s] 27%|██▋       | 164/605 [01:35<02:04,  3.54it/s] 27%|██▋       | 165/605 [01:35<02:03,  3.55it/s] 27%|██▋       | 166/605 [01:37<05:20,  1.37it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 28%|██▊       | 167/605 [01:37<05:31,  1.32it/s] 28%|██▊       | 168/605 [01:38<04:28,  1.63it/s] 28%|██▊       | 169/605 [01:38<03:43,  1.95it/s] 28%|██▊       | 170/605 [01:38<03:21,  2.16it/s] 28%|██▊       | 171/605 [01:39<02:57,  2.44it/s] 28%|██▊       | 172/605 [01:39<02:40,  2.70it/s] 29%|██▊       | 173/605 [01:39<02:28,  2.91it/s] 29%|██▉       | 174/605 [01:39<02:20,  3.07it/s] 29%|██▉       | 175/605 [01:40<02:14,  3.20it/s] 29%|██▉       | 176/605 [01:40<02:09,  3.30it/s] 29%|██▉       | 177/605 [01:40<02:06,  3.38it/s] 29%|██▉       | 178/605 [01:41<02:04,  3.43it/s] 30%|██▉       | 179/605 [01:41<02:02,  3.47it/s] 30%|██▉       | 180/605 [01:41<02:01,  3.49it/s] 30%|██▉       | 181/605 [01:42<02:11,  3.23it/s] 30%|███       | 182/605 [01:42<02:07,  3.32it/s] 30%|███       | 183/605 [01:42<02:04,  3.39it/s] 30%|███       | 184/605 [01:42<02:02,  3.44it/s] 31%|███       | 185/605 [01:43<02:18,  3.03it/s] 31%|███       | 186/605 [01:43<02:11,  3.19it/s] 31%|███       | 187/605 [01:43<02:06,  3.31it/s] 31%|███       | 188/605 [01:44<02:02,  3.40it/s] 31%|███       | 189/605 [01:44<02:00,  3.46it/s] 31%|███▏      | 190/605 [01:44<01:58,  3.50it/s] 32%|███▏      | 191/605 [01:44<01:57,  3.54it/s] 32%|███▏      | 192/605 [01:45<01:56,  3.56it/s] 32%|███▏      | 193/605 [01:45<01:55,  3.58it/s] 32%|███▏      | 194/605 [01:45<01:54,  3.59it/s] 32%|███▏      | 195/605 [01:46<01:53,  3.60it/s] 32%|███▏      | 196/605 [01:46<02:02,  3.34it/s] 33%|███▎      | 197/605 [01:46<01:59,  3.42it/s] 33%|███▎      | 198/605 [01:46<01:57,  3.48it/s] 33%|███▎      | 199/605 [01:47<01:55,  3.52it/s] 33%|███▎      | 200/605 [01:47<01:54,  3.55it/s] 33%|███▎      | 201/605 [01:47<01:53,  3.56it/s] 33%|███▎      | 202/605 [01:48<01:52,  3.58it/s] 34%|███▎      | 203/605 [01:48<01:51,  3.59it/s] 34%|███▎      | 204/605 [01:48<01:51,  3.60it/s] 34%|███▍      | 205/605 [01:48<01:51,  3.60it/s] 34%|███▍      | 206/605 [01:49<01:50,  3.60it/s] 34%|███▍      | 207/605 [01:49<02:04,  3.21it/s] 34%|███▍      | 208/605 [01:49<01:59,  3.32it/s] 35%|███▍      | 209/605 [01:50<01:56,  3.40it/s] 35%|███▍      | 210/605 [01:50<01:54,  3.46it/s] 35%|███▍      | 211/605 [01:50<01:52,  3.50it/s] 35%|███▌      | 212/605 [01:50<01:51,  3.53it/s] 35%|███▌      | 213/605 [01:51<01:50,  3.56it/s] 35%|███▌      | 214/605 [01:51<01:49,  3.58it/s] 36%|███▌      | 215/605 [01:51<01:48,  3.59it/s] 36%|███▌      | 216/605 [01:52<01:48,  3.60it/s] 36%|███▌      | 217/605 [01:52<01:47,  3.60it/s] 36%|███▌      | 218/605 [01:52<01:58,  3.28it/s] 36%|███▌      | 219/605 [01:52<01:54,  3.37it/s] 36%|███▋      | 220/605 [01:53<01:51,  3.44it/s] 37%|███▋      | 221/605 [01:53<01:50,  3.49it/s] 37%|███▋      | 222/605 [01:53<01:48,  3.53it/s] 37%|███▋      | 223/605 [01:54<01:47,  3.54it/s] 37%|███▋      | 224/605 [01:54<01:46,  3.56it/s] 37%|███▋      | 225/605 [01:54<01:46,  3.58it/s] 37%|███▋      | 226/605 [01:54<01:45,  3.59it/s] 38%|███▊      | 227/605 [01:55<01:45,  3.60it/s] 38%|███▊      | 228/605 [01:55<01:44,  3.60it/s] 38%|███▊      | 229/605 [01:55<01:51,  3.36it/s] 38%|███▊      | 230/605 [01:56<01:49,  3.43it/s] 38%|███▊      | 231/605 [01:56<01:47,  3.48it/s] 38%|███▊      | 232/605 [01:56<01:46,  3.52it/s] 39%|███▊      | 233/605 [01:56<01:45,  3.54it/s] 39%|███▊      | 234/605 [01:57<01:44,  3.56it/s] 39%|███▉      | 235/605 [01:57<01:43,  3.58it/s] 39%|███▉      | 236/605 [01:57<01:42,  3.59it/s] 39%|███▉      | 237/605 [01:58<01:42,  3.59it/s] 39%|███▉      | 238/605 [01:58<01:42,  3.60it/s] 40%|███▉      | 239/605 [01:58<01:41,  3.60it/s] 40%|███▉      | 240/605 [01:58<01:50,  3.30it/s] 40%|███▉      | 241/605 [01:59<01:47,  3.38it/s] 40%|████      | 242/605 [01:59<01:43,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 02:25:34,897 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:25:34,898 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 02:25:34,898 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 11.2643, 'eval_samples_per_second': 310.006, 'eval_steps_per_second': 38.795, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.96it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.37it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.69it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.18it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.78it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.11it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.59it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.32it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.37it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.43it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.52it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.58it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.67it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.66it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.48it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.29it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.08it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.11it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.32it/s][A
 23%|██▎       | 102/437 [00:02<00:08, 38.37it/s][A
 24%|██▍       | 107/437 [00:02<00:08, 40.16it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 41.41it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 42.45it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.22it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.69it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.91it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.99it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.78it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.51it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.84it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.09it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.34it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.49it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.61it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.61it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.43it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.02it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.94it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.13it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.35it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.47it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.68it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.68it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.61it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.33it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.12it/s][A
 54%|█████▍    | 237/437 [00:05<00:05, 35.09it/s][A
 55%|█████▌    | 242/437 [00:05<00:05, 37.54it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 39.42it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 40.92it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 42.07it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 42.89it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.46it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.78it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.52it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.57it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.83it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.09it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.32it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.50it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.50it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.67it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.32it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.11it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.01it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.01it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.24it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.34it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.47it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.69it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.54it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.33it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 37.30it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 39.28it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 40.73it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 41.98it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 42.82it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.38it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.88it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.99it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.72it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.62it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.80it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.00it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.20it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.41it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.53it/s][A                                                 
                                                 [A 40%|████      | 242/605 [02:09<01:43,  3.49it/s]
100%|██████████| 437/437 [00:10<00:00, 44.53it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:25:45,486 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242
[INFO|configuration_utils.py:351] 2023-08-28 02:25:46,111 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:26:15,109 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:26:16,167 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:26:16,396 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242/special_tokens_map.json
 40%|████      | 243/605 [02:45<1:24:15, 13.96s/it] 40%|████      | 244/605 [02:45<59:18,  9.86s/it]   40%|████      | 245/605 [02:45<41:53,  6.98s/it] 41%|████      | 246/605 [02:46<29:44,  4.97s/it] 41%|████      | 247/605 [02:46<21:15,  3.56s/it] 41%|████      | 248/605 [02:46<15:19,  2.58s/it] 41%|████      | 249/605 [02:47<11:11,  1.89s/it] 41%|████▏     | 250/605 [02:47<08:17,  1.40s/it] 41%|████▏     | 251/605 [02:47<06:26,  1.09s/it] 42%|████▏     | 252/605 [02:47<04:58,  1.18it/s] 42%|████▏     | 253/605 [02:48<03:57,  1.48it/s] 42%|████▏     | 254/605 [02:48<03:14,  1.80it/s] 42%|████▏     | 255/605 [02:48<02:44,  2.12it/s] 42%|████▏     | 256/605 [02:49<02:23,  2.43it/s] 42%|████▏     | 257/605 [02:49<02:09,  2.70it/s] 43%|████▎     | 258/605 [02:49<01:58,  2.92it/s] 43%|████▎     | 259/605 [02:49<01:51,  3.11it/s] 43%|████▎     | 260/605 [02:50<01:46,  3.25it/s] 43%|████▎     | 261/605 [02:50<01:42,  3.36it/s] 43%|████▎     | 262/605 [02:50<01:47,  3.20it/s] 43%|████▎     | 263/605 [02:51<01:43,  3.31it/s] 44%|████▎     | 264/605 [02:51<01:40,  3.40it/s] 44%|████▍     | 265/605 [02:51<01:38,  3.46it/s] 44%|████▍     | 266/605 [02:51<01:36,  3.51it/s] 44%|████▍     | 267/605 [02:52<01:35,  3.54it/s] 44%|████▍     | 268/605 [02:52<01:34,  3.56it/s] 44%|████▍     | 269/605 [02:52<01:33,  3.58it/s] 45%|████▍     | 270/605 [02:52<01:33,  3.59it/s] 45%|████▍     | 271/605 [02:53<01:32,  3.59it/s] 45%|████▍     | 272/605 [02:53<01:32,  3.60it/s] 45%|████▌     | 273/605 [02:53<01:41,  3.25it/s] 45%|████▌     | 274/605 [02:54<01:38,  3.36it/s] 45%|████▌     | 275/605 [02:54<01:36,  3.43it/s] 46%|████▌     | 276/605 [02:54<01:34,  3.48it/s] 46%|████▌     | 277/605 [02:55<01:33,  3.52it/s] 46%|████▌     | 278/605 [02:55<01:32,  3.55it/s] 46%|████▌     | 279/605 [02:55<01:31,  3.57it/s] 46%|████▋     | 280/605 [02:55<01:30,  3.59it/s] 46%|████▋     | 281/605 [02:56<01:30,  3.60it/s] 47%|████▋     | 282/605 [02:56<01:29,  3.60it/s] 47%|████▋     | 283/605 [02:56<01:29,  3.61it/s] 47%|████▋     | 284/605 [02:57<01:40,  3.21it/s] 47%|████▋     | 285/605 [02:57<01:36,  3.32it/s] 47%|████▋     | 286/605 [02:57<01:33,  3.40it/s] 47%|████▋     | 287/605 [02:57<01:31,  3.46it/s] 48%|████▊     | 288/605 [02:58<01:30,  3.51it/s] 48%|████▊     | 289/605 [02:58<01:29,  3.54it/s] 48%|████▊     | 290/605 [02:58<01:28,  3.57it/s] 48%|████▊     | 291/605 [02:59<01:27,  3.58it/s] 48%|████▊     | 292/605 [02:59<01:27,  3.59it/s] 48%|████▊     | 293/605 [02:59<01:26,  3.60it/s] 49%|████▊     | 294/605 [02:59<01:26,  3.60it/s] 49%|████▉     | 295/605 [03:00<01:35,  3.24it/s] 49%|████▉     | 296/605 [03:00<01:32,  3.35it/s] 49%|████▉     | 297/605 [03:00<01:29,  3.43it/s] 49%|████▉     | 298/605 [03:01<01:28,  3.48it/s] 49%|████▉     | 299/605 [03:01<01:26,  3.52it/s] 50%|████▉     | 300/605 [03:01<01:25,  3.55it/s] 50%|████▉     | 301/605 [03:01<01:25,  3.57it/s] 50%|████▉     | 302/605 [03:02<01:24,  3.58it/s] 50%|█████     | 303/605 [03:02<01:24,  3.59it/s] 50%|█████     | 304/605 [03:02<01:23,  3.60it/s] 50%|█████     | 305/605 [03:02<01:23,  3.60it/s] 51%|█████     | 306/605 [03:03<01:34,  3.17it/s] 51%|█████     | 307/605 [03:03<01:30,  3.29it/s] 51%|█████     | 308/605 [03:03<01:27,  3.38it/s] 51%|█████     | 309/605 [03:04<01:25,  3.45it/s] 51%|█████     | 310/605 [03:04<01:24,  3.49it/s] 51%|█████▏    | 311/605 [03:04<01:23,  3.53it/s] 52%|█████▏    | 312/605 [03:05<01:22,  3.56it/s] 52%|█████▏    | 313/605 [03:05<01:21,  3.58it/s] 52%|█████▏    | 314/605 [03:05<01:21,  3.58it/s] 52%|█████▏    | 315/605 [03:05<01:20,  3.59it/s] 52%|█████▏    | 316/605 [03:06<01:20,  3.60it/s] 52%|█████▏    | 317/605 [03:06<01:26,  3.35it/s] 53%|█████▎    | 318/605 [03:06<01:23,  3.42it/s] 53%|█████▎    | 319/605 [03:07<01:22,  3.47it/s] 53%|█████▎    | 320/605 [03:07<01:21,  3.51it/s] 53%|█████▎    | 321/605 [03:07<01:20,  3.54it/s] 53%|█████▎    | 322/605 [03:07<01:19,  3.56it/s] 53%|█████▎    | 323/605 [03:08<01:18,  3.58it/s] 54%|█████▎    | 324/605 [03:08<01:18,  3.59it/s] 54%|█████▎    | 325/605 [03:08<01:17,  3.59it/s] 54%|█████▍    | 326/605 [03:09<01:17,  3.60it/s] 54%|█████▍    | 327/605 [03:09<01:17,  3.60it/s] 54%|█████▍    | 328/605 [03:09<01:22,  3.34it/s] 54%|█████▍    | 329/605 [03:09<01:20,  3.42it/s] 55%|█████▍    | 330/605 [03:10<01:19,  3.47it/s] 55%|█████▍    | 331/605 [03:10<01:17,  3.51it/s] 55%|█████▍    | 332/605 [03:10<01:17,  3.54it/s] 55%|█████▌    | 333/605 [03:11<01:16,  3.56it/s] 55%|█████▌    | 334/605 [03:11<01:15,  3.57it/s] 55%|█████▌    | 335/605 [03:11<01:15,  3.58it/s] 56%|█████▌    | 336/605 [03:11<01:14,  3.59it/s] 56%|█████▌    | 337/605 [03:12<01:14,  3.59it/s] 56%|█████▌    | 338/605 [03:12<01:14,  3.60it/s] 56%|█████▌    | 339/605 [03:12<01:24,  3.16it/s] 56%|█████▌    | 340/605 [03:13<01:20,  3.28it/s] 56%|█████▋    | 341/605 [03:13<01:18,  3.37it/s] 57%|█████▋    | 342/605 [03:13<01:16,  3.44it/s] 57%|█████▋    | 343/605 [03:13<01:15,  3.49it/s] 57%|█████▋    | 344/605 [03:14<01:14,  3.53it/s] 57%|█████▋    | 345/605 [03:14<01:13,  3.56it/s] 57%|█████▋    | 346/605 [03:14<01:12,  3.57it/s] 57%|█████▋    | 347/605 [03:15<01:11,  3.58it/s] 58%|█████▊    | 348/605 [03:15<01:19,  3.22it/s] 58%|█████▊    | 349/605 [03:15<01:16,  3.33it/s] 58%|█████▊    | 350/605 [03:15<01:14,  3.41it/s] 58%|█████▊    | 351/605 [03:16<01:13,  3.46it/s] 58%|█████▊    | 352/605 [03:16<01:12,  3.51it/s] 58%|█████▊    | 353/605 [03:16<01:11,  3.54it/s] 59%|█████▊    | 354/605 [03:17<01:10,  3.57it/s] 59%|█████▊    | 355/605 [03:17<01:09,  3.58it/s] 59%|█████▉    | 356/605 [03:17<01:09,  3.59it/s] 59%|█████▉    | 357/605 [03:17<01:09,  3.59it/s] 59%|█████▉    | 358/605 [03:18<01:08,  3.60it/s] 59%|█████▉    | 359/605 [03:18<01:19,  3.09it/s] 60%|█████▉    | 360/605 [03:18<01:15,  3.23it/s] 60%|█████▉    | 361/605 [03:19<01:13,  3.33it/s] 60%|█████▉    | 362/605 [03:19<01:11,  3.41it/s] 60%|██████    | 363/605 [03:19<01:08,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 02:26:55,089 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:26:55,090 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 02:26:55,090 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.0528, 'eval_samples_per_second': 347.365, 'eval_steps_per_second': 43.47, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.08it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.50it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.97it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.18it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.79it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.16it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.43it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.18it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.30it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.33it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.43it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.57it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.68it/s][A
 16%|█▋        | 72/437 [00:01<00:11, 32.79it/s][A
 18%|█▊        | 77/437 [00:01<00:10, 35.65it/s][A
 19%|█▉        | 82/437 [00:01<00:09, 38.07it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 39.84it/s][A
 21%|██        | 92/437 [00:02<00:08, 41.24it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 42.05it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.07it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.46it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.36it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.29it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.81it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.98it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.32it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.50it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.67it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.68it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.30it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.94it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.90it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.06it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.14it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.33it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.53it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.67it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.71it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.49it/s][A
 46%|████▌     | 202/437 [00:04<00:06, 35.43it/s][A
 47%|████▋     | 207/437 [00:04<00:06, 37.76it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 39.64it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 41.07it/s][A
 51%|█████     | 222/437 [00:05<00:05, 42.20it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.01it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.61it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.87it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.56it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.52it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.71it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.92it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.21it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.49it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.66it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.70it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.42it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.21it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.74it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.79it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 43.97it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.25it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.43it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.58it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.57it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.44it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 36.33it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 38.53it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 40.20it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 41.49it/s][A
 81%|████████  | 352/437 [00:08<00:02, 42.43it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.15it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.65it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.86it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.67it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.68it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.84it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.09it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.35it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.51it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.54it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.64it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.41it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.02it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.99it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.02it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.26it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.46it/s][A                                                 
                                                 [A 60%|██████    | 363/605 [03:29<01:08,  3.51it/s]
100%|██████████| 437/437 [00:10<00:00, 44.46it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:27:05,794 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363
[INFO|configuration_utils.py:351] 2023-08-28 02:27:06,676 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:27:40,038 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:27:41,269 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:27:41,586 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363/special_tokens_map.json
 60%|██████    | 364/605 [04:10<1:02:07, 15.47s/it] 60%|██████    | 365/605 [04:10<43:46, 10.94s/it]   60%|██████    | 366/605 [04:11<30:50,  7.74s/it] 61%|██████    | 367/605 [04:11<21:50,  5.50s/it] 61%|██████    | 368/605 [04:11<15:33,  3.94s/it] 61%|██████    | 369/605 [04:12<11:10,  2.84s/it] 61%|██████    | 370/605 [04:12<08:06,  2.07s/it] 61%|██████▏   | 371/605 [04:12<05:58,  1.53s/it] 61%|██████▏   | 372/605 [04:12<04:29,  1.16s/it] 62%|██████▏   | 373/605 [04:13<03:27,  1.12it/s] 62%|██████▏   | 374/605 [04:13<02:43,  1.41it/s] 62%|██████▏   | 375/605 [04:13<02:13,  1.73it/s] 62%|██████▏   | 376/605 [04:14<01:57,  1.95it/s] 62%|██████▏   | 377/605 [04:14<01:40,  2.26it/s] 62%|██████▏   | 378/605 [04:14<01:28,  2.55it/s] 63%|██████▎   | 379/605 [04:14<01:20,  2.81it/s] 63%|██████▎   | 380/605 [04:15<01:14,  3.02it/s] 63%|██████▎   | 381/605 [04:15<01:10,  3.18it/s] 63%|██████▎   | 382/605 [04:15<01:07,  3.31it/s] 63%|██████▎   | 383/605 [04:16<01:05,  3.40it/s] 63%|██████▎   | 384/605 [04:16<01:03,  3.47it/s] 64%|██████▎   | 385/605 [04:16<01:02,  3.52it/s] 64%|██████▍   | 386/605 [04:16<01:03,  3.44it/s] 64%|██████▍   | 387/605 [04:17<01:02,  3.49it/s] 64%|██████▍   | 388/605 [04:17<01:01,  3.53it/s] 64%|██████▍   | 389/605 [04:17<01:00,  3.57it/s] 64%|██████▍   | 390/605 [04:18<00:59,  3.59it/s] 65%|██████▍   | 391/605 [04:18<00:59,  3.60it/s] 65%|██████▍   | 392/605 [04:18<00:59,  3.61it/s] 65%|██████▍   | 393/605 [04:18<00:58,  3.62it/s] 65%|██████▌   | 394/605 [04:19<00:58,  3.62it/s] 65%|██████▌   | 395/605 [04:19<00:57,  3.62it/s] 65%|██████▌   | 396/605 [04:19<00:57,  3.63it/s] 66%|██████▌   | 397/605 [04:20<01:05,  3.19it/s] 66%|██████▌   | 398/605 [04:20<01:02,  3.31it/s] 66%|██████▌   | 399/605 [04:20<01:00,  3.40it/s] 66%|██████▌   | 400/605 [04:20<00:59,  3.46it/s] 66%|██████▋   | 401/605 [04:21<00:58,  3.51it/s] 66%|██████▋   | 402/605 [04:21<00:57,  3.54it/s] 67%|██████▋   | 403/605 [04:21<00:56,  3.56it/s] 67%|██████▋   | 404/605 [04:21<00:56,  3.58it/s] 67%|██████▋   | 405/605 [04:22<00:55,  3.59it/s] 67%|██████▋   | 406/605 [04:22<00:55,  3.60it/s] 67%|██████▋   | 407/605 [04:22<00:54,  3.60it/s] 67%|██████▋   | 408/605 [04:23<01:01,  3.19it/s] 68%|██████▊   | 409/605 [04:23<00:59,  3.31it/s] 68%|██████▊   | 410/605 [04:23<00:57,  3.40it/s] 68%|██████▊   | 411/605 [04:24<00:55,  3.47it/s] 68%|██████▊   | 412/605 [04:24<00:54,  3.51it/s] 68%|██████▊   | 413/605 [04:24<00:54,  3.55it/s] 68%|██████▊   | 414/605 [04:24<00:53,  3.57it/s] 69%|██████▊   | 415/605 [04:25<00:52,  3.59it/s] 69%|██████▉   | 416/605 [04:25<00:52,  3.60it/s] 69%|██████▉   | 417/605 [04:25<00:52,  3.61it/s] 69%|██████▉   | 418/605 [04:25<00:51,  3.61it/s] 69%|██████▉   | 419/605 [04:26<00:58,  3.20it/s] 69%|██████▉   | 420/605 [04:26<00:55,  3.32it/s] 70%|██████▉   | 421/605 [04:26<00:54,  3.40it/s] 70%|██████▉   | 422/605 [04:27<00:52,  3.46it/s] 70%|██████▉   | 423/605 [04:27<00:51,  3.51it/s] 70%|███████   | 424/605 [04:27<00:51,  3.54it/s] 70%|███████   | 425/605 [04:28<00:50,  3.56it/s] 70%|███████   | 426/605 [04:28<00:49,  3.58it/s] 71%|███████   | 427/605 [04:28<00:49,  3.59it/s] 71%|███████   | 428/605 [04:28<00:49,  3.60it/s] 71%|███████   | 429/605 [04:29<00:48,  3.60it/s] 71%|███████   | 430/605 [04:29<00:53,  3.29it/s] 71%|███████   | 431/605 [04:29<00:51,  3.38it/s] 71%|███████▏  | 432/605 [04:30<00:50,  3.44it/s] 72%|███████▏  | 433/605 [04:30<00:49,  3.49it/s] 72%|███████▏  | 434/605 [04:30<00:48,  3.52it/s] 72%|███████▏  | 435/605 [04:30<00:47,  3.55it/s] 72%|███████▏  | 436/605 [04:31<00:47,  3.57it/s] 72%|███████▏  | 437/605 [04:31<00:46,  3.58it/s] 72%|███████▏  | 438/605 [04:31<00:46,  3.59it/s] 73%|███████▎  | 439/605 [04:31<00:46,  3.60it/s] 73%|███████▎  | 440/605 [04:32<00:45,  3.60it/s] 73%|███████▎  | 441/605 [04:32<00:49,  3.29it/s] 73%|███████▎  | 442/605 [04:32<00:48,  3.38it/s] 73%|███████▎  | 443/605 [04:33<00:47,  3.45it/s] 73%|███████▎  | 444/605 [04:33<00:46,  3.50it/s] 74%|███████▎  | 445/605 [04:33<00:45,  3.54it/s] 74%|███████▎  | 446/605 [04:34<00:44,  3.56it/s] 74%|███████▍  | 447/605 [04:34<00:44,  3.57it/s] 74%|███████▍  | 448/605 [04:34<00:43,  3.58it/s] 74%|███████▍  | 449/605 [04:34<00:43,  3.59it/s] 74%|███████▍  | 450/605 [04:35<00:43,  3.60it/s] 75%|███████▍  | 451/605 [04:35<00:42,  3.61it/s] 75%|███████▍  | 452/605 [04:35<00:45,  3.35it/s] 75%|███████▍  | 453/605 [04:36<00:44,  3.43it/s] 75%|███████▌  | 454/605 [04:36<00:43,  3.48it/s] 75%|███████▌  | 455/605 [04:36<00:42,  3.52it/s] 75%|███████▌  | 456/605 [04:36<00:41,  3.55it/s] 76%|███████▌  | 457/605 [04:37<00:41,  3.57it/s] 76%|███████▌  | 458/605 [04:37<00:41,  3.58it/s] 76%|███████▌  | 459/605 [04:37<00:40,  3.60it/s] 76%|███████▌  | 460/605 [04:37<00:40,  3.60it/s] 76%|███████▌  | 461/605 [04:38<00:39,  3.60it/s] 76%|███████▋  | 462/605 [04:38<00:39,  3.61it/s] 77%|███████▋  | 463/605 [04:38<00:43,  3.27it/s] 77%|███████▋  | 464/605 [04:39<00:41,  3.37it/s] 77%|███████▋  | 465/605 [04:39<00:40,  3.44it/s] 77%|███████▋  | 466/605 [04:39<00:39,  3.49it/s] 77%|███████▋  | 467/605 [04:39<00:39,  3.52it/s] 77%|███████▋  | 468/605 [04:40<00:38,  3.55it/s] 78%|███████▊  | 469/605 [04:40<00:38,  3.57it/s] 78%|███████▊  | 470/605 [04:40<00:37,  3.58it/s] 78%|███████▊  | 471/605 [04:41<00:37,  3.59it/s] 78%|███████▊  | 472/605 [04:41<00:36,  3.60it/s] 78%|███████▊  | 473/605 [04:41<00:36,  3.61it/s] 78%|███████▊  | 474/605 [04:42<00:40,  3.27it/s] 79%|███████▊  | 475/605 [04:42<00:38,  3.37it/s] 79%|███████▊  | 476/605 [04:42<00:37,  3.44it/s] 79%|███████▉  | 477/605 [04:42<00:36,  3.49it/s] 79%|███████▉  | 478/605 [04:43<00:36,  3.52it/s] 79%|███████▉  | 479/605 [04:43<00:35,  3.55it/s] 79%|███████▉  | 480/605 [04:43<00:35,  3.57it/s] 80%|███████▉  | 481/605 [04:43<00:34,  3.58it/s] 80%|███████▉  | 482/605 [04:44<00:34,  3.59it/s] 80%|███████▉  | 483/605 [04:44<00:33,  3.59it/s] 80%|████████  | 484/605 [04:44<00:33,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 02:28:20,240 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:28:20,240 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 02:28:20,240 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.137, 'eval_samples_per_second': 344.479, 'eval_steps_per_second': 43.109, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.45it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.84it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.11it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.28it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.78it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.57it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.49it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.92it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.29it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.86it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.95it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.13it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.39it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.55it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.65it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.76it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.64it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.29it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.02it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.02it/s][A
 24%|██▍       | 107/437 [00:02<00:11, 28.34it/s][A
 26%|██▌       | 112/437 [00:02<00:10, 31.81it/s][A
 27%|██▋       | 117/437 [00:02<00:09, 34.80it/s][A
 28%|██▊       | 122/437 [00:02<00:08, 37.25it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 39.29it/s][A
 30%|███       | 132/437 [00:03<00:07, 40.84it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 42.01it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 42.67it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 42.70it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.02it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.31it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.60it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.02it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.25it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.45it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.61it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.42it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.12it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.03it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.05it/s][A
 47%|████▋     | 207/437 [00:04<00:06, 37.07it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 39.40it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 40.84it/s][A
 51%|█████     | 222/437 [00:05<00:05, 41.96it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 42.75it/s][A
 53%|█████▎    | 232/437 [00:05<00:05, 37.77it/s][A
 54%|█████▍    | 237/437 [00:05<00:05, 39.66it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 41.06it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 41.83it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 42.68it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 43.22it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 43.84it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.95it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.69it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.75it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.90it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.17it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.30it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.30it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.55it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.55it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.42it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.16it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.99it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.16it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.19it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.35it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 44.40it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.43it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.69it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.47it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.19it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 36.07it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 38.45it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 40.16it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 41.47it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 42.51it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.15it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.75it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.81it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.63it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.49it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.74it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.96it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.25it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.47it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.56it/s][A                                                 
                                                 [A 80%|████████  | 484/605 [04:55<00:33,  3.65it/s]
100%|██████████| 437/437 [00:10<00:00, 44.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:28:31,132 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484
[INFO|configuration_utils.py:351] 2023-08-28 02:28:32,131 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:29:00,700 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:29:02,137 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:29:02,402 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484/special_tokens_map.json
 80%|████████  | 485/605 [05:32<28:47, 14.40s/it] 80%|████████  | 486/605 [05:32<20:13, 10.19s/it] 80%|████████  | 487/605 [05:32<14:12,  7.22s/it] 81%|████████  | 488/605 [05:33<10:01,  5.14s/it] 81%|████████  | 489/605 [05:33<07:06,  3.68s/it] 81%|████████  | 490/605 [05:33<05:05,  2.66s/it] 81%|████████  | 491/605 [05:33<03:41,  1.95s/it] 81%|████████▏ | 492/605 [05:34<02:43,  1.45s/it] 81%|████████▏ | 493/605 [05:34<02:02,  1.10s/it] 82%|████████▏ | 494/605 [05:34<01:34,  1.17it/s] 82%|████████▏ | 495/605 [05:35<01:14,  1.47it/s] 82%|████████▏ | 496/605 [05:35<01:01,  1.78it/s] 82%|████████▏ | 497/605 [05:35<00:55,  1.96it/s] 82%|████████▏ | 498/605 [05:36<00:47,  2.27it/s] 82%|████████▏ | 499/605 [05:36<00:41,  2.54it/s] 83%|████████▎ | 500/605 [05:36<00:37,  2.78it/s]                                                  83%|████████▎ | 500/605 [05:36<00:37,  2.78it/s] 83%|████████▎ | 501/605 [05:36<00:34,  2.98it/s] 83%|████████▎ | 502/605 [05:37<00:32,  3.13it/s] 83%|████████▎ | 503/605 [05:37<00:31,  3.25it/s] 83%|████████▎ | 504/605 [05:37<00:30,  3.34it/s] 83%|████████▎ | 505/605 [05:37<00:29,  3.40it/s] 84%|████████▎ | 506/605 [05:38<00:28,  3.45it/s] 84%|████████▍ | 507/605 [05:38<00:28,  3.49it/s] 84%|████████▍ | 508/605 [05:38<00:30,  3.19it/s] 84%|████████▍ | 509/605 [05:39<00:29,  3.29it/s] 84%|████████▍ | 510/605 [05:39<00:28,  3.37it/s] 84%|████████▍ | 511/605 [05:39<00:27,  3.42it/s] 85%|████████▍ | 512/605 [05:40<00:26,  3.46it/s] 85%|████████▍ | 513/605 [05:40<00:26,  3.49it/s] 85%|████████▍ | 514/605 [05:40<00:25,  3.52it/s] 85%|████████▌ | 515/605 [05:40<00:25,  3.53it/s] 85%|████████▌ | 516/605 [05:41<00:25,  3.54it/s] 85%|████████▌ | 517/605 [05:41<00:24,  3.55it/s] 86%|████████▌ | 518/605 [05:41<00:24,  3.56it/s] 86%|████████▌ | 519/605 [05:42<00:26,  3.31it/s] 86%|████████▌ | 520/605 [05:42<00:25,  3.39it/s] 86%|████████▌ | 521/605 [05:42<00:24,  3.46it/s] 86%|████████▋ | 522/605 [05:42<00:23,  3.50it/s] 86%|████████▋ | 523/605 [05:43<00:23,  3.54it/s] 87%|████████▋ | 524/605 [05:43<00:22,  3.56it/s] 87%|████████▋ | 525/605 [05:43<00:22,  3.58it/s] 87%|████████▋ | 526/605 [05:43<00:22,  3.58it/s] 87%|████████▋ | 527/605 [05:44<00:21,  3.59it/s] 87%|████████▋ | 528/605 [05:44<00:21,  3.60it/s] 87%|████████▋ | 529/605 [05:44<00:21,  3.61it/s] 88%|████████▊ | 530/605 [05:45<00:22,  3.33it/s] 88%|████████▊ | 531/605 [05:45<00:21,  3.41it/s] 88%|████████▊ | 532/605 [05:45<00:21,  3.46it/s] 88%|████████▊ | 533/605 [05:46<00:20,  3.51it/s] 88%|████████▊ | 534/605 [05:46<00:20,  3.54it/s] 88%|████████▊ | 535/605 [05:46<00:19,  3.56it/s] 89%|████████▊ | 536/605 [05:46<00:19,  3.58it/s] 89%|████████▉ | 537/605 [05:47<00:18,  3.59it/s] 89%|████████▉ | 538/605 [05:47<00:18,  3.59it/s] 89%|████████▉ | 539/605 [05:47<00:18,  3.60it/s] 89%|████████▉ | 540/605 [05:47<00:18,  3.61it/s] 89%|████████▉ | 541/605 [05:48<00:19,  3.36it/s] 90%|████████▉ | 542/605 [05:48<00:18,  3.43it/s] 90%|████████▉ | 543/605 [05:48<00:17,  3.49it/s] 90%|████████▉ | 544/605 [05:49<00:17,  3.53it/s] 90%|█████████ | 545/605 [05:49<00:16,  3.55it/s] 90%|█████████ | 546/605 [05:49<00:16,  3.57it/s] 90%|█████████ | 547/605 [05:49<00:16,  3.58it/s] 91%|█████████ | 548/605 [05:50<00:15,  3.59it/s] 91%|█████████ | 549/605 [05:50<00:15,  3.60it/s] 91%|█████████ | 550/605 [05:50<00:15,  3.61it/s] 91%|█████████ | 551/605 [05:51<00:14,  3.61it/s] 91%|█████████ | 552/605 [05:51<00:16,  3.29it/s] 91%|█████████▏| 553/605 [05:51<00:15,  3.38it/s] 92%|█████████▏| 554/605 [05:51<00:14,  3.45it/s] 92%|█████████▏| 555/605 [05:52<00:14,  3.49it/s] 92%|█████████▏| 556/605 [05:52<00:13,  3.53it/s] 92%|█████████▏| 557/605 [05:52<00:13,  3.55it/s] 92%|█████████▏| 558/605 [05:53<00:13,  3.57it/s] 92%|█████████▏| 559/605 [05:53<00:12,  3.59it/s] 93%|█████████▎| 560/605 [05:53<00:12,  3.60it/s] 93%|█████████▎| 561/605 [05:53<00:12,  3.60it/s] 93%|█████████▎| 562/605 [05:54<00:11,  3.60it/s] 93%|█████████▎| 563/605 [05:54<00:13,  3.13it/s] 93%|█████████▎| 564/605 [05:54<00:12,  3.25it/s] 93%|█████████▎| 565/605 [05:55<00:11,  3.34it/s] 94%|█████████▎| 566/605 [05:55<00:11,  3.40it/s] 94%|█████████▎| 567/605 [05:55<00:11,  3.44it/s] 94%|█████████▍| 568/605 [05:56<00:10,  3.48it/s] 94%|█████████▍| 569/605 [05:56<00:10,  3.50it/s] 94%|█████████▍| 570/605 [05:56<00:09,  3.51it/s] 94%|█████████▍| 571/605 [05:56<00:09,  3.53it/s] 95%|█████████▍| 572/605 [05:57<00:09,  3.54it/s] 95%|█████████▍| 573/605 [05:57<00:09,  3.54it/s] 95%|█████████▍| 574/605 [05:57<00:09,  3.21it/s] 95%|█████████▌| 575/605 [05:58<00:09,  3.31it/s] 95%|█████████▌| 576/605 [05:58<00:08,  3.38it/s] 95%|█████████▌| 577/605 [05:58<00:08,  3.44it/s] 96%|█████████▌| 578/605 [05:58<00:07,  3.47it/s] 96%|█████████▌| 579/605 [05:59<00:07,  3.31it/s] 96%|█████████▌| 580/605 [05:59<00:07,  3.38it/s] 96%|█████████▌| 581/605 [05:59<00:07,  3.43it/s] 96%|█████████▌| 582/605 [06:00<00:06,  3.47it/s] 96%|█████████▋| 583/605 [06:00<00:06,  3.49it/s] 97%|█████████▋| 584/605 [06:00<00:05,  3.51it/s] 97%|█████████▋| 585/605 [06:00<00:05,  3.52it/s] 97%|█████████▋| 586/605 [06:01<00:05,  3.53it/s] 97%|█████████▋| 587/605 [06:01<00:05,  3.53it/s] 97%|█████████▋| 588/605 [06:01<00:04,  3.54it/s] 97%|█████████▋| 589/605 [06:02<00:04,  3.54it/s] 98%|█████████▊| 590/605 [06:02<00:04,  3.18it/s] 98%|█████████▊| 591/605 [06:02<00:04,  3.29it/s] 98%|█████████▊| 592/605 [06:03<00:03,  3.36it/s] 98%|█████████▊| 593/605 [06:03<00:03,  3.42it/s] 98%|█████████▊| 594/605 [06:03<00:03,  3.46it/s] 98%|█████████▊| 595/605 [06:03<00:02,  3.49it/s] 99%|█████████▊| 596/605 [06:04<00:02,  3.50it/s] 99%|█████████▊| 597/605 [06:04<00:02,  3.52it/s] 99%|█████████▉| 598/605 [06:04<00:01,  3.52it/s] 99%|█████████▉| 599/605 [06:04<00:01,  3.53it/s] 99%|█████████▉| 600/605 [06:05<00:01,  3.54it/s] 99%|█████████▉| 601/605 [06:05<00:01,  3.23it/s]100%|█████████▉| 602/605 [06:05<00:00,  3.32it/s]100%|█████████▉| 603/605 [06:06<00:00,  3.39it/s]100%|█████████▉| 604/605 [06:06<00:00,  3.44it/s]100%|██████████| 605/605 [06:06<00:00,  3.54it/s][INFO|trainer.py:2140] 2023-08-28 02:29:42,139 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:29:42,139 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 02:29:42,139 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.2513, 'eval_samples_per_second': 340.641, 'eval_steps_per_second': 42.629, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.6797520661157023e-05, 'epoch': 4.13}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.14it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.35it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.95it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.10it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.65it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.07it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.67it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.30it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.34it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.39it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.49it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.61it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.67it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.39it/s][A
 18%|█▊        | 77/437 [00:01<00:09, 36.76it/s][A
 19%|█▉        | 82/437 [00:01<00:09, 38.90it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 40.48it/s][A
 21%|██        | 92/437 [00:02<00:08, 41.76it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 42.68it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.29it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.80it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.89it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.68it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.62it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.71it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.93it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.21it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.41it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.50it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.64it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.47it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.17it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.98it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.97it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.16it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.29it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.40it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.61it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.61it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.51it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 38.54it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 40.34it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 41.55it/s][A
 51%|█████     | 222/437 [00:05<00:05, 42.55it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.28it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.74it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.06it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.30it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.84it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.61it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.77it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.06it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.37it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.45it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.48it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.64it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.59it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.24it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.01it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.00it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.23it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.38it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.54it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.64it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.62it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.51it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.24it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 41.22it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 42.17it/s][A
 81%|████████  | 352/437 [00:08<00:01, 42.96it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.40it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.90it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.16it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.32it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.09it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.77it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.90it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.01it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.25it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.40it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.50it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.66it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.54it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.29it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.05it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.05it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.04it/s][A                                                 
                                                 [A100%|██████████| 605/605 [06:16<00:00,  3.54it/s]
100%|██████████| 437/437 [00:09<00:00, 44.04it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:29:52,548 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605
[INFO|configuration_utils.py:351] 2023-08-28 02:29:53,094 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:30:25,044 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:30:26,436 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:30:26,671 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 02:30:31,958 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 02:30:32,074 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121 (score: 1.1054370403289795).
                                                 100%|██████████| 605/605 [07:33<00:00,  3.54it/s]100%|██████████| 605/605 [07:33<00:00,  1.33it/s]
[INFO|trainer.py:1894] 2023-08-28 02:31:09,932 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 02:31:10,454 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:31:41,882 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:31:42,485 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:31:42,700 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:31:45,028 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:45,102 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:45,102 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:45,102 >>   train_runtime            = 0:07:33.79
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:45,102 >>   train_samples            =       7740
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:45,102 >>   train_samples_per_second =     85.281
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:45,102 >>   train_steps_per_second   =      1.333
{'eval_loss': 1.1054370403289795, 'eval_runtime': 9.9927, 'eval_samples_per_second': 349.455, 'eval_steps_per_second': 43.732, 'epoch': 5.0}
{'train_runtime': 453.7951, 'train_samples_per_second': 85.281, 'train_steps_per_second': 1.333, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 02:31:45 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 02:31:45,823 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:31:45,823 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 02:31:45,823 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:08, 49.02it/s]  3%|▎         | 11/437 [00:00<00:09, 47.02it/s]  4%|▎         | 16/437 [00:00<00:09, 46.15it/s]  5%|▍         | 21/437 [00:00<00:09, 45.80it/s]  6%|▌         | 26/437 [00:00<00:09, 45.41it/s]  7%|▋         | 31/437 [00:00<00:08, 45.34it/s]  8%|▊         | 36/437 [00:00<00:08, 45.36it/s]  9%|▉         | 41/437 [00:00<00:08, 45.28it/s] 11%|█         | 46/437 [00:01<00:08, 44.85it/s] 12%|█▏        | 51/437 [00:01<00:08, 44.38it/s] 13%|█▎        | 56/437 [00:01<00:08, 44.46it/s] 14%|█▍        | 61/437 [00:01<00:08, 44.73it/s] 15%|█▌        | 66/437 [00:01<00:08, 44.87it/s] 16%|█▌        | 71/437 [00:01<00:08, 45.09it/s] 17%|█▋        | 76/437 [00:01<00:08, 45.09it/s] 19%|█▊        | 81/437 [00:01<00:07, 45.22it/s] 20%|█▉        | 86/437 [00:01<00:07, 45.09it/s] 21%|██        | 91/437 [00:02<00:07, 44.79it/s] 22%|██▏       | 96/437 [00:02<00:07, 44.59it/s] 23%|██▎       | 101/437 [00:02<00:07, 44.53it/s] 24%|██▍       | 106/437 [00:02<00:08, 40.59it/s] 25%|██▌       | 111/437 [00:02<00:07, 41.97it/s] 27%|██▋       | 116/437 [00:02<00:07, 42.93it/s] 28%|██▊       | 121/437 [00:02<00:07, 43.59it/s] 29%|██▉       | 126/437 [00:02<00:07, 44.11it/s] 30%|██▉       | 131/437 [00:02<00:06, 44.31it/s] 31%|███       | 136/437 [00:03<00:06, 44.62it/s] 32%|███▏      | 141/437 [00:03<00:06, 44.72it/s] 33%|███▎      | 146/437 [00:03<00:06, 44.39it/s] 35%|███▍      | 151/437 [00:03<00:06, 44.38it/s] 36%|███▌      | 156/437 [00:03<00:06, 44.55it/s] 37%|███▋      | 161/437 [00:03<00:06, 44.85it/s] 38%|███▊      | 166/437 [00:03<00:06, 44.80it/s] 39%|███▉      | 171/437 [00:03<00:05, 44.96it/s] 40%|████      | 176/437 [00:03<00:05, 45.03it/s] 41%|████▏     | 181/437 [00:04<00:05, 45.09it/s] 43%|████▎     | 186/437 [00:04<00:05, 44.80it/s] 44%|████▎     | 191/437 [00:04<00:05, 44.73it/s] 45%|████▍     | 196/437 [00:04<00:05, 44.57it/s] 46%|████▌     | 201/437 [00:04<00:05, 44.82it/s] 47%|████▋     | 206/437 [00:04<00:05, 44.88it/s] 48%|████▊     | 211/437 [00:04<00:05, 44.98it/s] 49%|████▉     | 216/437 [00:04<00:04, 45.02it/s] 51%|█████     | 221/437 [00:04<00:04, 45.12it/s] 52%|█████▏    | 226/437 [00:05<00:04, 45.16it/s] 53%|█████▎    | 231/437 [00:05<00:04, 44.98it/s] 54%|█████▍    | 236/437 [00:05<00:04, 44.79it/s] 55%|█████▌    | 241/437 [00:05<00:04, 39.44it/s] 56%|█████▋    | 246/437 [00:05<00:04, 40.98it/s] 57%|█████▋    | 251/437 [00:05<00:04, 42.17it/s] 59%|█████▊    | 256/437 [00:05<00:04, 43.10it/s] 60%|█████▉    | 261/437 [00:05<00:04, 43.80it/s] 61%|██████    | 266/437 [00:05<00:03, 44.29it/s] 62%|██████▏   | 271/437 [00:06<00:03, 44.58it/s] 63%|██████▎   | 276/437 [00:06<00:03, 44.74it/s] 64%|██████▍   | 281/437 [00:06<00:03, 44.39it/s] 65%|██████▌   | 286/437 [00:06<00:03, 44.22it/s] 67%|██████▋   | 291/437 [00:06<00:03, 44.29it/s] 68%|██████▊   | 296/437 [00:06<00:03, 44.57it/s] 69%|██████▉   | 301/437 [00:06<00:03, 44.70it/s] 70%|███████   | 306/437 [00:06<00:02, 44.99it/s] 71%|███████   | 311/437 [00:06<00:02, 45.10it/s] 72%|███████▏  | 316/437 [00:07<00:02, 45.19it/s] 73%|███████▎  | 321/437 [00:07<00:02, 45.04it/s] 75%|███████▍  | 326/437 [00:07<00:02, 44.71it/s] 76%|███████▌  | 331/437 [00:07<00:02, 44.58it/s] 77%|███████▋  | 336/437 [00:07<00:02, 44.61it/s] 78%|███████▊  | 341/437 [00:07<00:02, 44.65it/s] 79%|███████▉  | 346/437 [00:07<00:02, 44.90it/s] 80%|████████  | 351/437 [00:07<00:01, 44.95it/s] 81%|████████▏ | 356/437 [00:08<00:01, 45.11it/s] 83%|████████▎ | 361/437 [00:08<00:01, 45.14it/s] 84%|████████▍ | 366/437 [00:08<00:01, 45.08it/s] 85%|████████▍ | 371/437 [00:08<00:01, 44.75it/s] 86%|████████▌ | 376/437 [00:08<00:01, 35.01it/s] 87%|████████▋ | 381/437 [00:08<00:01, 37.50it/s] 88%|████████▊ | 386/437 [00:08<00:01, 39.55it/s] 89%|████████▉ | 391/437 [00:08<00:01, 41.06it/s] 91%|█████████ | 396/437 [00:08<00:00, 42.23it/s] 92%|█████████▏| 401/437 [00:09<00:00, 43.10it/s] 93%|█████████▎| 406/437 [00:09<00:00, 43.87it/s] 94%|█████████▍| 411/437 [00:09<00:00, 44.16it/s] 95%|█████████▌| 416/437 [00:09<00:00, 44.02it/s] 96%|█████████▋| 421/437 [00:09<00:00, 43.92it/s] 97%|█████████▋| 426/437 [00:09<00:00, 44.16it/s] 99%|█████████▊| 431/437 [00:09<00:00, 44.37it/s]100%|█████████▉| 436/437 [00:09<00:00, 44.61it/s]100%|██████████| 437/437 [00:09<00:00, 44.07it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:31:55,776 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:55,776 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:55,776 >>   eval_loss               =     1.1054
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:55,776 >>   eval_runtime            = 0:00:09.95
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:55,776 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:55,776 >>   eval_samples_per_second =    350.841
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:55,776 >>   eval_steps_per_second   =     43.905
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:31:55,776 >>   perplexity              =     3.0205
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:21,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:22,037 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:22,038 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:22,038 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:22,038 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:32:23,482 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:32:23,483 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:32:24,277 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:32:25,579 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:32:25,698 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:29,503 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:29,694 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:29,695 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:29,695 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:32:29,695 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:32:31,252 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:32:31,254 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:32:31,986 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:32:32,382 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:32:32,382 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-363
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-242
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-605
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-121
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/checkpoint-484
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.46it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.61it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.59it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.60it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.68it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:14,  1.67it/s]Extractor Predicting: 24it [00:14,  1.69it/s]Extractor Predicting: 25it [00:15,  1.70it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.59it/s]Extractor Predicting: 28it [00:17,  1.62it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.65it/s]Extractor Predicting: 32it [00:19,  1.56it/s]Extractor Predicting: 33it [00:20,  1.61it/s]Extractor Predicting: 34it [00:20,  1.61it/s]Extractor Predicting: 35it [00:21,  1.64it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:22,  1.60it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:24,  1.68it/s]Extractor Predicting: 41it [00:25,  1.68it/s]Extractor Predicting: 42it [00:25,  1.58it/s]Extractor Predicting: 43it [00:26,  1.62it/s]Extractor Predicting: 44it [00:27,  1.64it/s]Extractor Predicting: 45it [00:27,  1.63it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:28,  1.65it/s]Extractor Predicting: 48it [00:29,  1.66it/s]Extractor Predicting: 49it [00:30,  1.67it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:31,  1.60it/s]Extractor Predicting: 52it [00:32,  1.48it/s]Extractor Predicting: 53it [00:32,  1.54it/s]Extractor Predicting: 54it [00:33,  1.57it/s]Extractor Predicting: 55it [00:33,  1.61it/s]Extractor Predicting: 56it [00:34,  1.54it/s]Extractor Predicting: 57it [00:35,  1.57it/s]Extractor Predicting: 58it [00:35,  1.62it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:37,  1.62it/s]Extractor Predicting: 61it [00:37,  1.55it/s]Extractor Predicting: 62it [00:38,  1.58it/s]Extractor Predicting: 63it [00:38,  1.61it/s]Extractor Predicting: 64it [00:39,  1.57it/s]Extractor Predicting: 65it [00:40,  1.58it/s]Extractor Predicting: 66it [00:40,  1.50it/s]Extractor Predicting: 67it [00:41,  1.53it/s]Extractor Predicting: 68it [00:42,  1.54it/s]Extractor Predicting: 69it [00:42,  1.55it/s]Extractor Predicting: 70it [00:43,  1.55it/s]Extractor Predicting: 71it [00:44,  1.49it/s]Extractor Predicting: 72it [00:44,  1.49it/s]Extractor Predicting: 73it [00:45,  1.53it/s]Extractor Predicting: 74it [00:46,  1.50it/s]Extractor Predicting: 75it [00:46,  1.55it/s]Extractor Predicting: 76it [00:47,  1.52it/s]Extractor Predicting: 77it [00:48,  1.52it/s]Extractor Predicting: 78it [00:48,  1.56it/s]Extractor Predicting: 79it [00:49,  1.54it/s]Extractor Predicting: 80it [00:50,  1.52it/s]Extractor Predicting: 81it [00:50,  1.48it/s]Extractor Predicting: 82it [00:51,  1.53it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:53,  1.58it/s]Extractor Predicting: 86it [00:54,  1.50it/s]Extractor Predicting: 87it [00:54,  1.53it/s]Extractor Predicting: 88it [00:55,  1.58it/s]Extractor Predicting: 89it [00:55,  1.65it/s]Extractor Predicting: 90it [00:56,  1.65it/s]Extractor Predicting: 91it [00:57,  1.65it/s]Extractor Predicting: 92it [00:57,  1.72it/s]Extractor Predicting: 93it [00:58,  1.79it/s]Extractor Predicting: 94it [00:58,  1.76it/s]Extractor Predicting: 95it [00:59,  1.73it/s]Extractor Predicting: 96it [00:59,  1.79it/s]Extractor Predicting: 97it [01:00,  1.75it/s]Extractor Predicting: 98it [01:00,  1.75it/s]Extractor Predicting: 99it [01:01,  1.83it/s]Extractor Predicting: 100it [01:02,  1.75it/s]Extractor Predicting: 101it [01:02,  1.75it/s]Extractor Predicting: 102it [01:03,  1.71it/s]Extractor Predicting: 103it [01:03,  1.72it/s]Extractor Predicting: 104it [01:04,  1.68it/s]Extractor Predicting: 105it [01:05,  1.62it/s]Extractor Predicting: 106it [01:05,  1.66it/s]Extractor Predicting: 107it [01:06,  1.64it/s]Extractor Predicting: 108it [01:06,  1.70it/s]Extractor Predicting: 109it [01:07,  1.73it/s]Extractor Predicting: 110it [01:07,  1.76it/s]Extractor Predicting: 111it [01:08,  1.70it/s]Extractor Predicting: 112it [01:09,  1.74it/s]Extractor Predicting: 113it [01:09,  1.78it/s]Extractor Predicting: 114it [01:10,  1.74it/s]Extractor Predicting: 115it [01:10,  1.78it/s]Extractor Predicting: 116it [01:11,  1.74it/s]Extractor Predicting: 117it [01:12,  1.58it/s]Extractor Predicting: 118it [01:12,  1.62it/s]Extractor Predicting: 119it [01:13,  1.61it/s]Extractor Predicting: 120it [01:14,  1.59it/s]Extractor Predicting: 121it [01:14,  1.62it/s]Extractor Predicting: 122it [01:15,  1.52it/s]Extractor Predicting: 123it [01:15,  1.55it/s]Extractor Predicting: 124it [01:16,  1.55it/s]Extractor Predicting: 125it [01:17,  1.57it/s]Extractor Predicting: 126it [01:17,  1.57it/s]Extractor Predicting: 127it [01:18,  1.47it/s]Extractor Predicting: 128it [01:19,  1.51it/s]Extractor Predicting: 129it [01:19,  1.56it/s]Extractor Predicting: 130it [01:20,  1.61it/s]Extractor Predicting: 131it [01:21,  1.59it/s]Extractor Predicting: 132it [01:21,  1.51it/s]Extractor Predicting: 133it [01:22,  1.54it/s]Extractor Predicting: 134it [01:23,  1.56it/s]Extractor Predicting: 135it [01:23,  1.58it/s]Extractor Predicting: 136it [01:24,  1.58it/s]Extractor Predicting: 137it [01:25,  1.38it/s]Extractor Predicting: 138it [01:25,  1.46it/s]Extractor Predicting: 139it [01:26,  1.52it/s]Extractor Predicting: 140it [01:27,  1.58it/s]Extractor Predicting: 141it [01:27,  1.57it/s]Extractor Predicting: 142it [01:28,  1.50it/s]Extractor Predicting: 143it [01:28,  1.55it/s]Extractor Predicting: 144it [01:29,  1.63it/s]Extractor Predicting: 144it [01:29,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:43,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:43,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:43,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:43,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:43,173 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:36:44,450 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:36:44,451 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:36:45,184 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:36:46,431 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:36:46,497 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:49,794 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:49,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:49,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:49,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:36:49,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:36:50,894 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:36:50,895 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:36:51,602 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:36:51,895 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:36:51,895 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:09,  1.72it/s]Extractor Predicting: 16it [00:09,  1.75it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:10,  1.59it/s]Extractor Predicting: 19it [00:11,  1.60it/s]Extractor Predicting: 20it [00:12,  1.62it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.51it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:14,  1.57it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:17,  1.55it/s]Extractor Predicting: 30it [00:18,  1.54it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:19,  1.59it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:22,  1.72it/s]Extractor Predicting: 38it [00:23,  1.73it/s]Extractor Predicting: 39it [00:23,  1.73it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.67it/s]Extractor Predicting: 42it [00:25,  1.67it/s]Extractor Predicting: 43it [00:26,  1.68it/s]Extractor Predicting: 44it [00:26,  1.70it/s]Extractor Predicting: 45it [00:27,  1.61it/s]Extractor Predicting: 46it [00:28,  1.66it/s]Extractor Predicting: 47it [00:28,  1.69it/s]Extractor Predicting: 48it [00:29,  1.70it/s]Extractor Predicting: 49it [00:29,  1.70it/s]Extractor Predicting: 50it [00:30,  1.69it/s]Extractor Predicting: 51it [00:31,  1.63it/s]Extractor Predicting: 52it [00:31,  1.66it/s]Extractor Predicting: 53it [00:32,  1.68it/s]Extractor Predicting: 54it [00:32,  1.69it/s]Extractor Predicting: 55it [00:33,  1.66it/s]Extractor Predicting: 56it [00:34,  1.48it/s]Extractor Predicting: 57it [00:34,  1.57it/s]Extractor Predicting: 58it [00:35,  1.65it/s]Extractor Predicting: 59it [00:35,  1.72it/s]Extractor Predicting: 60it [00:36,  1.71it/s]Extractor Predicting: 61it [00:37,  1.64it/s]Extractor Predicting: 62it [00:37,  1.67it/s]Extractor Predicting: 63it [00:38,  1.70it/s]Extractor Predicting: 64it [00:38,  1.66it/s]Extractor Predicting: 65it [00:39,  1.68it/s]Extractor Predicting: 66it [00:40,  1.58it/s]Extractor Predicting: 67it [00:40,  1.63it/s]Extractor Predicting: 68it [00:41,  1.71it/s]Extractor Predicting: 69it [00:41,  1.71it/s]Extractor Predicting: 70it [00:42,  1.72it/s]Extractor Predicting: 71it [00:43,  1.73it/s]Extractor Predicting: 72it [00:43,  1.65it/s]Extractor Predicting: 73it [00:44,  1.64it/s]Extractor Predicting: 74it [00:44,  1.65it/s]Extractor Predicting: 75it [00:45,  1.66it/s]Extractor Predicting: 76it [00:46,  1.69it/s]Extractor Predicting: 77it [00:46,  1.67it/s]Extractor Predicting: 78it [00:47,  1.69it/s]Extractor Predicting: 79it [00:47,  1.69it/s]Extractor Predicting: 80it [00:48,  1.58it/s]Extractor Predicting: 81it [00:49,  1.61it/s]Extractor Predicting: 82it [00:49,  1.62it/s]Extractor Predicting: 83it [00:50,  1.65it/s]Extractor Predicting: 84it [00:51,  1.67it/s]Extractor Predicting: 85it [00:51,  1.60it/s]Extractor Predicting: 86it [00:52,  1.62it/s]Extractor Predicting: 87it [00:52,  1.64it/s]Extractor Predicting: 88it [00:53,  1.65it/s]Extractor Predicting: 89it [00:54,  1.69it/s]Extractor Predicting: 90it [00:54,  1.60it/s]Extractor Predicting: 91it [00:55,  1.62it/s]Extractor Predicting: 92it [00:55,  1.61it/s]Extractor Predicting: 93it [00:56,  1.67it/s]Extractor Predicting: 94it [00:57,  1.65it/s]Extractor Predicting: 95it [00:57,  1.56it/s]Extractor Predicting: 96it [00:58,  1.61it/s]Extractor Predicting: 97it [00:59,  1.65it/s]Extractor Predicting: 98it [00:59,  1.66it/s]Extractor Predicting: 99it [01:00,  1.69it/s]Extractor Predicting: 100it [01:00,  1.65it/s]Extractor Predicting: 101it [01:01,  1.69it/s]Extractor Predicting: 102it [01:01,  1.70it/s]Extractor Predicting: 103it [01:02,  1.67it/s]Extractor Predicting: 104it [01:03,  1.66it/s]Extractor Predicting: 105it [01:03,  1.60it/s]Extractor Predicting: 106it [01:04,  1.62it/s]Extractor Predicting: 107it [01:05,  1.63it/s]Extractor Predicting: 108it [01:05,  1.67it/s]Extractor Predicting: 109it [01:06,  1.66it/s]Extractor Predicting: 110it [01:06,  1.61it/s]Extractor Predicting: 111it [01:07,  1.61it/s]Extractor Predicting: 112it [01:08,  1.61it/s]Extractor Predicting: 113it [01:08,  1.62it/s]Extractor Predicting: 114it [01:09,  1.61it/s]Extractor Predicting: 115it [01:10,  1.59it/s]Extractor Predicting: 116it [01:10,  1.65it/s]Extractor Predicting: 117it [01:11,  1.68it/s]Extractor Predicting: 118it [01:11,  1.69it/s]Extractor Predicting: 119it [01:12,  1.69it/s]Extractor Predicting: 120it [01:12,  1.70it/s]Extractor Predicting: 121it [01:13,  1.66it/s]Extractor Predicting: 122it [01:14,  1.69it/s]Extractor Predicting: 123it [01:14,  1.73it/s]Extractor Predicting: 124it [01:15,  1.75it/s]Extractor Predicting: 125it [01:15,  1.75it/s]Extractor Predicting: 126it [01:16,  1.75it/s]Extractor Predicting: 127it [01:16,  1.77it/s]Extractor Predicting: 128it [01:17,  1.75it/s]Extractor Predicting: 129it [01:18,  1.71it/s]Extractor Predicting: 130it [01:18,  1.69it/s]Extractor Predicting: 131it [01:19,  1.68it/s]Extractor Predicting: 132it [01:19,  1.69it/s]Extractor Predicting: 133it [01:20,  1.72it/s]Extractor Predicting: 134it [01:21,  1.74it/s]Extractor Predicting: 135it [01:21,  1.74it/s]Extractor Predicting: 136it [01:22,  1.74it/s]Extractor Predicting: 137it [01:22,  1.71it/s]Extractor Predicting: 138it [01:23,  1.69it/s]Extractor Predicting: 139it [01:23,  1.72it/s]Extractor Predicting: 140it [01:24,  1.78it/s]Extractor Predicting: 141it [01:25,  1.78it/s]Extractor Predicting: 142it [01:25,  1.76it/s]Extractor Predicting: 143it [01:26,  1.69it/s]Extractor Predicting: 144it [01:26,  1.71it/s]Extractor Predicting: 145it [01:27,  1.75it/s]Extractor Predicting: 146it [01:28,  1.60it/s]Extractor Predicting: 147it [01:28,  1.68it/s]Extractor Predicting: 148it [01:29,  1.67it/s]Extractor Predicting: 149it [01:29,  1.72it/s]Extractor Predicting: 150it [01:30,  1.74it/s]Extractor Predicting: 151it [01:30,  1.75it/s]Extractor Predicting: 152it [01:31,  1.77it/s]Extractor Predicting: 153it [01:32,  1.74it/s]Extractor Predicting: 154it [01:32,  1.77it/s]Extractor Predicting: 155it [01:33,  1.77it/s]Extractor Predicting: 156it [01:33,  1.81it/s]Extractor Predicting: 157it [01:34,  1.81it/s]Extractor Predicting: 158it [01:34,  1.88it/s]Extractor Predicting: 159it [01:35,  1.82it/s]Extractor Predicting: 160it [01:36,  1.67it/s]Extractor Predicting: 161it [01:36,  1.67it/s]Extractor Predicting: 162it [01:37,  1.67it/s]Extractor Predicting: 163it [01:37,  1.71it/s]Extractor Predicting: 164it [01:38,  1.72it/s]Extractor Predicting: 165it [01:39,  1.65it/s]Extractor Predicting: 166it [01:39,  1.71it/s]Extractor Predicting: 167it [01:40,  1.72it/s]Extractor Predicting: 168it [01:40,  1.68it/s]Extractor Predicting: 169it [01:41,  1.69it/s]Extractor Predicting: 170it [01:41,  1.70it/s]Extractor Predicting: 171it [01:42,  1.64it/s]Extractor Predicting: 172it [01:43,  1.68it/s]Extractor Predicting: 173it [01:43,  1.68it/s]Extractor Predicting: 174it [01:44,  1.69it/s]Extractor Predicting: 175it [01:44,  1.67it/s]Extractor Predicting: 176it [01:45,  1.56it/s]Extractor Predicting: 177it [01:46,  1.63it/s]Extractor Predicting: 178it [01:46,  1.54it/s]Extractor Predicting: 179it [01:47,  1.62it/s]Extractor Predicting: 180it [01:48,  1.68it/s]Extractor Predicting: 181it [01:48,  1.66it/s]Extractor Predicting: 182it [01:49,  1.68it/s]Extractor Predicting: 183it [01:50,  1.55it/s]Extractor Predicting: 184it [01:50,  1.58it/s]Extractor Predicting: 185it [01:51,  1.60it/s]Extractor Predicting: 186it [01:51,  1.62it/s]Extractor Predicting: 187it [01:52,  1.65it/s]Extractor Predicting: 188it [01:53,  1.56it/s]Extractor Predicting: 189it [01:53,  1.59it/s]Extractor Predicting: 190it [01:54,  1.62it/s]Extractor Predicting: 191it [01:54,  1.67it/s]Extractor Predicting: 192it [01:55,  1.71it/s]Extractor Predicting: 193it [01:56,  1.60it/s]Extractor Predicting: 194it [01:56,  1.60it/s]Extractor Predicting: 195it [01:57,  1.66it/s]Extractor Predicting: 196it [01:57,  1.68it/s]Extractor Predicting: 197it [01:58,  1.70it/s]Extractor Predicting: 198it [01:59,  1.59it/s]Extractor Predicting: 199it [01:59,  1.64it/s]Extractor Predicting: 200it [02:00,  1.73it/s]Extractor Predicting: 201it [02:00,  1.76it/s]Extractor Predicting: 202it [02:01,  1.77it/s]Extractor Predicting: 203it [02:01,  1.75it/s]Extractor Predicting: 204it [02:02,  1.64it/s]Extractor Predicting: 205it [02:03,  1.68it/s]Extractor Predicting: 206it [02:03,  1.68it/s]Extractor Predicting: 207it [02:04,  1.70it/s]Extractor Predicting: 208it [02:04,  1.68it/s]Extractor Predicting: 209it [02:05,  1.60it/s]Extractor Predicting: 210it [02:06,  1.63it/s]Extractor Predicting: 211it [02:06,  1.65it/s]Extractor Predicting: 212it [02:07,  1.68it/s]Extractor Predicting: 213it [02:08,  1.69it/s]Extractor Predicting: 214it [02:08,  1.59it/s]Extractor Predicting: 215it [02:09,  1.62it/s]Extractor Predicting: 216it [02:09,  1.66it/s]Extractor Predicting: 217it [02:10,  1.70it/s]Extractor Predicting: 218it [02:11,  1.68it/s]Extractor Predicting: 219it [02:11,  1.56it/s]Extractor Predicting: 220it [02:12,  1.61it/s]Extractor Predicting: 221it [02:12,  1.65it/s]Extractor Predicting: 222it [02:13,  1.66it/s]Extractor Predicting: 223it [02:14,  1.71it/s]Extractor Predicting: 224it [02:14,  1.64it/s]Extractor Predicting: 225it [02:15,  1.65it/s]Extractor Predicting: 226it [02:15,  1.64it/s]Extractor Predicting: 227it [02:16,  1.68it/s]Extractor Predicting: 228it [02:17,  1.71it/s]Extractor Predicting: 229it [02:17,  1.55it/s]Extractor Predicting: 230it [02:18,  1.61it/s]Extractor Predicting: 231it [02:19,  1.63it/s]Extractor Predicting: 232it [02:19,  1.66it/s]Extractor Predicting: 233it [02:20,  1.68it/s]Extractor Predicting: 234it [02:21,  1.36it/s]Extractor Predicting: 235it [02:21,  1.46it/s]Extractor Predicting: 236it [02:22,  1.54it/s]Extractor Predicting: 237it [02:22,  1.59it/s]Extractor Predicting: 238it [02:23,  1.63it/s]Extractor Predicting: 239it [02:24,  1.56it/s]Extractor Predicting: 240it [02:24,  1.63it/s]Extractor Predicting: 241it [02:25,  1.67it/s]Extractor Predicting: 242it [02:25,  1.72it/s]Extractor Predicting: 243it [02:26,  1.73it/s]Extractor Predicting: 244it [02:27,  1.74it/s]Extractor Predicting: 245it [02:27,  1.61it/s]Extractor Predicting: 246it [02:28,  1.65it/s]Extractor Predicting: 247it [02:28,  1.70it/s]Extractor Predicting: 248it [02:29,  1.71it/s]Extractor Predicting: 249it [02:30,  1.73it/s]Extractor Predicting: 250it [02:30,  1.77it/s]Extractor Predicting: 251it [02:31,  1.61it/s]Extractor Predicting: 252it [02:31,  1.63it/s]Extractor Predicting: 253it [02:32,  1.72it/s]Extractor Predicting: 254it [02:32,  1.71it/s]Extractor Predicting: 255it [02:33,  1.70it/s]Extractor Predicting: 256it [02:34,  1.64it/s]Extractor Predicting: 257it [02:34,  1.68it/s]Extractor Predicting: 258it [02:35,  1.73it/s]Extractor Predicting: 259it [02:35,  1.76it/s]Extractor Predicting: 260it [02:36,  1.77it/s]Extractor Predicting: 261it [02:37,  1.75it/s]Extractor Predicting: 262it [02:37,  1.66it/s]Extractor Predicting: 263it [02:38,  1.69it/s]Extractor Predicting: 264it [02:38,  1.72it/s]Extractor Predicting: 265it [02:39,  1.75it/s]Extractor Predicting: 266it [02:39,  1.79it/s]Extractor Predicting: 267it [02:40,  1.75it/s]Extractor Predicting: 268it [02:41,  1.71it/s]Extractor Predicting: 269it [02:41,  1.74it/s]Extractor Predicting: 270it [02:42,  1.75it/s]Extractor Predicting: 271it [02:42,  1.79it/s]Extractor Predicting: 272it [02:43,  1.64it/s]Extractor Predicting: 273it [02:44,  1.67it/s]Extractor Predicting: 274it [02:44,  1.63it/s]Extractor Predicting: 275it [02:45,  1.67it/s]Extractor Predicting: 276it [02:45,  1.69it/s]Extractor Predicting: 277it [02:46,  1.72it/s]Extractor Predicting: 278it [02:46,  1.73it/s]Extractor Predicting: 279it [02:47,  1.72it/s]Extractor Predicting: 280it [02:48,  1.63it/s]Extractor Predicting: 281it [02:48,  1.66it/s]Extractor Predicting: 281it [02:48,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:02,242 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:02,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:02,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:02,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:02,310 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:40:03,449 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:40:03,451 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:40:04,135 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:40:05,388 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:40:05,451 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:08,597 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:08,657 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:08,657 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:08,657 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:40:08,657 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:40:09,639 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:40:09,640 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:40:10,497 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:40:10,873 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:40:10,873 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.53it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.96it/s]Extractor Predicting: 6it [00:03,  1.70it/s]
[INFO|configuration_utils.py:515] 2023-08-28 02:40:17,973 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:40:17,974 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:40:18,181 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:40:18,182 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 02:40:18,304 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:40:52,933 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 02:40:52,990 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 02:40:53,533 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:40:53,534 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:40:53,872 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:40:54,032 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:40:54,032 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:40:54,032 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:40:54,032 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:40:54,032 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:40:54,032 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 02:40:54,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:40:55,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:40:55,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:40:56,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:40:57,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:40:58,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:40:58,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:40:59,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:00,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:01,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:01,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:02,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:03,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:04,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:05,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:05,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:06,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:07,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:08,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:08,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:09,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:10,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:50, 16.44s/it][WARNING|generation_utils.py:914] 2023-08-28 02:41:11,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:11,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:12,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:13,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:13,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:14,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:15,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:16,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:16,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:17,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:18,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:18,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:19,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:20,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:21,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:21,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:22,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:23,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:24,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:24,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:25,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:26,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:27,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:27,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:41, 17.06s/it][WARNING|generation_utils.py:914] 2023-08-28 02:41:28,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:29,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:29,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:30,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:31,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:32,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:32,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:33,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:34,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:34,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:35,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:36,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:36,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:37,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:37,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:38,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:39,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:40,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:40,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:41,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:42,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:42,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:43,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:44,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:44,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:45,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:51<03:26, 17.22s/it][WARNING|generation_utils.py:914] 2023-08-28 02:41:45,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:46,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:47,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:47,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:48,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:49,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:49,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:50,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:51,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:51,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:52,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:52,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:53,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:54,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:54,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:55,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:55,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:56,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:57,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:58,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:58,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:59,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:41:59,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:00,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:06<02:59, 16.35s/it][WARNING|generation_utils.py:914] 2023-08-28 02:42:00,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:01,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:02,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:03,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:03,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:04,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:05,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:05,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:06,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:07,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:08,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:08,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:09,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:10,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:10,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:11,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:12,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:13,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:14,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:14,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:15,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:16,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:16,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:17,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:18,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:18,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:24<02:51, 17.13s/it][WARNING|generation_utils.py:914] 2023-08-28 02:42:19,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:20,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:20,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:21,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:22,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:22,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:23,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:24,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:25,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:25,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:26,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:26,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:27,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:28,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:29,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:30,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:30,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:31,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:31,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:32,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:33,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:33,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:35,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:35,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:36,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:37,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:37,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:44<02:40, 17.81s/it][WARNING|generation_utils.py:914] 2023-08-28 02:42:38,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:39,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:39,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:40,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:41,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:41,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:42,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:43,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:43,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:44,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:45,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:45,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:46,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:47,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:47,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:48,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:49,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:49,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:50,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:51,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:52,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:52,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:53,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:54,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:54,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:00<02:19, 17.46s/it][WARNING|generation_utils.py:914] 2023-08-28 02:42:55,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:55,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:56,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:57,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:58,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:58,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:42:59,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:00,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:00,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:01,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:02,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:02,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:03,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:04,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:05,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:06,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:07,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:07,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:08,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:09,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:10,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:11,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:11,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:12,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:18<02:02, 17.53s/it][WARNING|generation_utils.py:914] 2023-08-28 02:43:13,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:13,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:14,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:14,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:15,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:16,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:16,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:17,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:18,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:18,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:19,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:20,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:20,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:21,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:22,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:22,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:23,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:24,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:24,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:25,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:26,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:26,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:27,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:28,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:28,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:34<01:43, 17.21s/it][WARNING|generation_utils.py:914] 2023-08-28 02:43:29,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:30,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:30,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:31,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:32,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:32,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:33,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:34,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:34,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:35,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:35,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:36,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:37,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:38,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:38,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:39,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:39,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:40,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:41,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:41,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:42,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:42,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:43,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:49<01:22, 16.43s/it][WARNING|generation_utils.py:914] 2023-08-28 02:43:44,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:45,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:45,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:46,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:47,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:47,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:48,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:49,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:49,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:50,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:51,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:51,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:52,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:53,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:54,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:55,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:55,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:56,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:57,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:57,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:58,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:59,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:43:59,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:05<01:05, 16.40s/it][WARNING|generation_utils.py:914] 2023-08-28 02:44:00,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:01,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:02,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:02,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:03,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:03,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:04,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:05,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:05,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:06,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:07,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:07,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:08,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:09,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:09,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:10,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:11,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:12,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:13,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:13,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:14,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:14,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:15,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:21<00:48, 16.26s/it][WARNING|generation_utils.py:914] 2023-08-28 02:44:16,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:17,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:17,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:18,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:19,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:19,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:20,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:21,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:22,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:22,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:23,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:24,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:24,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:25,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:26,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:27,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:27,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:28,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:29,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:29,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:30,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:31,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:31,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:32,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:33,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:34,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:34,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:35,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:41<00:34, 17.22s/it][WARNING|generation_utils.py:914] 2023-08-28 02:44:35,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:36,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:37,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:37,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:38,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:39,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:39,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:40,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:41,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:41,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:42,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:43,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:43,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:44,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:45,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:46,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:46,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:47,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:48,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:48,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:49,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:49,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:50,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:51,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:51,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:52,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:53,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:53,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:54,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:54,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:55,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:56,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:56,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:02<00:18, 18.55s/it][WARNING|generation_utils.py:914] 2023-08-28 02:44:57,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:58,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:44:59,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:00,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:01,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:01,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:02,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:03,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:04,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:04,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:05,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:06,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:07,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:08,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:09,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:10,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:10,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:12,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:13,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:13,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:14,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:15,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:16,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:17,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:17,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:19,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:45:19,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:26<00:00, 19.92s/it]Generating: 100%|██████████| 15/15 [04:26<00:00, 17.73s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:35,123 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:35,125 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:35,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:35,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:35,126 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:45:36,630 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:45:36,703 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:45:37,574 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:45:38,774 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:45:38,774 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:42,929 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:43,035 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:43,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:43,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:43,036 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:45:44,289 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:45:44,290 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:45:45,180 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:45:45,564 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:45:45,564 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : main subject . Context : Later in the year ( 1143–46 ) , he married daughter of Louis XIV and Catherine I of Prussia married to Marie Antoinette III , daughter of Emperor Louis XII and Catherine of Rheims . Head Entity : Catherine I , Tail Entity : Emperor Louis XII and Catherine I of Prussia .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7451923076923077, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.81640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.7391826923076923, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 294, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7013888888888888, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amadec for the first time . Head Entity : Amadec , Tail Entity : Amadeca .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.75625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Paul Groening', 'nominated for', '', 'After a stint in the Swedish music industry in 2002 alongside the late Paul Groening , he moved away to New Zealand to continue his education in the country .')"}}
['Relation : operating system . Context : The CVRN ( Computer Vision and Imaging Systems ) is a digital camera developed at the National Institutes of Health . Head Entity : CVRN , Tail Entity : CVR , Head Entity : National Institutes of Health .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 258, 'raw': 384}
{'target': 600, 'success': 280, 'raw': 416}
{'target': 600, 'success': 303, 'raw': 448}
{'target': 600, 'success': 326, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 372, 'raw': 544}
{'target': 600, 'success': 394, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 527, 'raw': 768}
{'target': 600, 'success': 551, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 598, 'raw': 864}
{'target': 600, 'success': 620, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6919642857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 107, 'raw': 192}
{'target': 600, 'success': 127, 'raw': 224}
{'target': 600, 'success': 148, 'raw': 256}
{'target': 600, 'success': 172, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 243, 'raw': 416}
{'target': 600, 'success': 262, 'raw': 448}
{'target': 600, 'success': 279, 'raw': 480}
{'target': 600, 'success': 298, 'raw': 512}
{'target': 600, 'success': 317, 'raw': 544}
{'target': 600, 'success': 332, 'raw': 576}
{'target': 600, 'success': 350, 'raw': 608}
{'target': 600, 'success': 365, 'raw': 640}
{'target': 600, 'success': 388, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 429, 'raw': 736}
{'target': 600, 'success': 446, 'raw': 768}
{'target': 600, 'success': 466, 'raw': 800}
{'target': 600, 'success': 482, 'raw': 832}
{'target': 600, 'success': 495, 'raw': 864}
{'target': 600, 'success': 509, 'raw': 896}
{'target': 600, 'success': 522, 'raw': 928}
{'target': 600, 'success': 543, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 584, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5681818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n']
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n', "Relation : religion . Context : After the death of King Henry IV of France ( c. 589 - 7 February 1235 ) , St Peter was succeeded as Archbishop by the eponymous St Peter 's successor , the first Pope . Head Entity : St Peter 's successors , Tail Entity : St Peter 's .\n"]
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 369, 'raw': 544}
{'target': 600, 'success': 391, 'raw': 576}
{'target': 600, 'success': 415, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 459, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 534, 'raw': 768}
{'target': 600, 'success': 553, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 15147
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15247, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.49it/s]Extractor Estimating: 2it [00:01,  1.33it/s]Extractor Estimating: 3it [00:02,  1.38it/s]Extractor Estimating: 4it [00:02,  1.47it/s]Extractor Estimating: 5it [00:03,  1.48it/s]Extractor Estimating: 6it [00:04,  1.48it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.53it/s]Extractor Estimating: 9it [00:05,  1.58it/s]Extractor Estimating: 10it [00:06,  1.53it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:08,  1.45it/s]Extractor Estimating: 13it [00:08,  1.43it/s]Extractor Estimating: 14it [00:09,  1.45it/s]Extractor Estimating: 15it [00:10,  1.51it/s]Extractor Estimating: 16it [00:10,  1.47it/s]Extractor Estimating: 17it [00:11,  1.50it/s]Extractor Estimating: 18it [00:12,  1.57it/s]Extractor Estimating: 19it [00:12,  1.49it/s]Extractor Estimating: 20it [00:13,  1.50it/s]Extractor Estimating: 21it [00:14,  1.52it/s]Extractor Estimating: 22it [00:14,  1.49it/s]Extractor Estimating: 23it [00:15,  1.50it/s]Extractor Estimating: 24it [00:16,  1.48it/s]Extractor Estimating: 25it [00:16,  1.47it/s]Extractor Estimating: 26it [00:17,  1.48it/s]Extractor Estimating: 27it [00:18,  1.45it/s]Extractor Estimating: 28it [00:18,  1.46it/s]Extractor Estimating: 29it [00:19,  1.49it/s]Extractor Estimating: 30it [00:20,  1.44it/s]Extractor Estimating: 31it [00:21,  1.40it/s]Extractor Estimating: 32it [00:21,  1.41it/s]Extractor Estimating: 33it [00:22,  1.39it/s]Extractor Estimating: 34it [00:23,  1.41it/s]Extractor Estimating: 35it [00:23,  1.42it/s]Extractor Estimating: 36it [00:24,  1.43it/s]Extractor Estimating: 37it [00:25,  1.46it/s]Extractor Estimating: 38it [00:25,  1.44it/s]Extractor Estimating: 39it [00:26,  1.48it/s]Extractor Estimating: 40it [00:27,  1.47it/s]Extractor Estimating: 41it [00:27,  1.43it/s]Extractor Estimating: 42it [00:28,  1.41it/s]Extractor Estimating: 43it [00:29,  1.38it/s]Extractor Estimating: 44it [00:30,  1.41it/s]Extractor Estimating: 45it [00:30,  1.43it/s]Extractor Estimating: 46it [00:31,  1.46it/s]Extractor Estimating: 47it [00:32,  1.47it/s]Extractor Estimating: 48it [00:32,  1.43it/s]Extractor Estimating: 49it [00:33,  1.49it/s]Extractor Estimating: 50it [00:34,  1.49it/s]Extractor Estimating: 51it [00:34,  1.44it/s]Extractor Estimating: 52it [00:35,  1.47it/s]Extractor Estimating: 53it [00:36,  1.45it/s]Extractor Estimating: 54it [00:36,  1.47it/s]Extractor Estimating: 55it [00:37,  1.50it/s]Extractor Estimating: 56it [00:38,  1.50it/s]Extractor Estimating: 57it [00:38,  1.50it/s]Extractor Estimating: 58it [00:39,  1.42it/s]Extractor Estimating: 59it [00:40,  1.47it/s]Extractor Estimating: 60it [00:40,  1.53it/s]Extractor Estimating: 61it [00:41,  1.56it/s]Extractor Estimating: 62it [00:42,  1.60it/s]Extractor Estimating: 63it [00:42,  1.64it/s]Extractor Estimating: 64it [00:43,  1.62it/s]Extractor Estimating: 65it [00:43,  1.62it/s]Extractor Estimating: 66it [00:44,  1.61it/s]Extractor Estimating: 67it [00:45,  1.56it/s]Extractor Estimating: 68it [00:45,  1.49it/s]Extractor Estimating: 69it [00:46,  1.49it/s]Extractor Estimating: 70it [00:47,  1.50it/s]Extractor Estimating: 71it [00:47,  1.51it/s]Extractor Estimating: 72it [00:48,  1.53it/s]Extractor Estimating: 73it [00:49,  1.51it/s]Extractor Estimating: 74it [00:49,  1.56it/s]Extractor Estimating: 75it [00:50,  1.60it/s]Extractor Estimating: 76it [00:51,  1.47it/s]Extractor Estimating: 77it [00:51,  1.48it/s]Extractor Estimating: 78it [00:52,  1.46it/s]Extractor Estimating: 79it [00:53,  1.53it/s]Extractor Estimating: 80it [00:53,  1.61it/s]Extractor Estimating: 81it [00:54,  1.63it/s]Extractor Estimating: 82it [00:54,  1.65it/s]Extractor Estimating: 83it [00:55,  1.62it/s]Extractor Estimating: 84it [00:56,  1.61it/s]Extractor Estimating: 85it [00:56,  1.61it/s]Extractor Estimating: 86it [00:57,  1.66it/s]Extractor Estimating: 87it [00:58,  1.61it/s]Extractor Estimating: 88it [00:58,  1.51it/s]Extractor Estimating: 89it [00:59,  1.48it/s]Extractor Estimating: 90it [01:00,  1.49it/s]Extractor Estimating: 91it [01:00,  1.53it/s]Extractor Estimating: 92it [01:01,  1.52it/s]Extractor Estimating: 93it [01:02,  1.46it/s]Extractor Estimating: 94it [01:02,  1.45it/s]Extractor Estimating: 95it [01:03,  1.39it/s]Extractor Estimating: 96it [01:04,  1.41it/s]Extractor Estimating: 97it [01:05,  1.47it/s]Extractor Estimating: 98it [01:05,  1.45it/s]Extractor Estimating: 99it [01:06,  1.47it/s]Extractor Estimating: 100it [01:06,  1.52it/s]Extractor Estimating: 101it [01:07,  1.52it/s]Extractor Estimating: 102it [01:08,  1.47it/s]Extractor Estimating: 103it [01:09,  1.40it/s]Extractor Estimating: 104it [01:09,  1.49it/s]Extractor Estimating: 105it [01:10,  1.55it/s]Extractor Estimating: 106it [01:10,  1.58it/s]Extractor Estimating: 107it [01:11,  1.60it/s]Extractor Estimating: 108it [01:12,  1.50it/s]Extractor Estimating: 109it [01:12,  1.56it/s]Extractor Estimating: 110it [01:13,  1.54it/s]Extractor Estimating: 111it [01:14,  1.55it/s]Extractor Estimating: 112it [01:14,  1.57it/s]Extractor Estimating: 113it [01:15,  1.59it/s]Extractor Estimating: 114it [01:16,  1.62it/s]Extractor Estimating: 115it [01:16,  1.50it/s]Extractor Estimating: 116it [01:17,  1.48it/s]Extractor Estimating: 117it [01:18,  1.53it/s]Extractor Estimating: 118it [01:18,  1.51it/s]Extractor Estimating: 119it [01:19,  1.54it/s]Extractor Estimating: 120it [01:20,  1.37it/s]Extractor Estimating: 121it [01:20,  1.45it/s]Extractor Estimating: 122it [01:21,  1.49it/s]Extractor Estimating: 123it [01:22,  1.53it/s]Extractor Estimating: 124it [01:22,  1.55it/s]Extractor Estimating: 125it [01:23,  1.54it/s]Extractor Estimating: 126it [01:24,  1.58it/s]Extractor Estimating: 127it [01:24,  1.56it/s]Extractor Estimating: 128it [01:25,  1.62it/s]Extractor Estimating: 129it [01:25,  1.62it/s]Extractor Estimating: 130it [01:26,  1.56it/s]Extractor Estimating: 131it [01:27,  1.52it/s]Extractor Estimating: 132it [01:27,  1.52it/s]Extractor Estimating: 133it [01:28,  1.53it/s]Extractor Estimating: 134it [01:29,  1.56it/s]Extractor Estimating: 135it [01:29,  1.52it/s]Extractor Estimating: 136it [01:30,  1.58it/s]Extractor Estimating: 137it [01:31,  1.54it/s]Extractor Estimating: 138it [01:31,  1.60it/s]Extractor Estimating: 139it [01:32,  1.64it/s]Extractor Estimating: 140it [01:32,  1.55it/s]Extractor Estimating: 141it [01:33,  1.60it/s]Extractor Estimating: 142it [01:34,  1.66it/s]Extractor Estimating: 143it [01:34,  1.66it/s]Extractor Estimating: 144it [01:35,  1.66it/s]Extractor Estimating: 145it [01:36,  1.60it/s]Extractor Estimating: 146it [01:36,  1.48it/s]Extractor Estimating: 147it [01:37,  1.54it/s]Extractor Estimating: 148it [01:37,  1.60it/s]Extractor Estimating: 149it [01:38,  1.62it/s]Extractor Estimating: 150it [01:39,  1.46it/s]Extractor Estimating: 151it [01:40,  1.48it/s]Extractor Estimating: 152it [01:40,  1.50it/s]Extractor Estimating: 153it [01:41,  1.50it/s]Extractor Estimating: 154it [01:42,  1.49it/s]Extractor Estimating: 155it [01:42,  1.40it/s]Extractor Estimating: 156it [01:43,  1.41it/s]Extractor Estimating: 157it [01:44,  1.42it/s]Extractor Estimating: 158it [01:44,  1.47it/s]Extractor Estimating: 159it [01:45,  1.54it/s]Extractor Estimating: 160it [01:46,  1.56it/s]Extractor Estimating: 161it [01:46,  1.53it/s]Extractor Estimating: 162it [01:47,  1.47it/s]Extractor Estimating: 163it [01:48,  1.47it/s]Extractor Estimating: 164it [01:48,  1.51it/s]Extractor Estimating: 165it [01:49,  1.50it/s]Extractor Estimating: 166it [01:50,  1.52it/s]Extractor Estimating: 167it [01:50,  1.47it/s]Extractor Estimating: 168it [01:51,  1.46it/s]Extractor Estimating: 169it [01:52,  1.51it/s]Extractor Estimating: 170it [01:52,  1.46it/s]Extractor Estimating: 171it [01:53,  1.44it/s]Extractor Estimating: 172it [01:54,  1.41it/s]Extractor Estimating: 173it [01:54,  1.48it/s]Extractor Estimating: 174it [01:55,  1.51it/s]Extractor Estimating: 175it [01:56,  1.53it/s]Extractor Estimating: 176it [01:56,  1.51it/s]Extractor Estimating: 177it [01:57,  1.35it/s]Extractor Estimating: 178it [01:58,  1.41it/s]Extractor Estimating: 179it [01:58,  1.52it/s]Extractor Estimating: 180it [01:59,  1.54it/s]Extractor Estimating: 181it [02:00,  1.56it/s]Extractor Estimating: 182it [02:00,  1.49it/s]Extractor Estimating: 183it [02:01,  1.52it/s]Extractor Estimating: 184it [02:02,  1.60it/s]Extractor Estimating: 185it [02:02,  1.57it/s]Extractor Estimating: 186it [02:03,  1.60it/s]Extractor Estimating: 187it [02:04,  1.48it/s]Extractor Estimating: 188it [02:04,  1.53it/s]Extractor Estimating: 189it [02:05,  1.56it/s]Extractor Estimating: 190it [02:06,  1.56it/s]Extractor Estimating: 191it [02:06,  1.58it/s]Extractor Estimating: 192it [02:07,  1.54it/s]Extractor Estimating: 193it [02:08,  1.46it/s]Extractor Estimating: 194it [02:08,  1.52it/s]Extractor Estimating: 195it [02:09,  1.57it/s]Extractor Estimating: 196it [02:10,  1.49it/s]Extractor Estimating: 197it [02:10,  1.42it/s]Extractor Estimating: 198it [02:11,  1.42it/s]Extractor Estimating: 199it [02:12,  1.46it/s]Extractor Estimating: 200it [02:12,  1.53it/s]Extractor Estimating: 201it [02:13,  1.51it/s]Extractor Estimating: 202it [02:14,  1.45it/s]Extractor Estimating: 203it [02:15,  1.38it/s]Extractor Estimating: 204it [02:15,  1.41it/s]Extractor Estimating: 205it [02:16,  1.42it/s]Extractor Estimating: 206it [02:17,  1.40it/s]Extractor Estimating: 207it [02:17,  1.35it/s]Extractor Estimating: 208it [02:18,  1.37it/s]Extractor Estimating: 209it [02:19,  1.42it/s]Extractor Estimating: 210it [02:19,  1.42it/s]Extractor Estimating: 211it [02:20,  1.45it/s]Extractor Estimating: 212it [02:21,  1.44it/s]Extractor Estimating: 213it [02:21,  1.49it/s]Extractor Estimating: 214it [02:22,  1.48it/s]Extractor Estimating: 215it [02:23,  1.51it/s]Extractor Estimating: 216it [02:23,  1.50it/s]Extractor Estimating: 217it [02:24,  1.45it/s]Extractor Estimating: 218it [02:25,  1.44it/s]Extractor Estimating: 219it [02:26,  1.47it/s]Extractor Estimating: 220it [02:26,  1.46it/s]Extractor Estimating: 221it [02:27,  1.39it/s]Extractor Estimating: 222it [02:28,  1.37it/s]Extractor Estimating: 223it [02:29,  1.39it/s]Extractor Estimating: 224it [02:29,  1.40it/s]Extractor Estimating: 225it [02:30,  1.39it/s]Extractor Estimating: 226it [02:30,  1.52it/s]Extractor Estimating: 227it [02:31,  1.51it/s]Extractor Estimating: 228it [02:32,  1.53it/s]Extractor Estimating: 229it [02:32,  1.57it/s]Extractor Estimating: 230it [02:33,  1.60it/s]Extractor Estimating: 231it [02:34,  1.55it/s]Extractor Estimating: 232it [02:34,  1.52it/s]Extractor Estimating: 233it [02:35,  1.64it/s]Extractor Estimating: 234it [02:35,  1.70it/s]Extractor Estimating: 235it [02:36,  1.73it/s]Extractor Estimating: 236it [02:36,  1.73it/s]Extractor Estimating: 237it [02:37,  1.76it/s]Extractor Estimating: 238it [02:38,  1.58it/s]Extractor Estimating: 239it [02:38,  1.58it/s]Extractor Estimating: 240it [02:39,  1.63it/s]Extractor Estimating: 241it [02:40,  1.64it/s]Extractor Estimating: 242it [02:40,  1.65it/s]Extractor Estimating: 243it [02:41,  1.58it/s]Extractor Estimating: 244it [02:42,  1.62it/s]Extractor Estimating: 245it [02:42,  1.65it/s]Extractor Estimating: 246it [02:43,  1.65it/s]Extractor Estimating: 247it [02:43,  1.65it/s]Extractor Estimating: 248it [02:44,  1.54it/s]Extractor Estimating: 249it [02:45,  1.56it/s]Extractor Estimating: 250it [02:45,  1.58it/s]Extractor Estimating: 251it [02:46,  1.51it/s]Extractor Estimating: 252it [02:47,  1.57it/s]Extractor Estimating: 253it [02:47,  1.55it/s]Extractor Estimating: 254it [02:48,  1.48it/s]Extractor Estimating: 255it [02:49,  1.45it/s]Extractor Estimating: 256it [02:49,  1.51it/s]Extractor Estimating: 257it [02:50,  1.53it/s]Extractor Estimating: 258it [02:51,  1.54it/s]Extractor Estimating: 259it [02:51,  1.59it/s]Extractor Estimating: 260it [02:52,  1.49it/s]Extractor Estimating: 261it [02:53,  1.51it/s]Extractor Estimating: 262it [02:53,  1.48it/s]Extractor Estimating: 263it [02:54,  1.37it/s]Extractor Estimating: 264it [02:55,  1.42it/s]Extractor Estimating: 265it [02:56,  1.42it/s]Extractor Estimating: 266it [02:56,  1.36it/s]Extractor Estimating: 267it [02:57,  1.40it/s]Extractor Estimating: 268it [02:58,  1.44it/s]Extractor Estimating: 269it [02:58,  1.44it/s]Extractor Estimating: 270it [02:59,  1.40it/s]Extractor Estimating: 271it [03:00,  1.38it/s]Extractor Estimating: 272it [03:01,  1.41it/s]Extractor Estimating: 273it [03:01,  1.44it/s]Extractor Estimating: 274it [03:02,  1.50it/s]Extractor Estimating: 275it [03:02,  1.50it/s]Extractor Estimating: 276it [03:03,  1.54it/s]Extractor Estimating: 277it [03:04,  1.55it/s]Extractor Estimating: 278it [03:04,  1.61it/s]Extractor Estimating: 279it [03:05,  1.69it/s]Extractor Estimating: 280it [03:05,  1.60it/s]Extractor Estimating: 281it [03:06,  1.62it/s]Extractor Estimating: 282it [03:07,  1.67it/s]Extractor Estimating: 283it [03:07,  1.68it/s]Extractor Estimating: 284it [03:08,  1.66it/s]Extractor Estimating: 285it [03:08,  1.63it/s]Extractor Estimating: 286it [03:09,  1.61it/s]Extractor Estimating: 287it [03:10,  1.57it/s]Extractor Estimating: 288it [03:10,  1.53it/s]Extractor Estimating: 289it [03:11,  1.54it/s]Extractor Estimating: 290it [03:12,  1.56it/s]Extractor Estimating: 291it [03:12,  1.55it/s]Extractor Estimating: 292it [03:13,  1.56it/s]Extractor Estimating: 293it [03:14,  1.56it/s]Extractor Estimating: 294it [03:14,  1.55it/s]Extractor Estimating: 295it [03:15,  1.52it/s]Extractor Estimating: 296it [03:16,  1.56it/s]Extractor Estimating: 297it [03:16,  1.54it/s]Extractor Estimating: 298it [03:17,  1.58it/s]Extractor Estimating: 299it [03:17,  1.59it/s]Extractor Estimating: 300it [03:18,  1.59it/s]Extractor Estimating: 301it [03:19,  1.50it/s]Extractor Estimating: 302it [03:19,  1.56it/s]Extractor Estimating: 303it [03:20,  1.59it/s]Extractor Estimating: 304it [03:21,  1.54it/s]Extractor Estimating: 305it [03:21,  1.52it/s]Extractor Estimating: 306it [03:22,  1.56it/s]Extractor Estimating: 307it [03:23,  1.50it/s]Extractor Estimating: 308it [03:23,  1.48it/s]Extractor Estimating: 309it [03:24,  1.60it/s]Extractor Estimating: 310it [03:25,  1.59it/s]Extractor Estimating: 311it [03:25,  1.53it/s]Extractor Estimating: 312it [03:26,  1.52it/s]Extractor Estimating: 313it [03:27,  1.53it/s]Extractor Estimating: 314it [03:27,  1.53it/s]Extractor Estimating: 315it [03:28,  1.57it/s]Extractor Estimating: 316it [03:28,  1.59it/s]Extractor Estimating: 317it [03:29,  1.61it/s]Extractor Estimating: 318it [03:30,  1.64it/s]Extractor Estimating: 319it [03:30,  1.64it/s]Extractor Estimating: 320it [03:31,  1.65it/s]Extractor Estimating: 321it [03:31,  1.69it/s]Extractor Estimating: 322it [03:32,  1.68it/s]Extractor Estimating: 323it [03:33,  1.65it/s]Extractor Estimating: 324it [03:33,  1.64it/s]Extractor Estimating: 325it [03:34,  1.66it/s]Extractor Estimating: 326it [03:34,  1.67it/s]Extractor Estimating: 327it [03:35,  1.70it/s]Extractor Estimating: 328it [03:36,  1.70it/s]Extractor Estimating: 329it [03:36,  1.78it/s]Extractor Estimating: 330it [03:37,  1.71it/s]Extractor Estimating: 331it [03:37,  1.78it/s]Extractor Estimating: 332it [03:38,  1.77it/s]Extractor Estimating: 333it [03:38,  1.84it/s]Extractor Estimating: 334it [03:39,  1.82it/s]Extractor Estimating: 335it [03:39,  1.78it/s]Extractor Estimating: 336it [03:40,  1.72it/s]Extractor Estimating: 337it [03:41,  1.76it/s]Extractor Estimating: 338it [03:41,  1.62it/s]Extractor Estimating: 339it [03:42,  1.64it/s]Extractor Estimating: 340it [03:43,  1.68it/s]Extractor Estimating: 341it [03:43,  1.74it/s]Extractor Estimating: 342it [03:44,  1.78it/s]Extractor Estimating: 343it [03:44,  1.81it/s]Extractor Estimating: 344it [03:45,  1.86it/s]Extractor Estimating: 345it [03:45,  1.84it/s]Extractor Estimating: 346it [03:46,  1.79it/s]Extractor Estimating: 347it [03:46,  1.80it/s]Extractor Estimating: 348it [03:47,  1.74it/s]Extractor Estimating: 349it [03:48,  1.73it/s]Extractor Estimating: 350it [03:48,  1.74it/s]Extractor Estimating: 351it [03:49,  1.60it/s]Extractor Estimating: 352it [03:50,  1.52it/s]Extractor Estimating: 353it [03:50,  1.51it/s]Extractor Estimating: 354it [03:51,  1.53it/s]Extractor Estimating: 355it [03:52,  1.47it/s]Extractor Estimating: 356it [03:52,  1.49it/s]Extractor Estimating: 357it [03:53,  1.52it/s]Extractor Estimating: 358it [03:53,  1.57it/s]Extractor Estimating: 359it [03:54,  1.53it/s]Extractor Estimating: 360it [03:55,  1.54it/s]Extractor Estimating: 361it [03:56,  1.52it/s]Extractor Estimating: 362it [03:56,  1.53it/s]Extractor Estimating: 363it [03:57,  1.62it/s]Extractor Estimating: 364it [03:57,  1.61it/s]Extractor Estimating: 365it [03:58,  1.58it/s]Extractor Estimating: 366it [03:59,  1.58it/s]Extractor Estimating: 367it [03:59,  1.62it/s]Extractor Estimating: 368it [04:00,  1.63it/s]Extractor Estimating: 369it [04:00,  1.56it/s]Extractor Estimating: 370it [04:01,  1.59it/s]Extractor Estimating: 371it [04:02,  1.60it/s]Extractor Estimating: 372it [04:02,  1.53it/s]Extractor Estimating: 373it [04:03,  1.50it/s]Extractor Estimating: 374it [04:04,  1.54it/s]Extractor Estimating: 375it [04:04,  1.55it/s]Extractor Estimating: 375it [04:04,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:03,052 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:03,168 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:03,168 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:03,168 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:03,169 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:51:04,062 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:51:04,063 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:51:04,742 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:51:05,842 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:51:05,842 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:09,172 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:09,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:09,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:09,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:51:09,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:51:09,968 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:51:09,969 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:51:10,611 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:51:10,783 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:51:10,783 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 05:11:01,384 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 05:11:02,023 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7878 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 25897
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25997, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25997, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.051, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.049, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.055, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 71, avg_time 1.070, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 171, avg_time 1.051, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 271, avg_time 2.177, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 42, avg_time 1.052, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 142, avg_time 1.075, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 242, avg_time 1.054, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 13, avg_time 1.054, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 113, avg_time 2.150, loss:nan
g_step 1200, step 213, avg_time 1.084, loss:nan
g_step 1300, step 313, avg_time 1.054, loss:nan
g_step 1400, step 84, avg_time 1.055, loss:nan
g_step 1500, step 184, avg_time 1.073, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 284, avg_time 2.142, loss:nan
g_step 1700, step 55, avg_time 1.069, loss:nan
g_step 1800, step 155, avg_time 1.074, loss:nan
g_step 1900, step 255, avg_time 1.047, loss:nan
g_step 2000, step 26, avg_time 1.069, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 126, avg_time 2.130, loss:nan
g_step 2200, step 226, avg_time 1.088, loss:nan
g_step 2300, step 326, avg_time 1.050, loss:nan
g_step 2400, step 97, avg_time 1.089, loss:nan
g_step 2500, step 197, avg_time 1.048, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 297, avg_time 2.138, loss:nan
g_step 2700, step 68, avg_time 1.056, loss:nan
g_step 2800, step 168, avg_time 1.062, loss:nan
g_step 2900, step 268, avg_time 1.055, loss:nan
g_step 3000, step 39, avg_time 1.051, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 139, avg_time 2.145, loss:nan
g_step 3200, step 239, avg_time 1.043, loss:nan
g_step 3300, step 10, avg_time 1.049, loss:nan
g_step 3400, step 110, avg_time 1.041, loss:nan
g_step 3500, step 210, avg_time 1.055, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 310, avg_time 2.117, loss:nan
g_step 3700, step 81, avg_time 1.046, loss:nan
g_step 3800, step 181, avg_time 1.053, loss:nan
g_step 3900, step 281, avg_time 1.045, loss:nan
g_step 4000, step 52, avg_time 1.023, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 152, avg_time 2.154, loss:nan
g_step 4200, step 252, avg_time 1.064, loss:nan
g_step 4300, step 23, avg_time 1.059, loss:nan
g_step 4400, step 123, avg_time 1.077, loss:nan
g_step 4500, step 223, avg_time 1.071, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 323, avg_time 2.142, loss:nan
g_step 4700, step 94, avg_time 1.065, loss:nan
g_step 4800, step 194, avg_time 1.062, loss:nan
g_step 4900, step 294, avg_time 1.076, loss:nan
g_step 5000, step 65, avg_time 1.045, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 165, avg_time 2.132, loss:nan
g_step 5200, step 265, avg_time 1.070, loss:nan
g_step 5300, step 36, avg_time 1.041, loss:nan
g_step 5400, step 136, avg_time 1.058, loss:nan
g_step 5500, step 236, avg_time 1.058, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 7, avg_time 2.122, loss:nan
g_step 5700, step 107, avg_time 1.030, loss:nan
g_step 5800, step 207, avg_time 1.045, loss:nan
g_step 5900, step 307, avg_time 1.071, loss:nan
g_step 6000, step 78, avg_time 1.045, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 178, avg_time 2.143, loss:nan
g_step 6200, step 278, avg_time 1.086, loss:nan
g_step 6300, step 49, avg_time 1.047, loss:nan
g_step 6400, step 149, avg_time 1.069, loss:nan
g_step 6500, step 249, avg_time 1.063, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 05:11:02 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 05:11:02 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_05-11-01_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 05:11:04 - WARNING - datasets.builder -   Using custom data configuration default-b2b0a15abb86e884
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b2b0a15abb86e884/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 05:11:11,650 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:11:11,651 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:11:11,652 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:11:11,653 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:11:12,405 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:12,726 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:12,727 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:12,727 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:12,727 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:12,727 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:11:12,727 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 05:11:13,858 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:11:17,136 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 05:11:17,343 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b2b0a15abb86e884/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:04,  1.50ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.50ba/s] 38%|███▊      | 3/8 [00:01<00:01,  3.15ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.56ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.87ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.06ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.20ba/s]100%|██████████| 8/8 [00:02<00:00,  4.43ba/s]100%|██████████| 8/8 [00:02<00:00,  3.66ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.56ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.53ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.15ba/s]100%|██████████| 4/4 [00:01<00:00,  4.24ba/s]100%|██████████| 4/4 [00:01<00:00,  3.33ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.04ba/s] 38%|███▊      | 3/8 [00:00<00:00,  5.06ba/s] 50%|█████     | 4/8 [00:00<00:00,  5.42ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  7.13ba/s]100%|██████████| 8/8 [00:01<00:00,  8.35ba/s]100%|██████████| 8/8 [00:01<00:00,  6.59ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.89ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.79ba/s]100%|██████████| 4/4 [00:00<00:00,  5.59ba/s]
[INFO|trainer.py:414] 2023-08-28 05:11:29,124 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 05:11:29,447 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 05:11:29,447 >>   Num examples = 7900
[INFO|trainer.py:1149] 2023-08-28 05:11:29,447 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 05:11:29,447 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 05:11:29,447 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 05:11:29,447 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 05:11:29,447 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:00<02:55,  3.50it/s]  0%|          | 2/615 [00:00<02:52,  3.55it/s]  0%|          | 3/615 [00:00<02:51,  3.57it/s]  1%|          | 4/615 [00:01<02:51,  3.57it/s]  1%|          | 5/615 [00:01<02:50,  3.58it/s]  1%|          | 6/615 [00:01<02:50,  3.58it/s]  1%|          | 7/615 [00:01<02:49,  3.58it/s]  1%|▏         | 8/615 [00:02<02:49,  3.58it/s]  1%|▏         | 9/615 [00:02<02:48,  3.59it/s]  2%|▏         | 10/615 [00:02<02:47,  3.60it/s]  2%|▏         | 11/615 [00:03<03:01,  3.32it/s]  2%|▏         | 12/615 [00:03<02:56,  3.41it/s]  2%|▏         | 13/615 [00:03<02:53,  3.48it/s]  2%|▏         | 14/615 [00:03<02:50,  3.53it/s]  2%|▏         | 15/615 [00:04<02:48,  3.55it/s]  3%|▎         | 16/615 [00:04<02:47,  3.58it/s]  3%|▎         | 17/615 [00:04<02:46,  3.59it/s]  3%|▎         | 18/615 [00:05<02:45,  3.60it/s]  3%|▎         | 19/615 [00:05<03:26,  2.88it/s]  3%|▎         | 20/615 [00:05<03:13,  3.07it/s]  3%|▎         | 21/615 [00:06<03:04,  3.22it/s]  4%|▎         | 22/615 [00:06<02:57,  3.33it/s]  4%|▎         | 23/615 [00:06<02:53,  3.42it/s]  4%|▍         | 24/615 [00:06<02:49,  3.48it/s]  4%|▍         | 25/615 [00:07<03:29,  2.81it/s]  4%|▍         | 26/615 [00:07<03:16,  3.00it/s]  4%|▍         | 27/615 [00:08<03:05,  3.17it/s]  5%|▍         | 28/615 [00:08<02:58,  3.29it/s]  5%|▍         | 29/615 [00:08<03:09,  3.10it/s]  5%|▍         | 30/615 [00:08<03:00,  3.24it/s]  5%|▌         | 31/615 [00:09<02:54,  3.35it/s]  5%|▌         | 32/615 [00:09<02:50,  3.42it/s]  5%|▌         | 33/615 [00:09<02:46,  3.49it/s]  6%|▌         | 34/615 [00:10<02:44,  3.53it/s]  6%|▌         | 35/615 [00:10<02:43,  3.56it/s]  6%|▌         | 36/615 [00:10<02:41,  3.58it/s]  6%|▌         | 37/615 [00:10<02:40,  3.60it/s]  6%|▌         | 38/615 [00:11<02:40,  3.60it/s]  6%|▋         | 39/615 [00:11<02:39,  3.61it/s]  7%|▋         | 40/615 [00:11<02:52,  3.32it/s]  7%|▋         | 41/615 [00:12<02:48,  3.41it/s]  7%|▋         | 42/615 [00:12<02:44,  3.47it/s]  7%|▋         | 43/615 [00:12<02:42,  3.52it/s]  7%|▋         | 44/615 [00:12<02:40,  3.55it/s]  7%|▋         | 45/615 [00:13<02:39,  3.57it/s]  7%|▋         | 46/615 [00:13<02:38,  3.59it/s]  8%|▊         | 47/615 [00:13<02:37,  3.60it/s]  8%|▊         | 48/615 [00:13<02:37,  3.61it/s]  8%|▊         | 49/615 [00:14<02:36,  3.61it/s]  8%|▊         | 50/615 [00:14<02:36,  3.62it/s]  8%|▊         | 51/615 [00:14<02:57,  3.19it/s]  8%|▊         | 52/615 [00:15<02:50,  3.31it/s]  9%|▊         | 53/615 [00:15<02:45,  3.40it/s]  9%|▉         | 54/615 [00:15<02:42,  3.46it/s]  9%|▉         | 55/615 [00:16<02:39,  3.51it/s]  9%|▉         | 56/615 [00:16<02:37,  3.54it/s]  9%|▉         | 57/615 [00:16<02:36,  3.57it/s]  9%|▉         | 58/615 [00:16<02:35,  3.59it/s] 10%|▉         | 59/615 [00:17<02:34,  3.60it/s] 10%|▉         | 60/615 [00:17<02:33,  3.60it/s] 10%|▉         | 61/615 [00:17<02:33,  3.61it/s] 10%|█         | 62/615 [00:18<02:47,  3.31it/s] 10%|█         | 63/615 [00:18<02:42,  3.40it/s] 10%|█         | 64/615 [00:18<02:39,  3.45it/s] 11%|█         | 65/615 [00:18<02:37,  3.49it/s] 11%|█         | 66/615 [00:19<02:36,  3.51it/s] 11%|█         | 67/615 [00:19<02:35,  3.53it/s] 11%|█         | 68/615 [00:19<02:34,  3.54it/s] 11%|█         | 69/615 [00:20<02:33,  3.55it/s] 11%|█▏        | 70/615 [00:20<02:33,  3.56it/s] 12%|█▏        | 71/615 [00:20<02:32,  3.56it/s] 12%|█▏        | 72/615 [00:20<02:32,  3.57it/s] 12%|█▏        | 73/615 [00:21<02:48,  3.22it/s] 12%|█▏        | 74/615 [00:21<02:42,  3.32it/s] 12%|█▏        | 75/615 [00:21<02:39,  3.39it/s] 12%|█▏        | 76/615 [00:22<02:36,  3.45it/s] 13%|█▎        | 77/615 [00:22<02:34,  3.48it/s] 13%|█▎        | 78/615 [00:22<02:32,  3.51it/s] 13%|█▎        | 79/615 [00:22<02:31,  3.53it/s] 13%|█▎        | 80/615 [00:23<02:31,  3.54it/s] 13%|█▎        | 81/615 [00:23<02:30,  3.56it/s] 13%|█▎        | 82/615 [00:23<02:29,  3.56it/s] 13%|█▎        | 83/615 [00:24<02:29,  3.57it/s] 14%|█▎        | 84/615 [00:24<03:08,  2.82it/s] 14%|█▍        | 85/615 [00:24<02:56,  3.00it/s] 14%|█▍        | 86/615 [00:25<02:47,  3.15it/s] 14%|█▍        | 87/615 [00:25<02:41,  3.27it/s] 14%|█▍        | 88/615 [00:25<02:37,  3.35it/s] 14%|█▍        | 89/615 [00:25<02:34,  3.41it/s] 15%|█▍        | 90/615 [00:26<02:32,  3.45it/s] 15%|█▍        | 91/615 [00:26<02:30,  3.48it/s] 15%|█▍        | 92/615 [00:26<02:29,  3.50it/s] 15%|█▌        | 93/615 [00:27<02:28,  3.52it/s] 15%|█▌        | 94/615 [00:27<02:49,  3.07it/s] 15%|█▌        | 95/615 [00:27<02:42,  3.21it/s] 16%|█▌        | 96/615 [00:28<02:36,  3.31it/s] 16%|█▌        | 97/615 [00:28<02:33,  3.38it/s] 16%|█▌        | 98/615 [00:28<02:30,  3.44it/s] 16%|█▌        | 99/615 [00:28<02:28,  3.48it/s] 16%|█▋        | 100/615 [00:29<02:26,  3.51it/s] 16%|█▋        | 101/615 [00:29<02:25,  3.53it/s] 17%|█▋        | 102/615 [00:29<02:24,  3.54it/s] 17%|█▋        | 103/615 [00:30<02:24,  3.55it/s] 17%|█▋        | 104/615 [00:30<02:23,  3.56it/s] 17%|█▋        | 105/615 [00:30<02:43,  3.12it/s] 17%|█▋        | 106/615 [00:30<02:36,  3.25it/s] 17%|█▋        | 107/615 [00:31<02:31,  3.35it/s] 18%|█▊        | 108/615 [00:31<02:28,  3.42it/s] 18%|█▊        | 109/615 [00:31<02:25,  3.48it/s] 18%|█▊        | 110/615 [00:32<04:14,  1.98it/s] 18%|█▊        | 111/615 [00:33<03:40,  2.28it/s] 18%|█▊        | 112/615 [00:33<03:15,  2.57it/s] 18%|█▊        | 113/615 [00:33<03:25,  2.45it/s] 19%|█▊        | 114/615 [00:34<03:04,  2.71it/s] 19%|█▊        | 115/615 [00:34<02:50,  2.93it/s] 19%|█▉        | 116/615 [00:34<02:40,  3.10it/s] 19%|█▉        | 117/615 [00:34<02:33,  3.24it/s] 19%|█▉        | 118/615 [00:35<02:28,  3.35it/s] 19%|█▉        | 119/615 [00:35<02:24,  3.43it/s] 20%|█▉        | 120/615 [00:35<02:22,  3.48it/s] 20%|█▉        | 121/615 [00:36<02:20,  3.52it/s] 20%|█▉        | 122/615 [00:36<02:18,  3.55it/s] 20%|██        | 123/615 [00:36<02:17,  3.57it/s][INFO|trainer.py:2140] 2023-08-28 05:12:06,188 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:12:06,188 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 05:12:06,188 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.42it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.88it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.96it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.00it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.24it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.65it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.36it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.28it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.47it/s][A
 12%|█▏        | 52/437 [00:01<00:10, 36.92it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 39.04it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 40.69it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 42.01it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 42.78it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.50it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.82it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.07it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.72it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 38.72it/s][A
 23%|██▎       | 102/437 [00:02<00:08, 40.43it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 41.64it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 42.57it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.32it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.77it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.06it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.13it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.80it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.59it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.87it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.98it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.37it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.39it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.66it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.74it/s][A
 41%|████      | 177/437 [00:04<00:06, 37.24it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 39.31it/s][A
 43%|████▎     | 187/437 [00:04<00:06, 40.83it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 42.09it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 42.93it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.41it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.90it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.06it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 43.84it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.54it/s][A
 52%|█████▏    | 227/437 [00:05<00:05, 37.32it/s][A
 53%|█████▎    | 232/437 [00:05<00:05, 39.27it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 40.84it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 41.88it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 42.84it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.34it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 43.90it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 43.92it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.62it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.69it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.85it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.11it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.15it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.36it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.59it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 40.41it/s][A
 70%|███████   | 307/437 [00:07<00:03, 41.61it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 42.45it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 42.95it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.37it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.77it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.95it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.19it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.98it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.10it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.22it/s][A
 82%|████████▏ | 357/437 [00:08<00:02, 35.78it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 38.21it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 39.95it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 41.28it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 42.35it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.01it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.64it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.77it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.53it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.57it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.74it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.16it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.20it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.49it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.67it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.67it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.37it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.37it/s][A 20%|██        | 123/615 [00:46<02:17,  3.57it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:12:17,268 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-28 05:12:17,790 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:12:47,672 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:12:48,663 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:12:48,932 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123/special_tokens_map.json
 20%|██        | 124/615 [01:22<1:55:12, 14.08s/it] 20%|██        | 125/615 [01:23<1:21:23,  9.97s/it] 20%|██        | 126/615 [01:23<57:32,  7.06s/it]   21%|██        | 127/615 [01:23<40:52,  5.03s/it] 21%|██        | 128/615 [01:24<29:14,  3.60s/it] 21%|██        | 129/615 [01:24<21:06,  2.61s/it] 21%|██        | 130/615 [01:24<15:25,  1.91s/it] 21%|██▏       | 131/615 [01:24<11:26,  1.42s/it] 21%|██▏       | 132/615 [01:25<08:39,  1.08s/it] 22%|██▏       | 133/615 [01:25<06:42,  1.20it/s] 22%|██▏       | 134/615 [01:25<05:20,  1.50it/s] 22%|██▏       | 135/615 [01:26<04:23,  1.82it/s] 22%|██▏       | 136/615 [01:26<03:56,  2.02it/s] 22%|██▏       | 137/615 [01:26<03:24,  2.34it/s] 22%|██▏       | 138/615 [01:26<03:02,  2.62it/s] 23%|██▎       | 139/615 [01:27<02:46,  2.86it/s] 23%|██▎       | 140/615 [01:27<02:35,  3.05it/s] 23%|██▎       | 141/615 [01:27<02:27,  3.20it/s] 23%|██▎       | 142/615 [01:28<02:22,  3.32it/s] 23%|██▎       | 143/615 [01:28<02:18,  3.41it/s] 23%|██▎       | 144/615 [01:28<02:15,  3.47it/s] 24%|██▎       | 145/615 [01:28<02:13,  3.51it/s] 24%|██▎       | 146/615 [01:29<02:12,  3.54it/s] 24%|██▍       | 147/615 [01:29<02:19,  3.36it/s] 24%|██▍       | 148/615 [01:29<02:16,  3.42it/s] 24%|██▍       | 149/615 [01:30<02:14,  3.47it/s] 24%|██▍       | 150/615 [01:30<02:12,  3.50it/s] 25%|██▍       | 151/615 [01:30<02:11,  3.53it/s] 25%|██▍       | 152/615 [01:30<02:10,  3.54it/s] 25%|██▍       | 153/615 [01:31<02:10,  3.55it/s] 25%|██▌       | 154/615 [01:31<02:09,  3.56it/s] 25%|██▌       | 155/615 [01:31<02:09,  3.56it/s] 25%|██▌       | 156/615 [01:32<02:08,  3.57it/s] 26%|██▌       | 157/615 [01:32<03:41,  2.07it/s] 26%|██▌       | 158/615 [01:33<03:23,  2.24it/s] 26%|██▌       | 159/615 [01:33<03:00,  2.52it/s] 26%|██▌       | 160/615 [01:33<02:53,  2.62it/s] 26%|██▌       | 161/615 [01:34<02:39,  2.84it/s] 26%|██▋       | 162/615 [01:34<02:29,  3.03it/s] 27%|██▋       | 163/615 [01:34<02:22,  3.18it/s] 27%|██▋       | 164/615 [01:35<02:17,  3.29it/s] 27%|██▋       | 165/615 [01:35<02:13,  3.37it/s] 27%|██▋       | 166/615 [01:35<02:10,  3.43it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/615 [01:35<02:08,  3.48it/s] 27%|██▋       | 168/615 [01:36<02:07,  3.51it/s] 27%|██▋       | 169/615 [01:36<02:16,  3.26it/s] 28%|██▊       | 170/615 [01:36<02:12,  3.35it/s] 28%|██▊       | 171/615 [01:37<02:10,  3.41it/s] 28%|██▊       | 172/615 [01:37<02:08,  3.46it/s] 28%|██▊       | 173/615 [01:37<02:12,  3.33it/s] 28%|██▊       | 174/615 [01:37<02:09,  3.40it/s] 28%|██▊       | 175/615 [01:38<02:07,  3.46it/s] 29%|██▊       | 176/615 [01:38<02:05,  3.49it/s] 29%|██▉       | 177/615 [01:38<02:04,  3.52it/s] 29%|██▉       | 178/615 [01:39<02:03,  3.53it/s] 29%|██▉       | 179/615 [01:39<02:02,  3.55it/s] 29%|██▉       | 180/615 [01:39<02:02,  3.56it/s] 29%|██▉       | 181/615 [01:39<02:01,  3.56it/s] 30%|██▉       | 182/615 [01:40<02:01,  3.57it/s] 30%|██▉       | 183/615 [01:40<02:01,  3.57it/s] 30%|██▉       | 184/615 [01:40<02:12,  3.24it/s] 30%|███       | 185/615 [01:41<02:09,  3.33it/s] 30%|███       | 186/615 [01:41<02:06,  3.40it/s] 30%|███       | 187/615 [01:41<02:04,  3.45it/s] 31%|███       | 188/615 [01:42<02:02,  3.48it/s] 31%|███       | 189/615 [01:42<02:01,  3.50it/s] 31%|███       | 190/615 [01:42<02:00,  3.52it/s] 31%|███       | 191/615 [01:42<01:59,  3.54it/s] 31%|███       | 192/615 [01:43<01:59,  3.55it/s] 31%|███▏      | 193/615 [01:43<01:58,  3.57it/s] 32%|███▏      | 194/615 [01:43<01:57,  3.58it/s] 32%|███▏      | 195/615 [01:44<02:08,  3.26it/s] 32%|███▏      | 196/615 [01:44<02:04,  3.36it/s] 32%|███▏      | 197/615 [01:44<02:01,  3.44it/s] 32%|███▏      | 198/615 [01:44<01:59,  3.49it/s] 32%|███▏      | 199/615 [01:45<01:57,  3.53it/s] 33%|███▎      | 200/615 [01:45<01:56,  3.56it/s] 33%|███▎      | 201/615 [01:45<01:55,  3.58it/s] 33%|███▎      | 202/615 [01:45<01:55,  3.59it/s] 33%|███▎      | 203/615 [01:46<01:54,  3.60it/s] 33%|███▎      | 204/615 [01:46<01:54,  3.60it/s] 33%|███▎      | 205/615 [01:46<01:53,  3.61it/s] 33%|███▎      | 206/615 [01:47<02:01,  3.37it/s] 34%|███▎      | 207/615 [01:47<01:58,  3.45it/s] 34%|███▍      | 208/615 [01:47<01:56,  3.50it/s] 34%|███▍      | 209/615 [01:47<01:54,  3.53it/s] 34%|███▍      | 210/615 [01:48<01:53,  3.56it/s] 34%|███▍      | 211/615 [01:48<01:52,  3.58it/s] 34%|███▍      | 212/615 [01:48<01:52,  3.59it/s] 35%|███▍      | 213/615 [01:49<01:51,  3.60it/s] 35%|███▍      | 214/615 [01:49<01:51,  3.61it/s] 35%|███▍      | 215/615 [01:49<01:50,  3.61it/s] 35%|███▌      | 216/615 [01:49<01:50,  3.61it/s] 35%|███▌      | 217/615 [01:50<01:59,  3.32it/s] 35%|███▌      | 218/615 [01:50<01:56,  3.41it/s] 36%|███▌      | 219/615 [01:50<01:54,  3.47it/s] 36%|███▌      | 220/615 [01:51<01:52,  3.51it/s] 36%|███▌      | 221/615 [01:51<01:51,  3.54it/s] 36%|███▌      | 222/615 [01:51<01:50,  3.56it/s] 36%|███▋      | 223/615 [01:51<01:49,  3.58it/s] 36%|███▋      | 224/615 [01:52<01:48,  3.59it/s] 37%|███▋      | 225/615 [01:52<01:48,  3.60it/s] 37%|███▋      | 226/615 [01:52<01:47,  3.61it/s] 37%|███▋      | 227/615 [01:53<01:47,  3.61it/s] 37%|███▋      | 228/615 [01:53<01:55,  3.35it/s] 37%|███▋      | 229/615 [01:53<01:52,  3.43it/s] 37%|███▋      | 230/615 [01:53<01:50,  3.48it/s] 38%|███▊      | 231/615 [01:54<01:49,  3.52it/s] 38%|███▊      | 232/615 [01:54<01:47,  3.55it/s] 38%|███▊      | 233/615 [01:54<01:46,  3.57it/s] 38%|███▊      | 234/615 [01:55<01:46,  3.59it/s] 38%|███▊      | 235/615 [01:55<01:45,  3.59it/s] 38%|███▊      | 236/615 [01:55<01:45,  3.60it/s] 39%|███▊      | 237/615 [01:55<01:44,  3.60it/s] 39%|███▊      | 238/615 [01:56<01:44,  3.61it/s] 39%|███▉      | 239/615 [01:56<01:56,  3.24it/s] 39%|███▉      | 240/615 [01:56<01:52,  3.34it/s] 39%|███▉      | 241/615 [01:57<01:49,  3.42it/s] 39%|███▉      | 242/615 [01:57<01:47,  3.48it/s] 40%|███▉      | 243/615 [01:57<01:45,  3.51it/s] 40%|███▉      | 244/615 [01:57<01:44,  3.55it/s] 40%|███▉      | 245/615 [01:58<01:43,  3.56it/s] 40%|████      | 246/615 [01:58<01:43,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 05:13:28,040 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:13:28,040 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 05:13:28,040 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.2033, 'eval_samples_per_second': 342.241, 'eval_steps_per_second': 42.829, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.60it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.05it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.23it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.05it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.13it/s][A
  7%|▋         | 32/437 [00:00<00:11, 36.13it/s][A
  8%|▊         | 37/437 [00:00<00:10, 38.60it/s][A
 10%|▉         | 42/437 [00:00<00:09, 40.39it/s][A
 11%|█         | 47/437 [00:01<00:09, 41.71it/s][A
 12%|█▏        | 52/437 [00:01<00:09, 42.72it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.37it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.75it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.85it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.66it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.65it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.77it/s][A
 20%|█▉        | 87/437 [00:02<00:07, 43.96it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.35it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.48it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.68it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.60it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.40it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.23it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.93it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.94it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.15it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.42it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.55it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.67it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.66it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.49it/s][A
 37%|███▋      | 162/437 [00:03<00:07, 38.57it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 40.29it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 41.58it/s][A
 41%|████      | 177/437 [00:04<00:06, 42.55it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.24it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.67it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.04it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.10it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.74it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.55it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.79it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.03it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.40it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.50it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.64it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.64it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.54it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.21it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.93it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.97it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.24it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.45it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.47it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.64it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.61it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.46it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.81it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 36.69it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 38.85it/s][A
 70%|███████   | 307/437 [00:07<00:03, 40.50it/s][A
 71%|███████▏  | 312/437 [00:07<00:03, 41.64it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 42.63it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.38it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.80it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.94it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.61it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.58it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.81it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.06it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.23it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.39it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.63it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.67it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.45it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.17it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.00it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.98it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.14it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.38it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.49it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 31.98it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 35.05it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 37.52it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 39.45it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 40.93it/s][A
100%|██████████| 437/437 [00:10<00:00, 42.07it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 42.07it/s][A 40%|████      | 246/615 [02:08<01:43,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:13:38,732 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 05:13:39,494 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:13:58,195 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:13:59,106 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:13:59,396 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246/special_tokens_map.json
 40%|████      | 247/615 [02:35<1:08:30, 11.17s/it] 40%|████      | 248/615 [02:35<48:29,  7.93s/it]   40%|████      | 249/615 [02:35<34:21,  5.63s/it] 41%|████      | 250/615 [02:35<24:29,  4.03s/it] 41%|████      | 251/615 [02:36<17:36,  2.90s/it] 41%|████      | 252/615 [02:36<12:48,  2.12s/it] 41%|████      | 253/615 [02:36<09:26,  1.56s/it] 41%|████▏     | 254/615 [02:37<07:05,  1.18s/it] 41%|████▏     | 255/615 [02:37<05:27,  1.10it/s] 42%|████▏     | 256/615 [02:37<04:18,  1.39it/s] 42%|████▏     | 257/615 [02:37<03:30,  1.70it/s] 42%|████▏     | 258/615 [02:38<02:56,  2.02it/s] 42%|████▏     | 259/615 [02:38<02:45,  2.15it/s] 42%|████▏     | 260/615 [02:38<02:25,  2.44it/s] 42%|████▏     | 261/615 [02:39<02:13,  2.66it/s] 43%|████▎     | 262/615 [02:39<02:02,  2.88it/s] 43%|████▎     | 263/615 [02:39<01:55,  3.05it/s] 43%|████▎     | 264/615 [02:40<01:49,  3.19it/s] 43%|████▎     | 265/615 [02:40<01:46,  3.30it/s] 43%|████▎     | 266/615 [02:40<01:43,  3.37it/s] 43%|████▎     | 267/615 [02:40<01:41,  3.43it/s] 44%|████▎     | 268/615 [02:41<01:40,  3.47it/s] 44%|████▎     | 269/615 [02:41<01:38,  3.50it/s] 44%|████▍     | 270/615 [02:41<01:38,  3.52it/s] 44%|████▍     | 271/615 [02:42<01:46,  3.22it/s] 44%|████▍     | 272/615 [02:42<01:43,  3.32it/s] 44%|████▍     | 273/615 [02:42<01:40,  3.39it/s] 45%|████▍     | 274/615 [02:42<01:39,  3.44it/s] 45%|████▍     | 275/615 [02:43<01:37,  3.48it/s] 45%|████▍     | 276/615 [02:43<01:36,  3.50it/s] 45%|████▌     | 277/615 [02:43<01:35,  3.52it/s] 45%|████▌     | 278/615 [02:44<01:35,  3.54it/s] 45%|████▌     | 279/615 [02:44<01:34,  3.55it/s] 46%|████▌     | 280/615 [02:44<01:34,  3.56it/s] 46%|████▌     | 281/615 [02:44<01:33,  3.56it/s] 46%|████▌     | 282/615 [02:45<01:40,  3.30it/s] 46%|████▌     | 283/615 [02:45<01:38,  3.37it/s] 46%|████▌     | 284/615 [02:45<01:36,  3.43it/s] 46%|████▋     | 285/615 [02:46<01:35,  3.47it/s] 47%|████▋     | 286/615 [02:46<01:33,  3.51it/s] 47%|████▋     | 287/615 [02:46<01:33,  3.52it/s] 47%|████▋     | 288/615 [02:46<01:32,  3.54it/s] 47%|████▋     | 289/615 [02:47<01:31,  3.55it/s] 47%|████▋     | 290/615 [02:47<01:31,  3.56it/s] 47%|████▋     | 291/615 [02:47<01:30,  3.56it/s] 47%|████▋     | 292/615 [02:48<01:30,  3.57it/s] 48%|████▊     | 293/615 [02:48<01:35,  3.38it/s] 48%|████▊     | 294/615 [02:48<01:33,  3.43it/s] 48%|████▊     | 295/615 [02:48<01:32,  3.47it/s] 48%|████▊     | 296/615 [02:49<01:31,  3.50it/s] 48%|████▊     | 297/615 [02:49<01:30,  3.53it/s] 48%|████▊     | 298/615 [02:49<01:29,  3.55it/s] 49%|████▊     | 299/615 [02:50<01:28,  3.58it/s] 49%|████▉     | 300/615 [02:50<01:27,  3.59it/s] 49%|████▉     | 301/615 [02:50<01:27,  3.60it/s] 49%|████▉     | 302/615 [02:50<01:26,  3.61it/s] 49%|████▉     | 303/615 [02:51<01:26,  3.61it/s] 49%|████▉     | 304/615 [02:51<01:31,  3.41it/s] 50%|████▉     | 305/615 [02:51<01:29,  3.47it/s] 50%|████▉     | 306/615 [02:52<01:27,  3.52it/s] 50%|████▉     | 307/615 [02:52<01:26,  3.55it/s] 50%|█████     | 308/615 [02:52<01:26,  3.56it/s] 50%|█████     | 309/615 [02:52<01:25,  3.58it/s] 50%|█████     | 310/615 [02:53<01:24,  3.60it/s] 51%|█████     | 311/615 [02:53<01:24,  3.60it/s] 51%|█████     | 312/615 [02:53<01:23,  3.61it/s] 51%|█████     | 313/615 [02:53<01:23,  3.61it/s] 51%|█████     | 314/615 [02:54<01:23,  3.62it/s] 51%|█████     | 315/615 [02:54<01:29,  3.34it/s] 51%|█████▏    | 316/615 [02:54<01:27,  3.42it/s] 52%|█████▏    | 317/615 [02:55<01:25,  3.48it/s] 52%|█████▏    | 318/615 [02:55<01:24,  3.52it/s] 52%|█████▏    | 319/615 [02:55<01:23,  3.55it/s] 52%|█████▏    | 320/615 [02:55<01:22,  3.57it/s] 52%|█████▏    | 321/615 [02:56<01:21,  3.59it/s] 52%|█████▏    | 322/615 [02:56<01:21,  3.59it/s] 53%|█████▎    | 323/615 [02:56<01:21,  3.60it/s] 53%|█████▎    | 324/615 [02:57<01:20,  3.61it/s] 53%|█████▎    | 325/615 [02:57<01:20,  3.61it/s] 53%|█████▎    | 326/615 [02:57<01:27,  3.30it/s] 53%|█████▎    | 327/615 [02:57<01:24,  3.39it/s] 53%|█████▎    | 328/615 [02:58<01:23,  3.46it/s] 53%|█████▎    | 329/615 [02:58<01:21,  3.50it/s] 54%|█████▎    | 330/615 [02:58<01:20,  3.54it/s] 54%|█████▍    | 331/615 [02:59<01:19,  3.56it/s] 54%|█████▍    | 332/615 [02:59<01:18,  3.58it/s] 54%|█████▍    | 333/615 [02:59<01:18,  3.59it/s] 54%|█████▍    | 334/615 [02:59<01:17,  3.60it/s] 54%|█████▍    | 335/615 [03:00<01:17,  3.61it/s] 55%|█████▍    | 336/615 [03:00<01:17,  3.61it/s] 55%|█████▍    | 337/615 [03:00<01:31,  3.04it/s] 55%|█████▍    | 338/615 [03:01<01:26,  3.20it/s] 55%|█████▌    | 339/615 [03:01<01:23,  3.32it/s] 55%|█████▌    | 340/615 [03:01<01:20,  3.40it/s] 55%|█████▌    | 341/615 [03:02<01:18,  3.47it/s] 56%|█████▌    | 342/615 [03:02<01:17,  3.52it/s] 56%|█████▌    | 343/615 [03:02<01:16,  3.55it/s] 56%|█████▌    | 344/615 [03:02<01:15,  3.57it/s] 56%|█████▌    | 345/615 [03:03<01:15,  3.59it/s] 56%|█████▋    | 346/615 [03:03<01:14,  3.60it/s] 56%|█████▋    | 347/615 [03:03<01:14,  3.61it/s] 57%|█████▋    | 348/615 [03:04<01:21,  3.28it/s] 57%|█████▋    | 349/615 [03:04<01:18,  3.38it/s] 57%|█████▋    | 350/615 [03:04<01:16,  3.45it/s] 57%|█████▋    | 351/615 [03:04<01:15,  3.50it/s] 57%|█████▋    | 352/615 [03:05<01:14,  3.53it/s] 57%|█████▋    | 353/615 [03:05<01:13,  3.56it/s] 58%|█████▊    | 354/615 [03:05<01:12,  3.58it/s] 58%|█████▊    | 355/615 [03:05<01:12,  3.59it/s] 58%|█████▊    | 356/615 [03:06<01:11,  3.60it/s] 58%|█████▊    | 357/615 [03:06<01:11,  3.60it/s] 58%|█████▊    | 358/615 [03:06<01:11,  3.60it/s] 58%|█████▊    | 359/615 [03:07<01:18,  3.24it/s] 59%|█████▊    | 360/615 [03:07<01:16,  3.35it/s] 59%|█████▊    | 361/615 [03:07<01:14,  3.43it/s] 59%|█████▉    | 362/615 [03:08<01:12,  3.48it/s] 59%|█████▉    | 363/615 [03:08<01:11,  3.52it/s] 59%|█████▉    | 364/615 [03:08<01:10,  3.55it/s] 59%|█████▉    | 365/615 [03:08<01:09,  3.57it/s] 60%|█████▉    | 366/615 [03:09<01:09,  3.59it/s] 60%|█████▉    | 367/615 [03:09<01:08,  3.60it/s] 60%|█████▉    | 368/615 [03:09<01:23,  2.95it/s] 60%|██████    | 369/615 [03:10<01:18,  3.12it/s][INFO|trainer.py:2140] 2023-08-28 05:14:39,728 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:14:39,728 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 05:14:39,728 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.1787, 'eval_samples_per_second': 343.07, 'eval_steps_per_second': 42.933, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.83it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.65it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.92it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.00it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.36it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.82it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.48it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.13it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.31it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.49it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.61it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.69it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.68it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.51it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.30it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.20it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.15it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.13it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.23it/s][A
 23%|██▎       | 102/437 [00:02<00:08, 39.61it/s][A
 24%|██▍       | 107/437 [00:02<00:08, 41.13it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 42.19it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 42.92it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.52it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.77it/s][A
 30%|███       | 132/437 [00:02<00:06, 43.80it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.83it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.72it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.89it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.13it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.34it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.55it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.62it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.49it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.36it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.22it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.00it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.02it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.23it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.41it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.55it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.71it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.60it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.42it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.22it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.09it/s][A
 54%|█████▍    | 237/437 [00:05<00:05, 35.74it/s][A
 55%|█████▌    | 242/437 [00:05<00:05, 38.05it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 39.90it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 41.26it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 42.31it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.12it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.63it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.81it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.57it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.41it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.61it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.95it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.22it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.37it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.63it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.73it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.43it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.98it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.87it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.97it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.10it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.20it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.47it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.70it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.69it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.33it/s][A
 84%|████████▍ | 367/437 [00:08<00:02, 31.84it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 34.89it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 37.38it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 39.36it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 40.92it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 42.02it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 42.89it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.32it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.17it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.21it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.48it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.86it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.13it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.41it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.53it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.53it/s][A 60%|██████    | 369/615 [03:20<01:18,  3.12it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:14:50,764 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-28 05:14:51,566 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:15:16,608 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:15:17,348 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:15:17,722 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369/special_tokens_map.json
 60%|██████    | 370/615 [03:52<53:20, 13.06s/it] 60%|██████    | 371/615 [03:53<37:37,  9.25s/it] 60%|██████    | 372/615 [03:53<26:33,  6.56s/it] 61%|██████    | 373/615 [03:53<18:51,  4.68s/it] 61%|██████    | 374/615 [03:54<13:28,  3.36s/it] 61%|██████    | 375/615 [03:54<09:44,  2.43s/it] 61%|██████    | 376/615 [03:54<07:07,  1.79s/it] 61%|██████▏   | 377/615 [03:54<05:17,  1.34s/it] 61%|██████▏   | 378/615 [03:55<04:01,  1.02s/it] 62%|██████▏   | 379/615 [03:55<03:08,  1.26it/s] 62%|██████▏   | 380/615 [03:55<02:30,  1.56it/s] 62%|██████▏   | 381/615 [03:56<02:04,  1.88it/s] 62%|██████▏   | 382/615 [03:56<01:52,  2.07it/s] 62%|██████▏   | 383/615 [03:56<01:38,  2.37it/s] 62%|██████▏   | 384/615 [03:57<01:27,  2.64it/s] 63%|██████▎   | 385/615 [03:57<01:20,  2.86it/s] 63%|██████▎   | 386/615 [03:57<01:15,  3.05it/s] 63%|██████▎   | 387/615 [03:57<01:11,  3.19it/s] 63%|██████▎   | 388/615 [03:58<01:08,  3.30it/s] 63%|██████▎   | 389/615 [03:58<01:06,  3.38it/s] 63%|██████▎   | 390/615 [03:58<01:05,  3.44it/s] 64%|██████▎   | 391/615 [03:58<01:04,  3.48it/s] 64%|██████▎   | 392/615 [03:59<01:03,  3.50it/s] 64%|██████▍   | 393/615 [03:59<01:08,  3.24it/s] 64%|██████▍   | 394/615 [03:59<01:06,  3.34it/s] 64%|██████▍   | 395/615 [04:00<01:04,  3.41it/s] 64%|██████▍   | 396/615 [04:00<01:03,  3.46it/s] 65%|██████▍   | 397/615 [04:00<01:02,  3.49it/s] 65%|██████▍   | 398/615 [04:01<01:01,  3.52it/s] 65%|██████▍   | 399/615 [04:01<01:01,  3.54it/s] 65%|██████▌   | 400/615 [04:01<01:00,  3.55it/s] 65%|██████▌   | 401/615 [04:01<01:00,  3.56it/s] 65%|██████▌   | 402/615 [04:02<00:59,  3.56it/s] 66%|██████▌   | 403/615 [04:02<00:59,  3.57it/s] 66%|██████▌   | 404/615 [04:02<01:04,  3.28it/s] 66%|██████▌   | 405/615 [04:03<01:02,  3.36it/s] 66%|██████▌   | 406/615 [04:03<01:01,  3.43it/s] 66%|██████▌   | 407/615 [04:03<00:59,  3.47it/s] 66%|██████▋   | 408/615 [04:03<00:59,  3.50it/s] 67%|██████▋   | 409/615 [04:04<00:58,  3.52it/s] 67%|██████▋   | 410/615 [04:04<00:58,  3.53it/s] 67%|██████▋   | 411/615 [04:04<00:57,  3.55it/s] 67%|██████▋   | 412/615 [04:05<00:57,  3.55it/s] 67%|██████▋   | 413/615 [04:05<00:56,  3.56it/s] 67%|██████▋   | 414/615 [04:05<00:56,  3.56it/s] 67%|██████▋   | 415/615 [04:05<01:02,  3.21it/s] 68%|██████▊   | 416/615 [04:06<01:00,  3.31it/s] 68%|██████▊   | 417/615 [04:06<00:58,  3.39it/s] 68%|██████▊   | 418/615 [04:06<00:57,  3.44it/s] 68%|██████▊   | 419/615 [04:07<00:56,  3.48it/s] 68%|██████▊   | 420/615 [04:07<00:55,  3.51it/s] 68%|██████▊   | 421/615 [04:07<00:54,  3.53it/s] 69%|██████▊   | 422/615 [04:07<00:54,  3.54it/s] 69%|██████▉   | 423/615 [04:08<00:54,  3.55it/s] 69%|██████▉   | 424/615 [04:08<00:53,  3.56it/s] 69%|██████▉   | 425/615 [04:08<00:53,  3.56it/s] 69%|██████▉   | 426/615 [04:09<01:02,  3.02it/s] 69%|██████▉   | 427/615 [04:09<00:59,  3.16it/s] 70%|██████▉   | 428/615 [04:09<00:57,  3.27it/s] 70%|██████▉   | 429/615 [04:10<00:55,  3.35it/s] 70%|██████▉   | 430/615 [04:10<00:54,  3.41it/s] 70%|███████   | 431/615 [04:10<00:53,  3.45it/s] 70%|███████   | 432/615 [04:10<00:52,  3.48it/s] 70%|███████   | 433/615 [04:11<01:06,  2.74it/s] 71%|███████   | 434/615 [04:11<01:01,  2.94it/s] 71%|███████   | 435/615 [04:12<00:57,  3.10it/s] 71%|███████   | 436/615 [04:12<00:55,  3.23it/s] 71%|███████   | 437/615 [04:12<00:53,  3.32it/s] 71%|███████   | 438/615 [04:12<00:52,  3.39it/s] 71%|███████▏  | 439/615 [04:13<00:51,  3.43it/s] 72%|███████▏  | 440/615 [04:13<00:50,  3.47it/s] 72%|███████▏  | 441/615 [04:13<00:49,  3.50it/s] 72%|███████▏  | 442/615 [04:13<00:49,  3.51it/s] 72%|███████▏  | 443/615 [04:14<00:54,  3.13it/s] 72%|███████▏  | 444/615 [04:14<00:52,  3.25it/s] 72%|███████▏  | 445/615 [04:14<00:50,  3.34it/s] 73%|███████▎  | 446/615 [04:15<00:49,  3.40it/s] 73%|███████▎  | 447/615 [04:15<00:48,  3.45it/s] 73%|███████▎  | 448/615 [04:15<00:47,  3.49it/s] 73%|███████▎  | 449/615 [04:16<00:47,  3.51it/s] 73%|███████▎  | 450/615 [04:16<00:46,  3.53it/s] 73%|███████▎  | 451/615 [04:16<00:46,  3.54it/s] 73%|███████▎  | 452/615 [04:16<00:45,  3.55it/s] 74%|███████▎  | 453/615 [04:17<00:45,  3.55it/s] 74%|███████▍  | 454/615 [04:17<00:49,  3.28it/s] 74%|███████▍  | 455/615 [04:17<00:47,  3.36it/s] 74%|███████▍  | 456/615 [04:18<00:46,  3.42it/s] 74%|███████▍  | 457/615 [04:18<00:45,  3.47it/s] 74%|███████▍  | 458/615 [04:18<00:44,  3.50it/s] 75%|███████▍  | 459/615 [04:18<00:44,  3.52it/s] 75%|███████▍  | 460/615 [04:19<00:43,  3.53it/s] 75%|███████▍  | 461/615 [04:19<00:43,  3.55it/s] 75%|███████▌  | 462/615 [04:19<00:43,  3.55it/s] 75%|███████▌  | 463/615 [04:20<00:42,  3.56it/s] 75%|███████▌  | 464/615 [04:20<00:42,  3.56it/s] 76%|███████▌  | 465/615 [04:20<00:44,  3.40it/s] 76%|███████▌  | 466/615 [04:20<00:43,  3.44it/s] 76%|███████▌  | 467/615 [04:21<00:42,  3.48it/s] 76%|███████▌  | 468/615 [04:21<00:41,  3.50it/s] 76%|███████▋  | 469/615 [04:21<00:41,  3.52it/s] 76%|███████▋  | 470/615 [04:22<00:41,  3.53it/s] 77%|███████▋  | 471/615 [04:22<00:40,  3.54it/s] 77%|███████▋  | 472/615 [04:22<00:40,  3.54it/s] 77%|███████▋  | 473/615 [04:22<00:40,  3.55it/s] 77%|███████▋  | 474/615 [04:23<00:39,  3.55it/s] 77%|███████▋  | 475/615 [04:23<00:39,  3.56it/s] 77%|███████▋  | 476/615 [04:23<00:41,  3.31it/s] 78%|███████▊  | 477/615 [04:24<00:40,  3.38it/s] 78%|███████▊  | 478/615 [04:24<00:39,  3.43it/s] 78%|███████▊  | 479/615 [04:24<00:39,  3.47it/s] 78%|███████▊  | 480/615 [04:24<00:38,  3.50it/s] 78%|███████▊  | 481/615 [04:25<00:38,  3.52it/s] 78%|███████▊  | 482/615 [04:25<00:37,  3.53it/s] 79%|███████▊  | 483/615 [04:25<00:37,  3.54it/s] 79%|███████▊  | 484/615 [04:26<00:36,  3.55it/s] 79%|███████▉  | 485/615 [04:26<00:36,  3.55it/s] 79%|███████▉  | 486/615 [04:26<00:36,  3.56it/s] 79%|███████▉  | 487/615 [04:26<00:38,  3.32it/s] 79%|███████▉  | 488/615 [04:27<00:37,  3.39it/s] 80%|███████▉  | 489/615 [04:27<00:36,  3.44it/s] 80%|███████▉  | 490/615 [04:27<00:36,  3.47it/s] 80%|███████▉  | 491/615 [04:28<00:35,  3.50it/s] 80%|████████  | 492/615 [04:28<00:35,  3.51it/s][INFO|trainer.py:2140] 2023-08-28 05:15:57,961 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:15:57,961 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 05:15:57,961 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.1125, 'eval_samples_per_second': 345.314, 'eval_steps_per_second': 43.214, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.65it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.70it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.24it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.30it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.48it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.89it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.36it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.12it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.18it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.42it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 38.29it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 40.06it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 41.45it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 42.51it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.13it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.68it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.86it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.01it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.73it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.65it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.77it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.14it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.43it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.57it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.69it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.65it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.47it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.13it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.83it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.98it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.27it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.51it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.53it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.68it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.71it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.35it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.12it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 36.30it/s][A
 45%|████▌     | 197/437 [00:04<00:06, 38.54it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 40.19it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 41.38it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 42.40it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.04it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.61it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.80it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.61it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.56it/s][A
 55%|█████▌    | 242/437 [00:05<00:05, 33.28it/s][A
 57%|█████▋    | 247/437 [00:05<00:05, 36.80it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 38.89it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 40.46it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 41.64it/s][A
 61%|██████    | 267/437 [00:06<00:03, 42.63it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.32it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.81it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.86it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.64it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.49it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.78it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.05it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.28it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.50it/s][A
 73%|███████▎  | 317/437 [00:07<00:03, 35.85it/s][A
 74%|███████▎  | 322/437 [00:07<00:03, 38.16it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 40.01it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 41.32it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 42.38it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 43.09it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.67it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.82it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.56it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.33it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.60it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.96it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.24it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.43it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 44.63it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.65it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.48it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.08it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.87it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.04it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.21it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.45it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.63it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.68it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.64it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.64it/s][A 80%|████████  | 492/615 [04:38<00:35,  3.51it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:16:08,683 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 05:16:09,171 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:16:24,894 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:16:25,390 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:16:25,528 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492/special_tokens_map.json
 80%|████████  | 493/615 [04:59<19:31,  9.61s/it] 80%|████████  | 494/615 [05:00<13:46,  6.83s/it] 80%|████████  | 495/615 [05:00<09:44,  4.87s/it] 81%|████████  | 496/615 [05:00<06:55,  3.49s/it] 81%|████████  | 497/615 [05:00<04:58,  2.53s/it] 81%|████████  | 498/615 [05:01<03:36,  1.85s/it] 81%|████████  | 499/615 [05:01<02:40,  1.38s/it] 81%|████████▏ | 500/615 [05:01<02:00,  1.05s/it]                                                  81%|████████▏ | 500/615 [05:01<02:00,  1.05s/it] 81%|████████▏ | 501/615 [05:02<01:33,  1.22it/s] 82%|████████▏ | 502/615 [05:02<01:14,  1.52it/s] 82%|████████▏ | 503/615 [05:02<01:00,  1.84it/s] 82%|████████▏ | 504/615 [05:02<00:51,  2.15it/s] 82%|████████▏ | 505/615 [05:03<00:47,  2.30it/s] 82%|████████▏ | 506/615 [05:03<00:42,  2.57it/s] 82%|████████▏ | 507/615 [05:03<00:38,  2.81it/s] 83%|████████▎ | 508/615 [05:04<00:35,  3.00it/s] 83%|████████▎ | 509/615 [05:04<00:33,  3.15it/s] 83%|████████▎ | 510/615 [05:04<00:32,  3.26it/s] 83%|████████▎ | 511/615 [05:04<00:31,  3.35it/s] 83%|████████▎ | 512/615 [05:05<00:30,  3.41it/s] 83%|████████▎ | 513/615 [05:05<00:29,  3.45it/s] 84%|████████▎ | 514/615 [05:05<00:28,  3.49it/s] 84%|████████▎ | 515/615 [05:06<00:28,  3.51it/s] 84%|████████▍ | 516/615 [05:06<00:30,  3.25it/s] 84%|████████▍ | 517/615 [05:06<00:29,  3.34it/s] 84%|████████▍ | 518/615 [05:06<00:28,  3.40it/s] 84%|████████▍ | 519/615 [05:07<00:27,  3.45it/s] 85%|████████▍ | 520/615 [05:07<00:27,  3.48it/s] 85%|████████▍ | 521/615 [05:07<00:26,  3.51it/s] 85%|████████▍ | 522/615 [05:08<00:26,  3.53it/s] 85%|████████▌ | 523/615 [05:08<00:25,  3.54it/s] 85%|████████▌ | 524/615 [05:08<00:25,  3.55it/s] 85%|████████▌ | 525/615 [05:08<00:25,  3.55it/s] 86%|████████▌ | 526/615 [05:09<00:25,  3.55it/s] 86%|████████▌ | 527/615 [05:09<00:27,  3.23it/s] 86%|████████▌ | 528/615 [05:09<00:26,  3.32it/s] 86%|████████▌ | 529/615 [05:10<00:25,  3.39it/s] 86%|████████▌ | 530/615 [05:10<00:24,  3.44it/s] 86%|████████▋ | 531/615 [05:10<00:24,  3.48it/s] 87%|████████▋ | 532/615 [05:11<00:23,  3.50it/s] 87%|████████▋ | 533/615 [05:11<00:23,  3.52it/s] 87%|████████▋ | 534/615 [05:11<00:22,  3.54it/s] 87%|████████▋ | 535/615 [05:11<00:22,  3.55it/s] 87%|████████▋ | 536/615 [05:12<00:22,  3.55it/s] 87%|████████▋ | 537/615 [05:12<00:21,  3.56it/s] 87%|████████▋ | 538/615 [05:12<00:23,  3.29it/s] 88%|████████▊ | 539/615 [05:13<00:22,  3.37it/s] 88%|████████▊ | 540/615 [05:13<00:21,  3.42it/s] 88%|████████▊ | 541/615 [05:13<00:21,  3.47it/s] 88%|████████▊ | 542/615 [05:13<00:20,  3.50it/s] 88%|████████▊ | 543/615 [05:14<00:20,  3.52it/s] 88%|████████▊ | 544/615 [05:14<00:20,  3.53it/s] 89%|████████▊ | 545/615 [05:14<00:19,  3.55it/s] 89%|████████▉ | 546/615 [05:15<00:19,  3.57it/s] 89%|████████▉ | 547/615 [05:15<00:18,  3.58it/s] 89%|████████▉ | 548/615 [05:15<00:20,  3.19it/s] 89%|████████▉ | 549/615 [05:15<00:19,  3.31it/s] 89%|████████▉ | 550/615 [05:16<00:19,  3.40it/s] 90%|████████▉ | 551/615 [05:16<00:18,  3.46it/s] 90%|████████▉ | 552/615 [05:16<00:17,  3.50it/s] 90%|████████▉ | 553/615 [05:17<00:17,  3.53it/s] 90%|█████████ | 554/615 [05:17<00:17,  3.56it/s] 90%|█████████ | 555/615 [05:17<00:16,  3.58it/s] 90%|█████████ | 556/615 [05:17<00:16,  3.59it/s] 91%|█████████ | 557/615 [05:18<00:16,  3.60it/s] 91%|█████████ | 558/615 [05:18<00:15,  3.60it/s] 91%|█████████ | 559/615 [05:18<00:17,  3.28it/s] 91%|█████████ | 560/615 [05:19<00:16,  3.38it/s] 91%|█████████ | 561/615 [05:19<00:15,  3.44it/s] 91%|█████████▏| 562/615 [05:19<00:15,  3.49it/s] 92%|█████████▏| 563/615 [05:19<00:14,  3.53it/s] 92%|█████████▏| 564/615 [05:20<00:14,  3.55it/s] 92%|█████████▏| 565/615 [05:20<00:13,  3.58it/s] 92%|█████████▏| 566/615 [05:20<00:13,  3.59it/s] 92%|█████████▏| 567/615 [05:21<00:13,  3.60it/s] 92%|█████████▏| 568/615 [05:21<00:13,  3.60it/s] 93%|█████████▎| 569/615 [05:21<00:12,  3.61it/s] 93%|█████████▎| 570/615 [05:21<00:13,  3.29it/s] 93%|█████████▎| 571/615 [05:22<00:13,  3.38it/s] 93%|█████████▎| 572/615 [05:22<00:12,  3.45it/s] 93%|█████████▎| 573/615 [05:22<00:12,  3.50it/s] 93%|█████████▎| 574/615 [05:23<00:11,  3.53it/s] 93%|█████████▎| 575/615 [05:23<00:11,  3.56it/s] 94%|█████████▎| 576/615 [05:23<00:10,  3.58it/s] 94%|█████████▍| 577/615 [05:23<00:10,  3.59it/s] 94%|█████████▍| 578/615 [05:24<00:10,  3.60it/s] 94%|█████████▍| 579/615 [05:24<00:09,  3.60it/s] 94%|█████████▍| 580/615 [05:24<00:09,  3.61it/s] 94%|█████████▍| 581/615 [05:25<00:10,  3.21it/s] 95%|█████████▍| 582/615 [05:25<00:09,  3.32it/s] 95%|█████████▍| 583/615 [05:25<00:09,  3.40it/s] 95%|█████████▍| 584/615 [05:25<00:08,  3.47it/s] 95%|█████████▌| 585/615 [05:26<00:08,  3.52it/s] 95%|█████████▌| 586/615 [05:26<00:08,  3.54it/s] 95%|█████████▌| 587/615 [05:26<00:07,  3.57it/s] 96%|█████████▌| 588/615 [05:27<00:07,  3.58it/s] 96%|█████████▌| 589/615 [05:27<00:07,  3.59it/s] 96%|█████████▌| 590/615 [05:27<00:06,  3.60it/s] 96%|█████████▌| 591/615 [05:27<00:06,  3.61it/s] 96%|█████████▋| 592/615 [05:28<00:07,  3.24it/s] 96%|█████████▋| 593/615 [05:28<00:06,  3.35it/s] 97%|█████████▋| 594/615 [05:28<00:06,  3.42it/s] 97%|█████████▋| 595/615 [05:29<00:05,  3.48it/s] 97%|█████████▋| 596/615 [05:29<00:05,  3.52it/s] 97%|█████████▋| 597/615 [05:29<00:05,  3.55it/s] 97%|█████████▋| 598/615 [05:29<00:04,  3.56it/s] 97%|█████████▋| 599/615 [05:30<00:04,  3.58it/s] 98%|█████████▊| 600/615 [05:30<00:04,  3.59it/s] 98%|█████████▊| 601/615 [05:30<00:03,  3.60it/s] 98%|█████████▊| 602/615 [05:31<00:03,  3.60it/s] 98%|█████████▊| 603/615 [05:31<00:03,  3.33it/s] 98%|█████████▊| 604/615 [05:31<00:03,  3.41it/s] 98%|█████████▊| 605/615 [05:31<00:02,  3.47it/s] 99%|█████████▊| 606/615 [05:32<00:02,  3.51it/s] 99%|█████████▊| 607/615 [05:32<00:02,  3.54it/s] 99%|█████████▉| 608/615 [05:32<00:01,  3.57it/s] 99%|█████████▉| 609/615 [05:33<00:01,  3.58it/s] 99%|█████████▉| 610/615 [05:33<00:01,  3.59it/s] 99%|█████████▉| 611/615 [05:33<00:01,  3.60it/s]100%|█████████▉| 612/615 [05:33<00:00,  3.60it/s]100%|█████████▉| 613/615 [05:34<00:00,  3.61it/s]100%|█████████▉| 614/615 [05:34<00:00,  3.32it/s]100%|██████████| 615/615 [05:34<00:00,  3.41it/s][INFO|trainer.py:2140] 2023-08-28 05:17:04,219 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:17:04,219 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 05:17:04,219 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.1694, 'eval_samples_per_second': 343.383, 'eval_steps_per_second': 42.972, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.95it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.88it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.88it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.76it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.21it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.60it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.36it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.32it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.44it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.54it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.66it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.73it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.71it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.53it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.16it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.18it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.06it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.15it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.38it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.53it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.66it/s][A
 26%|██▌       | 112/437 [00:02<00:08, 37.50it/s][A
 27%|██▋       | 117/437 [00:02<00:08, 39.46it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 40.91it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 41.96it/s][A
 30%|███       | 132/437 [00:03<00:07, 42.89it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.46it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.90it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.01it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.68it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.48it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.77it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.92it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.21it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.35it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.59it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.56it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.38it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.22it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.99it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.02it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.15it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.20it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.37it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.53it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.53it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.58it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 40.12it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 41.38it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 42.25it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 42.90it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 43.36it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.84it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.13it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.26it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.87it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.96it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.11it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.04it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.24it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.41it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.48it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.53it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.25it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.15it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.12it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.10it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.17it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.34it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.38it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.57it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.53it/s][A
 84%|████████▍ | 367/437 [00:08<00:02, 31.41it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 34.49it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 37.09it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 39.17it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 40.65it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 41.80it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 42.74it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.21it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.03it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.15it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.55it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.86it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.09it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.25it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.45it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.45it/s][A100%|██████████| 615/615 [05:44<00:00,  3.41it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 05:17:14,796 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-28 05:17:15,282 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:17:42,008 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:17:42,496 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:17:42,776 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 05:17:46,490 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 05:17:46,566 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123 (score: 1.1054370403289795).
                                                 100%|██████████| 615/615 [06:49<00:00,  3.41it/s]100%|██████████| 615/615 [06:49<00:00,  1.50it/s]
[INFO|trainer.py:1894] 2023-08-28 05:18:18,927 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 05:18:19,242 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 05:18:41,047 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 05:18:41,497 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 05:18:41,694 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:18:43,228 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:43,228 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:43,228 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:43,228 >>   train_runtime            = 0:06:49.10
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:43,228 >>   train_samples            =       7900
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:43,228 >>   train_samples_per_second =     96.553
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:43,228 >>   train_steps_per_second   =      1.503
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.0958, 'eval_samples_per_second': 345.887, 'eval_steps_per_second': 43.285, 'epoch': 5.0}
{'train_runtime': 409.1018, 'train_samples_per_second': 96.553, 'train_steps_per_second': 1.503, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 05:18:44 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 05:18:44,033 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 05:18:44,033 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 05:18:44,033 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:56,  7.69it/s]  3%|▎         | 11/437 [00:00<00:30, 14.19it/s]  4%|▎         | 16/437 [00:00<00:20, 20.29it/s]  5%|▍         | 21/437 [00:01<00:16, 25.70it/s]  6%|▌         | 26/437 [00:01<00:13, 30.25it/s]  7%|▋         | 31/437 [00:01<00:11, 33.96it/s]  8%|▊         | 36/437 [00:01<00:10, 36.98it/s]  9%|▉         | 41/437 [00:01<00:10, 39.25it/s] 11%|█         | 46/437 [00:01<00:09, 40.92it/s] 12%|█▏        | 51/437 [00:01<00:09, 41.78it/s] 13%|█▎        | 56/437 [00:01<00:08, 42.47it/s] 14%|█▍        | 61/437 [00:01<00:08, 43.30it/s] 15%|█▌        | 66/437 [00:02<00:08, 43.87it/s] 16%|█▌        | 71/437 [00:02<00:08, 44.37it/s] 17%|█▋        | 76/437 [00:02<00:08, 44.52it/s] 19%|█▊        | 81/437 [00:02<00:07, 44.74it/s] 20%|█▉        | 86/437 [00:02<00:07, 44.76it/s] 21%|██        | 91/437 [00:02<00:07, 44.76it/s] 22%|██▏       | 96/437 [00:02<00:07, 44.67it/s] 23%|██▎       | 101/437 [00:02<00:07, 44.58it/s] 24%|██▍       | 106/437 [00:02<00:07, 44.63it/s] 25%|██▌       | 111/437 [00:03<00:07, 44.67it/s] 27%|██▋       | 116/437 [00:03<00:07, 44.97it/s] 28%|██▊       | 121/437 [00:03<00:07, 45.06it/s] 29%|██▉       | 126/437 [00:03<00:06, 45.07it/s] 30%|██▉       | 131/437 [00:03<00:06, 45.06it/s] 31%|███       | 136/437 [00:03<00:08, 36.46it/s] 32%|███▏      | 141/437 [00:03<00:07, 38.78it/s] 33%|███▎      | 146/437 [00:03<00:07, 40.52it/s] 35%|███▍      | 151/437 [00:04<00:06, 41.90it/s] 36%|███▌      | 156/437 [00:04<00:06, 42.83it/s] 37%|███▋      | 161/437 [00:04<00:06, 43.65it/s] 38%|███▊      | 166/437 [00:04<00:06, 44.11it/s] 39%|███▉      | 171/437 [00:04<00:05, 44.47it/s] 40%|████      | 176/437 [00:04<00:05, 44.15it/s] 41%|████▏     | 181/437 [00:04<00:05, 43.99it/s] 43%|████▎     | 186/437 [00:04<00:05, 44.06it/s] 44%|████▎     | 191/437 [00:04<00:05, 44.49it/s] 45%|████▍     | 196/437 [00:05<00:05, 44.76it/s] 46%|████▌     | 201/437 [00:05<00:05, 44.95it/s] 47%|████▋     | 206/437 [00:05<00:05, 45.12it/s] 48%|████▊     | 211/437 [00:05<00:05, 45.14it/s] 49%|████▉     | 216/437 [00:05<00:04, 44.93it/s] 51%|█████     | 221/437 [00:05<00:04, 44.61it/s] 52%|█████▏    | 226/437 [00:05<00:04, 44.47it/s] 53%|█████▎    | 231/437 [00:05<00:04, 44.43it/s] 54%|█████▍    | 236/437 [00:05<00:04, 44.63it/s] 55%|█████▌    | 241/437 [00:06<00:04, 44.77it/s] 56%|█████▋    | 246/437 [00:06<00:04, 45.07it/s] 57%|█████▋    | 251/437 [00:06<00:04, 45.13it/s] 59%|█████▊    | 256/437 [00:06<00:04, 45.09it/s] 60%|█████▉    | 261/437 [00:06<00:03, 44.83it/s] 61%|██████    | 266/437 [00:06<00:03, 44.42it/s] 62%|██████▏   | 271/437 [00:06<00:04, 35.87it/s] 63%|██████▎   | 276/437 [00:06<00:04, 38.28it/s] 64%|██████▍   | 281/437 [00:07<00:03, 40.19it/s] 65%|██████▌   | 286/437 [00:07<00:03, 41.65it/s] 67%|██████▋   | 291/437 [00:07<00:03, 42.67it/s] 68%|██████▊   | 296/437 [00:07<00:03, 43.54it/s] 69%|██████▉   | 301/437 [00:07<00:03, 44.06it/s] 70%|███████   | 306/437 [00:07<00:02, 44.30it/s] 71%|███████   | 311/437 [00:07<00:02, 43.97it/s] 72%|███████▏  | 316/437 [00:07<00:02, 43.85it/s] 73%|███████▎  | 321/437 [00:07<00:02, 44.01it/s] 75%|███████▍  | 326/437 [00:08<00:02, 44.37it/s] 76%|███████▌  | 331/437 [00:08<00:02, 44.67it/s] 77%|███████▋  | 336/437 [00:08<00:02, 44.93it/s] 78%|███████▊  | 341/437 [00:08<00:02, 45.02it/s] 79%|███████▉  | 346/437 [00:08<00:02, 45.03it/s] 80%|████████  | 351/437 [00:08<00:01, 44.69it/s] 81%|████████▏ | 356/437 [00:08<00:01, 44.31it/s] 83%|████████▎ | 361/437 [00:08<00:01, 44.26it/s] 84%|████████▍ | 366/437 [00:08<00:01, 44.34it/s] 85%|████████▍ | 371/437 [00:09<00:01, 44.55it/s] 86%|████████▌ | 376/437 [00:09<00:01, 44.67it/s] 87%|████████▋ | 381/437 [00:09<00:01, 44.79it/s] 88%|████████▊ | 386/437 [00:09<00:01, 45.00it/s] 89%|████████▉ | 391/437 [00:09<00:01, 45.11it/s] 91%|█████████ | 396/437 [00:09<00:00, 44.94it/s] 92%|█████████▏| 401/437 [00:09<00:00, 44.57it/s] 93%|█████████▎| 406/437 [00:09<00:00, 32.99it/s] 94%|█████████▍| 411/437 [00:10<00:00, 35.84it/s] 95%|█████████▌| 416/437 [00:10<00:00, 38.28it/s] 96%|█████████▋| 421/437 [00:10<00:00, 40.03it/s] 97%|█████████▋| 426/437 [00:10<00:00, 41.54it/s] 99%|█████████▊| 431/437 [00:10<00:00, 42.62it/s]100%|█████████▉| 436/437 [00:10<00:00, 43.49it/s]100%|██████████| 437/437 [00:10<00:00, 40.91it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 05:18:54,839 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:54,839 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:54,839 >>   eval_loss               =     1.1054
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:54,839 >>   eval_runtime            = 0:00:10.80
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:54,839 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:54,839 >>   eval_samples_per_second =    323.138
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:54,839 >>   eval_steps_per_second   =     40.439
[INFO|trainer_pt_utils.py:913] 2023-08-28 05:18:54,839 >>   perplexity              =     3.0205
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:18,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:18,287 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:18,287 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:18,287 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:18,287 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:19:19,554 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:19:19,621 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:19:20,334 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:19:21,583 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:19:21,584 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:24,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:24,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:24,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:24,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:19:24,975 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:19:25,982 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:19:25,983 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:19:26,745 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:19:27,119 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:19:27,119 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-615
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-123
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/checkpoint-369
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.59it/s]Extractor Predicting: 8it [00:04,  1.60it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:09,  1.65it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.67it/s]Extractor Predicting: 23it [00:14,  1.69it/s]Extractor Predicting: 24it [00:14,  1.71it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.66it/s]Extractor Predicting: 28it [00:17,  1.66it/s]Extractor Predicting: 29it [00:17,  1.68it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:19,  1.61it/s]Extractor Predicting: 32it [00:19,  1.64it/s]Extractor Predicting: 33it [00:20,  1.66it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.62it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:24,  1.56it/s]Extractor Predicting: 41it [00:25,  1.59it/s]Extractor Predicting: 42it [00:25,  1.60it/s]Extractor Predicting: 43it [00:26,  1.64it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:27,  1.53it/s]Extractor Predicting: 46it [00:28,  1.45it/s]Extractor Predicting: 47it [00:29,  1.52it/s]Extractor Predicting: 48it [00:29,  1.55it/s]Extractor Predicting: 49it [00:30,  1.59it/s]Extractor Predicting: 50it [00:31,  1.50it/s]Extractor Predicting: 51it [00:31,  1.51it/s]Extractor Predicting: 52it [00:32,  1.54it/s]Extractor Predicting: 53it [00:32,  1.57it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:34,  1.56it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.61it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:36,  1.64it/s]Extractor Predicting: 60it [00:37,  1.56it/s]Extractor Predicting: 61it [00:37,  1.56it/s]Extractor Predicting: 62it [00:38,  1.58it/s]Extractor Predicting: 63it [00:39,  1.61it/s]Extractor Predicting: 64it [00:39,  1.57it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:41,  1.57it/s]Extractor Predicting: 67it [00:41,  1.57it/s]Extractor Predicting: 68it [00:42,  1.52it/s]Extractor Predicting: 69it [00:43,  1.54it/s]Extractor Predicting: 70it [00:43,  1.53it/s]Extractor Predicting: 71it [00:44,  1.54it/s]Extractor Predicting: 72it [00:45,  1.53it/s]Extractor Predicting: 73it [00:45,  1.50it/s]Extractor Predicting: 74it [00:46,  1.48it/s]Extractor Predicting: 75it [00:47,  1.54it/s]Extractor Predicting: 76it [00:47,  1.56it/s]Extractor Predicting: 77it [00:48,  1.55it/s]Extractor Predicting: 78it [00:49,  1.51it/s]Extractor Predicting: 79it [00:49,  1.50it/s]Extractor Predicting: 80it [00:50,  1.49it/s]Extractor Predicting: 81it [00:50,  1.51it/s]Extractor Predicting: 82it [00:51,  1.55it/s]Extractor Predicting: 83it [00:52,  1.50it/s]Extractor Predicting: 84it [00:52,  1.55it/s]Extractor Predicting: 85it [00:53,  1.56it/s]Extractor Predicting: 86it [00:54,  1.53it/s]Extractor Predicting: 87it [00:54,  1.56it/s]Extractor Predicting: 88it [00:55,  1.49it/s]Extractor Predicting: 89it [00:56,  1.58it/s]Extractor Predicting: 90it [00:56,  1.60it/s]Extractor Predicting: 91it [00:57,  1.68it/s]Extractor Predicting: 92it [00:57,  1.75it/s]Extractor Predicting: 93it [00:58,  1.81it/s]Extractor Predicting: 94it [00:58,  1.76it/s]Extractor Predicting: 95it [00:59,  1.72it/s]Extractor Predicting: 96it [01:00,  1.77it/s]Extractor Predicting: 97it [01:00,  1.74it/s]Extractor Predicting: 98it [01:01,  1.74it/s]Extractor Predicting: 99it [01:01,  1.82it/s]Extractor Predicting: 100it [01:02,  1.73it/s]Extractor Predicting: 101it [01:02,  1.73it/s]Extractor Predicting: 102it [01:03,  1.69it/s]Extractor Predicting: 103it [01:04,  1.70it/s]Extractor Predicting: 104it [01:04,  1.66it/s]Extractor Predicting: 105it [01:05,  1.59it/s]Extractor Predicting: 106it [01:06,  1.64it/s]Extractor Predicting: 107it [01:06,  1.64it/s]Extractor Predicting: 108it [01:07,  1.69it/s]Extractor Predicting: 109it [01:07,  1.72it/s]Extractor Predicting: 110it [01:08,  1.75it/s]Extractor Predicting: 111it [01:08,  1.68it/s]Extractor Predicting: 112it [01:09,  1.72it/s]Extractor Predicting: 113it [01:09,  1.77it/s]Extractor Predicting: 114it [01:10,  1.73it/s]Extractor Predicting: 115it [01:11,  1.76it/s]Extractor Predicting: 116it [01:11,  1.73it/s]Extractor Predicting: 117it [01:12,  1.70it/s]Extractor Predicting: 118it [01:13,  1.65it/s]Extractor Predicting: 119it [01:13,  1.63it/s]Extractor Predicting: 120it [01:14,  1.61it/s]Extractor Predicting: 121it [01:14,  1.63it/s]Extractor Predicting: 122it [01:15,  1.64it/s]Extractor Predicting: 123it [01:16,  1.56it/s]Extractor Predicting: 124it [01:16,  1.55it/s]Extractor Predicting: 125it [01:17,  1.58it/s]Extractor Predicting: 126it [01:18,  1.57it/s]Extractor Predicting: 127it [01:18,  1.58it/s]Extractor Predicting: 128it [01:19,  1.54it/s]Extractor Predicting: 129it [01:20,  1.57it/s]Extractor Predicting: 130it [01:20,  1.63it/s]Extractor Predicting: 131it [01:21,  1.59it/s]Extractor Predicting: 132it [01:21,  1.59it/s]Extractor Predicting: 133it [01:22,  1.37it/s]Extractor Predicting: 134it [01:23,  1.44it/s]Extractor Predicting: 135it [01:24,  1.49it/s]Extractor Predicting: 136it [01:24,  1.52it/s]Extractor Predicting: 137it [01:25,  1.54it/s]Extractor Predicting: 138it [01:26,  1.50it/s]Extractor Predicting: 139it [01:26,  1.54it/s]Extractor Predicting: 140it [01:27,  1.59it/s]Extractor Predicting: 141it [01:27,  1.57it/s]Extractor Predicting: 142it [01:28,  1.58it/s]Extractor Predicting: 143it [01:29,  1.54it/s]Extractor Predicting: 144it [01:29,  1.61it/s]Extractor Predicting: 144it [01:29,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:48,758 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:48,827 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:48,828 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:48,828 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:48,828 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:22:50,183 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:22:50,184 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:22:50,870 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:22:52,059 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:22:52,113 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:55,366 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:55,420 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:55,420 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:55,421 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:22:55,421 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:22:56,394 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:22:56,395 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:22:57,070 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:22:57,333 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:22:57,333 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.64it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.63it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:08,  1.64it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.71it/s]Extractor Predicting: 16it [00:09,  1.74it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:10,  1.64it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.60it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:15,  1.68it/s]Extractor Predicting: 26it [00:15,  1.66it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.65it/s]Extractor Predicting: 29it [00:17,  1.56it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:20,  1.60it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:22,  1.59it/s]Extractor Predicting: 38it [00:23,  1.63it/s]Extractor Predicting: 39it [00:24,  1.59it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.67it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.68it/s]Extractor Predicting: 44it [00:26,  1.70it/s]Extractor Predicting: 45it [00:27,  1.63it/s]Extractor Predicting: 46it [00:28,  1.68it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:29,  1.71it/s]Extractor Predicting: 49it [00:29,  1.70it/s]Extractor Predicting: 50it [00:30,  1.69it/s]Extractor Predicting: 51it [00:31,  1.62it/s]Extractor Predicting: 52it [00:31,  1.65it/s]Extractor Predicting: 53it [00:32,  1.66it/s]Extractor Predicting: 54it [00:32,  1.68it/s]Extractor Predicting: 55it [00:33,  1.66it/s]Extractor Predicting: 56it [00:34,  1.59it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:35,  1.70it/s]Extractor Predicting: 59it [00:35,  1.76it/s]Extractor Predicting: 60it [00:36,  1.75it/s]Extractor Predicting: 61it [00:37,  1.74it/s]Extractor Predicting: 62it [00:37,  1.62it/s]Extractor Predicting: 63it [00:38,  1.66it/s]Extractor Predicting: 64it [00:38,  1.64it/s]Extractor Predicting: 65it [00:39,  1.66it/s]Extractor Predicting: 66it [00:40,  1.66it/s]Extractor Predicting: 67it [00:40,  1.61it/s]Extractor Predicting: 68it [00:41,  1.70it/s]Extractor Predicting: 69it [00:41,  1.70it/s]Extractor Predicting: 70it [00:42,  1.71it/s]Extractor Predicting: 71it [00:43,  1.72it/s]Extractor Predicting: 72it [00:43,  1.72it/s]Extractor Predicting: 73it [00:44,  1.62it/s]Extractor Predicting: 74it [00:44,  1.63it/s]Extractor Predicting: 75it [00:45,  1.66it/s]Extractor Predicting: 76it [00:46,  1.68it/s]Extractor Predicting: 77it [00:46,  1.66it/s]Extractor Predicting: 78it [00:47,  1.68it/s]Extractor Predicting: 79it [00:47,  1.69it/s]Extractor Predicting: 80it [00:48,  1.63it/s]Extractor Predicting: 81it [00:49,  1.64it/s]Extractor Predicting: 82it [00:49,  1.64it/s]Extractor Predicting: 83it [00:50,  1.67it/s]Extractor Predicting: 84it [00:50,  1.68it/s]Extractor Predicting: 85it [00:51,  1.64it/s]Extractor Predicting: 86it [00:52,  1.64it/s]Extractor Predicting: 87it [00:52,  1.65it/s]Extractor Predicting: 88it [00:53,  1.66it/s]Extractor Predicting: 89it [00:53,  1.70it/s]Extractor Predicting: 90it [00:54,  1.70it/s]Extractor Predicting: 91it [00:55,  1.60it/s]Extractor Predicting: 92it [00:55,  1.60it/s]Extractor Predicting: 93it [00:56,  1.66it/s]Extractor Predicting: 94it [00:56,  1.64it/s]Extractor Predicting: 95it [00:57,  1.65it/s]Extractor Predicting: 96it [00:58,  1.60it/s]Extractor Predicting: 97it [00:58,  1.64it/s]Extractor Predicting: 98it [00:59,  1.65it/s]Extractor Predicting: 99it [00:59,  1.68it/s]Extractor Predicting: 100it [01:00,  1.71it/s]Extractor Predicting: 101it [01:01,  1.73it/s]Extractor Predicting: 102it [01:01,  1.65it/s]Extractor Predicting: 103it [01:02,  1.63it/s]Extractor Predicting: 104it [01:03,  1.63it/s]Extractor Predicting: 105it [01:03,  1.65it/s]Extractor Predicting: 106it [01:04,  1.66it/s]Extractor Predicting: 107it [01:04,  1.57it/s]Extractor Predicting: 108it [01:05,  1.62it/s]Extractor Predicting: 109it [01:06,  1.63it/s]Extractor Predicting: 110it [01:06,  1.66it/s]Extractor Predicting: 111it [01:07,  1.64it/s]Extractor Predicting: 112it [01:08,  1.55it/s]Extractor Predicting: 113it [01:08,  1.57it/s]Extractor Predicting: 114it [01:09,  1.58it/s]Extractor Predicting: 115it [01:09,  1.60it/s]Extractor Predicting: 116it [01:10,  1.66it/s]Extractor Predicting: 117it [01:11,  1.61it/s]Extractor Predicting: 118it [01:11,  1.64it/s]Extractor Predicting: 119it [01:12,  1.66it/s]Extractor Predicting: 120it [01:12,  1.66it/s]Extractor Predicting: 121it [01:13,  1.70it/s]Extractor Predicting: 122it [01:13,  1.72it/s]Extractor Predicting: 123it [01:14,  1.68it/s]Extractor Predicting: 124it [01:15,  1.71it/s]Extractor Predicting: 125it [01:15,  1.69it/s]Extractor Predicting: 126it [01:16,  1.70it/s]Extractor Predicting: 127it [01:16,  1.73it/s]Extractor Predicting: 128it [01:17,  1.72it/s]Extractor Predicting: 129it [01:18,  1.69it/s]Extractor Predicting: 130it [01:18,  1.61it/s]Extractor Predicting: 131it [01:19,  1.67it/s]Extractor Predicting: 132it [01:19,  1.68it/s]Extractor Predicting: 133it [01:20,  1.71it/s]Extractor Predicting: 134it [01:21,  1.72it/s]Extractor Predicting: 135it [01:21,  1.55it/s]Extractor Predicting: 136it [01:22,  1.54it/s]Extractor Predicting: 137it [01:23,  1.60it/s]Extractor Predicting: 138it [01:23,  1.61it/s]Extractor Predicting: 139it [01:24,  1.65it/s]Extractor Predicting: 140it [01:24,  1.73it/s]Extractor Predicting: 141it [01:25,  1.74it/s]Extractor Predicting: 142it [01:26,  1.67it/s]Extractor Predicting: 143it [01:26,  1.67it/s]Extractor Predicting: 144it [01:27,  1.69it/s]Extractor Predicting: 145it [01:27,  1.71it/s]Extractor Predicting: 146it [01:28,  1.75it/s]Extractor Predicting: 147it [01:28,  1.76it/s]Extractor Predicting: 148it [01:29,  1.67it/s]Extractor Predicting: 149it [01:30,  1.71it/s]Extractor Predicting: 150it [01:30,  1.71it/s]Extractor Predicting: 151it [01:31,  1.72it/s]Extractor Predicting: 152it [01:31,  1.73it/s]Extractor Predicting: 153it [01:32,  1.70it/s]Extractor Predicting: 154it [01:33,  1.65it/s]Extractor Predicting: 155it [01:33,  1.68it/s]Extractor Predicting: 156it [01:34,  1.73it/s]Extractor Predicting: 157it [01:34,  1.75it/s]Extractor Predicting: 158it [01:35,  1.82it/s]Extractor Predicting: 159it [01:35,  1.78it/s]Extractor Predicting: 160it [01:36,  1.68it/s]Extractor Predicting: 161it [01:37,  1.67it/s]Extractor Predicting: 162it [01:37,  1.67it/s]Extractor Predicting: 163it [01:38,  1.71it/s]Extractor Predicting: 164it [01:38,  1.72it/s]Extractor Predicting: 165it [01:39,  1.65it/s]Extractor Predicting: 166it [01:40,  1.70it/s]Extractor Predicting: 167it [01:40,  1.72it/s]Extractor Predicting: 168it [01:41,  1.68it/s]Extractor Predicting: 169it [01:41,  1.69it/s]Extractor Predicting: 170it [01:42,  1.69it/s]Extractor Predicting: 171it [01:43,  1.67it/s]Extractor Predicting: 172it [01:43,  1.69it/s]Extractor Predicting: 173it [01:44,  1.69it/s]Extractor Predicting: 174it [01:44,  1.68it/s]Extractor Predicting: 175it [01:45,  1.66it/s]Extractor Predicting: 176it [01:46,  1.55it/s]Extractor Predicting: 177it [01:46,  1.63it/s]Extractor Predicting: 178it [01:47,  1.64it/s]Extractor Predicting: 179it [01:47,  1.69it/s]Extractor Predicting: 180it [01:48,  1.73it/s]Extractor Predicting: 181it [01:49,  1.69it/s]Extractor Predicting: 182it [01:49,  1.65it/s]Extractor Predicting: 183it [01:50,  1.65it/s]Extractor Predicting: 184it [01:50,  1.65it/s]Extractor Predicting: 185it [01:51,  1.65it/s]Extractor Predicting: 186it [01:52,  1.66it/s]Extractor Predicting: 187it [01:52,  1.61it/s]Extractor Predicting: 188it [01:53,  1.61it/s]Extractor Predicting: 189it [01:53,  1.62it/s]Extractor Predicting: 190it [01:54,  1.64it/s]Extractor Predicting: 191it [01:55,  1.68it/s]Extractor Predicting: 192it [01:55,  1.61it/s]Extractor Predicting: 193it [01:56,  1.58it/s]Extractor Predicting: 194it [01:57,  1.59it/s]Extractor Predicting: 195it [01:57,  1.65it/s]Extractor Predicting: 196it [01:58,  1.67it/s]Extractor Predicting: 197it [01:58,  1.59it/s]Extractor Predicting: 198it [01:59,  1.60it/s]Extractor Predicting: 199it [02:00,  1.65it/s]Extractor Predicting: 200it [02:00,  1.73it/s]Extractor Predicting: 201it [02:01,  1.75it/s]Extractor Predicting: 202it [02:01,  1.77it/s]Extractor Predicting: 203it [02:02,  1.65it/s]Extractor Predicting: 204it [02:03,  1.67it/s]Extractor Predicting: 205it [02:03,  1.70it/s]Extractor Predicting: 206it [02:04,  1.69it/s]Extractor Predicting: 207it [02:04,  1.71it/s]Extractor Predicting: 208it [02:05,  1.61it/s]Extractor Predicting: 209it [02:06,  1.65it/s]Extractor Predicting: 210it [02:06,  1.66it/s]Extractor Predicting: 211it [02:07,  1.68it/s]Extractor Predicting: 212it [02:07,  1.70it/s]Extractor Predicting: 213it [02:08,  1.60it/s]Extractor Predicting: 214it [02:09,  1.61it/s]Extractor Predicting: 215it [02:09,  1.63it/s]Extractor Predicting: 216it [02:10,  1.68it/s]Extractor Predicting: 217it [02:10,  1.70it/s]Extractor Predicting: 218it [02:11,  1.64it/s]Extractor Predicting: 219it [02:12,  1.64it/s]Extractor Predicting: 220it [02:12,  1.68it/s]Extractor Predicting: 221it [02:13,  1.69it/s]Extractor Predicting: 222it [02:13,  1.69it/s]Extractor Predicting: 223it [02:14,  1.64it/s]Extractor Predicting: 224it [02:15,  1.67it/s]Extractor Predicting: 225it [02:15,  1.67it/s]Extractor Predicting: 226it [02:16,  1.67it/s]Extractor Predicting: 227it [02:16,  1.69it/s]Extractor Predicting: 228it [02:17,  1.71it/s]Extractor Predicting: 229it [02:17,  1.73it/s]Extractor Predicting: 230it [02:18,  1.74it/s]Extractor Predicting: 231it [02:19,  1.72it/s]Extractor Predicting: 232it [02:19,  1.72it/s]Extractor Predicting: 233it [02:20,  1.67it/s]Extractor Predicting: 234it [02:20,  1.70it/s]Extractor Predicting: 235it [02:21,  1.72it/s]Extractor Predicting: 236it [02:22,  1.73it/s]Extractor Predicting: 237it [02:22,  1.54it/s]Extractor Predicting: 238it [02:23,  1.55it/s]Extractor Predicting: 239it [02:24,  1.58it/s]Extractor Predicting: 240it [02:24,  1.65it/s]Extractor Predicting: 241it [02:25,  1.67it/s]Extractor Predicting: 242it [02:25,  1.72it/s]Extractor Predicting: 243it [02:26,  1.72it/s]Extractor Predicting: 244it [02:26,  1.67it/s]Extractor Predicting: 245it [02:27,  1.68it/s]Extractor Predicting: 246it [02:28,  1.69it/s]Extractor Predicting: 247it [02:28,  1.72it/s]Extractor Predicting: 248it [02:29,  1.73it/s]Extractor Predicting: 249it [02:29,  1.74it/s]Extractor Predicting: 250it [02:30,  1.71it/s]Extractor Predicting: 251it [02:31,  1.67it/s]Extractor Predicting: 252it [02:31,  1.68it/s]Extractor Predicting: 253it [02:32,  1.76it/s]Extractor Predicting: 254it [02:32,  1.74it/s]Extractor Predicting: 255it [02:33,  1.69it/s]Extractor Predicting: 256it [02:34,  1.63it/s]Extractor Predicting: 257it [02:34,  1.67it/s]Extractor Predicting: 258it [02:35,  1.72it/s]Extractor Predicting: 259it [02:35,  1.75it/s]Extractor Predicting: 260it [02:36,  1.77it/s]Extractor Predicting: 261it [02:36,  1.74it/s]Extractor Predicting: 262it [02:37,  1.67it/s]Extractor Predicting: 263it [02:38,  1.70it/s]Extractor Predicting: 264it [02:38,  1.72it/s]Extractor Predicting: 265it [02:39,  1.75it/s]Extractor Predicting: 266it [02:39,  1.80it/s]Extractor Predicting: 267it [02:40,  1.75it/s]Extractor Predicting: 268it [02:40,  1.65it/s]Extractor Predicting: 269it [02:41,  1.70it/s]Extractor Predicting: 270it [02:42,  1.72it/s]Extractor Predicting: 271it [02:42,  1.77it/s]Extractor Predicting: 272it [02:43,  1.81it/s]Extractor Predicting: 273it [02:43,  1.78it/s]Extractor Predicting: 274it [02:44,  1.71it/s]Extractor Predicting: 275it [02:44,  1.72it/s]Extractor Predicting: 276it [02:45,  1.74it/s]Extractor Predicting: 277it [02:46,  1.75it/s]Extractor Predicting: 278it [02:46,  1.74it/s]Extractor Predicting: 279it [02:47,  1.72it/s]Extractor Predicting: 280it [02:47,  1.64it/s]Extractor Predicting: 281it [02:48,  1.65it/s]Extractor Predicting: 281it [02:48,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:12,227 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:12,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:12,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:12,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:12,299 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:26:13,455 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:26:13,456 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:26:14,219 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:26:15,454 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:26:15,521 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:19,025 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:19,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:19,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:19,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:26:19,106 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:26:20,092 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:26:20,093 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:26:20,769 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:26:21,077 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:26:21,077 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.98it/s]Extractor Predicting: 6it [00:03,  1.70it/s]
[INFO|configuration_utils.py:515] 2023-08-28 05:26:28,536 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:26:28,537 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 05:26:28,705 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:26:28,706 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 05:26:28,789 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 05:27:03,444 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 05:27:03,523 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 05:27:05,221 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 05:27:05,222 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 05:27:05,793 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:06,086 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:06,086 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:06,086 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:06,086 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:06,086 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 05:27:06,086 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 05:27:07,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:08,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:08,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:09,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:10,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:11,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:11,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:12,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:13,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:14,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:15,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:15,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:16,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:17,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:18,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:18,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:19,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:20,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:21,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:22,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:22,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:23,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:56, 16.86s/it][WARNING|generation_utils.py:914] 2023-08-28 05:27:24,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:24,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:25,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:26,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:27,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:27,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:28,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:29,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:30,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:30,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:31,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:32,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:32,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:33,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:34,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:35,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:36,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:36,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:37,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:38,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:38,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:39,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:40,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:41,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:34<03:43, 17.19s/it][WARNING|generation_utils.py:914] 2023-08-28 05:27:41,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:42,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:43,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:43,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:44,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:45,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:45,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:46,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:47,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:48,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:48,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:49,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:49,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:50,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:51,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:51,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:52,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:53,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:54,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:54,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:55,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:56,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:57,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:57,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:58,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:27:58,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:52<03:29, 17.49s/it][WARNING|generation_utils.py:914] 2023-08-28 05:27:59,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:00,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:00,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:01,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:02,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:02,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:03,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:04,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:04,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:05,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:05,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:06,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:07,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:07,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:08,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:09,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:09,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:10,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:11,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:11,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:12,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:13,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:13,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:14,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:07<03:03, 16.68s/it][WARNING|generation_utils.py:914] 2023-08-28 05:28:15,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:15,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:16,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:17,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:18,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:18,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:19,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:19,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:20,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:21,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:22,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:22,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:23,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:24,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:25,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:25,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:26,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:27,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:28,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:29,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:29,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:30,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:31,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:31,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:32,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:32,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:26<02:53, 17.37s/it][WARNING|generation_utils.py:914] 2023-08-28 05:28:33,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:34,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:34,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:35,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:36,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:36,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:37,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:38,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:39,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:39,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:40,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:41,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:41,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:42,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:43,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:44,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:44,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:45,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:46,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:46,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:47,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:48,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:49,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:49,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:50,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:51,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:51,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:45<02:42, 18.02s/it][WARNING|generation_utils.py:914] 2023-08-28 05:28:52,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:53,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:54,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:54,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:55,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:56,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:56,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:57,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:58,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:58,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:28:59,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:00,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:00,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:01,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:02,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:02,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:03,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:04,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:05,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:05,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:06,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:07,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:08,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:08,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:09,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:02<02:22, 17.76s/it][WARNING|generation_utils.py:914] 2023-08-28 05:29:10,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:10,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:11,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:12,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:12,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:13,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:14,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:15,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:15,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:16,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:17,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:17,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:18,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:19,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:20,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:21,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:21,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:22,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:23,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:24,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:24,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:25,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:26,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:27,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:20<02:04, 17.72s/it][WARNING|generation_utils.py:914] 2023-08-28 05:29:27,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:28,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:29,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:29,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:30,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:31,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:31,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:32,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:33,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:33,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:34,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:35,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:35,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:36,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:37,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:37,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:38,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:39,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:39,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:40,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:41,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:41,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:42,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:43,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:43,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:37<01:44, 17.45s/it][WARNING|generation_utils.py:914] 2023-08-28 05:29:44,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:45,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:46,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:46,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:47,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:48,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:48,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:49,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:49,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:50,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:51,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:51,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:52,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:53,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:53,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:54,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:55,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:55,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:56,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:57,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:57,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:58,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:29:59,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:52<01:23, 16.76s/it][WARNING|generation_utils.py:914] 2023-08-28 05:29:59,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:00,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:01,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:01,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:02,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:03,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:04,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:04,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:05,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:06,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:06,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:07,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:08,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:09,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:10,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:11,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:11,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:12,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:13,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:13,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:14,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:15,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:15,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:09<01:07, 16.76s/it][WARNING|generation_utils.py:914] 2023-08-28 05:30:16,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:17,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:17,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:18,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:19,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:19,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:20,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:21,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:21,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:22,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:23,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:23,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:24,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:25,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:25,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:26,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:27,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:28,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:29,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:29,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:30,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:31,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:31,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:25<00:49, 16.51s/it][WARNING|generation_utils.py:914] 2023-08-28 05:30:32,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:33,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:33,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:34,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:35,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:35,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:36,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:37,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:38,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:38,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:39,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:40,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:40,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:41,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:42,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:42,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:43,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:44,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:45,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:45,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:46,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:47,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:48,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:48,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:49,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:49,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:50,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:51,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:44<00:34, 17.39s/it][WARNING|generation_utils.py:914] 2023-08-28 05:30:51,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:52,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:53,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:53,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:54,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:55,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:55,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:56,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:57,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:57,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:58,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:30:59,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:00,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:00,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:01,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:02,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:02,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:03,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:04,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:04,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:05,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:06,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:06,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:07,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:08,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:08,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:09,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:09,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:10,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:11,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:11,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:12,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:13,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:06<00:18, 18.73s/it][WARNING|generation_utils.py:914] 2023-08-28 05:31:13,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:14,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:15,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:16,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:17,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:18,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:18,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:19,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:20,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:21,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:22,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:22,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:23,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:24,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:25,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:26,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:27,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:28,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:29,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:30,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:30,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:31,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:32,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:33,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:34,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:35,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 05:31:36,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:29<00:00, 20.04s/it]Generating: 100%|██████████| 15/15 [04:29<00:00, 17.96s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:48,925 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:49,011 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:49,011 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:49,011 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:49,011 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:31:50,277 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:31:50,278 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:31:51,096 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:31:52,341 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:31:52,408 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:56,209 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:56,279 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:31:56,280 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:31:57,165 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:31:57,166 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:31:57,650 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:31:57,997 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:31:57,997 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : main subject . Context : Later in the year ( 1143–46 ) , he married daughter of Louis XIV and Catherine I of Prussia married to Marie Antoinette III , daughter of Emperor Louis XII and Catherine of Rheims . Head Entity : Catherine I , Tail Entity : Emperor Louis XII and Catherine I of Prussia .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7451923076923077, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.81640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.7391826923076923, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 294, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7013888888888888, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amadec for the first time . Head Entity : Amadec , Tail Entity : Amadeca .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.75625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Paul Groening', 'nominated for', '', 'After a stint in the Swedish music industry in 2002 alongside the late Paul Groening , he moved away to New Zealand to continue his education in the country .')"}}
['Relation : operating system . Context : The CVRN ( Computer Vision and Imaging Systems ) is a digital camera developed at the National Institutes of Health . Head Entity : CVRN , Tail Entity : CVR , Head Entity : National Institutes of Health .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 258, 'raw': 384}
{'target': 600, 'success': 280, 'raw': 416}
{'target': 600, 'success': 303, 'raw': 448}
{'target': 600, 'success': 326, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 372, 'raw': 544}
{'target': 600, 'success': 394, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 527, 'raw': 768}
{'target': 600, 'success': 551, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 598, 'raw': 864}
{'target': 600, 'success': 620, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6919642857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 107, 'raw': 192}
{'target': 600, 'success': 127, 'raw': 224}
{'target': 600, 'success': 148, 'raw': 256}
{'target': 600, 'success': 172, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 243, 'raw': 416}
{'target': 600, 'success': 262, 'raw': 448}
{'target': 600, 'success': 279, 'raw': 480}
{'target': 600, 'success': 298, 'raw': 512}
{'target': 600, 'success': 317, 'raw': 544}
{'target': 600, 'success': 332, 'raw': 576}
{'target': 600, 'success': 350, 'raw': 608}
{'target': 600, 'success': 365, 'raw': 640}
{'target': 600, 'success': 388, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 429, 'raw': 736}
{'target': 600, 'success': 446, 'raw': 768}
{'target': 600, 'success': 466, 'raw': 800}
{'target': 600, 'success': 482, 'raw': 832}
{'target': 600, 'success': 495, 'raw': 864}
{'target': 600, 'success': 509, 'raw': 896}
{'target': 600, 'success': 522, 'raw': 928}
{'target': 600, 'success': 543, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 584, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5681818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n']
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n', "Relation : religion . Context : After the death of King Henry IV of France ( c. 589 - 7 February 1235 ) , St Peter was succeeded as Archbishop by the eponymous St Peter 's successor , the first Pope . Head Entity : St Peter 's successors , Tail Entity : St Peter 's .\n"]
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 369, 'raw': 544}
{'target': 600, 'success': 391, 'raw': 576}
{'target': 600, 'success': 415, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 459, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 534, 'raw': 768}
{'target': 600, 'success': 553, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 15147
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15247, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.46it/s]Extractor Estimating: 2it [00:01,  1.31it/s]Extractor Estimating: 3it [00:02,  1.31it/s]Extractor Estimating: 4it [00:02,  1.41it/s]Extractor Estimating: 5it [00:03,  1.44it/s]Extractor Estimating: 6it [00:04,  1.52it/s]Extractor Estimating: 7it [00:04,  1.49it/s]Extractor Estimating: 8it [00:05,  1.43it/s]Extractor Estimating: 9it [00:06,  1.51it/s]Extractor Estimating: 10it [00:06,  1.48it/s]Extractor Estimating: 11it [00:07,  1.53it/s]Extractor Estimating: 12it [00:08,  1.46it/s]Extractor Estimating: 13it [00:09,  1.38it/s]Extractor Estimating: 14it [00:09,  1.42it/s]Extractor Estimating: 15it [00:10,  1.48it/s]Extractor Estimating: 16it [00:11,  1.49it/s]Extractor Estimating: 17it [00:11,  1.51it/s]Extractor Estimating: 18it [00:12,  1.51it/s]Extractor Estimating: 19it [00:13,  1.46it/s]Extractor Estimating: 20it [00:13,  1.47it/s]Extractor Estimating: 21it [00:14,  1.50it/s]Extractor Estimating: 22it [00:15,  1.48it/s]Extractor Estimating: 23it [00:15,  1.46it/s]Extractor Estimating: 24it [00:16,  1.44it/s]Extractor Estimating: 25it [00:17,  1.45it/s]Extractor Estimating: 26it [00:17,  1.46it/s]Extractor Estimating: 27it [00:18,  1.43it/s]Extractor Estimating: 28it [00:19,  1.41it/s]Extractor Estimating: 29it [00:19,  1.45it/s]Extractor Estimating: 30it [00:20,  1.41it/s]Extractor Estimating: 31it [00:21,  1.44it/s]Extractor Estimating: 32it [00:22,  1.43it/s]Extractor Estimating: 33it [00:22,  1.37it/s]Extractor Estimating: 34it [00:23,  1.40it/s]Extractor Estimating: 35it [00:24,  1.42it/s]Extractor Estimating: 36it [00:25,  1.34it/s]Extractor Estimating: 37it [00:25,  1.36it/s]Extractor Estimating: 38it [00:26,  1.38it/s]Extractor Estimating: 39it [00:27,  1.43it/s]Extractor Estimating: 40it [00:27,  1.44it/s]Extractor Estimating: 41it [00:28,  1.40it/s]Extractor Estimating: 42it [00:29,  1.32it/s]Extractor Estimating: 43it [00:30,  1.36it/s]Extractor Estimating: 44it [00:30,  1.39it/s]Extractor Estimating: 45it [00:31,  1.42it/s]Extractor Estimating: 46it [00:32,  1.45it/s]Extractor Estimating: 47it [00:32,  1.41it/s]Extractor Estimating: 48it [00:33,  1.44it/s]Extractor Estimating: 49it [00:34,  1.49it/s]Extractor Estimating: 50it [00:34,  1.49it/s]Extractor Estimating: 51it [00:35,  1.45it/s]Extractor Estimating: 52it [00:36,  1.42it/s]Extractor Estimating: 53it [00:36,  1.47it/s]Extractor Estimating: 54it [00:37,  1.48it/s]Extractor Estimating: 55it [00:38,  1.50it/s]Extractor Estimating: 56it [00:38,  1.50it/s]Extractor Estimating: 57it [00:39,  1.43it/s]Extractor Estimating: 58it [00:40,  1.43it/s]Extractor Estimating: 59it [00:40,  1.48it/s]Extractor Estimating: 60it [00:41,  1.54it/s]Extractor Estimating: 61it [00:42,  1.57it/s]Extractor Estimating: 62it [00:42,  1.57it/s]Extractor Estimating: 63it [00:43,  1.62it/s]Extractor Estimating: 64it [00:43,  1.62it/s]Extractor Estimating: 65it [00:44,  1.60it/s]Extractor Estimating: 66it [00:45,  1.59it/s]Extractor Estimating: 67it [00:46,  1.47it/s]Extractor Estimating: 68it [00:46,  1.48it/s]Extractor Estimating: 69it [00:47,  1.48it/s]Extractor Estimating: 70it [00:48,  1.49it/s]Extractor Estimating: 71it [00:48,  1.51it/s]Extractor Estimating: 72it [00:49,  1.48it/s]Extractor Estimating: 73it [00:50,  1.52it/s]Extractor Estimating: 74it [00:50,  1.57it/s]Extractor Estimating: 75it [00:51,  1.59it/s]Extractor Estimating: 76it [00:52,  1.45it/s]Extractor Estimating: 77it [00:52,  1.40it/s]Extractor Estimating: 78it [00:53,  1.46it/s]Extractor Estimating: 79it [00:54,  1.54it/s]Extractor Estimating: 80it [00:54,  1.61it/s]Extractor Estimating: 81it [00:55,  1.63it/s]Extractor Estimating: 82it [00:55,  1.59it/s]Extractor Estimating: 83it [00:56,  1.61it/s]Extractor Estimating: 84it [00:57,  1.61it/s]Extractor Estimating: 85it [00:57,  1.61it/s]Extractor Estimating: 86it [00:58,  1.65it/s]Extractor Estimating: 87it [00:59,  1.55it/s]Extractor Estimating: 88it [00:59,  1.53it/s]Extractor Estimating: 89it [01:00,  1.42it/s]Extractor Estimating: 90it [01:01,  1.45it/s]Extractor Estimating: 91it [01:01,  1.49it/s]Extractor Estimating: 92it [01:02,  1.50it/s]Extractor Estimating: 93it [01:03,  1.52it/s]Extractor Estimating: 94it [01:03,  1.46it/s]Extractor Estimating: 95it [01:04,  1.51it/s]Extractor Estimating: 96it [01:05,  1.50it/s]Extractor Estimating: 97it [01:05,  1.54it/s]Extractor Estimating: 98it [01:06,  1.55it/s]Extractor Estimating: 99it [01:07,  1.50it/s]Extractor Estimating: 100it [01:07,  1.54it/s]Extractor Estimating: 101it [01:08,  1.41it/s]Extractor Estimating: 102it [01:09,  1.39it/s]Extractor Estimating: 103it [01:09,  1.40it/s]Extractor Estimating: 104it [01:10,  1.43it/s]Extractor Estimating: 105it [01:11,  1.51it/s]Extractor Estimating: 106it [01:11,  1.54it/s]Extractor Estimating: 107it [01:12,  1.58it/s]Extractor Estimating: 108it [01:13,  1.56it/s]Extractor Estimating: 109it [01:13,  1.54it/s]Extractor Estimating: 110it [01:14,  1.53it/s]Extractor Estimating: 111it [01:15,  1.55it/s]Extractor Estimating: 112it [01:15,  1.57it/s]Extractor Estimating: 113it [01:16,  1.59it/s]Extractor Estimating: 114it [01:16,  1.57it/s]Extractor Estimating: 115it [01:17,  1.54it/s]Extractor Estimating: 116it [01:18,  1.51it/s]Extractor Estimating: 117it [01:18,  1.55it/s]Extractor Estimating: 118it [01:19,  1.52it/s]Extractor Estimating: 119it [01:20,  1.47it/s]Extractor Estimating: 120it [01:21,  1.43it/s]Extractor Estimating: 121it [01:21,  1.50it/s]Extractor Estimating: 122it [01:22,  1.51it/s]Extractor Estimating: 123it [01:22,  1.55it/s]Extractor Estimating: 124it [01:23,  1.53it/s]Extractor Estimating: 125it [01:24,  1.60it/s]Extractor Estimating: 126it [01:24,  1.62it/s]Extractor Estimating: 127it [01:25,  1.59it/s]Extractor Estimating: 128it [01:25,  1.63it/s]Extractor Estimating: 129it [01:26,  1.58it/s]Extractor Estimating: 130it [01:27,  1.59it/s]Extractor Estimating: 131it [01:27,  1.53it/s]Extractor Estimating: 132it [01:28,  1.53it/s]Extractor Estimating: 133it [01:29,  1.53it/s]Extractor Estimating: 134it [01:29,  1.51it/s]Extractor Estimating: 135it [01:30,  1.59it/s]Extractor Estimating: 136it [01:31,  1.63it/s]Extractor Estimating: 137it [01:31,  1.57it/s]Extractor Estimating: 138it [01:32,  1.62it/s]Extractor Estimating: 139it [01:32,  1.66it/s]Extractor Estimating: 140it [01:33,  1.64it/s]Extractor Estimating: 141it [01:34,  1.61it/s]Extractor Estimating: 142it [01:34,  1.67it/s]Extractor Estimating: 143it [01:35,  1.67it/s]Extractor Estimating: 144it [01:35,  1.66it/s]Extractor Estimating: 145it [01:36,  1.68it/s]Extractor Estimating: 146it [01:37,  1.53it/s]Extractor Estimating: 147it [01:37,  1.53it/s]Extractor Estimating: 148it [01:38,  1.58it/s]Extractor Estimating: 149it [01:39,  1.61it/s]Extractor Estimating: 150it [01:39,  1.53it/s]Extractor Estimating: 151it [01:40,  1.54it/s]Extractor Estimating: 152it [01:41,  1.52it/s]Extractor Estimating: 153it [01:41,  1.51it/s]Extractor Estimating: 154it [01:42,  1.50it/s]Extractor Estimating: 155it [01:43,  1.48it/s]Extractor Estimating: 156it [01:43,  1.47it/s]Extractor Estimating: 157it [01:44,  1.45it/s]Extractor Estimating: 158it [01:45,  1.49it/s]Extractor Estimating: 159it [01:45,  1.56it/s]Extractor Estimating: 160it [01:46,  1.57it/s]Extractor Estimating: 161it [01:47,  1.53it/s]Extractor Estimating: 162it [01:47,  1.43it/s]Extractor Estimating: 163it [01:48,  1.44it/s]Extractor Estimating: 164it [01:49,  1.50it/s]Extractor Estimating: 165it [01:49,  1.49it/s]Extractor Estimating: 166it [01:50,  1.51it/s]Extractor Estimating: 167it [01:51,  1.42it/s]Extractor Estimating: 168it [01:52,  1.42it/s]Extractor Estimating: 169it [01:52,  1.47it/s]Extractor Estimating: 170it [01:53,  1.43it/s]Extractor Estimating: 171it [01:54,  1.42it/s]Extractor Estimating: 172it [01:54,  1.40it/s]Extractor Estimating: 173it [01:55,  1.46it/s]Extractor Estimating: 174it [01:56,  1.50it/s]Extractor Estimating: 175it [01:56,  1.52it/s]Extractor Estimating: 176it [01:57,  1.50it/s]Extractor Estimating: 177it [01:58,  1.46it/s]Extractor Estimating: 178it [01:58,  1.49it/s]Extractor Estimating: 179it [01:59,  1.45it/s]Extractor Estimating: 180it [02:00,  1.48it/s]Extractor Estimating: 181it [02:00,  1.51it/s]Extractor Estimating: 182it [02:01,  1.44it/s]Extractor Estimating: 183it [02:02,  1.48it/s]Extractor Estimating: 184it [02:02,  1.56it/s]Extractor Estimating: 185it [02:03,  1.55it/s]Extractor Estimating: 186it [02:04,  1.58it/s]Extractor Estimating: 187it [02:04,  1.53it/s]Extractor Estimating: 188it [02:05,  1.49it/s]Extractor Estimating: 189it [02:06,  1.53it/s]Extractor Estimating: 190it [02:06,  1.53it/s]Extractor Estimating: 191it [02:07,  1.55it/s]Extractor Estimating: 192it [02:07,  1.61it/s]Extractor Estimating: 193it [02:08,  1.52it/s]Extractor Estimating: 194it [02:09,  1.56it/s]Extractor Estimating: 195it [02:09,  1.60it/s]Extractor Estimating: 196it [02:10,  1.52it/s]Extractor Estimating: 197it [02:11,  1.53it/s]Extractor Estimating: 198it [02:12,  1.38it/s]Extractor Estimating: 199it [02:12,  1.43it/s]Extractor Estimating: 200it [02:13,  1.51it/s]Extractor Estimating: 201it [02:14,  1.49it/s]Extractor Estimating: 202it [02:14,  1.49it/s]Extractor Estimating: 203it [02:15,  1.40it/s]Extractor Estimating: 204it [02:16,  1.43it/s]Extractor Estimating: 205it [02:16,  1.43it/s]Extractor Estimating: 206it [02:17,  1.40it/s]Extractor Estimating: 207it [02:18,  1.41it/s]Extractor Estimating: 208it [02:19,  1.36it/s]Extractor Estimating: 209it [02:19,  1.41it/s]Extractor Estimating: 210it [02:20,  1.41it/s]Extractor Estimating: 211it [02:21,  1.44it/s]Extractor Estimating: 212it [02:21,  1.48it/s]Extractor Estimating: 213it [02:22,  1.44it/s]Extractor Estimating: 214it [02:23,  1.44it/s]Extractor Estimating: 215it [02:23,  1.47it/s]Extractor Estimating: 216it [02:24,  1.47it/s]Extractor Estimating: 217it [02:25,  1.49it/s]Extractor Estimating: 218it [02:26,  1.39it/s]Extractor Estimating: 219it [02:26,  1.43it/s]Extractor Estimating: 220it [02:27,  1.42it/s]Extractor Estimating: 221it [02:28,  1.35it/s]Extractor Estimating: 222it [02:29,  1.34it/s]Extractor Estimating: 223it [02:29,  1.36it/s]Extractor Estimating: 224it [02:30,  1.37it/s]Extractor Estimating: 225it [02:31,  1.36it/s]Extractor Estimating: 226it [02:31,  1.50it/s]Extractor Estimating: 227it [02:32,  1.50it/s]Extractor Estimating: 228it [02:33,  1.51it/s]Extractor Estimating: 229it [02:33,  1.55it/s]Extractor Estimating: 230it [02:34,  1.58it/s]Extractor Estimating: 231it [02:34,  1.53it/s]Extractor Estimating: 232it [02:35,  1.54it/s]Extractor Estimating: 233it [02:36,  1.65it/s]Extractor Estimating: 234it [02:36,  1.70it/s]Extractor Estimating: 235it [02:37,  1.73it/s]Extractor Estimating: 236it [02:37,  1.73it/s]Extractor Estimating: 237it [02:38,  1.75it/s]Extractor Estimating: 238it [02:39,  1.55it/s]Extractor Estimating: 239it [02:39,  1.54it/s]Extractor Estimating: 240it [02:40,  1.60it/s]Extractor Estimating: 241it [02:40,  1.62it/s]Extractor Estimating: 242it [02:41,  1.63it/s]Extractor Estimating: 243it [02:42,  1.57it/s]Extractor Estimating: 244it [02:42,  1.61it/s]Extractor Estimating: 245it [02:43,  1.63it/s]Extractor Estimating: 246it [02:44,  1.64it/s]Extractor Estimating: 247it [02:44,  1.63it/s]Extractor Estimating: 248it [02:45,  1.54it/s]Extractor Estimating: 249it [02:45,  1.57it/s]Extractor Estimating: 250it [02:46,  1.59it/s]Extractor Estimating: 251it [02:47,  1.38it/s]Extractor Estimating: 252it [02:48,  1.46it/s]Extractor Estimating: 253it [02:48,  1.39it/s]Extractor Estimating: 254it [02:49,  1.37it/s]Extractor Estimating: 255it [02:50,  1.42it/s]Extractor Estimating: 256it [02:50,  1.48it/s]Extractor Estimating: 257it [02:51,  1.50it/s]Extractor Estimating: 258it [02:52,  1.48it/s]Extractor Estimating: 259it [02:52,  1.53it/s]Extractor Estimating: 260it [02:53,  1.53it/s]Extractor Estimating: 261it [02:54,  1.53it/s]Extractor Estimating: 262it [02:54,  1.49it/s]Extractor Estimating: 263it [02:55,  1.42it/s]Extractor Estimating: 264it [02:56,  1.45it/s]Extractor Estimating: 265it [02:56,  1.49it/s]Extractor Estimating: 266it [02:57,  1.40it/s]Extractor Estimating: 267it [02:58,  1.42it/s]Extractor Estimating: 268it [02:59,  1.35it/s]Extractor Estimating: 269it [03:00,  1.37it/s]Extractor Estimating: 270it [03:00,  1.40it/s]Extractor Estimating: 271it [03:01,  1.39it/s]Extractor Estimating: 272it [03:02,  1.41it/s]Extractor Estimating: 273it [03:02,  1.36it/s]Extractor Estimating: 274it [03:03,  1.43it/s]Extractor Estimating: 275it [03:04,  1.50it/s]Extractor Estimating: 276it [03:04,  1.54it/s]Extractor Estimating: 277it [03:05,  1.55it/s]Extractor Estimating: 278it [03:05,  1.60it/s]Extractor Estimating: 279it [03:06,  1.60it/s]Extractor Estimating: 280it [03:07,  1.58it/s]Extractor Estimating: 281it [03:07,  1.61it/s]Extractor Estimating: 282it [03:08,  1.65it/s]Extractor Estimating: 283it [03:08,  1.67it/s]Extractor Estimating: 284it [03:09,  1.60it/s]Extractor Estimating: 285it [03:10,  1.64it/s]Extractor Estimating: 286it [03:10,  1.61it/s]Extractor Estimating: 287it [03:11,  1.56it/s]Extractor Estimating: 288it [03:12,  1.52it/s]Extractor Estimating: 289it [03:13,  1.44it/s]Extractor Estimating: 290it [03:13,  1.53it/s]Extractor Estimating: 291it [03:14,  1.52it/s]Extractor Estimating: 292it [03:14,  1.54it/s]Extractor Estimating: 293it [03:15,  1.54it/s]Extractor Estimating: 294it [03:16,  1.47it/s]Extractor Estimating: 295it [03:16,  1.51it/s]Extractor Estimating: 296it [03:17,  1.55it/s]Extractor Estimating: 297it [03:18,  1.53it/s]Extractor Estimating: 298it [03:18,  1.56it/s]Extractor Estimating: 299it [03:19,  1.50it/s]Extractor Estimating: 300it [03:20,  1.52it/s]Extractor Estimating: 301it [03:20,  1.52it/s]Extractor Estimating: 302it [03:21,  1.57it/s]Extractor Estimating: 303it [03:21,  1.60it/s]Extractor Estimating: 304it [03:22,  1.48it/s]Extractor Estimating: 305it [03:23,  1.48it/s]Extractor Estimating: 306it [03:24,  1.56it/s]Extractor Estimating: 307it [03:24,  1.58it/s]Extractor Estimating: 308it [03:25,  1.54it/s]Extractor Estimating: 309it [03:25,  1.57it/s]Extractor Estimating: 310it [03:26,  1.56it/s]Extractor Estimating: 311it [03:27,  1.55it/s]Extractor Estimating: 312it [03:27,  1.53it/s]Extractor Estimating: 313it [03:28,  1.54it/s]Extractor Estimating: 314it [03:29,  1.47it/s]Extractor Estimating: 315it [03:29,  1.52it/s]Extractor Estimating: 316it [03:30,  1.55it/s]Extractor Estimating: 317it [03:31,  1.55it/s]Extractor Estimating: 318it [03:31,  1.58it/s]Extractor Estimating: 319it [03:32,  1.51it/s]Extractor Estimating: 320it [03:33,  1.52it/s]Extractor Estimating: 321it [03:33,  1.57it/s]Extractor Estimating: 322it [03:34,  1.57it/s]Extractor Estimating: 323it [03:35,  1.54it/s]Extractor Estimating: 324it [03:35,  1.53it/s]Extractor Estimating: 325it [03:36,  1.55it/s]Extractor Estimating: 326it [03:37,  1.40it/s]Extractor Estimating: 327it [03:37,  1.48it/s]Extractor Estimating: 328it [03:38,  1.52it/s]Extractor Estimating: 329it [03:38,  1.61it/s]Extractor Estimating: 330it [03:39,  1.57it/s]Extractor Estimating: 331it [03:40,  1.58it/s]Extractor Estimating: 332it [03:40,  1.61it/s]Extractor Estimating: 333it [03:41,  1.69it/s]Extractor Estimating: 334it [03:41,  1.68it/s]Extractor Estimating: 335it [03:42,  1.65it/s]Extractor Estimating: 336it [03:43,  1.61it/s]Extractor Estimating: 337it [03:43,  1.57it/s]Extractor Estimating: 338it [03:44,  1.61it/s]Extractor Estimating: 339it [03:45,  1.61it/s]Extractor Estimating: 340it [03:45,  1.62it/s]Extractor Estimating: 341it [03:46,  1.67it/s]Extractor Estimating: 342it [03:46,  1.63it/s]Extractor Estimating: 343it [03:47,  1.68it/s]Extractor Estimating: 344it [03:48,  1.72it/s]Extractor Estimating: 345it [03:48,  1.72it/s]Extractor Estimating: 346it [03:49,  1.68it/s]Extractor Estimating: 347it [03:49,  1.70it/s]Extractor Estimating: 348it [03:50,  1.56it/s]Extractor Estimating: 349it [03:51,  1.59it/s]Extractor Estimating: 350it [03:51,  1.61it/s]Extractor Estimating: 351it [03:52,  1.50it/s]Extractor Estimating: 352it [03:53,  1.45it/s]Extractor Estimating: 353it [03:54,  1.39it/s]Extractor Estimating: 354it [03:54,  1.43it/s]Extractor Estimating: 355it [03:55,  1.39it/s]Extractor Estimating: 356it [03:56,  1.41it/s]Extractor Estimating: 357it [03:56,  1.44it/s]Extractor Estimating: 358it [03:57,  1.43it/s]Extractor Estimating: 359it [03:58,  1.42it/s]Extractor Estimating: 360it [03:58,  1.44it/s]Extractor Estimating: 361it [03:59,  1.43it/s]Extractor Estimating: 362it [04:00,  1.46it/s]Extractor Estimating: 363it [04:00,  1.49it/s]Extractor Estimating: 364it [04:01,  1.50it/s]Extractor Estimating: 365it [04:02,  1.49it/s]Extractor Estimating: 366it [04:02,  1.50it/s]Extractor Estimating: 367it [04:03,  1.55it/s]Extractor Estimating: 368it [04:04,  1.48it/s]Extractor Estimating: 369it [04:05,  1.44it/s]Extractor Estimating: 370it [04:05,  1.48it/s]Extractor Estimating: 371it [04:06,  1.50it/s]Extractor Estimating: 372it [04:07,  1.45it/s]Extractor Estimating: 373it [04:07,  1.34it/s]Extractor Estimating: 374it [04:08,  1.40it/s]Extractor Estimating: 375it [04:09,  1.44it/s]Extractor Estimating: 375it [04:09,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:37,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:37,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:37,682 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:37,683 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:37,683 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 05:36:39,102 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 05:36:39,103 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:36:39,875 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 05:36:41,133 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:36:41,220 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:44,793 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:44,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:44,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:44,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 05:36:44,874 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 05:36:45,884 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 05:36:45,885 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 05:36:46,572 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 05:36:46,868 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 05:36:46,868 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 07:58:23,611 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 07:58:24,490 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7909 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 24294
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24394, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24394, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.072, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.074, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.086, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 70, avg_time 1.068, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 170, avg_time 1.072, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 270, avg_time 2.214, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 40, avg_time 1.089, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 140, avg_time 1.065, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 240, avg_time 1.065, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 10, avg_time 1.057, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 110, avg_time 2.153, loss:nan
g_step 1200, step 210, avg_time 1.063, loss:nan
g_step 1300, step 310, avg_time 1.448, loss:nan
g_step 1400, step 80, avg_time 1.071, loss:nan
g_step 1500, step 180, avg_time 1.067, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 280, avg_time 2.140, loss:nan
g_step 1700, step 50, avg_time 1.080, loss:nan
g_step 1800, step 150, avg_time 1.070, loss:nan
g_step 1900, step 250, avg_time 1.076, loss:nan
g_step 2000, step 20, avg_time 1.060, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 120, avg_time 2.118, loss:nan
g_step 2200, step 220, avg_time 1.063, loss:nan
g_step 2300, step 320, avg_time 1.085, loss:nan
g_step 2400, step 90, avg_time 1.052, loss:nan
g_step 2500, step 190, avg_time 1.066, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 290, avg_time 2.161, loss:nan
g_step 2700, step 60, avg_time 1.052, loss:nan
g_step 2800, step 160, avg_time 1.063, loss:nan
g_step 2900, step 260, avg_time 1.050, loss:nan
g_step 3000, step 30, avg_time 1.059, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 130, avg_time 2.129, loss:nan
g_step 3200, step 230, avg_time 1.052, loss:nan
g_step 3300, step 330, avg_time 1.044, loss:nan
g_step 3400, step 100, avg_time 1.025, loss:nan
g_step 3500, step 200, avg_time 1.082, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 300, avg_time 2.129, loss:nan
g_step 3700, step 70, avg_time 1.049, loss:nan
g_step 3800, step 170, avg_time 1.054, loss:nan
g_step 3900, step 270, avg_time 1.082, loss:nan
g_step 4000, step 40, avg_time 1.058, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 140, avg_time 2.139, loss:nan
g_step 4200, step 240, avg_time 1.061, loss:nan
g_step 4300, step 10, avg_time 1.084, loss:nan
g_step 4400, step 110, avg_time 1.073, loss:nan
g_step 4500, step 210, avg_time 1.076, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 310, avg_time 2.142, loss:nan
g_step 4700, step 80, avg_time 1.060, loss:nan
g_step 4800, step 180, avg_time 1.056, loss:nan
g_step 4900, step 280, avg_time 1.074, loss:nan
g_step 5000, step 50, avg_time 1.081, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 150, avg_time 2.144, loss:nan
g_step 5200, step 250, avg_time 1.061, loss:nan
g_step 5300, step 20, avg_time 1.068, loss:nan
g_step 5400, step 120, avg_time 1.091, loss:nan
g_step 5500, step 220, avg_time 1.062, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 320, avg_time 2.136, loss:nan
g_step 5700, step 90, avg_time 1.065, loss:nan
g_step 5800, step 190, avg_time 1.073, loss:nan
g_step 5900, step 290, avg_time 1.084, loss:nan
g_step 6000, step 60, avg_time 1.072, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 160, avg_time 2.148, loss:nan
g_step 6200, step 260, avg_time 1.052, loss:nan
g_step 6300, step 30, avg_time 1.076, loss:nan
g_step 6400, step 130, avg_time 1.066, loss:nan
g_step 6500, step 230, avg_time 1.068, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 330, avg_time 2.147, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 07:58:24 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 07:58:24 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_07-58-23_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 07:58:26 - WARNING - datasets.builder -   Using custom data configuration default-3575b11bf757045b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-3575b11bf757045b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 07:58:33,814 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:58:33,939 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 07:58:33,940 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 07:58:33,941 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 07:58:34,715 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:58:35,060 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:58:35,060 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:58:35,060 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:58:35,060 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:58:35,060 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 07:58:35,060 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 07:58:36,036 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 07:58:39,262 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 07:58:39,460 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-3575b11bf757045b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  1.90ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.93ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.54ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.25ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.70ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  3.98ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.15ba/s]100%|██████████| 8/8 [00:02<00:00,  4.40ba/s]100%|██████████| 8/8 [00:02<00:00,  3.75ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.41ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.38ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.08ba/s]100%|██████████| 4/4 [00:01<00:00,  4.21ba/s]100%|██████████| 4/4 [00:01<00:00,  3.22ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.76ba/s] 38%|███▊      | 3/8 [00:00<00:00,  6.11ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  7.79ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  8.79ba/s]100%|██████████| 8/8 [00:01<00:00,  7.55ba/s]100%|██████████| 8/8 [00:01<00:00,  7.13ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.11ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.80ba/s]100%|██████████| 4/4 [00:00<00:00,  6.96ba/s]100%|██████████| 4/4 [00:00<00:00,  5.44ba/s]
[INFO|trainer.py:414] 2023-08-28 07:58:50,481 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 07:58:50,859 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 07:58:50,859 >>   Num examples = 7920
[INFO|trainer.py:1149] 2023-08-28 07:58:50,859 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 07:58:50,859 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 07:58:50,859 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 07:58:50,859 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 07:58:50,859 >>   Total optimization steps = 620
  0%|          | 0/620 [00:00<?, ?it/s]  0%|          | 1/620 [00:00<03:01,  3.41it/s]  0%|          | 2/620 [00:00<02:55,  3.52it/s]  0%|          | 3/620 [00:00<02:54,  3.54it/s]  1%|          | 4/620 [00:01<02:52,  3.56it/s]  1%|          | 5/620 [00:01<02:52,  3.57it/s]  1%|          | 6/620 [00:01<02:51,  3.58it/s]  1%|          | 7/620 [00:01<02:51,  3.58it/s]  1%|▏         | 8/620 [00:02<02:50,  3.58it/s]  1%|▏         | 9/620 [00:02<02:50,  3.58it/s]  2%|▏         | 10/620 [00:02<02:50,  3.58it/s]  2%|▏         | 11/620 [00:03<03:10,  3.20it/s]  2%|▏         | 12/620 [00:03<03:03,  3.31it/s]  2%|▏         | 13/620 [00:03<02:59,  3.39it/s]  2%|▏         | 14/620 [00:04<02:55,  3.44it/s]  2%|▏         | 15/620 [00:04<02:53,  3.48it/s]  3%|▎         | 16/620 [00:04<02:52,  3.51it/s]  3%|▎         | 17/620 [00:04<02:50,  3.53it/s]  3%|▎         | 18/620 [00:05<02:50,  3.54it/s]  3%|▎         | 19/620 [00:05<02:49,  3.55it/s]  3%|▎         | 20/620 [00:05<02:48,  3.56it/s]  3%|▎         | 21/620 [00:05<02:48,  3.56it/s]  4%|▎         | 22/620 [00:06<03:01,  3.30it/s]  4%|▎         | 23/620 [00:06<02:56,  3.39it/s]  4%|▍         | 24/620 [00:06<02:52,  3.46it/s]  4%|▍         | 25/620 [00:07<02:49,  3.51it/s]  4%|▍         | 26/620 [00:07<02:47,  3.55it/s]  4%|▍         | 27/620 [00:07<02:45,  3.58it/s]  5%|▍         | 28/620 [00:07<02:44,  3.59it/s]  5%|▍         | 29/620 [00:08<02:52,  3.42it/s]  5%|▍         | 30/620 [00:08<02:49,  3.48it/s]  5%|▌         | 31/620 [00:08<02:46,  3.53it/s]  5%|▌         | 32/620 [00:09<02:45,  3.56it/s]  5%|▌         | 33/620 [00:09<02:44,  3.58it/s]  5%|▌         | 34/620 [00:09<02:42,  3.60it/s]  6%|▌         | 35/620 [00:09<02:42,  3.61it/s]  6%|▌         | 36/620 [00:10<02:41,  3.62it/s]  6%|▌         | 37/620 [00:10<02:41,  3.62it/s]  6%|▌         | 38/620 [00:10<02:40,  3.63it/s]  6%|▋         | 39/620 [00:11<02:40,  3.62it/s]  6%|▋         | 40/620 [00:11<02:55,  3.31it/s]  7%|▋         | 41/620 [00:11<02:50,  3.40it/s]  7%|▋         | 42/620 [00:11<02:46,  3.47it/s]  7%|▋         | 43/620 [00:12<02:44,  3.51it/s]  7%|▋         | 44/620 [00:12<02:42,  3.55it/s]  7%|▋         | 45/620 [00:12<02:41,  3.57it/s]  7%|▋         | 46/620 [00:13<02:39,  3.59it/s]  8%|▊         | 47/620 [00:13<02:39,  3.60it/s]  8%|▊         | 48/620 [00:13<02:38,  3.61it/s]  8%|▊         | 49/620 [00:13<02:37,  3.62it/s]  8%|▊         | 50/620 [00:14<02:37,  3.63it/s]  8%|▊         | 51/620 [00:14<03:07,  3.04it/s]  8%|▊         | 52/620 [00:14<02:57,  3.19it/s]  9%|▊         | 53/620 [00:15<02:50,  3.32it/s]  9%|▊         | 54/620 [00:15<02:46,  3.40it/s]  9%|▉         | 55/620 [00:15<02:42,  3.47it/s]  9%|▉         | 56/620 [00:16<02:40,  3.52it/s]  9%|▉         | 57/620 [00:16<02:38,  3.54it/s]  9%|▉         | 58/620 [00:16<02:37,  3.57it/s] 10%|▉         | 59/620 [00:16<02:36,  3.59it/s] 10%|▉         | 60/620 [00:17<02:35,  3.60it/s] 10%|▉         | 61/620 [00:17<02:34,  3.61it/s] 10%|█         | 62/620 [00:17<02:48,  3.32it/s] 10%|█         | 63/620 [00:18<02:43,  3.41it/s] 10%|█         | 64/620 [00:18<02:40,  3.47it/s] 10%|█         | 65/620 [00:18<02:37,  3.52it/s] 11%|█         | 66/620 [00:18<02:36,  3.55it/s] 11%|█         | 67/620 [00:19<02:34,  3.58it/s] 11%|█         | 68/620 [00:19<02:33,  3.59it/s] 11%|█         | 69/620 [00:19<02:32,  3.61it/s] 11%|█▏        | 70/620 [00:19<02:32,  3.61it/s] 11%|█▏        | 71/620 [00:20<02:31,  3.62it/s] 12%|█▏        | 72/620 [00:20<02:31,  3.62it/s] 12%|█▏        | 73/620 [00:20<02:52,  3.18it/s] 12%|█▏        | 74/620 [00:21<02:45,  3.30it/s] 12%|█▏        | 75/620 [00:21<02:40,  3.39it/s] 12%|█▏        | 76/620 [00:21<02:37,  3.46it/s] 12%|█▏        | 77/620 [00:22<02:35,  3.50it/s] 13%|█▎        | 78/620 [00:22<02:33,  3.54it/s] 13%|█▎        | 79/620 [00:22<02:31,  3.57it/s] 13%|█▎        | 80/620 [00:22<02:30,  3.59it/s] 13%|█▎        | 81/620 [00:23<02:29,  3.60it/s] 13%|█▎        | 82/620 [00:23<02:29,  3.60it/s] 13%|█▎        | 83/620 [00:23<02:28,  3.61it/s] 14%|█▎        | 84/620 [00:24<02:42,  3.29it/s] 14%|█▎        | 85/620 [00:24<02:38,  3.38it/s] 14%|█▍        | 86/620 [00:24<02:34,  3.45it/s] 14%|█▍        | 87/620 [00:24<02:32,  3.50it/s] 14%|█▍        | 88/620 [00:25<02:30,  3.54it/s] 14%|█▍        | 89/620 [00:25<02:29,  3.56it/s] 15%|█▍        | 90/620 [00:25<02:27,  3.58it/s] 15%|█▍        | 91/620 [00:25<02:27,  3.60it/s] 15%|█▍        | 92/620 [00:26<02:26,  3.61it/s] 15%|█▌        | 93/620 [00:26<02:26,  3.61it/s] 15%|█▌        | 94/620 [00:26<02:25,  3.61it/s] 15%|█▌        | 95/620 [00:27<02:42,  3.23it/s] 15%|█▌        | 96/620 [00:27<02:37,  3.34it/s] 16%|█▌        | 97/620 [00:27<02:33,  3.42it/s] 16%|█▌        | 98/620 [00:28<02:30,  3.48it/s] 16%|█▌        | 99/620 [00:28<02:27,  3.53it/s] 16%|█▌        | 100/620 [00:28<02:26,  3.55it/s] 16%|█▋        | 101/620 [00:28<02:25,  3.57it/s] 16%|█▋        | 102/620 [00:29<02:24,  3.59it/s] 17%|█▋        | 103/620 [00:29<02:39,  3.24it/s] 17%|█▋        | 104/620 [00:29<02:34,  3.34it/s] 17%|█▋        | 105/620 [00:30<02:30,  3.42it/s] 17%|█▋        | 106/620 [00:30<02:41,  3.18it/s] 17%|█▋        | 107/620 [00:30<02:35,  3.30it/s] 17%|█▋        | 108/620 [00:30<02:30,  3.39it/s] 18%|█▊        | 109/620 [00:31<02:27,  3.46it/s] 18%|█▊        | 110/620 [00:31<02:25,  3.50it/s] 18%|█▊        | 111/620 [00:31<02:23,  3.54it/s] 18%|█▊        | 112/620 [00:32<02:22,  3.57it/s] 18%|█▊        | 113/620 [00:32<02:21,  3.58it/s] 18%|█▊        | 114/620 [00:32<02:20,  3.60it/s] 19%|█▊        | 115/620 [00:33<04:19,  1.95it/s] 19%|█▊        | 116/620 [00:34<04:03,  2.07it/s] 19%|█▉        | 117/620 [00:34<03:31,  2.38it/s] 19%|█▉        | 118/620 [00:34<03:09,  2.65it/s] 19%|█▉        | 119/620 [00:34<02:53,  2.88it/s] 19%|█▉        | 120/620 [00:35<02:42,  3.07it/s] 20%|█▉        | 121/620 [00:35<02:35,  3.22it/s] 20%|█▉        | 122/620 [00:35<02:29,  3.33it/s] 20%|█▉        | 123/620 [00:36<02:25,  3.41it/s] 20%|██        | 124/620 [00:36<02:13,  3.71it/s][INFO|trainer.py:2140] 2023-08-28 07:59:27,102 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 07:59:27,103 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 07:59:27,103 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.82it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.98it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.09it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.96it/s][A
  6%|▌         | 27/437 [00:00<00:15, 26.67it/s][A
  7%|▋         | 32/437 [00:00<00:13, 30.87it/s][A
  8%|▊         | 37/437 [00:01<00:11, 34.29it/s][A
 10%|▉         | 42/437 [00:01<00:10, 37.05it/s][A
 11%|█         | 47/437 [00:01<00:09, 39.23it/s][A
 12%|█▏        | 52/437 [00:01<00:11, 34.35it/s][A
 13%|█▎        | 57/437 [00:01<00:10, 37.08it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 39.00it/s][A
 15%|█▌        | 67/437 [00:01<00:09, 40.63it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 41.85it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 42.78it/s][A
 19%|█▉        | 82/437 [00:02<00:08, 43.45it/s][A
 20%|█▉        | 87/437 [00:02<00:07, 43.87it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.59it/s][A
 22%|██▏       | 97/437 [00:02<00:12, 27.98it/s][A
 23%|██▎       | 102/437 [00:02<00:10, 31.58it/s][A
 24%|██▍       | 107/437 [00:02<00:09, 34.70it/s][A
 26%|██▌       | 112/437 [00:02<00:08, 37.16it/s][A
 27%|██▋       | 117/437 [00:03<00:08, 39.19it/s][A
 28%|██▊       | 122/437 [00:03<00:07, 40.75it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 41.92it/s][A
 30%|███       | 132/437 [00:03<00:07, 42.68it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 42.76it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.09it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.59it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.98it/s][A
 36%|███▌      | 157/437 [00:04<00:06, 44.22it/s][A
 37%|███▋      | 162/437 [00:04<00:06, 44.47it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 44.58it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.59it/s][A
 41%|████      | 177/437 [00:04<00:07, 34.74it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 37.12it/s][A
 43%|████▎     | 187/437 [00:04<00:06, 39.26it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 40.83it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 41.90it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 42.87it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 43.42it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 43.78it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 37.43it/s][A
 51%|█████     | 222/437 [00:05<00:05, 39.44it/s][A
 52%|█████▏    | 227/437 [00:05<00:05, 41.07it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 42.17it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.05it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 43.61it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 44.06it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 44.09it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 43.88it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.63it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.83it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.02it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.38it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.45it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 44.55it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 44.71it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.75it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 40.51it/s][A
 70%|███████   | 307/437 [00:07<00:03, 41.75it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 42.52it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.07it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.63it/s][A
 75%|███████▍  | 327/437 [00:08<00:02, 43.91it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 44.26it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 44.30it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 44.00it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 33.51it/s][A
 81%|████████  | 352/437 [00:08<00:02, 36.33it/s][A
 82%|████████▏ | 357/437 [00:08<00:02, 38.58it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 40.28it/s][A
 84%|████████▍ | 367/437 [00:09<00:01, 41.59it/s][A
 85%|████████▌ | 372/437 [00:09<00:01, 42.53it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 43.27it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 43.54it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.40it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.40it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.70it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.00it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.29it/s][A
 94%|█████████▍| 412/437 [00:10<00:00, 44.50it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 44.67it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 44.72it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.48it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.21it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.07it/s][A                                                 
                                                 [A 20%|██        | 124/620 [00:46<02:13,  3.71it/s]
100%|██████████| 437/437 [00:10<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 07:59:39,227 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124
[INFO|configuration_utils.py:351] 2023-08-28 07:59:40,005 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:00:05,230 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:00:05,863 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:00:06,112 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124/special_tokens_map.json
 20%|██        | 125/620 [01:18<1:45:58, 12.85s/it] 20%|██        | 126/620 [01:18<1:15:00,  9.11s/it] 20%|██        | 127/620 [01:19<53:05,  6.46s/it]   21%|██        | 128/620 [01:19<37:46,  4.61s/it] 21%|██        | 129/620 [01:19<27:04,  3.31s/it] 21%|██        | 130/620 [01:19<19:36,  2.40s/it] 21%|██        | 131/620 [01:20<14:22,  1.76s/it] 21%|██▏       | 132/620 [01:20<10:43,  1.32s/it] 21%|██▏       | 133/620 [01:20<08:10,  1.01s/it] 22%|██▏       | 134/620 [01:21<06:23,  1.27it/s] 22%|██▏       | 135/620 [01:21<05:08,  1.57it/s] 22%|██▏       | 136/620 [01:21<04:16,  1.89it/s] 22%|██▏       | 137/620 [01:22<03:54,  2.06it/s] 22%|██▏       | 138/620 [01:22<03:24,  2.36it/s] 22%|██▏       | 139/620 [01:22<03:03,  2.63it/s] 23%|██▎       | 140/620 [01:22<02:48,  2.86it/s] 23%|██▎       | 141/620 [01:23<02:37,  3.04it/s] 23%|██▎       | 142/620 [01:23<02:30,  3.18it/s] 23%|██▎       | 143/620 [01:23<02:25,  3.29it/s] 23%|██▎       | 144/620 [01:23<02:21,  3.37it/s] 23%|██▎       | 145/620 [01:24<02:18,  3.43it/s] 24%|██▎       | 146/620 [01:24<02:16,  3.47it/s] 24%|██▎       | 147/620 [01:24<02:15,  3.50it/s] 24%|██▍       | 148/620 [01:25<02:55,  2.69it/s] 24%|██▍       | 149/620 [01:25<02:42,  2.91it/s] 24%|██▍       | 150/620 [01:25<02:32,  3.08it/s] 24%|██▍       | 151/620 [01:26<02:26,  3.21it/s] 25%|██▍       | 152/620 [01:26<02:21,  3.31it/s] 25%|██▍       | 153/620 [01:26<02:17,  3.39it/s] 25%|██▍       | 154/620 [01:27<02:15,  3.44it/s] 25%|██▌       | 155/620 [01:27<02:13,  3.48it/s] 25%|██▌       | 156/620 [01:27<02:12,  3.51it/s] 25%|██▌       | 157/620 [01:27<02:11,  3.53it/s] 25%|██▌       | 158/620 [01:28<02:26,  3.16it/s] 26%|██▌       | 159/620 [01:28<02:20,  3.27it/s] 26%|██▌       | 160/620 [01:28<02:16,  3.36it/s] 26%|██▌       | 161/620 [01:29<02:13,  3.43it/s] 26%|██▌       | 162/620 [01:29<02:22,  3.22it/s] 26%|██▋       | 163/620 [01:29<02:17,  3.31it/s] 26%|██▋       | 164/620 [01:30<02:14,  3.40it/s] 27%|██▋       | 165/620 [01:30<02:11,  3.47it/s] 27%|██▋       | 166/620 [01:30<02:09,  3.51it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/620 [01:30<02:07,  3.55it/s] 27%|██▋       | 168/620 [01:31<02:06,  3.57it/s] 27%|██▋       | 169/620 [01:31<02:12,  3.41it/s] 27%|██▋       | 170/620 [01:31<02:09,  3.47it/s] 28%|██▊       | 171/620 [01:32<02:07,  3.53it/s] 28%|██▊       | 172/620 [01:32<02:05,  3.56it/s] 28%|██▊       | 173/620 [01:32<02:04,  3.58it/s] 28%|██▊       | 174/620 [01:32<02:04,  3.59it/s] 28%|██▊       | 175/620 [01:33<03:28,  2.13it/s] 28%|██▊       | 176/620 [01:34<03:03,  2.42it/s] 29%|██▊       | 177/620 [01:34<02:44,  2.69it/s] 29%|██▊       | 178/620 [01:34<02:38,  2.78it/s] 29%|██▉       | 179/620 [01:34<02:27,  2.99it/s] 29%|██▉       | 180/620 [01:35<02:19,  3.16it/s] 29%|██▉       | 181/620 [01:35<02:13,  3.29it/s] 29%|██▉       | 182/620 [01:35<02:09,  3.39it/s] 30%|██▉       | 183/620 [01:36<02:06,  3.46it/s] 30%|██▉       | 184/620 [01:36<02:04,  3.50it/s] 30%|██▉       | 185/620 [01:36<02:02,  3.54it/s] 30%|███       | 186/620 [01:36<02:01,  3.57it/s] 30%|███       | 187/620 [01:37<02:00,  3.58it/s] 30%|███       | 188/620 [01:37<02:00,  3.60it/s] 30%|███       | 189/620 [01:37<02:08,  3.36it/s] 31%|███       | 190/620 [01:38<02:05,  3.43it/s] 31%|███       | 191/620 [01:38<02:03,  3.49it/s] 31%|███       | 192/620 [01:38<02:01,  3.53it/s] 31%|███       | 193/620 [01:38<01:59,  3.56it/s] 31%|███▏      | 194/620 [01:39<01:59,  3.58it/s] 31%|███▏      | 195/620 [01:39<01:58,  3.59it/s] 32%|███▏      | 196/620 [01:39<01:57,  3.61it/s] 32%|███▏      | 197/620 [01:39<01:57,  3.61it/s] 32%|███▏      | 198/620 [01:40<01:56,  3.62it/s] 32%|███▏      | 199/620 [01:40<01:56,  3.62it/s] 32%|███▏      | 200/620 [01:40<01:55,  3.62it/s] 32%|███▏      | 201/620 [01:41<01:55,  3.62it/s] 33%|███▎      | 202/620 [01:41<01:55,  3.62it/s] 33%|███▎      | 203/620 [01:41<01:55,  3.63it/s] 33%|███▎      | 204/620 [01:41<01:54,  3.63it/s] 33%|███▎      | 205/620 [01:42<01:54,  3.63it/s] 33%|███▎      | 206/620 [01:42<01:54,  3.63it/s] 33%|███▎      | 207/620 [01:42<01:53,  3.63it/s] 34%|███▎      | 208/620 [01:42<01:53,  3.63it/s] 34%|███▎      | 209/620 [01:43<02:13,  3.07it/s] 34%|███▍      | 210/620 [01:43<02:07,  3.22it/s] 34%|███▍      | 211/620 [01:43<02:02,  3.33it/s] 34%|███▍      | 212/620 [01:44<01:59,  3.41it/s] 34%|███▍      | 213/620 [01:44<01:57,  3.47it/s] 35%|███▍      | 214/620 [01:44<01:55,  3.52it/s] 35%|███▍      | 215/620 [01:45<01:54,  3.55it/s] 35%|███▍      | 216/620 [01:45<01:53,  3.57it/s] 35%|███▌      | 217/620 [01:45<01:52,  3.59it/s] 35%|███▌      | 218/620 [01:45<01:51,  3.60it/s] 35%|███▌      | 219/620 [01:46<01:51,  3.61it/s] 35%|███▌      | 220/620 [01:46<02:01,  3.29it/s] 36%|███▌      | 221/620 [01:46<01:57,  3.38it/s] 36%|███▌      | 222/620 [01:47<01:55,  3.46it/s] 36%|███▌      | 223/620 [01:47<01:53,  3.51it/s] 36%|███▌      | 224/620 [01:47<01:51,  3.54it/s] 36%|███▋      | 225/620 [01:47<01:50,  3.57it/s] 36%|███▋      | 226/620 [01:48<01:49,  3.58it/s] 37%|███▋      | 227/620 [01:48<01:49,  3.60it/s] 37%|███▋      | 228/620 [01:48<01:48,  3.60it/s] 37%|███▋      | 229/620 [01:49<01:48,  3.61it/s] 37%|███▋      | 230/620 [01:49<01:48,  3.61it/s] 37%|███▋      | 231/620 [01:49<01:56,  3.35it/s] 37%|███▋      | 232/620 [01:49<01:53,  3.43it/s] 38%|███▊      | 233/620 [01:50<01:51,  3.48it/s] 38%|███▊      | 234/620 [01:50<01:49,  3.53it/s] 38%|███▊      | 235/620 [01:50<01:48,  3.55it/s] 38%|███▊      | 236/620 [01:51<01:47,  3.58it/s] 38%|███▊      | 237/620 [01:51<01:46,  3.59it/s] 38%|███▊      | 238/620 [01:51<01:46,  3.60it/s] 39%|███▊      | 239/620 [01:51<01:45,  3.60it/s] 39%|███▊      | 240/620 [01:52<01:45,  3.61it/s] 39%|███▉      | 241/620 [01:52<01:44,  3.61it/s] 39%|███▉      | 242/620 [01:52<01:49,  3.45it/s] 39%|███▉      | 243/620 [01:53<01:47,  3.50it/s] 39%|███▉      | 244/620 [01:53<01:46,  3.53it/s] 40%|███▉      | 245/620 [01:53<01:45,  3.56it/s] 40%|███▉      | 246/620 [01:53<01:44,  3.58it/s] 40%|███▉      | 247/620 [01:54<01:43,  3.59it/s] 40%|████      | 248/620 [01:54<01:36,  3.86it/s][INFO|trainer.py:2140] 2023-08-28 08:00:45,195 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:00:45,195 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 08:00:45,195 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.6246, 'eval_samples_per_second': 328.671, 'eval_steps_per_second': 41.131, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.21it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.03it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.22it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.02it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.41it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.92it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.49it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.25it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.23it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.36it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 40.03it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 41.39it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 42.43it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.14it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.65it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.88it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 43.92it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.99it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.76it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.96it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.09it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.40it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.53it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.66it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.61it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.48it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.88it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.14it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.14it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.39it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.53it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.63it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.63it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.73it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.53it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.14it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.10it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 40.61it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 41.82it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 42.76it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.30it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.84it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.13it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.25it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.08it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.79it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.81it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.04it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.27it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 39.07it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 41.47it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 42.22it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.02it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.61it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.75it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.01it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.16it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.20it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.87it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.92it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.20it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.38it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.58it/s][A
 74%|███████▎  | 322/437 [00:07<00:03, 31.08it/s][A
 75%|███████▍  | 327/437 [00:07<00:03, 34.25it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 36.82it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 38.97it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 40.55it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 41.75it/s][A
 81%|████████  | 352/437 [00:08<00:01, 42.78it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.19it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.22it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.19it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.51it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.93it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.81it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.48it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.58it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.66it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.55it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.15it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.96it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.07it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.26it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.27it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.48it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.61it/s][A                                                 
                                                 [A 40%|████      | 248/620 [02:04<01:36,  3.86it/s]
100%|██████████| 437/437 [00:10<00:00, 44.61it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:00:55,787 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248
[INFO|configuration_utils.py:351] 2023-08-28 08:00:56,385 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:01:24,071 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:01:25,376 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:01:25,575 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248/special_tokens_map.json
 40%|████      | 249/620 [02:40<1:26:52, 14.05s/it] 40%|████      | 250/620 [02:40<1:01:21,  9.95s/it] 40%|████      | 251/620 [02:41<43:20,  7.05s/it]   41%|████      | 252/620 [02:41<30:46,  5.02s/it] 41%|████      | 253/620 [02:41<22:10,  3.63s/it] 41%|████      | 254/620 [02:42<15:59,  2.62s/it] 41%|████      | 255/620 [02:42<11:40,  1.92s/it] 41%|████▏     | 256/620 [02:42<08:39,  1.43s/it] 41%|████▏     | 257/620 [02:42<06:33,  1.08s/it] 42%|████▏     | 258/620 [02:43<05:04,  1.19it/s] 42%|████▏     | 259/620 [02:43<04:03,  1.48it/s] 42%|████▏     | 260/620 [02:43<03:19,  1.80it/s] 42%|████▏     | 261/620 [02:44<02:49,  2.12it/s] 42%|████▏     | 262/620 [02:44<02:28,  2.41it/s] 42%|████▏     | 263/620 [02:44<02:13,  2.67it/s] 43%|████▎     | 264/620 [02:45<02:20,  2.53it/s] 43%|████▎     | 265/620 [02:45<02:07,  2.77it/s] 43%|████▎     | 266/620 [02:45<01:59,  2.97it/s] 43%|████▎     | 267/620 [02:45<01:52,  3.13it/s] 43%|████▎     | 268/620 [02:46<01:48,  3.25it/s] 43%|████▎     | 269/620 [02:46<01:44,  3.35it/s] 44%|████▎     | 270/620 [02:46<01:42,  3.41it/s] 44%|████▎     | 271/620 [02:47<01:40,  3.46it/s] 44%|████▍     | 272/620 [02:47<01:39,  3.49it/s] 44%|████▍     | 273/620 [02:47<01:38,  3.52it/s] 44%|████▍     | 274/620 [02:47<01:37,  3.54it/s] 44%|████▍     | 275/620 [02:48<01:47,  3.20it/s] 45%|████▍     | 276/620 [02:48<01:43,  3.32it/s] 45%|████▍     | 277/620 [02:48<01:40,  3.41it/s] 45%|████▍     | 278/620 [02:49<01:38,  3.47it/s] 45%|████▌     | 279/620 [02:49<01:36,  3.52it/s] 45%|████▌     | 280/620 [02:49<01:35,  3.55it/s] 45%|████▌     | 281/620 [02:49<01:34,  3.58it/s] 45%|████▌     | 282/620 [02:50<01:34,  3.59it/s] 46%|████▌     | 283/620 [02:50<01:33,  3.60it/s] 46%|████▌     | 284/620 [02:50<01:33,  3.61it/s] 46%|████▌     | 285/620 [02:51<01:32,  3.62it/s] 46%|████▌     | 286/620 [02:51<01:42,  3.26it/s] 46%|████▋     | 287/620 [02:51<01:39,  3.36it/s] 46%|████▋     | 288/620 [02:51<01:36,  3.43it/s] 47%|████▋     | 289/620 [02:52<01:34,  3.49it/s] 47%|████▋     | 290/620 [02:52<01:33,  3.53it/s] 47%|████▋     | 291/620 [02:52<01:32,  3.56it/s] 47%|████▋     | 292/620 [02:53<01:31,  3.58it/s] 47%|████▋     | 293/620 [02:53<01:31,  3.59it/s] 47%|████▋     | 294/620 [02:53<01:30,  3.61it/s] 48%|████▊     | 295/620 [02:53<01:30,  3.61it/s] 48%|████▊     | 296/620 [02:54<01:29,  3.61it/s] 48%|████▊     | 297/620 [02:54<01:38,  3.28it/s] 48%|████▊     | 298/620 [02:54<01:35,  3.38it/s] 48%|████▊     | 299/620 [02:55<01:33,  3.45it/s] 48%|████▊     | 300/620 [02:55<01:31,  3.50it/s] 49%|████▊     | 301/620 [02:55<01:30,  3.54it/s] 49%|████▊     | 302/620 [02:55<01:29,  3.56it/s] 49%|████▉     | 303/620 [02:56<01:28,  3.58it/s] 49%|████▉     | 304/620 [02:56<01:28,  3.59it/s] 49%|████▉     | 305/620 [02:56<01:27,  3.59it/s] 49%|████▉     | 306/620 [02:57<01:27,  3.60it/s] 50%|████▉     | 307/620 [02:57<01:26,  3.61it/s] 50%|████▉     | 308/620 [02:57<01:33,  3.34it/s] 50%|████▉     | 309/620 [02:57<01:31,  3.42it/s] 50%|█████     | 310/620 [02:58<01:29,  3.47it/s] 50%|█████     | 311/620 [02:58<01:27,  3.51it/s] 50%|█████     | 312/620 [02:58<01:26,  3.55it/s] 50%|█████     | 313/620 [02:59<01:26,  3.57it/s] 51%|█████     | 314/620 [02:59<01:25,  3.58it/s] 51%|█████     | 315/620 [02:59<01:25,  3.59it/s] 51%|█████     | 316/620 [02:59<01:24,  3.60it/s] 51%|█████     | 317/620 [03:00<01:34,  3.20it/s] 51%|█████▏    | 318/620 [03:00<01:31,  3.29it/s] 51%|█████▏    | 319/620 [03:00<01:37,  3.09it/s] 52%|█████▏    | 320/620 [03:01<01:32,  3.23it/s] 52%|█████▏    | 321/620 [03:01<01:29,  3.33it/s] 52%|█████▏    | 322/620 [03:01<01:27,  3.41it/s] 52%|█████▏    | 323/620 [03:02<01:25,  3.47it/s] 52%|█████▏    | 324/620 [03:02<01:24,  3.52it/s] 52%|█████▏    | 325/620 [03:02<01:23,  3.55it/s] 53%|█████▎    | 326/620 [03:02<01:22,  3.57it/s] 53%|█████▎    | 327/620 [03:03<01:21,  3.58it/s] 53%|█████▎    | 328/620 [03:03<01:21,  3.59it/s] 53%|█████▎    | 329/620 [03:03<01:20,  3.60it/s] 53%|█████▎    | 330/620 [03:04<01:30,  3.20it/s] 53%|█████▎    | 331/620 [03:04<01:27,  3.31it/s] 54%|█████▎    | 332/620 [03:04<01:24,  3.40it/s] 54%|█████▎    | 333/620 [03:04<01:22,  3.46it/s] 54%|█████▍    | 334/620 [03:05<01:21,  3.51it/s] 54%|█████▍    | 335/620 [03:05<01:20,  3.54it/s] 54%|█████▍    | 336/620 [03:05<01:19,  3.56it/s] 54%|█████▍    | 337/620 [03:06<01:19,  3.58it/s] 55%|█████▍    | 338/620 [03:06<01:18,  3.58it/s] 55%|█████▍    | 339/620 [03:06<01:18,  3.59it/s] 55%|█████▍    | 340/620 [03:06<01:17,  3.59it/s] 55%|█████▌    | 341/620 [03:07<01:22,  3.38it/s] 55%|█████▌    | 342/620 [03:07<01:20,  3.45it/s] 55%|█████▌    | 343/620 [03:07<01:19,  3.50it/s] 55%|█████▌    | 344/620 [03:08<01:18,  3.53it/s] 56%|█████▌    | 345/620 [03:08<01:17,  3.55it/s] 56%|█████▌    | 346/620 [03:08<01:16,  3.57it/s] 56%|█████▌    | 347/620 [03:08<01:16,  3.58it/s] 56%|█████▌    | 348/620 [03:09<01:15,  3.59it/s] 56%|█████▋    | 349/620 [03:09<01:15,  3.60it/s] 56%|█████▋    | 350/620 [03:09<01:14,  3.61it/s] 57%|█████▋    | 351/620 [03:09<01:14,  3.61it/s] 57%|█████▋    | 352/620 [03:10<01:23,  3.21it/s] 57%|█████▋    | 353/620 [03:10<01:20,  3.32it/s] 57%|█████▋    | 354/620 [03:10<01:18,  3.40it/s] 57%|█████▋    | 355/620 [03:11<01:16,  3.47it/s] 57%|█████▋    | 356/620 [03:11<01:15,  3.51it/s] 58%|█████▊    | 357/620 [03:11<01:14,  3.54it/s] 58%|█████▊    | 358/620 [03:12<01:13,  3.57it/s] 58%|█████▊    | 359/620 [03:12<01:12,  3.59it/s] 58%|█████▊    | 360/620 [03:12<01:17,  3.34it/s] 58%|█████▊    | 361/620 [03:12<01:15,  3.42it/s] 58%|█████▊    | 362/620 [03:13<01:14,  3.48it/s] 59%|█████▊    | 363/620 [03:13<01:13,  3.52it/s] 59%|█████▊    | 364/620 [03:13<01:12,  3.55it/s] 59%|█████▉    | 365/620 [03:14<01:11,  3.57it/s] 59%|█████▉    | 366/620 [03:14<01:10,  3.58it/s] 59%|█████▉    | 367/620 [03:14<01:10,  3.59it/s] 59%|█████▉    | 368/620 [03:14<01:09,  3.60it/s] 60%|█████▉    | 369/620 [03:15<01:09,  3.60it/s] 60%|█████▉    | 370/620 [03:15<01:09,  3.60it/s] 60%|█████▉    | 371/620 [03:15<01:15,  3.31it/s] 60%|██████    | 372/620 [03:15<01:08,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 08:02:06,845 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:02:06,845 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 08:02:06,845 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.1009, 'eval_samples_per_second': 345.712, 'eval_steps_per_second': 43.263, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.62it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.00it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.23it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.00it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.25it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.78it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.47it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.31it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.32it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.57it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.71it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.78it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.71it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.53it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.45it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.17it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.12it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.16it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.39it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.54it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.68it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.70it/s][A
 27%|██▋       | 117/437 [00:02<00:08, 37.18it/s][A
 28%|██▊       | 122/437 [00:02<00:08, 39.20it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 40.76it/s][A
 30%|███       | 132/437 [00:03<00:07, 41.94it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 42.79it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.46it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.88it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.01it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.73it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.62it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.89it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.08it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.41it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.51it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.81it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.79it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.42it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.03it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.68it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.75it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.08it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.34it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.65it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.65it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.81it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.62it/s][A
 57%|█████▋    | 247/437 [00:05<00:05, 33.30it/s][A
 58%|█████▊    | 252/437 [00:05<00:05, 36.14it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 38.42it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 40.09it/s][A
 61%|██████    | 267/437 [00:06<00:04, 41.47it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 42.38it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.15it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.52it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.38it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.49it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.70it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.98it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.31it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.49it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.62it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.66it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.48it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.24it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.99it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.19it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.25it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.38it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.59it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.66it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.67it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.54it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 37.30it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 39.34it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 40.92it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 42.02it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 42.84it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.42it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.83it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.04it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.74it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.68it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.81it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.23it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.41it/s][A                                                 
                                                 [A 60%|██████    | 372/620 [03:26<01:08,  3.62it/s]
100%|██████████| 437/437 [00:10<00:00, 44.41it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:02:17,405 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372
[INFO|configuration_utils.py:351] 2023-08-28 08:02:17,915 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:02:35,921 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:02:36,441 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:02:36,625 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372/special_tokens_map.json
 60%|██████    | 373/620 [03:51<44:03, 10.70s/it] 60%|██████    | 374/620 [03:51<31:10,  7.60s/it] 60%|██████    | 375/620 [03:51<22:04,  5.41s/it] 61%|██████    | 376/620 [03:51<15:43,  3.87s/it] 61%|██████    | 377/620 [03:52<11:17,  2.79s/it] 61%|██████    | 378/620 [03:52<08:12,  2.04s/it] 61%|██████    | 379/620 [03:52<06:03,  1.51s/it] 61%|██████▏   | 380/620 [03:53<04:33,  1.14s/it] 61%|██████▏   | 381/620 [03:53<03:30,  1.14it/s] 62%|██████▏   | 382/620 [03:53<02:46,  1.43it/s] 62%|██████▏   | 383/620 [03:53<02:15,  1.75it/s] 62%|██████▏   | 384/620 [03:54<01:53,  2.07it/s] 62%|██████▏   | 385/620 [03:54<01:43,  2.28it/s] 62%|██████▏   | 386/620 [03:54<01:31,  2.56it/s] 62%|██████▏   | 387/620 [03:55<01:22,  2.81it/s] 63%|██████▎   | 388/620 [03:55<01:16,  3.01it/s] 63%|██████▎   | 389/620 [03:55<01:12,  3.18it/s] 63%|██████▎   | 390/620 [03:55<01:09,  3.30it/s] 63%|██████▎   | 391/620 [03:56<01:07,  3.39it/s] 63%|██████▎   | 392/620 [03:56<01:05,  3.46it/s] 63%|██████▎   | 393/620 [03:56<01:04,  3.50it/s] 64%|██████▎   | 394/620 [03:56<01:03,  3.54it/s] 64%|██████▎   | 395/620 [03:57<01:03,  3.57it/s] 64%|██████▍   | 396/620 [03:57<01:06,  3.37it/s] 64%|██████▍   | 397/620 [03:57<01:04,  3.44it/s] 64%|██████▍   | 398/620 [03:58<01:03,  3.50it/s] 64%|██████▍   | 399/620 [03:58<01:02,  3.54it/s] 65%|██████▍   | 400/620 [03:58<01:01,  3.56it/s] 65%|██████▍   | 401/620 [03:58<01:01,  3.58it/s] 65%|██████▍   | 402/620 [03:59<01:00,  3.59it/s] 65%|██████▌   | 403/620 [03:59<01:00,  3.60it/s] 65%|██████▌   | 404/620 [03:59<00:59,  3.61it/s] 65%|██████▌   | 405/620 [04:00<00:59,  3.61it/s] 65%|██████▌   | 406/620 [04:00<00:59,  3.61it/s] 66%|██████▌   | 407/620 [04:00<00:58,  3.61it/s] 66%|██████▌   | 408/620 [04:00<00:58,  3.62it/s] 66%|██████▌   | 409/620 [04:01<00:58,  3.62it/s] 66%|██████▌   | 410/620 [04:01<00:57,  3.62it/s] 66%|██████▋   | 411/620 [04:01<01:01,  3.39it/s] 66%|██████▋   | 412/620 [04:02<01:00,  3.45it/s] 67%|██████▋   | 413/620 [04:02<00:59,  3.49it/s] 67%|██████▋   | 414/620 [04:02<00:58,  3.53it/s] 67%|██████▋   | 415/620 [04:02<00:57,  3.56it/s] 67%|██████▋   | 416/620 [04:03<00:57,  3.58it/s] 67%|██████▋   | 417/620 [04:03<00:56,  3.59it/s] 67%|██████▋   | 418/620 [04:03<00:56,  3.60it/s] 68%|██████▊   | 419/620 [04:03<00:55,  3.60it/s] 68%|██████▊   | 420/620 [04:04<00:55,  3.61it/s] 68%|██████▊   | 421/620 [04:04<00:55,  3.61it/s] 68%|██████▊   | 422/620 [04:04<01:00,  3.28it/s] 68%|██████▊   | 423/620 [04:05<00:58,  3.38it/s] 68%|██████▊   | 424/620 [04:05<00:56,  3.45it/s] 69%|██████▊   | 425/620 [04:05<00:55,  3.50it/s] 69%|██████▊   | 426/620 [04:06<00:54,  3.54it/s] 69%|██████▉   | 427/620 [04:06<00:54,  3.56it/s] 69%|██████▉   | 428/620 [04:06<00:53,  3.57it/s] 69%|██████▉   | 429/620 [04:06<00:53,  3.58it/s] 69%|██████▉   | 430/620 [04:07<00:52,  3.60it/s] 70%|██████▉   | 431/620 [04:07<00:52,  3.60it/s] 70%|██████▉   | 432/620 [04:07<00:52,  3.61it/s] 70%|██████▉   | 433/620 [04:08<00:57,  3.27it/s] 70%|███████   | 434/620 [04:08<00:55,  3.37it/s] 70%|███████   | 435/620 [04:08<00:53,  3.44it/s] 70%|███████   | 436/620 [04:08<00:52,  3.49it/s] 70%|███████   | 437/620 [04:09<00:51,  3.53it/s] 71%|███████   | 438/620 [04:09<00:51,  3.55it/s] 71%|███████   | 439/620 [04:09<00:50,  3.57it/s] 71%|███████   | 440/620 [04:09<00:50,  3.58it/s] 71%|███████   | 441/620 [04:10<00:49,  3.59it/s] 71%|███████▏  | 442/620 [04:10<00:49,  3.60it/s] 71%|███████▏  | 443/620 [04:10<00:49,  3.61it/s] 72%|███████▏  | 444/620 [04:11<00:52,  3.37it/s] 72%|███████▏  | 445/620 [04:11<00:50,  3.44it/s] 72%|███████▏  | 446/620 [04:11<00:49,  3.49it/s] 72%|███████▏  | 447/620 [04:11<00:48,  3.53it/s] 72%|███████▏  | 448/620 [04:12<00:48,  3.56it/s] 72%|███████▏  | 449/620 [04:12<00:47,  3.58it/s] 73%|███████▎  | 450/620 [04:12<00:47,  3.58it/s] 73%|███████▎  | 451/620 [04:13<00:47,  3.59it/s] 73%|███████▎  | 452/620 [04:13<00:46,  3.60it/s] 73%|███████▎  | 453/620 [04:13<00:46,  3.60it/s] 73%|███████▎  | 454/620 [04:13<00:46,  3.60it/s] 73%|███████▎  | 455/620 [04:14<00:48,  3.38it/s] 74%|███████▎  | 456/620 [04:14<00:47,  3.45it/s] 74%|███████▎  | 457/620 [04:14<00:46,  3.49it/s] 74%|███████▍  | 458/620 [04:15<00:45,  3.53it/s] 74%|███████▍  | 459/620 [04:15<00:45,  3.56it/s] 74%|███████▍  | 460/620 [04:15<00:44,  3.57it/s] 74%|███████▍  | 461/620 [04:15<00:44,  3.59it/s] 75%|███████▍  | 462/620 [04:16<00:43,  3.60it/s] 75%|███████▍  | 463/620 [04:16<00:43,  3.60it/s] 75%|███████▍  | 464/620 [04:16<00:43,  3.60it/s] 75%|███████▌  | 465/620 [04:17<00:43,  3.60it/s] 75%|███████▌  | 466/620 [04:17<00:47,  3.23it/s] 75%|███████▌  | 467/620 [04:17<00:45,  3.34it/s] 75%|███████▌  | 468/620 [04:17<00:44,  3.41it/s] 76%|███████▌  | 469/620 [04:18<00:43,  3.47it/s] 76%|███████▌  | 470/620 [04:18<00:42,  3.51it/s] 76%|███████▌  | 471/620 [04:18<00:42,  3.54it/s] 76%|███████▌  | 472/620 [04:19<00:41,  3.57it/s] 76%|███████▋  | 473/620 [04:19<00:41,  3.58it/s] 76%|███████▋  | 474/620 [04:19<00:40,  3.58it/s] 77%|███████▋  | 475/620 [04:19<00:40,  3.59it/s] 77%|███████▋  | 476/620 [04:20<00:40,  3.60it/s] 77%|███████▋  | 477/620 [04:20<00:42,  3.35it/s] 77%|███████▋  | 478/620 [04:20<00:41,  3.42it/s] 77%|███████▋  | 479/620 [04:21<00:40,  3.48it/s] 77%|███████▋  | 480/620 [04:21<00:39,  3.52it/s] 78%|███████▊  | 481/620 [04:21<00:39,  3.55it/s] 78%|███████▊  | 482/620 [04:21<00:38,  3.57it/s] 78%|███████▊  | 483/620 [04:22<00:38,  3.58it/s] 78%|███████▊  | 484/620 [04:22<00:37,  3.59it/s] 78%|███████▊  | 485/620 [04:22<00:37,  3.60it/s] 78%|███████▊  | 486/620 [04:23<00:37,  3.60it/s] 79%|███████▊  | 487/620 [04:23<00:36,  3.61it/s] 79%|███████▊  | 488/620 [04:23<00:39,  3.30it/s] 79%|███████▉  | 489/620 [04:23<00:38,  3.39it/s] 79%|███████▉  | 490/620 [04:24<00:37,  3.46it/s] 79%|███████▉  | 491/620 [04:24<00:36,  3.50it/s] 79%|███████▉  | 492/620 [04:24<00:36,  3.54it/s] 80%|███████▉  | 493/620 [04:25<00:35,  3.56it/s] 80%|███████▉  | 494/620 [04:25<00:35,  3.58it/s] 80%|███████▉  | 495/620 [04:25<00:34,  3.59it/s] 80%|████████  | 496/620 [04:25<00:32,  3.85it/s][INFO|trainer.py:2140] 2023-08-28 08:03:16,668 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:03:16,668 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 08:03:16,668 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.0835, 'eval_samples_per_second': 346.308, 'eval_steps_per_second': 43.338, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.84it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.79it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.97it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.87it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.21it/s][A
  7%|▋         | 32/437 [00:00<00:10, 37.23it/s][A
  8%|▊         | 37/437 [00:00<00:10, 39.46it/s][A
 10%|▉         | 42/437 [00:00<00:09, 41.03it/s][A
 11%|█         | 47/437 [00:01<00:09, 42.18it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 42.97it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.51it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.93it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.99it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.78it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.66it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.87it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.14it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.40it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.56it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.64it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.65it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.42it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.12it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.01it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.09it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.34it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.49it/s][A
 32%|███▏      | 142/437 [00:03<00:09, 31.26it/s][A
 34%|███▎      | 147/437 [00:03<00:08, 34.37it/s][A
 35%|███▍      | 152/437 [00:03<00:07, 37.00it/s][A
 36%|███▌      | 157/437 [00:03<00:07, 39.07it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 40.62it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 41.93it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 42.70it/s][A
 41%|████      | 177/437 [00:04<00:06, 43.23it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.10it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.21it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.46it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.84it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.17it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.48it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.55it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.64it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.35it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.06it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.96it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.94it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.13it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.45it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.62it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.73it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.64it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.44it/s][A
 62%|██████▏   | 272/437 [00:06<00:04, 37.02it/s][A
 63%|██████▎   | 277/437 [00:06<00:04, 39.00it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 40.65it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 41.86it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 42.77it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.42it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 43.85it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.96it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.75it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.56it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.71it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.01it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.30it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.47it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.58it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.69it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.56it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.11it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.94it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.00it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.15it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.41it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.58it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.71it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.73it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.47it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 38.80it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 40.49it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 41.82it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 42.50it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.26it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.77it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.13it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.06it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.06it/s][A 80%|████████  | 496/620 [04:35<00:32,  3.85it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:03:27,277 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496
[INFO|configuration_utils.py:351] 2023-08-28 08:03:27,783 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:03:35,693 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:03:35,964 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:03:36,130 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496/special_tokens_map.json
 80%|████████  | 497/620 [04:49<14:49,  7.24s/it] 80%|████████  | 498/620 [04:49<10:30,  5.17s/it] 80%|████████  | 499/620 [04:49<07:27,  3.70s/it] 81%|████████  | 500/620 [04:50<05:21,  2.68s/it]                                                  81%|████████  | 500/620 [04:50<05:21,  2.68s/it] 81%|████████  | 501/620 [04:50<03:52,  1.96s/it] 81%|████████  | 502/620 [04:50<02:51,  1.45s/it] 81%|████████  | 503/620 [04:51<02:08,  1.10s/it] 81%|████████▏ | 504/620 [04:51<01:39,  1.17it/s] 81%|████████▏ | 505/620 [04:51<01:18,  1.47it/s] 82%|████████▏ | 506/620 [04:51<01:04,  1.78it/s] 82%|████████▏ | 507/620 [04:52<00:53,  2.10it/s] 82%|████████▏ | 508/620 [04:52<00:46,  2.39it/s] 82%|████████▏ | 509/620 [04:52<00:44,  2.48it/s] 82%|████████▏ | 510/620 [04:53<00:40,  2.74it/s] 82%|████████▏ | 511/620 [04:53<00:37,  2.94it/s] 83%|████████▎ | 512/620 [04:53<00:34,  3.11it/s] 83%|████████▎ | 513/620 [04:53<00:33,  3.24it/s] 83%|████████▎ | 514/620 [04:54<00:31,  3.33it/s] 83%|████████▎ | 515/620 [04:54<00:30,  3.40it/s] 83%|████████▎ | 516/620 [04:54<00:30,  3.45it/s] 83%|████████▎ | 517/620 [04:55<00:29,  3.49it/s] 84%|████████▎ | 518/620 [04:55<00:29,  3.51it/s] 84%|████████▎ | 519/620 [04:55<00:28,  3.53it/s] 84%|████████▍ | 520/620 [04:55<00:30,  3.33it/s] 84%|████████▍ | 521/620 [04:56<00:29,  3.40it/s] 84%|████████▍ | 522/620 [04:56<00:28,  3.45it/s] 84%|████████▍ | 523/620 [04:56<00:27,  3.48it/s] 85%|████████▍ | 524/620 [04:57<00:27,  3.51it/s] 85%|████████▍ | 525/620 [04:57<00:26,  3.53it/s] 85%|████████▍ | 526/620 [04:57<00:26,  3.54it/s] 85%|████████▌ | 527/620 [04:57<00:26,  3.55it/s] 85%|████████▌ | 528/620 [04:58<00:25,  3.55it/s] 85%|████████▌ | 529/620 [04:58<00:25,  3.56it/s] 85%|████████▌ | 530/620 [04:58<00:25,  3.56it/s] 86%|████████▌ | 531/620 [04:59<00:28,  3.14it/s] 86%|████████▌ | 532/620 [04:59<00:27,  3.26it/s] 86%|████████▌ | 533/620 [04:59<00:26,  3.35it/s] 86%|████████▌ | 534/620 [05:00<00:29,  2.88it/s] 86%|████████▋ | 535/620 [05:00<00:27,  3.06it/s] 86%|████████▋ | 536/620 [05:00<00:26,  3.19it/s] 87%|████████▋ | 537/620 [05:01<00:25,  3.29it/s] 87%|████████▋ | 538/620 [05:01<00:24,  3.37it/s] 87%|████████▋ | 539/620 [05:01<00:23,  3.43it/s] 87%|████████▋ | 540/620 [05:01<00:23,  3.47it/s] 87%|████████▋ | 541/620 [05:02<00:22,  3.50it/s] 87%|████████▋ | 542/620 [05:02<00:22,  3.52it/s] 88%|████████▊ | 543/620 [05:02<00:21,  3.53it/s] 88%|████████▊ | 544/620 [05:03<00:25,  3.03it/s] 88%|████████▊ | 545/620 [05:03<00:23,  3.17it/s] 88%|████████▊ | 546/620 [05:03<00:22,  3.28it/s] 88%|████████▊ | 547/620 [05:03<00:21,  3.37it/s] 88%|████████▊ | 548/620 [05:04<00:21,  3.43it/s] 89%|████████▊ | 549/620 [05:04<00:20,  3.47it/s] 89%|████████▊ | 550/620 [05:04<00:19,  3.50it/s] 89%|████████▉ | 551/620 [05:05<00:19,  3.53it/s] 89%|████████▉ | 552/620 [05:05<00:19,  3.56it/s] 89%|████████▉ | 553/620 [05:05<00:18,  3.57it/s] 89%|████████▉ | 554/620 [05:05<00:18,  3.59it/s] 90%|████████▉ | 555/620 [05:06<00:20,  3.20it/s] 90%|████████▉ | 556/620 [05:06<00:19,  3.32it/s] 90%|████████▉ | 557/620 [05:06<00:18,  3.40it/s] 90%|█████████ | 558/620 [05:07<00:17,  3.46it/s] 90%|█████████ | 559/620 [05:07<00:17,  3.51it/s] 90%|█████████ | 560/620 [05:07<00:16,  3.54it/s] 90%|█████████ | 561/620 [05:07<00:16,  3.57it/s] 91%|█████████ | 562/620 [05:08<00:16,  3.58it/s] 91%|█████████ | 563/620 [05:08<00:15,  3.60it/s] 91%|█████████ | 564/620 [05:08<00:15,  3.59it/s] 91%|█████████ | 565/620 [05:09<00:15,  3.60it/s] 91%|█████████▏| 566/620 [05:09<00:16,  3.18it/s] 91%|█████████▏| 567/620 [05:09<00:16,  3.30it/s] 92%|█████████▏| 568/620 [05:10<00:15,  3.39it/s] 92%|█████████▏| 569/620 [05:10<00:14,  3.46it/s] 92%|█████████▏| 570/620 [05:10<00:14,  3.50it/s] 92%|█████████▏| 571/620 [05:10<00:13,  3.54it/s] 92%|█████████▏| 572/620 [05:11<00:13,  3.57it/s] 92%|█████████▏| 573/620 [05:11<00:13,  3.58it/s] 93%|█████████▎| 574/620 [05:11<00:12,  3.59it/s] 93%|█████████▎| 575/620 [05:11<00:12,  3.60it/s] 93%|█████████▎| 576/620 [05:12<00:12,  3.61it/s] 93%|█████████▎| 577/620 [05:12<00:13,  3.30it/s] 93%|█████████▎| 578/620 [05:12<00:12,  3.39it/s] 93%|█████████▎| 579/620 [05:13<00:11,  3.46it/s] 94%|█████████▎| 580/620 [05:13<00:11,  3.51it/s] 94%|█████████▎| 581/620 [05:13<00:11,  3.54it/s] 94%|█████████▍| 582/620 [05:14<00:10,  3.56it/s] 94%|█████████▍| 583/620 [05:14<00:10,  3.58it/s] 94%|█████████▍| 584/620 [05:14<00:10,  3.60it/s] 94%|█████████▍| 585/620 [05:14<00:09,  3.60it/s] 95%|█████████▍| 586/620 [05:15<00:09,  3.61it/s] 95%|█████████▍| 587/620 [05:15<00:09,  3.61it/s] 95%|█████████▍| 588/620 [05:15<00:09,  3.29it/s] 95%|█████████▌| 589/620 [05:16<00:09,  3.38it/s] 95%|█████████▌| 590/620 [05:16<00:08,  3.45it/s] 95%|█████████▌| 591/620 [05:16<00:08,  3.50it/s] 95%|█████████▌| 592/620 [05:16<00:07,  3.54it/s] 96%|█████████▌| 593/620 [05:17<00:07,  3.56it/s] 96%|█████████▌| 594/620 [05:17<00:07,  3.58it/s] 96%|█████████▌| 595/620 [05:17<00:06,  3.59it/s] 96%|█████████▌| 596/620 [05:17<00:06,  3.60it/s] 96%|█████████▋| 597/620 [05:18<00:06,  3.61it/s] 96%|█████████▋| 598/620 [05:18<00:06,  3.61it/s] 97%|█████████▋| 599/620 [05:18<00:06,  3.21it/s] 97%|█████████▋| 600/620 [05:19<00:06,  3.32it/s] 97%|█████████▋| 601/620 [05:19<00:05,  3.40it/s] 97%|█████████▋| 602/620 [05:19<00:05,  3.47it/s] 97%|█████████▋| 603/620 [05:20<00:04,  3.51it/s] 97%|█████████▋| 604/620 [05:20<00:04,  3.54it/s] 98%|█████████▊| 605/620 [05:20<00:04,  3.57it/s] 98%|█████████▊| 606/620 [05:20<00:03,  3.59it/s] 98%|█████████▊| 607/620 [05:21<00:03,  3.60it/s] 98%|█████████▊| 608/620 [05:21<00:03,  3.61it/s] 98%|█████████▊| 609/620 [05:21<00:03,  3.61it/s] 98%|█████████▊| 610/620 [05:22<00:03,  3.28it/s] 99%|█████████▊| 611/620 [05:22<00:02,  3.37it/s] 99%|█████████▊| 612/620 [05:22<00:02,  3.44it/s] 99%|█████████▉| 613/620 [05:22<00:02,  3.50it/s] 99%|█████████▉| 614/620 [05:23<00:01,  3.51it/s] 99%|█████████▉| 615/620 [05:23<00:01,  3.53it/s] 99%|█████████▉| 616/620 [05:23<00:01,  3.54it/s]100%|█████████▉| 617/620 [05:23<00:00,  3.54it/s]100%|█████████▉| 618/620 [05:24<00:00,  3.55it/s]100%|█████████▉| 619/620 [05:24<00:00,  3.55it/s]100%|██████████| 620/620 [05:24<00:00,  3.81it/s][INFO|trainer.py:2140] 2023-08-28 08:04:15,721 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:04:15,721 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 08:04:15,721 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.1697, 'eval_samples_per_second': 343.374, 'eval_steps_per_second': 42.971, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7298387096774197e-05, 'epoch': 4.03}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.72it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.00it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.21it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.42it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.86it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.56it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.44it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.75it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.19it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.19it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.29it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.44it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.54it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.68it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.68it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.67it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.44it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.12it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.07it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.15it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.31it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.38it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.58it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.63it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.46it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.31it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 40.11it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 41.51it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 42.46it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.15it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.72it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.03it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.30it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 44.13it/s][A
 41%|████      | 177/437 [00:03<00:05, 43.90it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.76it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.02it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.24it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.41it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.55it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.70it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.69it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.33it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.96it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.09it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.10it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.28it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.57it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.76it/s][A
 58%|█████▊    | 252/437 [00:06<00:08, 21.49it/s][A
 59%|█████▉    | 257/437 [00:06<00:07, 25.49it/s][A
 60%|█████▉    | 262/437 [00:06<00:05, 29.27it/s][A
 61%|██████    | 267/437 [00:06<00:05, 32.70it/s][A
 62%|██████▏   | 272/437 [00:06<00:04, 35.60it/s][A
 63%|██████▎   | 277/437 [00:06<00:04, 37.99it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 39.85it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 41.06it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 41.71it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 42.14it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 42.74it/s][A
 70%|███████   | 307/437 [00:07<00:03, 43.29it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.75it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.17it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.42it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.49it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.33it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.03it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 43.92it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.03it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.14it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.42it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.58it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 36.77it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 39.03it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 40.67it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 41.85it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 42.46it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.24it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.72it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.78it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.52it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.52it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.66it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.95it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.19it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.45it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.54it/s][A                                                 
                                                 [A100%|██████████| 620/620 [05:35<00:00,  3.81it/s]
100%|██████████| 437/437 [00:10<00:00, 44.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:04:26,258 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620
[INFO|configuration_utils.py:351] 2023-08-28 08:04:26,814 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:04:55,503 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:04:56,434 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:04:57,102 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 08:05:01,518 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 08:05:01,645 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124 (score: 1.1054370403289795).
                                                 100%|██████████| 620/620 [06:37<00:00,  3.81it/s]100%|██████████| 620/620 [06:37<00:00,  1.56it/s]
[INFO|trainer.py:1894] 2023-08-28 08:05:28,567 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 08:05:28,851 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:05:36,025 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:05:36,391 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:05:36,570 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:05:37,740 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:37,740 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:37,740 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:37,740 >>   train_runtime            = 0:06:37.46
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:37,740 >>   train_samples            =       7920
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:37,740 >>   train_samples_per_second =     99.631
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:37,740 >>   train_steps_per_second   =       1.56
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.3369, 'eval_samples_per_second': 337.82, 'eval_steps_per_second': 42.276, 'epoch': 5.0}
{'train_runtime': 397.4675, 'train_samples_per_second': 99.631, 'train_steps_per_second': 1.56, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 08:05:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 08:05:38,407 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:05:38,407 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 08:05:38,408 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.10it/s]  3%|▎         | 12/437 [00:00<00:08, 49.05it/s]  4%|▍         | 17/437 [00:00<00:08, 47.53it/s]  5%|▌         | 22/437 [00:00<00:08, 46.72it/s]  6%|▌         | 27/437 [00:00<00:08, 46.16it/s]  7%|▋         | 32/437 [00:00<00:08, 45.92it/s]  8%|▊         | 37/437 [00:00<00:09, 41.82it/s] 10%|▉         | 42/437 [00:00<00:09, 42.56it/s] 11%|█         | 47/437 [00:01<00:09, 43.11it/s] 12%|█▏        | 52/437 [00:01<00:08, 43.58it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.09it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.49it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.71it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.92it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.72it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.81it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.73it/s] 21%|██        | 92/437 [00:02<00:07, 44.69it/s] 22%|██▏       | 97/437 [00:02<00:07, 44.64it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.81it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.95it/s] 26%|██▌       | 112/437 [00:02<00:07, 45.04it/s] 27%|██▋       | 117/437 [00:02<00:07, 45.06it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.89it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.94it/s] 30%|███       | 132/437 [00:02<00:06, 44.79it/s] 31%|███▏      | 137/437 [00:03<00:06, 44.77it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.78it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.74it/s] 35%|███▍      | 152/437 [00:03<00:06, 45.03it/s] 36%|███▌      | 157/437 [00:03<00:06, 45.07it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.98it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.88it/s] 39%|███▉      | 172/437 [00:03<00:06, 39.88it/s] 41%|████      | 177/437 [00:03<00:06, 41.43it/s] 42%|████▏     | 182/437 [00:04<00:05, 42.61it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.32it/s] 44%|████▍     | 192/437 [00:04<00:05, 43.83it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.31it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.61it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.69it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.20it/s] 50%|████▉     | 217/437 [00:04<00:04, 44.20it/s] 51%|█████     | 222/437 [00:04<00:04, 44.41it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.66it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.91it/s] 54%|█████▍    | 237/437 [00:05<00:04, 45.07it/s] 55%|█████▌    | 242/437 [00:05<00:04, 45.18it/s] 57%|█████▋    | 247/437 [00:05<00:04, 45.20it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.82it/s] 59%|█████▉    | 257/437 [00:05<00:04, 44.49it/s] 60%|█████▉    | 262/437 [00:05<00:03, 44.28it/s] 61%|██████    | 267/437 [00:05<00:03, 44.45it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.71it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.88it/s] 65%|██████▍   | 282/437 [00:06<00:03, 45.04it/s] 66%|██████▌   | 287/437 [00:06<00:03, 45.12it/s] 67%|██████▋   | 292/437 [00:06<00:03, 45.15it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.79it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.54it/s] 70%|███████   | 307/437 [00:06<00:03, 35.47it/s] 71%|███████▏  | 312/437 [00:07<00:03, 37.93it/s] 73%|███████▎  | 317/437 [00:07<00:03, 39.90it/s] 74%|███████▎  | 322/437 [00:07<00:02, 41.33it/s] 75%|███████▍  | 327/437 [00:07<00:02, 42.49it/s] 76%|███████▌  | 332/437 [00:07<00:02, 43.30it/s] 77%|███████▋  | 337/437 [00:07<00:02, 43.96it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.22it/s] 79%|███████▉  | 347/437 [00:07<00:02, 43.98it/s] 81%|████████  | 352/437 [00:07<00:01, 43.94it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.06it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.30it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.59it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.81it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.89it/s] 87%|████████▋ | 382/437 [00:08<00:01, 45.11it/s] 89%|████████▊ | 387/437 [00:08<00:01, 45.00it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.61it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.47it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.45it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.55it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.76it/s] 95%|█████████▌| 417/437 [00:09<00:00, 44.86it/s] 97%|█████████▋| 422/437 [00:09<00:00, 44.98it/s] 98%|█████████▊| 427/437 [00:09<00:00, 44.86it/s] 99%|█████████▉| 432/437 [00:09<00:00, 44.76it/s]100%|██████████| 437/437 [00:09<00:00, 44.62it/s]100%|██████████| 437/437 [00:09<00:00, 43.82it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:05:48,398 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:48,398 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:48,398 >>   eval_loss               =     1.1054
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:48,398 >>   eval_runtime            = 0:00:09.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:48,398 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:48,398 >>   eval_samples_per_second =    349.547
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:48,398 >>   eval_steps_per_second   =     43.743
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:05:48,398 >>   perplexity              =     3.0205
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:13,254 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:13,318 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:13,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:13,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:13,319 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:06:14,478 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:06:14,479 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:06:15,240 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:06:16,539 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:06:16,638 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:20,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:20,334 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:20,335 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:20,335 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:06:20,335 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:06:21,582 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:06:21,583 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:06:22,339 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:06:22,643 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:06:22,643 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-248
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-496
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-372
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-124
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/checkpoint-620
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.58it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.67it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:10,  1.57it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.67it/s]Extractor Predicting: 21it [00:13,  1.59it/s]Extractor Predicting: 22it [00:13,  1.61it/s]Extractor Predicting: 23it [00:14,  1.65it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:15,  1.69it/s]Extractor Predicting: 26it [00:16,  1.60it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.62it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:20,  1.65it/s]Extractor Predicting: 34it [00:21,  1.64it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:22,  1.67it/s]Extractor Predicting: 37it [00:22,  1.67it/s]Extractor Predicting: 38it [00:23,  1.67it/s]Extractor Predicting: 39it [00:23,  1.68it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:26,  1.67it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:27,  1.58it/s]Extractor Predicting: 46it [00:28,  1.61it/s]Extractor Predicting: 47it [00:28,  1.64it/s]Extractor Predicting: 48it [00:29,  1.65it/s]Extractor Predicting: 49it [00:30,  1.66it/s]Extractor Predicting: 50it [00:30,  1.63it/s]Extractor Predicting: 51it [00:31,  1.60it/s]Extractor Predicting: 52it [00:31,  1.61it/s]Extractor Predicting: 53it [00:32,  1.63it/s]Extractor Predicting: 54it [00:33,  1.64it/s]Extractor Predicting: 55it [00:33,  1.57it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:36,  1.65it/s]Extractor Predicting: 60it [00:37,  1.53it/s]Extractor Predicting: 61it [00:37,  1.55it/s]Extractor Predicting: 62it [00:38,  1.57it/s]Extractor Predicting: 63it [00:38,  1.61it/s]Extractor Predicting: 64it [00:39,  1.57it/s]Extractor Predicting: 65it [00:40,  1.51it/s]Extractor Predicting: 66it [00:40,  1.52it/s]Extractor Predicting: 67it [00:41,  1.55it/s]Extractor Predicting: 68it [00:42,  1.55it/s]Extractor Predicting: 69it [00:42,  1.56it/s]Extractor Predicting: 70it [00:43,  1.50it/s]Extractor Predicting: 71it [00:44,  1.52it/s]Extractor Predicting: 72it [00:44,  1.52it/s]Extractor Predicting: 73it [00:45,  1.55it/s]Extractor Predicting: 74it [00:46,  1.53it/s]Extractor Predicting: 75it [00:46,  1.54it/s]Extractor Predicting: 76it [00:47,  1.56it/s]Extractor Predicting: 77it [00:48,  1.55it/s]Extractor Predicting: 78it [00:48,  1.59it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:50,  1.48it/s]Extractor Predicting: 81it [00:50,  1.38it/s]Extractor Predicting: 82it [00:51,  1.45it/s]Extractor Predicting: 83it [00:52,  1.45it/s]Extractor Predicting: 84it [00:52,  1.51it/s]Extractor Predicting: 85it [00:53,  1.52it/s]Extractor Predicting: 86it [00:54,  1.51it/s]Extractor Predicting: 87it [00:54,  1.53it/s]Extractor Predicting: 88it [00:55,  1.54it/s]Extractor Predicting: 89it [00:55,  1.62it/s]Extractor Predicting: 90it [00:56,  1.62it/s]Extractor Predicting: 91it [00:57,  1.70it/s]Extractor Predicting: 92it [00:57,  1.76it/s]Extractor Predicting: 93it [00:58,  1.81it/s]Extractor Predicting: 94it [00:58,  1.80it/s]Extractor Predicting: 95it [00:59,  1.75it/s]Extractor Predicting: 96it [00:59,  1.79it/s]Extractor Predicting: 97it [01:00,  1.75it/s]Extractor Predicting: 98it [01:00,  1.75it/s]Extractor Predicting: 99it [01:01,  1.82it/s]Extractor Predicting: 100it [01:02,  1.77it/s]Extractor Predicting: 101it [01:02,  1.76it/s]Extractor Predicting: 102it [01:03,  1.71it/s]Extractor Predicting: 103it [01:03,  1.73it/s]Extractor Predicting: 104it [01:04,  1.68it/s]Extractor Predicting: 105it [01:05,  1.61it/s]Extractor Predicting: 106it [01:05,  1.66it/s]Extractor Predicting: 107it [01:06,  1.64it/s]Extractor Predicting: 108it [01:06,  1.69it/s]Extractor Predicting: 109it [01:07,  1.72it/s]Extractor Predicting: 110it [01:07,  1.74it/s]Extractor Predicting: 111it [01:08,  1.70it/s]Extractor Predicting: 112it [01:09,  1.72it/s]Extractor Predicting: 113it [01:09,  1.77it/s]Extractor Predicting: 114it [01:10,  1.72it/s]Extractor Predicting: 115it [01:10,  1.77it/s]Extractor Predicting: 116it [01:11,  1.73it/s]Extractor Predicting: 117it [01:12,  1.65it/s]Extractor Predicting: 118it [01:12,  1.67it/s]Extractor Predicting: 119it [01:13,  1.66it/s]Extractor Predicting: 120it [01:13,  1.63it/s]Extractor Predicting: 121it [01:14,  1.64it/s]Extractor Predicting: 122it [01:15,  1.54it/s]Extractor Predicting: 123it [01:15,  1.56it/s]Extractor Predicting: 124it [01:16,  1.56it/s]Extractor Predicting: 125it [01:17,  1.58it/s]Extractor Predicting: 126it [01:17,  1.58it/s]Extractor Predicting: 127it [01:18,  1.51it/s]Extractor Predicting: 128it [01:19,  1.54it/s]Extractor Predicting: 129it [01:19,  1.58it/s]Extractor Predicting: 130it [01:20,  1.63it/s]Extractor Predicting: 131it [01:20,  1.60it/s]Extractor Predicting: 132it [01:21,  1.55it/s]Extractor Predicting: 133it [01:22,  1.56it/s]Extractor Predicting: 134it [01:23,  1.50it/s]Extractor Predicting: 135it [01:23,  1.54it/s]Extractor Predicting: 136it [01:24,  1.55it/s]Extractor Predicting: 137it [01:24,  1.58it/s]Extractor Predicting: 138it [01:25,  1.61it/s]Extractor Predicting: 139it [01:26,  1.56it/s]Extractor Predicting: 140it [01:26,  1.62it/s]Extractor Predicting: 141it [01:27,  1.61it/s]Extractor Predicting: 142it [01:27,  1.61it/s]Extractor Predicting: 143it [01:28,  1.63it/s]Extractor Predicting: 144it [01:29,  1.59it/s]Extractor Predicting: 144it [01:29,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:43,634 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:43,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:43,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:43,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:43,697 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:10:44,960 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:10:44,961 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:10:45,719 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:10:46,939 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:10:47,013 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:50,403 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:50,506 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:50,506 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:50,506 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:10:50,506 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:10:51,681 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:10:51,683 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:10:52,388 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:10:52,704 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:10:52,704 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:08,  1.63it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.71it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.64it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.52it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:14,  1.63it/s]Extractor Predicting: 25it [00:15,  1.67it/s]Extractor Predicting: 26it [00:15,  1.66it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:17,  1.58it/s]Extractor Predicting: 29it [00:17,  1.58it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:19,  1.60it/s]Extractor Predicting: 33it [00:20,  1.54it/s]Extractor Predicting: 34it [00:21,  1.59it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.65it/s]Extractor Predicting: 37it [00:22,  1.71it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:23,  1.67it/s]Extractor Predicting: 40it [00:24,  1.69it/s]Extractor Predicting: 41it [00:25,  1.70it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:26,  1.69it/s]Extractor Predicting: 44it [00:26,  1.65it/s]Extractor Predicting: 45it [00:27,  1.65it/s]Extractor Predicting: 46it [00:28,  1.68it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:29,  1.70it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.62it/s]Extractor Predicting: 51it [00:31,  1.66it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:32,  1.70it/s]Extractor Predicting: 54it [00:32,  1.70it/s]Extractor Predicting: 55it [00:33,  1.67it/s]Extractor Predicting: 56it [00:34,  1.61it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:35,  1.72it/s]Extractor Predicting: 59it [00:35,  1.76it/s]Extractor Predicting: 60it [00:36,  1.75it/s]Extractor Predicting: 61it [00:36,  1.74it/s]Extractor Predicting: 62it [00:37,  1.66it/s]Extractor Predicting: 63it [00:38,  1.69it/s]Extractor Predicting: 64it [00:38,  1.66it/s]Extractor Predicting: 65it [00:39,  1.68it/s]Extractor Predicting: 66it [00:39,  1.68it/s]Extractor Predicting: 67it [00:40,  1.64it/s]Extractor Predicting: 68it [00:41,  1.73it/s]Extractor Predicting: 69it [00:41,  1.71it/s]Extractor Predicting: 70it [00:42,  1.73it/s]Extractor Predicting: 71it [00:42,  1.73it/s]Extractor Predicting: 72it [00:43,  1.73it/s]Extractor Predicting: 73it [00:44,  1.62it/s]Extractor Predicting: 74it [00:44,  1.63it/s]Extractor Predicting: 75it [00:45,  1.67it/s]Extractor Predicting: 76it [00:45,  1.69it/s]Extractor Predicting: 77it [00:46,  1.67it/s]Extractor Predicting: 78it [00:47,  1.68it/s]Extractor Predicting: 79it [00:47,  1.64it/s]Extractor Predicting: 80it [00:48,  1.67it/s]Extractor Predicting: 81it [00:48,  1.68it/s]Extractor Predicting: 82it [00:49,  1.67it/s]Extractor Predicting: 83it [00:50,  1.69it/s]Extractor Predicting: 84it [00:50,  1.70it/s]Extractor Predicting: 85it [00:51,  1.57it/s]Extractor Predicting: 86it [00:52,  1.59it/s]Extractor Predicting: 87it [00:52,  1.62it/s]Extractor Predicting: 88it [00:53,  1.63it/s]Extractor Predicting: 89it [00:53,  1.68it/s]Extractor Predicting: 90it [00:54,  1.64it/s]Extractor Predicting: 91it [00:55,  1.66it/s]Extractor Predicting: 92it [00:55,  1.49it/s]Extractor Predicting: 93it [00:56,  1.57it/s]Extractor Predicting: 94it [00:57,  1.58it/s]Extractor Predicting: 95it [00:57,  1.56it/s]Extractor Predicting: 96it [00:58,  1.61it/s]Extractor Predicting: 97it [00:58,  1.64it/s]Extractor Predicting: 98it [00:59,  1.65it/s]Extractor Predicting: 99it [01:00,  1.68it/s]Extractor Predicting: 100it [01:00,  1.71it/s]Extractor Predicting: 101it [01:01,  1.67it/s]Extractor Predicting: 102it [01:01,  1.69it/s]Extractor Predicting: 103it [01:02,  1.66it/s]Extractor Predicting: 104it [01:03,  1.65it/s]Extractor Predicting: 105it [01:03,  1.66it/s]Extractor Predicting: 106it [01:04,  1.62it/s]Extractor Predicting: 107it [01:04,  1.62it/s]Extractor Predicting: 108it [01:05,  1.66it/s]Extractor Predicting: 109it [01:06,  1.65it/s]Extractor Predicting: 110it [01:06,  1.67it/s]Extractor Predicting: 111it [01:07,  1.58it/s]Extractor Predicting: 112it [01:07,  1.59it/s]Extractor Predicting: 113it [01:08,  1.60it/s]Extractor Predicting: 114it [01:09,  1.61it/s]Extractor Predicting: 115it [01:09,  1.62it/s]Extractor Predicting: 116it [01:10,  1.67it/s]Extractor Predicting: 117it [01:10,  1.69it/s]Extractor Predicting: 118it [01:11,  1.70it/s]Extractor Predicting: 119it [01:12,  1.69it/s]Extractor Predicting: 120it [01:12,  1.68it/s]Extractor Predicting: 121it [01:13,  1.71it/s]Extractor Predicting: 122it [01:13,  1.69it/s]Extractor Predicting: 123it [01:14,  1.73it/s]Extractor Predicting: 124it [01:14,  1.75it/s]Extractor Predicting: 125it [01:15,  1.74it/s]Extractor Predicting: 126it [01:16,  1.72it/s]Extractor Predicting: 127it [01:16,  1.74it/s]Extractor Predicting: 128it [01:17,  1.73it/s]Extractor Predicting: 129it [01:17,  1.69it/s]Extractor Predicting: 130it [01:18,  1.64it/s]Extractor Predicting: 131it [01:19,  1.69it/s]Extractor Predicting: 132it [01:19,  1.69it/s]Extractor Predicting: 133it [01:20,  1.71it/s]Extractor Predicting: 134it [01:20,  1.72it/s]Extractor Predicting: 135it [01:21,  1.73it/s]Extractor Predicting: 136it [01:22,  1.66it/s]Extractor Predicting: 137it [01:22,  1.68it/s]Extractor Predicting: 138it [01:23,  1.67it/s]Extractor Predicting: 139it [01:23,  1.70it/s]Extractor Predicting: 140it [01:24,  1.77it/s]Extractor Predicting: 141it [01:24,  1.78it/s]Extractor Predicting: 142it [01:25,  1.64it/s]Extractor Predicting: 143it [01:26,  1.66it/s]Extractor Predicting: 144it [01:26,  1.68it/s]Extractor Predicting: 145it [01:27,  1.71it/s]Extractor Predicting: 146it [01:27,  1.76it/s]Extractor Predicting: 147it [01:28,  1.79it/s]Extractor Predicting: 148it [01:29,  1.61it/s]Extractor Predicting: 149it [01:29,  1.67it/s]Extractor Predicting: 150it [01:30,  1.68it/s]Extractor Predicting: 151it [01:30,  1.70it/s]Extractor Predicting: 152it [01:31,  1.72it/s]Extractor Predicting: 153it [01:32,  1.64it/s]Extractor Predicting: 154it [01:32,  1.68it/s]Extractor Predicting: 155it [01:33,  1.72it/s]Extractor Predicting: 156it [01:33,  1.76it/s]Extractor Predicting: 157it [01:34,  1.77it/s]Extractor Predicting: 158it [01:34,  1.84it/s]Extractor Predicting: 159it [01:35,  1.71it/s]Extractor Predicting: 160it [01:36,  1.69it/s]Extractor Predicting: 161it [01:36,  1.69it/s]Extractor Predicting: 162it [01:37,  1.68it/s]Extractor Predicting: 163it [01:37,  1.72it/s]Extractor Predicting: 164it [01:38,  1.65it/s]Extractor Predicting: 165it [01:39,  1.71it/s]Extractor Predicting: 166it [01:39,  1.76it/s]Extractor Predicting: 167it [01:40,  1.76it/s]Extractor Predicting: 168it [01:40,  1.72it/s]Extractor Predicting: 169it [01:41,  1.74it/s]Extractor Predicting: 170it [01:42,  1.48it/s]Extractor Predicting: 171it [01:42,  1.55it/s]Extractor Predicting: 172it [01:43,  1.61it/s]Extractor Predicting: 173it [01:43,  1.63it/s]Extractor Predicting: 174it [01:44,  1.66it/s]Extractor Predicting: 175it [01:45,  1.59it/s]Extractor Predicting: 176it [01:46,  1.42it/s]Extractor Predicting: 177it [01:46,  1.52it/s]Extractor Predicting: 178it [01:47,  1.57it/s]Extractor Predicting: 179it [01:47,  1.64it/s]Extractor Predicting: 180it [01:48,  1.70it/s]Extractor Predicting: 181it [01:49,  1.56it/s]Extractor Predicting: 182it [01:49,  1.61it/s]Extractor Predicting: 183it [01:50,  1.65it/s]Extractor Predicting: 184it [01:50,  1.65it/s]Extractor Predicting: 185it [01:51,  1.66it/s]Extractor Predicting: 186it [01:52,  1.60it/s]Extractor Predicting: 187it [01:52,  1.63it/s]Extractor Predicting: 188it [01:53,  1.63it/s]Extractor Predicting: 189it [01:53,  1.64it/s]Extractor Predicting: 190it [01:54,  1.66it/s]Extractor Predicting: 191it [01:55,  1.64it/s]Extractor Predicting: 192it [01:55,  1.70it/s]Extractor Predicting: 193it [01:56,  1.47it/s]Extractor Predicting: 194it [01:57,  1.51it/s]Extractor Predicting: 195it [01:57,  1.59it/s]Extractor Predicting: 196it [01:58,  1.54it/s]Extractor Predicting: 197it [01:59,  1.60it/s]Extractor Predicting: 198it [01:59,  1.61it/s]Extractor Predicting: 199it [02:00,  1.65it/s]Extractor Predicting: 200it [02:00,  1.73it/s]Extractor Predicting: 201it [02:01,  1.76it/s]Extractor Predicting: 202it [02:01,  1.66it/s]Extractor Predicting: 203it [02:02,  1.67it/s]Extractor Predicting: 204it [02:03,  1.69it/s]Extractor Predicting: 205it [02:03,  1.72it/s]Extractor Predicting: 206it [02:04,  1.68it/s]Extractor Predicting: 207it [02:04,  1.64it/s]Extractor Predicting: 208it [02:05,  1.63it/s]Extractor Predicting: 209it [02:06,  1.66it/s]Extractor Predicting: 210it [02:06,  1.66it/s]Extractor Predicting: 211it [02:07,  1.68it/s]Extractor Predicting: 212it [02:08,  1.62it/s]Extractor Predicting: 213it [02:08,  1.64it/s]Extractor Predicting: 214it [02:09,  1.63it/s]Extractor Predicting: 215it [02:09,  1.63it/s]Extractor Predicting: 216it [02:10,  1.68it/s]Extractor Predicting: 217it [02:11,  1.61it/s]Extractor Predicting: 218it [02:11,  1.62it/s]Extractor Predicting: 219it [02:12,  1.63it/s]Extractor Predicting: 220it [02:12,  1.67it/s]Extractor Predicting: 221it [02:13,  1.69it/s]Extractor Predicting: 222it [02:14,  1.60it/s]Extractor Predicting: 223it [02:14,  1.65it/s]Extractor Predicting: 224it [02:15,  1.67it/s]Extractor Predicting: 225it [02:15,  1.66it/s]Extractor Predicting: 226it [02:16,  1.66it/s]Extractor Predicting: 227it [02:17,  1.68it/s]Extractor Predicting: 228it [02:17,  1.70it/s]Extractor Predicting: 229it [02:18,  1.72it/s]Extractor Predicting: 230it [02:18,  1.74it/s]Extractor Predicting: 231it [02:19,  1.72it/s]Extractor Predicting: 232it [02:20,  1.67it/s]Extractor Predicting: 233it [02:20,  1.68it/s]Extractor Predicting: 234it [02:21,  1.71it/s]Extractor Predicting: 235it [02:21,  1.72it/s]Extractor Predicting: 236it [02:22,  1.74it/s]Extractor Predicting: 237it [02:22,  1.72it/s]Extractor Predicting: 238it [02:23,  1.66it/s]Extractor Predicting: 239it [02:24,  1.66it/s]Extractor Predicting: 240it [02:24,  1.71it/s]Extractor Predicting: 241it [02:25,  1.73it/s]Extractor Predicting: 242it [02:25,  1.77it/s]Extractor Predicting: 243it [02:26,  1.76it/s]Extractor Predicting: 244it [02:26,  1.69it/s]Extractor Predicting: 245it [02:27,  1.70it/s]Extractor Predicting: 246it [02:28,  1.71it/s]Extractor Predicting: 247it [02:28,  1.74it/s]Extractor Predicting: 248it [02:29,  1.74it/s]Extractor Predicting: 249it [02:29,  1.75it/s]Extractor Predicting: 250it [02:30,  1.72it/s]Extractor Predicting: 251it [02:31,  1.69it/s]Extractor Predicting: 252it [02:31,  1.69it/s]Extractor Predicting: 253it [02:32,  1.76it/s]Extractor Predicting: 254it [02:32,  1.74it/s]Extractor Predicting: 255it [02:33,  1.72it/s]Extractor Predicting: 256it [02:34,  1.65it/s]Extractor Predicting: 257it [02:34,  1.69it/s]Extractor Predicting: 258it [02:35,  1.74it/s]Extractor Predicting: 259it [02:35,  1.77it/s]Extractor Predicting: 260it [02:36,  1.78it/s]Extractor Predicting: 261it [02:36,  1.74it/s]Extractor Predicting: 262it [02:37,  1.68it/s]Extractor Predicting: 263it [02:38,  1.69it/s]Extractor Predicting: 264it [02:38,  1.73it/s]Extractor Predicting: 265it [02:39,  1.76it/s]Extractor Predicting: 266it [02:39,  1.82it/s]Extractor Predicting: 267it [02:40,  1.76it/s]Extractor Predicting: 268it [02:40,  1.68it/s]Extractor Predicting: 269it [02:41,  1.73it/s]Extractor Predicting: 270it [02:42,  1.75it/s]Extractor Predicting: 271it [02:42,  1.79it/s]Extractor Predicting: 272it [02:43,  1.83it/s]Extractor Predicting: 273it [02:43,  1.80it/s]Extractor Predicting: 274it [02:44,  1.72it/s]Extractor Predicting: 275it [02:44,  1.74it/s]Extractor Predicting: 276it [02:45,  1.56it/s]Extractor Predicting: 277it [02:46,  1.61it/s]Extractor Predicting: 278it [02:46,  1.65it/s]Extractor Predicting: 279it [02:47,  1.56it/s]Extractor Predicting: 280it [02:48,  1.62it/s]Extractor Predicting: 281it [02:48,  1.64it/s]Extractor Predicting: 281it [02:48,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:05,291 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:05,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:05,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:05,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:05,391 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:14:06,466 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:14:06,467 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:14:06,935 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:14:08,171 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:14:08,171 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:11,724 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:11,828 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:11,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:11,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:14:11,829 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:14:12,945 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:14:12,946 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:14:13,652 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:14:14,015 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:14:14,015 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.52it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.99it/s]Extractor Predicting: 6it [00:03,  1.71it/s]
[INFO|configuration_utils.py:515] 2023-08-28 08:14:23,392 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:14:23,394 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 08:14:23,526 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:14:23,527 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 08:14:23,603 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 08:14:55,440 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 08:14:55,440 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 08:14:56,528 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:14:56,653 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 08:14:57,046 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:14:57,323 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:14:57,323 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:14:57,323 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:14:57,323 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:14:57,323 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:14:57,323 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 08:14:58,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:14:58,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:14:59,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:00,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:01,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:01,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:02,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:03,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:04,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:04,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:05,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:06,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:07,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:07,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:08,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:09,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:10,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:10,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:11,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:12,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:13,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:13,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:47, 16.23s/it][WARNING|generation_utils.py:914] 2023-08-28 08:15:14,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:15,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:15,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:16,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:17,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:17,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:18,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:19,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:20,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:20,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:21,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:22,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:22,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:23,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:24,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:25,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:25,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:26,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:27,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:28,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:28,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:29,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:30,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:31,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:37, 16.77s/it][WARNING|generation_utils.py:914] 2023-08-28 08:15:31,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:32,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:33,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:33,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:34,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:35,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:35,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:36,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:37,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:37,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:38,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:39,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:39,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:40,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:41,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:41,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:42,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:43,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:43,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:44,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:45,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:45,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:46,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:47,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:47,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:48,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:50<03:24, 17.08s/it][WARNING|generation_utils.py:914] 2023-08-28 08:15:49,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:49,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:50,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:51,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:51,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:52,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:52,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:53,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:54,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:54,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:55,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:55,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:56,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:57,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:57,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:58,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:59,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:15:59,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:00,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:00,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:01,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:02,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:02,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:03,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:05<02:58, 16.27s/it][WARNING|generation_utils.py:914] 2023-08-28 08:16:04,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:04,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:05,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:06,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:06,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:07,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:08,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:08,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:09,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:10,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:11,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:11,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:12,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:13,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:13,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:14,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:15,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:16,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:17,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:17,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:18,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:19,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:19,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:20,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:21,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:21,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:24<02:50, 17.02s/it][WARNING|generation_utils.py:914] 2023-08-28 08:16:22,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:23,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:23,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:24,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:25,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:25,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:26,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:27,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:28,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:28,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:29,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:29,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:30,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:31,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:32,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:32,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:33,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:34,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:34,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:35,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:36,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:36,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:37,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:38,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:39,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:39,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:40,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:43<02:39, 17.70s/it][WARNING|generation_utils.py:914] 2023-08-28 08:16:41,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:42,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:42,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:43,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:44,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:44,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:45,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:46,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:46,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:47,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:47,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:48,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:49,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:50,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:50,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:51,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:51,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:52,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:53,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:54,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:54,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:55,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:56,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:57,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:57,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:00<02:19, 17.47s/it][WARNING|generation_utils.py:914] 2023-08-28 08:16:58,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:59,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:16:59,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:00,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:01,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:01,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:02,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:03,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:03,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:04,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:05,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:05,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:06,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:07,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:08,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:09,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:09,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:10,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:11,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:12,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:12,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:13,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:14,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:15,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:17<02:02, 17.44s/it][WARNING|generation_utils.py:914] 2023-08-28 08:17:15,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:16,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:17,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:17,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:18,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:19,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:19,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:20,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:21,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:21,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:22,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:23,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:23,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:24,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:25,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:25,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:26,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:27,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:27,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:28,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:28,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:29,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:30,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:30,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:31,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:34<01:42, 17.15s/it][WARNING|generation_utils.py:914] 2023-08-28 08:17:32,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:33,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:33,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:34,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:34,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:35,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:36,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:36,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:37,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:38,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:38,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:39,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:39,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:40,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:41,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:41,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:42,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:43,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:43,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:44,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:45,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:45,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:46,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:48<01:21, 16.38s/it][WARNING|generation_utils.py:914] 2023-08-28 08:17:47,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:47,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:48,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:49,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:49,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:50,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:51,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:51,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:52,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:53,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:53,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:54,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:55,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:56,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:57,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:58,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:58,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:17:59,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:00,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:00,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:01,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:02,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:02,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:05<01:05, 16.36s/it][WARNING|generation_utils.py:914] 2023-08-28 08:18:03,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:04,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:04,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:05,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:05,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:06,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:07,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:08,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:08,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:09,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:09,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:10,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:11,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:11,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:12,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:13,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:14,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:14,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:15,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:16,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:16,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:17,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:18,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:20<00:48, 16.08s/it][WARNING|generation_utils.py:914] 2023-08-28 08:18:18,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:19,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:19,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:20,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:21,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:22,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:22,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:23,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:24,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:24,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:25,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:26,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:26,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:27,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:28,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:28,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:29,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:30,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:31,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:31,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:32,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:33,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:33,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:34,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:34,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:35,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:36,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:37,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:39<00:33, 16.92s/it][WARNING|generation_utils.py:914] 2023-08-28 08:18:37,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:38,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:38,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:39,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:40,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:40,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:41,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:42,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:42,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:43,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:44,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:44,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:45,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:46,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:47,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:47,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:48,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:49,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:49,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:50,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:51,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:51,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:52,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:52,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:53,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:54,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:54,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:55,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:56,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:56,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:57,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:57,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:18:58,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:00<00:18, 18.31s/it][WARNING|generation_utils.py:914] 2023-08-28 08:18:59,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:00,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:01,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:02,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:03,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:03,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:04,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:05,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:06,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:06,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:07,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:08,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:09,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:10,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:11,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:11,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:12,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:13,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:14,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:15,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:16,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:17,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:17,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:18,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:19,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:20,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 08:19:21,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:24<00:00, 19.79s/it]Generating: 100%|██████████| 15/15 [04:24<00:00, 17.61s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:35,469 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:35,558 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:35,558 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:35,559 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:35,559 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:19:36,819 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:19:36,820 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:19:37,576 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:19:38,790 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:19:38,868 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:42,313 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:42,431 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:42,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:42,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:19:42,432 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:19:43,533 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:19:43,534 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:19:44,272 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:19:44,608 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:19:44,608 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : main subject . Context : Later in the year ( 1143–46 ) , he married daughter of Louis XIV and Catherine I of Prussia married to Marie Antoinette III , daughter of Emperor Louis XII and Catherine of Rheims . Head Entity : Catherine I , Tail Entity : Emperor Louis XII and Catherine I of Prussia .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7451923076923077, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.81640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.7391826923076923, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 294, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7013888888888888, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amadec for the first time . Head Entity : Amadec , Tail Entity : Amadeca .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.75625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Paul Groening', 'nominated for', '', 'After a stint in the Swedish music industry in 2002 alongside the late Paul Groening , he moved away to New Zealand to continue his education in the country .')"}}
['Relation : operating system . Context : The CVRN ( Computer Vision and Imaging Systems ) is a digital camera developed at the National Institutes of Health . Head Entity : CVRN , Tail Entity : CVR , Head Entity : National Institutes of Health .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 258, 'raw': 384}
{'target': 600, 'success': 280, 'raw': 416}
{'target': 600, 'success': 303, 'raw': 448}
{'target': 600, 'success': 326, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 372, 'raw': 544}
{'target': 600, 'success': 394, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 527, 'raw': 768}
{'target': 600, 'success': 551, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 598, 'raw': 864}
{'target': 600, 'success': 620, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6919642857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 107, 'raw': 192}
{'target': 600, 'success': 127, 'raw': 224}
{'target': 600, 'success': 148, 'raw': 256}
{'target': 600, 'success': 172, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 243, 'raw': 416}
{'target': 600, 'success': 262, 'raw': 448}
{'target': 600, 'success': 279, 'raw': 480}
{'target': 600, 'success': 298, 'raw': 512}
{'target': 600, 'success': 317, 'raw': 544}
{'target': 600, 'success': 332, 'raw': 576}
{'target': 600, 'success': 350, 'raw': 608}
{'target': 600, 'success': 365, 'raw': 640}
{'target': 600, 'success': 388, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 429, 'raw': 736}
{'target': 600, 'success': 446, 'raw': 768}
{'target': 600, 'success': 466, 'raw': 800}
{'target': 600, 'success': 482, 'raw': 832}
{'target': 600, 'success': 495, 'raw': 864}
{'target': 600, 'success': 509, 'raw': 896}
{'target': 600, 'success': 522, 'raw': 928}
{'target': 600, 'success': 543, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 584, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5681818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n']
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n', "Relation : religion . Context : After the death of King Henry IV of France ( c. 589 - 7 February 1235 ) , St Peter was succeeded as Archbishop by the eponymous St Peter 's successor , the first Pope . Head Entity : St Peter 's successors , Tail Entity : St Peter 's .\n"]
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 369, 'raw': 544}
{'target': 600, 'success': 391, 'raw': 576}
{'target': 600, 'success': 415, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 459, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 534, 'raw': 768}
{'target': 600, 'success': 553, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 15147
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15247, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.45it/s]Extractor Estimating: 2it [00:01,  1.30it/s]Extractor Estimating: 3it [00:02,  1.35it/s]Extractor Estimating: 4it [00:02,  1.44it/s]Extractor Estimating: 5it [00:03,  1.45it/s]Extractor Estimating: 6it [00:04,  1.46it/s]Extractor Estimating: 7it [00:04,  1.46it/s]Extractor Estimating: 8it [00:05,  1.51it/s]Extractor Estimating: 9it [00:06,  1.57it/s]Extractor Estimating: 10it [00:06,  1.52it/s]Extractor Estimating: 11it [00:07,  1.49it/s]Extractor Estimating: 12it [00:08,  1.43it/s]Extractor Estimating: 13it [00:08,  1.42it/s]Extractor Estimating: 14it [00:09,  1.44it/s]Extractor Estimating: 15it [00:10,  1.43it/s]Extractor Estimating: 16it [00:11,  1.45it/s]Extractor Estimating: 17it [00:11,  1.48it/s]Extractor Estimating: 18it [00:12,  1.56it/s]Extractor Estimating: 19it [00:12,  1.49it/s]Extractor Estimating: 20it [00:13,  1.42it/s]Extractor Estimating: 21it [00:14,  1.46it/s]Extractor Estimating: 22it [00:15,  1.45it/s]Extractor Estimating: 23it [00:15,  1.52it/s]Extractor Estimating: 24it [00:16,  1.48it/s]Extractor Estimating: 25it [00:17,  1.41it/s]Extractor Estimating: 26it [00:17,  1.44it/s]Extractor Estimating: 27it [00:18,  1.41it/s]Extractor Estimating: 28it [00:19,  1.43it/s]Extractor Estimating: 29it [00:19,  1.48it/s]Extractor Estimating: 30it [00:20,  1.40it/s]Extractor Estimating: 31it [00:21,  1.43it/s]Extractor Estimating: 32it [00:22,  1.42it/s]Extractor Estimating: 33it [00:22,  1.43it/s]Extractor Estimating: 34it [00:23,  1.44it/s]Extractor Estimating: 35it [00:24,  1.32it/s]Extractor Estimating: 36it [00:25,  1.35it/s]Extractor Estimating: 37it [00:25,  1.41it/s]Extractor Estimating: 38it [00:26,  1.43it/s]Extractor Estimating: 39it [00:26,  1.47it/s]Extractor Estimating: 40it [00:27,  1.40it/s]Extractor Estimating: 41it [00:28,  1.38it/s]Extractor Estimating: 42it [00:29,  1.37it/s]Extractor Estimating: 43it [00:29,  1.41it/s]Extractor Estimating: 44it [00:30,  1.43it/s]Extractor Estimating: 45it [00:31,  1.37it/s]Extractor Estimating: 46it [00:32,  1.41it/s]Extractor Estimating: 47it [00:32,  1.44it/s]Extractor Estimating: 48it [00:33,  1.46it/s]Extractor Estimating: 49it [00:33,  1.51it/s]Extractor Estimating: 50it [00:34,  1.45it/s]Extractor Estimating: 51it [00:35,  1.42it/s]Extractor Estimating: 52it [00:36,  1.45it/s]Extractor Estimating: 53it [00:36,  1.49it/s]Extractor Estimating: 54it [00:37,  1.50it/s]Extractor Estimating: 55it [00:38,  1.45it/s]Extractor Estimating: 56it [00:38,  1.47it/s]Extractor Estimating: 57it [00:39,  1.47it/s]Extractor Estimating: 58it [00:40,  1.47it/s]Extractor Estimating: 59it [00:40,  1.51it/s]Extractor Estimating: 60it [00:41,  1.56it/s]Extractor Estimating: 61it [00:42,  1.59it/s]Extractor Estimating: 62it [00:42,  1.62it/s]Extractor Estimating: 63it [00:43,  1.66it/s]Extractor Estimating: 64it [00:43,  1.64it/s]Extractor Estimating: 65it [00:44,  1.55it/s]Extractor Estimating: 66it [00:45,  1.56it/s]Extractor Estimating: 67it [00:45,  1.53it/s]Extractor Estimating: 68it [00:46,  1.53it/s]Extractor Estimating: 69it [00:47,  1.51it/s]Extractor Estimating: 70it [00:47,  1.43it/s]Extractor Estimating: 71it [00:48,  1.46it/s]Extractor Estimating: 72it [00:49,  1.49it/s]Extractor Estimating: 73it [00:49,  1.53it/s]Extractor Estimating: 74it [00:50,  1.58it/s]Extractor Estimating: 75it [00:51,  1.51it/s]Extractor Estimating: 76it [00:51,  1.41it/s]Extractor Estimating: 77it [00:52,  1.44it/s]Extractor Estimating: 78it [00:53,  1.49it/s]Extractor Estimating: 79it [00:53,  1.57it/s]Extractor Estimating: 80it [00:54,  1.56it/s]Extractor Estimating: 81it [00:55,  1.60it/s]Extractor Estimating: 82it [00:55,  1.62it/s]Extractor Estimating: 83it [00:56,  1.64it/s]Extractor Estimating: 84it [00:56,  1.64it/s]Extractor Estimating: 85it [00:57,  1.38it/s]Extractor Estimating: 86it [00:58,  1.48it/s]Extractor Estimating: 87it [00:59,  1.49it/s]Extractor Estimating: 88it [00:59,  1.49it/s]Extractor Estimating: 89it [01:00,  1.47it/s]Extractor Estimating: 90it [01:01,  1.45it/s]Extractor Estimating: 91it [01:01,  1.49it/s]Extractor Estimating: 92it [01:02,  1.50it/s]Extractor Estimating: 93it [01:03,  1.53it/s]Extractor Estimating: 94it [01:03,  1.51it/s]Extractor Estimating: 95it [01:04,  1.48it/s]Extractor Estimating: 96it [01:05,  1.48it/s]Extractor Estimating: 97it [01:05,  1.53it/s]Extractor Estimating: 98it [01:06,  1.54it/s]Extractor Estimating: 99it [01:06,  1.55it/s]Extractor Estimating: 100it [01:07,  1.52it/s]Extractor Estimating: 101it [01:08,  1.52it/s]Extractor Estimating: 102it [01:09,  1.47it/s]Extractor Estimating: 103it [01:09,  1.47it/s]Extractor Estimating: 104it [01:10,  1.55it/s]Extractor Estimating: 105it [01:10,  1.53it/s]Extractor Estimating: 106it [01:11,  1.43it/s]Extractor Estimating: 107it [01:12,  1.50it/s]Extractor Estimating: 108it [01:13,  1.51it/s]Extractor Estimating: 109it [01:13,  1.56it/s]Extractor Estimating: 110it [01:14,  1.56it/s]Extractor Estimating: 111it [01:15,  1.47it/s]Extractor Estimating: 112it [01:15,  1.51it/s]Extractor Estimating: 113it [01:16,  1.56it/s]Extractor Estimating: 114it [01:16,  1.60it/s]Extractor Estimating: 115it [01:17,  1.57it/s]Extractor Estimating: 116it [01:18,  1.44it/s]Extractor Estimating: 117it [01:18,  1.50it/s]Extractor Estimating: 118it [01:19,  1.49it/s]Extractor Estimating: 119it [01:20,  1.52it/s]Extractor Estimating: 120it [01:20,  1.49it/s]Extractor Estimating: 121it [01:21,  1.47it/s]Extractor Estimating: 122it [01:22,  1.50it/s]Extractor Estimating: 123it [01:22,  1.54it/s]Extractor Estimating: 124it [01:23,  1.57it/s]Extractor Estimating: 125it [01:24,  1.64it/s]Extractor Estimating: 126it [01:24,  1.54it/s]Extractor Estimating: 127it [01:25,  1.41it/s]Extractor Estimating: 128it [01:26,  1.51it/s]Extractor Estimating: 129it [01:26,  1.54it/s]Extractor Estimating: 130it [01:27,  1.57it/s]Extractor Estimating: 131it [01:28,  1.44it/s]Extractor Estimating: 132it [01:28,  1.46it/s]Extractor Estimating: 133it [01:29,  1.49it/s]Extractor Estimating: 134it [01:30,  1.53it/s]Extractor Estimating: 135it [01:30,  1.60it/s]Extractor Estimating: 136it [01:31,  1.56it/s]Extractor Estimating: 137it [01:32,  1.53it/s]Extractor Estimating: 138it [01:32,  1.59it/s]Extractor Estimating: 139it [01:33,  1.63it/s]Extractor Estimating: 140it [01:33,  1.63it/s]Extractor Estimating: 141it [01:34,  1.59it/s]Extractor Estimating: 142it [01:35,  1.66it/s]Extractor Estimating: 143it [01:35,  1.66it/s]Extractor Estimating: 144it [01:36,  1.66it/s]Extractor Estimating: 145it [01:36,  1.68it/s]Extractor Estimating: 146it [01:37,  1.52it/s]Extractor Estimating: 147it [01:38,  1.50it/s]Extractor Estimating: 148it [01:38,  1.56it/s]Extractor Estimating: 149it [01:39,  1.58it/s]Extractor Estimating: 150it [01:40,  1.51it/s]Extractor Estimating: 151it [01:40,  1.52it/s]Extractor Estimating: 152it [01:41,  1.46it/s]Extractor Estimating: 153it [01:42,  1.47it/s]Extractor Estimating: 154it [01:42,  1.48it/s]Extractor Estimating: 155it [01:43,  1.46it/s]Extractor Estimating: 156it [01:44,  1.45it/s]Extractor Estimating: 157it [01:45,  1.48it/s]Extractor Estimating: 158it [01:45,  1.46it/s]Extractor Estimating: 159it [01:46,  1.54it/s]Extractor Estimating: 160it [01:46,  1.55it/s]Extractor Estimating: 161it [01:47,  1.53it/s]Extractor Estimating: 162it [01:48,  1.50it/s]Extractor Estimating: 163it [01:49,  1.42it/s]Extractor Estimating: 164it [01:49,  1.48it/s]Extractor Estimating: 165it [01:50,  1.47it/s]Extractor Estimating: 166it [01:51,  1.51it/s]Extractor Estimating: 167it [01:51,  1.51it/s]Extractor Estimating: 168it [01:55,  1.66s/it]Extractor Estimating: 169it [01:56,  1.41s/it]Extractor Estimating: 170it [01:57,  1.20s/it]Extractor Estimating: 171it [01:57,  1.05s/it]Extractor Estimating: 172it [01:58,  1.09it/s]Extractor Estimating: 173it [01:59,  1.22it/s]Extractor Estimating: 174it [02:00,  1.10it/s]Extractor Estimating: 175it [02:00,  1.22it/s]Extractor Estimating: 176it [02:01,  1.29it/s]Extractor Estimating: 177it [02:02,  1.36it/s]Extractor Estimating: 178it [02:02,  1.36it/s]Extractor Estimating: 179it [02:03,  1.48it/s]Extractor Estimating: 180it [02:04,  1.51it/s]Extractor Estimating: 181it [02:04,  1.54it/s]Extractor Estimating: 182it [02:05,  1.54it/s]Extractor Estimating: 183it [02:06,  1.50it/s]Extractor Estimating: 184it [02:06,  1.57it/s]Extractor Estimating: 185it [02:07,  1.57it/s]Extractor Estimating: 186it [02:07,  1.58it/s]Extractor Estimating: 187it [02:08,  1.54it/s]Extractor Estimating: 188it [02:09,  1.53it/s]Extractor Estimating: 189it [02:09,  1.56it/s]Extractor Estimating: 190it [02:10,  1.56it/s]Extractor Estimating: 191it [02:11,  1.57it/s]Extractor Estimating: 192it [02:11,  1.63it/s]Extractor Estimating: 193it [02:12,  1.57it/s]Extractor Estimating: 194it [02:12,  1.59it/s]Extractor Estimating: 195it [02:13,  1.63it/s]Extractor Estimating: 196it [02:14,  1.53it/s]Extractor Estimating: 197it [02:14,  1.54it/s]Extractor Estimating: 198it [02:15,  1.50it/s]Extractor Estimating: 199it [02:16,  1.45it/s]Extractor Estimating: 200it [02:16,  1.53it/s]Extractor Estimating: 201it [02:17,  1.51it/s]Extractor Estimating: 202it [02:18,  1.53it/s]Extractor Estimating: 203it [02:18,  1.48it/s]Extractor Estimating: 204it [02:19,  1.45it/s]Extractor Estimating: 205it [02:20,  1.45it/s]Extractor Estimating: 206it [02:21,  1.42it/s]Extractor Estimating: 207it [02:21,  1.42it/s]Extractor Estimating: 208it [02:22,  1.31it/s]Extractor Estimating: 209it [02:23,  1.30it/s]Extractor Estimating: 210it [02:24,  1.33it/s]Extractor Estimating: 211it [02:24,  1.38it/s]Extractor Estimating: 212it [02:25,  1.43it/s]Extractor Estimating: 213it [02:26,  1.48it/s]Extractor Estimating: 214it [02:26,  1.40it/s]Extractor Estimating: 215it [02:27,  1.43it/s]Extractor Estimating: 216it [02:28,  1.44it/s]Extractor Estimating: 217it [02:28,  1.47it/s]Extractor Estimating: 218it [02:29,  1.44it/s]Extractor Estimating: 219it [02:30,  1.42it/s]Extractor Estimating: 220it [02:31,  1.42it/s]Extractor Estimating: 221it [02:31,  1.35it/s]Extractor Estimating: 222it [02:32,  1.40it/s]Extractor Estimating: 223it [02:33,  1.40it/s]Extractor Estimating: 224it [02:34,  1.33it/s]Extractor Estimating: 225it [02:34,  1.34it/s]Extractor Estimating: 226it [02:35,  1.48it/s]Extractor Estimating: 227it [02:35,  1.53it/s]Extractor Estimating: 228it [02:36,  1.53it/s]Extractor Estimating: 229it [02:37,  1.52it/s]Extractor Estimating: 230it [02:37,  1.55it/s]Extractor Estimating: 231it [02:38,  1.51it/s]Extractor Estimating: 232it [02:39,  1.58it/s]Extractor Estimating: 233it [02:39,  1.69it/s]Extractor Estimating: 234it [02:40,  1.67it/s]Extractor Estimating: 235it [02:40,  1.70it/s]Extractor Estimating: 236it [02:41,  1.70it/s]Extractor Estimating: 237it [02:41,  1.73it/s]Extractor Estimating: 238it [02:42,  1.66it/s]Extractor Estimating: 239it [02:43,  1.61it/s]Extractor Estimating: 240it [02:43,  1.65it/s]Extractor Estimating: 241it [02:44,  1.66it/s]Extractor Estimating: 242it [02:45,  1.66it/s]Extractor Estimating: 243it [02:45,  1.65it/s]Extractor Estimating: 244it [02:46,  1.67it/s]Extractor Estimating: 245it [02:46,  1.62it/s]Extractor Estimating: 246it [02:47,  1.63it/s]Extractor Estimating: 247it [02:48,  1.63it/s]Extractor Estimating: 248it [02:48,  1.61it/s]Extractor Estimating: 249it [02:49,  1.62it/s]Extractor Estimating: 250it [02:50,  1.56it/s]Extractor Estimating: 251it [02:50,  1.49it/s]Extractor Estimating: 252it [02:51,  1.55it/s]Extractor Estimating: 253it [02:52,  1.54it/s]Extractor Estimating: 254it [02:52,  1.47it/s]Extractor Estimating: 255it [02:53,  1.45it/s]Extractor Estimating: 256it [02:54,  1.51it/s]Extractor Estimating: 257it [02:54,  1.53it/s]Extractor Estimating: 258it [02:55,  1.54it/s]Extractor Estimating: 259it [02:55,  1.59it/s]Extractor Estimating: 260it [02:56,  1.52it/s]Extractor Estimating: 261it [02:57,  1.53it/s]Extractor Estimating: 262it [02:58,  1.50it/s]Extractor Estimating: 263it [02:58,  1.49it/s]Extractor Estimating: 264it [02:59,  1.50it/s]Extractor Estimating: 265it [03:00,  1.47it/s]Extractor Estimating: 266it [03:00,  1.40it/s]Extractor Estimating: 267it [03:01,  1.43it/s]Extractor Estimating: 268it [03:02,  1.45it/s]Extractor Estimating: 269it [03:02,  1.44it/s]Extractor Estimating: 270it [03:03,  1.41it/s]Extractor Estimating: 271it [03:04,  1.40it/s]Extractor Estimating: 272it [03:05,  1.43it/s]Extractor Estimating: 273it [03:05,  1.44it/s]Extractor Estimating: 274it [03:06,  1.50it/s]Extractor Estimating: 275it [03:07,  1.49it/s]Extractor Estimating: 276it [03:07,  1.53it/s]Extractor Estimating: 277it [03:08,  1.55it/s]Extractor Estimating: 278it [03:08,  1.61it/s]Extractor Estimating: 279it [03:09,  1.69it/s]Extractor Estimating: 280it [03:10,  1.60it/s]Extractor Estimating: 281it [03:10,  1.62it/s]Extractor Estimating: 282it [03:11,  1.67it/s]Extractor Estimating: 283it [03:11,  1.68it/s]Extractor Estimating: 284it [03:12,  1.66it/s]Extractor Estimating: 285it [03:13,  1.57it/s]Extractor Estimating: 286it [03:13,  1.57it/s]Extractor Estimating: 287it [03:14,  1.54it/s]Extractor Estimating: 288it [03:15,  1.38it/s]Extractor Estimating: 289it [03:16,  1.43it/s]Extractor Estimating: 290it [03:16,  1.51it/s]Extractor Estimating: 291it [03:17,  1.51it/s]Extractor Estimating: 292it [03:17,  1.45it/s]Extractor Estimating: 293it [03:18,  1.48it/s]Extractor Estimating: 294it [03:19,  1.49it/s]Extractor Estimating: 295it [03:19,  1.52it/s]Extractor Estimating: 296it [03:20,  1.55it/s]Extractor Estimating: 297it [03:21,  1.47it/s]Extractor Estimating: 298it [03:21,  1.51it/s]Extractor Estimating: 299it [03:22,  1.53it/s]Extractor Estimating: 300it [03:23,  1.53it/s]Extractor Estimating: 301it [03:23,  1.52it/s]Extractor Estimating: 302it [03:24,  1.52it/s]Extractor Estimating: 303it [03:25,  1.55it/s]Extractor Estimating: 304it [03:25,  1.53it/s]Extractor Estimating: 305it [03:26,  1.52it/s]Extractor Estimating: 306it [03:27,  1.58it/s]Extractor Estimating: 307it [03:27,  1.52it/s]Extractor Estimating: 308it [03:28,  1.50it/s]Extractor Estimating: 309it [03:28,  1.62it/s]Extractor Estimating: 310it [03:29,  1.59it/s]Extractor Estimating: 311it [03:30,  1.57it/s]Extractor Estimating: 312it [03:31,  1.49it/s]Extractor Estimating: 313it [03:31,  1.50it/s]Extractor Estimating: 314it [03:32,  1.50it/s]Extractor Estimating: 315it [03:32,  1.54it/s]Extractor Estimating: 316it [03:33,  1.57it/s]Extractor Estimating: 317it [03:34,  1.51it/s]Extractor Estimating: 318it [03:34,  1.55it/s]Extractor Estimating: 319it [03:35,  1.55it/s]Extractor Estimating: 320it [03:36,  1.56it/s]Extractor Estimating: 321it [03:36,  1.60it/s]Extractor Estimating: 322it [03:37,  1.54it/s]Extractor Estimating: 323it [03:38,  1.53it/s]Extractor Estimating: 324it [03:38,  1.54it/s]Extractor Estimating: 325it [03:39,  1.56it/s]Extractor Estimating: 326it [03:39,  1.59it/s]Extractor Estimating: 327it [03:40,  1.53it/s]Extractor Estimating: 328it [03:41,  1.56it/s]Extractor Estimating: 329it [03:41,  1.64it/s]Extractor Estimating: 330it [03:42,  1.59it/s]Extractor Estimating: 331it [03:43,  1.66it/s]Extractor Estimating: 332it [03:43,  1.57it/s]Extractor Estimating: 333it [03:44,  1.67it/s]Extractor Estimating: 334it [03:44,  1.67it/s]Extractor Estimating: 335it [03:45,  1.65it/s]Extractor Estimating: 336it [03:46,  1.61it/s]Extractor Estimating: 337it [03:46,  1.65it/s]Extractor Estimating: 338it [03:47,  1.67it/s]Extractor Estimating: 339it [03:47,  1.66it/s]Extractor Estimating: 340it [03:48,  1.58it/s]Extractor Estimating: 341it [03:49,  1.64it/s]Extractor Estimating: 342it [03:49,  1.68it/s]Extractor Estimating: 343it [03:50,  1.72it/s]Extractor Estimating: 344it [03:50,  1.76it/s]Extractor Estimating: 345it [03:51,  1.75it/s]Extractor Estimating: 346it [03:52,  1.60it/s]Extractor Estimating: 347it [03:52,  1.64it/s]Extractor Estimating: 348it [03:53,  1.61it/s]Extractor Estimating: 349it [03:53,  1.63it/s]Extractor Estimating: 350it [03:54,  1.65it/s]Extractor Estimating: 351it [03:55,  1.44it/s]Extractor Estimating: 352it [03:56,  1.41it/s]Extractor Estimating: 353it [03:56,  1.42it/s]Extractor Estimating: 354it [03:57,  1.45it/s]Extractor Estimating: 355it [03:58,  1.40it/s]Extractor Estimating: 356it [03:59,  1.37it/s]Extractor Estimating: 357it [03:59,  1.30it/s]Extractor Estimating: 358it [04:00,  1.38it/s]Extractor Estimating: 359it [04:01,  1.39it/s]Extractor Estimating: 360it [04:01,  1.42it/s]Extractor Estimating: 361it [04:02,  1.35it/s]Extractor Estimating: 362it [04:03,  1.39it/s]Extractor Estimating: 363it [04:04,  1.49it/s]Extractor Estimating: 364it [04:04,  1.49it/s]Extractor Estimating: 365it [04:05,  1.48it/s]Extractor Estimating: 366it [04:06,  1.45it/s]Extractor Estimating: 367it [04:06,  1.50it/s]Extractor Estimating: 368it [04:07,  1.53it/s]Extractor Estimating: 369it [04:08,  1.47it/s]Extractor Estimating: 370it [04:08,  1.51it/s]Extractor Estimating: 371it [04:09,  1.48it/s]Extractor Estimating: 372it [04:10,  1.44it/s]Extractor Estimating: 373it [04:10,  1.42it/s]Extractor Estimating: 374it [04:11,  1.46it/s]Extractor Estimating: 375it [04:12,  1.47it/s]Extractor Estimating: 375it [04:12,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:26,303 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:26,376 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:26,376 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:26,376 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:26,376 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:24:27,326 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:24:27,327 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:24:27,863 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:24:29,141 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:24:29,141 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:31,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:31,635 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:31,635 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:31,635 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:24:31,635 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:24:32,441 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:24:32,442 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:24:32,963 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:24:33,325 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:24:33,325 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 10:47:11,869 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 10:47:12,787 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7892 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 22501
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22601, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22601, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.091, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.107, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.079, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 71, avg_time 1.079, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 171, avg_time 1.066, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 271, avg_time 2.187, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 42, avg_time 1.095, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 142, avg_time 1.085, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 242, avg_time 1.069, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 13, avg_time 1.094, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 113, avg_time 2.159, loss:nan
g_step 1200, step 213, avg_time 1.058, loss:nan
g_step 1300, step 313, avg_time 1.101, loss:nan
g_step 1400, step 84, avg_time 1.079, loss:nan
g_step 1500, step 184, avg_time 1.062, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 284, avg_time 2.144, loss:nan
g_step 1700, step 55, avg_time 1.080, loss:nan
g_step 1800, step 155, avg_time 1.084, loss:nan
g_step 1900, step 255, avg_time 1.079, loss:nan
g_step 2000, step 26, avg_time 1.096, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 126, avg_time 2.174, loss:nan
g_step 2200, step 226, avg_time 1.084, loss:nan
g_step 2300, step 326, avg_time 1.097, loss:nan
g_step 2400, step 97, avg_time 1.099, loss:nan
g_step 2500, step 197, avg_time 1.077, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 297, avg_time 2.179, loss:nan
g_step 2700, step 68, avg_time 1.090, loss:nan
g_step 2800, step 168, avg_time 1.091, loss:nan
g_step 2900, step 268, avg_time 1.091, loss:nan
g_step 3000, step 39, avg_time 1.076, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 139, avg_time 2.156, loss:nan
g_step 3200, step 239, avg_time 1.120, loss:nan
g_step 3300, step 10, avg_time 1.077, loss:nan
g_step 3400, step 110, avg_time 1.072, loss:nan
g_step 3500, step 210, avg_time 1.111, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 310, avg_time 2.160, loss:nan
g_step 3700, step 81, avg_time 1.094, loss:nan
g_step 3800, step 181, avg_time 1.090, loss:nan
g_step 3900, step 281, avg_time 1.079, loss:nan
g_step 4000, step 52, avg_time 1.090, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 152, avg_time 2.161, loss:nan
g_step 4200, step 252, avg_time 1.094, loss:nan
g_step 4300, step 23, avg_time 1.088, loss:nan
g_step 4400, step 123, avg_time 1.103, loss:nan
g_step 4500, step 223, avg_time 1.086, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 323, avg_time 2.154, loss:nan
g_step 4700, step 94, avg_time 1.085, loss:nan
g_step 4800, step 194, avg_time 1.074, loss:nan
g_step 4900, step 294, avg_time 1.096, loss:nan
g_step 5000, step 65, avg_time 1.090, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 165, avg_time 2.174, loss:nan
g_step 5200, step 265, avg_time 1.083, loss:nan
g_step 5300, step 36, avg_time 1.069, loss:nan
g_step 5400, step 136, avg_time 1.095, loss:nan
g_step 5500, step 236, avg_time 1.085, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 7, avg_time 2.171, loss:nan
g_step 5700, step 107, avg_time 1.083, loss:nan
g_step 5800, step 207, avg_time 1.085, loss:nan
g_step 5900, step 307, avg_time 1.085, loss:nan
g_step 6000, step 78, avg_time 1.102, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 178, avg_time 2.132, loss:nan
g_step 6200, step 278, avg_time 1.067, loss:nan
g_step 6300, step 49, avg_time 1.094, loss:nan
g_step 6400, step 149, avg_time 1.075, loss:nan
g_step 6500, step 249, avg_time 1.087, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:47:12 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:47:12 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-47-11_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:47:14 - WARNING - datasets.builder -   Using custom data configuration default-e3bca8af73b726d3
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e3bca8af73b726d3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:47:22,071 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:47:22,158 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:47:22,158 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:47:22,160 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:47:22,493 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:47:22,658 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:47:22,658 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:47:22,659 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:47:22,659 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:47:22,659 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:47:22,659 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:47:23,503 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:47:26,721 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:47:26,721 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e3bca8af73b726d3/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.18ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.59ba/s] 38%|███▊      | 3/8 [00:01<00:01,  3.27ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.71ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.00ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.20ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.32ba/s]100%|██████████| 8/8 [00:02<00:00,  4.57ba/s]100%|██████████| 8/8 [00:02<00:00,  3.89ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.16ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.11ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.65ba/s]100%|██████████| 4/4 [00:01<00:00,  4.79ba/s]100%|██████████| 4/4 [00:01<00:00,  3.95ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.93ba/s] 38%|███▊      | 3/8 [00:00<00:00,  6.34ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  8.05ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.01ba/s]100%|██████████| 8/8 [00:00<00:00,  8.13ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.31ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.07ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  5.56ba/s]100%|██████████| 4/4 [00:00<00:00,  5.75ba/s]
[INFO|trainer.py:414] 2023-08-28 10:47:38,460 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:47:38,802 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:47:38,802 >>   Num examples = 7899
[INFO|trainer.py:1149] 2023-08-28 10:47:38,802 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:47:38,802 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:47:38,802 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:47:38,802 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:47:38,802 >>   Total optimization steps = 615
  0%|          | 0/615 [00:00<?, ?it/s]  0%|          | 1/615 [00:02<27:14,  2.66s/it]  0%|          | 2/615 [00:04<20:02,  1.96s/it]  0%|          | 3/615 [00:05<16:20,  1.60s/it]  1%|          | 4/615 [00:06<12:50,  1.26s/it]  1%|          | 5/615 [00:06<10:19,  1.01s/it]  1%|          | 6/615 [00:06<07:44,  1.31it/s]  1%|          | 7/615 [00:07<06:06,  1.66it/s]  1%|▏         | 8/615 [00:07<05:59,  1.69it/s]  1%|▏         | 9/615 [00:08<04:58,  2.03it/s]  2%|▏         | 10/615 [00:08<04:17,  2.35it/s]  2%|▏         | 11/615 [00:08<03:48,  2.64it/s]  2%|▏         | 12/615 [00:08<03:29,  2.88it/s]  2%|▏         | 13/615 [00:09<03:15,  3.07it/s]  2%|▏         | 14/615 [00:09<03:22,  2.96it/s]  2%|▏         | 15/615 [00:09<03:26,  2.90it/s]  3%|▎         | 16/615 [00:10<03:13,  3.09it/s]  3%|▎         | 17/615 [00:10<03:04,  3.24it/s]  3%|▎         | 18/615 [00:10<02:58,  3.35it/s]  3%|▎         | 19/615 [00:10<02:53,  3.43it/s]  3%|▎         | 20/615 [00:11<02:50,  3.49it/s]  3%|▎         | 21/615 [00:11<02:48,  3.53it/s]  4%|▎         | 22/615 [00:11<02:46,  3.57it/s]  4%|▎         | 23/615 [00:12<02:44,  3.59it/s]  4%|▍         | 24/615 [00:12<02:44,  3.60it/s]  4%|▍         | 25/615 [00:12<02:43,  3.61it/s]  4%|▍         | 26/615 [00:12<02:42,  3.62it/s]  4%|▍         | 27/615 [00:13<02:41,  3.63it/s]  5%|▍         | 28/615 [00:13<02:41,  3.63it/s]  5%|▍         | 29/615 [00:13<02:41,  3.63it/s]  5%|▍         | 30/615 [00:13<02:40,  3.64it/s]  5%|▌         | 31/615 [00:14<02:40,  3.64it/s]  5%|▌         | 32/615 [00:14<02:40,  3.64it/s]  5%|▌         | 33/615 [00:14<02:39,  3.64it/s]  6%|▌         | 34/615 [00:15<02:39,  3.64it/s]  6%|▌         | 35/615 [00:15<02:39,  3.64it/s]  6%|▌         | 36/615 [00:15<02:38,  3.64it/s]  6%|▌         | 37/615 [00:15<02:38,  3.65it/s]  6%|▌         | 38/615 [00:16<02:38,  3.64it/s]  6%|▋         | 39/615 [00:16<02:38,  3.64it/s]  7%|▋         | 40/615 [00:16<02:38,  3.64it/s]  7%|▋         | 41/615 [00:16<02:37,  3.64it/s]  7%|▋         | 42/615 [00:17<02:37,  3.64it/s]  7%|▋         | 43/615 [00:17<02:37,  3.64it/s]  7%|▋         | 44/615 [00:17<02:37,  3.64it/s]  7%|▋         | 45/615 [00:18<02:36,  3.64it/s]  7%|▋         | 46/615 [00:18<02:36,  3.64it/s]  8%|▊         | 47/615 [00:18<02:36,  3.64it/s]  8%|▊         | 48/615 [00:18<02:36,  3.63it/s]  8%|▊         | 49/615 [00:19<02:35,  3.63it/s]  8%|▊         | 50/615 [00:19<02:35,  3.63it/s]  8%|▊         | 51/615 [00:19<02:35,  3.63it/s]  8%|▊         | 52/615 [00:20<02:35,  3.63it/s]  9%|▊         | 53/615 [00:20<02:34,  3.64it/s]  9%|▉         | 54/615 [00:20<02:34,  3.63it/s]  9%|▉         | 55/615 [00:20<02:33,  3.64it/s]  9%|▉         | 56/615 [00:21<02:33,  3.64it/s]  9%|▉         | 57/615 [00:21<02:33,  3.64it/s]  9%|▉         | 58/615 [00:21<02:33,  3.64it/s] 10%|▉         | 59/615 [00:21<02:32,  3.63it/s] 10%|▉         | 60/615 [00:22<02:32,  3.64it/s] 10%|▉         | 61/615 [00:22<02:32,  3.63it/s] 10%|█         | 62/615 [00:22<02:32,  3.64it/s] 10%|█         | 63/615 [00:23<02:32,  3.63it/s] 10%|█         | 64/615 [00:23<02:31,  3.63it/s] 11%|█         | 65/615 [00:23<02:31,  3.63it/s] 11%|█         | 66/615 [00:23<02:31,  3.63it/s] 11%|█         | 67/615 [00:24<02:31,  3.63it/s] 11%|█         | 68/615 [00:24<02:30,  3.63it/s] 11%|█         | 69/615 [00:24<02:30,  3.63it/s] 11%|█▏        | 70/615 [00:24<02:30,  3.63it/s] 12%|█▏        | 71/615 [00:25<02:29,  3.64it/s] 12%|█▏        | 72/615 [00:25<02:29,  3.63it/s] 12%|█▏        | 73/615 [00:25<02:29,  3.63it/s] 12%|█▏        | 74/615 [00:26<02:28,  3.64it/s] 12%|█▏        | 75/615 [00:26<02:28,  3.63it/s] 12%|█▏        | 76/615 [00:26<02:28,  3.64it/s] 13%|█▎        | 77/615 [00:26<02:28,  3.63it/s] 13%|█▎        | 78/615 [00:27<02:44,  3.27it/s] 13%|█▎        | 79/615 [00:27<02:39,  3.37it/s] 13%|█▎        | 80/615 [00:27<02:35,  3.44it/s] 13%|█▎        | 81/615 [00:28<02:32,  3.50it/s] 13%|█▎        | 82/615 [00:28<02:30,  3.53it/s] 13%|█▎        | 83/615 [00:28<02:50,  3.13it/s] 14%|█▎        | 84/615 [00:29<02:42,  3.26it/s] 14%|█▍        | 85/615 [00:29<02:37,  3.37it/s] 14%|█▍        | 86/615 [00:29<02:33,  3.44it/s] 14%|█▍        | 87/615 [00:29<02:30,  3.50it/s] 14%|█▍        | 88/615 [00:30<02:28,  3.54it/s] 14%|█▍        | 89/615 [00:30<02:27,  3.57it/s] 15%|█▍        | 90/615 [00:30<02:26,  3.59it/s] 15%|█▍        | 91/615 [00:30<02:25,  3.61it/s] 15%|█▍        | 92/615 [00:31<02:24,  3.62it/s] 15%|█▌        | 93/615 [00:31<02:24,  3.62it/s] 15%|█▌        | 94/615 [00:31<02:38,  3.28it/s] 15%|█▌        | 95/615 [00:32<02:33,  3.38it/s] 16%|█▌        | 96/615 [00:32<02:30,  3.45it/s] 16%|█▌        | 97/615 [00:32<02:28,  3.50it/s] 16%|█▌        | 98/615 [00:32<02:26,  3.53it/s] 16%|█▌        | 99/615 [00:34<05:31,  1.56it/s] 16%|█▋        | 100/615 [00:34<04:35,  1.87it/s] 16%|█▋        | 101/615 [00:35<04:11,  2.05it/s] 17%|█▋        | 102/615 [00:35<03:38,  2.35it/s] 17%|█▋        | 103/615 [00:35<03:15,  2.62it/s] 17%|█▋        | 104/615 [00:35<02:59,  2.85it/s] 17%|█▋        | 105/615 [00:36<02:48,  3.03it/s] 17%|█▋        | 106/615 [00:36<02:40,  3.18it/s] 17%|█▋        | 107/615 [00:36<02:34,  3.28it/s] 18%|█▊        | 108/615 [00:37<02:30,  3.37it/s] 18%|█▊        | 109/615 [00:37<02:26,  3.45it/s] 18%|█▊        | 110/615 [00:37<02:24,  3.50it/s] 18%|█▊        | 111/615 [00:37<02:22,  3.54it/s] 18%|█▊        | 112/615 [00:38<02:31,  3.32it/s] 18%|█▊        | 113/615 [00:38<02:27,  3.41it/s] 19%|█▊        | 114/615 [00:38<02:24,  3.47it/s] 19%|█▊        | 115/615 [00:39<02:22,  3.52it/s] 19%|█▉        | 116/615 [00:39<02:20,  3.55it/s] 19%|█▉        | 117/615 [00:39<02:19,  3.58it/s] 19%|█▉        | 118/615 [00:39<02:18,  3.59it/s] 19%|█▉        | 119/615 [00:40<02:17,  3.60it/s] 20%|█▉        | 120/615 [00:40<02:16,  3.61it/s] 20%|█▉        | 121/615 [00:40<02:16,  3.62it/s] 20%|█▉        | 122/615 [00:41<02:16,  3.62it/s] 20%|██        | 123/615 [00:41<02:25,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 10:48:20,293 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:48:20,293 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 10:48:20,293 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.37it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.22it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.14it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.36it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.40it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.90it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.50it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.37it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.47it/s][A
 12%|█▏        | 52/437 [00:01<00:09, 41.02it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 42.21it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.08it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.67it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.95it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.03it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.98it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.07it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.97it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.12it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.36it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.59it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.70it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.70it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 40.42it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 41.65it/s][A
 30%|███       | 132/437 [00:03<00:07, 42.43it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 42.96it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.42it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.94it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.07it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.29it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.11it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.14it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.35it/s][A
 41%|████      | 177/437 [00:04<00:06, 38.97it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 40.78it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 41.95it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 42.89it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.51it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.01it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.17it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.25it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.92it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.81it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.89it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.16it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.35it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.56it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.65it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 40.03it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 41.36it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 42.37it/s][A
 61%|██████    | 267/437 [00:06<00:03, 42.90it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.38it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.70it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.01it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.25it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.82it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.98it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 40.59it/s][A
 70%|███████   | 307/437 [00:07<00:03, 41.77it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 42.54it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.21it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.78it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.16it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.30it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.25it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.82it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 43.93it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.05it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.20it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.42it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.58it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.67it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.50it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 40.03it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 41.45it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 42.28it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.00it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.50it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.94it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.21it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.23it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.87it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.87it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.10it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.28it/s][A                                                 
                                                 [A 20%|██        | 123/615 [00:51<02:25,  3.38it/s]
100%|██████████| 437/437 [00:10<00:00, 44.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:48:31,243 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123
[INFO|configuration_utils.py:351] 2023-08-28 10:48:31,924 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:49:00,820 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:49:01,610 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:49:02,317 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123/special_tokens_map.json
 20%|██        | 124/615 [01:27<1:54:33, 14.00s/it] 20%|██        | 125/615 [01:27<1:20:58,  9.92s/it] 20%|██        | 126/615 [01:28<57:14,  7.02s/it]   21%|██        | 127/615 [01:28<40:40,  5.00s/it] 21%|██        | 128/615 [01:28<29:05,  3.58s/it] 21%|██        | 129/615 [01:28<20:59,  2.59s/it] 21%|██        | 130/615 [01:29<15:20,  1.90s/it] 21%|██▏       | 131/615 [01:29<11:23,  1.41s/it] 21%|██▏       | 132/615 [01:29<08:38,  1.07s/it] 22%|██▏       | 133/615 [01:29<06:42,  1.20it/s] 22%|██▏       | 134/615 [01:30<05:21,  1.50it/s] 22%|██▏       | 135/615 [01:30<04:24,  1.82it/s] 22%|██▏       | 136/615 [01:30<04:05,  1.95it/s] 22%|██▏       | 137/615 [01:31<03:31,  2.26it/s] 22%|██▏       | 138/615 [01:31<03:07,  2.55it/s] 23%|██▎       | 139/615 [01:31<02:50,  2.79it/s] 23%|██▎       | 140/615 [01:32<02:38,  2.99it/s] 23%|██▎       | 141/615 [01:32<02:30,  3.14it/s] 23%|██▎       | 142/615 [01:32<02:25,  3.26it/s] 23%|██▎       | 143/615 [01:32<02:20,  3.36it/s] 23%|██▎       | 144/615 [01:33<02:17,  3.42it/s] 24%|██▎       | 145/615 [01:34<04:09,  1.88it/s] 24%|██▎       | 146/615 [01:34<04:29,  1.74it/s] 24%|██▍       | 147/615 [01:35<03:47,  2.06it/s] 24%|██▍       | 148/615 [01:35<03:17,  2.36it/s] 24%|██▍       | 149/615 [01:35<02:57,  2.63it/s] 24%|██▍       | 150/615 [01:36<02:42,  2.86it/s] 25%|██▍       | 151/615 [01:36<02:32,  3.05it/s] 25%|██▍       | 152/615 [01:36<02:58,  2.59it/s] 25%|██▍       | 153/615 [01:37<02:43,  2.82it/s] 25%|██▌       | 154/615 [01:37<02:32,  3.01it/s] 25%|██▌       | 155/615 [01:37<02:25,  3.17it/s] 25%|██▌       | 156/615 [01:37<02:19,  3.29it/s] 26%|██▌       | 157/615 [01:38<02:15,  3.38it/s] 26%|██▌       | 158/615 [01:38<02:12,  3.44it/s] 26%|██▌       | 159/615 [01:38<02:10,  3.49it/s] 26%|██▌       | 160/615 [01:39<02:09,  3.52it/s] 26%|██▌       | 161/615 [01:39<02:08,  3.54it/s] 26%|██▋       | 162/615 [01:39<02:25,  3.12it/s] 27%|██▋       | 163/615 [01:40<02:19,  3.25it/s] 27%|██▋       | 164/615 [01:40<02:14,  3.35it/s] 27%|██▋       | 165/615 [01:40<02:11,  3.42it/s] 27%|██▋       | 166/615 [01:40<02:09,  3.47it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 27%|██▋       | 167/615 [01:41<02:07,  3.50it/s] 27%|██▋       | 168/615 [01:41<02:06,  3.53it/s] 27%|██▋       | 169/615 [01:41<02:05,  3.55it/s] 28%|██▊       | 170/615 [01:41<02:05,  3.56it/s] 28%|██▊       | 171/615 [01:42<02:04,  3.57it/s] 28%|██▊       | 172/615 [01:42<02:03,  3.58it/s] 28%|██▊       | 173/615 [01:42<02:18,  3.19it/s] 28%|██▊       | 174/615 [01:43<02:13,  3.30it/s] 28%|██▊       | 175/615 [01:43<02:09,  3.39it/s] 29%|██▊       | 176/615 [01:43<02:07,  3.45it/s] 29%|██▉       | 177/615 [01:44<02:05,  3.49it/s] 29%|██▉       | 178/615 [01:44<02:04,  3.52it/s] 29%|██▉       | 179/615 [01:44<02:02,  3.55it/s] 29%|██▉       | 180/615 [01:44<02:01,  3.58it/s] 29%|██▉       | 181/615 [01:45<02:00,  3.60it/s] 30%|██▉       | 182/615 [01:45<01:59,  3.62it/s] 30%|██▉       | 183/615 [01:45<01:59,  3.63it/s] 30%|██▉       | 184/615 [01:46<02:14,  3.21it/s] 30%|███       | 185/615 [01:46<02:09,  3.33it/s] 30%|███       | 186/615 [01:46<02:05,  3.42it/s] 30%|███       | 187/615 [01:46<02:02,  3.49it/s] 31%|███       | 188/615 [01:47<02:00,  3.54it/s] 31%|███       | 189/615 [01:47<01:59,  3.57it/s] 31%|███       | 190/615 [01:47<01:58,  3.59it/s] 31%|███       | 191/615 [01:48<01:57,  3.61it/s] 31%|███       | 192/615 [01:48<01:56,  3.62it/s] 31%|███▏      | 193/615 [01:48<01:56,  3.63it/s] 32%|███▏      | 194/615 [01:48<01:55,  3.63it/s] 32%|███▏      | 195/615 [01:49<02:08,  3.26it/s] 32%|███▏      | 196/615 [01:49<02:04,  3.36it/s] 32%|███▏      | 197/615 [01:49<02:01,  3.44it/s] 32%|███▏      | 198/615 [01:50<01:59,  3.49it/s] 32%|███▏      | 199/615 [01:50<01:58,  3.52it/s] 33%|███▎      | 200/615 [01:50<01:56,  3.56it/s] 33%|███▎      | 201/615 [01:50<01:55,  3.59it/s] 33%|███▎      | 202/615 [01:51<01:54,  3.60it/s] 33%|███▎      | 203/615 [01:51<01:53,  3.62it/s] 33%|███▎      | 204/615 [01:51<01:53,  3.63it/s] 33%|███▎      | 205/615 [01:51<01:52,  3.64it/s] 33%|███▎      | 206/615 [01:52<02:02,  3.33it/s] 34%|███▎      | 207/615 [01:52<01:59,  3.42it/s] 34%|███▍      | 208/615 [01:52<01:56,  3.49it/s] 34%|███▍      | 209/615 [01:53<01:54,  3.54it/s] 34%|███▍      | 210/615 [01:53<01:53,  3.57it/s] 34%|███▍      | 211/615 [01:53<01:52,  3.59it/s] 34%|███▍      | 212/615 [01:53<01:51,  3.61it/s] 35%|███▍      | 213/615 [01:54<01:50,  3.62it/s] 35%|███▍      | 214/615 [01:54<01:50,  3.63it/s] 35%|███▍      | 215/615 [01:54<01:50,  3.64it/s] 35%|███▌      | 216/615 [01:55<01:49,  3.64it/s] 35%|███▌      | 217/615 [01:55<01:59,  3.34it/s] 35%|███▌      | 218/615 [01:55<01:55,  3.42it/s] 36%|███▌      | 219/615 [01:55<01:53,  3.49it/s] 36%|███▌      | 220/615 [01:56<01:51,  3.53it/s] 36%|███▌      | 221/615 [01:56<01:50,  3.56it/s] 36%|███▌      | 222/615 [01:56<01:49,  3.59it/s] 36%|███▋      | 223/615 [01:57<01:48,  3.61it/s] 36%|███▋      | 224/615 [01:57<01:48,  3.62it/s] 37%|███▋      | 225/615 [01:57<01:47,  3.63it/s] 37%|███▋      | 226/615 [01:57<01:46,  3.64it/s] 37%|███▋      | 227/615 [01:58<01:46,  3.64it/s] 37%|███▋      | 228/615 [01:58<02:01,  3.17it/s] 37%|███▋      | 229/615 [01:58<01:56,  3.30it/s] 37%|███▋      | 230/615 [01:59<01:53,  3.40it/s] 38%|███▊      | 231/615 [01:59<01:50,  3.47it/s] 38%|███▊      | 232/615 [01:59<01:48,  3.52it/s] 38%|███▊      | 233/615 [01:59<01:47,  3.56it/s] 38%|███▊      | 234/615 [02:00<01:46,  3.58it/s] 38%|███▊      | 235/615 [02:00<01:45,  3.60it/s] 38%|███▊      | 236/615 [02:00<01:44,  3.61it/s] 39%|███▊      | 237/615 [02:01<01:44,  3.62it/s] 39%|███▊      | 238/615 [02:01<01:43,  3.63it/s] 39%|███▉      | 239/615 [02:01<01:58,  3.19it/s] 39%|███▉      | 240/615 [02:01<01:53,  3.31it/s] 39%|███▉      | 241/615 [02:02<01:49,  3.41it/s] 39%|███▉      | 242/615 [02:02<01:47,  3.48it/s] 40%|███▉      | 243/615 [02:02<01:45,  3.53it/s] 40%|███▉      | 244/615 [02:03<01:44,  3.55it/s] 40%|███▉      | 245/615 [02:03<01:43,  3.57it/s] 40%|████      | 246/615 [02:03<01:43,  3.58it/s][INFO|trainer.py:2140] 2023-08-28 10:49:42,566 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:49:42,566 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 10:49:42,566 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.0417, 'eval_samples_per_second': 347.75, 'eval_steps_per_second': 43.519, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.22it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.77it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.91it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.93it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.32it/s][A
  7%|▋         | 32/437 [00:00<00:11, 36.19it/s][A
  8%|▊         | 37/437 [00:00<00:10, 38.66it/s][A
 10%|▉         | 42/437 [00:00<00:09, 40.46it/s][A
 11%|█         | 47/437 [00:01<00:09, 41.80it/s][A
 12%|█▏        | 52/437 [00:01<00:09, 42.77it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.40it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.90it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.03it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.78it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.61it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.86it/s][A
 20%|█▉        | 87/437 [00:02<00:07, 44.13it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.35it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.54it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.71it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.79it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.64it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.32it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.18it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.19it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.35it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.52it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.67it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.77it/s][A
 35%|███▍      | 152/437 [00:03<00:08, 35.19it/s][A
 36%|███▌      | 157/437 [00:03<00:07, 37.66it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 39.60it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 41.10it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 42.17it/s][A
 41%|████      | 177/437 [00:04<00:06, 42.97it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.59it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.88it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.68it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.66it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.79it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.08it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.27it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.54it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.68it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.77it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.69it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.30it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.22it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.12it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.25it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.40it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.58it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.71it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.78it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.63it/s][A
 65%|██████▍   | 282/437 [00:06<00:04, 36.85it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 38.96it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 40.60it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 41.84it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 42.72it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.39it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.86it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.04it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.82it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.68it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.95it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.27it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.50it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.65it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.68it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.74it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.52it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.20it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.12it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.21it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.37it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.55it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.69it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.80it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.80it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.60it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.36it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 35.78it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 38.18it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 40.04it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 41.40it/s][A
100%|██████████| 437/437 [00:10<00:00, 42.43it/s][A                                                 
                                                 [A 40%|████      | 246/615 [02:13<01:43,  3.58it/s]
100%|██████████| 437/437 [00:10<00:00, 42.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:49:53,020 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 10:49:53,729 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:50:18,375 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:50:19,172 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:50:19,455 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246/special_tokens_map.json
 40%|████      | 247/615 [02:44<1:16:41, 12.50s/it] 40%|████      | 248/615 [02:45<54:11,  8.86s/it]   40%|████      | 249/615 [02:45<38:20,  6.29s/it] 41%|████      | 250/615 [02:45<27:16,  4.48s/it] 41%|████      | 251/615 [02:45<19:32,  3.22s/it] 41%|████      | 252/615 [02:46<14:08,  2.34s/it] 41%|████      | 253/615 [02:46<10:22,  1.72s/it] 41%|████▏     | 254/615 [02:46<07:44,  1.29s/it] 41%|████▏     | 255/615 [02:46<05:54,  1.02it/s] 42%|████▏     | 256/615 [02:47<04:37,  1.29it/s] 42%|████▏     | 257/615 [02:47<03:43,  1.60it/s] 42%|████▏     | 258/615 [02:47<03:05,  1.93it/s] 42%|████▏     | 259/615 [02:48<02:44,  2.16it/s] 42%|████▏     | 260/615 [02:48<02:24,  2.46it/s] 42%|████▏     | 261/615 [02:48<02:09,  2.73it/s] 43%|████▎     | 262/615 [02:48<01:59,  2.95it/s] 43%|████▎     | 263/615 [02:49<01:52,  3.13it/s] 43%|████▎     | 264/615 [02:49<01:47,  3.27it/s] 43%|████▎     | 265/615 [02:49<01:43,  3.38it/s] 43%|████▎     | 266/615 [02:50<01:41,  3.46it/s] 43%|████▎     | 267/615 [02:50<01:39,  3.51it/s] 44%|████▎     | 268/615 [02:50<01:37,  3.55it/s] 44%|████▎     | 269/615 [02:50<01:36,  3.58it/s] 44%|████▍     | 270/615 [02:51<01:48,  3.19it/s] 44%|████▍     | 271/615 [02:51<01:43,  3.31it/s] 44%|████▍     | 272/615 [02:51<01:40,  3.41it/s] 44%|████▍     | 273/615 [02:52<01:38,  3.48it/s] 45%|████▍     | 274/615 [02:52<01:36,  3.52it/s] 45%|████▍     | 275/615 [02:52<01:35,  3.55it/s] 45%|████▍     | 276/615 [02:52<01:34,  3.58it/s] 45%|████▌     | 277/615 [02:53<01:33,  3.60it/s] 45%|████▌     | 278/615 [02:53<01:33,  3.61it/s] 45%|████▌     | 279/615 [02:53<01:32,  3.62it/s] 46%|████▌     | 280/615 [02:54<01:32,  3.63it/s] 46%|████▌     | 281/615 [02:54<01:37,  3.42it/s] 46%|████▌     | 282/615 [02:54<01:35,  3.48it/s] 46%|████▌     | 283/615 [02:54<01:34,  3.53it/s] 46%|████▌     | 284/615 [02:55<01:32,  3.56it/s] 46%|████▋     | 285/615 [02:55<01:31,  3.59it/s] 47%|████▋     | 286/615 [02:55<01:31,  3.60it/s] 47%|████▋     | 287/615 [02:55<01:30,  3.62it/s] 47%|████▋     | 288/615 [02:56<01:30,  3.63it/s] 47%|████▋     | 289/615 [02:56<01:29,  3.63it/s] 47%|████▋     | 290/615 [02:56<01:29,  3.64it/s] 47%|████▋     | 291/615 [02:57<01:28,  3.64it/s] 47%|████▋     | 292/615 [02:57<01:42,  3.15it/s] 48%|████▊     | 293/615 [02:57<01:38,  3.28it/s] 48%|████▊     | 294/615 [02:58<01:34,  3.38it/s] 48%|████▊     | 295/615 [02:58<01:32,  3.46it/s] 48%|████▊     | 296/615 [02:58<01:30,  3.51it/s] 48%|████▊     | 297/615 [02:58<01:29,  3.55it/s] 48%|████▊     | 298/615 [02:59<01:28,  3.58it/s] 49%|████▊     | 299/615 [02:59<01:27,  3.60it/s] 49%|████▉     | 300/615 [02:59<01:27,  3.62it/s] 49%|████▉     | 301/615 [02:59<01:26,  3.63it/s] 49%|████▉     | 302/615 [03:00<01:26,  3.63it/s] 49%|████▉     | 303/615 [03:00<01:32,  3.39it/s] 49%|████▉     | 304/615 [03:00<01:29,  3.46it/s] 50%|████▉     | 305/615 [03:01<01:28,  3.52it/s] 50%|████▉     | 306/615 [03:01<01:26,  3.56it/s] 50%|████▉     | 307/615 [03:01<01:26,  3.58it/s] 50%|█████     | 308/615 [03:01<01:25,  3.60it/s] 50%|█████     | 309/615 [03:02<01:24,  3.62it/s] 50%|█████     | 310/615 [03:02<01:24,  3.63it/s] 51%|█████     | 311/615 [03:02<01:23,  3.63it/s] 51%|█████     | 312/615 [03:03<01:23,  3.64it/s] 51%|█████     | 313/615 [03:03<01:22,  3.64it/s] 51%|█████     | 314/615 [03:03<01:32,  3.26it/s] 51%|█████     | 315/615 [03:03<01:29,  3.37it/s] 51%|█████▏    | 316/615 [03:04<01:26,  3.44it/s] 52%|█████▏    | 317/615 [03:04<01:25,  3.50it/s] 52%|█████▏    | 318/615 [03:04<01:23,  3.55it/s] 52%|█████▏    | 319/615 [03:05<01:22,  3.57it/s] 52%|█████▏    | 320/615 [03:05<01:22,  3.59it/s] 52%|█████▏    | 321/615 [03:05<01:21,  3.61it/s] 52%|█████▏    | 322/615 [03:05<01:20,  3.62it/s] 53%|█████▎    | 323/615 [03:06<01:20,  3.63it/s] 53%|█████▎    | 324/615 [03:06<01:20,  3.63it/s] 53%|█████▎    | 325/615 [03:06<01:29,  3.23it/s] 53%|█████▎    | 326/615 [03:07<01:26,  3.35it/s] 53%|█████▎    | 327/615 [03:07<01:23,  3.43it/s] 53%|█████▎    | 328/615 [03:07<01:22,  3.50it/s] 53%|█████▎    | 329/615 [03:07<01:20,  3.54it/s] 54%|█████▎    | 330/615 [03:08<01:19,  3.57it/s] 54%|█████▍    | 331/615 [03:08<01:19,  3.59it/s] 54%|█████▍    | 332/615 [03:09<01:44,  2.71it/s] 54%|█████▍    | 333/615 [03:09<01:36,  2.93it/s] 54%|█████▍    | 334/615 [03:09<01:30,  3.12it/s] 54%|█████▍    | 335/615 [03:09<01:25,  3.26it/s] 55%|█████▍    | 336/615 [03:10<01:22,  3.37it/s] 55%|█████▍    | 337/615 [03:10<01:20,  3.45it/s] 55%|█████▍    | 338/615 [03:10<01:19,  3.50it/s] 55%|█████▌    | 339/615 [03:10<01:17,  3.55it/s] 55%|█████▌    | 340/615 [03:11<01:16,  3.57it/s] 55%|█████▌    | 341/615 [03:11<01:16,  3.59it/s] 56%|█████▌    | 342/615 [03:11<01:23,  3.27it/s] 56%|█████▌    | 343/615 [03:12<01:20,  3.37it/s] 56%|█████▌    | 344/615 [03:12<01:18,  3.44it/s] 56%|█████▌    | 345/615 [03:12<01:17,  3.50it/s] 56%|█████▋    | 346/615 [03:13<01:15,  3.54it/s] 56%|█████▋    | 347/615 [03:13<01:15,  3.57it/s] 57%|█████▋    | 348/615 [03:13<01:14,  3.59it/s] 57%|█████▋    | 349/615 [03:13<01:13,  3.61it/s] 57%|█████▋    | 350/615 [03:14<01:13,  3.62it/s] 57%|█████▋    | 351/615 [03:14<01:12,  3.63it/s] 57%|█████▋    | 352/615 [03:14<01:12,  3.63it/s] 57%|█████▋    | 353/615 [03:15<01:23,  3.13it/s] 58%|█████▊    | 354/615 [03:15<01:19,  3.27it/s] 58%|█████▊    | 355/615 [03:15<01:17,  3.38it/s] 58%|█████▊    | 356/615 [03:15<01:15,  3.45it/s] 58%|█████▊    | 357/615 [03:16<01:13,  3.51it/s] 58%|█████▊    | 358/615 [03:16<01:12,  3.55it/s] 58%|█████▊    | 359/615 [03:16<01:11,  3.58it/s] 59%|█████▊    | 360/615 [03:16<01:10,  3.60it/s] 59%|█████▊    | 361/615 [03:17<01:10,  3.61it/s] 59%|█████▉    | 362/615 [03:17<01:09,  3.62it/s] 59%|█████▉    | 363/615 [03:17<01:09,  3.63it/s] 59%|█████▉    | 364/615 [03:18<01:23,  3.02it/s] 59%|█████▉    | 365/615 [03:18<01:18,  3.18it/s] 60%|█████▉    | 366/615 [03:18<01:15,  3.31it/s] 60%|█████▉    | 367/615 [03:19<01:12,  3.40it/s] 60%|█████▉    | 368/615 [03:19<01:11,  3.47it/s] 60%|██████    | 369/615 [03:19<01:09,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 10:50:58,573 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:50:58,573 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 10:50:58,573 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.1402, 'eval_samples_per_second': 344.372, 'eval_steps_per_second': 43.096, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.98it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.80it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.17it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.16it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.43it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.99it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.62it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.39it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.58it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.69it/s][A
 13%|█▎        | 57/437 [00:01<00:10, 35.38it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 37.88it/s][A
 15%|█▌        | 67/437 [00:01<00:09, 39.80it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 41.26it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 42.35it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.18it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 43.68it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.91it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.68it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.51it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.84it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.18it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.38it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.59it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.65it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.81it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.56it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.26it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.09it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.17it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.39it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.55it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.69it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.76it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.79it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.63it/s][A
 43%|████▎     | 187/437 [00:04<00:06, 38.99it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 40.72it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 41.92it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 42.83it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.51it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.93it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.28it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.23it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.94it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.89it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.89it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.19it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.44it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.66it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.73it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.85it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.66it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.27it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.20it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.23it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.30it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.59it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.70it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.79it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.58it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.57it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.38it/s][A
 74%|███████▎  | 322/437 [00:07<00:03, 29.95it/s][A
 75%|███████▍  | 327/437 [00:07<00:03, 33.82it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 36.51it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 38.68it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 40.41it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 41.69it/s][A
 81%|████████  | 352/437 [00:08<00:01, 42.72it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.32it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.57it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.56it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.56it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.66it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.05it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.38it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.70it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.68it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.72it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.28it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.06it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.94it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.21it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.40it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.55it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.73it/s][A                                                 
                                                 [A 60%|██████    | 369/615 [03:29<01:09,  3.52it/s]
100%|██████████| 437/437 [00:10<00:00, 44.73it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:51:09,217 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369
[INFO|configuration_utils.py:351] 2023-08-28 10:51:09,994 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:51:31,007 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:51:31,465 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:51:31,623 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369/special_tokens_map.json
 60%|██████    | 370/615 [03:55<45:15, 11.08s/it] 60%|██████    | 371/615 [03:56<31:59,  7.87s/it] 60%|██████    | 372/615 [03:56<22:38,  5.59s/it] 61%|██████    | 373/615 [03:56<16:07,  4.00s/it] 61%|██████    | 374/615 [03:57<11:34,  2.88s/it] 61%|██████    | 375/615 [03:57<08:23,  2.10s/it] 61%|██████    | 376/615 [03:57<06:11,  1.55s/it] 61%|██████▏   | 377/615 [03:57<04:38,  1.17s/it] 61%|██████▏   | 378/615 [03:58<03:33,  1.11it/s] 62%|██████▏   | 379/615 [03:58<02:48,  1.40it/s] 62%|██████▏   | 380/615 [03:58<02:17,  1.71it/s] 62%|██████▏   | 381/615 [03:59<01:55,  2.03it/s] 62%|██████▏   | 382/615 [03:59<01:45,  2.21it/s] 62%|██████▏   | 383/615 [03:59<01:32,  2.50it/s] 62%|██████▏   | 384/615 [03:59<01:23,  2.75it/s] 63%|██████▎   | 385/615 [04:00<01:17,  2.96it/s] 63%|██████▎   | 386/615 [04:00<01:13,  3.13it/s] 63%|██████▎   | 387/615 [04:00<01:10,  3.25it/s] 63%|██████▎   | 388/615 [04:01<01:07,  3.35it/s] 63%|██████▎   | 389/615 [04:01<01:06,  3.42it/s] 63%|██████▎   | 390/615 [04:01<01:04,  3.47it/s] 64%|██████▎   | 391/615 [04:01<01:03,  3.51it/s] 64%|██████▎   | 392/615 [04:02<01:03,  3.54it/s] 64%|██████▍   | 393/615 [04:02<01:08,  3.22it/s] 64%|██████▍   | 394/615 [04:02<01:06,  3.32it/s] 64%|██████▍   | 395/615 [04:03<01:04,  3.40it/s] 64%|██████▍   | 396/615 [04:03<01:03,  3.46it/s] 65%|██████▍   | 397/615 [04:03<01:02,  3.49it/s] 65%|██████▍   | 398/615 [04:03<01:01,  3.52it/s] 65%|██████▍   | 399/615 [04:04<01:01,  3.54it/s] 65%|██████▌   | 400/615 [04:04<01:00,  3.56it/s] 65%|██████▌   | 401/615 [04:04<00:59,  3.57it/s] 65%|██████▌   | 402/615 [04:05<00:59,  3.58it/s] 66%|██████▌   | 403/615 [04:05<00:59,  3.58it/s] 66%|██████▌   | 404/615 [04:05<01:07,  3.14it/s] 66%|██████▌   | 405/615 [04:06<01:04,  3.26it/s] 66%|██████▌   | 406/615 [04:06<01:02,  3.36it/s] 66%|██████▌   | 407/615 [04:06<01:00,  3.42it/s] 66%|██████▋   | 408/615 [04:06<00:59,  3.47it/s] 67%|██████▋   | 409/615 [04:07<00:58,  3.51it/s] 67%|██████▋   | 410/615 [04:07<00:57,  3.54it/s] 67%|██████▋   | 411/615 [04:07<00:57,  3.56it/s] 67%|██████▋   | 412/615 [04:08<00:56,  3.57it/s] 67%|██████▋   | 413/615 [04:08<00:56,  3.58it/s] 67%|██████▋   | 414/615 [04:08<00:56,  3.58it/s] 67%|██████▋   | 415/615 [04:08<01:03,  3.16it/s] 68%|██████▊   | 416/615 [04:09<01:00,  3.28it/s] 68%|██████▊   | 417/615 [04:09<00:58,  3.37it/s] 68%|██████▊   | 418/615 [04:09<00:57,  3.43it/s] 68%|██████▊   | 419/615 [04:10<00:56,  3.48it/s] 68%|██████▊   | 420/615 [04:10<00:59,  3.27it/s] 68%|██████▊   | 421/615 [04:10<00:57,  3.37it/s] 69%|██████▊   | 422/615 [04:10<00:56,  3.43it/s] 69%|██████▉   | 423/615 [04:11<00:55,  3.48it/s] 69%|██████▉   | 424/615 [04:11<00:54,  3.52it/s] 69%|██████▉   | 425/615 [04:11<00:53,  3.56it/s] 69%|██████▉   | 426/615 [04:12<00:52,  3.58it/s] 69%|██████▉   | 427/615 [04:12<00:52,  3.60it/s] 70%|██████▉   | 428/615 [04:12<00:51,  3.62it/s] 70%|██████▉   | 429/615 [04:12<00:51,  3.63it/s] 70%|██████▉   | 430/615 [04:13<00:50,  3.63it/s] 70%|███████   | 431/615 [04:13<00:55,  3.32it/s] 70%|███████   | 432/615 [04:13<00:53,  3.41it/s] 70%|███████   | 433/615 [04:14<00:52,  3.48it/s] 71%|███████   | 434/615 [04:14<00:51,  3.53it/s] 71%|███████   | 435/615 [04:14<00:50,  3.57it/s] 71%|███████   | 436/615 [04:14<00:49,  3.59it/s] 71%|███████   | 437/615 [04:15<00:49,  3.61it/s] 71%|███████   | 438/615 [04:15<00:48,  3.62it/s] 71%|███████▏  | 439/615 [04:15<00:48,  3.63it/s] 72%|███████▏  | 440/615 [04:16<00:48,  3.63it/s] 72%|███████▏  | 441/615 [04:16<00:47,  3.64it/s] 72%|███████▏  | 442/615 [04:16<00:52,  3.29it/s] 72%|███████▏  | 443/615 [04:16<00:50,  3.39it/s] 72%|███████▏  | 444/615 [04:17<00:49,  3.47it/s] 72%|███████▏  | 445/615 [04:17<00:48,  3.52it/s] 73%|███████▎  | 446/615 [04:17<00:47,  3.56it/s] 73%|███████▎  | 447/615 [04:18<00:46,  3.59it/s] 73%|███████▎  | 448/615 [04:18<00:46,  3.60it/s] 73%|███████▎  | 449/615 [04:18<00:45,  3.62it/s] 73%|███████▎  | 450/615 [04:18<00:45,  3.62it/s] 73%|███████▎  | 451/615 [04:19<00:45,  3.63it/s] 73%|███████▎  | 452/615 [04:19<00:44,  3.63it/s] 74%|███████▎  | 453/615 [04:19<00:48,  3.34it/s] 74%|███████▍  | 454/615 [04:20<00:46,  3.43it/s] 74%|███████▍  | 455/615 [04:20<00:45,  3.49it/s] 74%|███████▍  | 456/615 [04:20<00:44,  3.54it/s] 74%|███████▍  | 457/615 [04:20<00:44,  3.57it/s] 74%|███████▍  | 458/615 [04:21<00:43,  3.59it/s] 75%|███████▍  | 459/615 [04:21<00:43,  3.61it/s] 75%|███████▍  | 460/615 [04:21<00:42,  3.62it/s] 75%|███████▍  | 461/615 [04:21<00:42,  3.63it/s] 75%|███████▌  | 462/615 [04:22<00:42,  3.64it/s] 75%|███████▌  | 463/615 [04:22<00:41,  3.64it/s] 75%|███████▌  | 464/615 [04:22<00:45,  3.35it/s] 76%|███████▌  | 465/615 [04:23<00:43,  3.44it/s] 76%|███████▌  | 466/615 [04:23<00:42,  3.50it/s] 76%|███████▌  | 467/615 [04:23<00:41,  3.54it/s] 76%|███████▌  | 468/615 [04:23<00:41,  3.57it/s] 76%|███████▋  | 469/615 [04:24<00:40,  3.59it/s] 76%|███████▋  | 470/615 [04:24<00:40,  3.61it/s] 77%|███████▋  | 471/615 [04:24<00:39,  3.62it/s] 77%|███████▋  | 472/615 [04:25<00:39,  3.63it/s] 77%|███████▋  | 473/615 [04:25<00:39,  3.63it/s] 77%|███████▋  | 474/615 [04:25<00:38,  3.64it/s] 77%|███████▋  | 475/615 [04:25<00:43,  3.23it/s] 77%|███████▋  | 476/615 [04:26<00:41,  3.34it/s] 78%|███████▊  | 477/615 [04:26<00:40,  3.43it/s] 78%|███████▊  | 478/615 [04:26<00:39,  3.49it/s] 78%|███████▊  | 479/615 [04:27<00:38,  3.53it/s] 78%|███████▊  | 480/615 [04:27<00:43,  3.10it/s] 78%|███████▊  | 481/615 [04:27<00:41,  3.24it/s] 78%|███████▊  | 482/615 [04:28<00:39,  3.35it/s] 79%|███████▊  | 483/615 [04:28<00:38,  3.43it/s] 79%|███████▊  | 484/615 [04:28<00:37,  3.49it/s] 79%|███████▉  | 485/615 [04:28<00:40,  3.20it/s] 79%|███████▉  | 486/615 [04:29<00:38,  3.32it/s] 79%|███████▉  | 487/615 [04:29<00:37,  3.41it/s] 79%|███████▉  | 488/615 [04:29<00:36,  3.48it/s] 80%|███████▉  | 489/615 [04:30<00:35,  3.53it/s] 80%|███████▉  | 490/615 [04:30<00:35,  3.56it/s] 80%|███████▉  | 491/615 [04:30<00:34,  3.58it/s] 80%|████████  | 492/615 [04:30<00:34,  3.60it/s][INFO|trainer.py:2140] 2023-08-28 10:52:09,807 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:52:09,808 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 10:52:09,808 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.1153, 'eval_samples_per_second': 345.219, 'eval_steps_per_second': 43.202, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.27it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.05it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.13it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.13it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.35it/s][A
  7%|▋         | 32/437 [00:00<00:10, 37.06it/s][A
  8%|▊         | 37/437 [00:00<00:10, 39.34it/s][A
 10%|▉         | 42/437 [00:00<00:09, 40.97it/s][A
 11%|█         | 47/437 [00:01<00:09, 42.16it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 43.02it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.62it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.01it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.07it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.85it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.69it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.78it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.06it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.35it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.52it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.74it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.78it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.64it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.26it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.98it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.94it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.27it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.44it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.70it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.73it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.75it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.66it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.28it/s][A
 38%|███▊      | 167/437 [00:03<00:07, 35.92it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 38.18it/s][A
 41%|████      | 177/437 [00:04<00:06, 40.01it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 41.44it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 42.43it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.17it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.76it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.89it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.72it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.65it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 43.83it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.95it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.34it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.49it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.68it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.76it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.64it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.23it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.07it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.10it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.13it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.30it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.60it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.73it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.78it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.60it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 36.76it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 38.94it/s][A
 70%|███████   | 307/437 [00:07<00:03, 40.55it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 41.87it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 42.72it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.38it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.93it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.93it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.70it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.55it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.77it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.04it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.34it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.54it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.71it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.79it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.60it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.30it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.13it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.87it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.21it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.30it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.57it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.77it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.69it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.50it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 33.13it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 35.99it/s][A
100%|██████████| 437/437 [00:10<00:00, 38.28it/s][A                                                 
                                                 [A 80%|████████  | 492/615 [04:41<00:34,  3.60it/s]
100%|██████████| 437/437 [00:10<00:00, 38.28it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:52:20,402 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492
[INFO|configuration_utils.py:351] 2023-08-28 10:52:20,966 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:52:43,200 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:52:44,017 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:52:44,337 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492/special_tokens_map.json
 80%|████████  | 493/615 [05:10<24:43, 12.16s/it] 80%|████████  | 494/615 [05:11<17:23,  8.63s/it] 80%|████████  | 495/615 [05:11<12:14,  6.12s/it] 81%|████████  | 496/615 [05:11<08:42,  4.39s/it] 81%|████████  | 497/615 [05:12<06:12,  3.16s/it] 81%|████████  | 498/615 [05:12<04:28,  2.29s/it] 81%|████████  | 499/615 [05:12<03:16,  1.69s/it] 81%|████████▏ | 500/615 [05:12<02:25,  1.27s/it]                                                  81%|████████▏ | 500/615 [05:12<02:25,  1.27s/it] 81%|████████▏ | 501/615 [05:13<01:50,  1.03it/s] 82%|████████▏ | 502/615 [05:13<01:26,  1.31it/s] 82%|████████▏ | 503/615 [05:13<01:09,  1.62it/s] 82%|████████▏ | 504/615 [05:14<00:57,  1.94it/s] 82%|████████▏ | 505/615 [05:14<00:48,  2.25it/s] 82%|████████▏ | 506/615 [05:14<00:43,  2.53it/s] 82%|████████▏ | 507/615 [05:14<00:41,  2.62it/s] 83%|████████▎ | 508/615 [05:15<00:37,  2.87it/s] 83%|████████▎ | 509/615 [05:15<00:34,  3.06it/s] 83%|████████▎ | 510/615 [05:15<00:32,  3.22it/s] 83%|████████▎ | 511/615 [05:16<00:31,  3.34it/s] 83%|████████▎ | 512/615 [05:16<00:30,  3.43it/s] 83%|████████▎ | 513/615 [05:16<00:29,  3.49it/s] 84%|████████▎ | 514/615 [05:16<00:28,  3.54it/s] 84%|████████▎ | 515/615 [05:17<00:27,  3.57it/s] 84%|████████▍ | 516/615 [05:17<00:27,  3.60it/s] 84%|████████▍ | 517/615 [05:17<00:27,  3.62it/s] 84%|████████▍ | 518/615 [05:18<00:29,  3.24it/s] 84%|████████▍ | 519/615 [05:18<00:28,  3.35it/s] 85%|████████▍ | 520/615 [05:18<00:27,  3.44it/s] 85%|████████▍ | 521/615 [05:18<00:26,  3.50it/s] 85%|████████▍ | 522/615 [05:19<00:26,  3.54it/s] 85%|████████▌ | 523/615 [05:19<00:25,  3.58it/s] 85%|████████▌ | 524/615 [05:19<00:25,  3.60it/s] 85%|████████▌ | 525/615 [05:19<00:24,  3.62it/s] 86%|████████▌ | 526/615 [05:20<00:24,  3.62it/s] 86%|████████▌ | 527/615 [05:20<00:24,  3.63it/s] 86%|████████▌ | 528/615 [05:20<00:23,  3.63it/s] 86%|████████▌ | 529/615 [05:21<00:27,  3.14it/s] 86%|████████▌ | 530/615 [05:21<00:25,  3.28it/s] 86%|████████▋ | 531/615 [05:21<00:24,  3.38it/s] 87%|████████▋ | 532/615 [05:22<00:23,  3.46it/s] 87%|████████▋ | 533/615 [05:22<00:23,  3.52it/s] 87%|████████▋ | 534/615 [05:22<00:22,  3.56it/s] 87%|████████▋ | 535/615 [05:22<00:22,  3.59it/s] 87%|████████▋ | 536/615 [05:23<00:21,  3.61it/s] 87%|████████▋ | 537/615 [05:23<00:21,  3.62it/s] 87%|████████▋ | 538/615 [05:23<00:21,  3.63it/s] 88%|████████▊ | 539/615 [05:23<00:20,  3.63it/s] 88%|████████▊ | 540/615 [05:24<00:23,  3.18it/s] 88%|████████▊ | 541/615 [05:24<00:22,  3.31it/s] 88%|████████▊ | 542/615 [05:24<00:21,  3.41it/s] 88%|████████▊ | 543/615 [05:25<00:20,  3.48it/s] 88%|████████▊ | 544/615 [05:25<00:20,  3.52it/s] 89%|████████▊ | 545/615 [05:25<00:19,  3.56it/s] 89%|████████▉ | 546/615 [05:25<00:19,  3.59it/s] 89%|████████▉ | 547/615 [05:26<00:18,  3.61it/s] 89%|████████▉ | 548/615 [05:26<00:18,  3.62it/s] 89%|████████▉ | 549/615 [05:26<00:18,  3.63it/s] 89%|████████▉ | 550/615 [05:27<00:17,  3.63it/s] 90%|████████▉ | 551/615 [05:27<00:22,  2.84it/s] 90%|████████▉ | 552/615 [05:27<00:20,  3.02it/s] 90%|████████▉ | 553/615 [05:28<00:19,  3.19it/s] 90%|█████████ | 554/615 [05:28<00:18,  3.31it/s] 90%|█████████ | 555/615 [05:28<00:17,  3.41it/s] 90%|█████████ | 556/615 [05:28<00:16,  3.48it/s] 91%|█████████ | 557/615 [05:29<00:16,  3.53it/s] 91%|█████████ | 558/615 [05:29<00:15,  3.56it/s] 91%|█████████ | 559/615 [05:29<00:15,  3.59it/s] 91%|█████████ | 560/615 [05:30<00:15,  3.61it/s] 91%|█████████ | 561/615 [05:30<00:17,  3.10it/s] 91%|█████████▏| 562/615 [05:30<00:16,  3.25it/s] 92%|█████████▏| 563/615 [05:31<00:15,  3.36it/s] 92%|█████████▏| 564/615 [05:31<00:14,  3.44it/s] 92%|█████████▏| 565/615 [05:31<00:14,  3.50it/s] 92%|█████████▏| 566/615 [05:31<00:13,  3.55it/s] 92%|█████████▏| 567/615 [05:32<00:13,  3.58it/s] 92%|█████████▏| 568/615 [05:32<00:13,  3.60it/s] 93%|█████████▎| 569/615 [05:32<00:12,  3.62it/s] 93%|█████████▎| 570/615 [05:32<00:12,  3.63it/s] 93%|█████████▎| 571/615 [05:33<00:12,  3.64it/s] 93%|█████████▎| 572/615 [05:33<00:13,  3.14it/s] 93%|█████████▎| 573/615 [05:33<00:12,  3.26it/s] 93%|█████████▎| 574/615 [05:34<00:12,  3.36it/s] 93%|█████████▎| 575/615 [05:34<00:11,  3.44it/s] 94%|█████████▎| 576/615 [05:34<00:11,  3.49it/s] 94%|█████████▍| 577/615 [05:35<00:10,  3.53it/s] 94%|█████████▍| 578/615 [05:35<00:10,  3.57it/s] 94%|█████████▍| 579/615 [05:35<00:10,  3.58it/s] 94%|█████████▍| 580/615 [05:35<00:09,  3.60it/s] 94%|█████████▍| 581/615 [05:36<00:09,  3.61it/s] 95%|█████████▍| 582/615 [05:36<00:09,  3.62it/s] 95%|█████████▍| 583/615 [05:36<00:10,  3.15it/s] 95%|█████████▍| 584/615 [05:37<00:09,  3.29it/s] 95%|█████████▌| 585/615 [05:37<00:08,  3.38it/s] 95%|█████████▌| 586/615 [05:37<00:08,  3.45it/s] 95%|█████████▌| 587/615 [05:37<00:07,  3.51it/s] 96%|█████████▌| 588/615 [05:38<00:07,  3.54it/s] 96%|█████████▌| 589/615 [05:38<00:07,  3.57it/s] 96%|█████████▌| 590/615 [05:38<00:06,  3.59it/s] 96%|█████████▌| 591/615 [05:39<00:06,  3.60it/s] 96%|█████████▋| 592/615 [05:39<00:06,  3.61it/s] 96%|█████████▋| 593/615 [05:39<00:06,  3.62it/s] 97%|█████████▋| 594/615 [05:39<00:06,  3.23it/s] 97%|█████████▋| 595/615 [05:40<00:05,  3.34it/s] 97%|█████████▋| 596/615 [05:40<00:05,  3.43it/s] 97%|█████████▋| 597/615 [05:40<00:05,  3.48it/s] 97%|█████████▋| 598/615 [05:41<00:04,  3.52it/s] 97%|█████████▋| 599/615 [05:41<00:04,  3.56it/s] 98%|█████████▊| 600/615 [05:41<00:04,  3.58it/s] 98%|█████████▊| 601/615 [05:41<00:03,  3.59it/s] 98%|█████████▊| 602/615 [05:42<00:03,  3.60it/s] 98%|█████████▊| 603/615 [05:42<00:03,  3.52it/s] 98%|█████████▊| 604/615 [05:42<00:03,  3.55it/s] 98%|█████████▊| 605/615 [05:43<00:02,  3.58it/s] 99%|█████████▊| 606/615 [05:43<00:02,  3.59it/s] 99%|█████████▊| 607/615 [05:43<00:02,  3.60it/s] 99%|█████████▉| 608/615 [05:43<00:01,  3.61it/s] 99%|█████████▉| 609/615 [05:44<00:01,  3.62it/s] 99%|█████████▉| 610/615 [05:44<00:01,  3.63it/s] 99%|█████████▉| 611/615 [05:44<00:01,  3.62it/s]100%|█████████▉| 612/615 [05:44<00:00,  3.63it/s]100%|█████████▉| 613/615 [05:45<00:00,  3.63it/s]100%|█████████▉| 614/615 [05:45<00:00,  3.36it/s]100%|██████████| 615/615 [05:45<00:00,  3.43it/s][INFO|trainer.py:2140] 2023-08-28 10:53:24,674 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:53:24,674 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 10:53:24,674 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.1687, 'eval_samples_per_second': 343.405, 'eval_steps_per_second': 42.975, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.7134146341463415e-05, 'epoch': 4.06}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.70it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.06it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.15it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.13it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.44it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.73it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.52it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.36it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.36it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.61it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.72it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.71it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.76it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.48it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.31it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.23it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.20it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.24it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.47it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.67it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.76it/s][A
 26%|██▌       | 112/437 [00:02<00:08, 37.75it/s][A
 27%|██▋       | 117/437 [00:02<00:08, 39.59it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 41.10it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 42.22it/s][A
 30%|███       | 132/437 [00:03<00:07, 43.02it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.57it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.98it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.04it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.84it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.62it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.83it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.02it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.29it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.49it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.52it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.72it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.66it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.38it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.18it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.09it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.32it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.34it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.46it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.73it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.76it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.47it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.44it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 39.38it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 40.93it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 42.09it/s][A
 60%|█████▉    | 262/437 [00:05<00:04, 42.93it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.45it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.91it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.15it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.14it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.87it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.62it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.99it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.23it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.47it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.55it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.67it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.70it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.50it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.03it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.98it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.10it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.36it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.61it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.76it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.82it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.64it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.26it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.13it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 37.64it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 39.57it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 41.04it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 42.05it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 42.91it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.53it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.98it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.01it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.76it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.63it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.81it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.07it/s][A                                                 
                                                 [A100%|██████████| 615/615 [05:55<00:00,  3.43it/s]
100%|██████████| 437/437 [00:09<00:00, 44.07it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:53:35,279 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615
[INFO|configuration_utils.py:351] 2023-08-28 10:53:35,875 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:53:51,652 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:53:52,308 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:53:52,510 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:53:55,794 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:53:55,894 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123 (score: 1.1054370403289795).
                                                 100%|██████████| 615/615 [06:49<00:00,  3.43it/s]100%|██████████| 615/615 [06:49<00:00,  1.50it/s]
[INFO|trainer.py:1894] 2023-08-28 10:54:28,804 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 10:54:29,301 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:54:59,380 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:55:00,910 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:55:01,247 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:55:03,742 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:03,873 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:03,873 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:03,873 >>   train_runtime            = 0:06:49.85
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:03,873 >>   train_samples            =       7899
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:03,873 >>   train_samples_per_second =     96.363
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:03,873 >>   train_steps_per_second   =      1.501
{'eval_loss': 1.1054370403289795, 'eval_runtime': 9.9969, 'eval_samples_per_second': 349.307, 'eval_steps_per_second': 43.713, 'epoch': 5.0}
{'train_runtime': 409.8558, 'train_samples_per_second': 96.363, 'train_steps_per_second': 1.501, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 10:55:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:55:05,191 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:55:05,191 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 10:55:05,191 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 54.81it/s]  3%|▎         | 12/437 [00:00<00:08, 48.84it/s]  4%|▍         | 17/437 [00:00<00:08, 47.73it/s]  5%|▌         | 22/437 [00:00<00:08, 47.16it/s]  6%|▌         | 27/437 [00:00<00:08, 46.77it/s]  7%|▋         | 32/437 [00:00<00:08, 46.63it/s]  8%|▊         | 37/437 [00:00<00:08, 46.46it/s] 10%|▉         | 42/437 [00:00<00:08, 45.92it/s] 11%|█         | 47/437 [00:01<00:08, 45.45it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.86it/s] 13%|█▎        | 57/437 [00:01<00:10, 35.73it/s] 14%|█▍        | 62/437 [00:01<00:09, 38.87it/s] 15%|█▌        | 67/437 [00:01<00:09, 40.66it/s] 16%|█▋        | 72/437 [00:01<00:10, 34.09it/s] 18%|█▊        | 77/437 [00:01<00:09, 36.95it/s] 19%|█▉        | 82/437 [00:01<00:09, 39.20it/s] 20%|█▉        | 87/437 [00:02<00:08, 40.88it/s] 21%|██        | 92/437 [00:02<00:08, 42.13it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.23it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.84it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.20it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.03it/s] 27%|██▋       | 117/437 [00:02<00:07, 43.89it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.22it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.60it/s] 30%|███       | 132/437 [00:03<00:06, 44.92it/s] 31%|███▏      | 137/437 [00:03<00:06, 45.09it/s] 32%|███▏      | 142/437 [00:03<00:06, 45.21it/s] 34%|███▎      | 147/437 [00:03<00:09, 30.84it/s] 35%|███▍      | 152/437 [00:03<00:08, 34.04it/s] 36%|███▌      | 157/437 [00:03<00:07, 36.91it/s] 37%|███▋      | 162/437 [00:03<00:07, 39.18it/s] 38%|███▊      | 167/437 [00:04<00:06, 40.89it/s] 39%|███▉      | 172/437 [00:04<00:06, 42.11it/s] 41%|████      | 177/437 [00:04<00:06, 43.15it/s] 42%|████▏     | 182/437 [00:04<00:05, 43.62it/s] 43%|████▎     | 187/437 [00:04<00:05, 43.75it/s] 44%|████▍     | 192/437 [00:04<00:05, 43.68it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.13it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.43it/s] 47%|████▋     | 207/437 [00:04<00:05, 44.68it/s] 49%|████▊     | 212/437 [00:05<00:05, 44.98it/s] 50%|████▉     | 217/437 [00:05<00:04, 45.01it/s] 51%|█████     | 222/437 [00:05<00:04, 45.20it/s] 52%|█████▏    | 227/437 [00:05<00:04, 45.06it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.69it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.62it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.51it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.79it/s] 58%|█████▊    | 252/437 [00:05<00:04, 44.98it/s] 59%|█████▉    | 257/437 [00:06<00:03, 45.12it/s] 60%|█████▉    | 262/437 [00:06<00:03, 45.32it/s] 61%|██████    | 267/437 [00:06<00:03, 45.28it/s] 62%|██████▏   | 272/437 [00:06<00:04, 36.67it/s] 63%|██████▎   | 277/437 [00:06<00:04, 38.83it/s] 65%|██████▍   | 282/437 [00:06<00:03, 40.48it/s] 66%|██████▌   | 287/437 [00:06<00:03, 41.92it/s] 67%|██████▋   | 292/437 [00:06<00:03, 42.92it/s] 68%|██████▊   | 297/437 [00:06<00:03, 43.66it/s] 69%|██████▉   | 302/437 [00:07<00:03, 44.17it/s] 70%|███████   | 307/437 [00:07<00:02, 44.47it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.11it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.24it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.23it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.38it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.81it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.98it/s] 78%|███████▊  | 342/437 [00:07<00:02, 45.14it/s] 79%|███████▉  | 347/437 [00:08<00:01, 45.23it/s] 81%|████████  | 352/437 [00:08<00:01, 45.08it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.81it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.56it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.65it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.72it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.87it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.98it/s] 89%|████████▊ | 387/437 [00:08<00:01, 45.06it/s] 90%|████████▉ | 392/437 [00:09<00:00, 45.24it/s] 91%|█████████ | 397/437 [00:09<00:00, 45.09it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.80it/s] 93%|█████████▎| 407/437 [00:09<00:00, 35.99it/s] 94%|█████████▍| 412/437 [00:09<00:00, 38.30it/s] 95%|█████████▌| 417/437 [00:09<00:00, 40.17it/s] 97%|█████████▋| 422/437 [00:09<00:00, 41.59it/s] 98%|█████████▊| 427/437 [00:09<00:00, 42.68it/s] 99%|█████████▉| 432/437 [00:10<00:00, 43.52it/s]100%|██████████| 437/437 [00:10<00:00, 44.12it/s]100%|██████████| 437/437 [00:10<00:00, 42.89it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:55:15,415 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:15,415 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:15,415 >>   eval_loss               =     1.1054
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:15,415 >>   eval_runtime            = 0:00:10.22
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:15,415 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:15,415 >>   eval_samples_per_second =    341.552
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:15,415 >>   eval_steps_per_second   =     42.743
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:55:15,415 >>   perplexity              =     3.0205
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:39,375 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:39,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:39,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:39,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:39,426 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:55:40,296 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:55:40,297 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:55:40,989 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:55:42,374 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:55:42,467 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:45,457 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:45,544 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:45,544 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:45,545 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:55:45,545 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:55:46,634 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:55:46,635 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:55:47,396 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:55:47,774 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:55:47,774 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-492
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-123
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-615
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/checkpoint-369
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.68it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.62it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.52it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.66it/s]Extractor Predicting: 24it [00:14,  1.69it/s]Extractor Predicting: 25it [00:15,  1.70it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:16,  1.70it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:17,  1.64it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:20,  1.50it/s]Extractor Predicting: 34it [00:20,  1.54it/s]Extractor Predicting: 35it [00:21,  1.60it/s]Extractor Predicting: 36it [00:22,  1.63it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:23,  1.56it/s]Extractor Predicting: 39it [00:23,  1.61it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:25,  1.66it/s]Extractor Predicting: 42it [00:25,  1.64it/s]Extractor Predicting: 43it [00:26,  1.57it/s]Extractor Predicting: 44it [00:27,  1.61it/s]Extractor Predicting: 45it [00:27,  1.61it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:28,  1.65it/s]Extractor Predicting: 48it [00:29,  1.59it/s]Extractor Predicting: 49it [00:30,  1.61it/s]Extractor Predicting: 50it [00:30,  1.66it/s]Extractor Predicting: 51it [00:31,  1.62it/s]Extractor Predicting: 52it [00:31,  1.62it/s]Extractor Predicting: 53it [00:32,  1.57it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:33,  1.62it/s]Extractor Predicting: 56it [00:34,  1.49it/s]Extractor Predicting: 57it [00:35,  1.54it/s]Extractor Predicting: 58it [00:35,  1.60it/s]Extractor Predicting: 59it [00:36,  1.61it/s]Extractor Predicting: 60it [00:37,  1.48it/s]Extractor Predicting: 61it [00:38,  1.43it/s]Extractor Predicting: 62it [00:38,  1.48it/s]Extractor Predicting: 63it [00:39,  1.54it/s]Extractor Predicting: 64it [00:39,  1.52it/s]Extractor Predicting: 65it [00:40,  1.55it/s]Extractor Predicting: 66it [00:41,  1.47it/s]Extractor Predicting: 67it [00:41,  1.50it/s]Extractor Predicting: 68it [00:42,  1.51it/s]Extractor Predicting: 69it [00:43,  1.53it/s]Extractor Predicting: 70it [00:43,  1.53it/s]Extractor Predicting: 71it [00:44,  1.50it/s]Extractor Predicting: 72it [00:45,  1.49it/s]Extractor Predicting: 73it [00:45,  1.53it/s]Extractor Predicting: 74it [00:46,  1.50it/s]Extractor Predicting: 75it [00:47,  1.55it/s]Extractor Predicting: 76it [00:47,  1.52it/s]Extractor Predicting: 77it [00:48,  1.51it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:49,  1.53it/s]Extractor Predicting: 80it [00:50,  1.51it/s]Extractor Predicting: 81it [00:51,  1.46it/s]Extractor Predicting: 82it [00:51,  1.52it/s]Extractor Predicting: 83it [00:52,  1.55it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:53,  1.57it/s]Extractor Predicting: 86it [00:54,  1.48it/s]Extractor Predicting: 87it [00:55,  1.52it/s]Extractor Predicting: 88it [00:55,  1.57it/s]Extractor Predicting: 89it [00:56,  1.65it/s]Extractor Predicting: 90it [00:56,  1.64it/s]Extractor Predicting: 91it [00:57,  1.64it/s]Extractor Predicting: 92it [00:57,  1.72it/s]Extractor Predicting: 93it [00:58,  1.78it/s]Extractor Predicting: 94it [00:58,  1.83it/s]Extractor Predicting: 95it [00:59,  1.77it/s]Extractor Predicting: 96it [01:00,  1.81it/s]Extractor Predicting: 97it [01:00,  1.68it/s]Extractor Predicting: 98it [01:01,  1.70it/s]Extractor Predicting: 99it [01:01,  1.78it/s]Extractor Predicting: 100it [01:02,  1.80it/s]Extractor Predicting: 101it [01:02,  1.78it/s]Extractor Predicting: 102it [01:03,  1.73it/s]Extractor Predicting: 103it [01:04,  1.65it/s]Extractor Predicting: 104it [01:04,  1.63it/s]Extractor Predicting: 105it [01:05,  1.65it/s]Extractor Predicting: 106it [01:06,  1.69it/s]Extractor Predicting: 107it [01:06,  1.66it/s]Extractor Predicting: 108it [01:07,  1.72it/s]Extractor Predicting: 109it [01:07,  1.74it/s]Extractor Predicting: 110it [01:08,  1.69it/s]Extractor Predicting: 111it [01:08,  1.73it/s]Extractor Predicting: 112it [01:09,  1.75it/s]Extractor Predicting: 113it [01:09,  1.80it/s]Extractor Predicting: 114it [01:10,  1.75it/s]Extractor Predicting: 115it [01:11,  1.79it/s]Extractor Predicting: 116it [01:11,  1.67it/s]Extractor Predicting: 117it [01:12,  1.67it/s]Extractor Predicting: 118it [01:12,  1.69it/s]Extractor Predicting: 119it [01:13,  1.67it/s]Extractor Predicting: 120it [01:14,  1.64it/s]Extractor Predicting: 121it [01:14,  1.59it/s]Extractor Predicting: 122it [01:15,  1.61it/s]Extractor Predicting: 123it [01:16,  1.61it/s]Extractor Predicting: 124it [01:16,  1.59it/s]Extractor Predicting: 125it [01:17,  1.61it/s]Extractor Predicting: 126it [01:18,  1.53it/s]Extractor Predicting: 127it [01:18,  1.55it/s]Extractor Predicting: 128it [01:19,  1.57it/s]Extractor Predicting: 129it [01:19,  1.61it/s]Extractor Predicting: 130it [01:20,  1.65it/s]Extractor Predicting: 131it [01:21,  1.54it/s]Extractor Predicting: 132it [01:21,  1.55it/s]Extractor Predicting: 133it [01:22,  1.56it/s]Extractor Predicting: 134it [01:23,  1.58it/s]Extractor Predicting: 135it [01:23,  1.60it/s]Extractor Predicting: 136it [01:24,  1.55it/s]Extractor Predicting: 137it [01:25,  1.57it/s]Extractor Predicting: 138it [01:25,  1.61it/s]Extractor Predicting: 139it [01:26,  1.63it/s]Extractor Predicting: 140it [01:26,  1.67it/s]Extractor Predicting: 141it [01:27,  1.55it/s]Extractor Predicting: 142it [01:28,  1.57it/s]Extractor Predicting: 143it [01:28,  1.60it/s]Extractor Predicting: 144it [01:29,  1.65it/s]Extractor Predicting: 144it [01:29,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:50,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:50,837 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:50,838 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:50,838 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:50,838 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:57:52,087 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:57:52,088 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:57:53,050 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:57:54,323 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:57:54,438 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:59,282 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:59,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:59,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:59,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:57:59,387 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:58:00,927 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:58:00,929 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:58:01,968 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:58:02,319 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:58:02,400 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.64it/s]Extractor Predicting: 9it [00:05,  1.63it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.71it/s]Extractor Predicting: 16it [00:09,  1.72it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:11,  1.56it/s]Extractor Predicting: 20it [00:12,  1.58it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:16,  1.63it/s]Extractor Predicting: 27it [00:16,  1.62it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:17,  1.56it/s]Extractor Predicting: 30it [00:18,  1.55it/s]Extractor Predicting: 31it [00:19,  1.58it/s]Extractor Predicting: 32it [00:19,  1.59it/s]Extractor Predicting: 33it [00:20,  1.54it/s]Extractor Predicting: 34it [00:21,  1.58it/s]Extractor Predicting: 35it [00:21,  1.61it/s]Extractor Predicting: 36it [00:22,  1.66it/s]Extractor Predicting: 37it [00:22,  1.72it/s]Extractor Predicting: 38it [00:23,  1.73it/s]Extractor Predicting: 39it [00:24,  1.65it/s]Extractor Predicting: 40it [00:24,  1.68it/s]Extractor Predicting: 41it [00:25,  1.70it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:26,  1.69it/s]Extractor Predicting: 44it [00:27,  1.65it/s]Extractor Predicting: 45it [00:27,  1.65it/s]Extractor Predicting: 46it [00:28,  1.69it/s]Extractor Predicting: 47it [00:28,  1.71it/s]Extractor Predicting: 48it [00:29,  1.70it/s]Extractor Predicting: 49it [00:29,  1.69it/s]Extractor Predicting: 50it [00:30,  1.62it/s]Extractor Predicting: 51it [00:31,  1.65it/s]Extractor Predicting: 52it [00:31,  1.67it/s]Extractor Predicting: 53it [00:32,  1.68it/s]Extractor Predicting: 54it [00:32,  1.69it/s]Extractor Predicting: 55it [00:33,  1.58it/s]Extractor Predicting: 56it [00:34,  1.62it/s]Extractor Predicting: 57it [00:34,  1.68it/s]Extractor Predicting: 58it [00:35,  1.72it/s]Extractor Predicting: 59it [00:35,  1.77it/s]Extractor Predicting: 60it [00:36,  1.76it/s]Extractor Predicting: 61it [00:37,  1.68it/s]Extractor Predicting: 62it [00:37,  1.70it/s]Extractor Predicting: 63it [00:38,  1.72it/s]Extractor Predicting: 64it [00:38,  1.68it/s]Extractor Predicting: 65it [00:39,  1.69it/s]Extractor Predicting: 66it [00:40,  1.65it/s]Extractor Predicting: 67it [00:40,  1.68it/s]Extractor Predicting: 68it [00:41,  1.76it/s]Extractor Predicting: 69it [00:41,  1.67it/s]Extractor Predicting: 70it [00:42,  1.69it/s]Extractor Predicting: 71it [00:43,  1.70it/s]Extractor Predicting: 72it [00:43,  1.71it/s]Extractor Predicting: 73it [00:44,  1.68it/s]Extractor Predicting: 74it [00:44,  1.60it/s]Extractor Predicting: 75it [00:45,  1.64it/s]Extractor Predicting: 76it [00:46,  1.52it/s]Extractor Predicting: 77it [00:46,  1.55it/s]Extractor Predicting: 78it [00:47,  1.60it/s]Extractor Predicting: 79it [00:48,  1.53it/s]Extractor Predicting: 80it [00:48,  1.58it/s]Extractor Predicting: 81it [00:49,  1.60it/s]Extractor Predicting: 82it [00:49,  1.61it/s]Extractor Predicting: 83it [00:50,  1.64it/s]Extractor Predicting: 84it [00:51,  1.61it/s]Extractor Predicting: 85it [00:51,  1.67it/s]Extractor Predicting: 86it [00:52,  1.65it/s]Extractor Predicting: 87it [00:52,  1.67it/s]Extractor Predicting: 88it [00:53,  1.66it/s]Extractor Predicting: 89it [00:54,  1.64it/s]Extractor Predicting: 90it [00:54,  1.65it/s]Extractor Predicting: 91it [00:55,  1.67it/s]Extractor Predicting: 92it [00:56,  1.64it/s]Extractor Predicting: 93it [00:56,  1.69it/s]Extractor Predicting: 94it [00:57,  1.67it/s]Extractor Predicting: 95it [00:57,  1.63it/s]Extractor Predicting: 96it [00:58,  1.66it/s]Extractor Predicting: 97it [00:58,  1.68it/s]Extractor Predicting: 98it [00:59,  1.68it/s]Extractor Predicting: 99it [01:00,  1.69it/s]Extractor Predicting: 100it [01:00,  1.72it/s]Extractor Predicting: 101it [01:01,  1.68it/s]Extractor Predicting: 102it [01:01,  1.70it/s]Extractor Predicting: 103it [01:02,  1.66it/s]Extractor Predicting: 104it [01:03,  1.64it/s]Extractor Predicting: 105it [01:03,  1.66it/s]Extractor Predicting: 106it [01:04,  1.58it/s]Extractor Predicting: 107it [01:05,  1.60it/s]Extractor Predicting: 108it [01:05,  1.64it/s]Extractor Predicting: 109it [01:06,  1.63it/s]Extractor Predicting: 110it [01:06,  1.65it/s]Extractor Predicting: 111it [01:07,  1.55it/s]Extractor Predicting: 112it [01:08,  1.57it/s]Extractor Predicting: 113it [01:08,  1.58it/s]Extractor Predicting: 114it [01:09,  1.59it/s]Extractor Predicting: 115it [01:10,  1.61it/s]Extractor Predicting: 116it [01:10,  1.61it/s]Extractor Predicting: 117it [01:11,  1.64it/s]Extractor Predicting: 118it [01:11,  1.66it/s]Extractor Predicting: 119it [01:12,  1.57it/s]Extractor Predicting: 120it [01:13,  1.60it/s]Extractor Predicting: 121it [01:13,  1.65it/s]Extractor Predicting: 122it [01:14,  1.69it/s]Extractor Predicting: 123it [01:14,  1.72it/s]Extractor Predicting: 124it [01:15,  1.66it/s]Extractor Predicting: 125it [01:16,  1.68it/s]Extractor Predicting: 126it [01:16,  1.70it/s]Extractor Predicting: 127it [01:17,  1.72it/s]Extractor Predicting: 128it [01:17,  1.71it/s]Extractor Predicting: 129it [01:18,  1.68it/s]Extractor Predicting: 130it [01:19,  1.61it/s]Extractor Predicting: 131it [01:19,  1.67it/s]Extractor Predicting: 132it [01:20,  1.68it/s]Extractor Predicting: 133it [01:20,  1.70it/s]Extractor Predicting: 134it [01:21,  1.73it/s]Extractor Predicting: 135it [01:21,  1.74it/s]Extractor Predicting: 136it [01:22,  1.67it/s]Extractor Predicting: 137it [01:23,  1.70it/s]Extractor Predicting: 138it [01:23,  1.69it/s]Extractor Predicting: 139it [01:24,  1.72it/s]Extractor Predicting: 140it [01:24,  1.78it/s]Extractor Predicting: 141it [01:25,  1.79it/s]Extractor Predicting: 142it [01:26,  1.70it/s]Extractor Predicting: 143it [01:26,  1.69it/s]Extractor Predicting: 144it [01:27,  1.71it/s]Extractor Predicting: 145it [01:27,  1.73it/s]Extractor Predicting: 146it [01:28,  1.77it/s]Extractor Predicting: 147it [01:28,  1.79it/s]Extractor Predicting: 148it [01:29,  1.65it/s]Extractor Predicting: 149it [01:30,  1.70it/s]Extractor Predicting: 150it [01:30,  1.71it/s]Extractor Predicting: 151it [01:31,  1.72it/s]Extractor Predicting: 152it [01:31,  1.73it/s]Extractor Predicting: 153it [01:32,  1.70it/s]Extractor Predicting: 154it [01:33,  1.68it/s]Extractor Predicting: 155it [01:33,  1.72it/s]Extractor Predicting: 156it [01:34,  1.76it/s]Extractor Predicting: 157it [01:34,  1.78it/s]Extractor Predicting: 158it [01:35,  1.85it/s]Extractor Predicting: 159it [01:35,  1.81it/s]Extractor Predicting: 160it [01:36,  1.61it/s]Extractor Predicting: 161it [01:37,  1.63it/s]Extractor Predicting: 162it [01:37,  1.65it/s]Extractor Predicting: 163it [01:38,  1.71it/s]Extractor Predicting: 164it [01:38,  1.72it/s]Extractor Predicting: 165it [01:39,  1.69it/s]Extractor Predicting: 166it [01:39,  1.74it/s]Extractor Predicting: 167it [01:40,  1.75it/s]Extractor Predicting: 168it [01:41,  1.72it/s]Extractor Predicting: 169it [01:41,  1.73it/s]Extractor Predicting: 170it [01:42,  1.73it/s]Extractor Predicting: 171it [01:42,  1.65it/s]Extractor Predicting: 172it [01:43,  1.69it/s]Extractor Predicting: 173it [01:44,  1.69it/s]Extractor Predicting: 174it [01:44,  1.70it/s]Extractor Predicting: 175it [01:45,  1.68it/s]Extractor Predicting: 176it [01:45,  1.63it/s]Extractor Predicting: 177it [01:46,  1.43it/s]Extractor Predicting: 178it [01:47,  1.50it/s]Extractor Predicting: 179it [01:48,  1.58it/s]Extractor Predicting: 180it [01:48,  1.64it/s]Extractor Predicting: 181it [01:49,  1.63it/s]Extractor Predicting: 182it [01:49,  1.57it/s]Extractor Predicting: 183it [01:50,  1.61it/s]Extractor Predicting: 184it [01:51,  1.61it/s]Extractor Predicting: 185it [01:51,  1.62it/s]Extractor Predicting: 186it [01:52,  1.63it/s]Extractor Predicting: 187it [01:53,  1.56it/s]Extractor Predicting: 188it [01:53,  1.57it/s]Extractor Predicting: 189it [01:54,  1.58it/s]Extractor Predicting: 190it [01:54,  1.61it/s]Extractor Predicting: 191it [01:55,  1.66it/s]Extractor Predicting: 192it [01:56,  1.61it/s]Extractor Predicting: 193it [01:56,  1.56it/s]Extractor Predicting: 194it [01:57,  1.57it/s]Extractor Predicting: 195it [01:57,  1.63it/s]Extractor Predicting: 196it [01:58,  1.65it/s]Extractor Predicting: 197it [01:59,  1.62it/s]Extractor Predicting: 198it [01:59,  1.61it/s]Extractor Predicting: 199it [02:00,  1.65it/s]Extractor Predicting: 200it [02:00,  1.74it/s]Extractor Predicting: 201it [02:01,  1.77it/s]Extractor Predicting: 202it [02:01,  1.77it/s]Extractor Predicting: 203it [02:02,  1.66it/s]Extractor Predicting: 204it [02:03,  1.67it/s]Extractor Predicting: 205it [02:03,  1.70it/s]Extractor Predicting: 206it [02:04,  1.69it/s]Extractor Predicting: 207it [02:05,  1.71it/s]Extractor Predicting: 208it [02:05,  1.60it/s]Extractor Predicting: 209it [02:06,  1.64it/s]Extractor Predicting: 210it [02:06,  1.66it/s]Extractor Predicting: 211it [02:07,  1.68it/s]Extractor Predicting: 212it [02:08,  1.70it/s]Extractor Predicting: 213it [02:08,  1.59it/s]Extractor Predicting: 214it [02:09,  1.60it/s]Extractor Predicting: 215it [02:09,  1.63it/s]Extractor Predicting: 216it [02:10,  1.68it/s]Extractor Predicting: 217it [02:11,  1.70it/s]Extractor Predicting: 218it [02:11,  1.60it/s]Extractor Predicting: 219it [02:12,  1.61it/s]Extractor Predicting: 220it [02:12,  1.66it/s]Extractor Predicting: 221it [02:13,  1.68it/s]Extractor Predicting: 222it [02:14,  1.68it/s]Extractor Predicting: 223it [02:14,  1.71it/s]Extractor Predicting: 224it [02:15,  1.72it/s]Extractor Predicting: 225it [02:15,  1.72it/s]Extractor Predicting: 226it [02:16,  1.70it/s]Extractor Predicting: 227it [02:17,  1.63it/s]Extractor Predicting: 228it [02:17,  1.66it/s]Extractor Predicting: 229it [02:18,  1.70it/s]Extractor Predicting: 230it [02:18,  1.73it/s]Extractor Predicting: 231it [02:19,  1.72it/s]Extractor Predicting: 232it [02:19,  1.72it/s]Extractor Predicting: 233it [02:20,  1.63it/s]Extractor Predicting: 234it [02:21,  1.67it/s]Extractor Predicting: 235it [02:21,  1.70it/s]Extractor Predicting: 236it [02:22,  1.72it/s]Extractor Predicting: 237it [02:22,  1.71it/s]Extractor Predicting: 238it [02:23,  1.72it/s]Extractor Predicting: 239it [02:24,  1.63it/s]Extractor Predicting: 240it [02:24,  1.69it/s]Extractor Predicting: 241it [02:25,  1.70it/s]Extractor Predicting: 242it [02:25,  1.75it/s]Extractor Predicting: 243it [02:26,  1.75it/s]Extractor Predicting: 244it [02:27,  1.75it/s]Extractor Predicting: 245it [02:27,  1.67it/s]Extractor Predicting: 246it [02:28,  1.69it/s]Extractor Predicting: 247it [02:28,  1.73it/s]Extractor Predicting: 248it [02:29,  1.73it/s]Extractor Predicting: 249it [02:29,  1.75it/s]Extractor Predicting: 250it [02:30,  1.77it/s]Extractor Predicting: 251it [02:31,  1.63it/s]Extractor Predicting: 252it [02:31,  1.65it/s]Extractor Predicting: 253it [02:32,  1.74it/s]Extractor Predicting: 254it [02:32,  1.73it/s]Extractor Predicting: 255it [02:33,  1.52it/s]Extractor Predicting: 256it [02:34,  1.48it/s]Extractor Predicting: 257it [02:35,  1.56it/s]Extractor Predicting: 258it [02:35,  1.64it/s]Extractor Predicting: 259it [02:36,  1.70it/s]Extractor Predicting: 260it [02:36,  1.72it/s]Extractor Predicting: 261it [02:37,  1.71it/s]Extractor Predicting: 262it [02:37,  1.62it/s]Extractor Predicting: 263it [02:38,  1.66it/s]Extractor Predicting: 264it [02:39,  1.71it/s]Extractor Predicting: 265it [02:39,  1.74it/s]Extractor Predicting: 266it [02:40,  1.79it/s]Extractor Predicting: 267it [02:40,  1.74it/s]Extractor Predicting: 268it [02:41,  1.65it/s]Extractor Predicting: 269it [02:41,  1.70it/s]Extractor Predicting: 270it [02:42,  1.71it/s]Extractor Predicting: 271it [02:43,  1.76it/s]Extractor Predicting: 272it [02:43,  1.81it/s]Extractor Predicting: 273it [02:44,  1.78it/s]Extractor Predicting: 274it [02:44,  1.77it/s]Extractor Predicting: 275it [02:45,  1.75it/s]Extractor Predicting: 276it [02:45,  1.75it/s]Extractor Predicting: 277it [02:46,  1.75it/s]Extractor Predicting: 278it [02:47,  1.75it/s]Extractor Predicting: 279it [02:47,  1.62it/s]Extractor Predicting: 280it [02:48,  1.66it/s]Extractor Predicting: 281it [02:48,  1.65it/s]Extractor Predicting: 281it [02:48,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:21,916 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:22,028 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:22,028 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:22,028 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:22,028 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:01:23,049 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:01:23,050 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:01:23,495 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:01:24,873 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:01:24,874 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:28,672 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:28,674 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:28,674 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:28,674 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:01:28,674 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:01:29,938 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:01:29,939 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:01:30,702 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:01:31,165 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:01:31,165 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.29it/s]Extractor Predicting: 2it [00:01,  1.37it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.53it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.97it/s]Extractor Predicting: 6it [00:03,  1.68it/s]
[INFO|configuration_utils.py:515] 2023-08-28 11:01:40,257 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:01:40,259 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 11:01:40,451 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:01:40,452 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 11:01:40,532 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 11:02:15,419 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 11:02:15,525 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 11:02:16,314 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 11:02:16,315 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 11:02:16,610 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:02:16,790 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:02:16,790 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:02:16,790 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:02:16,790 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:02:16,790 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 11:02:16,790 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 11:02:17,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:18,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:19,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:19,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:20,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:21,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:22,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:23,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:23,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:24,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:25,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:25,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:26,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:27,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:28,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:28,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:29,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:30,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:31,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:31,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:32,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:33,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:45, 16.12s/it][WARNING|generation_utils.py:914] 2023-08-28 11:02:33,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:34,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:35,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:35,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:36,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:37,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:37,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:38,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:39,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:40,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:40,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:41,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:42,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:42,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:43,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:44,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:45,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:45,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:46,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:47,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:48,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:48,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:49,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:50,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:36, 16.69s/it][WARNING|generation_utils.py:914] 2023-08-28 11:02:50,878 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:51,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:52,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:52,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:53,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:54,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:54,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:55,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:56,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:56,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:57,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:58,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:58,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:02:59,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:00,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:00,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:01,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:02,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:02,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:03,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:04,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:05,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:05,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:06,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:06,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:07,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:50<03:23, 16.96s/it][WARNING|generation_utils.py:914] 2023-08-28 11:03:08,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:08,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:09,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:10,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:10,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:11,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:12,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:12,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:13,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:13,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:14,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:14,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:15,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:16,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:16,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:17,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:18,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:18,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:19,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:20,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:20,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:21,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:22,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:22,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:05<02:58, 16.27s/it][WARNING|generation_utils.py:914] 2023-08-28 11:03:23,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:24,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:24,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:25,697 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:26,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:26,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:27,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:28,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:29,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:29,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:30,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:31,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:31,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:32,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:33,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:33,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:34,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:35,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:36,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:37,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:37,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:38,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:39,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:39,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:40,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:41,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:24<02:50, 17.01s/it][WARNING|generation_utils.py:914] 2023-08-28 11:03:41,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:42,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:42,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:43,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:44,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:44,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:45,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:46,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:47,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:47,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:48,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:49,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:49,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:50,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:51,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:52,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:52,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:53,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:53,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:54,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:55,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:55,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:56,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:57,750 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:58,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:58,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:03:59,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:42<02:38, 17.62s/it][WARNING|generation_utils.py:914] 2023-08-28 11:04:00,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:01,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:01,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:02,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:03,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:03,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:04,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:05,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:05,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:06,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:07,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:07,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:08,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:09,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:09,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:10,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:11,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:11,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:12,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:13,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:14,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:14,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:15,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:16,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:16,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:59<02:19, 17.45s/it][WARNING|generation_utils.py:914] 2023-08-28 11:04:17,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:18,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:19,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:19,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:20,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:21,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:21,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:22,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:23,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:23,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:24,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:25,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:26,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:26,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:27,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:28,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:29,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:30,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:30,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:31,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:32,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:33,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:33,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:34,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:17<02:01, 17.40s/it][WARNING|generation_utils.py:914] 2023-08-28 11:04:35,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:35,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:36,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:36,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:37,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:38,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:38,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:39,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:40,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:40,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:41,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:42,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:42,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:43,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:43,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:44,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:45,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:45,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:46,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:47,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:47,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:48,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:49,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:49,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:50,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:33<01:42, 17.08s/it][WARNING|generation_utils.py:914] 2023-08-28 11:04:51,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:52,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:52,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:53,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:53,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:54,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:55,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:55,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:56,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:56,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:57,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:58,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:58,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:04:59,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:00,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:00,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:01,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:02,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:02,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:03,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:03,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:04,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:05,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:48<01:21, 16.32s/it][WARNING|generation_utils.py:914] 2023-08-28 11:05:05,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:06,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:07,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:07,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:08,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:09,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:10,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:10,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:11,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:12,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:12,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:13,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:14,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:15,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:16,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:16,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:17,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:18,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:18,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:19,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:20,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:21,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:21,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:04<01:05, 16.35s/it][WARNING|generation_utils.py:914] 2023-08-28 11:05:22,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:23,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:23,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:24,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:25,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:25,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:26,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:27,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:27,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:28,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:29,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:29,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:30,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:31,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:31,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:32,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:33,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:34,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:34,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:35,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:36,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:36,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:37,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:20<00:48, 16.21s/it][WARNING|generation_utils.py:914] 2023-08-28 11:05:38,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:38,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:39,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:40,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:40,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:41,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:42,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:43,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:43,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:44,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:45,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:45,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:46,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:47,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:47,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:48,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:49,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:49,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:50,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:51,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:51,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:52,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:53,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:54,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:54,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:55,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:55,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:56,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:39<00:34, 17.07s/it][WARNING|generation_utils.py:914] 2023-08-28 11:05:57,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:57,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:58,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:59,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:05:59,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:00,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:01,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:01,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:02,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:03,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:03,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:04,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:05,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:05,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:06,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:07,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:08,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:08,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:09,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:10,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:10,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:11,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:12,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:12,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:13,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:13,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:14,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:15,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:15,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:16,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:16,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:17,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:18,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:01<00:18, 18.43s/it][WARNING|generation_utils.py:914] 2023-08-28 11:06:18,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:19,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:20,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:21,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:22,304 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:23,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:23,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:24,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:25,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:26,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:27,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:27,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:28,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:29,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:30,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:31,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:32,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:33,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:34,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:34,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:35,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:36,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:37,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:38,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:38,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:39,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 11:06:40,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:23<00:00, 19.70s/it]Generating: 100%|██████████| 15/15 [04:23<00:00, 17.59s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:06:55,161 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:06:55,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:06:55,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:06:55,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:06:55,261 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:06:56,622 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:06:56,624 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:06:57,547 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:06:58,913 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:06:58,984 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:07:03,561 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:07:03,669 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:07:03,670 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:07:03,670 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:07:03,670 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:07:04,963 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:07:04,964 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:07:05,750 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:07:06,128 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:07:06,128 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : main subject . Context : Later in the year ( 1143–46 ) , he married daughter of Louis XIV and Catherine I of Prussia married to Marie Antoinette III , daughter of Emperor Louis XII and Catherine of Rheims . Head Entity : Catherine I , Tail Entity : Emperor Louis XII and Catherine I of Prussia .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7451923076923077, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.81640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.7391826923076923, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 294, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7013888888888888, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amadec for the first time . Head Entity : Amadec , Tail Entity : Amadeca .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.75625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Paul Groening', 'nominated for', '', 'After a stint in the Swedish music industry in 2002 alongside the late Paul Groening , he moved away to New Zealand to continue his education in the country .')"}}
['Relation : operating system . Context : The CVRN ( Computer Vision and Imaging Systems ) is a digital camera developed at the National Institutes of Health . Head Entity : CVRN , Tail Entity : CVR , Head Entity : National Institutes of Health .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 258, 'raw': 384}
{'target': 600, 'success': 280, 'raw': 416}
{'target': 600, 'success': 303, 'raw': 448}
{'target': 600, 'success': 326, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 372, 'raw': 544}
{'target': 600, 'success': 394, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 527, 'raw': 768}
{'target': 600, 'success': 551, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 598, 'raw': 864}
{'target': 600, 'success': 620, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6919642857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 107, 'raw': 192}
{'target': 600, 'success': 127, 'raw': 224}
{'target': 600, 'success': 148, 'raw': 256}
{'target': 600, 'success': 172, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 243, 'raw': 416}
{'target': 600, 'success': 262, 'raw': 448}
{'target': 600, 'success': 279, 'raw': 480}
{'target': 600, 'success': 298, 'raw': 512}
{'target': 600, 'success': 317, 'raw': 544}
{'target': 600, 'success': 332, 'raw': 576}
{'target': 600, 'success': 350, 'raw': 608}
{'target': 600, 'success': 365, 'raw': 640}
{'target': 600, 'success': 388, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 429, 'raw': 736}
{'target': 600, 'success': 446, 'raw': 768}
{'target': 600, 'success': 466, 'raw': 800}
{'target': 600, 'success': 482, 'raw': 832}
{'target': 600, 'success': 495, 'raw': 864}
{'target': 600, 'success': 509, 'raw': 896}
{'target': 600, 'success': 522, 'raw': 928}
{'target': 600, 'success': 543, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 584, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5681818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n']
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n', "Relation : religion . Context : After the death of King Henry IV of France ( c. 589 - 7 February 1235 ) , St Peter was succeeded as Archbishop by the eponymous St Peter 's successor , the first Pope . Head Entity : St Peter 's successors , Tail Entity : St Peter 's .\n"]
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 369, 'raw': 544}
{'target': 600, 'success': 391, 'raw': 576}
{'target': 600, 'success': 415, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 459, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 534, 'raw': 768}
{'target': 600, 'success': 553, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 15147
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15247, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.45it/s]Extractor Estimating: 2it [00:01,  1.29it/s]Extractor Estimating: 3it [00:02,  1.35it/s]Extractor Estimating: 4it [00:02,  1.44it/s]Extractor Estimating: 5it [00:03,  1.45it/s]Extractor Estimating: 6it [00:04,  1.47it/s]Extractor Estimating: 7it [00:04,  1.46it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:06,  1.56it/s]Extractor Estimating: 10it [00:06,  1.51it/s]Extractor Estimating: 11it [00:07,  1.51it/s]Extractor Estimating: 12it [00:08,  1.44it/s]Extractor Estimating: 13it [00:08,  1.42it/s]Extractor Estimating: 14it [00:09,  1.44it/s]Extractor Estimating: 15it [00:10,  1.49it/s]Extractor Estimating: 16it [00:10,  1.46it/s]Extractor Estimating: 17it [00:11,  1.48it/s]Extractor Estimating: 18it [00:12,  1.55it/s]Extractor Estimating: 19it [00:12,  1.48it/s]Extractor Estimating: 20it [00:13,  1.49it/s]Extractor Estimating: 21it [00:14,  1.50it/s]Extractor Estimating: 22it [00:14,  1.47it/s]Extractor Estimating: 23it [00:15,  1.54it/s]Extractor Estimating: 24it [00:16,  1.50it/s]Extractor Estimating: 25it [00:16,  1.48it/s]Extractor Estimating: 26it [00:17,  1.46it/s]Extractor Estimating: 27it [00:18,  1.35it/s]Extractor Estimating: 28it [00:19,  1.39it/s]Extractor Estimating: 29it [00:19,  1.44it/s]Extractor Estimating: 30it [00:20,  1.41it/s]Extractor Estimating: 31it [00:21,  1.41it/s]Extractor Estimating: 32it [00:22,  1.41it/s]Extractor Estimating: 33it [00:22,  1.42it/s]Extractor Estimating: 34it [00:23,  1.42it/s]Extractor Estimating: 35it [00:24,  1.44it/s]Extractor Estimating: 36it [00:24,  1.44it/s]Extractor Estimating: 37it [00:25,  1.48it/s]Extractor Estimating: 38it [00:26,  1.48it/s]Extractor Estimating: 39it [00:26,  1.50it/s]Extractor Estimating: 40it [00:27,  1.48it/s]Extractor Estimating: 41it [00:28,  1.43it/s]Extractor Estimating: 42it [00:28,  1.40it/s]Extractor Estimating: 43it [00:29,  1.43it/s]Extractor Estimating: 44it [00:30,  1.44it/s]Extractor Estimating: 45it [00:30,  1.45it/s]Extractor Estimating: 46it [00:31,  1.48it/s]Extractor Estimating: 47it [00:32,  1.45it/s]Extractor Estimating: 48it [00:32,  1.47it/s]Extractor Estimating: 49it [00:33,  1.52it/s]Extractor Estimating: 50it [00:34,  1.51it/s]Extractor Estimating: 51it [00:34,  1.47it/s]Extractor Estimating: 52it [00:35,  1.44it/s]Extractor Estimating: 53it [00:36,  1.49it/s]Extractor Estimating: 54it [00:36,  1.49it/s]Extractor Estimating: 55it [00:37,  1.52it/s]Extractor Estimating: 56it [00:38,  1.51it/s]Extractor Estimating: 57it [00:39,  1.45it/s]Extractor Estimating: 58it [00:39,  1.45it/s]Extractor Estimating: 59it [00:40,  1.49it/s]Extractor Estimating: 60it [00:40,  1.55it/s]Extractor Estimating: 61it [00:41,  1.58it/s]Extractor Estimating: 62it [00:42,  1.55it/s]Extractor Estimating: 63it [00:42,  1.61it/s]Extractor Estimating: 64it [00:43,  1.61it/s]Extractor Estimating: 65it [00:44,  1.60it/s]Extractor Estimating: 66it [00:44,  1.59it/s]Extractor Estimating: 67it [00:45,  1.44it/s]Extractor Estimating: 68it [00:46,  1.47it/s]Extractor Estimating: 69it [00:46,  1.47it/s]Extractor Estimating: 70it [00:47,  1.49it/s]Extractor Estimating: 71it [00:48,  1.51it/s]Extractor Estimating: 72it [00:48,  1.48it/s]Extractor Estimating: 73it [00:49,  1.52it/s]Extractor Estimating: 74it [00:50,  1.57it/s]Extractor Estimating: 75it [00:50,  1.60it/s]Extractor Estimating: 76it [00:51,  1.46it/s]Extractor Estimating: 77it [00:52,  1.42it/s]Extractor Estimating: 78it [00:52,  1.47it/s]Extractor Estimating: 79it [00:53,  1.55it/s]Extractor Estimating: 80it [00:53,  1.63it/s]Extractor Estimating: 81it [00:54,  1.65it/s]Extractor Estimating: 82it [00:55,  1.67it/s]Extractor Estimating: 83it [00:55,  1.68it/s]Extractor Estimating: 84it [00:56,  1.58it/s]Extractor Estimating: 85it [00:57,  1.59it/s]Extractor Estimating: 86it [00:57,  1.64it/s]Extractor Estimating: 87it [00:58,  1.59it/s]Extractor Estimating: 88it [00:58,  1.55it/s]Extractor Estimating: 89it [00:59,  1.46it/s]Extractor Estimating: 90it [01:00,  1.48it/s]Extractor Estimating: 91it [01:01,  1.52it/s]Extractor Estimating: 92it [01:01,  1.52it/s]Extractor Estimating: 93it [01:02,  1.54it/s]Extractor Estimating: 94it [01:03,  1.45it/s]Extractor Estimating: 95it [01:03,  1.51it/s]Extractor Estimating: 96it [01:04,  1.39it/s]Extractor Estimating: 97it [01:05,  1.46it/s]Extractor Estimating: 98it [01:05,  1.49it/s]Extractor Estimating: 99it [01:06,  1.34it/s]Extractor Estimating: 100it [01:07,  1.43it/s]Extractor Estimating: 101it [01:07,  1.46it/s]Extractor Estimating: 102it [01:08,  1.43it/s]Extractor Estimating: 103it [01:09,  1.43it/s]Extractor Estimating: 104it [01:10,  1.43it/s]Extractor Estimating: 105it [01:10,  1.51it/s]Extractor Estimating: 106it [01:11,  1.54it/s]Extractor Estimating: 107it [01:11,  1.58it/s]Extractor Estimating: 108it [01:12,  1.55it/s]Extractor Estimating: 109it [01:13,  1.55it/s]Extractor Estimating: 110it [01:13,  1.54it/s]Extractor Estimating: 111it [01:14,  1.55it/s]Extractor Estimating: 112it [01:15,  1.58it/s]Extractor Estimating: 113it [01:15,  1.61it/s]Extractor Estimating: 114it [01:16,  1.51it/s]Extractor Estimating: 115it [01:17,  1.51it/s]Extractor Estimating: 116it [01:17,  1.49it/s]Extractor Estimating: 117it [01:18,  1.53it/s]Extractor Estimating: 118it [01:19,  1.51it/s]Extractor Estimating: 119it [01:19,  1.49it/s]Extractor Estimating: 120it [01:20,  1.47it/s]Extractor Estimating: 121it [01:21,  1.53it/s]Extractor Estimating: 122it [01:21,  1.54it/s]Extractor Estimating: 123it [01:22,  1.57it/s]Extractor Estimating: 124it [01:23,  1.51it/s]Extractor Estimating: 125it [01:23,  1.50it/s]Extractor Estimating: 126it [01:24,  1.54it/s]Extractor Estimating: 127it [01:24,  1.54it/s]Extractor Estimating: 128it [01:25,  1.60it/s]Extractor Estimating: 129it [01:26,  1.61it/s]Extractor Estimating: 130it [01:26,  1.56it/s]Extractor Estimating: 131it [01:27,  1.51it/s]Extractor Estimating: 132it [01:28,  1.51it/s]Extractor Estimating: 133it [01:28,  1.52it/s]Extractor Estimating: 134it [01:29,  1.55it/s]Extractor Estimating: 135it [01:30,  1.56it/s]Extractor Estimating: 136it [01:30,  1.61it/s]Extractor Estimating: 137it [01:31,  1.56it/s]Extractor Estimating: 138it [01:31,  1.61it/s]Extractor Estimating: 139it [01:32,  1.65it/s]Extractor Estimating: 140it [01:33,  1.56it/s]Extractor Estimating: 141it [01:33,  1.61it/s]Extractor Estimating: 142it [01:34,  1.67it/s]Extractor Estimating: 143it [01:34,  1.67it/s]Extractor Estimating: 144it [01:35,  1.66it/s]Extractor Estimating: 145it [01:36,  1.59it/s]Extractor Estimating: 146it [01:37,  1.47it/s]Extractor Estimating: 147it [01:37,  1.54it/s]Extractor Estimating: 148it [01:38,  1.58it/s]Extractor Estimating: 149it [01:38,  1.60it/s]Extractor Estimating: 150it [01:39,  1.47it/s]Extractor Estimating: 151it [01:40,  1.50it/s]Extractor Estimating: 152it [01:40,  1.50it/s]Extractor Estimating: 153it [01:41,  1.50it/s]Extractor Estimating: 154it [01:42,  1.49it/s]Extractor Estimating: 155it [01:43,  1.40it/s]Extractor Estimating: 156it [01:43,  1.41it/s]Extractor Estimating: 157it [01:44,  1.45it/s]Extractor Estimating: 158it [01:45,  1.49it/s]Extractor Estimating: 159it [01:45,  1.56it/s]Extractor Estimating: 160it [01:46,  1.49it/s]Extractor Estimating: 161it [01:47,  1.49it/s]Extractor Estimating: 162it [01:47,  1.47it/s]Extractor Estimating: 163it [01:48,  1.46it/s]Extractor Estimating: 164it [01:49,  1.51it/s]Extractor Estimating: 165it [01:49,  1.45it/s]Extractor Estimating: 166it [01:50,  1.49it/s]Extractor Estimating: 167it [01:51,  1.51it/s]Extractor Estimating: 168it [01:51,  1.47it/s]Extractor Estimating: 169it [01:52,  1.52it/s]Extractor Estimating: 170it [01:53,  1.38it/s]Extractor Estimating: 171it [01:54,  1.38it/s]Extractor Estimating: 172it [01:54,  1.44it/s]Extractor Estimating: 173it [01:55,  1.49it/s]Extractor Estimating: 174it [01:55,  1.53it/s]Extractor Estimating: 175it [01:56,  1.55it/s]Extractor Estimating: 176it [01:57,  1.52it/s]Extractor Estimating: 177it [01:57,  1.47it/s]Extractor Estimating: 178it [01:58,  1.50it/s]Extractor Estimating: 179it [01:59,  1.59it/s]Extractor Estimating: 180it [01:59,  1.59it/s]Extractor Estimating: 181it [02:00,  1.59it/s]Extractor Estimating: 182it [02:01,  1.51it/s]Extractor Estimating: 183it [02:01,  1.54it/s]Extractor Estimating: 184it [02:02,  1.61it/s]Extractor Estimating: 185it [02:02,  1.59it/s]Extractor Estimating: 186it [02:03,  1.62it/s]Extractor Estimating: 187it [02:04,  1.48it/s]Extractor Estimating: 188it [02:04,  1.54it/s]Extractor Estimating: 189it [02:05,  1.57it/s]Extractor Estimating: 190it [02:06,  1.57it/s]Extractor Estimating: 191it [02:06,  1.58it/s]Extractor Estimating: 192it [02:07,  1.57it/s]Extractor Estimating: 193it [02:08,  1.61it/s]Extractor Estimating: 194it [02:08,  1.63it/s]Extractor Estimating: 195it [02:09,  1.65it/s]Extractor Estimating: 196it [02:09,  1.54it/s]Extractor Estimating: 197it [02:10,  1.38it/s]Extractor Estimating: 198it [02:11,  1.38it/s]Extractor Estimating: 199it [02:12,  1.42it/s]Extractor Estimating: 200it [02:12,  1.50it/s]Extractor Estimating: 201it [02:13,  1.48it/s]Extractor Estimating: 202it [02:14,  1.43it/s]Extractor Estimating: 203it [02:14,  1.42it/s]Extractor Estimating: 204it [02:15,  1.44it/s]Extractor Estimating: 205it [02:16,  1.43it/s]Extractor Estimating: 206it [02:17,  1.40it/s]Extractor Estimating: 207it [02:17,  1.34it/s]Extractor Estimating: 208it [02:18,  1.36it/s]Extractor Estimating: 209it [02:19,  1.41it/s]Extractor Estimating: 210it [02:20,  1.40it/s]Extractor Estimating: 211it [02:20,  1.43it/s]Extractor Estimating: 212it [02:21,  1.42it/s]Extractor Estimating: 213it [02:22,  1.47it/s]Extractor Estimating: 214it [02:22,  1.46it/s]Extractor Estimating: 215it [02:23,  1.48it/s]Extractor Estimating: 216it [02:24,  1.48it/s]Extractor Estimating: 217it [02:24,  1.44it/s]Extractor Estimating: 218it [02:25,  1.43it/s]Extractor Estimating: 219it [02:26,  1.46it/s]Extractor Estimating: 220it [02:26,  1.44it/s]Extractor Estimating: 221it [02:27,  1.37it/s]Extractor Estimating: 222it [02:28,  1.35it/s]Extractor Estimating: 223it [02:29,  1.37it/s]Extractor Estimating: 224it [02:29,  1.38it/s]Extractor Estimating: 225it [02:30,  1.37it/s]Extractor Estimating: 226it [02:31,  1.50it/s]Extractor Estimating: 227it [02:31,  1.49it/s]Extractor Estimating: 228it [02:32,  1.51it/s]Extractor Estimating: 229it [02:33,  1.55it/s]Extractor Estimating: 230it [02:33,  1.58it/s]Extractor Estimating: 231it [02:34,  1.52it/s]Extractor Estimating: 232it [02:35,  1.53it/s]Extractor Estimating: 233it [02:35,  1.65it/s]Extractor Estimating: 234it [02:36,  1.70it/s]Extractor Estimating: 235it [02:36,  1.72it/s]Extractor Estimating: 236it [02:37,  1.72it/s]Extractor Estimating: 237it [02:37,  1.75it/s]Extractor Estimating: 238it [02:38,  1.58it/s]Extractor Estimating: 239it [02:39,  1.56it/s]Extractor Estimating: 240it [02:39,  1.62it/s]Extractor Estimating: 241it [02:40,  1.64it/s]Extractor Estimating: 242it [02:40,  1.65it/s]Extractor Estimating: 243it [02:41,  1.59it/s]Extractor Estimating: 244it [02:42,  1.63it/s]Extractor Estimating: 245it [02:42,  1.65it/s]Extractor Estimating: 246it [02:43,  1.65it/s]Extractor Estimating: 247it [02:44,  1.64it/s]Extractor Estimating: 248it [02:44,  1.53it/s]Extractor Estimating: 249it [02:45,  1.56it/s]Extractor Estimating: 250it [02:45,  1.58it/s]Extractor Estimating: 251it [02:46,  1.51it/s]Extractor Estimating: 252it [02:47,  1.57it/s]Extractor Estimating: 253it [02:48,  1.48it/s]Extractor Estimating: 254it [02:48,  1.43it/s]Extractor Estimating: 255it [02:49,  1.47it/s]Extractor Estimating: 256it [02:50,  1.53it/s]Extractor Estimating: 257it [02:50,  1.55it/s]Extractor Estimating: 258it [02:51,  1.48it/s]Extractor Estimating: 259it [02:52,  1.54it/s]Extractor Estimating: 260it [02:52,  1.54it/s]Extractor Estimating: 261it [02:53,  1.54it/s]Extractor Estimating: 262it [02:54,  1.50it/s]Extractor Estimating: 263it [02:54,  1.42it/s]Extractor Estimating: 264it [02:55,  1.46it/s]Extractor Estimating: 265it [02:56,  1.39it/s]Extractor Estimating: 266it [02:57,  1.34it/s]Extractor Estimating: 267it [02:57,  1.38it/s]Extractor Estimating: 268it [02:58,  1.42it/s]Extractor Estimating: 269it [02:59,  1.34it/s]Extractor Estimating: 270it [02:59,  1.38it/s]Extractor Estimating: 271it [03:00,  1.37it/s]Extractor Estimating: 272it [03:01,  1.40it/s]Extractor Estimating: 273it [03:01,  1.43it/s]Extractor Estimating: 274it [03:02,  1.44it/s]Extractor Estimating: 275it [03:03,  1.50it/s]Extractor Estimating: 276it [03:03,  1.54it/s]Extractor Estimating: 277it [03:04,  1.55it/s]Extractor Estimating: 278it [03:05,  1.60it/s]Extractor Estimating: 279it [03:05,  1.59it/s]Extractor Estimating: 280it [03:06,  1.58it/s]Extractor Estimating: 281it [03:06,  1.60it/s]Extractor Estimating: 282it [03:07,  1.64it/s]Extractor Estimating: 283it [03:08,  1.66it/s]Extractor Estimating: 284it [03:08,  1.59it/s]Extractor Estimating: 285it [03:09,  1.62it/s]Extractor Estimating: 286it [03:10,  1.60it/s]Extractor Estimating: 287it [03:10,  1.55it/s]Extractor Estimating: 288it [03:11,  1.52it/s]Extractor Estimating: 289it [03:12,  1.44it/s]Extractor Estimating: 290it [03:12,  1.52it/s]Extractor Estimating: 291it [03:13,  1.52it/s]Extractor Estimating: 292it [03:14,  1.54it/s]Extractor Estimating: 293it [03:14,  1.54it/s]Extractor Estimating: 294it [03:15,  1.47it/s]Extractor Estimating: 295it [03:16,  1.51it/s]Extractor Estimating: 296it [03:16,  1.54it/s]Extractor Estimating: 297it [03:17,  1.53it/s]Extractor Estimating: 298it [03:17,  1.56it/s]Extractor Estimating: 299it [03:18,  1.51it/s]Extractor Estimating: 300it [03:19,  1.52it/s]Extractor Estimating: 301it [03:20,  1.53it/s]Extractor Estimating: 302it [03:20,  1.58it/s]Extractor Estimating: 303it [03:21,  1.60it/s]Extractor Estimating: 304it [03:21,  1.50it/s]Extractor Estimating: 305it [03:22,  1.50it/s]Extractor Estimating: 306it [03:23,  1.58it/s]Extractor Estimating: 307it [03:23,  1.59it/s]Extractor Estimating: 308it [03:24,  1.55it/s]Extractor Estimating: 309it [03:25,  1.54it/s]Extractor Estimating: 310it [03:25,  1.55it/s]Extractor Estimating: 311it [03:26,  1.54it/s]Extractor Estimating: 312it [03:27,  1.53it/s]Extractor Estimating: 313it [03:27,  1.52it/s]Extractor Estimating: 314it [03:28,  1.52it/s]Extractor Estimating: 315it [03:29,  1.56it/s]Extractor Estimating: 316it [03:29,  1.51it/s]Extractor Estimating: 317it [03:30,  1.52it/s]Extractor Estimating: 318it [03:30,  1.56it/s]Extractor Estimating: 319it [03:31,  1.55it/s]Extractor Estimating: 320it [03:32,  1.56it/s]Extractor Estimating: 321it [03:32,  1.55it/s]Extractor Estimating: 322it [03:33,  1.55it/s]Extractor Estimating: 323it [03:34,  1.53it/s]Extractor Estimating: 324it [03:34,  1.53it/s]Extractor Estimating: 325it [03:35,  1.56it/s]Extractor Estimating: 326it [03:36,  1.53it/s]Extractor Estimating: 327it [03:36,  1.58it/s]Extractor Estimating: 328it [03:37,  1.60it/s]Extractor Estimating: 329it [03:37,  1.67it/s]Extractor Estimating: 330it [03:38,  1.61it/s]Extractor Estimating: 331it [03:39,  1.63it/s]Extractor Estimating: 332it [03:39,  1.65it/s]Extractor Estimating: 333it [03:40,  1.73it/s]Extractor Estimating: 334it [03:40,  1.72it/s]Extractor Estimating: 335it [03:41,  1.67it/s]Extractor Estimating: 336it [03:42,  1.63it/s]Extractor Estimating: 337it [03:43,  1.46it/s]Extractor Estimating: 338it [03:43,  1.53it/s]Extractor Estimating: 339it [03:44,  1.54it/s]Extractor Estimating: 340it [03:44,  1.58it/s]Extractor Estimating: 341it [03:45,  1.64it/s]Extractor Estimating: 342it [03:46,  1.60it/s]Extractor Estimating: 343it [03:46,  1.65it/s]Extractor Estimating: 344it [03:47,  1.70it/s]Extractor Estimating: 345it [03:47,  1.71it/s]Extractor Estimating: 346it [03:48,  1.68it/s]Extractor Estimating: 347it [03:48,  1.71it/s]Extractor Estimating: 348it [03:49,  1.49it/s]Extractor Estimating: 349it [03:50,  1.54it/s]Extractor Estimating: 350it [03:50,  1.57it/s]Extractor Estimating: 351it [03:51,  1.48it/s]Extractor Estimating: 352it [03:52,  1.44it/s]Extractor Estimating: 353it [03:53,  1.34it/s]Extractor Estimating: 354it [03:54,  1.39it/s]Extractor Estimating: 355it [03:54,  1.36it/s]Extractor Estimating: 356it [03:55,  1.39it/s]Extractor Estimating: 357it [03:56,  1.44it/s]Extractor Estimating: 358it [03:56,  1.42it/s]Extractor Estimating: 359it [03:57,  1.41it/s]Extractor Estimating: 360it [03:58,  1.44it/s]Extractor Estimating: 361it [03:58,  1.43it/s]Extractor Estimating: 362it [03:59,  1.45it/s]Extractor Estimating: 363it [04:00,  1.53it/s]Extractor Estimating: 364it [04:00,  1.47it/s]Extractor Estimating: 365it [04:01,  1.46it/s]Extractor Estimating: 366it [04:02,  1.48it/s]Extractor Estimating: 367it [04:02,  1.53it/s]Extractor Estimating: 368it [04:03,  1.55it/s]Extractor Estimating: 369it [04:04,  1.41it/s]Extractor Estimating: 370it [04:04,  1.47it/s]Extractor Estimating: 371it [04:05,  1.49it/s]Extractor Estimating: 372it [04:06,  1.44it/s]Extractor Estimating: 373it [04:07,  1.42it/s]Extractor Estimating: 374it [04:07,  1.38it/s]Extractor Estimating: 375it [04:08,  1.41it/s]Extractor Estimating: 375it [04:08,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:46,402 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:46,465 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:46,465 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:46,466 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:46,466 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 11:11:47,177 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 11:11:47,178 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:11:47,557 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 11:11:48,793 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:11:48,793 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:50,572 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:50,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:50,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:50,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 11:11:50,637 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 11:11:51,301 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 11:11:51,302 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 11:11:51,715 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 11:11:51,994 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 11:11:51,994 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 13:31:36,841 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 13:31:38,572 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7696 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 20372
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20472, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20472, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.123, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.076, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.117, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 79, avg_time 1.113, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 179, avg_time 1.127, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 279, avg_time 2.208, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 58, avg_time 1.076, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 158, avg_time 1.119, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 258, avg_time 1.113, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 37, avg_time 1.125, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 137, avg_time 2.192, loss:nan
g_step 1200, step 237, avg_time 1.095, loss:nan
g_step 1300, step 16, avg_time 1.090, loss:nan
g_step 1400, step 116, avg_time 1.138, loss:nan
g_step 1500, step 216, avg_time 1.087, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 316, avg_time 2.198, loss:nan
g_step 1700, step 95, avg_time 1.109, loss:nan
g_step 1800, step 195, avg_time 1.103, loss:nan
g_step 1900, step 295, avg_time 1.105, loss:nan
g_step 2000, step 74, avg_time 1.130, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 174, avg_time 2.169, loss:nan
g_step 2200, step 274, avg_time 1.088, loss:nan
g_step 2300, step 53, avg_time 1.100, loss:nan
g_step 2400, step 153, avg_time 1.104, loss:nan
g_step 2500, step 253, avg_time 1.100, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 32, avg_time 2.163, loss:nan
g_step 2700, step 132, avg_time 1.081, loss:nan
g_step 2800, step 232, avg_time 1.116, loss:nan
g_step 2900, step 11, avg_time 1.112, loss:nan
g_step 3000, step 111, avg_time 1.107, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 211, avg_time 2.183, loss:nan
g_step 3200, step 311, avg_time 1.084, loss:nan
g_step 3300, step 90, avg_time 1.101, loss:nan
g_step 3400, step 190, avg_time 1.109, loss:nan
g_step 3500, step 290, avg_time 1.072, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 69, avg_time 2.158, loss:nan
g_step 3700, step 169, avg_time 1.087, loss:nan
g_step 3800, step 269, avg_time 1.094, loss:nan
g_step 3900, step 48, avg_time 1.095, loss:nan
g_step 4000, step 148, avg_time 1.079, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 248, avg_time 2.162, loss:nan
g_step 4200, step 27, avg_time 1.078, loss:nan
g_step 4300, step 127, avg_time 1.118, loss:nan
g_step 4400, step 227, avg_time 1.093, loss:nan
g_step 4500, step 6, avg_time 1.098, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 106, avg_time 2.172, loss:nan
g_step 4700, step 206, avg_time 1.112, loss:nan
g_step 4800, step 306, avg_time 1.099, loss:nan
g_step 4900, step 85, avg_time 1.106, loss:nan
g_step 5000, step 185, avg_time 1.110, loss:nan
learning rate was adjusted to 0.0008
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 285, avg_time 2.179, loss:nan
g_step 5200, step 64, avg_time 1.123, loss:nan
g_step 5300, step 164, avg_time 1.085, loss:nan
g_step 5400, step 264, avg_time 1.111, loss:nan
g_step 5500, step 43, avg_time 1.100, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 143, avg_time 2.171, loss:nan
g_step 5700, step 243, avg_time 1.090, loss:nan
g_step 5800, step 22, avg_time 1.111, loss:nan
g_step 5900, step 122, avg_time 1.122, loss:nan
g_step 6000, step 222, avg_time 1.092, loss:nan
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 1, avg_time 2.184, loss:nan
g_step 6200, step 101, avg_time 1.099, loss:nan
g_step 6300, step 201, avg_time 1.102, loss:nan
g_step 6400, step 301, avg_time 1.122, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 13:31:38 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 13:31:38 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_13-31-36_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 13:31:41 - WARNING - datasets.builder -   Using custom data configuration default-e7490adc22dff222
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e7490adc22dff222/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 13:31:50,987 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:31:50,988 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 13:31:50,989 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 13:31:50,990 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 13:31:51,321 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:31:51,499 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:31:51,499 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:31:51,499 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:31:51,500 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:31:51,500 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 13:31:51,500 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 13:31:52,784 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 13:31:56,048 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 13:31:56,048 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e7490adc22dff222/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:05,  1.36ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.35ba/s] 38%|███▊      | 3/8 [00:01<00:01,  3.08ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.55ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.89ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.14ba/s] 88%|████████▊ | 7/8 [00:02<00:00,  4.31ba/s]100%|██████████| 8/8 [00:02<00:00,  4.85ba/s]100%|██████████| 8/8 [00:02<00:00,  3.72ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.41ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.37ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.02ba/s]100%|██████████| 4/4 [00:01<00:00,  4.10ba/s]100%|██████████| 4/4 [00:01<00:00,  3.17ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:04,  1.75ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.28ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.73ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  6.89ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  8.16ba/s]100%|██████████| 8/8 [00:01<00:00,  6.48ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.26ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.00ba/s]100%|██████████| 4/4 [00:00<00:00,  7.21ba/s]100%|██████████| 4/4 [00:00<00:00,  5.69ba/s]
[INFO|trainer.py:414] 2023-08-28 13:32:09,668 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 13:32:10,221 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 13:32:10,221 >>   Num examples = 7700
[INFO|trainer.py:1149] 2023-08-28 13:32:10,221 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 13:32:10,221 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 13:32:10,221 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 13:32:10,221 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 13:32:10,221 >>   Total optimization steps = 600
  0%|          | 0/600 [00:00<?, ?it/s]  0%|          | 1/600 [00:00<02:54,  3.43it/s]  0%|          | 2/600 [00:00<02:49,  3.53it/s]  0%|          | 3/600 [00:01<04:16,  2.33it/s]  1%|          | 4/600 [00:01<03:40,  2.70it/s]  1%|          | 5/600 [00:01<03:20,  2.96it/s]  1%|          | 6/600 [00:02<03:08,  3.14it/s]  1%|          | 7/600 [00:02<03:01,  3.27it/s]  1%|▏         | 8/600 [00:02<02:55,  3.37it/s]  2%|▏         | 9/600 [00:02<02:52,  3.43it/s]  2%|▏         | 10/600 [00:03<02:49,  3.47it/s]  2%|▏         | 11/600 [00:03<02:47,  3.51it/s]  2%|▏         | 12/600 [00:03<02:46,  3.52it/s]  2%|▏         | 13/600 [00:03<02:45,  3.54it/s]  2%|▏         | 14/600 [00:04<02:44,  3.55it/s]  2%|▎         | 15/600 [00:04<02:44,  3.57it/s]  3%|▎         | 16/600 [00:04<02:43,  3.57it/s]  3%|▎         | 17/600 [00:05<02:43,  3.57it/s]  3%|▎         | 18/600 [00:05<02:42,  3.58it/s]  3%|▎         | 19/600 [00:05<02:42,  3.57it/s]  3%|▎         | 20/600 [00:05<02:42,  3.58it/s]  4%|▎         | 21/600 [00:06<02:41,  3.58it/s]  4%|▎         | 22/600 [00:06<02:41,  3.58it/s]  4%|▍         | 23/600 [00:06<02:41,  3.58it/s]  4%|▍         | 24/600 [00:07<02:40,  3.58it/s]  4%|▍         | 25/600 [00:07<02:40,  3.59it/s]  4%|▍         | 26/600 [00:07<02:39,  3.60it/s]  4%|▍         | 27/600 [00:07<02:38,  3.62it/s]  5%|▍         | 28/600 [00:08<02:37,  3.62it/s]  5%|▍         | 29/600 [00:08<02:37,  3.62it/s]  5%|▌         | 30/600 [00:08<02:37,  3.62it/s]  5%|▌         | 31/600 [00:08<02:36,  3.63it/s]  5%|▌         | 32/600 [00:09<02:36,  3.63it/s]  6%|▌         | 33/600 [00:09<02:36,  3.63it/s]  6%|▌         | 34/600 [00:09<02:35,  3.63it/s]  6%|▌         | 35/600 [00:10<02:35,  3.63it/s]  6%|▌         | 36/600 [00:10<02:35,  3.63it/s]  6%|▌         | 37/600 [00:10<02:35,  3.63it/s]  6%|▋         | 38/600 [00:10<02:34,  3.63it/s]  6%|▋         | 39/600 [00:11<02:34,  3.63it/s]  7%|▋         | 40/600 [00:11<02:34,  3.63it/s]  7%|▋         | 41/600 [00:11<02:34,  3.63it/s]  7%|▋         | 42/600 [00:11<02:33,  3.63it/s]  7%|▋         | 43/600 [00:12<02:33,  3.62it/s]  7%|▋         | 44/600 [00:12<02:33,  3.62it/s]  8%|▊         | 45/600 [00:12<02:33,  3.62it/s]  8%|▊         | 46/600 [00:13<02:32,  3.62it/s]  8%|▊         | 47/600 [00:13<02:32,  3.62it/s]  8%|▊         | 48/600 [00:13<02:32,  3.62it/s]  8%|▊         | 49/600 [00:13<02:31,  3.63it/s]  8%|▊         | 50/600 [00:14<02:31,  3.63it/s]  8%|▊         | 51/600 [00:14<02:31,  3.63it/s]  9%|▊         | 52/600 [00:14<02:31,  3.63it/s]  9%|▉         | 53/600 [00:15<02:30,  3.62it/s]  9%|▉         | 54/600 [00:15<02:30,  3.62it/s]  9%|▉         | 55/600 [00:15<02:30,  3.62it/s]  9%|▉         | 56/600 [00:15<02:29,  3.63it/s] 10%|▉         | 57/600 [00:16<02:30,  3.62it/s] 10%|▉         | 58/600 [00:16<02:29,  3.62it/s] 10%|▉         | 59/600 [00:16<02:29,  3.62it/s] 10%|█         | 60/600 [00:16<02:29,  3.62it/s] 10%|█         | 61/600 [00:17<02:28,  3.62it/s] 10%|█         | 62/600 [00:17<02:28,  3.62it/s] 10%|█         | 63/600 [00:17<02:28,  3.62it/s] 11%|█         | 64/600 [00:18<02:28,  3.62it/s] 11%|█         | 65/600 [00:18<02:27,  3.62it/s] 11%|█         | 66/600 [00:18<02:27,  3.62it/s] 11%|█         | 67/600 [00:18<02:27,  3.62it/s] 11%|█▏        | 68/600 [00:19<02:26,  3.62it/s] 12%|█▏        | 69/600 [00:19<02:26,  3.61it/s] 12%|█▏        | 70/600 [00:19<02:26,  3.61it/s] 12%|█▏        | 71/600 [00:20<02:26,  3.61it/s] 12%|█▏        | 72/600 [00:20<02:26,  3.61it/s] 12%|█▏        | 73/600 [00:20<02:25,  3.62it/s] 12%|█▏        | 74/600 [00:20<02:25,  3.62it/s] 12%|█▎        | 75/600 [00:21<02:25,  3.62it/s] 13%|█▎        | 76/600 [00:21<02:24,  3.62it/s] 13%|█▎        | 77/600 [00:21<02:24,  3.62it/s] 13%|█▎        | 78/600 [00:22<02:53,  3.01it/s] 13%|█▎        | 79/600 [00:22<02:44,  3.17it/s] 13%|█▎        | 80/600 [00:22<02:37,  3.30it/s] 14%|█▎        | 81/600 [00:22<02:33,  3.38it/s] 14%|█▎        | 82/600 [00:23<02:29,  3.45it/s] 14%|█▍        | 83/600 [00:23<02:27,  3.50it/s] 14%|█▍        | 84/600 [00:23<02:25,  3.54it/s] 14%|█▍        | 85/600 [00:24<02:24,  3.57it/s] 14%|█▍        | 86/600 [00:24<02:23,  3.58it/s] 14%|█▍        | 87/600 [00:24<02:22,  3.59it/s] 15%|█▍        | 88/600 [00:25<02:48,  3.04it/s] 15%|█▍        | 89/600 [00:25<02:39,  3.20it/s] 15%|█▌        | 90/600 [00:25<02:33,  3.31it/s] 15%|█▌        | 91/600 [00:25<02:29,  3.40it/s] 15%|█▌        | 92/600 [00:26<02:26,  3.47it/s] 16%|█▌        | 93/600 [00:26<02:24,  3.51it/s] 16%|█▌        | 94/600 [00:26<02:22,  3.55it/s] 16%|█▌        | 95/600 [00:28<06:21,  1.32it/s] 16%|█▌        | 96/600 [00:29<05:34,  1.51it/s] 16%|█▌        | 97/600 [00:29<04:36,  1.82it/s] 16%|█▋        | 98/600 [00:29<03:55,  2.13it/s] 16%|█▋        | 99/600 [00:29<03:26,  2.42it/s] 17%|█▋        | 100/600 [00:30<03:06,  2.68it/s] 17%|█▋        | 101/600 [00:30<02:52,  2.90it/s] 17%|█▋        | 102/600 [00:30<02:42,  3.07it/s] 17%|█▋        | 103/600 [00:30<02:35,  3.20it/s] 17%|█▋        | 104/600 [00:31<02:30,  3.30it/s] 18%|█▊        | 105/600 [00:31<02:25,  3.39it/s] 18%|█▊        | 106/600 [00:32<02:55,  2.81it/s] 18%|█▊        | 107/600 [00:32<02:43,  3.01it/s] 18%|█▊        | 108/600 [00:32<02:35,  3.17it/s] 18%|█▊        | 109/600 [00:32<02:28,  3.30it/s] 18%|█▊        | 110/600 [00:33<02:24,  3.39it/s] 18%|█▊        | 111/600 [00:33<02:21,  3.45it/s] 19%|█▊        | 112/600 [00:33<02:19,  3.50it/s] 19%|█▉        | 113/600 [00:33<02:17,  3.55it/s] 19%|█▉        | 114/600 [00:34<02:16,  3.57it/s] 19%|█▉        | 115/600 [00:34<02:15,  3.58it/s] 19%|█▉        | 116/600 [00:34<02:38,  3.05it/s] 20%|█▉        | 117/600 [00:35<02:31,  3.20it/s] 20%|█▉        | 118/600 [00:35<02:25,  3.31it/s] 20%|█▉        | 119/600 [00:35<02:21,  3.40it/s] 20%|██        | 120/600 [00:36<02:18,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 13:32:46,340 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:32:46,340 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:32:46,340 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.33it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.97it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.97it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.99it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.23it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.86it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.65it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.46it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.55it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.58it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.79it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.85it/s][A
 15%|█▌        | 67/437 [00:01<00:12, 30.64it/s][A
 16%|█▋        | 72/437 [00:01<00:10, 33.90it/s][A
 18%|█▊        | 77/437 [00:01<00:09, 36.62it/s][A
 19%|█▉        | 82/437 [00:01<00:09, 38.82it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 40.54it/s][A
 21%|██        | 92/437 [00:02<00:08, 41.84it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 42.76it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.26it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.30it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.30it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.50it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.91it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.22it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.39it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.62it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.71it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.59it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.40it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.28it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.17it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.21it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.52it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.68it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.83it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.73it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.59it/s][A
 45%|████▌     | 197/437 [00:04<00:06, 34.99it/s][A
 46%|████▌     | 202/437 [00:04<00:06, 37.53it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 39.51it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 41.03it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 42.19it/s][A
 51%|█████     | 222/437 [00:05<00:05, 42.97it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.53it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.92it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.59it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.47it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.74it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.11it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.43it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.59it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.73it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.81it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.55it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.26it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.97it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.14it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.45it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.60it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.68it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.69it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.59it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.34it/s][A
 75%|███████▍  | 327/437 [00:07<00:03, 31.37it/s][A
 76%|███████▌  | 332/437 [00:07<00:03, 34.52it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 37.11it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 39.21it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 40.81it/s][A
 81%|████████  | 352/437 [00:08<00:02, 41.96it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 42.82it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.29it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.29it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.33it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.59it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.93it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 44.30it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.54it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.67it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.73it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.52it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.18it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.06it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.16it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.30it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.45it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.62it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.62it/s][A 20%|██        | 120/600 [00:46<02:18,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:32:58,172 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120
[INFO|configuration_utils.py:351] 2023-08-28 13:32:59,438 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:33:38,057 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:33:39,458 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:33:39,842 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120/special_tokens_map.json
 20%|██        | 121/600 [01:36<2:27:18, 18.45s/it] 20%|██        | 122/600 [01:37<1:43:59, 13.05s/it] 20%|██        | 123/600 [01:37<1:13:18,  9.22s/it] 21%|██        | 124/600 [01:37<51:52,  6.54s/it]   21%|██        | 125/600 [01:38<36:53,  4.66s/it] 21%|██        | 126/600 [01:38<26:26,  3.35s/it] 21%|██        | 127/600 [01:38<19:07,  2.43s/it] 21%|██▏       | 128/600 [01:39<14:01,  1.78s/it] 22%|██▏       | 129/600 [01:39<10:27,  1.33s/it] 22%|██▏       | 130/600 [01:39<07:57,  1.02s/it] 22%|██▏       | 131/600 [01:39<06:13,  1.26it/s] 22%|██▏       | 132/600 [01:40<05:00,  1.56it/s] 22%|██▏       | 133/600 [01:40<04:23,  1.77it/s] 22%|██▏       | 134/600 [01:40<03:42,  2.09it/s] 22%|██▎       | 135/600 [01:41<03:14,  2.39it/s] 23%|██▎       | 136/600 [01:41<02:55,  2.65it/s] 23%|██▎       | 137/600 [01:41<02:41,  2.87it/s] 23%|██▎       | 138/600 [01:41<02:31,  3.05it/s] 23%|██▎       | 139/600 [01:42<02:24,  3.19it/s] 23%|██▎       | 140/600 [01:42<02:19,  3.29it/s] 24%|██▎       | 141/600 [01:42<02:16,  3.37it/s] 24%|██▎       | 142/600 [01:43<02:13,  3.43it/s] 24%|██▍       | 143/600 [01:43<02:11,  3.47it/s] 24%|██▍       | 144/600 [01:43<02:24,  3.16it/s] 24%|██▍       | 145/600 [01:44<02:18,  3.28it/s] 24%|██▍       | 146/600 [01:44<02:15,  3.36it/s] 24%|██▍       | 147/600 [01:44<02:12,  3.42it/s] 25%|██▍       | 148/600 [01:44<02:10,  3.47it/s] 25%|██▍       | 149/600 [01:45<02:08,  3.50it/s] 25%|██▌       | 150/600 [01:45<02:07,  3.52it/s] 25%|██▌       | 151/600 [01:45<02:06,  3.54it/s] 25%|██▌       | 152/600 [01:45<02:05,  3.57it/s] 26%|██▌       | 153/600 [01:46<02:04,  3.59it/s] 26%|██▌       | 154/600 [01:46<02:03,  3.60it/s] 26%|██▌       | 155/600 [01:46<02:18,  3.20it/s] 26%|██▌       | 156/600 [01:47<02:13,  3.32it/s] 26%|██▌       | 157/600 [01:47<02:10,  3.40it/s] 26%|██▋       | 158/600 [01:47<02:07,  3.47it/s] 26%|██▋       | 159/600 [01:48<02:05,  3.51it/s] 27%|██▋       | 160/600 [01:48<02:03,  3.55it/s] 27%|██▋       | 161/600 [01:48<02:03,  3.57it/s] 27%|██▋       | 162/600 [01:48<02:02,  3.57it/s] 27%|██▋       | 163/600 [01:49<02:02,  3.57it/s] 27%|██▋       | 164/600 [01:49<02:02,  3.57it/s] 28%|██▊       | 165/600 [01:49<02:01,  3.57it/s] 28%|██▊       | 166/600 [01:50<02:20,  3.09it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 28%|██▊       | 167/600 [01:50<02:14,  3.22it/s] 28%|██▊       | 168/600 [01:50<02:10,  3.32it/s] 28%|██▊       | 169/600 [01:50<02:07,  3.39it/s] 28%|██▊       | 170/600 [01:51<02:04,  3.44it/s] 28%|██▊       | 171/600 [01:51<02:03,  3.48it/s] 29%|██▊       | 172/600 [01:51<02:01,  3.51it/s] 29%|██▉       | 173/600 [01:52<02:01,  3.53it/s] 29%|██▉       | 174/600 [01:52<02:00,  3.54it/s] 29%|██▉       | 175/600 [01:52<01:59,  3.55it/s] 29%|██▉       | 176/600 [01:52<01:59,  3.56it/s] 30%|██▉       | 177/600 [01:53<02:15,  3.12it/s] 30%|██▉       | 178/600 [01:53<02:10,  3.24it/s] 30%|██▉       | 179/600 [01:53<02:06,  3.34it/s] 30%|███       | 180/600 [01:54<02:03,  3.40it/s] 30%|███       | 181/600 [01:54<02:01,  3.45it/s] 30%|███       | 182/600 [01:54<01:59,  3.49it/s] 30%|███       | 183/600 [01:54<01:58,  3.52it/s] 31%|███       | 184/600 [01:55<01:57,  3.53it/s] 31%|███       | 185/600 [01:55<01:57,  3.55it/s] 31%|███       | 186/600 [01:55<01:55,  3.57it/s] 31%|███       | 187/600 [01:56<01:55,  3.59it/s] 31%|███▏      | 188/600 [01:56<02:11,  3.13it/s] 32%|███▏      | 189/600 [01:56<02:05,  3.27it/s] 32%|███▏      | 190/600 [01:57<02:01,  3.37it/s] 32%|███▏      | 191/600 [01:57<01:58,  3.45it/s] 32%|███▏      | 192/600 [01:57<01:56,  3.50it/s] 32%|███▏      | 193/600 [01:57<01:54,  3.54it/s] 32%|███▏      | 194/600 [01:58<01:53,  3.57it/s] 32%|███▎      | 195/600 [01:58<01:52,  3.59it/s] 33%|███▎      | 196/600 [01:58<01:52,  3.60it/s] 33%|███▎      | 197/600 [01:58<01:51,  3.61it/s] 33%|███▎      | 198/600 [01:59<01:51,  3.62it/s] 33%|███▎      | 199/600 [01:59<02:11,  3.06it/s] 33%|███▎      | 200/600 [01:59<02:04,  3.22it/s] 34%|███▎      | 201/600 [02:00<01:59,  3.33it/s] 34%|███▎      | 202/600 [02:00<01:56,  3.41it/s] 34%|███▍      | 203/600 [02:00<01:54,  3.48it/s] 34%|███▍      | 204/600 [02:01<01:52,  3.52it/s] 34%|███▍      | 205/600 [02:01<01:51,  3.55it/s] 34%|███▍      | 206/600 [02:01<01:50,  3.57it/s] 34%|███▍      | 207/600 [02:01<01:49,  3.59it/s] 35%|███▍      | 208/600 [02:02<01:49,  3.60it/s] 35%|███▍      | 209/600 [02:02<01:48,  3.61it/s] 35%|███▌      | 210/600 [02:02<02:04,  3.13it/s] 35%|███▌      | 211/600 [02:03<01:59,  3.26it/s] 35%|███▌      | 212/600 [02:03<01:55,  3.37it/s] 36%|███▌      | 213/600 [02:03<01:52,  3.44it/s] 36%|███▌      | 214/600 [02:03<01:50,  3.50it/s] 36%|███▌      | 215/600 [02:04<01:58,  3.24it/s] 36%|███▌      | 216/600 [02:04<01:54,  3.35it/s] 36%|███▌      | 217/600 [02:04<01:51,  3.43it/s] 36%|███▋      | 218/600 [02:05<01:49,  3.48it/s] 36%|███▋      | 219/600 [02:05<01:48,  3.53it/s] 37%|███▋      | 220/600 [02:05<01:46,  3.56it/s] 37%|███▋      | 221/600 [02:06<01:45,  3.58it/s] 37%|███▋      | 222/600 [02:06<01:45,  3.59it/s] 37%|███▋      | 223/600 [02:06<01:44,  3.60it/s] 37%|███▋      | 224/600 [02:06<01:44,  3.61it/s] 38%|███▊      | 225/600 [02:07<01:43,  3.61it/s] 38%|███▊      | 226/600 [02:07<01:57,  3.19it/s] 38%|███▊      | 227/600 [02:07<01:52,  3.31it/s] 38%|███▊      | 228/600 [02:08<01:49,  3.40it/s] 38%|███▊      | 229/600 [02:08<01:47,  3.46it/s] 38%|███▊      | 230/600 [02:08<01:45,  3.51it/s] 38%|███▊      | 231/600 [02:08<01:44,  3.54it/s] 39%|███▊      | 232/600 [02:09<01:43,  3.57it/s] 39%|███▉      | 233/600 [02:09<01:42,  3.58it/s] 39%|███▉      | 234/600 [02:09<01:41,  3.59it/s] 39%|███▉      | 235/600 [02:09<01:41,  3.59it/s] 39%|███▉      | 236/600 [02:10<01:41,  3.58it/s] 40%|███▉      | 237/600 [02:10<01:52,  3.24it/s] 40%|███▉      | 238/600 [02:10<01:48,  3.33it/s] 40%|███▉      | 239/600 [02:11<01:46,  3.40it/s] 40%|████      | 240/600 [02:11<01:44,  3.45it/s][INFO|trainer.py:2140] 2023-08-28 13:34:21,760 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:34:21,761 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:34:21,761 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.2699, 'eval_samples_per_second': 340.023, 'eval_steps_per_second': 42.552, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.91it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.90it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.08it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.07it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.22it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.77it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.46it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.39it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.42it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.64it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.83it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.92it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.94it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.37it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.58it/s][A
 19%|█▉        | 82/437 [00:01<00:10, 34.03it/s][A
 20%|█▉        | 87/437 [00:02<00:09, 36.72it/s][A
 21%|██        | 92/437 [00:02<00:08, 38.83it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 40.50it/s][A
 23%|██▎       | 102/437 [00:02<00:08, 41.78it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 42.76it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.45it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.74it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.57it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.62it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.81it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.02it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.43it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.55it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.70it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.81it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.66it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.34it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.20it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.26it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.41it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.60it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.70it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.80it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.79it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.68it/s][A
 49%|████▊     | 212/437 [00:04<00:06, 33.31it/s][A
 50%|████▉     | 217/437 [00:05<00:06, 36.13it/s][A
 51%|█████     | 222/437 [00:05<00:05, 38.44it/s][A
 52%|█████▏    | 227/437 [00:05<00:05, 40.12it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 41.49it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 42.52it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.23it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.61it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.43it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.49it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 43.81it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.07it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.33it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.54it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.75it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.83it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.70it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.36it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.11it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.04it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.24it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.46it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.67it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.74it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.80it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.71it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 36.56it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 38.78it/s][A
 81%|████████  | 352/437 [00:08<00:02, 40.52it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 41.74it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 42.67it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.30it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.86it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.04it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.78it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.66it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.92it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.22it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.44it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.56it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.68it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.68it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.44it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.98it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.80it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.04it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.04it/s][A 40%|████      | 240/600 [02:21<01:44,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:34:32,813 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 13:34:33,659 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:35:01,355 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:35:02,230 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:35:02,500 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240/special_tokens_map.json
 40%|████      | 241/600 [02:58<1:25:04, 14.22s/it] 40%|████      | 242/600 [02:58<1:00:02, 10.06s/it] 40%|████      | 243/600 [02:58<42:24,  7.13s/it]   41%|████      | 244/600 [02:59<30:05,  5.07s/it] 41%|████      | 245/600 [02:59<21:30,  3.63s/it] 41%|████      | 246/600 [02:59<15:30,  2.63s/it] 41%|████      | 247/600 [02:59<11:18,  1.92s/it] 41%|████▏     | 248/600 [03:00<08:23,  1.43s/it] 42%|████▏     | 249/600 [03:00<06:20,  1.08s/it] 42%|████▏     | 250/600 [03:00<04:54,  1.19it/s] 42%|████▏     | 251/600 [03:01<03:54,  1.49it/s] 42%|████▏     | 252/600 [03:01<03:12,  1.81it/s] 42%|████▏     | 253/600 [03:01<02:54,  1.98it/s] 42%|████▏     | 254/600 [03:02<02:30,  2.30it/s] 42%|████▎     | 255/600 [03:02<02:13,  2.58it/s] 43%|████▎     | 256/600 [03:02<02:01,  2.83it/s] 43%|████▎     | 257/600 [03:02<01:53,  3.03it/s] 43%|████▎     | 258/600 [03:03<01:47,  3.19it/s] 43%|████▎     | 259/600 [03:03<01:43,  3.31it/s] 43%|████▎     | 260/600 [03:03<01:40,  3.40it/s] 44%|████▎     | 261/600 [03:03<01:37,  3.47it/s] 44%|████▎     | 262/600 [03:04<01:36,  3.52it/s] 44%|████▍     | 263/600 [03:04<01:34,  3.55it/s] 44%|████▍     | 264/600 [03:04<01:43,  3.24it/s] 44%|████▍     | 265/600 [03:05<01:39,  3.35it/s] 44%|████▍     | 266/600 [03:05<01:37,  3.43it/s] 44%|████▍     | 267/600 [03:05<01:42,  3.24it/s] 45%|████▍     | 268/600 [03:06<01:39,  3.35it/s] 45%|████▍     | 269/600 [03:06<01:36,  3.43it/s] 45%|████▌     | 270/600 [03:06<01:34,  3.49it/s] 45%|████▌     | 271/600 [03:06<01:33,  3.54it/s] 45%|████▌     | 272/600 [03:07<01:32,  3.56it/s] 46%|████▌     | 273/600 [03:07<01:31,  3.58it/s] 46%|████▌     | 274/600 [03:07<01:30,  3.60it/s] 46%|████▌     | 275/600 [03:07<01:30,  3.61it/s] 46%|████▌     | 276/600 [03:08<01:29,  3.62it/s] 46%|████▌     | 277/600 [03:08<01:29,  3.62it/s] 46%|████▋     | 278/600 [03:08<01:36,  3.35it/s] 46%|████▋     | 279/600 [03:09<01:33,  3.43it/s] 47%|████▋     | 280/600 [03:09<01:31,  3.49it/s] 47%|████▋     | 281/600 [03:09<01:30,  3.53it/s] 47%|████▋     | 282/600 [03:09<01:29,  3.57it/s] 47%|████▋     | 283/600 [03:10<01:28,  3.58it/s] 47%|████▋     | 284/600 [03:10<01:27,  3.59it/s] 48%|████▊     | 285/600 [03:10<01:27,  3.61it/s] 48%|████▊     | 286/600 [03:11<01:27,  3.61it/s] 48%|████▊     | 287/600 [03:11<01:26,  3.62it/s] 48%|████▊     | 288/600 [03:11<01:26,  3.62it/s] 48%|████▊     | 289/600 [03:11<01:33,  3.34it/s] 48%|████▊     | 290/600 [03:12<01:30,  3.42it/s] 48%|████▊     | 291/600 [03:12<01:28,  3.48it/s] 49%|████▊     | 292/600 [03:12<01:27,  3.53it/s] 49%|████▉     | 293/600 [03:13<01:26,  3.55it/s] 49%|████▉     | 294/600 [03:13<01:25,  3.57it/s] 49%|████▉     | 295/600 [03:13<01:24,  3.59it/s] 49%|████▉     | 296/600 [03:13<01:24,  3.61it/s] 50%|████▉     | 297/600 [03:14<01:23,  3.61it/s] 50%|████▉     | 298/600 [03:14<01:23,  3.61it/s] 50%|████▉     | 299/600 [03:14<01:23,  3.62it/s] 50%|█████     | 300/600 [03:15<01:32,  3.26it/s] 50%|█████     | 301/600 [03:15<01:28,  3.36it/s] 50%|█████     | 302/600 [03:15<01:26,  3.44it/s] 50%|█████     | 303/600 [03:15<01:25,  3.49it/s] 51%|█████     | 304/600 [03:16<01:23,  3.53it/s] 51%|█████     | 305/600 [03:16<01:22,  3.56it/s] 51%|█████     | 306/600 [03:16<01:21,  3.59it/s] 51%|█████     | 307/600 [03:17<01:21,  3.60it/s] 51%|█████▏    | 308/600 [03:17<01:21,  3.60it/s] 52%|█████▏    | 309/600 [03:17<01:20,  3.60it/s] 52%|█████▏    | 310/600 [03:17<01:20,  3.61it/s] 52%|█████▏    | 311/600 [03:18<01:28,  3.25it/s] 52%|█████▏    | 312/600 [03:18<01:25,  3.36it/s] 52%|█████▏    | 313/600 [03:18<01:23,  3.44it/s] 52%|█████▏    | 314/600 [03:19<01:21,  3.49it/s] 52%|█████▎    | 315/600 [03:19<01:20,  3.53it/s] 53%|█████▎    | 316/600 [03:19<01:19,  3.57it/s] 53%|█████▎    | 317/600 [03:19<01:18,  3.58it/s] 53%|█████▎    | 318/600 [03:20<01:18,  3.59it/s] 53%|█████▎    | 319/600 [03:20<01:17,  3.61it/s] 53%|█████▎    | 320/600 [03:20<01:17,  3.61it/s] 54%|█████▎    | 321/600 [03:21<01:17,  3.62it/s] 54%|█████▎    | 322/600 [03:21<01:32,  3.01it/s] 54%|█████▍    | 323/600 [03:21<01:27,  3.17it/s] 54%|█████▍    | 324/600 [03:22<01:38,  2.79it/s] 54%|█████▍    | 325/600 [03:22<01:31,  2.99it/s] 54%|█████▍    | 326/600 [03:22<01:26,  3.16it/s] 55%|█████▍    | 327/600 [03:23<01:23,  3.29it/s] 55%|█████▍    | 328/600 [03:23<01:20,  3.38it/s] 55%|█████▍    | 329/600 [03:23<01:18,  3.45it/s] 55%|█████▌    | 330/600 [03:23<01:17,  3.50it/s] 55%|█████▌    | 331/600 [03:24<01:15,  3.54it/s] 55%|█████▌    | 332/600 [03:24<01:28,  3.04it/s] 56%|█████▌    | 333/600 [03:24<01:23,  3.20it/s] 56%|█████▌    | 334/600 [03:25<01:20,  3.31it/s] 56%|█████▌    | 335/600 [03:25<01:17,  3.40it/s] 56%|█████▌    | 336/600 [03:25<01:16,  3.46it/s] 56%|█████▌    | 337/600 [03:25<01:15,  3.51it/s] 56%|█████▋    | 338/600 [03:26<01:13,  3.54it/s] 56%|█████▋    | 339/600 [03:26<01:13,  3.57it/s] 57%|█████▋    | 340/600 [03:26<01:12,  3.58it/s] 57%|█████▋    | 341/600 [03:27<01:12,  3.59it/s] 57%|█████▋    | 342/600 [03:27<01:11,  3.60it/s] 57%|█████▋    | 343/600 [03:27<01:22,  3.12it/s] 57%|█████▋    | 344/600 [03:28<01:18,  3.26it/s] 57%|█████▊    | 345/600 [03:28<01:15,  3.36it/s] 58%|█████▊    | 346/600 [03:28<01:13,  3.44it/s] 58%|█████▊    | 347/600 [03:28<01:12,  3.49it/s] 58%|█████▊    | 348/600 [03:29<01:11,  3.53it/s] 58%|█████▊    | 349/600 [03:29<01:10,  3.56it/s] 58%|█████▊    | 350/600 [03:29<01:09,  3.58it/s] 58%|█████▊    | 351/600 [03:29<01:09,  3.60it/s] 59%|█████▊    | 352/600 [03:30<01:08,  3.60it/s] 59%|█████▉    | 353/600 [03:30<01:08,  3.61it/s] 59%|█████▉    | 354/600 [03:30<01:19,  3.09it/s] 59%|█████▉    | 355/600 [03:31<01:15,  3.23it/s] 59%|█████▉    | 356/600 [03:31<01:13,  3.34it/s] 60%|█████▉    | 357/600 [03:31<01:11,  3.41it/s] 60%|█████▉    | 358/600 [03:32<01:09,  3.48it/s] 60%|█████▉    | 359/600 [03:32<01:08,  3.52it/s] 60%|██████    | 360/600 [03:32<01:07,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 13:35:42,866 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:35:42,866 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:35:42,866 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.1661, 'eval_samples_per_second': 343.494, 'eval_steps_per_second': 42.986, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.18it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.04it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.08it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.99it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.33it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.81it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.64it/s][A
 10%|▉         | 42/437 [00:01<00:08, 44.40it/s][A
 11%|█         | 47/437 [00:01<00:11, 34.32it/s][A
 12%|█▏        | 52/437 [00:01<00:10, 37.06it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 39.18it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 40.81it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 41.94it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 42.84it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.47it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.73it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 43.56it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.54it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.87it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.14it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.46it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.58it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.71it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.70it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.59it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.17it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.04it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.17it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.41it/s][A
 35%|███▍      | 152/437 [00:03<00:11, 25.31it/s][A
 36%|███▌      | 157/437 [00:03<00:09, 29.14it/s][A
 37%|███▋      | 162/437 [00:04<00:08, 32.57it/s][A
 38%|███▊      | 167/437 [00:04<00:07, 35.49it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 37.93it/s][A
 41%|████      | 177/437 [00:04<00:06, 39.76it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 41.22it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 42.12it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 42.44it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 42.88it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.24it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 43.73it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 44.09it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.22it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.46it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.65it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.46it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.93it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.24it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.15it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 44.33it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.55it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.74it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.71it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.72it/s][A
 63%|██████▎   | 277/437 [00:06<00:04, 35.75it/s][A
 65%|██████▍   | 282/437 [00:06<00:04, 38.17it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 39.99it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 41.38it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 42.39it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 43.09it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.72it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.86it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.66it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.60it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.83it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.12it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 44.38it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 44.54it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.69it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.77it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.58it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.25it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.04it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.12it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.34it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 44.45it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 44.62it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.75it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.79it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.51it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 34.94it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 37.48it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 39.44it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 40.89it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 42.10it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 42.99it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.52it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 43.52it/s][A 60%|██████    | 360/600 [03:43<01:07,  3.55it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:35:54,032 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360
[INFO|configuration_utils.py:351] 2023-08-28 13:35:55,022 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:36:31,938 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:36:33,272 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:36:33,561 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360/special_tokens_map.json
 60%|██████    | 361/600 [04:29<1:08:43, 17.25s/it] 60%|██████    | 362/600 [04:29<48:22, 12.20s/it]   60%|██████    | 363/600 [04:30<34:03,  8.62s/it] 61%|██████    | 364/600 [04:30<24:03,  6.12s/it] 61%|██████    | 365/600 [04:30<17:06,  4.37s/it] 61%|██████    | 366/600 [04:30<12:14,  3.14s/it] 61%|██████    | 367/600 [04:31<08:51,  2.28s/it] 61%|██████▏   | 368/600 [04:31<06:29,  1.68s/it] 62%|██████▏   | 369/600 [04:31<04:51,  1.26s/it] 62%|██████▏   | 370/600 [04:32<03:42,  1.04it/s] 62%|██████▏   | 371/600 [04:32<02:54,  1.32it/s] 62%|██████▏   | 372/600 [04:32<02:20,  1.62it/s] 62%|██████▏   | 373/600 [04:33<02:03,  1.84it/s] 62%|██████▏   | 374/600 [04:33<01:45,  2.15it/s] 62%|██████▎   | 375/600 [04:33<01:32,  2.44it/s] 63%|██████▎   | 376/600 [04:33<01:22,  2.70it/s] 63%|██████▎   | 377/600 [04:34<01:16,  2.92it/s] 63%|██████▎   | 378/600 [04:34<01:11,  3.10it/s] 63%|██████▎   | 379/600 [04:34<01:08,  3.24it/s] 63%|██████▎   | 380/600 [04:34<01:05,  3.35it/s] 64%|██████▎   | 381/600 [04:35<01:03,  3.43it/s] 64%|██████▎   | 382/600 [04:35<01:02,  3.49it/s] 64%|██████▍   | 383/600 [04:35<01:01,  3.53it/s] 64%|██████▍   | 384/600 [04:36<01:08,  3.15it/s] 64%|██████▍   | 385/600 [04:36<01:05,  3.28it/s] 64%|██████▍   | 386/600 [04:36<01:03,  3.38it/s] 64%|██████▍   | 387/600 [04:37<01:01,  3.45it/s] 65%|██████▍   | 388/600 [04:37<01:00,  3.50it/s] 65%|██████▍   | 389/600 [04:37<00:59,  3.54it/s] 65%|██████▌   | 390/600 [04:37<00:58,  3.57it/s] 65%|██████▌   | 391/600 [04:38<01:08,  3.05it/s] 65%|██████▌   | 392/600 [04:38<01:04,  3.21it/s] 66%|██████▌   | 393/600 [04:38<01:02,  3.32it/s] 66%|██████▌   | 394/600 [04:39<01:00,  3.41it/s] 66%|██████▌   | 395/600 [04:39<00:59,  3.47it/s] 66%|██████▌   | 396/600 [04:39<00:57,  3.52it/s] 66%|██████▌   | 397/600 [04:39<00:57,  3.55it/s] 66%|██████▋   | 398/600 [04:40<00:56,  3.57it/s] 66%|██████▋   | 399/600 [04:40<00:55,  3.59it/s] 67%|██████▋   | 400/600 [04:40<00:55,  3.60it/s] 67%|██████▋   | 401/600 [04:41<00:55,  3.61it/s] 67%|██████▋   | 402/600 [04:41<01:03,  3.11it/s] 67%|██████▋   | 403/600 [04:41<01:00,  3.25it/s] 67%|██████▋   | 404/600 [04:42<00:58,  3.35it/s] 68%|██████▊   | 405/600 [04:42<00:56,  3.43it/s] 68%|██████▊   | 406/600 [04:42<00:55,  3.49it/s] 68%|██████▊   | 407/600 [04:42<00:54,  3.53it/s] 68%|██████▊   | 408/600 [04:43<00:54,  3.56it/s] 68%|██████▊   | 409/600 [04:43<00:53,  3.57it/s] 68%|██████▊   | 410/600 [04:43<00:52,  3.59it/s] 68%|██████▊   | 411/600 [04:43<00:52,  3.60it/s] 69%|██████▊   | 412/600 [04:44<00:52,  3.61it/s] 69%|██████▉   | 413/600 [04:44<00:55,  3.34it/s] 69%|██████▉   | 414/600 [04:44<00:54,  3.42it/s] 69%|██████▉   | 415/600 [04:45<00:53,  3.48it/s] 69%|██████▉   | 416/600 [04:45<00:52,  3.52it/s] 70%|██████▉   | 417/600 [04:45<00:51,  3.55it/s] 70%|██████▉   | 418/600 [04:45<00:50,  3.57it/s] 70%|██████▉   | 419/600 [04:46<00:50,  3.59it/s] 70%|███████   | 420/600 [04:46<00:50,  3.60it/s] 70%|███████   | 421/600 [04:46<00:49,  3.61it/s] 70%|███████   | 422/600 [04:47<00:49,  3.61it/s] 70%|███████   | 423/600 [04:47<00:49,  3.61it/s] 71%|███████   | 424/600 [04:47<00:52,  3.34it/s] 71%|███████   | 425/600 [04:47<00:51,  3.43it/s] 71%|███████   | 426/600 [04:48<00:49,  3.48it/s] 71%|███████   | 427/600 [04:48<00:49,  3.52it/s] 71%|███████▏  | 428/600 [04:48<00:48,  3.55it/s] 72%|███████▏  | 429/600 [04:49<00:47,  3.58it/s] 72%|███████▏  | 430/600 [04:49<00:47,  3.59it/s] 72%|███████▏  | 431/600 [04:49<00:47,  3.60it/s] 72%|███████▏  | 432/600 [04:49<00:46,  3.61it/s] 72%|███████▏  | 433/600 [04:50<00:46,  3.61it/s] 72%|███████▏  | 434/600 [04:50<00:45,  3.62it/s] 72%|███████▎  | 435/600 [04:50<00:51,  3.20it/s] 73%|███████▎  | 436/600 [04:51<00:49,  3.31it/s] 73%|███████▎  | 437/600 [04:51<00:48,  3.40it/s] 73%|███████▎  | 438/600 [04:51<00:46,  3.46it/s] 73%|███████▎  | 439/600 [04:51<00:45,  3.51it/s] 73%|███████▎  | 440/600 [04:52<00:45,  3.54it/s] 74%|███████▎  | 441/600 [04:52<00:44,  3.56it/s] 74%|███████▎  | 442/600 [04:52<00:44,  3.58it/s] 74%|███████▍  | 443/600 [04:53<00:43,  3.59it/s] 74%|███████▍  | 444/600 [04:53<00:43,  3.60it/s] 74%|███████▍  | 445/600 [04:53<00:42,  3.61it/s] 74%|███████▍  | 446/600 [04:54<00:49,  3.13it/s] 74%|███████▍  | 447/600 [04:54<00:46,  3.26it/s] 75%|███████▍  | 448/600 [04:54<00:45,  3.36it/s] 75%|███████▍  | 449/600 [04:54<00:43,  3.44it/s] 75%|███████▌  | 450/600 [04:55<00:43,  3.48it/s] 75%|███████▌  | 451/600 [04:55<00:42,  3.53it/s] 75%|███████▌  | 452/600 [04:55<00:41,  3.55it/s] 76%|███████▌  | 453/600 [04:55<00:41,  3.57it/s] 76%|███████▌  | 454/600 [04:56<00:40,  3.58it/s] 76%|███████▌  | 455/600 [04:56<00:40,  3.60it/s] 76%|███████▌  | 456/600 [04:56<00:39,  3.61it/s] 76%|███████▌  | 457/600 [04:57<00:43,  3.26it/s] 76%|███████▋  | 458/600 [04:57<00:42,  3.36it/s] 76%|███████▋  | 459/600 [04:57<00:41,  3.43it/s] 77%|███████▋  | 460/600 [04:57<00:40,  3.49it/s] 77%|███████▋  | 461/600 [04:58<00:39,  3.53it/s] 77%|███████▋  | 462/600 [04:58<00:38,  3.55it/s] 77%|███████▋  | 463/600 [04:58<00:38,  3.57it/s] 77%|███████▋  | 464/600 [04:59<00:37,  3.58it/s] 78%|███████▊  | 465/600 [04:59<00:37,  3.59it/s] 78%|███████▊  | 466/600 [04:59<00:37,  3.60it/s] 78%|███████▊  | 467/600 [04:59<00:36,  3.61it/s] 78%|███████▊  | 468/600 [05:00<00:40,  3.28it/s] 78%|███████▊  | 469/600 [05:00<00:38,  3.37it/s] 78%|███████▊  | 470/600 [05:00<00:37,  3.44it/s] 78%|███████▊  | 471/600 [05:01<00:36,  3.49it/s] 79%|███████▊  | 472/600 [05:01<00:36,  3.53it/s] 79%|███████▉  | 473/600 [05:01<00:35,  3.56it/s] 79%|███████▉  | 474/600 [05:01<00:35,  3.57it/s] 79%|███████▉  | 475/600 [05:02<00:34,  3.58it/s] 79%|███████▉  | 476/600 [05:02<00:34,  3.59it/s] 80%|███████▉  | 477/600 [05:02<00:34,  3.59it/s] 80%|███████▉  | 478/600 [05:03<00:33,  3.60it/s] 80%|███████▉  | 479/600 [05:03<00:37,  3.27it/s] 80%|████████  | 480/600 [05:03<00:35,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 13:37:13,968 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:37:13,968 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:37:13,968 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.4239, 'eval_samples_per_second': 335.001, 'eval_steps_per_second': 41.923, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.49it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.50it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.09it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.20it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.36it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.71it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.39it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.00it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.14it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.23it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.44it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.52it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.59it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.62it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.36it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.13it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.08it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.09it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.20it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.45it/s][A
 24%|██▍       | 107/437 [00:02<00:08, 37.73it/s][A
 26%|██▌       | 112/437 [00:02<00:08, 39.66it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 41.17it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 42.23it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 42.97it/s][A
 30%|███       | 132/437 [00:03<00:07, 43.50it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.77it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.82it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.61it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.55it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.72it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.01it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.29it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.42it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.48it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.55it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.37it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.06it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.92it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.99it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.21it/s][A
 49%|████▊     | 212/437 [00:05<00:07, 29.74it/s][A
 50%|████▉     | 217/437 [00:05<00:06, 32.97it/s][A
 51%|█████     | 222/437 [00:05<00:05, 35.85it/s][A
 52%|█████▏    | 227/437 [00:05<00:05, 38.24it/s][A
 53%|█████▎    | 232/437 [00:05<00:05, 40.07it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 41.46it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 42.56it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.08it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.11it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 43.14it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.31it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.63it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.07it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.35it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.57it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.70it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.74it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.39it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.11it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.05it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.19it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.40it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.58it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.69it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.79it/s][A
 77%|███████▋  | 337/437 [00:07<00:03, 32.71it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 35.69it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 38.03it/s][A
 81%|████████  | 352/437 [00:08<00:02, 39.89it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 41.26it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 42.36it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.10it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.48it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.31it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.39it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.76it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.11it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.37it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.51it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.62it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.67it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.34it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.00it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.98it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.09it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.28it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.28it/s][A 80%|████████  | 480/600 [05:14<00:35,  3.36it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:37:24,815 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480
[INFO|configuration_utils.py:351] 2023-08-28 13:37:25,606 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:37:54,766 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:37:55,868 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:37:56,156 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480/special_tokens_map.json
 80%|████████  | 481/600 [05:51<29:03, 14.66s/it] 80%|████████  | 482/600 [05:52<20:24, 10.38s/it] 80%|████████  | 483/600 [05:52<14:19,  7.35s/it] 81%|████████  | 484/600 [05:52<10:06,  5.23s/it] 81%|████████  | 485/600 [05:53<07:10,  3.74s/it] 81%|████████  | 486/600 [05:53<05:08,  2.70s/it] 81%|████████  | 487/600 [05:53<03:43,  1.98s/it] 81%|████████▏ | 488/600 [05:53<02:44,  1.47s/it] 82%|████████▏ | 489/600 [05:54<02:03,  1.11s/it] 82%|████████▏ | 490/600 [05:54<01:34,  1.16it/s] 82%|████████▏ | 491/600 [05:54<01:14,  1.45it/s] 82%|████████▏ | 492/600 [05:55<01:01,  1.77it/s] 82%|████████▏ | 493/600 [05:55<00:55,  1.92it/s] 82%|████████▏ | 494/600 [05:55<00:47,  2.22it/s] 82%|████████▎ | 495/600 [05:56<00:41,  2.51it/s] 83%|████████▎ | 496/600 [05:56<00:37,  2.76it/s] 83%|████████▎ | 497/600 [05:56<00:34,  2.96it/s] 83%|████████▎ | 498/600 [05:56<00:32,  3.12it/s] 83%|████████▎ | 499/600 [05:57<00:31,  3.25it/s] 83%|████████▎ | 500/600 [05:57<00:29,  3.35it/s]                                                  83%|████████▎ | 500/600 [05:57<00:29,  3.35it/s] 84%|████████▎ | 501/600 [05:57<00:28,  3.43it/s] 84%|████████▎ | 502/600 [05:57<00:28,  3.49it/s] 84%|████████▍ | 503/600 [05:58<00:27,  3.54it/s] 84%|████████▍ | 504/600 [05:58<00:30,  3.17it/s] 84%|████████▍ | 505/600 [05:58<00:28,  3.30it/s] 84%|████████▍ | 506/600 [05:59<00:27,  3.39it/s] 84%|████████▍ | 507/600 [05:59<00:26,  3.46it/s] 85%|████████▍ | 508/600 [05:59<00:26,  3.51it/s] 85%|████████▍ | 509/600 [06:00<00:25,  3.55it/s] 85%|████████▌ | 510/600 [06:00<00:25,  3.57it/s] 85%|████████▌ | 511/600 [06:00<00:24,  3.59it/s] 85%|████████▌ | 512/600 [06:00<00:24,  3.60it/s] 86%|████████▌ | 513/600 [06:01<00:24,  3.61it/s] 86%|████████▌ | 514/600 [06:01<00:23,  3.61it/s] 86%|████████▌ | 515/600 [06:01<00:26,  3.23it/s] 86%|████████▌ | 516/600 [06:02<00:25,  3.34it/s] 86%|████████▌ | 517/600 [06:02<00:24,  3.42it/s] 86%|████████▋ | 518/600 [06:02<00:23,  3.48it/s] 86%|████████▋ | 519/600 [06:02<00:22,  3.52it/s] 87%|████████▋ | 520/600 [06:03<00:22,  3.55it/s] 87%|████████▋ | 521/600 [06:03<00:22,  3.58it/s] 87%|████████▋ | 522/600 [06:03<00:21,  3.59it/s] 87%|████████▋ | 523/600 [06:03<00:21,  3.60it/s] 87%|████████▋ | 524/600 [06:04<00:21,  3.61it/s] 88%|████████▊ | 525/600 [06:04<00:20,  3.62it/s] 88%|████████▊ | 526/600 [06:04<00:23,  3.20it/s] 88%|████████▊ | 527/600 [06:05<00:21,  3.32it/s] 88%|████████▊ | 528/600 [06:05<00:21,  3.41it/s] 88%|████████▊ | 529/600 [06:05<00:20,  3.47it/s] 88%|████████▊ | 530/600 [06:06<00:19,  3.52it/s] 88%|████████▊ | 531/600 [06:06<00:19,  3.55it/s] 89%|████████▊ | 532/600 [06:06<00:19,  3.58it/s] 89%|████████▉ | 533/600 [06:06<00:18,  3.59it/s] 89%|████████▉ | 534/600 [06:07<00:18,  3.60it/s] 89%|████████▉ | 535/600 [06:07<00:18,  3.60it/s] 89%|████████▉ | 536/600 [06:07<00:17,  3.61it/s] 90%|████████▉ | 537/600 [06:08<00:19,  3.30it/s] 90%|████████▉ | 538/600 [06:08<00:18,  3.40it/s] 90%|████████▉ | 539/600 [06:08<00:17,  3.46it/s] 90%|█████████ | 540/600 [06:08<00:17,  3.51it/s] 90%|█████████ | 541/600 [06:09<00:16,  3.54it/s] 90%|█████████ | 542/600 [06:09<00:16,  3.57it/s] 90%|█████████ | 543/600 [06:09<00:15,  3.57it/s] 91%|█████████ | 544/600 [06:09<00:15,  3.57it/s] 91%|█████████ | 545/600 [06:10<00:17,  3.17it/s] 91%|█████████ | 546/600 [06:10<00:16,  3.28it/s] 91%|█████████ | 547/600 [06:10<00:15,  3.36it/s] 91%|█████████▏| 548/600 [06:11<00:15,  3.42it/s] 92%|█████████▏| 549/600 [06:11<00:14,  3.47it/s] 92%|█████████▏| 550/600 [06:11<00:14,  3.50it/s] 92%|█████████▏| 551/600 [06:12<00:13,  3.52it/s] 92%|█████████▏| 552/600 [06:12<00:13,  3.55it/s] 92%|█████████▏| 553/600 [06:12<00:13,  3.58it/s] 92%|█████████▏| 554/600 [06:12<00:12,  3.59it/s] 92%|█████████▎| 555/600 [06:13<00:12,  3.60it/s] 93%|█████████▎| 556/600 [06:13<00:13,  3.22it/s] 93%|█████████▎| 557/600 [06:13<00:12,  3.33it/s] 93%|█████████▎| 558/600 [06:14<00:12,  3.41it/s] 93%|█████████▎| 559/600 [06:14<00:11,  3.48it/s] 93%|█████████▎| 560/600 [06:14<00:11,  3.52it/s] 94%|█████████▎| 561/600 [06:14<00:10,  3.55it/s] 94%|█████████▎| 562/600 [06:15<00:10,  3.57it/s] 94%|█████████▍| 563/600 [06:15<00:10,  3.58it/s] 94%|█████████▍| 564/600 [06:15<00:10,  3.60it/s] 94%|█████████▍| 565/600 [06:16<00:09,  3.61it/s] 94%|█████████▍| 566/600 [06:16<00:09,  3.61it/s] 94%|█████████▍| 567/600 [06:16<00:09,  3.38it/s] 95%|█████████▍| 568/600 [06:16<00:09,  3.44it/s] 95%|█████████▍| 569/600 [06:17<00:08,  3.49it/s] 95%|█████████▌| 570/600 [06:17<00:08,  3.52it/s] 95%|█████████▌| 571/600 [06:17<00:08,  3.56it/s] 95%|█████████▌| 572/600 [06:18<00:07,  3.58it/s] 96%|█████████▌| 573/600 [06:18<00:07,  3.59it/s] 96%|█████████▌| 574/600 [06:18<00:07,  3.60it/s] 96%|█████████▌| 575/600 [06:18<00:06,  3.60it/s] 96%|█████████▌| 576/600 [06:19<00:06,  3.61it/s] 96%|█████████▌| 577/600 [06:19<00:06,  3.62it/s] 96%|█████████▋| 578/600 [06:19<00:06,  3.27it/s] 96%|█████████▋| 579/600 [06:20<00:06,  3.37it/s] 97%|█████████▋| 580/600 [06:20<00:05,  3.43it/s] 97%|█████████▋| 581/600 [06:20<00:05,  3.49it/s] 97%|█████████▋| 582/600 [06:20<00:05,  3.53it/s] 97%|█████████▋| 583/600 [06:21<00:04,  3.55it/s] 97%|█████████▋| 584/600 [06:21<00:04,  3.58it/s] 98%|█████████▊| 585/600 [06:21<00:04,  3.58it/s] 98%|█████████▊| 586/600 [06:22<00:03,  3.59it/s] 98%|█████████▊| 587/600 [06:22<00:03,  3.60it/s] 98%|█████████▊| 588/600 [06:22<00:03,  3.61it/s] 98%|█████████▊| 589/600 [06:22<00:03,  3.14it/s] 98%|█████████▊| 590/600 [06:23<00:03,  3.26it/s] 98%|█████████▊| 591/600 [06:23<00:02,  3.36it/s] 99%|█████████▊| 592/600 [06:23<00:02,  3.44it/s] 99%|█████████▉| 593/600 [06:24<00:02,  3.49it/s] 99%|█████████▉| 594/600 [06:24<00:01,  3.53it/s] 99%|█████████▉| 595/600 [06:24<00:01,  3.56it/s] 99%|█████████▉| 596/600 [06:24<00:01,  3.58it/s]100%|█████████▉| 597/600 [06:25<00:00,  3.59it/s]100%|█████████▉| 598/600 [06:25<00:00,  3.60it/s]100%|█████████▉| 599/600 [06:25<00:00,  3.61it/s]100%|██████████| 600/600 [06:26<00:00,  3.13it/s][INFO|trainer.py:2140] 2023-08-28 13:38:36,387 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:38:36,387 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:38:36,387 >>   Batch size = 8
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.2604, 'eval_samples_per_second': 340.336, 'eval_steps_per_second': 42.591, 'epoch': 4.0}
{'loss': nan, 'learning_rate': 1.6625e-05, 'epoch': 4.17}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.88it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.92it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.25it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.40it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.84it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.06it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.45it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.29it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.42it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.63it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.72it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.82it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.96it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.80it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.46it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.25it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.14it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.85it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.54it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.66it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.84it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.83it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.73it/s][A
 28%|██▊       | 122/437 [00:02<00:08, 35.83it/s][A
 29%|██▉       | 127/437 [00:02<00:08, 38.22it/s][A
 30%|███       | 132/437 [00:03<00:07, 40.01it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 41.46it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 42.45it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.22it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.74it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.97it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.79it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.64it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.71it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.05it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.28it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.54it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.69it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.78it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.72it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.34it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.08it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.08it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.25it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.35it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.63it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.69it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.85it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.62it/s][A
 58%|█████▊    | 252/437 [00:05<00:05, 34.53it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 37.15it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 39.08it/s][A
 61%|██████    | 267/437 [00:06<00:04, 40.71it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 41.98it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 42.86it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.58it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 43.82it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.47it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.60it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.83it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.08it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.38it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.58it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.72it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.19it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.69it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.27it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.15it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.19it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.46it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.61it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.69it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.85it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.76it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.59it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 35.33it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 37.77it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 39.66it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 41.17it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 42.24it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.07it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.65it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 43.89it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.56it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.64it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.82it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.16it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.16it/s][A100%|██████████| 600/600 [06:36<00:00,  3.13it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 13:38:47,100 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600
[INFO|configuration_utils.py:351] 2023-08-28 13:38:47,905 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:39:13,249 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:39:14,191 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:39:14,555 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 13:39:21,588 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 13:39:21,880 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120 (score: 1.1054370403289795).
                                                 100%|██████████| 600/600 [07:51<00:00,  3.13it/s]100%|██████████| 600/600 [07:51<00:00,  1.27it/s]
[INFO|trainer.py:1894] 2023-08-28 13:40:02,312 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 13:40:02,724 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 13:40:33,265 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 13:40:34,351 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 13:40:34,690 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:40:38,010 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:38,165 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:38,165 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:38,165 >>   train_runtime            = 0:07:51.52
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:38,165 >>   train_samples            =       7700
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:38,166 >>   train_samples_per_second =      81.65
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:38,166 >>   train_steps_per_second   =      1.272
{'eval_loss': 1.1054370403289795, 'eval_runtime': 10.091, 'eval_samples_per_second': 346.052, 'eval_steps_per_second': 43.306, 'epoch': 5.0}
{'train_runtime': 471.5251, 'train_samples_per_second': 81.65, 'train_steps_per_second': 1.272, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 13:40:39 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 13:40:39,692 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 13:40:39,692 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 13:40:39,692 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:08, 49.79it/s]  3%|▎         | 11/437 [00:00<00:09, 47.14it/s]  4%|▎         | 16/437 [00:00<00:09, 46.22it/s]  5%|▍         | 21/437 [00:00<00:09, 45.82it/s]  6%|▌         | 26/437 [00:00<00:09, 45.66it/s]  7%|▋         | 31/437 [00:00<00:08, 45.48it/s]  8%|▊         | 36/437 [00:00<00:08, 45.41it/s]  9%|▉         | 41/437 [00:00<00:08, 45.27it/s] 11%|█         | 46/437 [00:01<00:08, 44.68it/s] 12%|█▏        | 51/437 [00:01<00:08, 44.44it/s] 13%|█▎        | 56/437 [00:01<00:08, 44.52it/s] 14%|█▍        | 61/437 [00:01<00:08, 44.65it/s] 15%|█▌        | 66/437 [00:01<00:10, 34.67it/s] 16%|█▌        | 71/437 [00:01<00:09, 37.27it/s] 17%|█▋        | 76/437 [00:01<00:09, 39.41it/s] 19%|█▊        | 81/437 [00:01<00:08, 41.14it/s] 20%|█▉        | 86/437 [00:02<00:08, 42.35it/s] 21%|██        | 91/437 [00:02<00:07, 43.28it/s] 22%|██▏       | 96/437 [00:02<00:07, 43.92it/s] 23%|██▎       | 101/437 [00:02<00:07, 44.25it/s] 24%|██▍       | 106/437 [00:02<00:07, 44.03it/s] 25%|██▌       | 111/437 [00:02<00:07, 43.93it/s] 27%|██▋       | 116/437 [00:02<00:07, 43.89it/s] 28%|██▊       | 121/437 [00:02<00:07, 44.24it/s] 29%|██▉       | 126/437 [00:02<00:06, 44.63it/s] 30%|██▉       | 131/437 [00:03<00:06, 44.78it/s] 31%|███       | 136/437 [00:03<00:06, 44.92it/s] 32%|███▏      | 141/437 [00:03<00:06, 45.23it/s] 33%|███▎      | 146/437 [00:03<00:06, 45.18it/s] 35%|███▍      | 151/437 [00:03<00:06, 44.79it/s] 36%|███▌      | 156/437 [00:03<00:06, 44.56it/s] 37%|███▋      | 161/437 [00:03<00:06, 44.41it/s] 38%|███▊      | 166/437 [00:03<00:06, 44.52it/s] 39%|███▉      | 171/437 [00:03<00:05, 44.75it/s] 40%|████      | 176/437 [00:04<00:05, 44.89it/s] 41%|████▏     | 181/437 [00:04<00:05, 45.12it/s] 43%|████▎     | 186/437 [00:04<00:05, 45.25it/s] 44%|████▎     | 191/437 [00:04<00:05, 45.19it/s] 45%|████▍     | 196/437 [00:04<00:06, 36.81it/s] 46%|████▌     | 201/437 [00:04<00:06, 38.96it/s] 47%|████▋     | 206/437 [00:04<00:05, 40.69it/s] 48%|████▊     | 211/437 [00:04<00:05, 41.92it/s] 49%|████▉     | 216/437 [00:04<00:05, 42.98it/s] 51%|█████     | 221/437 [00:05<00:04, 43.72it/s] 52%|█████▏    | 226/437 [00:05<00:04, 44.23it/s] 53%|█████▎    | 231/437 [00:05<00:04, 44.51it/s] 54%|█████▍    | 236/437 [00:05<00:04, 44.17it/s] 55%|█████▌    | 241/437 [00:05<00:04, 44.09it/s] 56%|█████▋    | 246/437 [00:05<00:04, 44.24it/s] 57%|█████▋    | 251/437 [00:05<00:04, 44.42it/s] 59%|█████▊    | 256/437 [00:05<00:04, 44.61it/s] 60%|█████▉    | 261/437 [00:05<00:03, 44.84it/s] 61%|██████    | 266/437 [00:06<00:03, 44.88it/s] 62%|██████▏   | 271/437 [00:06<00:03, 45.11it/s] 63%|██████▎   | 276/437 [00:06<00:03, 45.00it/s] 64%|██████▍   | 281/437 [00:06<00:03, 44.65it/s] 65%|██████▌   | 286/437 [00:06<00:03, 44.60it/s] 67%|██████▋   | 291/437 [00:06<00:03, 44.52it/s] 68%|██████▊   | 296/437 [00:06<00:03, 44.68it/s] 69%|██████▉   | 301/437 [00:06<00:03, 44.83it/s] 70%|███████   | 306/437 [00:06<00:02, 44.92it/s] 71%|███████   | 311/437 [00:07<00:02, 45.03it/s] 72%|███████▏  | 316/437 [00:07<00:02, 45.06it/s] 73%|███████▎  | 321/437 [00:07<00:02, 44.92it/s] 75%|███████▍  | 326/437 [00:07<00:02, 44.71it/s] 76%|███████▌  | 331/437 [00:07<00:02, 36.03it/s] 77%|███████▋  | 336/437 [00:07<00:02, 38.43it/s] 78%|███████▊  | 341/437 [00:07<00:02, 40.14it/s] 79%|███████▉  | 346/437 [00:07<00:02, 41.64it/s] 80%|████████  | 351/437 [00:08<00:02, 42.69it/s] 81%|████████▏ | 356/437 [00:08<00:01, 43.44it/s] 83%|████████▎ | 361/437 [00:08<00:01, 44.12it/s] 84%|████████▍ | 366/437 [00:08<00:01, 44.36it/s] 85%|████████▍ | 371/437 [00:08<00:01, 44.15it/s] 86%|████████▌ | 376/437 [00:08<00:01, 44.05it/s] 87%|████████▋ | 381/437 [00:08<00:01, 44.13it/s] 88%|████████▊ | 386/437 [00:08<00:01, 44.50it/s] 89%|████████▉ | 391/437 [00:08<00:01, 44.64it/s] 91%|█████████ | 396/437 [00:09<00:00, 44.93it/s] 92%|█████████▏| 401/437 [00:09<00:00, 45.12it/s] 93%|█████████▎| 406/437 [00:09<00:00, 45.29it/s] 94%|█████████▍| 411/437 [00:09<00:00, 44.98it/s] 95%|█████████▌| 416/437 [00:09<00:00, 44.79it/s] 96%|█████████▋| 421/437 [00:09<00:00, 44.62it/s] 97%|█████████▋| 426/437 [00:09<00:00, 44.42it/s] 99%|█████████▊| 431/437 [00:09<00:00, 44.67it/s]100%|█████████▉| 436/437 [00:09<00:00, 44.85it/s]100%|██████████| 437/437 [00:10<00:00, 43.70it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 13:40:49,730 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:49,731 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:49,731 >>   eval_loss               =     1.1054
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:49,731 >>   eval_runtime            = 0:00:10.03
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:49,731 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:49,731 >>   eval_samples_per_second =    347.865
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:49,731 >>   eval_steps_per_second   =     43.533
[INFO|trainer_pt_utils.py:913] 2023-08-28 13:40:49,731 >>   perplexity              =     3.0205
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:30,384 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:30,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:30,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:30,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:30,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:41:31,222 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:41:31,223 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:41:31,536 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:41:32,759 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:41:32,759 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:35,027 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:35,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:35,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:35,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:41:35,093 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:41:35,896 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:41:35,897 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:41:36,286 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:41:36,614 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:41:36,614 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-480
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-120
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-600
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-240
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/generator/iter5/model/checkpoint-360
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.66it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.59it/s]Extractor Predicting: 9it [00:05,  1.64it/s]Extractor Predicting: 10it [00:06,  1.66it/s]Extractor Predicting: 11it [00:06,  1.57it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.61it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.62it/s]Extractor Predicting: 16it [00:10,  1.56it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:13,  1.61it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.66it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:15,  1.69it/s]Extractor Predicting: 26it [00:16,  1.67it/s]Extractor Predicting: 27it [00:16,  1.60it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:18,  1.61it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:20,  1.60it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:21,  1.62it/s]Extractor Predicting: 36it [00:22,  1.63it/s]Extractor Predicting: 37it [00:23,  1.52it/s]Extractor Predicting: 38it [00:23,  1.57it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.66it/s]Extractor Predicting: 42it [00:26,  1.55it/s]Extractor Predicting: 43it [00:26,  1.60it/s]Extractor Predicting: 44it [00:27,  1.63it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:29,  1.59it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:31,  1.66it/s]Extractor Predicting: 51it [00:31,  1.63it/s]Extractor Predicting: 52it [00:32,  1.53it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:33,  1.59it/s]Extractor Predicting: 55it [00:34,  1.61it/s]Extractor Predicting: 56it [00:34,  1.64it/s]Extractor Predicting: 57it [00:35,  1.55it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:37,  1.62it/s]Extractor Predicting: 61it [00:37,  1.61it/s]Extractor Predicting: 62it [00:38,  1.54it/s]Extractor Predicting: 63it [00:39,  1.47it/s]Extractor Predicting: 64it [00:40,  1.47it/s]Extractor Predicting: 65it [00:40,  1.51it/s]Extractor Predicting: 66it [00:41,  1.51it/s]Extractor Predicting: 67it [00:42,  1.45it/s]Extractor Predicting: 68it [00:42,  1.48it/s]Extractor Predicting: 69it [00:43,  1.51it/s]Extractor Predicting: 70it [00:44,  1.52it/s]Extractor Predicting: 71it [00:44,  1.53it/s]Extractor Predicting: 72it [00:45,  1.52it/s]Extractor Predicting: 73it [00:45,  1.55it/s]Extractor Predicting: 74it [00:46,  1.52it/s]Extractor Predicting: 75it [00:47,  1.47it/s]Extractor Predicting: 76it [00:48,  1.51it/s]Extractor Predicting: 77it [00:48,  1.51it/s]Extractor Predicting: 78it [00:49,  1.55it/s]Extractor Predicting: 79it [00:49,  1.53it/s]Extractor Predicting: 80it [00:50,  1.44it/s]Extractor Predicting: 81it [00:51,  1.47it/s]Extractor Predicting: 82it [00:52,  1.53it/s]Extractor Predicting: 83it [00:52,  1.56it/s]Extractor Predicting: 84it [00:53,  1.59it/s]Extractor Predicting: 85it [00:54,  1.47it/s]Extractor Predicting: 86it [00:54,  1.47it/s]Extractor Predicting: 87it [00:55,  1.51it/s]Extractor Predicting: 88it [00:55,  1.56it/s]Extractor Predicting: 89it [00:56,  1.64it/s]Extractor Predicting: 90it [00:57,  1.52it/s]Extractor Predicting: 91it [00:57,  1.62it/s]Extractor Predicting: 92it [00:58,  1.70it/s]Extractor Predicting: 93it [00:58,  1.77it/s]Extractor Predicting: 94it [00:59,  1.82it/s]Extractor Predicting: 95it [00:59,  1.77it/s]Extractor Predicting: 96it [01:00,  1.68it/s]Extractor Predicting: 97it [01:01,  1.68it/s]Extractor Predicting: 98it [01:01,  1.69it/s]Extractor Predicting: 99it [01:02,  1.78it/s]Extractor Predicting: 100it [01:02,  1.80it/s]Extractor Predicting: 101it [01:03,  1.78it/s]Extractor Predicting: 102it [01:04,  1.62it/s]Extractor Predicting: 103it [01:04,  1.66it/s]Extractor Predicting: 104it [01:05,  1.65it/s]Extractor Predicting: 105it [01:05,  1.67it/s]Extractor Predicting: 106it [01:06,  1.70it/s]Extractor Predicting: 107it [01:07,  1.60it/s]Extractor Predicting: 108it [01:07,  1.66it/s]Extractor Predicting: 109it [01:08,  1.71it/s]Extractor Predicting: 110it [01:08,  1.74it/s]Extractor Predicting: 111it [01:09,  1.76it/s]Extractor Predicting: 112it [01:09,  1.77it/s]Extractor Predicting: 113it [01:10,  1.70it/s]Extractor Predicting: 114it [01:11,  1.69it/s]Extractor Predicting: 115it [01:11,  1.75it/s]Extractor Predicting: 116it [01:12,  1.72it/s]Extractor Predicting: 117it [01:12,  1.69it/s]Extractor Predicting: 118it [01:13,  1.71it/s]Extractor Predicting: 119it [01:14,  1.59it/s]Extractor Predicting: 120it [01:14,  1.59it/s]Extractor Predicting: 121it [01:15,  1.58it/s]Extractor Predicting: 122it [01:16,  1.61it/s]Extractor Predicting: 123it [01:16,  1.61it/s]Extractor Predicting: 124it [01:17,  1.60it/s]Extractor Predicting: 125it [01:17,  1.61it/s]Extractor Predicting: 126it [01:18,  1.42it/s]Extractor Predicting: 127it [01:19,  1.47it/s]Extractor Predicting: 128it [01:20,  1.51it/s]Extractor Predicting: 129it [01:20,  1.57it/s]Extractor Predicting: 130it [01:21,  1.61it/s]Extractor Predicting: 131it [01:21,  1.54it/s]Extractor Predicting: 132it [01:22,  1.56it/s]Extractor Predicting: 133it [01:23,  1.58it/s]Extractor Predicting: 134it [01:23,  1.59it/s]Extractor Predicting: 135it [01:24,  1.61it/s]Extractor Predicting: 136it [01:25,  1.50it/s]Extractor Predicting: 137it [01:25,  1.54it/s]Extractor Predicting: 138it [01:26,  1.59it/s]Extractor Predicting: 139it [01:26,  1.61it/s]Extractor Predicting: 140it [01:27,  1.65it/s]Extractor Predicting: 141it [01:28,  1.52it/s]Extractor Predicting: 142it [01:28,  1.55it/s]Extractor Predicting: 143it [01:29,  1.59it/s]Extractor Predicting: 144it [01:30,  1.63it/s]Extractor Predicting: 144it [01:30,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:33,291 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:33,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:33,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:33,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:33,352 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:43:34,629 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:43:34,630 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:43:35,438 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:43:36,832 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:43:36,990 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:40,717 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:40,910 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:40,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:40,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:43:40,911 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:43:42,413 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:43:42,414 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:43:43,462 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:43:43,888 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:43:44,021 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.65it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.62it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.60it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.67it/s]Extractor Predicting: 15it [00:09,  1.70it/s]Extractor Predicting: 16it [00:09,  1.73it/s]Extractor Predicting: 17it [00:10,  1.68it/s]Extractor Predicting: 18it [00:10,  1.68it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:14,  1.64it/s]Extractor Predicting: 24it [00:14,  1.68it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.64it/s]Extractor Predicting: 27it [00:16,  1.63it/s]Extractor Predicting: 28it [00:17,  1.64it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.53it/s]Extractor Predicting: 31it [00:19,  1.57it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:20,  1.63it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:21,  1.56it/s]Extractor Predicting: 36it [00:22,  1.62it/s]Extractor Predicting: 37it [00:22,  1.69it/s]Extractor Predicting: 38it [00:23,  1.70it/s]Extractor Predicting: 39it [00:23,  1.71it/s]Extractor Predicting: 40it [00:24,  1.72it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:25,  1.64it/s]Extractor Predicting: 43it [00:26,  1.66it/s]Extractor Predicting: 44it [00:26,  1.68it/s]Extractor Predicting: 45it [00:27,  1.67it/s]Extractor Predicting: 46it [00:28,  1.61it/s]Extractor Predicting: 47it [00:28,  1.65it/s]Extractor Predicting: 48it [00:29,  1.67it/s]Extractor Predicting: 49it [00:29,  1.68it/s]Extractor Predicting: 50it [00:30,  1.67it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:31,  1.63it/s]Extractor Predicting: 53it [00:32,  1.66it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:33,  1.65it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:34,  1.66it/s]Extractor Predicting: 58it [00:35,  1.72it/s]Extractor Predicting: 59it [00:35,  1.77it/s]Extractor Predicting: 60it [00:36,  1.76it/s]Extractor Predicting: 61it [00:36,  1.75it/s]Extractor Predicting: 62it [00:37,  1.67it/s]Extractor Predicting: 63it [00:38,  1.70it/s]Extractor Predicting: 64it [00:38,  1.67it/s]Extractor Predicting: 65it [00:39,  1.69it/s]Extractor Predicting: 66it [00:39,  1.69it/s]Extractor Predicting: 67it [00:40,  1.57it/s]Extractor Predicting: 68it [00:41,  1.66it/s]Extractor Predicting: 69it [00:41,  1.67it/s]Extractor Predicting: 70it [00:42,  1.69it/s]Extractor Predicting: 71it [00:43,  1.61it/s]Extractor Predicting: 72it [00:43,  1.65it/s]Extractor Predicting: 73it [00:44,  1.64it/s]Extractor Predicting: 74it [00:44,  1.65it/s]Extractor Predicting: 75it [00:45,  1.67it/s]Extractor Predicting: 76it [00:46,  1.60it/s]Extractor Predicting: 77it [00:46,  1.59it/s]Extractor Predicting: 78it [00:47,  1.63it/s]Extractor Predicting: 79it [00:47,  1.63it/s]Extractor Predicting: 80it [00:48,  1.65it/s]Extractor Predicting: 81it [00:49,  1.57it/s]Extractor Predicting: 82it [00:49,  1.59it/s]Extractor Predicting: 83it [00:50,  1.62it/s]Extractor Predicting: 84it [00:51,  1.65it/s]Extractor Predicting: 85it [00:51,  1.69it/s]Extractor Predicting: 86it [00:52,  1.55it/s]Extractor Predicting: 87it [00:52,  1.59it/s]Extractor Predicting: 88it [00:53,  1.60it/s]Extractor Predicting: 89it [00:54,  1.65it/s]Extractor Predicting: 90it [00:54,  1.66it/s]Extractor Predicting: 91it [00:55,  1.57it/s]Extractor Predicting: 92it [00:56,  1.57it/s]Extractor Predicting: 93it [00:56,  1.64it/s]Extractor Predicting: 94it [00:57,  1.63it/s]Extractor Predicting: 95it [00:57,  1.65it/s]Extractor Predicting: 96it [00:58,  1.59it/s]Extractor Predicting: 97it [00:59,  1.63it/s]Extractor Predicting: 98it [00:59,  1.65it/s]Extractor Predicting: 99it [01:00,  1.68it/s]Extractor Predicting: 100it [01:00,  1.71it/s]Extractor Predicting: 101it [01:01,  1.73it/s]Extractor Predicting: 102it [01:02,  1.58it/s]Extractor Predicting: 103it [01:02,  1.58it/s]Extractor Predicting: 104it [01:03,  1.59it/s]Extractor Predicting: 105it [01:03,  1.63it/s]Extractor Predicting: 106it [01:04,  1.64it/s]Extractor Predicting: 107it [01:05,  1.58it/s]Extractor Predicting: 108it [01:05,  1.63it/s]Extractor Predicting: 109it [01:06,  1.64it/s]Extractor Predicting: 110it [01:06,  1.66it/s]Extractor Predicting: 111it [01:07,  1.65it/s]Extractor Predicting: 112it [01:08,  1.59it/s]Extractor Predicting: 113it [01:08,  1.61it/s]Extractor Predicting: 114it [01:09,  1.61it/s]Extractor Predicting: 115it [01:10,  1.63it/s]Extractor Predicting: 116it [01:10,  1.69it/s]Extractor Predicting: 117it [01:11,  1.71it/s]Extractor Predicting: 118it [01:11,  1.71it/s]Extractor Predicting: 119it [01:12,  1.71it/s]Extractor Predicting: 120it [01:12,  1.69it/s]Extractor Predicting: 121it [01:13,  1.66it/s]Extractor Predicting: 122it [01:14,  1.69it/s]Extractor Predicting: 123it [01:14,  1.73it/s]Extractor Predicting: 124it [01:15,  1.76it/s]Extractor Predicting: 125it [01:15,  1.75it/s]Extractor Predicting: 126it [01:16,  1.75it/s]Extractor Predicting: 127it [01:17,  1.68it/s]Extractor Predicting: 128it [01:17,  1.69it/s]Extractor Predicting: 129it [01:18,  1.67it/s]Extractor Predicting: 130it [01:18,  1.65it/s]Extractor Predicting: 131it [01:19,  1.71it/s]Extractor Predicting: 132it [01:20,  1.57it/s]Extractor Predicting: 133it [01:20,  1.63it/s]Extractor Predicting: 134it [01:21,  1.68it/s]Extractor Predicting: 135it [01:21,  1.71it/s]Extractor Predicting: 136it [01:22,  1.72it/s]Extractor Predicting: 137it [01:23,  1.62it/s]Extractor Predicting: 138it [01:23,  1.63it/s]Extractor Predicting: 139it [01:24,  1.67it/s]Extractor Predicting: 140it [01:24,  1.75it/s]Extractor Predicting: 141it [01:25,  1.77it/s]Extractor Predicting: 142it [01:25,  1.76it/s]Extractor Predicting: 143it [01:26,  1.60it/s]Extractor Predicting: 144it [01:27,  1.65it/s]Extractor Predicting: 145it [01:27,  1.69it/s]Extractor Predicting: 146it [01:28,  1.74it/s]Extractor Predicting: 147it [01:28,  1.77it/s]Extractor Predicting: 148it [01:29,  1.72it/s]Extractor Predicting: 149it [01:30,  1.65it/s]Extractor Predicting: 150it [01:30,  1.67it/s]Extractor Predicting: 151it [01:31,  1.69it/s]Extractor Predicting: 152it [01:31,  1.71it/s]Extractor Predicting: 153it [01:32,  1.69it/s]Extractor Predicting: 154it [01:33,  1.73it/s]Extractor Predicting: 155it [01:33,  1.65it/s]Extractor Predicting: 156it [01:34,  1.72it/s]Extractor Predicting: 157it [01:34,  1.74it/s]Extractor Predicting: 158it [01:35,  1.82it/s]Extractor Predicting: 159it [01:35,  1.78it/s]Extractor Predicting: 160it [01:36,  1.75it/s]Extractor Predicting: 161it [01:37,  1.66it/s]Extractor Predicting: 162it [01:37,  1.67it/s]Extractor Predicting: 163it [01:38,  1.71it/s]Extractor Predicting: 164it [01:38,  1.72it/s]Extractor Predicting: 165it [01:39,  1.76it/s]Extractor Predicting: 166it [01:39,  1.80it/s]Extractor Predicting: 167it [01:40,  1.56it/s]Extractor Predicting: 168it [01:41,  1.57it/s]Extractor Predicting: 169it [01:41,  1.62it/s]Extractor Predicting: 170it [01:42,  1.65it/s]Extractor Predicting: 171it [01:43,  1.68it/s]Extractor Predicting: 172it [01:43,  1.70it/s]Extractor Predicting: 173it [01:44,  1.57it/s]Extractor Predicting: 174it [01:45,  1.62it/s]Extractor Predicting: 175it [01:45,  1.61it/s]Extractor Predicting: 176it [01:46,  1.59it/s]Extractor Predicting: 177it [01:46,  1.65it/s]Extractor Predicting: 178it [01:47,  1.58it/s]Extractor Predicting: 179it [01:48,  1.64it/s]Extractor Predicting: 180it [01:48,  1.69it/s]Extractor Predicting: 181it [01:49,  1.67it/s]Extractor Predicting: 182it [01:49,  1.68it/s]Extractor Predicting: 183it [01:50,  1.59it/s]Extractor Predicting: 184it [01:51,  1.60it/s]Extractor Predicting: 185it [01:51,  1.62it/s]Extractor Predicting: 186it [01:52,  1.64it/s]Extractor Predicting: 187it [01:52,  1.66it/s]Extractor Predicting: 188it [01:53,  1.54it/s]Extractor Predicting: 189it [01:54,  1.57it/s]Extractor Predicting: 190it [01:54,  1.60it/s]Extractor Predicting: 191it [01:55,  1.65it/s]Extractor Predicting: 192it [01:56,  1.70it/s]Extractor Predicting: 193it [01:56,  1.57it/s]Extractor Predicting: 194it [01:57,  1.58it/s]Extractor Predicting: 195it [01:57,  1.64it/s]Extractor Predicting: 196it [01:58,  1.66it/s]Extractor Predicting: 197it [01:59,  1.69it/s]Extractor Predicting: 198it [01:59,  1.57it/s]Extractor Predicting: 199it [02:00,  1.62it/s]Extractor Predicting: 200it [02:00,  1.71it/s]Extractor Predicting: 201it [02:01,  1.74it/s]Extractor Predicting: 202it [02:02,  1.74it/s]Extractor Predicting: 203it [02:02,  1.72it/s]Extractor Predicting: 204it [02:03,  1.58it/s]Extractor Predicting: 205it [02:03,  1.64it/s]Extractor Predicting: 206it [02:04,  1.65it/s]Extractor Predicting: 207it [02:05,  1.68it/s]Extractor Predicting: 208it [02:05,  1.66it/s]Extractor Predicting: 209it [02:06,  1.60it/s]Extractor Predicting: 210it [02:07,  1.62it/s]Extractor Predicting: 211it [02:07,  1.65it/s]Extractor Predicting: 212it [02:08,  1.68it/s]Extractor Predicting: 213it [02:08,  1.70it/s]Extractor Predicting: 214it [02:09,  1.52it/s]Extractor Predicting: 215it [02:10,  1.56it/s]Extractor Predicting: 216it [02:10,  1.63it/s]Extractor Predicting: 217it [02:11,  1.67it/s]Extractor Predicting: 218it [02:11,  1.65it/s]Extractor Predicting: 219it [02:12,  1.65it/s]Extractor Predicting: 220it [02:13,  1.68it/s]Extractor Predicting: 221it [02:13,  1.70it/s]Extractor Predicting: 222it [02:14,  1.69it/s]Extractor Predicting: 223it [02:14,  1.65it/s]Extractor Predicting: 224it [02:15,  1.66it/s]Extractor Predicting: 225it [02:16,  1.67it/s]Extractor Predicting: 226it [02:16,  1.65it/s]Extractor Predicting: 227it [02:17,  1.68it/s]Extractor Predicting: 228it [02:18,  1.57it/s]Extractor Predicting: 229it [02:18,  1.63it/s]Extractor Predicting: 230it [02:19,  1.67it/s]Extractor Predicting: 231it [02:19,  1.68it/s]Extractor Predicting: 232it [02:20,  1.69it/s]Extractor Predicting: 233it [02:20,  1.61it/s]Extractor Predicting: 234it [02:21,  1.65it/s]Extractor Predicting: 235it [02:22,  1.69it/s]Extractor Predicting: 236it [02:22,  1.71it/s]Extractor Predicting: 237it [02:23,  1.71it/s]Extractor Predicting: 238it [02:23,  1.72it/s]Extractor Predicting: 239it [02:24,  1.61it/s]Extractor Predicting: 240it [02:25,  1.67it/s]Extractor Predicting: 241it [02:25,  1.70it/s]Extractor Predicting: 242it [02:26,  1.75it/s]Extractor Predicting: 243it [02:26,  1.75it/s]Extractor Predicting: 244it [02:27,  1.74it/s]Extractor Predicting: 245it [02:28,  1.65it/s]Extractor Predicting: 246it [02:28,  1.67it/s]Extractor Predicting: 247it [02:29,  1.72it/s]Extractor Predicting: 248it [02:29,  1.72it/s]Extractor Predicting: 249it [02:30,  1.74it/s]Extractor Predicting: 250it [02:30,  1.77it/s]Extractor Predicting: 251it [02:31,  1.66it/s]Extractor Predicting: 252it [02:32,  1.67it/s]Extractor Predicting: 253it [02:32,  1.75it/s]Extractor Predicting: 254it [02:33,  1.74it/s]Extractor Predicting: 255it [02:33,  1.72it/s]Extractor Predicting: 256it [02:34,  1.71it/s]Extractor Predicting: 257it [02:35,  1.63it/s]Extractor Predicting: 258it [02:35,  1.69it/s]Extractor Predicting: 259it [02:36,  1.73it/s]Extractor Predicting: 260it [02:36,  1.75it/s]Extractor Predicting: 261it [02:37,  1.73it/s]Extractor Predicting: 262it [02:37,  1.76it/s]Extractor Predicting: 263it [02:38,  1.67it/s]Extractor Predicting: 264it [02:39,  1.71it/s]Extractor Predicting: 265it [02:39,  1.75it/s]Extractor Predicting: 266it [02:40,  1.80it/s]Extractor Predicting: 267it [02:40,  1.76it/s]Extractor Predicting: 268it [02:41,  1.75it/s]Extractor Predicting: 269it [02:41,  1.69it/s]Extractor Predicting: 270it [02:42,  1.71it/s]Extractor Predicting: 271it [02:43,  1.78it/s]Extractor Predicting: 272it [02:43,  1.83it/s]Extractor Predicting: 273it [02:44,  1.80it/s]Extractor Predicting: 274it [02:44,  1.80it/s]Extractor Predicting: 275it [02:45,  1.79it/s]Extractor Predicting: 276it [02:45,  1.67it/s]Extractor Predicting: 277it [02:46,  1.70it/s]Extractor Predicting: 278it [02:47,  1.71it/s]Extractor Predicting: 279it [02:47,  1.71it/s]Extractor Predicting: 280it [02:48,  1.73it/s]Extractor Predicting: 281it [02:48,  1.69it/s]Extractor Predicting: 281it [02:48,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:59,872 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:59,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:59,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:59,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:46:59,963 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 13:47:01,359 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 13:47:01,360 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:47:02,139 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 13:47:03,508 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:47:03,618 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:07,241 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:07,348 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:07,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:07,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 13:47:07,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 13:47:08,505 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 13:47:08,506 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 13:47:09,333 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 13:47:09,769 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 13:47:09,769 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.50it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.89it/s]Extractor Predicting: 6it [00:03,  1.66it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_10_seed_0', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_10_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:16<03:57, 16.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:33<03:40, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:51<03:26, 17.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:06<02:59, 16.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:24<02:49, 16.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:43<02:39, 17.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:00<02:19, 17.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:17<02:01, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:33<01:42, 17.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:48<01:21, 16.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:04<01:04, 16.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:19<00:47, 15.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:40<00:34, 17.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [04:02<00:18, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:24<00:00, 19.85s/it]Generating: 100%|██████████| 15/15 [04:24<00:00, 17.64s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 384, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 472, 'raw': 544}
{'target': 600, 'success': 497, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : composer .', 'success_rate': 0.8721590909090909, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : main subject . Context : Later in the year ( 1143–46 ) , he married daughter of Louis XIV and Catherine I of Prussia married to Marie Antoinette III , daughter of Emperor Louis XII and Catherine of Rheims . Head Entity : Catherine I , Tail Entity : Emperor Louis XII and Catherine I of Prussia .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 370, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 549, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 595, 'raw': 736}
{'target': 600, 'success': 618, 'raw': 768}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8046875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 42, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 115, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 211, 'raw': 288}
{'target': 600, 'success': 236, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 329, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 384, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 511, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 599, 'raw': 800}
{'target': 600, 'success': 620, 'raw': 832}
{'prompt': 'Relation : participant in .', 'success_rate': 0.7451923076923077, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 256, 'raw': 320}
{'target': 600, 'success': 284, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 337, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 416, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 493, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 627, 'raw': 768}
{'prompt': 'Relation : platform .', 'success_rate': 0.81640625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 198, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 241, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 286, 'raw': 384}
{'target': 600, 'success': 307, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 427, 'raw': 576}
{'target': 600, 'success': 449, 'raw': 608}
{'target': 600, 'success': 465, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 516, 'raw': 704}
{'target': 600, 'success': 543, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 615, 'raw': 832}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.7391826923076923, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 90, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 139, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 184, 'raw': 256}
{'target': 600, 'success': 204, 'raw': 288}
{'target': 600, 'success': 228, 'raw': 320}
{'target': 600, 'success': 249, 'raw': 352}
{'target': 600, 'success': 273, 'raw': 384}
{'target': 600, 'success': 294, 'raw': 416}
{'target': 600, 'success': 317, 'raw': 448}
{'target': 600, 'success': 340, 'raw': 480}
{'target': 600, 'success': 360, 'raw': 512}
{'target': 600, 'success': 384, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 426, 'raw': 608}
{'target': 600, 'success': 447, 'raw': 640}
{'target': 600, 'success': 471, 'raw': 672}
{'target': 600, 'success': 494, 'raw': 704}
{'target': 600, 'success': 517, 'raw': 736}
{'target': 600, 'success': 536, 'raw': 768}
{'target': 600, 'success': 560, 'raw': 800}
{'target': 600, 'success': 585, 'raw': 832}
{'target': 600, 'success': 606, 'raw': 864}
{'prompt': 'Relation : competition class .', 'success_rate': 0.7013888888888888, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : location . Context : Later in the year ( October 1887 ) , a young French journalist named Louis Boulouz had visited Amadec for the first time . Head Entity : Amadec , Tail Entity : Amadeca .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 142, 'raw': 192}
{'target': 600, 'success': 164, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 214, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 290, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 362, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 413, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 459, 'raw': 608}
{'target': 600, 'success': 485, 'raw': 640}
{'target': 600, 'success': 507, 'raw': 672}
{'target': 600, 'success': 529, 'raw': 704}
{'target': 600, 'success': 555, 'raw': 736}
{'target': 600, 'success': 582, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : location .', 'success_rate': 0.75625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 263, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 338, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 436, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 563, 'raw': 704}
{'target': 600, 'success': 586, 'raw': 736}
{'target': 600, 'success': 612, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 93, 'raw': 128}
{'target': 600, 'success': 114, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 215, 'raw': 288}
{'target': 600, 'success': 235, 'raw': 320}
{'target': 600, 'success': 257, 'raw': 352}
{'target': 600, 'success': 282, 'raw': 384}
{'target': 600, 'success': 306, 'raw': 416}
{'target': 600, 'success': 333, 'raw': 448}
{'target': 600, 'success': 357, 'raw': 480}
{'target': 600, 'success': 382, 'raw': 512}
{'target': 600, 'success': 407, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 455, 'raw': 608}
{'target': 600, 'success': 480, 'raw': 640}
{'target': 600, 'success': 505, 'raw': 672}
{'target': 600, 'success': 530, 'raw': 704}
{'target': 600, 'success': 554, 'raw': 736}
{'target': 600, 'success': 583, 'raw': 768}
{'target': 600, 'success': 605, 'raw': 800}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.75625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Paul Groening', 'nominated for', '', 'After a stint in the Swedish music industry in 2002 alongside the late Paul Groening , he moved away to New Zealand to continue his education in the country .')"}}
['Relation : operating system . Context : The CVRN ( Computer Vision and Imaging Systems ) is a digital camera developed at the National Institutes of Health . Head Entity : CVRN , Tail Entity : CVR , Head Entity : National Institutes of Health .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 482, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 591, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8410326086956522, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 425, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 483, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 597, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.845108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 532, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 45, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 84, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 126, 'raw': 192}
{'target': 600, 'success': 147, 'raw': 224}
{'target': 600, 'success': 169, 'raw': 256}
{'target': 600, 'success': 193, 'raw': 288}
{'target': 600, 'success': 212, 'raw': 320}
{'target': 600, 'success': 236, 'raw': 352}
{'target': 600, 'success': 258, 'raw': 384}
{'target': 600, 'success': 280, 'raw': 416}
{'target': 600, 'success': 303, 'raw': 448}
{'target': 600, 'success': 326, 'raw': 480}
{'target': 600, 'success': 350, 'raw': 512}
{'target': 600, 'success': 372, 'raw': 544}
{'target': 600, 'success': 394, 'raw': 576}
{'target': 600, 'success': 419, 'raw': 608}
{'target': 600, 'success': 446, 'raw': 640}
{'target': 600, 'success': 465, 'raw': 672}
{'target': 600, 'success': 490, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 527, 'raw': 768}
{'target': 600, 'success': 551, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 598, 'raw': 864}
{'target': 600, 'success': 620, 'raw': 896}
{'prompt': 'Relation : position held .', 'success_rate': 0.6919642857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 40, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 107, 'raw': 192}
{'target': 600, 'success': 127, 'raw': 224}
{'target': 600, 'success': 148, 'raw': 256}
{'target': 600, 'success': 172, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 243, 'raw': 416}
{'target': 600, 'success': 262, 'raw': 448}
{'target': 600, 'success': 279, 'raw': 480}
{'target': 600, 'success': 298, 'raw': 512}
{'target': 600, 'success': 317, 'raw': 544}
{'target': 600, 'success': 332, 'raw': 576}
{'target': 600, 'success': 350, 'raw': 608}
{'target': 600, 'success': 365, 'raw': 640}
{'target': 600, 'success': 388, 'raw': 672}
{'target': 600, 'success': 407, 'raw': 704}
{'target': 600, 'success': 429, 'raw': 736}
{'target': 600, 'success': 446, 'raw': 768}
{'target': 600, 'success': 466, 'raw': 800}
{'target': 600, 'success': 482, 'raw': 832}
{'target': 600, 'success': 495, 'raw': 864}
{'target': 600, 'success': 509, 'raw': 896}
{'target': 600, 'success': 522, 'raw': 928}
{'target': 600, 'success': 543, 'raw': 960}
{'target': 600, 'success': 564, 'raw': 992}
{'target': 600, 'success': 584, 'raw': 1024}
{'target': 600, 'success': 600, 'raw': 1056}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.5681818181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n']
['Relation : religion . Context : Later in the year ( 1143–46 ) , he married Leda of Bactria , daughter of Darius I of Persia ; the death of Darius II led to her death in 1134 . Head Entity : Leda , Tail Entity : Persian .\n', "Relation : religion . Context : After the death of King Henry IV of France ( c. 589 - 7 February 1235 ) , St Peter was succeeded as Archbishop by the eponymous St Peter 's successor , the first Pope . Head Entity : St Peter 's successors , Tail Entity : St Peter 's .\n"]
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 44, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 110, 'raw': 160}
{'target': 600, 'success': 138, 'raw': 192}
{'target': 600, 'success': 160, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 199, 'raw': 288}
{'target': 600, 'success': 221, 'raw': 320}
{'target': 600, 'success': 241, 'raw': 352}
{'target': 600, 'success': 264, 'raw': 384}
{'target': 600, 'success': 285, 'raw': 416}
{'target': 600, 'success': 308, 'raw': 448}
{'target': 600, 'success': 323, 'raw': 480}
{'target': 600, 'success': 348, 'raw': 512}
{'target': 600, 'success': 369, 'raw': 544}
{'target': 600, 'success': 391, 'raw': 576}
{'target': 600, 'success': 415, 'raw': 608}
{'target': 600, 'success': 438, 'raw': 640}
{'target': 600, 'success': 459, 'raw': 672}
{'target': 600, 'success': 484, 'raw': 704}
{'target': 600, 'success': 510, 'raw': 736}
{'target': 600, 'success': 534, 'raw': 768}
{'target': 600, 'success': 553, 'raw': 800}
{'target': 600, 'success': 575, 'raw': 832}
{'target': 600, 'success': 601, 'raw': 864}
{'prompt': 'Relation : religion .', 'success_rate': 0.6956018518518519, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/0_ext.jsonl'}}
estimate vocab size: 15147
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15247, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_10_seed_0/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:18, 18.98s/it]Extractor Estimating: 2it [00:21,  9.28s/it]Extractor Estimating: 3it [00:22,  5.35s/it]Extractor Estimating: 4it [00:22,  3.54s/it]Extractor Estimating: 5it [00:23,  2.50s/it]Extractor Estimating: 6it [00:24,  1.85s/it]Extractor Estimating: 7it [00:24,  1.46s/it]Extractor Estimating: 8it [00:25,  1.19s/it]Extractor Estimating: 9it [00:26,  1.02s/it]Extractor Estimating: 10it [00:26,  1.08it/s]Extractor Estimating: 11it [00:27,  1.21it/s]Extractor Estimating: 12it [00:29,  1.11s/it]Extractor Estimating: 13it [00:30,  1.04s/it]Extractor Estimating: 14it [00:30,  1.09it/s]Extractor Estimating: 15it [00:31,  1.22it/s]Extractor Estimating: 16it [00:31,  1.30it/s]Extractor Estimating: 17it [00:32,  1.38it/s]Extractor Estimating: 18it [00:33,  1.37it/s]Extractor Estimating: 19it [00:34,  1.21it/s]Extractor Estimating: 20it [00:34,  1.29it/s]Extractor Estimating: 21it [00:35,  1.38it/s]Extractor Estimating: 22it [00:36,  1.15it/s]Extractor Estimating: 23it [00:37,  1.22it/s]Extractor Estimating: 24it [00:38,  1.28it/s]Extractor Estimating: 25it [00:38,  1.34it/s]Extractor Estimating: 26it [00:39,  1.40it/s]Extractor Estimating: 27it [00:40,  1.40it/s]Extractor Estimating: 28it [00:40,  1.36it/s]Extractor Estimating: 29it [00:41,  1.43it/s]Extractor Estimating: 30it [00:42,  1.41it/s]Extractor Estimating: 31it [00:42,  1.46it/s]Extractor Estimating: 32it [00:43,  1.45it/s]Extractor Estimating: 33it [00:44,  1.39it/s]Extractor Estimating: 34it [00:45,  1.42it/s]Extractor Estimating: 35it [00:45,  1.45it/s]Extractor Estimating: 36it [00:46,  1.37it/s]Extractor Estimating: 37it [00:47,  1.42it/s]Extractor Estimating: 38it [00:48,  1.36it/s]Extractor Estimating: 39it [00:48,  1.43it/s]Extractor Estimating: 40it [00:49,  1.44it/s]Extractor Estimating: 41it [00:50,  1.42it/s]Extractor Estimating: 42it [00:50,  1.42it/s]Extractor Estimating: 43it [00:51,  1.35it/s]Extractor Estimating: 44it [00:52,  1.40it/s]Extractor Estimating: 45it [00:52,  1.44it/s]Extractor Estimating: 46it [00:53,  1.48it/s]Extractor Estimating: 47it [00:54,  1.49it/s]Extractor Estimating: 48it [00:54,  1.43it/s]Extractor Estimating: 49it [00:55,  1.49it/s]Extractor Estimating: 50it [00:56,  1.50it/s]Extractor Estimating: 51it [00:56,  1.47it/s]Extractor Estimating: 52it [00:57,  1.49it/s]Extractor Estimating: 53it [00:58,  1.42it/s]Extractor Estimating: 54it [00:59,  1.46it/s]Extractor Estimating: 55it [00:59,  1.50it/s]Extractor Estimating: 56it [01:00,  1.51it/s]Extractor Estimating: 57it [01:00,  1.51it/s]Extractor Estimating: 58it [01:01,  1.42it/s]Extractor Estimating: 59it [01:02,  1.47it/s]Extractor Estimating: 60it [01:02,  1.55it/s]Extractor Estimating: 61it [01:03,  1.59it/s]Extractor Estimating: 62it [01:04,  1.63it/s]Extractor Estimating: 63it [01:04,  1.68it/s]Extractor Estimating: 64it [01:05,  1.66it/s]Extractor Estimating: 65it [01:05,  1.64it/s]Extractor Estimating: 66it [01:06,  1.62it/s]Extractor Estimating: 67it [01:07,  1.58it/s]Extractor Estimating: 68it [01:07,  1.53it/s]Extractor Estimating: 69it [01:08,  1.53it/s]Extractor Estimating: 70it [01:09,  1.54it/s]Extractor Estimating: 71it [01:09,  1.55it/s]Extractor Estimating: 72it [01:10,  1.57it/s]Extractor Estimating: 73it [01:11,  1.51it/s]Extractor Estimating: 74it [01:11,  1.57it/s]Extractor Estimating: 75it [01:12,  1.61it/s]Extractor Estimating: 76it [01:15,  1.31s/it]Extractor Estimating: 77it [01:16,  1.14s/it]Extractor Estimating: 78it [01:16,  1.02it/s]Extractor Estimating: 79it [01:17,  1.17it/s]Extractor Estimating: 80it [01:17,  1.31it/s]Extractor Estimating: 81it [01:18,  1.41it/s]Extractor Estimating: 82it [01:19,  1.42it/s]Extractor Estimating: 83it [01:19,  1.50it/s]Extractor Estimating: 84it [01:20,  1.54it/s]Extractor Estimating: 85it [01:20,  1.57it/s]Extractor Estimating: 86it [01:21,  1.64it/s]Extractor Estimating: 87it [01:22,  1.48it/s]Extractor Estimating: 88it [01:22,  1.50it/s]Extractor Estimating: 89it [01:23,  1.49it/s]Extractor Estimating: 90it [01:24,  1.51it/s]Extractor Estimating: 91it [01:24,  1.55it/s]Extractor Estimating: 92it [01:25,  1.47it/s]Extractor Estimating: 93it [01:26,  1.50it/s]Extractor Estimating: 94it [01:26,  1.49it/s]Extractor Estimating: 95it [01:27,  1.54it/s]Extractor Estimating: 96it [01:28,  1.53it/s]Extractor Estimating: 97it [01:28,  1.46it/s]Extractor Estimating: 98it [01:29,  1.50it/s]Extractor Estimating: 99it [01:30,  1.53it/s]Extractor Estimating: 100it [01:30,  1.58it/s]Extractor Estimating: 101it [01:31,  1.45it/s]Extractor Estimating: 102it [01:32,  1.32it/s]Extractor Estimating: 103it [01:33,  1.36it/s]Extractor Estimating: 104it [01:33,  1.47it/s]Extractor Estimating: 105it [01:34,  1.55it/s]Extractor Estimating: 106it [01:34,  1.59it/s]Extractor Estimating: 107it [01:35,  1.50it/s]Extractor Estimating: 108it [01:36,  1.52it/s]Extractor Estimating: 109it [01:36,  1.58it/s]Extractor Estimating: 110it [01:37,  1.58it/s]Extractor Estimating: 111it [01:38,  1.60it/s]Extractor Estimating: 112it [01:38,  1.46it/s]Extractor Estimating: 113it [01:39,  1.52it/s]Extractor Estimating: 114it [01:40,  1.58it/s]Extractor Estimating: 115it [01:40,  1.57it/s]Extractor Estimating: 116it [01:41,  1.54it/s]Extractor Estimating: 117it [01:42,  1.52it/s]Extractor Estimating: 118it [01:42,  1.51it/s]Extractor Estimating: 119it [01:43,  1.55it/s]Extractor Estimating: 120it [01:43,  1.51it/s]Extractor Estimating: 121it [01:44,  1.57it/s]Extractor Estimating: 122it [01:45,  1.50it/s]Extractor Estimating: 123it [01:45,  1.54it/s]Extractor Estimating: 124it [01:46,  1.56it/s]Extractor Estimating: 125it [01:47,  1.64it/s]Extractor Estimating: 126it [01:47,  1.66it/s]Extractor Estimating: 127it [01:48,  1.55it/s]Extractor Estimating: 128it [01:48,  1.62it/s]Extractor Estimating: 129it [01:49,  1.64it/s]Extractor Estimating: 130it [01:50,  1.65it/s]Extractor Estimating: 131it [01:50,  1.60it/s]Extractor Estimating: 132it [01:51,  1.50it/s]Extractor Estimating: 133it [01:52,  1.52it/s]Extractor Estimating: 134it [01:52,  1.56it/s]Extractor Estimating: 135it [01:53,  1.65it/s]Extractor Estimating: 136it [01:53,  1.68it/s]Extractor Estimating: 137it [01:54,  1.56it/s]Extractor Estimating: 138it [01:55,  1.62it/s]Extractor Estimating: 139it [01:55,  1.66it/s]Extractor Estimating: 140it [01:56,  1.66it/s]Extractor Estimating: 141it [01:56,  1.69it/s]Extractor Estimating: 142it [01:57,  1.67it/s]Extractor Estimating: 143it [01:58,  1.69it/s]Extractor Estimating: 144it [01:58,  1.69it/s]Extractor Estimating: 145it [01:59,  1.72it/s]Extractor Estimating: 146it [02:00,  1.56it/s]Extractor Estimating: 147it [02:00,  1.54it/s]Extractor Estimating: 148it [02:01,  1.60it/s]Extractor Estimating: 149it [02:01,  1.63it/s]Extractor Estimating: 150it [02:02,  1.56it/s]Extractor Estimating: 151it [02:03,  1.57it/s]Extractor Estimating: 152it [02:04,  1.47it/s]Extractor Estimating: 153it [02:04,  1.49it/s]Extractor Estimating: 154it [02:05,  1.50it/s]Extractor Estimating: 155it [02:06,  1.48it/s]Extractor Estimating: 156it [02:06,  1.48it/s]Extractor Estimating: 157it [02:07,  1.51it/s]Extractor Estimating: 158it [02:07,  1.54it/s]Extractor Estimating: 159it [02:08,  1.61it/s]Extractor Estimating: 160it [02:09,  1.55it/s]Extractor Estimating: 161it [02:09,  1.53it/s]Extractor Estimating: 162it [02:10,  1.51it/s]Extractor Estimating: 163it [02:11,  1.50it/s]Extractor Estimating: 164it [02:11,  1.55it/s]Extractor Estimating: 165it [02:12,  1.44it/s]Extractor Estimating: 166it [02:13,  1.49it/s]Extractor Estimating: 167it [02:13,  1.51it/s]Extractor Estimating: 168it [02:14,  1.50it/s]Extractor Estimating: 169it [02:15,  1.54it/s]Extractor Estimating: 170it [02:16,  1.40it/s]Extractor Estimating: 171it [02:16,  1.40it/s]Extractor Estimating: 172it [02:17,  1.46it/s]Extractor Estimating: 173it [02:17,  1.52it/s]Extractor Estimating: 174it [02:18,  1.56it/s]Extractor Estimating: 175it [02:19,  1.46it/s]Extractor Estimating: 176it [02:20,  1.47it/s]Extractor Estimating: 177it [02:20,  1.51it/s]Extractor Estimating: 178it [02:21,  1.53it/s]Extractor Estimating: 179it [02:21,  1.50it/s]Extractor Estimating: 180it [02:22,  1.39it/s]Extractor Estimating: 181it [02:23,  1.46it/s]Extractor Estimating: 182it [02:24,  1.50it/s]Extractor Estimating: 183it [02:24,  1.54it/s]Extractor Estimating: 184it [02:25,  1.61it/s]Extractor Estimating: 185it [02:26,  1.49it/s]Extractor Estimating: 186it [02:26,  1.53it/s]Extractor Estimating: 187it [02:27,  1.51it/s]Extractor Estimating: 188it [02:27,  1.57it/s]Extractor Estimating: 189it [02:28,  1.60it/s]Extractor Estimating: 190it [02:29,  1.52it/s]Extractor Estimating: 191it [02:29,  1.55it/s]Extractor Estimating: 192it [02:30,  1.62it/s]Extractor Estimating: 193it [02:30,  1.65it/s]Extractor Estimating: 194it [02:31,  1.66it/s]Extractor Estimating: 195it [02:32,  1.58it/s]Extractor Estimating: 196it [02:32,  1.51it/s]Extractor Estimating: 197it [02:33,  1.54it/s]Extractor Estimating: 198it [02:34,  1.51it/s]Extractor Estimating: 199it [02:34,  1.53it/s]Extractor Estimating: 200it [02:35,  1.48it/s]Extractor Estimating: 201it [02:36,  1.48it/s]Extractor Estimating: 202it [02:37,  1.44it/s]Extractor Estimating: 203it [02:37,  1.43it/s]Extractor Estimating: 204it [02:38,  1.46it/s]Extractor Estimating: 205it [02:39,  1.46it/s]Extractor Estimating: 206it [02:40,  1.34it/s]Extractor Estimating: 207it [02:40,  1.38it/s]Extractor Estimating: 208it [02:41,  1.40it/s]Extractor Estimating: 209it [02:42,  1.44it/s]Extractor Estimating: 210it [02:42,  1.44it/s]Extractor Estimating: 211it [02:43,  1.39it/s]Extractor Estimating: 212it [02:44,  1.45it/s]Extractor Estimating: 213it [02:44,  1.50it/s]Extractor Estimating: 214it [02:45,  1.50it/s]Extractor Estimating: 215it [02:46,  1.52it/s]Extractor Estimating: 216it [02:46,  1.37it/s]Extractor Estimating: 217it [02:47,  1.43it/s]Extractor Estimating: 218it [02:48,  1.45it/s]Extractor Estimating: 219it [02:48,  1.48it/s]Extractor Estimating: 220it [02:49,  1.48it/s]Extractor Estimating: 221it [02:50,  1.22it/s]Extractor Estimating: 222it [02:51,  1.30it/s]Extractor Estimating: 223it [02:52,  1.34it/s]Extractor Estimating: 224it [02:52,  1.37it/s]Extractor Estimating: 225it [02:53,  1.27it/s]Extractor Estimating: 226it [02:54,  1.43it/s]Extractor Estimating: 227it [02:54,  1.51it/s]Extractor Estimating: 228it [02:55,  1.53it/s]Extractor Estimating: 229it [02:55,  1.57it/s]Extractor Estimating: 230it [02:56,  1.49it/s]Extractor Estimating: 231it [02:57,  1.49it/s]Extractor Estimating: 232it [02:57,  1.57it/s]Extractor Estimating: 233it [02:58,  1.69it/s]Extractor Estimating: 234it [02:58,  1.74it/s]Extractor Estimating: 235it [02:59,  1.65it/s]Extractor Estimating: 236it [03:00,  1.69it/s]Extractor Estimating: 237it [03:00,  1.74it/s]Extractor Estimating: 238it [03:01,  1.67it/s]Extractor Estimating: 239it [03:02,  1.63it/s]Extractor Estimating: 240it [03:02,  1.59it/s]Extractor Estimating: 241it [03:03,  1.63it/s]Extractor Estimating: 242it [03:03,  1.65it/s]Extractor Estimating: 243it [03:04,  1.65it/s]Extractor Estimating: 244it [03:05,  1.68it/s]Extractor Estimating: 245it [03:05,  1.55it/s]Extractor Estimating: 246it [03:06,  1.59it/s]Extractor Estimating: 247it [03:06,  1.62it/s]Extractor Estimating: 248it [03:07,  1.50it/s]Extractor Estimating: 249it [03:08,  1.55it/s]Extractor Estimating: 250it [03:08,  1.59it/s]Extractor Estimating: 251it [03:09,  1.40it/s]Extractor Estimating: 252it [03:10,  1.40it/s]Extractor Estimating: 253it [03:11,  1.44it/s]Extractor Estimating: 254it [03:11,  1.42it/s]Extractor Estimating: 255it [03:12,  1.48it/s]Extractor Estimating: 256it [03:13,  1.53it/s]Extractor Estimating: 257it [03:13,  1.43it/s]Extractor Estimating: 258it [03:14,  1.49it/s]Extractor Estimating: 259it [03:15,  1.56it/s]Extractor Estimating: 260it [03:15,  1.57it/s]Extractor Estimating: 261it [03:16,  1.57it/s]Extractor Estimating: 262it [03:17,  1.42it/s]Extractor Estimating: 263it [03:17,  1.44it/s]Extractor Estimating: 264it [03:18,  1.48it/s]Extractor Estimating: 265it [03:19,  1.53it/s]Extractor Estimating: 266it [03:19,  1.45it/s]Extractor Estimating: 267it [03:20,  1.38it/s]Extractor Estimating: 268it [03:21,  1.43it/s]Extractor Estimating: 269it [03:22,  1.43it/s]Extractor Estimating: 270it [03:22,  1.46it/s]Extractor Estimating: 271it [03:23,  1.44it/s]Extractor Estimating: 272it [03:24,  1.33it/s]Extractor Estimating: 273it [03:25,  1.39it/s]Extractor Estimating: 274it [03:25,  1.47it/s]Extractor Estimating: 275it [03:26,  1.54it/s]Extractor Estimating: 276it [03:26,  1.58it/s]Extractor Estimating: 277it [03:27,  1.48it/s]Extractor Estimating: 278it [03:28,  1.57it/s]Extractor Estimating: 279it [03:28,  1.67it/s]Extractor Estimating: 280it [03:29,  1.65it/s]Extractor Estimating: 281it [03:29,  1.66it/s]Extractor Estimating: 282it [03:30,  1.52it/s]Extractor Estimating: 283it [03:31,  1.58it/s]Extractor Estimating: 284it [03:31,  1.60it/s]Extractor Estimating: 285it [03:32,  1.64it/s]Extractor Estimating: 286it [03:32,  1.63it/s]Extractor Estimating: 287it [03:33,  1.45it/s]Extractor Estimating: 288it [03:34,  1.46it/s]Extractor Estimating: 289it [03:35,  1.50it/s]Extractor Estimating: 290it [03:35,  1.58it/s]Extractor Estimating: 291it [03:36,  1.57it/s]Extractor Estimating: 292it [03:37,  1.48it/s]Extractor Estimating: 293it [03:37,  1.51it/s]Extractor Estimating: 294it [03:38,  1.52it/s]Extractor Estimating: 295it [03:38,  1.56it/s]Extractor Estimating: 296it [03:39,  1.59it/s]Extractor Estimating: 297it [03:40,  1.59it/s]Extractor Estimating: 298it [03:40,  1.61it/s]Extractor Estimating: 299it [03:41,  1.56it/s]Extractor Estimating: 300it [03:42,  1.57it/s]Extractor Estimating: 301it [03:42,  1.56it/s]Extractor Estimating: 302it [03:43,  1.62it/s]Extractor Estimating: 303it [03:43,  1.64it/s]Extractor Estimating: 304it [03:44,  1.52it/s]Extractor Estimating: 305it [03:45,  1.53it/s]Extractor Estimating: 306it [03:45,  1.61it/s]Extractor Estimating: 307it [03:46,  1.63it/s]Extractor Estimating: 308it [03:47,  1.59it/s]Extractor Estimating: 309it [03:47,  1.59it/s]Extractor Estimating: 310it [03:48,  1.59it/s]Extractor Estimating: 311it [03:49,  1.58it/s]Extractor Estimating: 312it [03:49,  1.56it/s]Extractor Estimating: 313it [03:50,  1.57it/s]Extractor Estimating: 314it [03:51,  1.45it/s]Extractor Estimating: 315it [03:51,  1.51it/s]Extractor Estimating: 316it [03:52,  1.55it/s]Extractor Estimating: 317it [03:52,  1.56it/s]Extractor Estimating: 318it [03:53,  1.60it/s]Extractor Estimating: 319it [03:54,  1.47it/s]Extractor Estimating: 320it [03:54,  1.51it/s]Extractor Estimating: 321it [03:55,  1.56it/s]Extractor Estimating: 322it [03:56,  1.57it/s]Extractor Estimating: 323it [03:56,  1.55it/s]Extractor Estimating: 324it [03:57,  1.46it/s]Extractor Estimating: 325it [03:58,  1.51it/s]Extractor Estimating: 326it [03:59,  1.43it/s]Extractor Estimating: 327it [03:59,  1.51it/s]Extractor Estimating: 328it [04:00,  1.56it/s]Extractor Estimating: 329it [04:00,  1.54it/s]Extractor Estimating: 330it [04:01,  1.52it/s]Extractor Estimating: 331it [04:02,  1.61it/s]Extractor Estimating: 332it [04:02,  1.65it/s]Extractor Estimating: 333it [04:03,  1.74it/s]Extractor Estimating: 334it [04:03,  1.74it/s]Extractor Estimating: 335it [04:04,  1.57it/s]Extractor Estimating: 336it [04:05,  1.57it/s]Extractor Estimating: 337it [04:05,  1.63it/s]Extractor Estimating: 338it [04:06,  1.67it/s]Extractor Estimating: 339it [04:06,  1.66it/s]Extractor Estimating: 340it [04:07,  1.57it/s]Extractor Estimating: 341it [04:08,  1.65it/s]Extractor Estimating: 342it [04:08,  1.69it/s]Extractor Estimating: 343it [04:09,  1.73it/s]Extractor Estimating: 344it [04:09,  1.77it/s]Extractor Estimating: 345it [04:10,  1.77it/s]Extractor Estimating: 346it [04:10,  1.72it/s]Extractor Estimating: 347it [04:11,  1.74it/s]Extractor Estimating: 348it [04:12,  1.59it/s]Extractor Estimating: 349it [04:12,  1.62it/s]Extractor Estimating: 350it [04:13,  1.65it/s]Extractor Estimating: 351it [04:14,  1.54it/s]Extractor Estimating: 352it [04:14,  1.48it/s]Extractor Estimating: 353it [04:15,  1.41it/s]Extractor Estimating: 354it [04:16,  1.45it/s]Extractor Estimating: 355it [04:17,  1.41it/s]Extractor Estimating: 356it [04:17,  1.43it/s]Extractor Estimating: 357it [04:18,  1.47it/s]Extractor Estimating: 358it [04:19,  1.45it/s]Extractor Estimating: 359it [04:19,  1.45it/s]Extractor Estimating: 360it [04:20,  1.46it/s]Extractor Estimating: 361it [04:21,  1.45it/s]Extractor Estimating: 362it [04:21,  1.48it/s]Extractor Estimating: 363it [04:22,  1.47it/s]Extractor Estimating: 364it [04:23,  1.49it/s]Extractor Estimating: 365it [04:23,  1.49it/s]Extractor Estimating: 366it [04:24,  1.51it/s]Extractor Estimating: 367it [04:25,  1.57it/s]Extractor Estimating: 368it [04:25,  1.46it/s]Extractor Estimating: 369it [04:26,  1.44it/s]Extractor Estimating: 370it [04:27,  1.50it/s]Extractor Estimating: 371it [04:27,  1.52it/s]Extractor Estimating: 372it [04:28,  1.48it/s]Extractor Estimating: 373it [04:29,  1.36it/s]Extractor Estimating: 374it [04:30,  1.42it/s]Extractor Estimating: 375it [04:30,  1.48it/s]Extractor Estimating: 375it [04:30,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1500, 'num_train': 6000}
num of filtered data: 7476 mean pseudo reward: 0.9099362007922495
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 27096
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27196, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_10_seed_0/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=27196, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.244, loss:1175.9300
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.936, loss:1134.1383
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.021, loss:1147.1658
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.958, loss:1105.1546
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 0.964, loss:1072.0972
>> valid entity prec:0.5394, rec:0.6548, f1:0.5915
>> valid relation prec:0.2716, rec:0.0774, f1:0.1204
>> valid relation with NER prec:0.2716, rec:0.0774, f1:0.1204
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.254, loss:1054.9374
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.946, loss:1030.3363
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.965, loss:1023.3326
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 0.954, loss:1076.5264
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 0.962, loss:1017.7013
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5770, rec:0.5887, f1:0.5828
>> valid relation prec:0.3682, rec:0.0696, f1:0.1171
>> valid relation with NER prec:0.3682, rec:0.0696, f1:0.1171
g_step 1100, step 164, avg_time 2.234, loss:1018.4426
g_step 1200, step 264, avg_time 0.950, loss:1019.1025
g_step 1300, step 52, avg_time 0.963, loss:954.8275
g_step 1400, step 152, avg_time 0.963, loss:941.1040
g_step 1500, step 252, avg_time 0.967, loss:949.3045
>> valid entity prec:0.5629, rec:0.6532, f1:0.6047
>> valid relation prec:0.3236, rec:0.0923, f1:0.1436
>> valid relation with NER prec:0.3236, rec:0.0923, f1:0.1436
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 40, avg_time 2.256, loss:962.8469
g_step 1700, step 140, avg_time 0.970, loss:907.5595
g_step 1800, step 240, avg_time 0.956, loss:903.2550
g_step 1900, step 28, avg_time 0.958, loss:890.5546
g_step 2000, step 128, avg_time 0.957, loss:861.5547
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5722, rec:0.5725, f1:0.5724
>> valid relation prec:0.2818, rec:0.1192, f1:0.1675
>> valid relation with NER prec:0.2818, rec:0.1192, f1:0.1675
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 2100, step 228, avg_time 2.220, loss:889.7596
g_step 2200, step 16, avg_time 0.944, loss:835.3858
g_step 2300, step 116, avg_time 0.943, loss:826.6298
g_step 2400, step 216, avg_time 0.961, loss:821.0856
g_step 2500, step 4, avg_time 0.960, loss:827.2641
>> valid entity prec:0.5435, rec:0.5893, f1:0.5655
>> valid relation prec:0.2833, rec:0.1163, f1:0.1649
>> valid relation with NER prec:0.2833, rec:0.1163, f1:0.1649
g_step 2600, step 104, avg_time 2.228, loss:740.3828
g_step 2700, step 204, avg_time 0.954, loss:796.2996
g_step 2800, step 304, avg_time 0.953, loss:810.9924
g_step 2900, step 92, avg_time 0.958, loss:749.0244
g_step 3000, step 192, avg_time 0.942, loss:760.6513
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5760, rec:0.5711, f1:0.5735
>> valid relation prec:0.2223, rec:0.0885, f1:0.1266
>> valid relation with NER prec:0.2223, rec:0.0885, f1:0.1266
g_step 3100, step 292, avg_time 2.212, loss:755.5996
g_step 3200, step 80, avg_time 0.955, loss:691.7654
g_step 3300, step 180, avg_time 0.960, loss:724.0757
g_step 3400, step 280, avg_time 0.956, loss:739.2560
g_step 3500, step 68, avg_time 0.966, loss:682.5442
>> valid entity prec:0.5742, rec:0.5472, f1:0.5604
>> valid relation prec:0.2382, rec:0.0779, f1:0.1174
>> valid relation with NER prec:0.2382, rec:0.0779, f1:0.1174
g_step 3600, step 168, avg_time 2.232, loss:690.3993
g_step 3700, step 268, avg_time 0.954, loss:715.4876
g_step 3800, step 56, avg_time 0.959, loss:661.0973
g_step 3900, step 156, avg_time 0.946, loss:667.3273
g_step 4000, step 256, avg_time 0.972, loss:674.7349
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5806, rec:0.5531, f1:0.5665
>> valid relation prec:0.2350, rec:0.0923, f1:0.1325
>> valid relation with NER prec:0.2350, rec:0.0923, f1:0.1325
g_step 4100, step 44, avg_time 2.226, loss:667.2528
g_step 4200, step 144, avg_time 0.953, loss:609.3757
g_step 4300, step 244, avg_time 0.954, loss:619.7062
g_step 4400, step 32, avg_time 0.962, loss:633.0565
g_step 4500, step 132, avg_time 0.971, loss:614.8943
>> valid entity prec:0.6017, rec:0.4953, f1:0.5433
>> valid relation prec:0.2432, rec:0.0948, f1:0.1365
>> valid relation with NER prec:0.2432, rec:0.0948, f1:0.1365
g_step 4600, step 232, avg_time 2.207, loss:609.9650
g_step 4700, step 20, avg_time 0.951, loss:588.2027
g_step 4800, step 120, avg_time 0.946, loss:590.6610
g_step 4900, step 220, avg_time 0.970, loss:581.8962
g_step 5000, step 8, avg_time 0.959, loss:597.0043
learning rate was adjusted to 0.0008
>> valid entity prec:0.6174, rec:0.5456, f1:0.5793
>> valid relation prec:0.2300, rec:0.1063, f1:0.1454
>> valid relation with NER prec:0.2300, rec:0.1063, f1:0.1454
g_step 5100, step 108, avg_time 2.215, loss:537.4548
g_step 5200, step 208, avg_time 0.955, loss:561.4909
g_step 5300, step 308, avg_time 0.950, loss:566.8316
g_step 5400, step 96, avg_time 0.946, loss:523.7606
g_step 5500, step 196, avg_time 0.970, loss:535.6930
>> valid entity prec:0.5896, rec:0.5591, f1:0.5740
>> valid relation prec:0.2508, rec:0.1120, f1:0.1549
>> valid relation with NER prec:0.2508, rec:0.1120, f1:0.1549
g_step 5600, step 296, avg_time 2.201, loss:564.2049
g_step 5700, step 84, avg_time 0.942, loss:515.5606
g_step 5800, step 184, avg_time 0.956, loss:508.8045
g_step 5900, step 284, avg_time 0.949, loss:526.8401
g_step 6000, step 72, avg_time 0.944, loss:505.6962
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5865, rec:0.4861, f1:0.5316
>> valid relation prec:0.2354, rec:0.0911, f1:0.1314
>> valid relation with NER prec:0.2354, rec:0.0911, f1:0.1314
g_step 6100, step 172, avg_time 2.203, loss:508.2818
g_step 6200, step 272, avg_time 0.952, loss:499.2924
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:30:50 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:30:51 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-30-50_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:30:56 - WARNING - datasets.builder -   Using custom data configuration default-31e296109de93693
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-31e296109de93693/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]1 tables [00:01,  1.96s/ tables]                                [INFO|configuration_utils.py:515] 2023-08-28 16:31:29,717 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:31:30,088 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:31:30,088 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:31:30,089 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:31:31,398 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:31:31,683 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:31:31,683 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:31:31,683 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:31:31,683 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:31:31,683 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:31:31,683 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:31:34,276 >> loading weights file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:31:38,387 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:31:38,561 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_10_seed_0/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-31e296109de93693/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 16:31:38 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x150d2661ff80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:06<00:46,  6.70s/ba] 25%|██▌       | 2/8 [00:07<00:18,  3.04s/ba] 38%|███▊      | 3/8 [00:07<00:08,  1.75s/ba] 50%|█████     | 4/8 [00:07<00:04,  1.14s/ba] 62%|██████▎   | 5/8 [00:07<00:02,  1.24ba/s] 75%|███████▌  | 6/8 [00:08<00:01,  1.66ba/s] 88%|████████▊ | 7/8 [00:08<00:00,  2.11ba/s]100%|██████████| 8/8 [00:08<00:00,  2.80ba/s]100%|██████████| 8/8 [00:08<00:00,  1.04s/ba]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:05,  1.75s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.16ba/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.76ba/s]100%|██████████| 4/4 [00:02<00:00,  2.31ba/s]100%|██████████| 4/4 [00:02<00:00,  1.64ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:01<00:09,  1.29s/ba] 25%|██▌       | 2/8 [00:01<00:03,  1.66ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.59ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  5.34ba/s]100%|██████████| 8/8 [00:01<00:00,  7.24ba/s]100%|██████████| 8/8 [00:01<00:00,  4.24ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:03,  1.18s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.82ba/s]100%|██████████| 4/4 [00:01<00:00,  4.08ba/s]100%|██████████| 4/4 [00:01<00:00,  2.82ba/s]
[INFO|trainer.py:414] 2023-08-28 16:32:32,371 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:32:34,105 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:32:34,105 >>   Num examples = 7504
[INFO|trainer.py:1149] 2023-08-28 16:32:34,105 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:32:34,105 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:32:34,105 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:32:34,105 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:32:34,105 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:06<59:13,  6.08s/it]  0%|          | 2/585 [00:12<1:02:42,  6.45s/it]  1%|          | 3/585 [00:16<49:08,  5.07s/it]    1%|          | 4/585 [00:18<37:04,  3.83s/it]  1%|          | 5/585 [00:19<27:42,  2.87s/it]  1%|          | 6/585 [00:21<25:57,  2.69s/it]  1%|          | 7/585 [00:23<22:20,  2.32s/it]  1%|▏         | 8/585 [00:23<16:56,  1.76s/it]  2%|▏         | 9/585 [00:24<12:29,  1.30s/it]  2%|▏         | 10/585 [00:24<09:28,  1.01it/s]  2%|▏         | 11/585 [00:24<07:24,  1.29it/s]  2%|▏         | 12/585 [00:25<07:22,  1.30it/s]  2%|▏         | 13/585 [00:25<05:57,  1.60it/s]  2%|▏         | 14/585 [00:26<05:45,  1.65it/s]  3%|▎         | 15/585 [00:26<04:50,  1.96it/s]  3%|▎         | 16/585 [00:26<04:12,  2.25it/s]  3%|▎         | 17/585 [00:27<03:45,  2.52it/s]  3%|▎         | 18/585 [00:27<03:27,  2.74it/s]  3%|▎         | 19/585 [00:27<03:13,  2.93it/s]  3%|▎         | 20/585 [00:27<03:03,  3.08it/s]  4%|▎         | 21/585 [00:28<02:56,  3.20it/s]  4%|▍         | 22/585 [00:28<02:51,  3.29it/s]  4%|▍         | 23/585 [00:28<02:47,  3.35it/s]  4%|▍         | 24/585 [00:29<03:56,  2.38it/s]  4%|▍         | 25/585 [00:29<03:32,  2.63it/s]  4%|▍         | 26/585 [00:30<03:16,  2.85it/s]  5%|▍         | 27/585 [00:30<03:04,  3.02it/s]  5%|▍         | 28/585 [00:30<02:56,  3.15it/s]  5%|▍         | 29/585 [00:30<02:51,  3.25it/s]  5%|▌         | 30/585 [00:31<02:47,  3.32it/s]  5%|▌         | 31/585 [00:31<02:44,  3.37it/s]  5%|▌         | 32/585 [00:31<02:42,  3.41it/s]  6%|▌         | 33/585 [00:32<02:40,  3.44it/s]  6%|▌         | 34/585 [00:32<04:05,  2.25it/s]  6%|▌         | 35/585 [00:33<03:38,  2.52it/s]  6%|▌         | 36/585 [00:33<03:19,  2.75it/s]  6%|▋         | 37/585 [00:33<03:06,  2.94it/s]  6%|▋         | 38/585 [00:34<02:56,  3.09it/s]  7%|▋         | 39/585 [00:34<02:50,  3.21it/s]  7%|▋         | 40/585 [00:34<02:45,  3.29it/s]  7%|▋         | 41/585 [00:34<02:42,  3.35it/s]  7%|▋         | 42/585 [00:35<02:39,  3.40it/s]  7%|▋         | 43/585 [00:36<04:19,  2.09it/s]  8%|▊         | 44/585 [00:36<03:47,  2.38it/s]  8%|▊         | 45/585 [00:36<03:25,  2.63it/s]  8%|▊         | 46/585 [00:36<03:09,  2.85it/s]  8%|▊         | 47/585 [00:37<02:58,  3.02it/s]  8%|▊         | 48/585 [00:37<02:50,  3.15it/s]  8%|▊         | 49/585 [00:37<02:44,  3.26it/s]  9%|▊         | 50/585 [00:38<02:40,  3.33it/s]  9%|▊         | 51/585 [00:38<02:38,  3.38it/s]  9%|▉         | 52/585 [00:38<03:12,  2.77it/s]  9%|▉         | 53/585 [00:39<02:59,  2.96it/s]  9%|▉         | 54/585 [00:39<02:50,  3.11it/s]  9%|▉         | 55/585 [00:39<02:44,  3.22it/s] 10%|▉         | 56/585 [00:40<02:40,  3.30it/s] 10%|▉         | 57/585 [00:40<02:37,  3.36it/s] 10%|▉         | 58/585 [00:40<02:34,  3.41it/s] 10%|█         | 59/585 [00:40<02:33,  3.44it/s] 10%|█         | 60/585 [00:41<02:31,  3.46it/s] 10%|█         | 61/585 [00:41<02:30,  3.47it/s] 11%|█         | 62/585 [00:42<05:39,  1.54it/s] 11%|█         | 63/585 [00:43<04:41,  1.85it/s] 11%|█         | 64/585 [00:43<04:01,  2.16it/s] 11%|█         | 65/585 [00:43<03:33,  2.44it/s] 11%|█▏        | 66/585 [00:44<03:13,  2.68it/s] 11%|█▏        | 67/585 [00:44<02:59,  2.88it/s] 12%|█▏        | 68/585 [00:44<02:49,  3.05it/s] 12%|█▏        | 69/585 [00:45<03:43,  2.31it/s] 12%|█▏        | 70/585 [00:45<03:20,  2.57it/s] 12%|█▏        | 71/585 [00:45<03:04,  2.79it/s] 12%|█▏        | 72/585 [00:46<02:52,  2.97it/s] 12%|█▏        | 73/585 [00:46<02:44,  3.12it/s] 13%|█▎        | 74/585 [00:46<02:38,  3.23it/s] 13%|█▎        | 75/585 [00:47<02:34,  3.31it/s] 13%|█▎        | 76/585 [00:47<02:31,  3.36it/s] 13%|█▎        | 77/585 [00:47<02:29,  3.40it/s] 13%|█▎        | 78/585 [00:47<02:27,  3.43it/s] 14%|█▎        | 79/585 [00:48<03:02,  2.77it/s] 14%|█▎        | 80/585 [00:48<02:50,  2.96it/s] 14%|█▍        | 81/585 [00:48<02:42,  3.10it/s] 14%|█▍        | 82/585 [00:49<02:36,  3.21it/s] 14%|█▍        | 83/585 [00:49<02:32,  3.29it/s] 14%|█▍        | 84/585 [00:49<02:29,  3.36it/s] 15%|█▍        | 85/585 [00:50<02:27,  3.40it/s] 15%|█▍        | 86/585 [00:50<02:25,  3.43it/s] 15%|█▍        | 87/585 [00:50<02:24,  3.45it/s] 15%|█▌        | 88/585 [00:50<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:51<04:01,  2.06it/s] 15%|█▌        | 90/585 [00:52<03:30,  2.35it/s] 16%|█▌        | 91/585 [00:52<03:09,  2.60it/s] 16%|█▌        | 92/585 [00:52<02:54,  2.82it/s] 16%|█▌        | 93/585 [00:53<02:44,  3.00it/s] 16%|█▌        | 94/585 [00:53<03:18,  2.47it/s] 16%|█▌        | 95/585 [00:53<03:00,  2.71it/s] 16%|█▋        | 96/585 [00:54<04:19,  1.88it/s] 17%|█▋        | 97/585 [00:56<06:14,  1.30it/s] 17%|█▋        | 98/585 [00:56<05:03,  1.61it/s] 17%|█▋        | 99/585 [00:56<04:13,  1.92it/s] 17%|█▋        | 100/585 [00:57<03:38,  2.22it/s] 17%|█▋        | 101/585 [00:57<03:30,  2.30it/s] 17%|█▋        | 102/585 [00:57<03:08,  2.56it/s] 18%|█▊        | 103/585 [00:58<02:53,  2.79it/s] 18%|█▊        | 104/585 [00:58<02:42,  2.97it/s] 18%|█▊        | 105/585 [00:58<02:34,  3.11it/s] 18%|█▊        | 106/585 [00:58<02:28,  3.22it/s] 18%|█▊        | 107/585 [00:59<02:25,  3.30it/s] 18%|█▊        | 108/585 [00:59<02:22,  3.35it/s] 19%|█▊        | 109/585 [00:59<02:20,  3.39it/s] 19%|█▉        | 110/585 [01:00<02:18,  3.42it/s] 19%|█▉        | 111/585 [01:00<02:17,  3.44it/s] 19%|█▉        | 112/585 [01:01<03:48,  2.07it/s] 19%|█▉        | 113/585 [01:01<03:19,  2.36it/s] 19%|█▉        | 114/585 [01:01<02:59,  2.62it/s] 20%|█▉        | 115/585 [01:02<02:46,  2.83it/s] 20%|█▉        | 116/585 [01:02<02:36,  3.00it/s] 20%|██        | 117/585 [01:02<02:29,  3.14it/s][INFO|trainer.py:2140] 2023-08-28 16:33:36,790 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:33:36,790 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:33:36,790 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.75it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.27it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.55it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.87it/s][A
  6%|▌         | 27/437 [00:01<00:09, 45.23it/s][A
  7%|▋         | 32/437 [00:01<00:23, 17.08it/s][A
  8%|▊         | 37/437 [00:01<00:18, 21.33it/s][A
 10%|▉         | 42/437 [00:01<00:15, 25.55it/s][A
 11%|█         | 47/437 [00:01<00:13, 29.50it/s][A
 12%|█▏        | 52/437 [00:02<00:19, 20.10it/s][A
 13%|█▎        | 57/437 [00:02<00:21, 17.68it/s][A
 14%|█▍        | 62/437 [00:02<00:17, 21.89it/s][A
 15%|█▌        | 67/437 [00:02<00:14, 25.92it/s][A
 16%|█▋        | 72/437 [00:02<00:12, 29.74it/s][A
 18%|█▊        | 77/437 [00:02<00:10, 33.12it/s][A
 19%|█▉        | 82/437 [00:02<00:09, 35.99it/s][A
 20%|█▉        | 87/437 [00:03<00:09, 38.27it/s][A
 21%|██        | 92/437 [00:03<00:11, 29.85it/s][A
 22%|██▏       | 97/437 [00:03<00:10, 33.23it/s][A
 23%|██▎       | 102/437 [00:03<00:09, 36.08it/s][A
 24%|██▍       | 107/437 [00:03<00:08, 38.36it/s][A
 26%|██▌       | 112/437 [00:04<00:13, 23.79it/s][A
 27%|██▋       | 117/437 [00:04<00:11, 27.70it/s][A
 28%|██▊       | 122/437 [00:04<00:10, 31.35it/s][A
 29%|██▉       | 127/437 [00:04<00:08, 34.48it/s][A
 30%|███       | 132/437 [00:04<00:08, 37.08it/s][A
 31%|███▏      | 137/437 [00:04<00:07, 39.17it/s][A
 32%|███▏      | 142/437 [00:04<00:07, 40.79it/s][A
 34%|███▎      | 147/437 [00:04<00:06, 41.76it/s][A
 35%|███▍      | 152/437 [00:04<00:06, 42.14it/s][A
 36%|███▌      | 157/437 [00:05<00:06, 42.52it/s][A
 37%|███▋      | 162/437 [00:05<00:06, 42.97it/s][A
 38%|███▊      | 167/437 [00:05<00:06, 43.53it/s][A
 39%|███▉      | 172/437 [00:05<00:06, 43.97it/s][A
 41%|████      | 177/437 [00:06<00:19, 13.18it/s][A
 42%|████▏     | 182/437 [00:07<00:20, 12.73it/s][A
 42%|████▏     | 185/437 [00:07<00:23, 10.63it/s][A
 43%|████▎     | 190/437 [00:07<00:17, 14.20it/s][A
 45%|████▍     | 195/437 [00:07<00:13, 18.15it/s][A
 46%|████▌     | 200/437 [00:07<00:10, 22.34it/s][A
 47%|████▋     | 205/437 [00:07<00:08, 26.45it/s][A
 48%|████▊     | 210/437 [00:07<00:07, 30.27it/s][A
 49%|████▉     | 215/437 [00:12<01:02,  3.52it/s][A
 50%|████▉     | 218/437 [00:12<00:59,  3.70it/s][A
 51%|█████     | 223/437 [00:12<00:40,  5.31it/s][A
 52%|█████▏    | 228/437 [00:12<00:28,  7.40it/s][A
 53%|█████▎    | 233/437 [00:13<00:20, 10.03it/s][A
 54%|█████▍    | 238/437 [00:13<00:15, 13.22it/s][A
 56%|█████▌    | 243/437 [00:13<00:11, 16.88it/s][A
 57%|█████▋    | 248/437 [00:13<00:09, 20.84it/s][A
 58%|█████▊    | 253/437 [00:13<00:07, 24.90it/s][A
 59%|█████▉    | 258/437 [00:13<00:06, 28.65it/s][A
 60%|██████    | 263/437 [00:13<00:05, 31.92it/s][A
 61%|██████▏   | 268/437 [00:13<00:04, 34.74it/s][A
 62%|██████▏   | 273/437 [00:13<00:04, 37.22it/s][A
 64%|██████▎   | 278/437 [00:14<00:04, 39.19it/s][A
 65%|██████▍   | 283/437 [00:14<00:03, 40.86it/s][A
 66%|██████▌   | 288/437 [00:14<00:03, 41.99it/s][A
 67%|██████▋   | 293/437 [00:14<00:03, 42.89it/s][A
 68%|██████▊   | 298/437 [00:14<00:06, 21.75it/s][A
 69%|██████▉   | 303/437 [00:15<00:05, 26.06it/s][A
 70%|███████   | 307/437 [00:16<00:09, 13.22it/s][A
 71%|███████   | 310/437 [00:16<00:10, 12.00it/s][A
 72%|███████▏  | 315/437 [00:16<00:07, 15.99it/s][A
 73%|███████▎  | 320/437 [00:16<00:05, 20.18it/s][A
 74%|███████▍  | 325/437 [00:16<00:04, 24.56it/s][A
 76%|███████▌  | 330/437 [00:16<00:03, 28.64it/s][A
 77%|███████▋  | 335/437 [00:16<00:03, 32.26it/s][A
 78%|███████▊  | 340/437 [00:17<00:05, 17.95it/s][A
 79%|███████▉  | 345/437 [00:17<00:04, 22.02it/s][A
 80%|████████  | 350/437 [00:17<00:03, 26.03it/s][A
 81%|████████  | 354/437 [00:17<00:03, 26.62it/s][A
 82%|████████▏ | 359/437 [00:17<00:02, 30.91it/s][A
 83%|████████▎ | 364/437 [00:17<00:02, 34.26it/s][A
 84%|████████▍ | 369/437 [00:17<00:01, 36.98it/s][A
 86%|████████▌ | 374/437 [00:17<00:01, 39.11it/s][A
 87%|████████▋ | 379/437 [00:18<00:01, 40.43it/s][A
 88%|████████▊ | 384/437 [00:18<00:01, 41.60it/s][A
 89%|████████▉ | 389/437 [00:18<00:01, 42.56it/s][A
 90%|█████████ | 394/437 [00:18<00:00, 43.05it/s][A
 91%|█████████▏| 399/437 [00:18<00:00, 43.15it/s][A
 92%|█████████▏| 404/437 [00:18<00:00, 43.39it/s][A
 94%|█████████▎| 409/437 [00:19<00:00, 43.86it/s][A
 95%|█████████▍| 414/437 [00:19<00:00, 23.85it/s][A
 96%|█████████▌| 419/437 [00:19<00:00, 27.80it/s][A
 97%|█████████▋| 424/437 [00:19<00:00, 31.39it/s][A
 98%|█████████▊| 429/437 [00:19<00:00, 34.52it/s][A
 99%|█████████▉| 434/437 [00:19<00:00, 37.14it/s][A                                                 
                                                 [A 20%|██        | 117/585 [01:22<02:29,  3.14it/s]
100%|██████████| 437/437 [00:19<00:00, 37.14it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:33:59,714 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 16:34:03,109 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:36:11,762 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:36:17,914 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:36:19,633 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [08:45<18:03:07, 139.16s/it] 20%|██        | 119/585 [08:46<12:39:20, 97.77s/it]  21%|██        | 120/585 [08:47<8:51:03, 68.52s/it]  21%|██        | 121/585 [08:47<6:11:36, 48.05s/it] 21%|██        | 122/585 [08:47<4:20:13, 33.72s/it] 21%|██        | 123/585 [08:48<3:02:25, 23.69s/it] 21%|██        | 124/585 [08:48<2:08:04, 16.67s/it] 21%|██▏       | 125/585 [08:48<1:30:06, 11.75s/it] 22%|██▏       | 126/585 [08:48<1:03:35,  8.31s/it] 22%|██▏       | 127/585 [08:49<46:02,  6.03s/it]   22%|██▏       | 128/585 [08:49<32:48,  4.31s/it] 22%|██▏       | 129/585 [08:50<23:33,  3.10s/it] 22%|██▏       | 130/585 [08:50<17:06,  2.26s/it] 22%|██▏       | 131/585 [08:50<12:35,  1.66s/it] 23%|██▎       | 132/585 [08:51<09:26,  1.25s/it] 23%|██▎       | 133/585 [08:51<07:14,  1.04it/s] 23%|██▎       | 134/585 [08:51<05:41,  1.32it/s] 23%|██▎       | 135/585 [08:51<04:37,  1.62it/s] 23%|██▎       | 136/585 [08:52<03:51,  1.94it/s] 23%|██▎       | 137/585 [08:52<04:03,  1.84it/s] 24%|██▎       | 138/585 [08:53<03:28,  2.14it/s] 24%|██▍       | 139/585 [08:53<03:03,  2.43it/s] 24%|██▍       | 140/585 [08:53<02:46,  2.67it/s] 24%|██▍       | 141/585 [08:53<02:34,  2.88it/s] 24%|██▍       | 142/585 [08:54<02:25,  3.04it/s] 24%|██▍       | 143/585 [08:54<02:19,  3.17it/s] 25%|██▍       | 144/585 [08:54<02:15,  3.27it/s] 25%|██▍       | 145/585 [08:55<02:11,  3.34it/s] 25%|██▍       | 146/585 [08:55<02:09,  3.39it/s] 25%|██▌       | 147/585 [08:56<02:59,  2.44it/s] 25%|██▌       | 148/585 [08:56<02:42,  2.68it/s] 25%|██▌       | 149/585 [08:56<02:31,  2.88it/s] 26%|██▌       | 150/585 [08:56<02:22,  3.04it/s] 26%|██▌       | 151/585 [08:57<02:16,  3.17it/s] 26%|██▌       | 152/585 [08:57<02:12,  3.26it/s] 26%|██▌       | 153/585 [08:57<02:09,  3.33it/s] 26%|██▋       | 154/585 [08:58<02:07,  3.39it/s] 26%|██▋       | 155/585 [08:58<02:05,  3.42it/s] 27%|██▋       | 156/585 [08:58<02:04,  3.44it/s] 27%|██▋       | 157/585 [08:59<02:47,  2.56it/s] 27%|██▋       | 158/585 [08:59<02:33,  2.78it/s] 27%|██▋       | 159/585 [08:59<02:23,  2.96it/s] 27%|██▋       | 160/585 [09:00<02:38,  2.69it/s] 28%|██▊       | 161/585 [09:00<02:26,  2.89it/s] 28%|██▊       | 162/585 [09:00<02:18,  3.05it/s] 28%|██▊       | 163/585 [09:01<02:12,  3.18it/s] 28%|██▊       | 164/585 [09:01<02:08,  3.27it/s] 28%|██▊       | 165/585 [09:01<02:05,  3.34it/s] 28%|██▊       | 166/585 [09:02<02:43,  2.56it/s] 29%|██▊       | 167/585 [09:02<02:29,  2.79it/s] 29%|██▊       | 168/585 [09:02<02:20,  2.97it/s] 29%|██▉       | 169/585 [09:03<02:13,  3.11it/s] 29%|██▉       | 170/585 [09:03<02:08,  3.22it/s] 29%|██▉       | 171/585 [09:03<02:05,  3.30it/s] 29%|██▉       | 172/585 [09:04<02:03,  3.36it/s] 30%|██▉       | 173/585 [09:04<02:01,  3.40it/s] 30%|██▉       | 174/585 [09:04<01:59,  3.43it/s] 30%|██▉       | 175/585 [09:04<01:58,  3.45it/s] 30%|███       | 176/585 [09:05<02:28,  2.75it/s] 30%|███       | 177/585 [09:05<02:18,  2.94it/s] 30%|███       | 178/585 [09:06<02:11,  3.09it/s] 31%|███       | 179/585 [09:06<02:06,  3.21it/s] 31%|███       | 180/585 [09:06<02:03,  3.29it/s] 31%|███       | 181/585 [09:06<02:00,  3.35it/s] 31%|███       | 182/585 [09:07<01:58,  3.40it/s] 31%|███▏      | 183/585 [09:07<01:57,  3.43it/s] 31%|███▏      | 184/585 [09:07<01:56,  3.46it/s] 32%|███▏      | 185/585 [09:07<01:55,  3.47it/s] 32%|███▏      | 186/585 [09:08<02:22,  2.81it/s] 32%|███▏      | 187/585 [09:08<02:13,  2.98it/s] 32%|███▏      | 188/585 [09:09<02:07,  3.12it/s] 32%|███▏      | 189/585 [09:09<02:02,  3.23it/s] 32%|███▏      | 190/585 [09:09<01:59,  3.30it/s] 33%|███▎      | 191/585 [09:09<01:57,  3.36it/s] 33%|███▎      | 192/585 [09:10<01:55,  3.41it/s] 33%|███▎      | 193/585 [09:10<01:54,  3.44it/s] 33%|███▎      | 194/585 [09:10<01:53,  3.46it/s] 33%|███▎      | 195/585 [09:11<01:52,  3.46it/s] 34%|███▎      | 196/585 [09:11<02:40,  2.42it/s] 34%|███▎      | 197/585 [09:12<02:25,  2.67it/s] 34%|███▍      | 198/585 [09:12<02:14,  2.87it/s] 34%|███▍      | 199/585 [09:12<02:07,  3.03it/s] 34%|███▍      | 200/585 [09:12<02:01,  3.16it/s] 34%|███▍      | 201/585 [09:13<01:58,  3.25it/s] 35%|███▍      | 202/585 [09:13<01:55,  3.32it/s] 35%|███▍      | 203/585 [09:13<01:53,  3.38it/s] 35%|███▍      | 204/585 [09:14<01:51,  3.42it/s] 35%|███▌      | 205/585 [09:14<01:50,  3.45it/s] 35%|███▌      | 206/585 [09:15<02:59,  2.11it/s] 35%|███▌      | 207/585 [09:15<02:37,  2.40it/s] 36%|███▌      | 208/585 [09:15<02:22,  2.65it/s] 36%|███▌      | 209/585 [09:16<02:11,  2.86it/s] 36%|███▌      | 210/585 [09:16<02:03,  3.03it/s] 36%|███▌      | 211/585 [09:16<01:58,  3.16it/s] 36%|███▌      | 212/585 [09:16<01:54,  3.26it/s] 36%|███▋      | 213/585 [09:17<01:51,  3.32it/s] 37%|███▋      | 214/585 [09:17<01:49,  3.37it/s] 37%|███▋      | 215/585 [09:18<02:14,  2.75it/s] 37%|███▋      | 216/585 [09:18<02:05,  2.94it/s] 37%|███▋      | 217/585 [09:18<01:59,  3.08it/s] 37%|███▋      | 218/585 [09:18<01:54,  3.20it/s] 37%|███▋      | 219/585 [09:19<01:51,  3.28it/s] 38%|███▊      | 220/585 [09:19<01:49,  3.35it/s] 38%|███▊      | 221/585 [09:19<01:47,  3.39it/s] 38%|███▊      | 222/585 [09:20<01:45,  3.43it/s] 38%|███▊      | 223/585 [09:20<01:44,  3.45it/s] 38%|███▊      | 224/585 [09:20<01:44,  3.47it/s] 38%|███▊      | 225/585 [09:21<03:32,  1.69it/s] 39%|███▊      | 226/585 [09:22<02:59,  2.00it/s] 39%|███▉      | 227/585 [09:22<02:35,  2.30it/s] 39%|███▉      | 228/585 [09:22<02:19,  2.56it/s] 39%|███▉      | 229/585 [09:23<02:07,  2.78it/s] 39%|███▉      | 230/585 [09:23<01:59,  2.96it/s] 39%|███▉      | 231/585 [09:23<01:53,  3.11it/s] 40%|███▉      | 232/585 [09:23<01:49,  3.21it/s] 40%|███▉      | 233/585 [09:24<02:59,  1.96it/s] 40%|████      | 234/585 [09:25<02:35,  2.25it/s][INFO|trainer.py:2140] 2023-08-28 16:41:59,335 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:41:59,335 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:41:59,335 >>   Batch size = 8
{'eval_loss': 1.038416862487793, 'eval_runtime': 19.7756, 'eval_samples_per_second': 176.582, 'eval_steps_per_second': 22.098, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.34it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.81it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.57it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.76it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.08it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.80it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.71it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.57it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.56it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.75it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.84it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.78it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.66it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.56it/s][A
 18%|█▊        | 77/437 [00:02<00:08, 44.59it/s][A
 19%|█▉        | 82/437 [00:02<00:22, 15.77it/s][A
 20%|█▉        | 87/437 [00:02<00:17, 19.60it/s][A
 21%|██        | 92/437 [00:02<00:14, 23.62it/s][A
 22%|██▏       | 97/437 [00:02<00:12, 27.55it/s][A
 23%|██▎       | 102/437 [00:02<00:10, 31.22it/s][A
 24%|██▍       | 107/437 [00:03<00:09, 34.37it/s][A
 26%|██▌       | 112/437 [00:03<00:08, 37.02it/s][A
 27%|██▋       | 117/437 [00:03<00:08, 39.00it/s][A
 28%|██▊       | 122/437 [00:03<00:07, 40.24it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 41.09it/s][A
 30%|███       | 132/437 [00:03<00:07, 42.04it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 42.89it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.51it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.96it/s][A
 35%|███▍      | 152/437 [00:04<00:06, 44.31it/s][A
 36%|███▌      | 157/437 [00:04<00:06, 44.54it/s][A
 37%|███▋      | 162/437 [00:04<00:06, 44.37it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 44.18it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 44.08it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.17it/s][A
 42%|████▏     | 182/437 [00:05<00:05, 44.28it/s][A
 43%|████▎     | 187/437 [00:05<00:12, 20.57it/s][A
 44%|████▍     | 192/437 [00:05<00:09, 24.58it/s][A
 45%|████▌     | 197/437 [00:05<00:08, 28.46it/s][A
 46%|████▌     | 202/437 [00:05<00:07, 32.00it/s][A
 47%|████▋     | 207/437 [00:05<00:06, 35.05it/s][A
 49%|████▊     | 212/437 [00:05<00:06, 37.43it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 39.43it/s][A
 51%|█████     | 222/437 [00:06<00:05, 40.85it/s][A
 52%|█████▏    | 227/437 [00:06<00:07, 26.87it/s][A
 53%|█████▎    | 232/437 [00:06<00:06, 30.56it/s][A
 54%|█████▍    | 237/437 [00:06<00:05, 33.82it/s][A
 55%|█████▌    | 242/437 [00:06<00:05, 36.58it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 38.81it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 40.51it/s][A
 59%|█████▉    | 257/437 [00:07<00:04, 41.76it/s][A
 60%|█████▉    | 262/437 [00:07<00:04, 42.66it/s][A
 61%|██████    | 267/437 [00:07<00:03, 42.86it/s][A
 62%|██████▏   | 272/437 [00:07<00:03, 42.93it/s][A
 63%|██████▎   | 277/437 [00:07<00:03, 43.23it/s][A
 65%|██████▍   | 282/437 [00:07<00:03, 43.65it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 44.03it/s][A
 67%|██████▋   | 292/437 [00:08<00:04, 29.49it/s][A
 68%|██████▊   | 297/437 [00:08<00:04, 32.90it/s][A
 69%|██████▉   | 302/437 [00:08<00:03, 35.77it/s][A
 70%|███████   | 307/437 [00:08<00:03, 38.12it/s][A
 71%|███████▏  | 312/437 [00:08<00:03, 40.01it/s][A
 73%|███████▎  | 317/437 [00:08<00:02, 41.40it/s][A
 74%|███████▎  | 322/437 [00:08<00:02, 42.43it/s][A
 75%|███████▍  | 327/437 [00:08<00:02, 43.02it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 43.03it/s][A
 77%|███████▋  | 337/437 [00:09<00:02, 43.10it/s][A
 78%|███████▊  | 342/437 [00:09<00:02, 43.42it/s][A
 79%|███████▉  | 347/437 [00:09<00:02, 43.81it/s][A
 81%|████████  | 352/437 [00:09<00:01, 44.14it/s][A
 82%|████████▏ | 357/437 [00:09<00:01, 44.44it/s][A
 83%|████████▎ | 362/437 [00:09<00:01, 44.58it/s][A
 84%|████████▍ | 367/437 [00:09<00:01, 44.63it/s][A
 85%|████████▌ | 372/437 [00:09<00:01, 44.61it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 44.25it/s][A
 87%|████████▋ | 382/437 [00:10<00:01, 44.06it/s][A
 89%|████████▊ | 387/437 [00:10<00:01, 44.09it/s][A
 90%|████████▉ | 392/437 [00:10<00:01, 44.17it/s][A
 91%|█████████ | 397/437 [00:10<00:00, 44.44it/s][A
 92%|█████████▏| 402/437 [00:10<00:01, 32.80it/s][A
 93%|█████████▎| 406/437 [00:13<00:02, 10.81it/s][A
 94%|█████████▎| 409/437 [00:16<00:08,  3.26it/s][A
 94%|█████████▍| 411/437 [00:16<00:10,  2.49it/s][A
 95%|█████████▌| 416/437 [00:16<00:05,  3.87it/s][A
 96%|█████████▋| 421/437 [00:16<00:02,  5.69it/s][A
 97%|█████████▋| 426/437 [00:17<00:01,  8.03it/s][A
 99%|█████████▊| 431/437 [00:17<00:00, 10.93it/s][A
100%|█████████▉| 436/437 [00:17<00:00, 14.36it/s][A                                                 
                                                 [A 40%|████      | 234/585 [09:42<02:35,  2.25it/s]
100%|██████████| 437/437 [00:17<00:00, 14.36it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:42:19,511 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 16:42:22,193 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:43:59,746 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:44:05,844 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:44:07,638 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [14:59<9:47:20, 100.69s/it] 40%|████      | 236/585 [15:01<6:53:14, 71.04s/it]  41%|████      | 237/585 [15:01<4:48:56, 49.82s/it] 41%|████      | 238/585 [15:02<3:22:10, 34.96s/it] 41%|████      | 239/585 [15:02<2:21:37, 24.56s/it] 41%|████      | 240/585 [15:02<1:39:20, 17.28s/it] 41%|████      | 241/585 [15:04<1:11:33, 12.48s/it] 41%|████▏     | 242/585 [15:04<50:26,  8.82s/it]   42%|████▏     | 243/585 [15:04<35:42,  6.26s/it] 42%|████▏     | 244/585 [15:04<25:24,  4.47s/it] 42%|████▏     | 245/585 [15:05<18:13,  3.22s/it] 42%|████▏     | 246/585 [15:05<13:12,  2.34s/it] 42%|████▏     | 247/585 [15:05<09:42,  1.72s/it] 42%|████▏     | 248/585 [15:06<07:48,  1.39s/it] 43%|████▎     | 249/585 [15:06<05:56,  1.06s/it] 43%|████▎     | 250/585 [15:07<04:38,  1.20it/s] 43%|████▎     | 251/585 [15:07<03:43,  1.50it/s] 43%|████▎     | 252/585 [15:07<03:04,  1.80it/s] 43%|████▎     | 253/585 [15:07<02:37,  2.10it/s] 43%|████▎     | 254/585 [15:08<02:19,  2.38it/s] 44%|████▎     | 255/585 [15:08<02:05,  2.62it/s] 44%|████▍     | 256/585 [15:08<01:56,  2.82it/s] 44%|████▍     | 257/585 [15:09<01:49,  2.99it/s] 44%|████▍     | 258/585 [15:09<02:04,  2.62it/s] 44%|████▍     | 259/585 [15:09<01:55,  2.83it/s] 44%|████▍     | 260/585 [15:10<01:48,  3.00it/s] 45%|████▍     | 261/585 [15:10<01:43,  3.14it/s] 45%|████▍     | 262/585 [15:10<01:39,  3.24it/s] 45%|████▍     | 263/585 [15:11<01:37,  3.31it/s] 45%|████▌     | 264/585 [15:11<01:35,  3.36it/s] 45%|████▌     | 265/585 [15:11<01:34,  3.40it/s] 45%|████▌     | 266/585 [15:11<01:33,  3.43it/s] 46%|████▌     | 267/585 [15:12<01:32,  3.45it/s] 46%|████▌     | 268/585 [15:12<02:20,  2.26it/s] 46%|████▌     | 269/585 [15:13<02:05,  2.52it/s] 46%|████▌     | 270/585 [15:13<01:54,  2.75it/s] 46%|████▋     | 271/585 [15:13<01:47,  2.93it/s] 46%|████▋     | 272/585 [15:14<01:41,  3.08it/s] 47%|████▋     | 273/585 [15:14<01:37,  3.19it/s] 47%|████▋     | 274/585 [15:14<01:34,  3.27it/s] 47%|████▋     | 275/585 [15:14<01:32,  3.34it/s] 47%|████▋     | 276/585 [15:15<01:31,  3.38it/s] 47%|████▋     | 277/585 [15:16<02:26,  2.10it/s] 48%|████▊     | 278/585 [15:16<02:08,  2.38it/s] 48%|████▊     | 279/585 [15:16<01:56,  2.63it/s] 48%|████▊     | 280/585 [15:17<01:47,  2.84it/s] 48%|████▊     | 281/585 [15:17<01:41,  3.01it/s] 48%|████▊     | 282/585 [15:17<01:36,  3.13it/s] 48%|████▊     | 283/585 [15:17<01:33,  3.23it/s] 49%|████▊     | 284/585 [15:18<01:30,  3.31it/s] 49%|████▊     | 285/585 [15:18<01:29,  3.36it/s] 49%|████▉     | 286/585 [15:19<02:14,  2.22it/s] 49%|████▉     | 287/585 [15:19<01:59,  2.49it/s] 49%|████▉     | 288/585 [15:19<01:49,  2.72it/s] 49%|████▉     | 289/585 [15:20<01:41,  2.92it/s] 50%|████▉     | 290/585 [15:20<01:36,  3.07it/s] 50%|████▉     | 291/585 [15:20<01:32,  3.19it/s] 50%|████▉     | 292/585 [15:20<01:29,  3.27it/s] 50%|█████     | 293/585 [15:21<01:27,  3.33it/s] 50%|█████     | 294/585 [15:21<01:26,  3.38it/s] 50%|█████     | 295/585 [15:22<02:45,  1.75it/s] 51%|█████     | 296/585 [15:23<02:20,  2.06it/s] 51%|█████     | 297/585 [15:23<02:02,  2.35it/s] 51%|█████     | 298/585 [15:23<01:50,  2.61it/s] 51%|█████     | 299/585 [15:23<01:41,  2.82it/s] 51%|█████▏    | 300/585 [15:24<01:35,  2.99it/s] 51%|█████▏    | 301/585 [15:24<01:30,  3.13it/s] 52%|█████▏    | 302/585 [15:24<01:27,  3.23it/s] 52%|█████▏    | 303/585 [15:26<02:57,  1.59it/s] 52%|█████▏    | 304/585 [15:26<02:27,  1.90it/s] 52%|█████▏    | 305/585 [15:26<02:07,  2.20it/s] 52%|█████▏    | 306/585 [15:26<01:52,  2.48it/s] 52%|█████▏    | 307/585 [15:27<01:42,  2.72it/s] 53%|█████▎    | 308/585 [15:27<01:35,  2.91it/s] 53%|█████▎    | 309/585 [15:27<01:30,  3.06it/s] 53%|█████▎    | 310/585 [15:28<01:42,  2.67it/s] 53%|█████▎    | 311/585 [15:28<01:35,  2.87it/s] 53%|█████▎    | 312/585 [15:28<01:29,  3.03it/s] 54%|█████▎    | 313/585 [15:29<01:26,  3.16it/s] 54%|█████▎    | 314/585 [15:29<01:23,  3.25it/s] 54%|█████▍    | 315/585 [15:29<01:21,  3.32it/s] 54%|█████▍    | 316/585 [15:30<01:19,  3.37it/s] 54%|█████▍    | 317/585 [15:30<01:18,  3.41it/s] 54%|█████▍    | 318/585 [15:30<01:17,  3.43it/s] 55%|█████▍    | 319/585 [15:30<01:17,  3.45it/s] 55%|█████▍    | 320/585 [15:31<01:44,  2.54it/s] 55%|█████▍    | 321/585 [15:33<03:34,  1.23it/s] 55%|█████▌    | 322/585 [15:33<02:52,  1.53it/s] 55%|█████▌    | 323/585 [15:33<02:22,  1.83it/s] 55%|█████▌    | 324/585 [15:34<02:02,  2.13it/s] 56%|█████▌    | 325/585 [15:34<02:08,  2.03it/s] 56%|█████▌    | 326/585 [15:35<02:51,  1.51it/s] 56%|█████▌    | 327/585 [15:36<02:22,  1.81it/s] 56%|█████▌    | 328/585 [15:36<02:01,  2.11it/s] 56%|█████▌    | 329/585 [15:36<01:47,  2.39it/s] 56%|█████▋    | 330/585 [15:36<01:37,  2.62it/s] 57%|█████▋    | 331/585 [15:37<01:29,  2.83it/s] 57%|█████▋    | 332/585 [15:37<01:24,  2.99it/s] 57%|█████▋    | 333/585 [15:37<01:21,  3.11it/s] 57%|█████▋    | 334/585 [15:38<02:01,  2.07it/s] 57%|█████▋    | 335/585 [15:38<01:46,  2.35it/s] 57%|█████▋    | 336/585 [15:39<01:35,  2.61it/s] 58%|█████▊    | 337/585 [15:39<01:27,  2.82it/s] 58%|█████▊    | 338/585 [15:39<01:22,  2.99it/s] 58%|█████▊    | 339/585 [15:40<01:18,  3.13it/s] 58%|█████▊    | 340/585 [15:40<01:15,  3.22it/s] 58%|█████▊    | 341/585 [15:40<01:14,  3.29it/s] 58%|█████▊    | 342/585 [15:41<01:12,  3.35it/s] 59%|█████▊    | 343/585 [15:42<02:33,  1.58it/s] 59%|█████▉    | 344/585 [15:42<02:07,  1.89it/s] 59%|█████▉    | 345/585 [15:42<01:49,  2.19it/s] 59%|█████▉    | 346/585 [15:43<01:36,  2.46it/s] 59%|█████▉    | 347/585 [15:43<01:28,  2.70it/s] 59%|█████▉    | 348/585 [15:43<01:21,  2.90it/s] 60%|█████▉    | 349/585 [15:44<01:17,  3.06it/s] 60%|█████▉    | 350/585 [15:44<01:43,  2.27it/s] 60%|██████    | 351/585 [15:45<01:32,  2.54it/s][INFO|trainer.py:2140] 2023-08-28 16:48:19,270 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:48:19,270 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:48:19,270 >>   Batch size = 8
{'eval_loss': 1.022438406944275, 'eval_runtime': 17.3781, 'eval_samples_per_second': 200.943, 'eval_steps_per_second': 25.147, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.79it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.07it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.05it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.17it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.49it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.25it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.09it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.76it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.76it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.84it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.01it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 45.04it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.91it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.71it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.65it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.57it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.60it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.53it/s][A
 22%|██▏       | 97/437 [00:02<00:18, 18.66it/s][A
 23%|██▎       | 102/437 [00:02<00:14, 22.66it/s][A
 24%|██▍       | 106/437 [00:03<00:15, 20.84it/s][A
 25%|██▌       | 111/437 [00:03<00:12, 25.17it/s][A
 27%|██▋       | 116/437 [00:03<00:11, 29.10it/s][A
 28%|██▊       | 121/437 [00:03<00:09, 32.59it/s][A
 29%|██▉       | 126/437 [00:03<00:08, 35.58it/s][A
 30%|██▉       | 131/437 [00:03<00:08, 38.10it/s][A
 31%|███       | 136/437 [00:03<00:07, 40.02it/s][A
 32%|███▏      | 141/437 [00:03<00:07, 41.47it/s][A
 33%|███▎      | 146/437 [00:03<00:06, 42.26it/s][A
 35%|███▍      | 151/437 [00:04<00:06, 42.67it/s][A
 36%|███▌      | 156/437 [00:04<00:06, 43.00it/s][A
 37%|███▋      | 161/437 [00:04<00:06, 43.37it/s][A
 38%|███▊      | 166/437 [00:04<00:06, 43.79it/s][A
 39%|███▉      | 171/437 [00:04<00:06, 44.15it/s][A
 40%|████      | 176/437 [00:04<00:05, 44.34it/s][A
 41%|████▏     | 181/437 [00:04<00:05, 44.69it/s][A
 43%|████▎     | 186/437 [00:04<00:05, 44.71it/s][A
 44%|████▎     | 191/437 [00:04<00:05, 44.58it/s][A
 45%|████▍     | 196/437 [00:05<00:05, 44.35it/s][A
 46%|████▌     | 201/437 [00:05<00:05, 44.20it/s][A
 47%|████▋     | 206/437 [00:05<00:10, 22.02it/s][A
 48%|████▊     | 211/437 [00:05<00:08, 26.02it/s][A
 49%|████▉     | 216/437 [00:05<00:07, 29.77it/s][A
 51%|█████     | 221/437 [00:05<00:06, 33.17it/s][A
 52%|█████▏    | 226/437 [00:06<00:05, 35.97it/s][A
 53%|█████▎    | 231/437 [00:06<00:05, 38.32it/s][A
 54%|█████▍    | 236/437 [00:06<00:05, 40.10it/s][A
 55%|█████▌    | 241/437 [00:06<00:04, 41.41it/s][A
 56%|█████▋    | 246/437 [00:06<00:04, 41.85it/s][A
 57%|█████▋    | 251/437 [00:06<00:04, 42.30it/s][A
 59%|█████▊    | 256/437 [00:06<00:04, 42.90it/s][A
 60%|█████▉    | 261/437 [00:06<00:04, 43.46it/s][A
 61%|██████    | 266/437 [00:06<00:03, 43.92it/s][A
 62%|██████▏   | 271/437 [00:07<00:03, 44.23it/s][A
 63%|██████▎   | 276/437 [00:11<00:40,  3.99it/s][A
 64%|██████▍   | 280/437 [00:11<00:31,  4.98it/s][A
 65%|██████▌   | 285/437 [00:11<00:22,  6.90it/s][A
 66%|██████▋   | 290/437 [00:11<00:15,  9.36it/s][A
 68%|██████▊   | 295/437 [00:11<00:11, 12.35it/s][A
 69%|██████▊   | 300/437 [00:11<00:08, 15.85it/s][A
 70%|██████▉   | 305/437 [00:11<00:06, 19.73it/s][A
 71%|███████   | 310/437 [00:11<00:05, 23.77it/s][A
 72%|███████▏  | 315/437 [00:11<00:04, 27.64it/s][A
 73%|███████▎  | 320/437 [00:12<00:03, 31.03it/s][A
 74%|███████▍  | 325/437 [00:12<00:03, 33.93it/s][A
 76%|███████▌  | 330/437 [00:12<00:02, 36.44it/s][A
 77%|███████▋  | 335/437 [00:12<00:02, 38.52it/s][A
 78%|███████▊  | 340/437 [00:12<00:02, 40.33it/s][A
 79%|███████▉  | 345/437 [00:12<00:02, 41.64it/s][A
 80%|████████  | 350/437 [00:12<00:02, 42.64it/s][A
 81%|████████  | 355/437 [00:12<00:01, 43.31it/s][A
 82%|████████▏ | 360/437 [00:13<00:01, 43.68it/s][A
 84%|████████▎ | 365/437 [00:13<00:01, 43.72it/s][A
 85%|████████▍ | 370/437 [00:13<00:01, 43.76it/s][A
 86%|████████▌ | 375/437 [00:13<00:01, 43.76it/s][A
 87%|████████▋ | 380/437 [00:13<00:01, 44.02it/s][A
 88%|████████▊ | 385/437 [00:13<00:01, 44.20it/s][A
 89%|████████▉ | 390/437 [00:13<00:01, 44.46it/s][A
 90%|█████████ | 395/437 [00:13<00:00, 44.66it/s][A
 92%|█████████▏| 400/437 [00:13<00:00, 44.70it/s][A
 93%|█████████▎| 405/437 [00:14<00:00, 44.56it/s][A
 94%|█████████▍| 410/437 [00:14<00:00, 30.54it/s][A
 95%|█████████▍| 415/437 [00:14<00:00, 33.78it/s][A
 96%|█████████▌| 420/437 [00:14<00:00, 36.50it/s][A
 97%|█████████▋| 425/437 [00:14<00:00, 38.75it/s][A
 98%|█████████▊| 430/437 [00:14<00:00, 40.43it/s][A
100%|█████████▉| 435/437 [00:14<00:00, 41.43it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [16:00<01:32,  2.54it/s]
100%|██████████| 437/437 [00:14<00:00, 41.43it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:48:37,325 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 16:48:39,453 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:50:37,834 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:50:44,869 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:50:46,752 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [21:40<6:55:38, 107.03s/it] 60%|██████    | 353/585 [21:42<4:51:02, 75.27s/it]  61%|██████    | 354/585 [21:42<3:23:11, 52.78s/it] 61%|██████    | 355/585 [21:42<2:21:57, 37.03s/it] 61%|██████    | 356/585 [21:43<1:39:15, 26.01s/it] 61%|██████    | 357/585 [21:43<1:09:30, 18.29s/it] 61%|██████    | 358/585 [21:43<48:46, 12.89s/it]   61%|██████▏   | 359/585 [21:43<34:19,  9.11s/it] 62%|██████▏   | 360/585 [21:44<24:14,  6.47s/it] 62%|██████▏   | 361/585 [21:44<17:27,  4.68s/it] 62%|██████▏   | 362/585 [21:44<12:29,  3.36s/it] 62%|██████▏   | 363/585 [21:45<09:01,  2.44s/it] 62%|██████▏   | 364/585 [21:45<06:36,  1.80s/it] 62%|██████▏   | 365/585 [21:45<04:55,  1.34s/it] 63%|██████▎   | 366/585 [21:46<03:45,  1.03s/it] 63%|██████▎   | 367/585 [21:46<02:55,  1.24it/s] 63%|██████▎   | 368/585 [21:46<02:21,  1.53it/s] 63%|██████▎   | 369/585 [21:47<01:57,  1.84it/s] 63%|██████▎   | 370/585 [21:47<01:40,  2.14it/s] 63%|██████▎   | 371/585 [21:47<01:50,  1.94it/s] 64%|██████▎   | 372/585 [21:48<01:35,  2.23it/s] 64%|██████▍   | 373/585 [21:48<01:25,  2.49it/s] 64%|██████▍   | 374/585 [21:48<01:17,  2.72it/s] 64%|██████▍   | 375/585 [21:49<01:12,  2.90it/s] 64%|██████▍   | 376/585 [21:49<01:08,  3.04it/s] 64%|██████▍   | 377/585 [21:49<01:06,  3.15it/s] 65%|██████▍   | 378/585 [21:49<01:04,  3.23it/s] 65%|██████▍   | 379/585 [21:50<01:02,  3.29it/s] 65%|██████▍   | 380/585 [21:50<01:01,  3.33it/s] 65%|██████▌   | 381/585 [21:51<01:42,  1.99it/s] 65%|██████▌   | 382/585 [21:51<01:28,  2.28it/s] 65%|██████▌   | 383/585 [21:52<01:19,  2.54it/s] 66%|██████▌   | 384/585 [21:52<01:13,  2.75it/s] 66%|██████▌   | 385/585 [21:52<01:08,  2.93it/s] 66%|██████▌   | 386/585 [21:52<01:04,  3.06it/s] 66%|██████▌   | 387/585 [21:53<01:02,  3.17it/s] 66%|██████▋   | 388/585 [21:53<01:00,  3.24it/s] 66%|██████▋   | 389/585 [21:54<01:17,  2.53it/s] 67%|██████▋   | 390/585 [21:54<01:10,  2.75it/s] 67%|██████▋   | 391/585 [21:54<01:06,  2.92it/s] 67%|██████▋   | 392/585 [21:55<01:03,  3.06it/s] 67%|██████▋   | 393/585 [21:55<01:00,  3.17it/s] 67%|██████▋   | 394/585 [21:55<00:58,  3.25it/s] 68%|██████▊   | 395/585 [21:55<00:57,  3.30it/s] 68%|██████▊   | 396/585 [21:56<00:56,  3.34it/s] 68%|██████▊   | 397/585 [21:56<00:55,  3.37it/s] 68%|██████▊   | 398/585 [21:56<00:55,  3.39it/s] 68%|██████▊   | 399/585 [21:57<01:10,  2.63it/s] 68%|██████▊   | 400/585 [21:57<01:05,  2.83it/s] 69%|██████▊   | 401/585 [21:57<01:01,  2.99it/s] 69%|██████▊   | 402/585 [21:58<00:58,  3.11it/s] 69%|██████▉   | 403/585 [21:58<00:56,  3.20it/s] 69%|██████▉   | 404/585 [21:58<00:55,  3.27it/s] 69%|██████▉   | 405/585 [21:59<00:54,  3.32it/s] 69%|██████▉   | 406/585 [21:59<00:53,  3.35it/s] 70%|██████▉   | 407/585 [21:59<00:52,  3.38it/s] 70%|██████▉   | 408/585 [21:59<00:52,  3.39it/s] 70%|██████▉   | 409/585 [22:00<01:01,  2.88it/s] 70%|███████   | 410/585 [22:00<00:57,  3.03it/s] 70%|███████   | 411/585 [22:01<00:55,  3.14it/s] 70%|███████   | 412/585 [22:01<00:53,  3.22it/s] 71%|███████   | 413/585 [22:01<00:52,  3.29it/s] 71%|███████   | 414/585 [22:01<00:51,  3.33it/s] 71%|███████   | 415/585 [22:02<00:50,  3.36it/s] 71%|███████   | 416/585 [22:02<00:49,  3.38it/s] 71%|███████▏  | 417/585 [22:02<00:49,  3.40it/s] 71%|███████▏  | 418/585 [22:03<00:48,  3.41it/s] 72%|███████▏  | 419/585 [22:03<01:00,  2.76it/s] 72%|███████▏  | 420/585 [22:03<00:56,  2.93it/s] 72%|███████▏  | 421/585 [22:04<00:53,  3.07it/s] 72%|███████▏  | 422/585 [22:04<00:51,  3.17it/s] 72%|███████▏  | 423/585 [22:04<00:49,  3.24it/s] 72%|███████▏  | 424/585 [22:05<00:48,  3.30it/s] 73%|███████▎  | 425/585 [22:05<00:47,  3.34it/s] 73%|███████▎  | 426/585 [22:05<00:47,  3.36it/s] 73%|███████▎  | 427/585 [22:05<00:46,  3.39it/s] 73%|███████▎  | 428/585 [22:06<00:46,  3.40it/s] 73%|███████▎  | 429/585 [22:06<00:56,  2.77it/s] 74%|███████▎  | 430/585 [22:07<00:52,  2.94it/s] 74%|███████▎  | 431/585 [22:07<00:50,  3.07it/s] 74%|███████▍  | 432/585 [22:07<00:48,  3.17it/s] 74%|███████▍  | 433/585 [22:07<00:46,  3.25it/s] 74%|███████▍  | 434/585 [22:08<00:45,  3.30it/s] 74%|███████▍  | 435/585 [22:08<00:44,  3.34it/s] 75%|███████▍  | 436/585 [22:08<00:44,  3.37it/s] 75%|███████▍  | 437/585 [22:09<00:43,  3.39it/s] 75%|███████▍  | 438/585 [22:09<00:43,  3.40it/s] 75%|███████▌  | 439/585 [22:10<01:29,  1.63it/s] 75%|███████▌  | 440/585 [22:10<01:14,  1.94it/s] 75%|███████▌  | 441/585 [22:11<01:04,  2.24it/s] 76%|███████▌  | 442/585 [22:11<00:57,  2.51it/s] 76%|███████▌  | 443/585 [22:11<00:51,  2.74it/s] 76%|███████▌  | 444/585 [22:12<00:48,  2.93it/s] 76%|███████▌  | 445/585 [22:12<00:45,  3.07it/s] 76%|███████▌  | 446/585 [22:14<01:43,  1.34it/s] 76%|███████▋  | 447/585 [22:14<01:23,  1.64it/s] 77%|███████▋  | 448/585 [22:14<01:10,  1.95it/s] 77%|███████▋  | 449/585 [22:15<01:00,  2.24it/s] 77%|███████▋  | 450/585 [22:15<00:54,  2.50it/s] 77%|███████▋  | 451/585 [22:15<00:49,  2.72it/s] 77%|███████▋  | 452/585 [22:16<01:25,  1.56it/s] 77%|███████▋  | 453/585 [22:17<01:10,  1.86it/s] 78%|███████▊  | 454/585 [22:17<01:00,  2.16it/s] 78%|███████▊  | 455/585 [22:17<00:53,  2.43it/s] 78%|███████▊  | 456/585 [22:18<00:48,  2.66it/s] 78%|███████▊  | 457/585 [22:18<00:44,  2.85it/s] 78%|███████▊  | 458/585 [22:18<00:42,  3.00it/s] 78%|███████▊  | 459/585 [22:20<01:34,  1.34it/s] 79%|███████▊  | 460/585 [22:20<01:16,  1.64it/s] 79%|███████▉  | 461/585 [22:20<01:03,  1.94it/s] 79%|███████▉  | 462/585 [22:21<00:55,  2.23it/s] 79%|███████▉  | 463/585 [22:21<00:48,  2.49it/s] 79%|███████▉  | 464/585 [22:21<00:44,  2.72it/s] 79%|███████▉  | 465/585 [22:22<01:10,  1.71it/s] 80%|███████▉  | 466/585 [22:23<00:59,  2.01it/s] 80%|███████▉  | 467/585 [22:23<00:51,  2.30it/s] 80%|████████  | 468/585 [22:23<00:45,  2.56it/s][INFO|trainer.py:2140] 2023-08-28 16:54:57,901 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:54:57,901 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 16:54:57,901 >>   Batch size = 8
{'eval_loss': 1.024864673614502, 'eval_runtime': 14.9576, 'eval_samples_per_second': 233.459, 'eval_steps_per_second': 29.216, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.79it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.44it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.60it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.55it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.32it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.98it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.82it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.62it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.68it/s][A
 12%|█▏        | 52/437 [00:02<00:30, 12.53it/s][A
 13%|█▎        | 57/437 [00:02<00:23, 16.06it/s][A
 14%|█▍        | 62/437 [00:02<00:18, 19.95it/s][A
 15%|█▌        | 67/437 [00:02<00:15, 24.00it/s][A
 16%|█▋        | 72/437 [00:02<00:13, 27.95it/s][A
 18%|█▊        | 77/437 [00:02<00:11, 31.57it/s][A
 19%|█▉        | 82/437 [00:02<00:10, 34.67it/s][A
 20%|█▉        | 87/437 [00:02<00:09, 37.13it/s][A
 21%|██        | 92/437 [00:02<00:08, 38.66it/s][A
 22%|██▏       | 97/437 [00:03<00:08, 40.11it/s][A
 23%|██▎       | 102/437 [00:03<00:08, 41.29it/s][A
 24%|██▍       | 107/437 [00:03<00:07, 42.21it/s][A
 26%|██▌       | 112/437 [00:03<00:07, 43.10it/s][A
 27%|██▋       | 117/437 [00:03<00:07, 43.67it/s][A
 28%|██▊       | 122/437 [00:03<00:07, 44.09it/s][A
 29%|██▉       | 127/437 [00:03<00:06, 44.36it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.41it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.10it/s][A
 32%|███▏      | 142/437 [00:04<00:06, 43.97it/s][A
 34%|███▎      | 147/437 [00:04<00:17, 16.40it/s][A
 35%|███▍      | 151/437 [00:06<00:38,  7.41it/s][A
 36%|███▌      | 156/437 [00:06<00:27, 10.05it/s][A
 37%|███▋      | 161/437 [00:06<00:20, 13.22it/s][A
 38%|███▊      | 166/437 [00:06<00:16, 16.87it/s][A
 39%|███▉      | 171/437 [00:06<00:12, 20.83it/s][A
 40%|████      | 176/437 [00:06<00:10, 24.90it/s][A
 41%|████▏     | 181/437 [00:06<00:08, 28.78it/s][A
 43%|████▎     | 186/437 [00:07<00:07, 32.33it/s][A
 44%|████▎     | 191/437 [00:08<00:07, 35.04it/s][A
 45%|████▍     | 196/437 [00:08<00:18, 12.77it/s][A
 46%|████▌     | 201/437 [00:08<00:14, 16.28it/s][A
 47%|████▋     | 206/437 [00:08<00:11, 20.14it/s][A
 48%|████▊     | 210/437 [00:08<00:15, 14.89it/s][A
 49%|████▉     | 215/437 [00:08<00:11, 18.91it/s][A
 50%|█████     | 219/437 [00:14<01:23,  2.60it/s][A
 51%|█████     | 222/437 [00:14<01:07,  3.16it/s][A
 52%|█████▏    | 227/437 [00:14<00:45,  4.65it/s][A
 53%|█████▎    | 232/437 [00:14<00:31,  6.61it/s][A
 54%|█████▍    | 237/437 [00:14<00:22,  9.09it/s][A
 55%|█████▌    | 242/437 [00:14<00:16, 12.12it/s][A
 57%|█████▋    | 247/437 [00:14<00:12, 15.67it/s][A
 58%|█████▊    | 252/437 [00:15<00:09, 19.60it/s][A
 59%|█████▉    | 257/437 [00:15<00:07, 23.68it/s][A
 60%|█████▉    | 262/437 [00:15<00:06, 27.52it/s][A
 61%|██████    | 267/437 [00:15<00:05, 30.93it/s][A
 62%|██████▏   | 272/437 [00:15<00:04, 33.89it/s][A
 63%|██████▎   | 277/437 [00:15<00:04, 36.54it/s][A
 65%|██████▍   | 282/437 [00:15<00:04, 38.72it/s][A
 66%|██████▌   | 287/437 [00:15<00:03, 40.43it/s][A
 67%|██████▋   | 292/437 [00:15<00:03, 41.73it/s][A
 68%|██████▊   | 297/437 [00:16<00:03, 42.66it/s][A
 69%|██████▉   | 302/437 [00:16<00:03, 43.25it/s][A
 70%|███████   | 307/437 [00:16<00:02, 43.36it/s][A
 71%|███████▏  | 312/437 [00:16<00:02, 43.49it/s][A
 73%|███████▎  | 317/437 [00:16<00:02, 43.62it/s][A
 74%|███████▎  | 322/437 [00:16<00:02, 43.84it/s][A
 75%|███████▍  | 327/437 [00:16<00:02, 44.07it/s][A
 76%|███████▌  | 332/437 [00:16<00:02, 44.35it/s][A
 77%|███████▋  | 337/437 [00:16<00:02, 44.60it/s][A
 78%|███████▊  | 342/437 [00:17<00:02, 44.76it/s][A
 79%|███████▉  | 347/437 [00:17<00:02, 44.62it/s][A
 81%|████████  | 352/437 [00:17<00:04, 17.22it/s][A
 82%|████████▏ | 357/437 [00:17<00:03, 21.15it/s][A
 83%|████████▎ | 362/437 [00:18<00:02, 25.13it/s][A
 84%|████████▍ | 367/437 [00:18<00:02, 29.00it/s][A
 85%|████████▌ | 372/437 [00:18<00:02, 32.42it/s][A
 86%|████████▋ | 377/437 [00:18<00:01, 35.41it/s][A
 87%|████████▋ | 382/437 [00:18<00:01, 37.83it/s][A
 89%|████████▊ | 387/437 [00:18<00:01, 39.57it/s][A
 90%|████████▉ | 392/437 [00:18<00:01, 40.42it/s][A
 91%|█████████ | 397/437 [00:18<00:00, 41.41it/s][A
 92%|█████████▏| 402/437 [00:19<00:00, 42.23it/s][A
 93%|█████████▎| 407/437 [00:19<00:00, 43.02it/s][A
 94%|█████████▍| 412/437 [00:19<00:00, 43.60it/s][A
 95%|█████████▌| 417/437 [00:19<00:00, 44.05it/s][A
 97%|█████████▋| 422/437 [00:19<00:00, 44.33it/s][A
 98%|█████████▊| 427/437 [00:19<00:00, 44.51it/s][A
 99%|█████████▉| 432/437 [00:19<00:00, 44.41it/s][A
100%|██████████| 437/437 [00:19<00:00, 44.10it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [22:43<00:45,  2.56it/s]
100%|██████████| 437/437 [00:19<00:00, 44.10it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:55:20,482 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 16:55:23,268 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:57:08,890 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:57:14,241 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:57:15,154 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [28:07<3:19:46, 103.33s/it] 80%|████████  | 470/585 [28:07<2:18:53, 72.46s/it]  81%|████████  | 471/585 [28:08<1:36:32, 50.81s/it] 81%|████████  | 472/585 [28:08<1:07:08, 35.65s/it] 81%|████████  | 473/585 [28:08<46:45, 25.04s/it]   81%|████████  | 474/585 [28:08<32:35, 17.62s/it] 81%|████████  | 475/585 [28:09<22:46, 12.42s/it] 81%|████████▏ | 476/585 [28:09<15:57,  8.78s/it] 82%|████████▏ | 477/585 [28:09<11:13,  6.23s/it] 82%|████████▏ | 478/585 [28:10<07:56,  4.45s/it] 82%|████████▏ | 479/585 [28:10<05:39,  3.20s/it] 82%|████████▏ | 480/585 [28:10<04:14,  2.43s/it] 82%|████████▏ | 481/585 [28:11<03:05,  1.79s/it] 82%|████████▏ | 482/585 [28:11<02:17,  1.34s/it] 83%|████████▎ | 483/585 [28:11<01:44,  1.02s/it] 83%|████████▎ | 484/585 [28:12<01:21,  1.24it/s] 83%|████████▎ | 485/585 [28:12<01:04,  1.54it/s] 83%|████████▎ | 486/585 [28:12<00:53,  1.85it/s] 83%|████████▎ | 487/585 [28:12<00:45,  2.14it/s] 83%|████████▎ | 488/585 [28:13<00:40,  2.42it/s] 84%|████████▎ | 489/585 [28:13<00:36,  2.65it/s] 84%|████████▍ | 490/585 [28:15<01:10,  1.35it/s] 84%|████████▍ | 491/585 [28:15<00:56,  1.65it/s] 84%|████████▍ | 492/585 [28:15<00:47,  1.96it/s] 84%|████████▍ | 493/585 [28:16<00:40,  2.26it/s] 84%|████████▍ | 494/585 [28:16<00:35,  2.53it/s] 85%|████████▍ | 495/585 [28:16<00:32,  2.76it/s] 85%|████████▍ | 496/585 [28:17<01:00,  1.48it/s] 85%|████████▍ | 497/585 [28:18<00:49,  1.79it/s] 85%|████████▌ | 498/585 [28:18<00:41,  2.10it/s] 85%|████████▌ | 499/585 [28:18<00:36,  2.38it/s] 85%|████████▌ | 500/585 [28:19<00:32,  2.64it/s]                                                  85%|████████▌ | 500/585 [28:19<00:32,  2.64it/s] 86%|████████▌ | 501/585 [28:19<00:29,  2.84it/s] 86%|████████▌ | 502/585 [28:19<00:27,  3.01it/s] 86%|████████▌ | 503/585 [28:20<00:40,  2.01it/s] 86%|████████▌ | 504/585 [28:20<00:35,  2.31it/s] 86%|████████▋ | 505/585 [28:21<00:31,  2.57it/s] 86%|████████▋ | 506/585 [28:21<00:28,  2.79it/s] 87%|████████▋ | 507/585 [28:21<00:26,  2.97it/s] 87%|████████▋ | 508/585 [28:22<00:24,  3.11it/s] 87%|████████▋ | 509/585 [28:22<00:23,  3.22it/s] 87%|████████▋ | 510/585 [28:22<00:22,  3.29it/s] 87%|████████▋ | 511/585 [28:22<00:22,  3.35it/s] 88%|████████▊ | 512/585 [28:23<00:39,  1.86it/s] 88%|████████▊ | 513/585 [28:24<00:33,  2.16it/s] 88%|████████▊ | 514/585 [28:24<00:29,  2.44it/s] 88%|████████▊ | 515/585 [28:24<00:26,  2.68it/s] 88%|████████▊ | 516/585 [28:25<00:23,  2.88it/s] 88%|████████▊ | 517/585 [28:25<00:22,  3.04it/s] 89%|████████▊ | 518/585 [28:25<00:21,  3.16it/s] 89%|████████▊ | 519/585 [28:25<00:20,  3.25it/s] 89%|████████▉ | 520/585 [28:26<00:22,  2.88it/s] 89%|████████▉ | 521/585 [28:26<00:21,  3.04it/s] 89%|████████▉ | 522/585 [28:26<00:19,  3.16it/s] 89%|████████▉ | 523/585 [28:27<00:19,  3.25it/s] 90%|████████▉ | 524/585 [28:27<00:18,  3.31it/s] 90%|████████▉ | 525/585 [28:27<00:17,  3.36it/s] 90%|████████▉ | 526/585 [28:28<00:17,  3.40it/s] 90%|█████████ | 527/585 [28:28<00:16,  3.42it/s] 90%|█████████ | 528/585 [28:28<00:16,  3.44it/s] 90%|█████████ | 529/585 [28:28<00:16,  3.46it/s] 91%|█████████ | 530/585 [28:29<00:27,  2.04it/s] 91%|█████████ | 531/585 [28:30<00:23,  2.33it/s] 91%|█████████ | 532/585 [28:30<00:20,  2.58it/s] 91%|█████████ | 533/585 [28:30<00:18,  2.80it/s] 91%|█████████▏| 534/585 [28:31<00:17,  2.98it/s] 91%|█████████▏| 535/585 [28:31<00:16,  3.12it/s] 92%|█████████▏| 536/585 [28:31<00:15,  3.23it/s] 92%|█████████▏| 537/585 [28:31<00:14,  3.30it/s] 92%|█████████▏| 538/585 [28:32<00:14,  3.35it/s] 92%|█████████▏| 539/585 [28:32<00:17,  2.65it/s] 92%|█████████▏| 540/585 [28:33<00:15,  2.86it/s] 92%|█████████▏| 541/585 [28:33<00:14,  3.02it/s] 93%|█████████▎| 542/585 [28:33<00:13,  3.15it/s] 93%|█████████▎| 543/585 [28:33<00:12,  3.24it/s] 93%|█████████▎| 544/585 [28:34<00:12,  3.31it/s] 93%|█████████▎| 545/585 [28:34<00:11,  3.36it/s] 93%|█████████▎| 546/585 [28:34<00:11,  3.40it/s] 94%|█████████▎| 547/585 [28:35<00:11,  3.43it/s] 94%|█████████▎| 548/585 [28:35<00:10,  3.45it/s] 94%|█████████▍| 549/585 [28:36<00:16,  2.13it/s] 94%|█████████▍| 550/585 [28:36<00:14,  2.41it/s] 94%|█████████▍| 551/585 [28:36<00:12,  2.66it/s] 94%|█████████▍| 552/585 [28:37<00:11,  2.86it/s] 95%|█████████▍| 553/585 [28:37<00:10,  3.03it/s] 95%|█████████▍| 554/585 [28:37<00:09,  3.15it/s] 95%|█████████▍| 555/585 [28:37<00:09,  3.25it/s] 95%|█████████▌| 556/585 [28:38<00:08,  3.31it/s] 95%|█████████▌| 557/585 [28:38<00:08,  3.37it/s] 95%|█████████▌| 558/585 [28:40<00:20,  1.35it/s] 96%|█████████▌| 559/585 [28:40<00:15,  1.65it/s] 96%|█████████▌| 560/585 [28:40<00:12,  1.95it/s] 96%|█████████▌| 561/585 [28:41<00:10,  2.24it/s] 96%|█████████▌| 562/585 [28:41<00:09,  2.51it/s] 96%|█████████▌| 563/585 [28:41<00:08,  2.73it/s] 96%|█████████▋| 564/585 [28:42<00:07,  2.91it/s] 97%|█████████▋| 565/585 [28:43<00:10,  1.83it/s] 97%|█████████▋| 566/585 [28:43<00:08,  2.13it/s] 97%|█████████▋| 567/585 [28:43<00:07,  2.40it/s] 97%|█████████▋| 568/585 [28:43<00:06,  2.64it/s] 97%|█████████▋| 569/585 [28:44<00:05,  2.84it/s] 97%|█████████▋| 570/585 [28:44<00:05,  2.99it/s] 98%|█████████▊| 571/585 [28:44<00:04,  3.11it/s] 98%|█████████▊| 572/585 [28:45<00:04,  3.20it/s] 98%|█████████▊| 573/585 [28:45<00:05,  2.22it/s] 98%|█████████▊| 574/585 [28:46<00:04,  2.48it/s] 98%|█████████▊| 575/585 [28:46<00:03,  2.71it/s] 98%|█████████▊| 576/585 [28:46<00:03,  2.89it/s] 99%|█████████▊| 577/585 [28:47<00:02,  3.03it/s] 99%|█████████▉| 578/585 [28:47<00:02,  3.14it/s] 99%|█████████▉| 579/585 [28:47<00:01,  3.22it/s] 99%|█████████▉| 580/585 [28:47<00:01,  3.29it/s] 99%|█████████▉| 581/585 [28:48<00:01,  3.33it/s] 99%|█████████▉| 582/585 [28:49<00:01,  1.89it/s]100%|█████████▉| 583/585 [28:49<00:00,  2.19it/s]100%|█████████▉| 584/585 [28:49<00:00,  2.47it/s]100%|██████████| 585/585 [28:50<00:00,  2.70it/s][INFO|trainer.py:2140] 2023-08-28 17:01:24,291 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:01:24,291 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 17:01:24,291 >>   Batch size = 8
{'eval_loss': 1.0325469970703125, 'eval_runtime': 19.847, 'eval_samples_per_second': 175.946, 'eval_steps_per_second': 22.018, 'epoch': 4.0}
{'loss': 0.8244, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 57.00it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.08it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.30it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.21it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.69it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.18it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.97it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.73it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.86it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.06it/s][A
 13%|█▎        | 57/437 [00:02<00:26, 14.17it/s][A
 14%|█▍        | 62/437 [00:02<00:20, 17.91it/s][A
 15%|█▌        | 67/437 [00:02<00:16, 21.90it/s][A
 16%|█▋        | 72/437 [00:02<00:14, 25.95it/s][A
 18%|█▊        | 77/437 [00:02<00:12, 29.78it/s][A
 19%|█▉        | 82/437 [00:02<00:10, 33.17it/s][A
 20%|█▉        | 87/437 [00:02<00:09, 36.08it/s][A
 21%|██        | 92/437 [00:02<00:08, 38.34it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 39.70it/s][A
 23%|██▎       | 102/437 [00:03<00:08, 40.81it/s][A
 24%|██▍       | 107/437 [00:03<00:07, 41.79it/s][A
 26%|██▌       | 112/437 [00:03<00:07, 42.72it/s][A
 27%|██▋       | 117/437 [00:03<00:07, 43.38it/s][A
 28%|██▊       | 122/437 [00:03<00:07, 43.92it/s][A
 29%|██▉       | 127/437 [00:03<00:06, 44.38it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.66it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.38it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.22it/s][A
 34%|███▎      | 147/437 [00:04<00:06, 44.10it/s][A
 35%|███▍      | 152/437 [00:04<00:06, 44.08it/s][A
 36%|███▌      | 157/437 [00:04<00:09, 30.47it/s][A
 37%|███▋      | 162/437 [00:04<00:08, 33.82it/s][A
 38%|███▊      | 167/437 [00:04<00:07, 36.54it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 38.80it/s][A
 41%|████      | 177/437 [00:04<00:06, 40.53it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 41.82it/s][A
 43%|████▎     | 187/437 [00:05<00:05, 42.83it/s][A
 44%|████▍     | 192/437 [00:05<00:05, 43.42it/s][A
 45%|████▌     | 197/437 [00:05<00:05, 43.41it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 43.43it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 43.79it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 44.10it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.45it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.64it/s][A
 52%|█████▏    | 227/437 [00:06<00:04, 44.92it/s][A
 53%|█████▎    | 232/437 [00:06<00:04, 44.90it/s][A
 54%|█████▍    | 237/437 [00:06<00:04, 44.79it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 44.46it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 44.29it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 44.24it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.08it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.68it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.87it/s][A
 62%|██████▏   | 272/437 [00:07<00:03, 44.87it/s][A
 63%|██████▎   | 277/437 [00:07<00:03, 45.07it/s][A
 65%|██████▍   | 282/437 [00:07<00:03, 44.79it/s][A
 66%|██████▌   | 287/437 [00:07<00:08, 18.33it/s][A
 67%|██████▋   | 292/437 [00:07<00:06, 22.30it/s][A
 68%|██████▊   | 297/437 [00:08<00:05, 26.30it/s][A
 69%|██████▉   | 302/437 [00:08<00:04, 30.07it/s][A
 70%|███████   | 307/437 [00:08<00:03, 33.43it/s][A
 71%|███████▏  | 312/437 [00:08<00:03, 36.27it/s][A
 73%|███████▎  | 317/437 [00:08<00:03, 38.63it/s][A
 74%|███████▎  | 322/437 [00:09<00:06, 16.85it/s][A
 75%|███████▍  | 327/437 [00:09<00:05, 20.83it/s][A
 76%|███████▌  | 332/437 [00:09<00:04, 24.84it/s][A
 77%|███████▋  | 337/437 [00:09<00:03, 28.75it/s][A
 78%|███████▊  | 342/437 [00:09<00:02, 32.30it/s][A
 79%|███████▉  | 347/437 [00:09<00:02, 35.34it/s][A
 81%|████████  | 352/437 [00:09<00:02, 37.81it/s][A
 82%|████████▏ | 357/437 [00:10<00:02, 39.78it/s][A
 83%|████████▎ | 362/437 [00:10<00:01, 40.88it/s][A
 84%|████████▍ | 367/437 [00:10<00:01, 41.49it/s][A
 85%|████████▌ | 372/437 [00:10<00:02, 29.21it/s][A
 86%|████████▋ | 377/437 [00:10<00:01, 32.64it/s][A
 87%|████████▋ | 382/437 [00:10<00:01, 35.44it/s][A
 89%|████████▊ | 387/437 [00:10<00:01, 38.03it/s][A
 90%|████████▉ | 392/437 [00:10<00:01, 39.88it/s][A
 91%|█████████ | 397/437 [00:11<00:00, 41.35it/s][A
 92%|█████████▏| 402/437 [00:11<00:00, 42.46it/s][A
 93%|█████████▎| 407/437 [00:11<00:00, 42.94it/s][A
 94%|█████████▍| 412/437 [00:11<00:00, 43.03it/s][A
 95%|█████████▌| 417/437 [00:11<00:00, 42.99it/s][A
 97%|█████████▋| 422/437 [00:11<00:00, 43.35it/s][A
 98%|█████████▊| 427/437 [00:11<00:00, 43.86it/s][A
 99%|█████████▉| 432/437 [00:11<00:00, 44.16it/s][A
100%|██████████| 437/437 [00:11<00:00, 44.45it/s][A                                                 
                                                 [A100%|██████████| 585/585 [29:02<00:00,  2.70it/s]
100%|██████████| 437/437 [00:11<00:00, 44.45it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 17:01:38,507 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 17:01:41,864 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:03:40,972 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:03:47,186 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:03:48,643 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 17:07:49,636 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 17:07:50,066 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234 (score: 1.022438406944275).
                                                 100%|██████████| 585/585 [37:05<00:00,  2.70it/s]100%|██████████| 585/585 [37:05<00:00,  3.80s/it]
[INFO|trainer.py:1894] 2023-08-28 17:09:48,835 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 17:09:51,738 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 17:11:40,335 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 17:11:45,084 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 17:11:45,626 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:11:54,350 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:11:54,517 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:11:54,517 >>   train_loss               =     0.8188
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:11:54,517 >>   train_runtime            = 0:37:04.73
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:11:54,518 >>   train_samples            =       7504
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:11:54,518 >>   train_samples_per_second =     16.865
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:11:54,518 >>   train_steps_per_second   =      0.263
{'eval_loss': 1.0361111164093018, 'eval_runtime': 12.0014, 'eval_samples_per_second': 290.966, 'eval_steps_per_second': 36.412, 'epoch': 5.0}
{'train_runtime': 2224.7353, 'train_samples_per_second': 16.865, 'train_steps_per_second': 0.263, 'train_loss': 0.8187827477088341, 'epoch': 5.0}
08/28/2023 17:11:58 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 17:11:58,475 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 17:11:58,476 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 17:11:58,476 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:08, 50.61it/s]  3%|▎         | 12/437 [00:00<00:08, 47.50it/s]  4%|▍         | 17/437 [00:00<00:09, 46.65it/s]  5%|▌         | 22/437 [00:00<00:08, 46.16it/s]  6%|▌         | 27/437 [00:00<00:08, 46.00it/s]  7%|▋         | 32/437 [00:00<00:08, 45.90it/s]  8%|▊         | 37/437 [00:00<00:08, 45.84it/s] 10%|▉         | 42/437 [00:00<00:08, 45.58it/s] 11%|█         | 47/437 [00:01<00:08, 45.22it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.97it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.93it/s] 14%|█▍        | 62/437 [00:01<00:08, 45.00it/s] 15%|█▌        | 67/437 [00:01<00:08, 45.11it/s] 16%|█▋        | 72/437 [00:01<00:08, 45.24it/s] 18%|█▊        | 77/437 [00:01<00:07, 45.27it/s] 19%|█▉        | 82/437 [00:01<00:07, 45.28it/s] 20%|█▉        | 87/437 [00:01<00:07, 45.03it/s] 21%|██        | 92/437 [00:02<00:07, 44.94it/s] 22%|██▏       | 97/437 [00:02<00:07, 44.84it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.66it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.98it/s] 26%|██▌       | 112/437 [00:02<00:07, 45.06it/s] 27%|██▋       | 117/437 [00:02<00:07, 45.19it/s] 28%|██▊       | 122/437 [00:02<00:06, 45.23it/s] 29%|██▉       | 127/437 [00:02<00:06, 45.12it/s] 30%|███       | 132/437 [00:03<00:12, 25.30it/s] 31%|███▏      | 137/437 [00:03<00:10, 29.09it/s] 32%|███▏      | 142/437 [00:03<00:09, 32.62it/s] 34%|███▎      | 147/437 [00:03<00:08, 35.66it/s] 35%|███▍      | 152/437 [00:03<00:07, 38.19it/s] 36%|███▌      | 157/437 [00:03<00:06, 40.13it/s] 37%|███▋      | 162/437 [00:03<00:06, 41.65it/s] 38%|███▊      | 167/437 [00:03<00:06, 42.57it/s] 39%|███▉      | 172/437 [00:04<00:06, 42.88it/s] 41%|████      | 177/437 [00:04<00:06, 43.09it/s] 42%|████▏     | 182/437 [00:04<00:05, 43.62it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.15it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.51it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.89it/s] 46%|████▌     | 202/437 [00:04<00:05, 45.02it/s] 47%|████▋     | 207/437 [00:04<00:05, 45.17it/s] 49%|████▊     | 212/437 [00:04<00:05, 44.93it/s] 50%|████▉     | 217/437 [00:05<00:04, 44.64it/s] 51%|█████     | 222/437 [00:05<00:04, 44.44it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.56it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.68it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.95it/s] 55%|█████▌    | 242/437 [00:05<00:04, 45.03it/s] 57%|█████▋    | 247/437 [00:05<00:04, 45.27it/s] 58%|█████▊    | 252/437 [00:05<00:04, 45.28it/s] 59%|█████▉    | 257/437 [00:06<00:08, 21.03it/s] 60%|█████▉    | 262/437 [00:06<00:06, 25.05it/s] 61%|██████    | 267/437 [00:06<00:05, 28.99it/s] 62%|██████▏   | 272/437 [00:06<00:05, 32.54it/s] 63%|██████▎   | 277/437 [00:06<00:04, 35.58it/s] 65%|██████▍   | 282/437 [00:06<00:04, 38.11it/s] 66%|██████▌   | 287/437 [00:07<00:03, 40.08it/s] 67%|██████▋   | 292/437 [00:07<00:03, 41.45it/s] 68%|██████▊   | 297/437 [00:07<00:03, 42.00it/s] 69%|██████▉   | 302/437 [00:07<00:03, 42.52it/s] 70%|███████   | 307/437 [00:07<00:03, 43.09it/s] 71%|███████▏  | 312/437 [00:07<00:02, 43.64it/s] 73%|███████▎  | 317/437 [00:07<00:02, 44.09it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.50it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.78it/s] 76%|███████▌  | 332/437 [00:08<00:02, 44.90it/s] 77%|███████▋  | 337/437 [00:08<00:02, 44.95it/s] 78%|███████▊  | 342/437 [00:08<00:02, 44.69it/s] 79%|███████▉  | 347/437 [00:08<00:02, 44.43it/s] 81%|████████  | 352/437 [00:08<00:01, 44.38it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.49it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.70it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.93it/s] 85%|████████▌ | 372/437 [00:08<00:01, 45.12it/s] 86%|████████▋ | 377/437 [00:09<00:02, 26.44it/s] 87%|████████▋ | 382/437 [00:09<00:01, 30.26it/s] 89%|████████▊ | 387/437 [00:09<00:01, 33.63it/s] 90%|████████▉ | 392/437 [00:09<00:01, 36.50it/s] 91%|█████████ | 397/437 [00:09<00:01, 38.72it/s] 92%|█████████▏| 402/437 [00:09<00:00, 40.50it/s] 93%|█████████▎| 407/437 [00:09<00:00, 41.90it/s] 94%|█████████▍| 412/437 [00:10<00:00, 42.70it/s] 95%|█████████▌| 417/437 [00:10<00:00, 42.89it/s] 97%|█████████▋| 422/437 [00:10<00:00, 43.16it/s] 98%|█████████▊| 427/437 [00:10<00:00, 43.60it/s] 99%|█████████▉| 432/437 [00:10<00:00, 44.01it/s]100%|██████████| 437/437 [00:10<00:00, 44.32it/s]100%|██████████| 437/437 [00:10<00:00, 40.97it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 17:12:09,179 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:12:09,179 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:12:09,179 >>   eval_loss               =     1.0224
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:12:09,179 >>   eval_runtime            = 0:00:10.70
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:12:09,179 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:12:09,179 >>   eval_samples_per_second =    326.243
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:12:09,179 >>   eval_steps_per_second   =     40.827
[INFO|trainer_pt_utils.py:913] 2023-08-28 17:12:09,179 >>   perplexity              =       2.78
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:55,659 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:55,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:55,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:55,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:13:55,796 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:14:01,219 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:14:01,560 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:14:02,172 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:14:04,649 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:14:04,782 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:09,729 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:10,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:10,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:10,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:14:10,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:14:14,876 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:14:15,084 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:14:18,061 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:14:20,621 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:14:20,621 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/checkpoint-585
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.15it/s]Extractor Predicting: 6it [00:04,  1.27it/s]Extractor Predicting: 7it [00:05,  1.35it/s]Extractor Predicting: 8it [00:05,  1.40it/s]Extractor Predicting: 9it [00:07,  1.08it/s]Extractor Predicting: 10it [00:07,  1.20it/s]Extractor Predicting: 11it [00:08,  1.30it/s]Extractor Predicting: 12it [00:08,  1.38it/s]Extractor Predicting: 13it [00:10,  1.04it/s]Extractor Predicting: 14it [00:11,  1.15it/s]Extractor Predicting: 15it [00:11,  1.25it/s]Extractor Predicting: 16it [00:12,  1.34it/s]Extractor Predicting: 17it [00:14,  1.10s/it]Extractor Predicting: 18it [00:14,  1.03it/s]Extractor Predicting: 19it [00:15,  1.18it/s]Extractor Predicting: 20it [00:16,  1.18it/s]Extractor Predicting: 21it [00:17,  1.29it/s]Extractor Predicting: 22it [00:17,  1.37it/s]Extractor Predicting: 23it [00:18,  1.44it/s]Extractor Predicting: 24it [00:18,  1.49it/s]Extractor Predicting: 25it [00:20,  1.12it/s]Extractor Predicting: 26it [00:20,  1.22it/s]Extractor Predicting: 27it [00:21,  1.32it/s]Extractor Predicting: 28it [00:22,  1.39it/s]Extractor Predicting: 29it [00:23,  1.11it/s]Extractor Predicting: 30it [00:24,  1.19it/s]Extractor Predicting: 31it [00:24,  1.30it/s]Extractor Predicting: 32it [00:25,  1.38it/s]Extractor Predicting: 33it [00:27,  1.04s/it]Extractor Predicting: 34it [00:27,  1.09it/s]Extractor Predicting: 35it [00:28,  1.21it/s]Extractor Predicting: 36it [00:29,  1.16it/s]Extractor Predicting: 37it [00:30,  1.26it/s]Extractor Predicting: 38it [00:30,  1.27it/s]Extractor Predicting: 39it [00:32,  1.00s/it]Extractor Predicting: 40it [00:32,  1.14it/s]Extractor Predicting: 41it [00:33,  1.25it/s]Extractor Predicting: 42it [00:34,  1.32it/s]Extractor Predicting: 43it [00:35,  1.21it/s]Extractor Predicting: 44it [00:35,  1.31it/s]Extractor Predicting: 45it [00:36,  1.37it/s]Extractor Predicting: 46it [00:37,  1.43it/s]Extractor Predicting: 47it [00:37,  1.48it/s]Extractor Predicting: 48it [00:38,  1.20it/s]Extractor Predicting: 49it [00:39,  1.30it/s]Extractor Predicting: 50it [00:40,  1.39it/s]Extractor Predicting: 51it [00:40,  1.41it/s]Extractor Predicting: 52it [00:41,  1.18it/s]Extractor Predicting: 53it [00:42,  1.28it/s]Extractor Predicting: 54it [00:43,  1.35it/s]Extractor Predicting: 55it [00:43,  1.42it/s]Extractor Predicting: 56it [00:45,  1.09it/s]Extractor Predicting: 57it [00:45,  1.20it/s]Extractor Predicting: 58it [00:46,  1.31it/s]Extractor Predicting: 59it [00:47,  1.37it/s]Extractor Predicting: 60it [00:48,  1.20it/s]Extractor Predicting: 61it [00:48,  1.28it/s]Extractor Predicting: 62it [00:49,  1.35it/s]Extractor Predicting: 63it [00:50,  1.42it/s]Extractor Predicting: 64it [00:52,  1.16s/it]Extractor Predicting: 65it [00:52,  1.00s/it]Extractor Predicting: 66it [00:53,  1.10it/s]Extractor Predicting: 67it [00:54,  1.03s/it]Extractor Predicting: 68it [00:55,  1.08it/s]Extractor Predicting: 69it [00:56,  1.19it/s]Extractor Predicting: 70it [00:56,  1.26it/s]Extractor Predicting: 71it [00:58,  1.15it/s]Extractor Predicting: 72it [00:58,  1.22it/s]Extractor Predicting: 73it [00:59,  1.30it/s]Extractor Predicting: 74it [01:00,  1.33it/s]Extractor Predicting: 75it [01:00,  1.32it/s]Extractor Predicting: 76it [01:01,  1.38it/s]Extractor Predicting: 77it [01:02,  1.40it/s]Extractor Predicting: 78it [01:03,  1.20it/s]Extractor Predicting: 79it [01:04,  1.26it/s]Extractor Predicting: 80it [01:04,  1.31it/s]Extractor Predicting: 81it [01:05,  1.35it/s]Extractor Predicting: 82it [01:06,  1.24it/s]Extractor Predicting: 83it [01:07,  1.32it/s]Extractor Predicting: 84it [01:07,  1.39it/s]Extractor Predicting: 85it [01:08,  1.43it/s]Extractor Predicting: 86it [01:08,  1.43it/s]Extractor Predicting: 87it [01:10,  1.19it/s]Extractor Predicting: 88it [01:10,  1.30it/s]Extractor Predicting: 89it [01:11,  1.40it/s]Extractor Predicting: 90it [01:11,  1.46it/s]Extractor Predicting: 91it [01:12,  1.55it/s]Extractor Predicting: 92it [01:13,  1.20it/s]Extractor Predicting: 93it [01:14,  1.34it/s]Extractor Predicting: 94it [01:14,  1.46it/s]Extractor Predicting: 95it [01:15,  1.50it/s]Extractor Predicting: 96it [01:16,  1.58it/s]Extractor Predicting: 97it [01:17,  1.33it/s]Extractor Predicting: 98it [01:17,  1.42it/s]Extractor Predicting: 99it [01:18,  1.54it/s]Extractor Predicting: 100it [01:18,  1.61it/s]Extractor Predicting: 101it [01:19,  1.61it/s]Extractor Predicting: 102it [01:20,  1.43it/s]Extractor Predicting: 103it [01:20,  1.49it/s]Extractor Predicting: 104it [01:21,  1.51it/s]Extractor Predicting: 105it [01:22,  1.55it/s]Extractor Predicting: 106it [01:22,  1.59it/s]Extractor Predicting: 107it [01:23,  1.30it/s]Extractor Predicting: 108it [01:24,  1.41it/s]Extractor Predicting: 109it [01:24,  1.50it/s]Extractor Predicting: 110it [01:25,  1.56it/s]Extractor Predicting: 111it [01:26,  1.61it/s]Extractor Predicting: 112it [01:28,  1.06s/it]Extractor Predicting: 113it [01:28,  1.11it/s]Extractor Predicting: 114it [01:29,  1.21it/s]Extractor Predicting: 115it [01:30,  1.22it/s]Extractor Predicting: 116it [01:30,  1.31it/s]Extractor Predicting: 117it [01:31,  1.38it/s]Extractor Predicting: 118it [01:32,  1.45it/s]Extractor Predicting: 119it [01:32,  1.47it/s]Extractor Predicting: 120it [01:33,  1.35it/s]Extractor Predicting: 121it [01:35,  1.13s/it]Extractor Predicting: 122it [01:36,  1.04s/it]Extractor Predicting: 123it [01:38,  1.36s/it]Extractor Predicting: 124it [01:39,  1.15s/it]Extractor Predicting: 125it [01:39,  1.00s/it]Extractor Predicting: 126it [01:40,  1.03it/s]Extractor Predicting: 127it [01:41,  1.14it/s]Extractor Predicting: 128it [01:42,  1.24it/s]Extractor Predicting: 129it [01:42,  1.33it/s]Extractor Predicting: 130it [01:43,  1.41it/s]Extractor Predicting: 131it [01:44,  1.21it/s]Extractor Predicting: 132it [01:45,  1.29it/s]Extractor Predicting: 133it [01:45,  1.35it/s]Extractor Predicting: 134it [01:46,  1.40it/s]Extractor Predicting: 135it [01:47,  1.34it/s]Extractor Predicting: 136it [01:47,  1.39it/s]Extractor Predicting: 137it [01:48,  1.43it/s]Extractor Predicting: 138it [01:49,  1.48it/s]Extractor Predicting: 139it [01:49,  1.50it/s]Extractor Predicting: 140it [01:51,  1.16it/s]Extractor Predicting: 141it [01:51,  1.24it/s]Extractor Predicting: 142it [01:52,  1.30it/s]Extractor Predicting: 143it [01:53,  1.38it/s]Extractor Predicting: 144it [01:54,  1.20it/s]Extractor Predicting: 144it [01:54,  1.26it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:00,392 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:00,879 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:00,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:00,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:00,880 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:18:05,765 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:18:06,228 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:18:08,125 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:18:10,570 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:18:11,135 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:16,575 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:17,029 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:17,029 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:17,029 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:18:17,029 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:18:20,568 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:18:20,773 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:18:22,385 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:18:23,567 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:18:24,157 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5482517482517483,
  "recall": 0.11225658648339061,
  "score": 0.18635607321131448,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.70it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.60it/s]Extractor Predicting: 8it [00:05,  1.11it/s]Extractor Predicting: 9it [00:06,  1.23it/s]Extractor Predicting: 10it [00:07,  1.33it/s]Extractor Predicting: 11it [00:07,  1.41it/s]Extractor Predicting: 12it [00:08,  1.28it/s]Extractor Predicting: 13it [00:09,  1.35it/s]Extractor Predicting: 14it [00:09,  1.43it/s]Extractor Predicting: 15it [00:10,  1.51it/s]Extractor Predicting: 16it [00:11,  1.57it/s]Extractor Predicting: 17it [00:12,  1.07it/s]Extractor Predicting: 18it [00:13,  1.19it/s]Extractor Predicting: 19it [00:13,  1.28it/s]Extractor Predicting: 20it [00:14,  1.36it/s]Extractor Predicting: 21it [00:15,  1.14it/s]Extractor Predicting: 22it [00:16,  1.21it/s]Extractor Predicting: 23it [00:17,  1.32it/s]Extractor Predicting: 24it [00:17,  1.42it/s]Extractor Predicting: 25it [00:19,  1.03it/s]Extractor Predicting: 26it [00:19,  1.14it/s]Extractor Predicting: 27it [00:20,  1.24it/s]Extractor Predicting: 28it [00:21,  1.32it/s]Extractor Predicting: 29it [00:22,  1.03it/s]Extractor Predicting: 30it [00:23,  1.13it/s]Extractor Predicting: 31it [00:24,  1.23it/s]Extractor Predicting: 32it [00:24,  1.31it/s]Extractor Predicting: 33it [00:25,  1.12it/s]Extractor Predicting: 34it [00:26,  1.24it/s]Extractor Predicting: 35it [00:27,  1.34it/s]Extractor Predicting: 36it [00:27,  1.43it/s]Extractor Predicting: 37it [00:28,  1.27it/s]Extractor Predicting: 38it [00:29,  1.37it/s]Extractor Predicting: 39it [00:29,  1.44it/s]Extractor Predicting: 40it [00:30,  1.49it/s]Extractor Predicting: 41it [00:31,  1.54it/s]Extractor Predicting: 42it [00:32,  1.32it/s]Extractor Predicting: 43it [00:32,  1.38it/s]Extractor Predicting: 44it [00:33,  1.45it/s]Extractor Predicting: 45it [00:34,  1.49it/s]Extractor Predicting: 46it [00:34,  1.55it/s]Extractor Predicting: 47it [00:35,  1.59it/s]Extractor Predicting: 48it [00:36,  1.28it/s]Extractor Predicting: 49it [00:36,  1.36it/s]Extractor Predicting: 50it [00:37,  1.42it/s]Extractor Predicting: 51it [00:38,  1.48it/s]Extractor Predicting: 52it [00:38,  1.53it/s]Extractor Predicting: 53it [00:39,  1.37it/s]Extractor Predicting: 54it [00:40,  1.44it/s]Extractor Predicting: 55it [00:40,  1.47it/s]Extractor Predicting: 56it [00:41,  1.52it/s]Extractor Predicting: 57it [00:42,  1.58it/s]Extractor Predicting: 58it [00:43,  1.28it/s]Extractor Predicting: 59it [00:43,  1.40it/s]Extractor Predicting: 60it [00:44,  1.46it/s]Extractor Predicting: 61it [00:45,  1.51it/s]Extractor Predicting: 62it [00:45,  1.54it/s]Extractor Predicting: 63it [00:46,  1.29it/s]Extractor Predicting: 64it [00:47,  1.35it/s]Extractor Predicting: 65it [00:48,  1.42it/s]Extractor Predicting: 66it [00:48,  1.48it/s]Extractor Predicting: 67it [00:49,  1.53it/s]Extractor Predicting: 68it [00:50,  1.39it/s]Extractor Predicting: 69it [00:50,  1.45it/s]Extractor Predicting: 70it [00:51,  1.51it/s]Extractor Predicting: 71it [00:51,  1.54it/s]Extractor Predicting: 72it [00:52,  1.56it/s]Extractor Predicting: 73it [00:53,  1.47it/s]Extractor Predicting: 74it [00:53,  1.50it/s]Extractor Predicting: 75it [00:54,  1.54it/s]Extractor Predicting: 76it [00:55,  1.57it/s]Extractor Predicting: 77it [00:55,  1.55it/s]Extractor Predicting: 78it [00:56,  1.39it/s]Extractor Predicting: 79it [00:57,  1.45it/s]Extractor Predicting: 80it [00:57,  1.49it/s]Extractor Predicting: 81it [00:58,  1.51it/s]Extractor Predicting: 82it [00:59,  1.52it/s]Extractor Predicting: 83it [01:00,  1.46it/s]Extractor Predicting: 84it [01:00,  1.51it/s]Extractor Predicting: 85it [01:01,  1.56it/s]Extractor Predicting: 86it [01:02,  1.41it/s]Extractor Predicting: 87it [01:02,  1.47it/s]Extractor Predicting: 88it [01:04,  1.07it/s]Extractor Predicting: 89it [01:04,  1.20it/s]Extractor Predicting: 90it [01:05,  1.29it/s]Extractor Predicting: 91it [01:06,  1.36it/s]Extractor Predicting: 92it [01:06,  1.33it/s]Extractor Predicting: 93it [01:07,  1.43it/s]Extractor Predicting: 94it [01:08,  1.45it/s]Extractor Predicting: 95it [01:08,  1.49it/s]Extractor Predicting: 96it [01:09,  1.54it/s]Extractor Predicting: 97it [01:10,  1.21it/s]Extractor Predicting: 98it [01:11,  1.31it/s]Extractor Predicting: 99it [01:11,  1.40it/s]Extractor Predicting: 100it [01:12,  1.47it/s]Extractor Predicting: 101it [01:13,  1.39it/s]Extractor Predicting: 102it [01:13,  1.46it/s]Extractor Predicting: 103it [01:14,  1.47it/s]Extractor Predicting: 104it [01:15,  1.49it/s]Extractor Predicting: 105it [01:15,  1.53it/s]Extractor Predicting: 106it [01:16,  1.25it/s]Extractor Predicting: 107it [01:17,  1.33it/s]Extractor Predicting: 108it [01:18,  1.41it/s]Extractor Predicting: 109it [01:18,  1.46it/s]Extractor Predicting: 110it [01:20,  1.03it/s]Extractor Predicting: 111it [01:21,  1.14it/s]Extractor Predicting: 112it [01:21,  1.24it/s]Extractor Predicting: 113it [01:22,  1.32it/s]Extractor Predicting: 114it [01:23,  1.03it/s]Extractor Predicting: 115it [01:24,  1.15it/s]Extractor Predicting: 116it [01:25,  1.28it/s]Extractor Predicting: 117it [01:25,  1.38it/s]Extractor Predicting: 118it [01:27,  1.02it/s]Extractor Predicting: 119it [01:27,  1.15it/s]Extractor Predicting: 120it [01:28,  1.26it/s]Extractor Predicting: 121it [01:29,  1.36it/s]Extractor Predicting: 122it [01:29,  1.28it/s]Extractor Predicting: 123it [01:30,  1.39it/s]Extractor Predicting: 124it [01:31,  1.47it/s]Extractor Predicting: 125it [01:31,  1.52it/s]Extractor Predicting: 126it [01:32,  1.56it/s]Extractor Predicting: 127it [01:34,  1.02it/s]Extractor Predicting: 128it [01:34,  1.15it/s]Extractor Predicting: 129it [01:35,  1.25it/s]Extractor Predicting: 130it [01:35,  1.33it/s]Extractor Predicting: 131it [01:36,  1.43it/s]Extractor Predicting: 132it [01:37,  1.49it/s]Extractor Predicting: 133it [01:37,  1.45it/s]Extractor Predicting: 134it [01:38,  1.51it/s]Extractor Predicting: 135it [01:39,  1.55it/s]Extractor Predicting: 136it [01:39,  1.58it/s]Extractor Predicting: 137it [01:40,  1.61it/s]Extractor Predicting: 138it [01:41,  1.32it/s]Extractor Predicting: 139it [01:41,  1.40it/s]Extractor Predicting: 140it [01:42,  1.52it/s]Extractor Predicting: 141it [01:43,  1.57it/s]Extractor Predicting: 142it [01:43,  1.58it/s]Extractor Predicting: 143it [01:44,  1.27it/s]Extractor Predicting: 144it [01:45,  1.36it/s]Extractor Predicting: 145it [01:46,  1.45it/s]Extractor Predicting: 146it [01:46,  1.52it/s]Extractor Predicting: 147it [01:47,  1.57it/s]Extractor Predicting: 148it [01:48,  1.14it/s]Extractor Predicting: 149it [01:49,  1.26it/s]Extractor Predicting: 150it [01:49,  1.35it/s]Extractor Predicting: 151it [01:50,  1.43it/s]Extractor Predicting: 152it [01:51,  1.20it/s]Extractor Predicting: 153it [01:52,  1.28it/s]Extractor Predicting: 154it [01:52,  1.39it/s]Extractor Predicting: 155it [01:53,  1.46it/s]Extractor Predicting: 156it [01:54,  1.53it/s]Extractor Predicting: 157it [01:54,  1.40it/s]Extractor Predicting: 158it [01:55,  1.51it/s]Extractor Predicting: 159it [01:56,  1.54it/s]Extractor Predicting: 160it [01:56,  1.54it/s]Extractor Predicting: 161it [01:57,  1.56it/s]Extractor Predicting: 162it [01:58,  1.31it/s]Extractor Predicting: 163it [01:58,  1.41it/s]Extractor Predicting: 164it [01:59,  1.47it/s]Extractor Predicting: 165it [02:00,  1.55it/s]Extractor Predicting: 166it [02:00,  1.60it/s]Extractor Predicting: 167it [02:01,  1.29it/s]Extractor Predicting: 168it [02:02,  1.34it/s]Extractor Predicting: 169it [02:03,  1.42it/s]Extractor Predicting: 170it [02:03,  1.34it/s]Extractor Predicting: 171it [02:04,  1.21it/s]Extractor Predicting: 172it [02:05,  1.32it/s]Extractor Predicting: 173it [02:06,  1.30it/s]Extractor Predicting: 174it [02:07,  1.38it/s]Extractor Predicting: 175it [02:07,  1.41it/s]Extractor Predicting: 176it [02:08,  1.43it/s]Extractor Predicting: 177it [02:09,  1.39it/s]Extractor Predicting: 178it [02:09,  1.45it/s]Extractor Predicting: 179it [02:10,  1.52it/s]Extractor Predicting: 180it [02:10,  1.57it/s]Extractor Predicting: 181it [02:11,  1.56it/s]Extractor Predicting: 182it [02:12,  1.48it/s]Extractor Predicting: 183it [02:12,  1.52it/s]Extractor Predicting: 184it [02:13,  1.53it/s]Extractor Predicting: 185it [02:14,  1.55it/s]Extractor Predicting: 186it [02:14,  1.56it/s]Extractor Predicting: 187it [02:16,  1.18it/s]Extractor Predicting: 188it [02:16,  1.27it/s]Extractor Predicting: 189it [02:17,  1.35it/s]Extractor Predicting: 190it [02:18,  1.41it/s]Extractor Predicting: 191it [02:18,  1.38it/s]Extractor Predicting: 192it [02:19,  1.47it/s]Extractor Predicting: 193it [02:20,  1.46it/s]Extractor Predicting: 194it [02:20,  1.48it/s]Extractor Predicting: 195it [02:21,  1.55it/s]Extractor Predicting: 196it [02:22,  1.34it/s]Extractor Predicting: 197it [02:22,  1.43it/s]Extractor Predicting: 198it [02:23,  1.45it/s]Extractor Predicting: 199it [02:24,  1.51it/s]Extractor Predicting: 200it [02:24,  1.60it/s]Extractor Predicting: 201it [02:26,  1.04it/s]Extractor Predicting: 202it [02:27,  1.17it/s]Extractor Predicting: 203it [02:27,  1.27it/s]Extractor Predicting: 204it [02:28,  1.36it/s]Extractor Predicting: 205it [02:29,  1.10it/s]Extractor Predicting: 206it [02:30,  1.21it/s]Extractor Predicting: 207it [02:30,  1.31it/s]Extractor Predicting: 208it [02:31,  1.37it/s]Extractor Predicting: 209it [02:33,  1.04s/it]Extractor Predicting: 210it [02:33,  1.09it/s]Extractor Predicting: 211it [02:34,  1.21it/s]Extractor Predicting: 212it [02:35,  1.18it/s]Extractor Predicting: 213it [02:36,  1.28it/s]Extractor Predicting: 214it [02:36,  1.36it/s]Extractor Predicting: 215it [02:37,  1.36it/s]Extractor Predicting: 216it [02:38,  1.44it/s]Extractor Predicting: 217it [02:38,  1.51it/s]Extractor Predicting: 218it [02:39,  1.53it/s]Extractor Predicting: 219it [02:40,  1.25it/s]Extractor Predicting: 220it [02:40,  1.35it/s]Extractor Predicting: 221it [02:41,  1.42it/s]Extractor Predicting: 222it [02:42,  1.47it/s]Extractor Predicting: 223it [02:42,  1.54it/s]Extractor Predicting: 224it [02:43,  1.31it/s]Extractor Predicting: 225it [02:44,  1.38it/s]Extractor Predicting: 226it [02:45,  1.42it/s]Extractor Predicting: 227it [02:45,  1.48it/s]Extractor Predicting: 228it [02:46,  1.52it/s]Extractor Predicting: 229it [02:47,  1.39it/s]Extractor Predicting: 230it [02:47,  1.46it/s]Extractor Predicting: 231it [02:48,  1.49it/s]Extractor Predicting: 232it [02:49,  1.53it/s]Extractor Predicting: 233it [02:49,  1.55it/s]Extractor Predicting: 234it [02:50,  1.44it/s]Extractor Predicting: 235it [02:51,  1.51it/s]Extractor Predicting: 236it [02:51,  1.55it/s]Extractor Predicting: 237it [02:52,  1.56it/s]Extractor Predicting: 238it [02:52,  1.58it/s]Extractor Predicting: 239it [02:53,  1.42it/s]Extractor Predicting: 240it [02:54,  1.50it/s]Extractor Predicting: 241it [02:54,  1.55it/s]Extractor Predicting: 242it [02:55,  1.61it/s]Extractor Predicting: 243it [02:56,  1.63it/s]Extractor Predicting: 244it [02:57,  1.33it/s]Extractor Predicting: 245it [02:57,  1.41it/s]Extractor Predicting: 246it [02:58,  1.48it/s]Extractor Predicting: 247it [02:59,  1.54it/s]Extractor Predicting: 248it [02:59,  1.57it/s]Extractor Predicting: 249it [03:00,  1.32it/s]Extractor Predicting: 250it [03:01,  1.42it/s]Extractor Predicting: 251it [03:01,  1.46it/s]Extractor Predicting: 252it [03:02,  1.50it/s]Extractor Predicting: 253it [03:03,  1.58it/s]Extractor Predicting: 254it [03:03,  1.48it/s]Extractor Predicting: 255it [03:04,  1.37it/s]Extractor Predicting: 256it [03:05,  1.44it/s]Extractor Predicting: 257it [03:05,  1.50it/s]Extractor Predicting: 258it [03:06,  1.56it/s]Extractor Predicting: 259it [03:07,  1.48it/s]Extractor Predicting: 260it [03:07,  1.53it/s]Extractor Predicting: 261it [03:09,  1.16it/s]Extractor Predicting: 262it [03:09,  1.28it/s]Extractor Predicting: 263it [03:10,  1.37it/s]Extractor Predicting: 264it [03:10,  1.46it/s]Extractor Predicting: 265it [03:12,  1.18it/s]Extractor Predicting: 266it [03:12,  1.32it/s]Extractor Predicting: 267it [03:13,  1.38it/s]Extractor Predicting: 268it [03:14,  1.45it/s]Extractor Predicting: 269it [03:14,  1.35it/s]Extractor Predicting: 270it [03:15,  1.43it/s]Extractor Predicting: 271it [03:16,  1.52it/s]Extractor Predicting: 272it [03:16,  1.60it/s]Extractor Predicting: 273it [03:17,  1.61it/s]Extractor Predicting: 274it [03:18,  1.05it/s]Extractor Predicting: 275it [03:19,  1.18it/s]Extractor Predicting: 276it [03:20,  1.29it/s]Extractor Predicting: 277it [03:20,  1.38it/s]Extractor Predicting: 278it [03:22,  1.13it/s]Extractor Predicting: 279it [03:22,  1.23it/s]Extractor Predicting: 280it [03:23,  1.34it/s]Extractor Predicting: 281it [03:23,  1.40it/s]Extractor Predicting: 281it [03:24,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:14,466 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:14,609 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:14,609 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:14,609 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:14,609 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:23:18,548 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:23:18,708 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:23:20,141 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:23:22,930 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:23:23,407 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:29,923 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:30,498 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:30,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:30,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:23:30,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:23:32,975 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:23:33,126 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:23:34,126 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:23:34,664 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:23:34,664 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4892405063291139,
  "recall": 0.11463740174996292,
  "score": 0.18575033040970804,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.28it/s]Extractor Predicting: 5it [00:03,  1.36it/s]Extractor Predicting: 6it [00:03,  1.82it/s]Extractor Predicting: 6it [00:03,  1.57it/s]
[INFO|configuration_utils.py:515] 2023-08-28 17:23:59,113 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:23:59,206 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 17:23:59,912 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:23:59,913 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 17:24:00,206 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 17:26:19,458 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 17:26:20,023 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 17:26:23,415 >> loading configuration file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 17:26:23,571 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 17:26:25,143 >> Didn't find file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:26:25,566 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:26:25,566 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:26:25,567 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:26:25,567 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:26:25,567 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 17:26:25,567 >> loading file outputs/wrapper/fewrel/unseen_10_seed_0/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.4,
  "recall": 0.01556420233463035,
  "score": 0.0299625468164794,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_10_seed_0/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 17:26:26,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:27,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:28,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:28,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:29,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:29,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:31,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:31,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:32,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:33,142 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:33,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:34,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:35,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:36,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:37,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:37,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:38,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:38,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:39,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:40,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:41,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:14<03:27, 14.79s/it][WARNING|generation_utils.py:914] 2023-08-28 17:26:41,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:42,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:43,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:44,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:45,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:45,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:46,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:47,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:48,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:48,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:49,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:50,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:52,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:53,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:53,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:55,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:56,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:56,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:57,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:58,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:26:59,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:00,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:00,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:35<03:58, 18.32s/it][WARNING|generation_utils.py:914] 2023-08-28 17:27:02,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:03,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:03,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:04,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:05,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:06,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:06,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:07,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:08,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:09,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:10,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:10,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:12,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:12,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:13,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:13,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:15,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:15,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:16,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:16,888 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:17,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:19,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:19,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:53<03:36, 18.05s/it][WARNING|generation_utils.py:914] 2023-08-28 17:27:20,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:20,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:22,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:23,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:23,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:24,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:24,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:25,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:26,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:27,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:27,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:28,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:28,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:29,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:30,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:31,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:31,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:32,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:33,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:33,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:34,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:34,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:08<03:05, 16.83s/it][WARNING|generation_utils.py:914] 2023-08-28 17:27:35,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:35,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:36,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:37,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:37,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:38,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:39,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:39,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:40,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:41,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:43,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:43,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:44,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:44,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:45,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:46,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:47,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:47,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:48,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:49,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:50,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:50,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:51,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:52,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:26<02:52, 17.24s/it][WARNING|generation_utils.py:914] 2023-08-28 17:27:53,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:54,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:55,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:55,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:56,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:57,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:58,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:58,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:59,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:27:59,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:00,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:01,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:01,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:02,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:02,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:03,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:04,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:04,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:05,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:05,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:06,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:06,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:07,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:08,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:08,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:42<02:31, 16.88s/it][WARNING|generation_utils.py:914] 2023-08-28 17:28:09,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:10,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:11,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:12,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:12,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:13,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:14,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:15,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:16,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:17,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:18,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:19,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:19,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:20,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:21,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:21,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:22,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:23,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:24,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:25,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:26,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:26,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:27,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:28,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [02:02<02:23, 17.88s/it][WARNING|generation_utils.py:914] 2023-08-28 17:28:29,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:29,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:30,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:31,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:32,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:32,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:33,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:33,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:35,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:35,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:36,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:37,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:37,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:38,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:39,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:39,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:40,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:40,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:41,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:42,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:42,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:43,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:44,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:17<01:59, 17.13s/it][WARNING|generation_utils.py:914] 2023-08-28 17:28:44,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:45,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:46,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:46,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:47,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:48,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:48,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:49,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:50,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:51,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:51,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:52,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:53,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:53,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:54,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:55,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:55,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:56,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:57,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:57,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:58,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:28:59,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:00,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:01,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:35<01:42, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-28 17:29:02,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:02,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:03,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:03,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:04,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:05,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:05,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:07,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:07,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:08,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:09,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:09,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:10,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:11,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:11,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:12,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:13,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:13,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:14,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:14,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:15,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:49<01:21, 16.22s/it][WARNING|generation_utils.py:914] 2023-08-28 17:29:16,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:16,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:17,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:17,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:18,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:19,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:20,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:20,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:21,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:21,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:22,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:23,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:24,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:24,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:25,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:25,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:26,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:28,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:28,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:29,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:29,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:30,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [03:04<01:03, 15.89s/it][WARNING|generation_utils.py:914] 2023-08-28 17:29:31,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:31,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:32,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:33,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:33,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:34,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:34,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:35,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:36,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:37,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:38,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:38,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:39,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:40,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:40,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:41,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:41,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:42,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:43,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:44,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:45,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:45,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:46,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:20<00:47, 15.89s/it][WARNING|generation_utils.py:914] 2023-08-28 17:29:47,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:47,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:48,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:48,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:49,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:50,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:50,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:51,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:52,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:53,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:53,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:54,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:54,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:55,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:56,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:56,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:57,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:57,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:59,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:29:59,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:00,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:00,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:01,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:02,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:03,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:03,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:37<00:32, 16.23s/it][WARNING|generation_utils.py:914] 2023-08-28 17:30:04,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:05,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:05,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:06,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:07,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:07,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:08,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:09,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:10,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:10,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:11,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:12,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:12,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:13,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:13,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:14,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:15,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:15,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:16,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:17,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:17,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:18,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:19,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:19,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:20,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:20,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:22,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:22,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:23,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:56<00:17, 17.28s/it][WARNING|generation_utils.py:914] 2023-08-28 17:30:24,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:25,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:26,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:26,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:27,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:29,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:30,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:31,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:31,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:32,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:33,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:34,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:35,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:35,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:36,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:37,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:38,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:38,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:40,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:40,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:41,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:42,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:43,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:44,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 17:30:45,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [04:19<00:00, 18.92s/it]Generating: 100%|██████████| 15/15 [04:19<00:00, 17.31s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:31:46,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:31:46,854 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:31:46,854 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:31:46,854 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:31:46,854 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:31:50,538 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:31:50,941 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:31:52,275 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:31:53,832 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:31:54,813 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:32:01,758 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:32:01,937 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:32:01,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:32:01,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:32:01,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:32:06,672 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:32:07,020 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:32:08,689 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:32:09,468 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:32:09,468 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9136904761904762, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 266, 'raw': 320}
{'target': 600, 'success': 288, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 550, 'raw': 672}
{'target': 600, 'success': 576, 'raw': 704}
{'target': 600, 'success': 601, 'raw': 736}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8165760869565217, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 450, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8288043478260869, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8849431818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : taxon rank . Context : The Second Prince of the Northern Counties , Countess Alixandra , was at the head of the Government in Burgundy during the first quarter of 1808 . Head Entity : Countess Alixandra , Tail Entity : Countess Napoleon .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 202, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 251, 'raw': 320}
{'target': 600, 'success': 276, 'raw': 352}
{'target': 600, 'success': 300, 'raw': 384}
{'target': 600, 'success': 328, 'raw': 416}
{'target': 600, 'success': 352, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 405, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 458, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 507, 'raw': 640}
{'target': 600, 'success': 531, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 584, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.7942708333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 221, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 272, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 318, 'raw': 416}
{'target': 600, 'success': 341, 'raw': 448}
{'target': 600, 'success': 369, 'raw': 480}
{'target': 600, 'success': 392, 'raw': 512}
{'target': 600, 'success': 411, 'raw': 544}
{'target': 600, 'success': 437, 'raw': 576}
{'target': 600, 'success': 461, 'raw': 608}
{'target': 600, 'success': 483, 'raw': 640}
{'target': 600, 'success': 506, 'raw': 672}
{'target': 600, 'success': 535, 'raw': 704}
{'target': 600, 'success': 562, 'raw': 736}
{'target': 600, 'success': 588, 'raw': 768}
{'target': 600, 'success': 611, 'raw': 800}
{'prompt': 'Relation : competition class .', 'success_rate': 0.76375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', "('Swiss Open', 'competition class', '', 'He was second in the Swiss Open before having two titles .')"}}
["Relation : location . Context : The city of Buenos Aires , Brazil is the nearest Latin American cities to Buenos Aires at approximately 98.8 miles away on Paraguay 's eastern border , and via its natural gas production on the Chacay River . Head Entity : chacay river , Tail Entity : Buenos Aires .\n"]
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 310, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 433, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 537, 'raw': 672}
{'target': 600, 'success': 561, 'raw': 704}
{'target': 600, 'success': 587, 'raw': 736}
{'target': 600, 'success': 611, 'raw': 768}
{'prompt': 'Relation : location .', 'success_rate': 0.7955729166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 566, 'raw': 672}
{'target': 600, 'success': 594, 'raw': 704}
{'target': 600, 'success': 619, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8410326086956522, 'errors': {'', "('Republic of Cyprus', 'member of political party', '', 'Although it has been held by a number of political parties , the government is the only one in the Republic of Cyprus that exercises the legislative power in Parliament and is therefore not an electoral body .')", 'not enough values to unpack (expected 2, got 1)'}}
['Relation : nominated for . Context : Later in 2008 , he won his third consecutive Emmy Award for Best Actor for his role in the TV series " The Big Bang Theory " , for best actress in a play . Head Entity : Best Actor , Tail Entity : Emmy Award for Best Actress .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 592, 'raw': 736}
{'target': 600, 'success': 617, 'raw': 768}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8033854166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 349, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 404, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8958333333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 491, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 598, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8806818181818182, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 567, 'raw': 672}
{'target': 600, 'success': 592, 'raw': 704}
{'target': 600, 'success': 616, 'raw': 736}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8369565217391305, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 217, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 263, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 308, 'raw': 416}
{'target': 600, 'success': 332, 'raw': 448}
{'target': 600, 'success': 355, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 452, 'raw': 608}
{'target': 600, 'success': 476, 'raw': 640}
{'target': 600, 'success': 498, 'raw': 672}
{'target': 600, 'success': 523, 'raw': 704}
{'target': 600, 'success': 548, 'raw': 736}
{'target': 600, 'success': 574, 'raw': 768}
{'target': 600, 'success': 597, 'raw': 800}
{'target': 600, 'success': 621, 'raw': 832}
{'prompt': 'Relation : position held .', 'success_rate': 0.7463942307692307, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 67, 'raw': 96}
{'target': 600, 'success': 88, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 151, 'raw': 224}
{'target': 600, 'success': 167, 'raw': 256}
{'target': 600, 'success': 188, 'raw': 288}
{'target': 600, 'success': 208, 'raw': 320}
{'target': 600, 'success': 230, 'raw': 352}
{'target': 600, 'success': 251, 'raw': 384}
{'target': 600, 'success': 271, 'raw': 416}
{'target': 600, 'success': 299, 'raw': 448}
{'target': 600, 'success': 318, 'raw': 480}
{'target': 600, 'success': 339, 'raw': 512}
{'target': 600, 'success': 365, 'raw': 544}
{'target': 600, 'success': 388, 'raw': 576}
{'target': 600, 'success': 404, 'raw': 608}
{'target': 600, 'success': 423, 'raw': 640}
{'target': 600, 'success': 441, 'raw': 672}
{'target': 600, 'success': 466, 'raw': 704}
{'target': 600, 'success': 488, 'raw': 736}
{'target': 600, 'success': 510, 'raw': 768}
{'target': 600, 'success': 533, 'raw': 800}
{'target': 600, 'success': 555, 'raw': 832}
{'target': 600, 'success': 579, 'raw': 864}
{'target': 600, 'success': 598, 'raw': 896}
{'target': 600, 'success': 624, 'raw': 928}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.6724137931034483, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 190, 'raw': 256}
{'target': 600, 'success': 212, 'raw': 288}
{'target': 600, 'success': 237, 'raw': 320}
{'target': 600, 'success': 262, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 338, 'raw': 448}
{'target': 600, 'success': 363, 'raw': 480}
{'target': 600, 'success': 389, 'raw': 512}
{'target': 600, 'success': 416, 'raw': 544}
{'target': 600, 'success': 442, 'raw': 576}
{'target': 600, 'success': 466, 'raw': 608}
{'target': 600, 'success': 491, 'raw': 640}
{'target': 600, 'success': 518, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 569, 'raw': 736}
{'target': 600, 'success': 598, 'raw': 768}
{'target': 600, 'success': 625, 'raw': 800}
{'prompt': 'Relation : religion .', 'success_rate': 0.78125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/1_ext.jsonl'}}
estimate vocab size: 13036
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13136, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.47it/s]Extractor Estimating: 2it [00:01,  1.48it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.57it/s]Extractor Estimating: 5it [00:03,  1.63it/s]Extractor Estimating: 6it [00:04,  1.43it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.52it/s]Extractor Estimating: 9it [00:05,  1.60it/s]Extractor Estimating: 10it [00:06,  1.69it/s]Extractor Estimating: 11it [00:08,  1.11s/it]Extractor Estimating: 12it [00:09,  1.04it/s]Extractor Estimating: 13it [00:10,  1.11it/s]Extractor Estimating: 14it [00:12,  1.33s/it]Extractor Estimating: 15it [00:12,  1.11s/it]Extractor Estimating: 16it [00:13,  1.03it/s]Extractor Estimating: 17it [00:15,  1.14s/it]Extractor Estimating: 18it [00:15,  1.01it/s]Extractor Estimating: 19it [00:16,  1.10it/s]Extractor Estimating: 20it [00:17,  1.22it/s]Extractor Estimating: 21it [00:18,  1.02it/s]Extractor Estimating: 22it [00:19,  1.15it/s]Extractor Estimating: 23it [00:19,  1.26it/s]Extractor Estimating: 24it [00:20,  1.37it/s]Extractor Estimating: 25it [00:21,  1.16it/s]Extractor Estimating: 26it [00:22,  1.23it/s]Extractor Estimating: 27it [00:22,  1.26it/s]Extractor Estimating: 28it [00:23,  1.26it/s]Extractor Estimating: 29it [00:24,  1.24it/s]Extractor Estimating: 30it [00:25,  1.32it/s]Extractor Estimating: 31it [00:25,  1.36it/s]Extractor Estimating: 32it [00:26,  1.43it/s]Extractor Estimating: 33it [00:27,  1.44it/s]Extractor Estimating: 34it [00:28,  1.29it/s]Extractor Estimating: 35it [00:28,  1.37it/s]Extractor Estimating: 36it [00:29,  1.42it/s]Extractor Estimating: 37it [00:29,  1.50it/s]Extractor Estimating: 38it [00:30,  1.51it/s]Extractor Estimating: 39it [00:31,  1.19it/s]Extractor Estimating: 40it [00:32,  1.25it/s]Extractor Estimating: 41it [00:33,  1.28it/s]Extractor Estimating: 42it [00:33,  1.36it/s]Extractor Estimating: 43it [00:34,  1.20it/s]Extractor Estimating: 44it [00:35,  1.31it/s]Extractor Estimating: 45it [00:36,  1.36it/s]Extractor Estimating: 46it [00:36,  1.37it/s]Extractor Estimating: 47it [00:38,  1.11it/s]Extractor Estimating: 48it [00:38,  1.18it/s]Extractor Estimating: 49it [00:39,  1.28it/s]Extractor Estimating: 50it [00:40,  1.36it/s]Extractor Estimating: 51it [00:41,  1.07it/s]Extractor Estimating: 52it [00:42,  1.20it/s]Extractor Estimating: 53it [00:42,  1.31it/s]Extractor Estimating: 54it [00:43,  1.43it/s]Extractor Estimating: 55it [00:43,  1.49it/s]Extractor Estimating: 56it [00:44,  1.55it/s]Extractor Estimating: 57it [00:45,  1.59it/s]Extractor Estimating: 58it [00:45,  1.46it/s]Extractor Estimating: 59it [00:46,  1.53it/s]Extractor Estimating: 60it [00:47,  1.64it/s]Extractor Estimating: 61it [00:47,  1.68it/s]Extractor Estimating: 62it [00:48,  1.70it/s]Extractor Estimating: 63it [00:48,  1.55it/s]Extractor Estimating: 64it [00:49,  1.56it/s]Extractor Estimating: 65it [00:50,  1.59it/s]Extractor Estimating: 66it [00:50,  1.63it/s]Extractor Estimating: 67it [00:51,  1.64it/s]Extractor Estimating: 68it [00:52,  1.37it/s]Extractor Estimating: 69it [00:52,  1.46it/s]Extractor Estimating: 70it [00:53,  1.47it/s]Extractor Estimating: 71it [00:54,  1.57it/s]Extractor Estimating: 72it [00:54,  1.60it/s]Extractor Estimating: 73it [00:56,  1.24it/s]Extractor Estimating: 74it [00:56,  1.35it/s]Extractor Estimating: 75it [00:57,  1.39it/s]Extractor Estimating: 76it [00:57,  1.47it/s]Extractor Estimating: 77it [00:58,  1.30it/s]Extractor Estimating: 78it [00:59,  1.44it/s]Extractor Estimating: 79it [00:59,  1.48it/s]Extractor Estimating: 80it [01:00,  1.54it/s]Extractor Estimating: 81it [01:01,  1.64it/s]Extractor Estimating: 82it [01:01,  1.51it/s]Extractor Estimating: 83it [01:02,  1.59it/s]Extractor Estimating: 84it [01:03,  1.60it/s]Extractor Estimating: 85it [01:03,  1.60it/s]Extractor Estimating: 86it [01:04,  1.68it/s]Extractor Estimating: 87it [01:05,  1.46it/s]Extractor Estimating: 88it [01:05,  1.55it/s]Extractor Estimating: 89it [01:06,  1.66it/s]Extractor Estimating: 90it [01:06,  1.72it/s]Extractor Estimating: 91it [01:07,  1.73it/s]Extractor Estimating: 92it [01:08,  1.52it/s]Extractor Estimating: 93it [01:08,  1.60it/s]Extractor Estimating: 94it [01:09,  1.48it/s]Extractor Estimating: 95it [01:09,  1.60it/s]Extractor Estimating: 96it [01:10,  1.69it/s]Extractor Estimating: 97it [01:11,  1.55it/s]Extractor Estimating: 98it [01:11,  1.60it/s]Extractor Estimating: 99it [01:12,  1.66it/s]Extractor Estimating: 100it [01:12,  1.77it/s]Extractor Estimating: 101it [01:13,  1.37it/s]Extractor Estimating: 102it [01:14,  1.46it/s]Extractor Estimating: 103it [01:15,  1.49it/s]Extractor Estimating: 104it [01:15,  1.54it/s]Extractor Estimating: 105it [01:16,  1.28it/s]Extractor Estimating: 106it [01:17,  1.36it/s]Extractor Estimating: 107it [01:18,  1.44it/s]Extractor Estimating: 108it [01:18,  1.47it/s]Extractor Estimating: 109it [01:19,  1.57it/s]Extractor Estimating: 110it [01:20,  1.41it/s]Extractor Estimating: 111it [01:20,  1.47it/s]Extractor Estimating: 112it [01:21,  1.58it/s]Extractor Estimating: 113it [01:21,  1.65it/s]Extractor Estimating: 114it [01:22,  1.68it/s]Extractor Estimating: 115it [01:23,  1.49it/s]Extractor Estimating: 116it [01:23,  1.53it/s]Extractor Estimating: 117it [01:24,  1.62it/s]Extractor Estimating: 118it [01:24,  1.68it/s]Extractor Estimating: 119it [01:25,  1.63it/s]Extractor Estimating: 120it [01:26,  1.35it/s]Extractor Estimating: 121it [01:27,  1.41it/s]Extractor Estimating: 122it [01:27,  1.48it/s]Extractor Estimating: 123it [01:28,  1.55it/s]Extractor Estimating: 124it [01:29,  1.57it/s]Extractor Estimating: 125it [01:29,  1.47it/s]Extractor Estimating: 126it [01:30,  1.55it/s]Extractor Estimating: 127it [01:30,  1.63it/s]Extractor Estimating: 128it [01:31,  1.68it/s]Extractor Estimating: 129it [01:32,  1.73it/s]Extractor Estimating: 130it [01:32,  1.77it/s]Extractor Estimating: 131it [01:33,  1.38it/s]Extractor Estimating: 132it [01:34,  1.52it/s]Extractor Estimating: 133it [01:34,  1.62it/s]Extractor Estimating: 134it [01:35,  1.66it/s]Extractor Estimating: 135it [01:35,  1.70it/s]Extractor Estimating: 136it [01:37,  1.13it/s]Extractor Estimating: 137it [01:37,  1.27it/s]Extractor Estimating: 138it [01:38,  1.40it/s]Extractor Estimating: 139it [01:39,  1.48it/s]Extractor Estimating: 140it [01:39,  1.44it/s]Extractor Estimating: 141it [01:40,  1.53it/s]Extractor Estimating: 142it [01:40,  1.59it/s]Extractor Estimating: 143it [01:41,  1.69it/s]Extractor Estimating: 144it [01:41,  1.74it/s]Extractor Estimating: 145it [01:42,  1.74it/s]Extractor Estimating: 146it [01:43,  1.40it/s]Extractor Estimating: 147it [01:45,  1.07it/s]Extractor Estimating: 148it [01:45,  1.23it/s]Extractor Estimating: 149it [01:46,  1.38it/s]Extractor Estimating: 150it [01:46,  1.47it/s]Extractor Estimating: 151it [01:47,  1.14it/s]Extractor Estimating: 152it [01:48,  1.24it/s]Extractor Estimating: 153it [01:49,  1.33it/s]Extractor Estimating: 154it [01:49,  1.43it/s]Extractor Estimating: 155it [01:50,  1.21it/s]Extractor Estimating: 156it [01:51,  1.32it/s]Extractor Estimating: 157it [01:52,  1.39it/s]Extractor Estimating: 158it [01:53,  1.31it/s]Extractor Estimating: 159it [01:54,  1.04it/s]Extractor Estimating: 160it [01:55,  1.15it/s]Extractor Estimating: 161it [01:55,  1.26it/s]Extractor Estimating: 162it [01:56,  1.34it/s]Extractor Estimating: 163it [01:57,  1.17it/s]Extractor Estimating: 164it [01:58,  1.28it/s]Extractor Estimating: 165it [01:58,  1.26it/s]Extractor Estimating: 166it [01:59,  1.36it/s]Extractor Estimating: 167it [02:00,  1.25it/s]Extractor Estimating: 168it [02:01,  1.30it/s]Extractor Estimating: 169it [02:01,  1.40it/s]Extractor Estimating: 170it [02:02,  1.48it/s]Extractor Estimating: 171it [02:02,  1.56it/s]Extractor Estimating: 172it [02:03,  1.37it/s]Extractor Estimating: 173it [02:04,  1.46it/s]Extractor Estimating: 174it [02:05,  1.51it/s]Extractor Estimating: 175it [02:05,  1.48it/s]Extractor Estimating: 176it [02:06,  1.54it/s]Extractor Estimating: 177it [02:07,  1.50it/s]Extractor Estimating: 178it [02:07,  1.52it/s]Extractor Estimating: 179it [02:08,  1.59it/s]Extractor Estimating: 180it [02:08,  1.61it/s]Extractor Estimating: 181it [02:09,  1.60it/s]Extractor Estimating: 182it [02:10,  1.16it/s]Extractor Estimating: 183it [02:11,  1.26it/s]Extractor Estimating: 184it [02:12,  1.39it/s]Extractor Estimating: 185it [02:12,  1.51it/s]Extractor Estimating: 186it [02:13,  1.35it/s]Extractor Estimating: 187it [02:14,  1.40it/s]Extractor Estimating: 188it [02:14,  1.44it/s]Extractor Estimating: 189it [02:15,  1.53it/s]Extractor Estimating: 190it [02:15,  1.59it/s]Extractor Estimating: 191it [02:17,  1.21it/s]Extractor Estimating: 192it [02:17,  1.32it/s]Extractor Estimating: 193it [02:18,  1.42it/s]Extractor Estimating: 194it [02:19,  1.49it/s]Extractor Estimating: 195it [02:20,  1.13it/s]Extractor Estimating: 196it [02:20,  1.27it/s]Extractor Estimating: 197it [02:21,  1.34it/s]Extractor Estimating: 198it [02:22,  1.47it/s]Extractor Estimating: 199it [02:22,  1.54it/s]Extractor Estimating: 200it [02:24,  1.14it/s]Extractor Estimating: 201it [02:24,  1.22it/s]Extractor Estimating: 202it [02:25,  1.31it/s]Extractor Estimating: 203it [02:26,  1.40it/s]Extractor Estimating: 204it [02:26,  1.35it/s]Extractor Estimating: 205it [02:27,  1.45it/s]Extractor Estimating: 206it [02:28,  1.45it/s]Extractor Estimating: 207it [02:28,  1.45it/s]Extractor Estimating: 208it [02:29,  1.46it/s]Extractor Estimating: 209it [02:30,  1.36it/s]Extractor Estimating: 210it [02:31,  1.34it/s]Extractor Estimating: 211it [02:31,  1.36it/s]Extractor Estimating: 212it [02:32,  1.40it/s]Extractor Estimating: 213it [02:33,  1.43it/s]Extractor Estimating: 214it [02:33,  1.45it/s]Extractor Estimating: 215it [02:34,  1.52it/s]Extractor Estimating: 216it [02:35,  1.52it/s]Extractor Estimating: 217it [02:35,  1.49it/s]Extractor Estimating: 218it [02:36,  1.41it/s]Extractor Estimating: 219it [02:37,  1.46it/s]Extractor Estimating: 220it [02:37,  1.46it/s]Extractor Estimating: 221it [02:38,  1.47it/s]Extractor Estimating: 222it [02:39,  1.48it/s]Extractor Estimating: 223it [02:39,  1.46it/s]Extractor Estimating: 224it [02:40,  1.49it/s]Extractor Estimating: 225it [02:41,  1.50it/s]Extractor Estimating: 226it [02:41,  1.65it/s]Extractor Estimating: 227it [02:42,  1.71it/s]Extractor Estimating: 228it [02:42,  1.77it/s]Extractor Estimating: 229it [02:43,  1.62it/s]Extractor Estimating: 230it [02:43,  1.72it/s]Extractor Estimating: 231it [02:44,  1.78it/s]Extractor Estimating: 232it [02:45,  1.78it/s]Extractor Estimating: 233it [02:45,  1.76it/s]Extractor Estimating: 234it [02:46,  1.80it/s]Extractor Estimating: 235it [02:46,  1.73it/s]Extractor Estimating: 236it [02:47,  1.73it/s]Extractor Estimating: 237it [02:47,  1.81it/s]Extractor Estimating: 238it [02:48,  1.66it/s]Extractor Estimating: 239it [02:49,  1.73it/s]Extractor Estimating: 240it [02:49,  1.61it/s]Extractor Estimating: 241it [02:50,  1.62it/s]Extractor Estimating: 242it [02:50,  1.69it/s]Extractor Estimating: 243it [02:51,  1.70it/s]Extractor Estimating: 244it [02:52,  1.68it/s]Extractor Estimating: 245it [02:52,  1.74it/s]Extractor Estimating: 246it [02:53,  1.77it/s]Extractor Estimating: 247it [02:54,  1.44it/s]Extractor Estimating: 248it [02:54,  1.52it/s]Extractor Estimating: 249it [02:55,  1.54it/s]Extractor Estimating: 250it [02:55,  1.61it/s]Extractor Estimating: 251it [02:56,  1.61it/s]Extractor Estimating: 252it [02:57,  1.29it/s]Extractor Estimating: 253it [02:58,  1.32it/s]Extractor Estimating: 254it [02:59,  1.40it/s]Extractor Estimating: 255it [02:59,  1.44it/s]Extractor Estimating: 256it [03:00,  1.24it/s]Extractor Estimating: 257it [03:01,  1.34it/s]Extractor Estimating: 258it [03:01,  1.44it/s]Extractor Estimating: 259it [03:02,  1.47it/s]Extractor Estimating: 260it [03:03,  1.53it/s]Extractor Estimating: 261it [03:03,  1.44it/s]Extractor Estimating: 262it [03:04,  1.46it/s]Extractor Estimating: 263it [03:05,  1.48it/s]Extractor Estimating: 264it [03:05,  1.51it/s]Extractor Estimating: 265it [03:06,  1.55it/s]Extractor Estimating: 266it [03:07,  1.56it/s]Extractor Estimating: 267it [03:07,  1.58it/s]Extractor Estimating: 268it [03:08,  1.61it/s]Extractor Estimating: 269it [03:08,  1.59it/s]Extractor Estimating: 270it [03:09,  1.57it/s]Extractor Estimating: 271it [03:10,  1.50it/s]Extractor Estimating: 272it [03:11,  1.52it/s]Extractor Estimating: 273it [03:11,  1.55it/s]Extractor Estimating: 274it [03:12,  1.60it/s]Extractor Estimating: 275it [03:12,  1.56it/s]Extractor Estimating: 276it [03:13,  1.55it/s]Extractor Estimating: 277it [03:14,  1.61it/s]Extractor Estimating: 278it [03:14,  1.67it/s]Extractor Estimating: 279it [03:15,  1.64it/s]Extractor Estimating: 280it [03:15,  1.68it/s]Extractor Estimating: 281it [03:16,  1.69it/s]Extractor Estimating: 282it [03:17,  1.56it/s]Extractor Estimating: 283it [03:17,  1.65it/s]Extractor Estimating: 284it [03:18,  1.62it/s]Extractor Estimating: 285it [03:18,  1.68it/s]Extractor Estimating: 286it [03:19,  1.66it/s]Extractor Estimating: 287it [03:20,  1.64it/s]Extractor Estimating: 288it [03:20,  1.65it/s]Extractor Estimating: 289it [03:21,  1.65it/s]Extractor Estimating: 290it [03:21,  1.69it/s]Extractor Estimating: 291it [03:22,  1.76it/s]Extractor Estimating: 292it [03:23,  1.73it/s]Extractor Estimating: 293it [03:23,  1.74it/s]Extractor Estimating: 294it [03:24,  1.75it/s]Extractor Estimating: 295it [03:25,  1.24it/s]Extractor Estimating: 296it [03:26,  1.33it/s]Extractor Estimating: 297it [03:26,  1.42it/s]Extractor Estimating: 298it [03:27,  1.52it/s]Extractor Estimating: 299it [03:28,  1.43it/s]Extractor Estimating: 300it [03:28,  1.48it/s]Extractor Estimating: 301it [03:29,  1.51it/s]Extractor Estimating: 302it [03:29,  1.53it/s]Extractor Estimating: 303it [03:30,  1.54it/s]Extractor Estimating: 304it [03:31,  1.29it/s]Extractor Estimating: 305it [03:32,  1.41it/s]Extractor Estimating: 306it [03:32,  1.55it/s]Extractor Estimating: 307it [03:33,  1.61it/s]Extractor Estimating: 308it [03:33,  1.65it/s]Extractor Estimating: 309it [03:34,  1.31it/s]Extractor Estimating: 310it [03:35,  1.30it/s]Extractor Estimating: 311it [03:36,  1.41it/s]Extractor Estimating: 312it [03:36,  1.46it/s]Extractor Estimating: 313it [03:38,  1.20it/s]Extractor Estimating: 314it [03:38,  1.30it/s]Extractor Estimating: 315it [03:39,  1.40it/s]Extractor Estimating: 316it [03:39,  1.53it/s]Extractor Estimating: 317it [03:40,  1.57it/s]Extractor Estimating: 318it [03:41,  1.38it/s]Extractor Estimating: 319it [03:41,  1.51it/s]Extractor Estimating: 320it [03:42,  1.55it/s]Extractor Estimating: 321it [03:43,  1.58it/s]Extractor Estimating: 322it [03:43,  1.59it/s]Extractor Estimating: 323it [03:44,  1.51it/s]Extractor Estimating: 324it [03:45,  1.58it/s]Extractor Estimating: 325it [03:45,  1.65it/s]Extractor Estimating: 326it [03:46,  1.63it/s]Extractor Estimating: 327it [03:46,  1.69it/s]Extractor Estimating: 328it [03:47,  1.57it/s]Extractor Estimating: 329it [03:47,  1.68it/s]Extractor Estimating: 330it [03:48,  1.73it/s]Extractor Estimating: 331it [03:49,  1.74it/s]Extractor Estimating: 332it [03:49,  1.80it/s]Extractor Estimating: 333it [03:50,  1.82it/s]Extractor Estimating: 334it [03:50,  1.58it/s]Extractor Estimating: 335it [03:51,  1.59it/s]Extractor Estimating: 336it [03:52,  1.66it/s]Extractor Estimating: 337it [03:52,  1.63it/s]Extractor Estimating: 338it [03:53,  1.73it/s]Extractor Estimating: 339it [03:53,  1.69it/s]Extractor Estimating: 340it [03:54,  1.68it/s]Extractor Estimating: 341it [03:55,  1.62it/s]Extractor Estimating: 342it [03:55,  1.63it/s]Extractor Estimating: 343it [03:56,  1.68it/s]Extractor Estimating: 344it [03:56,  1.71it/s]Extractor Estimating: 345it [03:57,  1.76it/s]Extractor Estimating: 346it [03:57,  1.74it/s]Extractor Estimating: 347it [03:59,  1.11it/s]Extractor Estimating: 348it [04:00,  1.27it/s]Extractor Estimating: 349it [04:00,  1.43it/s]Extractor Estimating: 350it [04:01,  1.45it/s]Extractor Estimating: 351it [04:03,  1.00s/it]Extractor Estimating: 352it [04:03,  1.11it/s]Extractor Estimating: 353it [04:04,  1.21it/s]Extractor Estimating: 354it [04:06,  1.07s/it]Extractor Estimating: 355it [04:06,  1.04it/s]Extractor Estimating: 356it [04:07,  1.14it/s]Extractor Estimating: 357it [04:08,  1.05s/it]Extractor Estimating: 358it [04:09,  1.07it/s]Extractor Estimating: 359it [04:10,  1.21it/s]Extractor Estimating: 360it [04:10,  1.31it/s]Extractor Estimating: 361it [04:12,  1.12s/it]Extractor Estimating: 362it [04:13,  1.01it/s]Extractor Estimating: 363it [04:13,  1.14it/s]Extractor Estimating: 364it [04:15,  1.03it/s]Extractor Estimating: 365it [04:15,  1.14it/s]Extractor Estimating: 366it [04:16,  1.25it/s]Extractor Estimating: 367it [04:17,  1.33it/s]Extractor Estimating: 368it [04:17,  1.30it/s]Extractor Estimating: 369it [04:18,  1.40it/s]Extractor Estimating: 370it [04:19,  1.42it/s]Extractor Estimating: 371it [04:19,  1.47it/s]Extractor Estimating: 372it [04:20,  1.52it/s]Extractor Estimating: 373it [04:21,  1.47it/s]Extractor Estimating: 374it [04:21,  1.55it/s]Extractor Estimating: 375it [04:22,  1.63it/s]Extractor Estimating: 375it [04:23,  1.43it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:05,198 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:05,677 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:05,677 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:05,678 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:05,678 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 17:38:11,382 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 17:38:11,973 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:38:13,164 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 17:38:15,777 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:38:15,925 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:20,583 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:21,116 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:21,117 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:21,117 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 17:38:21,117 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 17:38:23,372 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 17:38:23,373 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 17:38:24,843 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 17:38:25,470 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 17:38:25,471 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 19:43:54,115 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 19:43:59,855 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 4500}
num of filtered data: 7482 mean pseudo reward: 0.9295738996784172
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 24998
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 25098, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=25098, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.943, loss:904.3740
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.950, loss:878.1174
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.945, loss:845.3742
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 88, avg_time 0.940, loss:830.7381
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 188, avg_time 0.953, loss:819.8552
>> valid entity prec:0.5685, rec:0.6047, f1:0.5860
>> valid relation prec:0.3199, rec:0.1223, f1:0.1770
>> valid relation with NER prec:0.3199, rec:0.1223, f1:0.1770
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 288, avg_time 2.297, loss:857.6374
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 76, avg_time 0.928, loss:796.9800
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 176, avg_time 0.942, loss:854.7133
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 276, avg_time 0.952, loss:845.4714
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 64, avg_time 0.961, loss:818.9235
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6025, rec:0.5018, f1:0.5476
>> valid relation prec:0.3307, rec:0.0831, f1:0.1328
>> valid relation with NER prec:0.3307, rec:0.0831, f1:0.1328
g_step 1100, step 164, avg_time 2.152, loss:808.1303
g_step 1200, step 264, avg_time 0.930, loss:840.3183
g_step 1300, step 52, avg_time 0.947, loss:809.7834
g_step 1400, step 152, avg_time 0.939, loss:764.9685
g_step 1500, step 252, avg_time 0.937, loss:833.5103
>> valid entity prec:0.6245, rec:0.5476, f1:0.5836
>> valid relation prec:0.4372, rec:0.0937, f1:0.1543
>> valid relation with NER prec:0.4372, rec:0.0937, f1:0.1543
g_step 1600, step 40, avg_time 2.150, loss:763.1736
g_step 1700, step 140, avg_time 0.923, loss:737.1834
g_step 1800, step 240, avg_time 0.935, loss:768.2386
g_step 1900, step 28, avg_time 0.938, loss:759.8306
g_step 2000, step 128, avg_time 0.919, loss:711.3500
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5750, rec:0.5486, f1:0.5615
>> valid relation prec:0.3311, rec:0.1143, f1:0.1700
>> valid relation with NER prec:0.3311, rec:0.1143, f1:0.1700
g_step 2100, step 228, avg_time 2.145, loss:713.9868
g_step 2200, step 16, avg_time 0.944, loss:736.5256
g_step 2300, step 116, avg_time 0.931, loss:668.8241
g_step 2400, step 216, avg_time 0.939, loss:694.1380
g_step 2500, step 4, avg_time 0.928, loss:712.9725
>> valid entity prec:0.6148, rec:0.5334, f1:0.5712
>> valid relation prec:0.3289, rec:0.0926, f1:0.1445
>> valid relation with NER prec:0.3289, rec:0.0926, f1:0.1445
g_step 2600, step 104, avg_time 2.145, loss:646.0493
g_step 2700, step 204, avg_time 0.929, loss:673.2837
g_step 2800, step 304, avg_time 0.938, loss:674.4505
g_step 2900, step 92, avg_time 0.943, loss:604.1579
g_step 3000, step 192, avg_time 0.928, loss:647.3017
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6064, rec:0.5715, f1:0.5884
>> valid relation prec:0.2811, rec:0.1074, f1:0.1555
>> valid relation with NER prec:0.2811, rec:0.1074, f1:0.1555
new max entity f1 on valid!
g_step 3100, step 292, avg_time 2.149, loss:637.2952
g_step 3200, step 80, avg_time 0.928, loss:599.3886
g_step 3300, step 180, avg_time 0.945, loss:590.8071
g_step 3400, step 280, avg_time 0.913, loss:619.3562
g_step 3500, step 68, avg_time 0.926, loss:578.0805
>> valid entity prec:0.5805, rec:0.5895, f1:0.5850
>> valid relation prec:0.2923, rec:0.1109, f1:0.1608
>> valid relation with NER prec:0.2923, rec:0.1109, f1:0.1608
g_step 3600, step 168, avg_time 2.133, loss:560.8166
g_step 3700, step 268, avg_time 0.934, loss:594.8719
g_step 3800, step 56, avg_time 0.950, loss:561.2397
g_step 3900, step 156, avg_time 0.913, loss:538.9930
g_step 4000, step 256, avg_time 0.945, loss:552.5067
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6127, rec:0.5155, f1:0.5599
>> valid relation prec:0.3019, rec:0.1109, f1:0.1622
>> valid relation with NER prec:0.3019, rec:0.1109, f1:0.1622
g_step 4100, step 44, avg_time 2.157, loss:539.2702
g_step 4200, step 144, avg_time 0.935, loss:511.3988
g_step 4300, step 244, avg_time 2.132, loss:532.0152
g_step 4400, step 32, avg_time 1.313, loss:535.4454
g_step 4500, step 132, avg_time 0.934, loss:514.8584
>> valid entity prec:0.5805, rec:0.5787, f1:0.5796
>> valid relation prec:0.2219, rec:0.1029, f1:0.1406
>> valid relation with NER prec:0.2219, rec:0.1029, f1:0.1406
g_step 4600, step 232, avg_time 2.380, loss:502.2563
g_step 4700, step 20, avg_time 0.916, loss:512.1270
g_step 4800, step 120, avg_time 0.924, loss:465.7256
g_step 4900, step 220, avg_time 0.922, loss:491.5853
g_step 5000, step 8, avg_time 0.931, loss:496.7239
learning rate was adjusted to 0.0008
>> valid entity prec:0.6005, rec:0.5429, f1:0.5702
>> valid relation prec:0.2445, rec:0.1186, f1:0.1598
>> valid relation with NER prec:0.2445, rec:0.1186, f1:0.1598
g_step 5100, step 108, avg_time 2.136, loss:458.4430
g_step 5200, step 208, avg_time 0.933, loss:469.2759
g_step 5300, step 308, avg_time 0.927, loss:478.2750
g_step 5400, step 96, avg_time 0.926, loss:419.4893
g_step 5500, step 196, avg_time 0.924, loss:472.0596
>> valid entity prec:0.5995, rec:0.5171, f1:0.5552
>> valid relation prec:0.2454, rec:0.1106, f1:0.1525
>> valid relation with NER prec:0.2454, rec:0.1106, f1:0.1525
g_step 5600, step 296, avg_time 2.141, loss:469.7528
g_step 5700, step 84, avg_time 0.917, loss:437.7233
g_step 5800, step 184, avg_time 0.925, loss:431.1170
g_step 5900, step 284, avg_time 0.931, loss:440.9893
g_step 6000, step 72, avg_time 0.922, loss:411.7033
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5924, rec:0.5285, f1:0.5586
>> valid relation prec:0.2257, rec:0.1109, f1:0.1487
>> valid relation with NER prec:0.2257, rec:0.1109, f1:0.1487
g_step 6100, step 172, avg_time 2.126, loss:415.4048
g_step 6200, step 272, avg_time 0.925, loss:415.1128
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 19:44:00 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 19:44:00 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_19-43-53_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 19:44:24 - WARNING - datasets.builder -   Using custom data configuration default-ee1c95de590fa45b
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ee1c95de590fa45b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [01:10, 70.79s/ tables]                                0 tables [00:00, ? tables/s]1 tables [00:03,  3.57s/ tables]                                [INFO|configuration_utils.py:515] 2023-08-28 19:46:29,854 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:46:30,040 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 19:46:30,041 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 19:46:30,042 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 19:46:32,047 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:46:32,339 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:46:32,340 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:46:32,340 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:46:32,340 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:46:32,340 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 19:46:32,340 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 19:46:44,357 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 19:46:52,237 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 19:46:53,192 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ee1c95de590fa45b/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:10<01:13, 10.55s/ba] 25%|██▌       | 2/8 [00:10<00:26,  4.48s/ba] 38%|███▊      | 3/8 [00:10<00:12,  2.53s/ba] 50%|█████     | 4/8 [00:11<00:06,  1.61s/ba] 62%|██████▎   | 5/8 [00:11<00:03,  1.10s/ba] 75%|███████▌  | 6/8 [00:11<00:01,  1.19ba/s] 88%|████████▊ | 7/8 [00:11<00:00,  1.59ba/s]100%|██████████| 8/8 [00:12<00:00,  1.58ba/s]100%|██████████| 8/8 [00:12<00:00,  1.57s/ba]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:02<00:06,  2.02s/ba] 50%|█████     | 2/4 [00:02<00:01,  1.03ba/s] 75%|███████▌  | 3/4 [00:02<00:00,  1.59ba/s]100%|██████████| 4/4 [00:02<00:00,  2.37ba/s]100%|██████████| 4/4 [00:02<00:00,  1.55ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:01<00:07,  1.13s/ba] 25%|██▌       | 2/8 [00:01<00:03,  1.90ba/s] 50%|█████     | 4/8 [00:01<00:01,  4.00ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  5.87ba/s]100%|██████████| 8/8 [00:01<00:00,  7.91ba/s]100%|██████████| 8/8 [00:01<00:00,  4.74ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:04,  1.44s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.52ba/s]100%|██████████| 4/4 [00:01<00:00,  3.53ba/s]100%|██████████| 4/4 [00:01<00:00,  2.39ba/s]
[INFO|trainer.py:414] 2023-08-28 19:47:58,916 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 19:48:02,054 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 19:48:02,500 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 19:48:02,500 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 19:48:02,500 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 19:48:02,500 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 19:48:02,500 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 19:48:02,500 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:33<5:29:20, 33.84s/it]  0%|          | 2/585 [00:37<2:38:19, 16.29s/it]  1%|          | 3/585 [00:41<1:41:29, 10.46s/it]  1%|          | 4/585 [00:43<1:08:18,  7.05s/it]  1%|          | 5/585 [00:44<47:45,  4.94s/it]    1%|          | 6/585 [00:45<36:00,  3.73s/it]  1%|          | 7/585 [00:46<25:06,  2.61s/it]  1%|▏         | 8/585 [00:46<18:45,  1.95s/it]  2%|▏         | 9/585 [00:46<13:44,  1.43s/it]  2%|▏         | 10/585 [00:47<10:20,  1.08s/it]  2%|▏         | 11/585 [00:47<08:00,  1.19it/s]  2%|▏         | 12/585 [00:47<06:50,  1.40it/s]  2%|▏         | 13/585 [00:48<05:35,  1.70it/s]  2%|▏         | 14/585 [00:48<04:44,  2.01it/s]  3%|▎         | 15/585 [00:49<05:53,  1.61it/s]  3%|▎         | 16/585 [00:49<05:42,  1.66it/s]  3%|▎         | 17/585 [00:55<20:04,  2.12s/it]  3%|▎         | 18/585 [00:56<16:35,  1.76s/it]  3%|▎         | 19/585 [00:56<12:24,  1.32s/it]  3%|▎         | 20/585 [00:57<09:29,  1.01s/it]  4%|▎         | 21/585 [00:57<07:26,  1.26it/s]  4%|▍         | 22/585 [00:57<06:00,  1.56it/s]  4%|▍         | 23/585 [00:57<05:00,  1.87it/s]  4%|▍         | 24/585 [00:58<04:18,  2.17it/s]  4%|▍         | 25/585 [00:58<03:49,  2.44it/s]  4%|▍         | 26/585 [00:58<03:28,  2.68it/s]  5%|▍         | 27/585 [00:59<05:13,  1.78it/s]  5%|▍         | 28/585 [01:00<04:27,  2.08it/s]  5%|▍         | 29/585 [01:00<03:54,  2.37it/s]  5%|▌         | 30/585 [01:00<03:32,  2.62it/s]  5%|▌         | 31/585 [01:01<03:16,  2.82it/s]  5%|▌         | 32/585 [01:01<03:05,  2.99it/s]  6%|▌         | 33/585 [01:01<02:57,  3.11it/s]  6%|▌         | 34/585 [01:01<02:51,  3.21it/s]  6%|▌         | 35/585 [01:02<03:40,  2.50it/s]  6%|▌         | 36/585 [01:02<03:21,  2.73it/s]  6%|▋         | 37/585 [01:03<03:07,  2.92it/s]  6%|▋         | 38/585 [01:03<02:57,  3.08it/s]  7%|▋         | 39/585 [01:03<02:50,  3.19it/s]  7%|▋         | 40/585 [01:03<02:46,  3.28it/s]  7%|▋         | 41/585 [01:04<02:42,  3.35it/s]  7%|▋         | 42/585 [01:04<02:40,  3.39it/s]  7%|▋         | 43/585 [01:04<02:38,  3.42it/s]  8%|▊         | 44/585 [01:05<02:36,  3.45it/s]  8%|▊         | 45/585 [01:06<04:41,  1.92it/s]  8%|▊         | 46/585 [01:07<06:30,  1.38it/s]  8%|▊         | 47/585 [01:07<05:18,  1.69it/s]  8%|▊         | 48/585 [01:07<04:28,  2.00it/s]  8%|▊         | 49/585 [01:08<03:53,  2.30it/s]  9%|▊         | 50/585 [01:08<03:28,  2.56it/s]  9%|▊         | 51/585 [01:08<03:11,  2.79it/s]  9%|▉         | 52/585 [01:09<02:59,  2.97it/s]  9%|▉         | 53/585 [01:09<04:28,  1.98it/s]  9%|▉         | 54/585 [01:10<03:53,  2.28it/s]  9%|▉         | 55/585 [01:10<03:28,  2.54it/s] 10%|▉         | 56/585 [01:10<03:10,  2.77it/s] 10%|▉         | 57/585 [01:11<02:58,  2.96it/s] 10%|▉         | 58/585 [01:11<02:49,  3.10it/s] 10%|█         | 59/585 [01:11<02:43,  3.21it/s] 10%|█         | 60/585 [01:11<02:39,  3.29it/s] 10%|█         | 61/585 [01:12<02:36,  3.35it/s] 11%|█         | 62/585 [01:12<03:15,  2.68it/s] 11%|█         | 63/585 [01:13<03:00,  2.89it/s] 11%|█         | 64/585 [01:13<02:51,  3.05it/s] 11%|█         | 65/585 [01:13<02:44,  3.17it/s] 11%|█▏        | 66/585 [01:13<02:39,  3.26it/s] 11%|█▏        | 67/585 [01:14<02:35,  3.33it/s] 12%|█▏        | 68/585 [01:14<02:32,  3.38it/s] 12%|█▏        | 69/585 [01:14<02:30,  3.42it/s] 12%|█▏        | 70/585 [01:15<02:29,  3.45it/s] 12%|█▏        | 71/585 [01:15<02:28,  3.46it/s] 12%|█▏        | 72/585 [01:16<04:35,  1.86it/s] 12%|█▏        | 73/585 [01:16<03:56,  2.17it/s] 13%|█▎        | 74/585 [01:16<03:28,  2.45it/s] 13%|█▎        | 75/585 [01:17<03:09,  2.69it/s] 13%|█▎        | 76/585 [01:17<02:55,  2.90it/s] 13%|█▎        | 77/585 [01:17<02:46,  3.05it/s] 13%|█▎        | 78/585 [01:18<02:39,  3.18it/s] 14%|█▎        | 79/585 [01:18<02:34,  3.27it/s] 14%|█▎        | 80/585 [01:19<04:22,  1.92it/s] 14%|█▍        | 81/585 [01:19<03:46,  2.22it/s] 14%|█▍        | 82/585 [01:20<03:21,  2.50it/s] 14%|█▍        | 83/585 [01:20<03:03,  2.73it/s] 14%|█▍        | 84/585 [01:20<02:51,  2.93it/s] 15%|█▍        | 85/585 [01:20<02:42,  3.08it/s] 15%|█▍        | 86/585 [01:21<02:36,  3.20it/s] 15%|█▍        | 87/585 [01:21<02:31,  3.28it/s] 15%|█▌        | 88/585 [01:23<06:15,  1.32it/s] 15%|█▌        | 89/585 [01:23<05:05,  1.62it/s] 15%|█▌        | 90/585 [01:23<04:16,  1.93it/s] 16%|█▌        | 91/585 [01:24<03:41,  2.23it/s] 16%|█▌        | 92/585 [01:24<03:16,  2.51it/s] 16%|█▌        | 93/585 [01:24<02:59,  2.74it/s] 16%|█▌        | 94/585 [01:25<04:04,  2.01it/s] 16%|█▌        | 95/585 [01:25<03:32,  2.30it/s] 16%|█▋        | 96/585 [01:26<03:10,  2.57it/s] 17%|█▋        | 97/585 [01:26<02:54,  2.79it/s] 17%|█▋        | 98/585 [01:26<02:43,  2.97it/s] 17%|█▋        | 99/585 [01:26<02:36,  3.11it/s] 17%|█▋        | 100/585 [01:27<02:30,  3.22it/s] 17%|█▋        | 101/585 [01:27<02:26,  3.30it/s] 17%|█▋        | 102/585 [01:27<02:23,  3.36it/s] 18%|█▊        | 103/585 [01:29<04:53,  1.64it/s] 18%|█▊        | 104/585 [01:29<04:05,  1.96it/s] 18%|█▊        | 105/585 [01:29<03:32,  2.25it/s] 18%|█▊        | 106/585 [01:29<03:09,  2.52it/s] 18%|█▊        | 107/585 [01:30<02:53,  2.75it/s] 18%|█▊        | 108/585 [01:30<02:42,  2.94it/s] 19%|█▊        | 109/585 [01:30<02:34,  3.09it/s] 19%|█▉        | 110/585 [01:32<04:42,  1.68it/s] 19%|█▉        | 111/585 [01:32<03:58,  1.99it/s] 19%|█▉        | 112/585 [01:32<03:26,  2.29it/s] 19%|█▉        | 113/585 [01:32<03:05,  2.55it/s] 19%|█▉        | 114/585 [01:33<02:49,  2.78it/s] 20%|█▉        | 115/585 [01:33<02:38,  2.96it/s] 20%|█▉        | 116/585 [01:33<02:31,  3.10it/s] 20%|██        | 117/585 [01:34<02:25,  3.21it/s][INFO|trainer.py:2140] 2023-08-28 19:49:38,082 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:49:38,082 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 19:49:38,082 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.87it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.21it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.37it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.53it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.93it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.68it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.33it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.58it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.14it/s][A
 12%|█▏        | 52/437 [00:01<00:14, 27.23it/s][A
 13%|█▎        | 57/437 [00:02<00:31, 12.25it/s][A
 14%|█▎        | 60/437 [00:02<00:35, 10.67it/s][A
 15%|█▍        | 65/437 [00:02<00:26, 14.29it/s][A
 16%|█▌        | 70/437 [00:02<00:20, 18.28it/s][A
 17%|█▋        | 75/437 [00:03<00:16, 22.50it/s][A
 18%|█▊        | 80/437 [00:03<00:13, 26.62it/s][A
 19%|█▉        | 85/437 [00:03<00:11, 30.48it/s][A
 21%|██        | 90/437 [00:04<00:24, 13.89it/s][A
 22%|██▏       | 95/437 [00:04<00:19, 17.63it/s][A
 23%|██▎       | 100/437 [00:04<00:15, 21.56it/s][A
 24%|██▍       | 105/437 [00:04<00:13, 25.54it/s][A
 25%|██▌       | 110/437 [00:04<00:11, 29.35it/s][A
 26%|██▋       | 115/437 [00:04<00:13, 23.87it/s][A
 27%|██▋       | 120/437 [00:04<00:11, 27.78it/s][A
 29%|██▊       | 125/437 [00:05<00:09, 31.38it/s][A
 30%|██▉       | 130/437 [00:05<00:08, 34.47it/s][A
 31%|███       | 135/437 [00:05<00:08, 37.13it/s][A
 32%|███▏      | 140/437 [00:05<00:07, 39.17it/s][A
 33%|███▎      | 145/437 [00:05<00:07, 40.80it/s][A
 34%|███▍      | 150/437 [00:05<00:06, 41.81it/s][A
 35%|███▌      | 155/437 [00:05<00:06, 42.15it/s][A
 37%|███▋      | 160/437 [00:05<00:06, 42.48it/s][A
 38%|███▊      | 165/437 [00:05<00:06, 43.01it/s][A
 39%|███▉      | 170/437 [00:06<00:06, 43.47it/s][A
 40%|████      | 175/437 [00:06<00:12, 21.42it/s][A
 41%|████      | 180/437 [00:06<00:10, 25.52it/s][A
 42%|████▏     | 184/437 [00:07<00:13, 18.37it/s][A
 43%|████▎     | 189/437 [00:07<00:10, 22.59it/s][A
 44%|████▍     | 194/437 [00:07<00:09, 26.65it/s][A
 46%|████▌     | 199/437 [00:07<00:07, 30.46it/s][A
 47%|████▋     | 204/437 [00:07<00:06, 33.77it/s][A
 48%|████▊     | 209/437 [00:08<00:06, 36.60it/s][A
 49%|████▉     | 214/437 [00:08<00:19, 11.65it/s][A
 50%|█████     | 219/437 [00:08<00:14, 15.01it/s][A
 51%|█████▏    | 224/437 [00:08<00:11, 18.77it/s][A
 52%|█████▏    | 229/437 [00:09<00:09, 22.76it/s][A
 54%|█████▎    | 234/437 [00:09<00:07, 26.62it/s][A
 55%|█████▍    | 239/437 [00:09<00:06, 30.23it/s][A
 56%|█████▌    | 244/437 [00:09<00:05, 33.75it/s][A
 57%|█████▋    | 249/437 [00:09<00:05, 36.36it/s][A
 58%|█████▊    | 254/437 [00:09<00:04, 38.17it/s][A
 59%|█████▉    | 259/437 [00:09<00:04, 39.50it/s][A
 60%|██████    | 264/437 [00:09<00:04, 40.69it/s][A
 62%|██████▏   | 269/437 [00:09<00:04, 41.77it/s][A
 63%|██████▎   | 274/437 [00:10<00:03, 42.62it/s][A
 64%|██████▍   | 279/437 [00:10<00:03, 43.30it/s][A
 65%|██████▍   | 284/437 [00:10<00:03, 43.77it/s][A
 66%|██████▌   | 289/437 [00:10<00:03, 44.09it/s][A
 67%|██████▋   | 294/437 [00:10<00:03, 44.20it/s][A
 68%|██████▊   | 299/437 [00:11<00:05, 24.07it/s][A
 69%|██████▉   | 303/437 [00:11<00:06, 21.07it/s][A
 70%|███████   | 306/437 [00:11<00:06, 18.80it/s][A
 71%|███████   | 311/437 [00:11<00:05, 23.80it/s][A
 72%|███████▏  | 316/437 [00:11<00:04, 28.10it/s][A
 73%|███████▎  | 321/437 [00:11<00:03, 31.91it/s][A
 74%|███████▍  | 325/437 [00:12<00:04, 23.37it/s][A
 76%|███████▌  | 330/437 [00:12<00:03, 27.65it/s][A
 77%|███████▋  | 335/437 [00:12<00:03, 31.42it/s][A
 78%|███████▊  | 339/437 [00:12<00:03, 29.72it/s][A
 79%|███████▊  | 344/437 [00:12<00:02, 33.52it/s][A
 80%|███████▉  | 349/437 [00:12<00:02, 36.43it/s][A
 81%|████████  | 354/437 [00:12<00:02, 36.02it/s][A
 82%|████████▏ | 359/437 [00:12<00:01, 39.18it/s][A
 83%|████████▎ | 364/437 [00:13<00:01, 40.78it/s][A
 84%|████████▍ | 369/437 [00:13<00:01, 41.97it/s][A
 86%|████████▌ | 374/437 [00:14<00:06, 10.46it/s][A
 86%|████████▋ | 378/437 [00:14<00:05,  9.89it/s][A
 88%|████████▊ | 383/437 [00:14<00:04, 13.19it/s][A
 89%|████████▉ | 388/437 [00:15<00:02, 16.88it/s][A
 90%|████████▉ | 393/437 [00:15<00:02, 20.90it/s][A
 91%|█████████ | 398/437 [00:15<00:01, 24.98it/s][A
 92%|█████████▏| 403/437 [00:15<00:01, 28.90it/s][A
 93%|█████████▎| 408/437 [00:16<00:02, 13.66it/s][A
 95%|█████████▍| 413/437 [00:16<00:01, 17.50it/s][A
 96%|█████████▌| 418/437 [00:16<00:00, 21.43it/s][A
 97%|█████████▋| 423/437 [00:16<00:00, 25.42it/s][A
 98%|█████████▊| 428/437 [00:16<00:00, 29.27it/s][A
 99%|█████████▉| 433/437 [00:20<00:00,  4.09it/s][A
100%|█████████▉| 436/437 [00:20<00:00,  4.55it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:20<00:00,  4.55it/s][A 20%|██        | 117/585 [01:56<02:25,  3.21it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:50:04,274 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 19:50:10,965 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 19:52:30,107 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 19:52:37,153 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 19:52:38,676 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [08:27<16:08:11, 124.39s/it] 20%|██        | 119/585 [08:29<11:18:52, 87.41s/it]  21%|██        | 120/585 [08:29<7:54:51, 61.27s/it]  21%|██        | 121/585 [08:29<5:32:20, 42.98s/it] 21%|██        | 122/585 [08:29<3:52:47, 30.17s/it] 21%|██        | 123/585 [08:30<2:43:15, 21.20s/it] 21%|██        | 124/585 [08:30<1:54:41, 14.93s/it] 21%|██▏       | 125/585 [08:30<1:20:45, 10.53s/it] 22%|██▏       | 126/585 [08:31<57:03,  7.46s/it]   22%|██▏       | 127/585 [08:31<40:48,  5.35s/it] 22%|██▏       | 128/585 [08:31<29:09,  3.83s/it] 22%|██▏       | 129/585 [08:32<21:00,  2.76s/it] 22%|██▏       | 130/585 [08:32<15:19,  2.02s/it] 22%|██▏       | 131/585 [08:32<11:20,  1.50s/it] 23%|██▎       | 132/585 [08:32<08:34,  1.13s/it] 23%|██▎       | 133/585 [08:33<08:20,  1.11s/it] 23%|██▎       | 134/585 [08:34<06:28,  1.16it/s] 23%|██▎       | 135/585 [08:34<05:35,  1.34it/s] 23%|██▎       | 136/585 [08:34<04:32,  1.65it/s] 23%|██▎       | 137/585 [08:35<03:48,  1.96it/s] 24%|██▎       | 138/585 [08:35<03:17,  2.26it/s] 24%|██▍       | 139/585 [08:35<02:56,  2.53it/s] 24%|██▍       | 140/585 [08:36<02:40,  2.76it/s] 24%|██▍       | 141/585 [08:36<02:30,  2.95it/s] 24%|██▍       | 142/585 [08:36<02:22,  3.11it/s] 24%|██▍       | 143/585 [08:36<02:17,  3.22it/s] 25%|██▍       | 144/585 [08:37<02:13,  3.30it/s] 25%|██▍       | 145/585 [08:37<02:51,  2.56it/s] 25%|██▍       | 146/585 [08:38<02:37,  2.79it/s] 25%|██▌       | 147/585 [08:38<02:27,  2.97it/s] 25%|██▌       | 148/585 [08:38<02:20,  3.12it/s] 25%|██▌       | 149/585 [08:39<02:15,  3.23it/s] 26%|██▌       | 150/585 [08:39<02:11,  3.31it/s] 26%|██▌       | 151/585 [08:39<02:09,  3.36it/s] 26%|██▌       | 152/585 [08:39<02:07,  3.41it/s] 26%|██▌       | 153/585 [08:40<02:05,  3.44it/s] 26%|██▋       | 154/585 [08:40<02:04,  3.46it/s] 26%|██▋       | 155/585 [08:41<02:46,  2.58it/s] 27%|██▋       | 156/585 [08:41<02:33,  2.80it/s] 27%|██▋       | 157/585 [08:41<02:23,  2.98it/s] 27%|██▋       | 158/585 [08:41<02:16,  3.12it/s] 27%|██▋       | 159/585 [08:42<02:11,  3.23it/s] 27%|██▋       | 160/585 [08:42<02:08,  3.31it/s] 28%|██▊       | 161/585 [08:42<02:05,  3.37it/s] 28%|██▊       | 162/585 [08:43<02:04,  3.41it/s] 28%|██▊       | 163/585 [08:43<02:02,  3.44it/s] 28%|██▊       | 164/585 [08:43<02:01,  3.46it/s] 28%|██▊       | 165/585 [08:44<02:23,  2.94it/s] 28%|██▊       | 166/585 [08:44<02:15,  3.09it/s] 29%|██▊       | 167/585 [08:44<02:10,  3.21it/s] 29%|██▊       | 168/585 [08:44<02:06,  3.29it/s] 29%|██▉       | 169/585 [08:45<02:04,  3.35it/s] 29%|██▉       | 170/585 [08:45<02:02,  3.40it/s] 29%|██▉       | 171/585 [08:45<02:00,  3.43it/s] 29%|██▉       | 172/585 [08:46<01:59,  3.46it/s] 30%|██▉       | 173/585 [08:46<01:58,  3.47it/s] 30%|██▉       | 174/585 [08:46<01:58,  3.48it/s] 30%|██▉       | 175/585 [08:47<03:00,  2.28it/s] 30%|███       | 176/585 [08:47<02:40,  2.54it/s] 30%|███       | 177/585 [08:47<02:27,  2.77it/s] 30%|███       | 178/585 [08:48<02:17,  2.96it/s] 31%|███       | 179/585 [08:48<02:10,  3.10it/s] 31%|███       | 180/585 [08:48<02:05,  3.22it/s] 31%|███       | 181/585 [08:49<02:02,  3.29it/s] 31%|███       | 182/585 [08:49<02:00,  3.35it/s] 31%|███▏      | 183/585 [08:49<01:58,  3.40it/s] 31%|███▏      | 184/585 [08:51<04:43,  1.42it/s] 32%|███▏      | 185/585 [08:51<03:52,  1.72it/s] 32%|███▏      | 186/585 [08:51<03:17,  2.02it/s] 32%|███▏      | 187/585 [08:52<02:52,  2.31it/s] 32%|███▏      | 188/585 [08:52<02:34,  2.58it/s] 32%|███▏      | 189/585 [08:52<02:21,  2.80it/s] 32%|███▏      | 190/585 [08:53<03:24,  1.93it/s] 33%|███▎      | 191/585 [08:53<02:56,  2.24it/s] 33%|███▎      | 192/585 [08:54<02:36,  2.51it/s] 33%|███▎      | 193/585 [08:54<02:22,  2.75it/s] 33%|███▎      | 194/585 [08:54<02:13,  2.94it/s] 33%|███▎      | 195/585 [08:55<02:06,  3.09it/s] 34%|███▎      | 196/585 [08:55<02:01,  3.21it/s] 34%|███▎      | 197/585 [08:55<01:57,  3.29it/s] 34%|███▍      | 198/585 [08:55<01:55,  3.35it/s] 34%|███▍      | 199/585 [08:57<03:59,  1.61it/s] 34%|███▍      | 200/585 [08:57<03:20,  1.92it/s] 34%|███▍      | 201/585 [08:57<02:52,  2.22it/s] 35%|███▍      | 202/585 [08:58<02:33,  2.50it/s] 35%|███▍      | 203/585 [08:58<02:19,  2.74it/s] 35%|███▍      | 204/585 [08:58<02:09,  2.93it/s] 35%|███▌      | 205/585 [08:59<02:03,  3.09it/s] 35%|███▌      | 206/585 [08:59<02:41,  2.35it/s] 35%|███▌      | 207/585 [09:00<02:24,  2.61it/s] 36%|███▌      | 208/585 [09:00<02:13,  2.83it/s] 36%|███▌      | 209/585 [09:00<02:45,  2.27it/s] 36%|███▌      | 210/585 [09:01<02:27,  2.54it/s] 36%|███▌      | 211/585 [09:01<02:14,  2.77it/s] 36%|███▌      | 212/585 [09:01<02:05,  2.96it/s] 36%|███▋      | 213/585 [09:02<01:59,  3.11it/s] 37%|███▋      | 214/585 [09:02<01:55,  3.22it/s] 37%|███▋      | 215/585 [09:02<01:52,  3.30it/s] 37%|███▋      | 216/585 [09:02<01:49,  3.36it/s] 37%|███▋      | 217/585 [09:03<01:48,  3.41it/s] 37%|███▋      | 218/585 [09:04<03:14,  1.88it/s] 37%|███▋      | 219/585 [09:04<02:47,  2.19it/s] 38%|███▊      | 220/585 [09:04<02:27,  2.47it/s] 38%|███▊      | 221/585 [09:05<02:14,  2.71it/s] 38%|███▊      | 222/585 [09:05<02:04,  2.91it/s] 38%|███▊      | 223/585 [09:05<01:58,  3.07it/s] 38%|███▊      | 224/585 [09:06<01:53,  3.19it/s] 38%|███▊      | 225/585 [09:06<01:49,  3.28it/s] 39%|███▊      | 226/585 [09:07<03:52,  1.54it/s] 39%|███▉      | 227/585 [09:08<03:13,  1.85it/s] 39%|███▉      | 228/585 [09:08<02:45,  2.16it/s] 39%|███▉      | 229/585 [09:08<02:25,  2.44it/s] 39%|███▉      | 230/585 [09:08<02:12,  2.69it/s] 39%|███▉      | 231/585 [09:09<02:02,  2.89it/s] 40%|███▉      | 232/585 [09:09<01:55,  3.05it/s] 40%|███▉      | 233/585 [09:10<03:43,  1.57it/s] 40%|████      | 234/585 [09:11<03:06,  1.89it/s][INFO|trainer.py:2140] 2023-08-28 19:57:13,625 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 19:57:13,625 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 19:57:13,625 >>   Batch size = 8
{'eval_loss': 1.0291342735290527, 'eval_runtime': 20.7838, 'eval_samples_per_second': 168.016, 'eval_steps_per_second': 21.026, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.38it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.24it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.10it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.03it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.36it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.11it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.93it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.73it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.89it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.06it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 45.18it/s][A
 14%|█▍        | 62/437 [00:02<00:08, 45.13it/s][A
 15%|█▌        | 67/437 [00:02<00:29, 12.65it/s][A
 16%|█▋        | 72/437 [00:02<00:22, 16.16it/s][A
 18%|█▊        | 77/437 [00:02<00:17, 20.04it/s][A
 19%|█▉        | 82/437 [00:02<00:14, 24.09it/s][A
 20%|█▉        | 87/437 [00:02<00:12, 28.01it/s][A
 21%|██        | 92/437 [00:03<00:14, 23.63it/s][A
 22%|██▏       | 97/437 [00:03<00:12, 27.67it/s][A
 23%|██▎       | 102/437 [00:03<00:10, 31.31it/s][A
 24%|██▍       | 107/437 [00:03<00:09, 34.52it/s][A
 26%|██▌       | 112/437 [00:03<00:08, 37.17it/s][A
 27%|██▋       | 117/437 [00:03<00:08, 39.30it/s][A
 28%|██▊       | 122/437 [00:03<00:07, 40.91it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 42.16it/s][A
 30%|███       | 132/437 [00:04<00:07, 42.60it/s][A
 31%|███▏      | 137/437 [00:04<00:06, 42.86it/s][A
 32%|███▏      | 142/437 [00:04<00:06, 43.33it/s][A
 34%|███▎      | 147/437 [00:04<00:06, 43.81it/s][A
 35%|███▍      | 152/437 [00:04<00:13, 21.69it/s][A
 36%|███▌      | 157/437 [00:04<00:10, 25.69it/s][A
 37%|███▋      | 162/437 [00:05<00:09, 29.52it/s][A
 38%|███▊      | 167/437 [00:05<00:08, 32.99it/s][A
 39%|███▉      | 172/437 [00:05<00:07, 35.94it/s][A
 41%|████      | 177/437 [00:05<00:06, 38.32it/s][A
 42%|████▏     | 182/437 [00:05<00:06, 40.17it/s][A
 43%|████▎     | 187/437 [00:05<00:06, 41.42it/s][A
 44%|████▍     | 192/437 [00:05<00:05, 42.01it/s][A
 45%|████▌     | 197/437 [00:05<00:05, 42.57it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 43.08it/s][A
 47%|████▋     | 207/437 [00:06<00:05, 43.72it/s][A
 49%|████▊     | 212/437 [00:06<00:05, 44.14it/s][A
 50%|████▉     | 217/437 [00:06<00:04, 44.50it/s][A
 51%|█████     | 222/437 [00:06<00:04, 44.71it/s][A
 52%|█████▏    | 227/437 [00:06<00:04, 44.90it/s][A
 53%|█████▎    | 232/437 [00:06<00:04, 44.73it/s][A
 54%|█████▍    | 237/437 [00:06<00:04, 44.35it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 44.32it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 44.49it/s][A
 58%|█████▊    | 252/437 [00:07<00:04, 44.63it/s][A
 59%|█████▉    | 257/437 [00:07<00:04, 44.74it/s][A
 60%|█████▉    | 262/437 [00:07<00:03, 44.83it/s][A
 61%|██████    | 267/437 [00:08<00:03, 45.03it/s][A
 62%|██████▏   | 272/437 [00:08<00:10, 15.15it/s][A
 63%|██████▎   | 277/437 [00:08<00:08, 18.92it/s][A
 65%|██████▍   | 282/437 [00:08<00:06, 22.93it/s][A
 66%|██████▌   | 287/437 [00:08<00:05, 26.94it/s][A
 67%|██████▋   | 292/437 [00:08<00:04, 30.70it/s][A
 68%|██████▊   | 297/437 [00:08<00:04, 34.00it/s][A
 69%|██████▉   | 302/437 [00:08<00:03, 36.77it/s][A
 70%|███████   | 307/437 [00:09<00:03, 38.84it/s][A
 71%|███████▏  | 312/437 [00:09<00:03, 40.06it/s][A
 73%|███████▎  | 317/437 [00:09<00:02, 41.02it/s][A
 74%|███████▎  | 322/437 [00:09<00:02, 41.97it/s][A
 75%|███████▍  | 327/437 [00:09<00:02, 42.82it/s][A
 76%|███████▌  | 332/437 [00:09<00:02, 43.60it/s][A
 77%|███████▋  | 337/437 [00:09<00:02, 44.14it/s][A
 78%|███████▊  | 342/437 [00:09<00:02, 44.49it/s][A
 79%|███████▉  | 347/437 [00:09<00:02, 44.43it/s][A
 81%|████████  | 352/437 [00:10<00:01, 44.83it/s][A
 82%|████████▏ | 357/437 [00:10<00:01, 44.44it/s][A
 83%|████████▎ | 362/437 [00:10<00:01, 44.13it/s][A
 84%|████████▍ | 367/437 [00:10<00:01, 44.25it/s][A
 85%|████████▌ | 372/437 [00:10<00:01, 44.47it/s][A
 86%|████████▋ | 377/437 [00:16<00:20,  2.87it/s][A
 87%|████████▋ | 381/437 [00:16<00:15,  3.62it/s][A
 88%|████████▊ | 386/437 [00:16<00:10,  5.09it/s][A
 89%|████████▉ | 391/437 [00:16<00:06,  7.02it/s][A
 91%|█████████ | 396/437 [00:16<00:04,  9.47it/s][A
 92%|█████████▏| 401/437 [00:16<00:02, 12.47it/s][A
 93%|█████████▎| 406/437 [00:16<00:01, 15.99it/s][A
 94%|█████████▍| 411/437 [00:16<00:01, 19.88it/s][A
 95%|█████████▌| 416/437 [00:17<00:00, 23.93it/s][A
 96%|█████████▋| 421/437 [00:17<00:00, 27.68it/s][A
 97%|█████████▋| 426/437 [00:17<00:00, 31.05it/s][A
 99%|█████████▊| 431/437 [00:17<00:00, 34.06it/s][A
100%|█████████▉| 436/437 [00:17<00:00, 36.66it/s][A                                                 
                                                 [A 40%|████      | 234/585 [09:28<03:06,  1.89it/s]
100%|██████████| 437/437 [00:17<00:00, 36.66it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 19:57:34,136 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 19:57:35,670 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:00:01,953 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:00:11,707 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:00:14,894 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [15:26<10:59:36, 113.07s/it] 40%|████      | 236/585 [15:27<7:42:11, 79.46s/it]   41%|████      | 237/585 [15:28<5:23:06, 55.71s/it] 41%|████      | 238/585 [15:28<3:46:01, 39.08s/it] 41%|████      | 239/585 [15:28<2:38:15, 27.44s/it] 41%|████      | 240/585 [15:28<1:50:57, 19.30s/it] 41%|████      | 241/585 [15:29<1:17:56, 13.60s/it] 41%|████▏     | 242/585 [15:29<54:53,  9.60s/it]   42%|████▏     | 243/585 [15:29<38:48,  6.81s/it] 42%|████▏     | 244/585 [15:31<29:25,  5.18s/it] 42%|████▏     | 245/585 [15:31<21:01,  3.71s/it] 42%|████▏     | 246/585 [15:31<15:09,  2.68s/it] 42%|████▏     | 247/585 [15:32<11:04,  1.97s/it] 42%|████▏     | 248/585 [15:32<08:12,  1.46s/it] 43%|████▎     | 249/585 [15:32<06:13,  1.11s/it] 43%|████▎     | 250/585 [15:32<04:49,  1.16it/s] 43%|████▎     | 251/585 [15:33<04:11,  1.33it/s] 43%|████▎     | 252/585 [15:33<03:24,  1.63it/s] 43%|████▎     | 253/585 [15:33<02:51,  1.94it/s] 43%|████▎     | 254/585 [15:34<02:28,  2.23it/s] 44%|████▎     | 255/585 [15:34<02:12,  2.50it/s] 44%|████▍     | 256/585 [15:34<02:00,  2.72it/s] 44%|████▍     | 257/585 [15:35<01:52,  2.92it/s] 44%|████▍     | 258/585 [15:35<01:46,  3.08it/s] 44%|████▍     | 259/585 [15:35<01:41,  3.20it/s] 44%|████▍     | 260/585 [15:35<01:38,  3.29it/s] 45%|████▍     | 261/585 [15:36<02:21,  2.29it/s] 45%|████▍     | 262/585 [15:37<02:06,  2.56it/s] 45%|████▍     | 263/585 [15:37<01:55,  2.79it/s] 45%|████▌     | 264/585 [15:37<01:47,  2.97it/s] 45%|████▌     | 265/585 [15:37<01:42,  3.12it/s] 45%|████▌     | 266/585 [15:38<01:38,  3.23it/s] 46%|████▌     | 267/585 [15:38<01:36,  3.31it/s] 46%|████▌     | 268/585 [15:38<01:34,  3.37it/s] 46%|████▌     | 269/585 [15:39<01:32,  3.41it/s] 46%|████▌     | 270/585 [15:39<01:54,  2.76it/s] 46%|████▋     | 271/585 [15:39<01:46,  2.95it/s] 46%|████▋     | 272/585 [15:40<01:41,  3.10it/s] 47%|████▋     | 273/585 [15:40<01:37,  3.21it/s] 47%|████▋     | 274/585 [15:40<01:34,  3.29it/s] 47%|████▋     | 275/585 [15:40<01:32,  3.35it/s] 47%|████▋     | 276/585 [15:41<01:31,  3.39it/s] 47%|████▋     | 277/585 [15:41<01:29,  3.43it/s] 48%|████▊     | 278/585 [15:41<01:29,  3.45it/s] 48%|████▊     | 279/585 [15:42<01:28,  3.47it/s] 48%|████▊     | 280/585 [15:42<01:58,  2.57it/s] 48%|████▊     | 281/585 [15:43<01:48,  2.79it/s] 48%|████▊     | 282/585 [15:43<01:41,  2.97it/s] 48%|████▊     | 283/585 [15:43<01:36,  3.12it/s] 49%|████▊     | 284/585 [15:43<01:33,  3.22it/s] 49%|████▊     | 285/585 [15:44<01:30,  3.30it/s] 49%|████▉     | 286/585 [15:44<01:29,  3.36it/s] 49%|████▉     | 287/585 [15:44<01:27,  3.39it/s] 49%|████▉     | 288/585 [15:45<01:26,  3.42it/s] 49%|████▉     | 289/585 [15:45<01:25,  3.45it/s] 50%|████▉     | 290/585 [15:46<02:12,  2.23it/s] 50%|████▉     | 291/585 [15:46<01:57,  2.50it/s] 50%|████▉     | 292/585 [15:46<01:47,  2.73it/s] 50%|█████     | 293/585 [15:46<01:39,  2.93it/s] 50%|█████     | 294/585 [15:47<01:34,  3.08it/s] 50%|█████     | 295/585 [15:47<01:30,  3.19it/s] 51%|█████     | 296/585 [15:47<01:28,  3.28it/s] 51%|█████     | 297/585 [15:48<01:26,  3.35it/s] 51%|█████     | 298/585 [15:48<01:24,  3.39it/s] 51%|█████     | 299/585 [15:49<02:04,  2.29it/s] 51%|█████▏    | 300/585 [15:49<01:51,  2.55it/s] 51%|█████▏    | 301/585 [15:49<01:42,  2.78it/s] 52%|█████▏    | 302/585 [15:50<01:35,  2.96it/s] 52%|█████▏    | 303/585 [15:50<01:30,  3.11it/s] 52%|█████▏    | 304/585 [15:50<01:27,  3.22it/s] 52%|█████▏    | 305/585 [15:50<01:24,  3.30it/s] 52%|█████▏    | 306/585 [15:51<01:23,  3.36it/s] 52%|█████▏    | 307/585 [15:51<01:21,  3.41it/s] 53%|█████▎    | 308/585 [15:52<02:07,  2.18it/s] 53%|█████▎    | 309/585 [15:52<01:52,  2.46it/s] 53%|█████▎    | 310/585 [15:52<01:41,  2.70it/s] 53%|█████▎    | 311/585 [15:53<01:34,  2.90it/s] 53%|█████▎    | 312/585 [15:53<01:29,  3.06it/s] 54%|█████▎    | 313/585 [15:53<01:25,  3.19it/s] 54%|█████▎    | 314/585 [15:54<01:22,  3.28it/s] 54%|█████▍    | 315/585 [15:54<01:20,  3.34it/s] 54%|█████▍    | 316/585 [15:54<01:19,  3.39it/s] 54%|█████▍    | 317/585 [15:55<01:47,  2.50it/s] 54%|█████▍    | 318/585 [15:55<01:37,  2.74it/s] 55%|█████▍    | 319/585 [15:55<01:30,  2.93it/s] 55%|█████▍    | 320/585 [15:56<01:25,  3.08it/s] 55%|█████▍    | 321/585 [15:56<01:22,  3.18it/s] 55%|█████▌    | 322/585 [15:56<01:20,  3.26it/s] 55%|█████▌    | 323/585 [15:56<01:19,  3.31it/s] 55%|█████▌    | 324/585 [15:57<01:17,  3.35it/s] 56%|█████▌    | 325/585 [15:57<01:16,  3.38it/s] 56%|█████▌    | 326/585 [15:57<01:16,  3.40it/s] 56%|█████▌    | 327/585 [15:58<01:37,  2.64it/s] 56%|█████▌    | 328/585 [15:58<01:30,  2.84it/s] 56%|█████▌    | 329/585 [15:58<01:25,  3.00it/s] 56%|█████▋    | 330/585 [15:59<01:21,  3.12it/s] 57%|█████▋    | 331/585 [15:59<01:18,  3.22it/s] 57%|█████▋    | 332/585 [15:59<01:17,  3.29it/s] 57%|█████▋    | 333/585 [16:00<01:15,  3.33it/s] 57%|█████▋    | 334/585 [16:00<01:14,  3.37it/s] 57%|█████▋    | 335/585 [16:00<01:13,  3.39it/s] 57%|█████▋    | 336/585 [16:01<01:12,  3.41it/s] 58%|█████▊    | 337/585 [16:02<02:21,  1.75it/s] 58%|█████▊    | 338/585 [16:02<02:00,  2.06it/s] 58%|█████▊    | 339/585 [16:02<01:45,  2.34it/s] 58%|█████▊    | 340/585 [16:03<01:34,  2.59it/s] 58%|█████▊    | 341/585 [16:03<01:27,  2.80it/s] 58%|█████▊    | 342/585 [16:03<01:22,  2.96it/s] 59%|█████▊    | 343/585 [16:03<01:18,  3.09it/s] 59%|█████▉    | 344/585 [16:04<01:15,  3.19it/s] 59%|█████▉    | 345/585 [16:05<01:49,  2.19it/s] 59%|█████▉    | 346/585 [16:05<01:37,  2.46it/s] 59%|█████▉    | 347/585 [16:05<01:28,  2.69it/s] 59%|█████▉    | 348/585 [16:05<01:22,  2.88it/s] 60%|█████▉    | 349/585 [16:06<01:17,  3.03it/s] 60%|█████▉    | 350/585 [16:06<01:14,  3.15it/s] 60%|██████    | 351/585 [16:06<01:12,  3.23it/s][INFO|trainer.py:2140] 2023-08-28 20:04:09,321 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:04:09,321 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:04:09,322 >>   Batch size = 8
{'eval_loss': 1.037089228630066, 'eval_runtime': 17.5926, 'eval_samples_per_second': 198.492, 'eval_steps_per_second': 24.84, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.96it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.65it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.00it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.15it/s][A
  6%|▌         | 27/437 [00:00<00:17, 23.27it/s][A
  7%|▋         | 32/437 [00:00<00:14, 27.74it/s][A
  8%|▊         | 37/437 [00:01<00:12, 31.70it/s][A
 10%|▉         | 42/437 [00:01<00:11, 35.00it/s][A
 11%|█         | 47/437 [00:01<00:10, 37.68it/s][A
 12%|█▏        | 52/437 [00:01<00:09, 39.76it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 41.28it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 42.25it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 42.59it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 42.96it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.27it/s][A
 19%|█▉        | 82/437 [00:02<00:08, 43.86it/s][A
 20%|█▉        | 87/437 [00:02<00:07, 44.26it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.55it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.85it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.90it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.87it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.51it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.26it/s][A
 28%|██▊       | 122/437 [00:03<00:07, 44.31it/s][A
 29%|██▉       | 127/437 [00:03<00:06, 44.39it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.59it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.79it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.96it/s][A
 34%|███▎      | 147/437 [00:04<00:06, 45.11it/s][A
 35%|███▍      | 152/437 [00:04<00:17, 16.40it/s][A
 36%|███▌      | 157/437 [00:04<00:13, 20.28it/s][A
 37%|███▋      | 162/437 [00:04<00:11, 24.29it/s][A
 38%|███▊      | 167/437 [00:04<00:09, 28.10it/s][A
 39%|███▉      | 172/437 [00:04<00:08, 31.73it/s][A
 41%|████      | 177/437 [00:04<00:07, 34.87it/s][A
 42%|████▏     | 182/437 [00:04<00:06, 37.48it/s][A
 43%|████▎     | 187/437 [00:05<00:06, 39.37it/s][A
 44%|████▍     | 192/437 [00:05<00:06, 40.50it/s][A
 45%|████▌     | 197/437 [00:05<00:05, 41.44it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 42.23it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 43.00it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 43.61it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.01it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.37it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.66it/s][A
 53%|█████▎    | 232/437 [00:06<00:04, 44.59it/s][A
 54%|█████▍    | 237/437 [00:06<00:04, 44.33it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 44.27it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 44.27it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 44.38it/s][A
 59%|█████▉    | 257/437 [00:07<00:04, 44.64it/s][A
 60%|█████▉    | 262/437 [00:07<00:13, 12.83it/s][A
 61%|██████    | 267/437 [00:07<00:10, 16.34it/s][A
 62%|██████▏   | 272/437 [00:07<00:08, 20.20it/s][A
 63%|██████▎   | 277/437 [00:08<00:06, 24.23it/s][A
 65%|██████▍   | 282/437 [00:08<00:05, 28.17it/s][A
 66%|██████▌   | 287/437 [00:08<00:04, 31.77it/s][A
 67%|██████▋   | 292/437 [00:08<00:04, 34.92it/s][A
 68%|██████▊   | 297/437 [00:08<00:03, 37.35it/s][A
 69%|██████▉   | 302/437 [00:08<00:03, 38.75it/s][A
 70%|███████   | 307/437 [00:08<00:03, 40.10it/s][A
 71%|███████▏  | 312/437 [00:08<00:03, 41.32it/s][A
 73%|███████▎  | 317/437 [00:08<00:02, 42.30it/s][A
 74%|███████▎  | 322/437 [00:09<00:02, 43.19it/s][A
 75%|███████▍  | 327/437 [00:09<00:02, 43.76it/s][A
 76%|███████▌  | 332/437 [00:09<00:02, 44.26it/s][A
 77%|███████▋  | 337/437 [00:09<00:02, 44.55it/s][A
 78%|███████▊  | 342/437 [00:09<00:02, 44.50it/s][A
 79%|███████▉  | 347/437 [00:09<00:02, 44.18it/s][A
 81%|████████  | 352/437 [00:10<00:01, 44.05it/s][A
 82%|████████▏ | 357/437 [00:11<00:07, 10.38it/s][A
 83%|████████▎ | 362/437 [00:11<00:05, 13.50it/s][A
 84%|████████▍ | 367/437 [00:11<00:04, 17.10it/s][A
 85%|████████▌ | 372/437 [00:11<00:03, 21.04it/s][A
 86%|████████▋ | 377/437 [00:11<00:02, 25.06it/s][A
 87%|████████▋ | 382/437 [00:11<00:01, 28.96it/s][A
 89%|████████▊ | 387/437 [00:11<00:01, 32.48it/s][A
 90%|████████▉ | 392/437 [00:11<00:01, 35.44it/s][A
 91%|█████████ | 397/437 [00:11<00:01, 37.46it/s][A
 92%|█████████▏| 402/437 [00:12<00:00, 38.97it/s][A
 93%|█████████▎| 407/437 [00:12<00:00, 40.38it/s][A
 94%|█████████▍| 412/437 [00:12<00:00, 41.61it/s][A
 95%|█████████▌| 417/437 [00:12<00:00, 42.58it/s][A
 97%|█████████▋| 422/437 [00:12<00:00, 43.29it/s][A
 98%|█████████▊| 427/437 [00:12<00:00, 43.85it/s][A
 99%|█████████▉| 432/437 [00:13<00:00, 44.30it/s][A
100%|██████████| 437/437 [00:13<00:00, 22.47it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [16:20<01:12,  3.23it/s]
100%|██████████| 437/437 [00:13<00:00, 22.47it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:04:26,671 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 20:04:30,718 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:06:44,247 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:06:51,419 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:06:54,490 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [22:52<7:54:04, 122.08s/it] 60%|██████    | 353/585 [22:54<5:31:40, 85.78s/it]  61%|██████    | 354/585 [22:54<3:51:29, 60.13s/it] 61%|██████    | 355/585 [22:54<2:41:40, 42.18s/it] 61%|██████    | 356/585 [22:54<1:53:00, 29.61s/it] 61%|██████    | 357/585 [22:55<1:19:05, 20.81s/it] 61%|██████    | 358/585 [22:55<55:27, 14.66s/it]   61%|██████▏   | 359/585 [22:55<38:58, 10.35s/it] 62%|██████▏   | 360/585 [22:56<27:29,  7.33s/it] 62%|██████▏   | 361/585 [22:56<19:38,  5.26s/it] 62%|██████▏   | 362/585 [22:56<14:00,  3.77s/it] 62%|██████▏   | 363/585 [22:57<10:04,  2.73s/it] 62%|██████▏   | 364/585 [22:57<07:20,  1.99s/it] 62%|██████▏   | 365/585 [22:57<05:25,  1.48s/it] 63%|██████▎   | 366/585 [22:57<04:05,  1.12s/it] 63%|██████▎   | 367/585 [22:58<03:09,  1.15it/s] 63%|██████▎   | 368/585 [22:58<02:30,  1.44it/s] 63%|██████▎   | 369/585 [22:58<02:03,  1.75it/s] 63%|██████▎   | 370/585 [22:59<01:44,  2.06it/s] 63%|██████▎   | 371/585 [22:59<01:58,  1.81it/s] 64%|██████▎   | 372/585 [23:00<01:40,  2.12it/s] 64%|██████▍   | 373/585 [23:00<01:28,  2.41it/s] 64%|██████▍   | 374/585 [23:00<01:19,  2.66it/s] 64%|██████▍   | 375/585 [23:00<01:13,  2.87it/s] 64%|██████▍   | 376/585 [23:01<01:08,  3.04it/s] 64%|██████▍   | 377/585 [23:01<01:05,  3.15it/s] 65%|██████▍   | 378/585 [23:01<01:03,  3.25it/s] 65%|██████▍   | 379/585 [23:02<01:01,  3.33it/s] 65%|██████▍   | 380/585 [23:02<01:00,  3.38it/s] 65%|██████▌   | 381/585 [23:03<01:50,  1.84it/s] 65%|██████▌   | 382/585 [23:03<01:34,  2.15it/s] 65%|██████▌   | 383/585 [23:04<01:22,  2.44it/s] 66%|██████▌   | 384/585 [23:04<01:14,  2.68it/s] 66%|██████▌   | 385/585 [23:04<01:09,  2.89it/s] 66%|██████▌   | 386/585 [23:04<01:05,  3.05it/s] 66%|██████▌   | 387/585 [23:05<01:02,  3.18it/s] 66%|██████▋   | 388/585 [23:05<01:00,  3.27it/s] 66%|██████▋   | 389/585 [23:06<01:22,  2.37it/s] 67%|██████▋   | 390/585 [23:06<01:14,  2.63it/s] 67%|██████▋   | 391/585 [23:06<01:08,  2.84it/s] 67%|██████▋   | 392/585 [23:07<01:04,  3.01it/s] 67%|██████▋   | 393/585 [23:07<01:01,  3.14it/s] 67%|██████▋   | 394/585 [23:07<00:58,  3.24it/s] 68%|██████▊   | 395/585 [23:07<00:57,  3.32it/s] 68%|██████▊   | 396/585 [23:08<00:56,  3.37it/s] 68%|██████▊   | 397/585 [23:08<00:55,  3.41it/s] 68%|██████▊   | 398/585 [23:08<00:54,  3.44it/s] 68%|██████▊   | 399/585 [23:10<01:54,  1.63it/s] 68%|██████▊   | 400/585 [23:10<01:35,  1.94it/s] 69%|██████▊   | 401/585 [23:10<01:22,  2.24it/s] 69%|██████▊   | 402/585 [23:10<01:12,  2.51it/s] 69%|██████▉   | 403/585 [23:11<01:06,  2.75it/s] 69%|██████▉   | 404/585 [23:11<01:01,  2.94it/s] 69%|██████▉   | 405/585 [23:11<00:58,  3.09it/s] 69%|██████▉   | 406/585 [23:12<01:04,  2.76it/s] 70%|██████▉   | 407/585 [23:12<01:00,  2.95it/s] 70%|██████▉   | 408/585 [23:12<00:57,  3.10it/s] 70%|██████▉   | 409/585 [23:13<00:54,  3.21it/s] 70%|███████   | 410/585 [23:13<00:53,  3.29it/s] 70%|███████   | 411/585 [23:13<00:51,  3.36it/s] 70%|███████   | 412/585 [23:13<00:50,  3.40it/s] 71%|███████   | 413/585 [23:14<00:50,  3.43it/s] 71%|███████   | 414/585 [23:14<00:49,  3.45it/s] 71%|███████   | 415/585 [23:14<00:49,  3.47it/s] 71%|███████   | 416/585 [23:15<00:55,  3.04it/s] 71%|███████▏  | 417/585 [23:15<00:53,  3.16it/s] 71%|███████▏  | 418/585 [23:15<00:51,  3.26it/s] 72%|███████▏  | 419/585 [23:16<00:49,  3.33it/s] 72%|███████▏  | 420/585 [23:16<00:48,  3.38it/s] 72%|███████▏  | 421/585 [23:16<00:48,  3.41it/s] 72%|███████▏  | 422/585 [23:16<00:47,  3.44it/s] 72%|███████▏  | 423/585 [23:17<00:46,  3.46it/s] 72%|███████▏  | 424/585 [23:17<00:46,  3.47it/s] 73%|███████▎  | 425/585 [23:17<00:45,  3.48it/s] 73%|███████▎  | 426/585 [23:18<00:45,  3.49it/s] 73%|███████▎  | 427/585 [23:19<01:18,  2.02it/s] 73%|███████▎  | 428/585 [23:19<01:07,  2.32it/s] 73%|███████▎  | 429/585 [23:19<01:00,  2.58it/s] 74%|███████▎  | 430/585 [23:19<00:55,  2.80it/s] 74%|███████▎  | 431/585 [23:20<00:51,  2.98it/s] 74%|███████▍  | 432/585 [23:20<00:49,  3.12it/s] 74%|███████▍  | 433/585 [23:20<00:47,  3.23it/s] 74%|███████▍  | 434/585 [23:21<00:45,  3.30it/s] 74%|███████▍  | 435/585 [23:21<00:44,  3.36it/s] 75%|███████▍  | 436/585 [23:22<00:59,  2.49it/s] 75%|███████▍  | 437/585 [23:22<00:54,  2.73it/s] 75%|███████▍  | 438/585 [23:22<00:50,  2.92it/s] 75%|███████▌  | 439/585 [23:22<00:47,  3.08it/s] 75%|███████▌  | 440/585 [23:23<00:45,  3.20it/s] 75%|███████▌  | 441/585 [23:23<00:43,  3.28it/s] 76%|███████▌  | 442/585 [23:23<00:42,  3.35it/s] 76%|███████▌  | 443/585 [23:24<00:41,  3.39it/s] 76%|███████▌  | 444/585 [23:24<00:41,  3.43it/s] 76%|███████▌  | 445/585 [23:24<00:40,  3.45it/s] 76%|███████▌  | 446/585 [23:26<01:33,  1.49it/s] 76%|███████▋  | 447/585 [23:26<01:17,  1.79it/s] 77%|███████▋  | 448/585 [23:26<01:05,  2.09it/s] 77%|███████▋  | 449/585 [23:27<00:57,  2.38it/s] 77%|███████▋  | 450/585 [23:27<00:51,  2.63it/s] 77%|███████▋  | 451/585 [23:27<00:47,  2.85it/s] 77%|███████▋  | 452/585 [23:27<00:44,  3.02it/s] 77%|███████▋  | 453/585 [23:28<00:41,  3.15it/s] 78%|███████▊  | 454/585 [23:28<00:40,  3.25it/s] 78%|███████▊  | 455/585 [23:29<00:55,  2.34it/s] 78%|███████▊  | 456/585 [23:29<00:49,  2.59it/s] 78%|███████▊  | 457/585 [23:29<00:45,  2.81it/s] 78%|███████▊  | 458/585 [23:29<00:42,  2.99it/s] 78%|███████▊  | 459/585 [23:30<00:40,  3.13it/s] 79%|███████▊  | 460/585 [23:30<00:38,  3.23it/s] 79%|███████▉  | 461/585 [23:30<00:37,  3.31it/s] 79%|███████▉  | 462/585 [23:31<00:36,  3.37it/s] 79%|███████▉  | 463/585 [23:31<00:35,  3.41it/s] 79%|███████▉  | 464/585 [23:31<00:35,  3.44it/s] 79%|███████▉  | 465/585 [23:32<00:41,  2.92it/s] 80%|███████▉  | 466/585 [23:32<00:38,  3.07it/s] 80%|███████▉  | 467/585 [23:32<00:36,  3.19it/s] 80%|████████  | 468/585 [23:33<00:35,  3.28it/s][INFO|trainer.py:2140] 2023-08-28 20:11:35,561 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:11:35,561 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:11:35,561 >>   Batch size = 8
{'eval_loss': 1.0493088960647583, 'eval_runtime': 13.2409, 'eval_samples_per_second': 263.728, 'eval_steps_per_second': 33.004, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.95it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.56it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.81it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.96it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.36it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.07it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.98it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.85it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.85it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 45.11it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.91it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.86it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.73it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.52it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.56it/s][A
 19%|█▉        | 82/437 [00:02<00:13, 25.74it/s][A
 20%|█▉        | 87/437 [00:02<00:11, 29.61it/s][A
 21%|██        | 92/437 [00:02<00:10, 33.05it/s][A
 22%|██▏       | 97/437 [00:02<00:09, 35.94it/s][A
 23%|██▎       | 102/437 [00:02<00:08, 38.35it/s][A
 24%|██▍       | 107/437 [00:02<00:08, 40.21it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 41.62it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 42.45it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 42.06it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 43.01it/s][A
 30%|███       | 132/437 [00:03<00:07, 43.43it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.88it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.31it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.61it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.82it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.95it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.65it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.32it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.22it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.24it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.49it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.67it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.91it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.99it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 44.86it/s][A
 47%|████▋     | 207/437 [00:05<00:12, 18.32it/s][A
 49%|████▊     | 212/437 [00:05<00:10, 22.31it/s][A
 50%|████▉     | 217/437 [00:05<00:08, 26.28it/s][A
 51%|█████     | 222/437 [00:05<00:07, 30.06it/s][A
 52%|█████▏    | 227/437 [00:05<00:06, 33.50it/s][A
 53%|█████▎    | 232/437 [00:05<00:05, 36.31it/s][A
 54%|█████▍    | 237/437 [00:06<00:05, 38.63it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 40.26it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 41.14it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 41.87it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 42.42it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.13it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.68it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.98it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.41it/s][A
 65%|██████▍   | 282/437 [00:07<00:03, 44.57it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 44.68it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 44.47it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.25it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.33it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.31it/s][A
 71%|███████▏  | 312/437 [00:08<00:02, 44.48it/s][A
 73%|███████▎  | 317/437 [00:08<00:09, 12.05it/s][A
 74%|███████▎  | 322/437 [00:08<00:07, 15.45it/s][A
 75%|███████▍  | 327/437 [00:09<00:05, 19.24it/s][A
 76%|███████▌  | 332/437 [00:09<00:04, 23.26it/s][A
 77%|███████▋  | 337/437 [00:09<00:03, 27.23it/s][A
 78%|███████▊  | 342/437 [00:09<00:03, 30.94it/s][A
 79%|███████▉  | 347/437 [00:09<00:02, 34.22it/s][A
 81%|████████  | 352/437 [00:09<00:02, 36.82it/s][A
 82%|████████▏ | 357/437 [00:09<00:02, 38.52it/s][A
 83%|████████▎ | 362/437 [00:09<00:01, 39.93it/s][A
 84%|████████▍ | 367/437 [00:10<00:01, 41.01it/s][A
 85%|████████▌ | 372/437 [00:10<00:01, 42.06it/s][A
 86%|████████▋ | 377/437 [00:10<00:01, 42.83it/s][A
 87%|████████▋ | 382/437 [00:10<00:01, 43.61it/s][A
 89%|████████▊ | 387/437 [00:10<00:01, 44.09it/s][A
 90%|████████▉ | 392/437 [00:10<00:01, 44.43it/s][A
 91%|█████████ | 397/437 [00:10<00:00, 44.55it/s][A
 92%|█████████▏| 402/437 [00:11<00:00, 44.26it/s][A
 93%|█████████▎| 407/437 [00:11<00:01, 22.50it/s][A
 94%|█████████▍| 412/437 [00:11<00:00, 26.48it/s][A
 95%|█████████▌| 417/437 [00:11<00:00, 30.21it/s][A
 97%|█████████▋| 422/437 [00:11<00:00, 33.51it/s][A
 98%|█████████▊| 427/437 [00:11<00:00, 36.37it/s][A
 99%|█████████▉| 432/437 [00:11<00:00, 38.70it/s][A
100%|██████████| 437/437 [00:11<00:00, 40.50it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [23:45<00:35,  3.28it/s]
100%|██████████| 437/437 [00:11<00:00, 40.50it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:11:50,248 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 20:11:52,259 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:13:20,985 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:13:25,011 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:13:26,471 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [27:45<2:26:42, 75.88s/it] 80%|████████  | 470/585 [27:45<1:42:09, 53.30s/it] 81%|████████  | 471/585 [27:46<1:11:03, 37.40s/it] 81%|████████  | 472/585 [27:46<49:28, 26.27s/it]   81%|████████  | 473/585 [27:46<34:28, 18.47s/it] 81%|████████  | 474/585 [27:47<24:04, 13.02s/it] 81%|████████  | 475/585 [27:47<16:51,  9.20s/it] 81%|████████▏ | 476/585 [27:47<11:51,  6.53s/it] 82%|████████▏ | 477/585 [27:47<08:22,  4.66s/it] 82%|████████▏ | 478/585 [27:48<05:57,  3.35s/it] 82%|████████▏ | 479/585 [27:48<04:17,  2.43s/it] 82%|████████▏ | 480/585 [27:49<03:38,  2.08s/it] 82%|████████▏ | 481/585 [27:50<02:40,  1.55s/it] 82%|████████▏ | 482/585 [27:50<02:00,  1.17s/it] 83%|████████▎ | 483/585 [27:50<01:32,  1.10it/s] 83%|████████▎ | 484/585 [27:50<01:12,  1.39it/s] 83%|████████▎ | 485/585 [27:51<00:59,  1.69it/s] 83%|████████▎ | 486/585 [27:51<00:49,  2.00it/s] 83%|████████▎ | 487/585 [27:51<00:48,  2.04it/s] 83%|████████▎ | 488/585 [27:52<00:41,  2.32it/s] 84%|████████▎ | 489/585 [27:52<00:37,  2.57it/s] 84%|████████▍ | 490/585 [27:52<00:34,  2.79it/s] 84%|████████▍ | 491/585 [27:53<00:31,  2.96it/s] 84%|████████▍ | 492/585 [27:53<00:30,  3.09it/s] 84%|████████▍ | 493/585 [27:53<00:28,  3.19it/s] 84%|████████▍ | 494/585 [27:53<00:27,  3.27it/s] 85%|████████▍ | 495/585 [27:54<00:27,  3.33it/s] 85%|████████▍ | 496/585 [27:54<00:26,  3.38it/s] 85%|████████▍ | 497/585 [27:55<00:30,  2.93it/s] 85%|████████▌ | 498/585 [27:55<00:28,  3.08it/s] 85%|████████▌ | 499/585 [27:55<00:26,  3.19it/s] 85%|████████▌ | 500/585 [27:55<00:25,  3.28it/s]                                                  85%|████████▌ | 500/585 [27:55<00:25,  3.28it/s] 86%|████████▌ | 501/585 [27:56<00:28,  2.92it/s] 86%|████████▌ | 502/585 [27:56<00:27,  3.06it/s] 86%|████████▌ | 503/585 [27:56<00:25,  3.17it/s] 86%|████████▌ | 504/585 [27:57<00:24,  3.25it/s] 86%|████████▋ | 505/585 [27:57<00:24,  3.31it/s] 86%|████████▋ | 506/585 [27:57<00:23,  3.35it/s] 87%|████████▋ | 507/585 [27:58<00:32,  2.41it/s] 87%|████████▋ | 508/585 [27:58<00:29,  2.65it/s] 87%|████████▋ | 509/585 [27:59<00:26,  2.85it/s] 87%|████████▋ | 510/585 [27:59<00:24,  3.00it/s] 87%|████████▋ | 511/585 [27:59<00:23,  3.13it/s] 88%|████████▊ | 512/585 [27:59<00:22,  3.22it/s] 88%|████████▊ | 513/585 [28:00<00:21,  3.28it/s] 88%|████████▊ | 514/585 [28:00<00:21,  3.33it/s] 88%|████████▊ | 515/585 [28:00<00:20,  3.36it/s] 88%|████████▊ | 516/585 [28:01<00:26,  2.62it/s] 88%|████████▊ | 517/585 [28:01<00:24,  2.83it/s] 89%|████████▊ | 518/585 [28:01<00:22,  2.99it/s] 89%|████████▊ | 519/585 [28:02<00:21,  3.11it/s] 89%|████████▉ | 520/585 [28:02<00:20,  3.21it/s] 89%|████████▉ | 521/585 [28:02<00:19,  3.27it/s] 89%|████████▉ | 522/585 [28:03<00:18,  3.32it/s] 89%|████████▉ | 523/585 [28:03<00:18,  3.36it/s] 90%|████████▉ | 524/585 [28:03<00:17,  3.39it/s] 90%|████████▉ | 525/585 [28:03<00:17,  3.41it/s] 90%|████████▉ | 526/585 [28:04<00:27,  2.11it/s] 90%|█████████ | 527/585 [28:05<00:24,  2.39it/s] 90%|█████████ | 528/585 [28:05<00:21,  2.63it/s] 90%|█████████ | 529/585 [28:05<00:19,  2.84it/s] 91%|█████████ | 530/585 [28:05<00:18,  3.00it/s] 91%|█████████ | 531/585 [28:06<00:17,  3.12it/s] 91%|█████████ | 532/585 [28:06<00:16,  3.22it/s] 91%|█████████ | 533/585 [28:06<00:15,  3.29it/s] 91%|█████████▏| 534/585 [28:07<00:15,  3.34it/s] 91%|█████████▏| 535/585 [28:07<00:21,  2.35it/s] 92%|█████████▏| 536/585 [28:08<00:18,  2.60it/s] 92%|█████████▏| 537/585 [28:08<00:17,  2.81it/s] 92%|█████████▏| 538/585 [28:08<00:15,  2.98it/s] 92%|█████████▏| 539/585 [28:09<00:14,  3.10it/s] 92%|█████████▏| 540/585 [28:09<00:14,  3.20it/s] 92%|█████████▏| 541/585 [28:09<00:13,  3.27it/s] 93%|█████████▎| 542/585 [28:09<00:12,  3.33it/s] 93%|█████████▎| 543/585 [28:10<00:12,  3.36it/s] 93%|█████████▎| 544/585 [28:10<00:16,  2.51it/s] 93%|█████████▎| 545/585 [28:11<00:14,  2.73it/s] 93%|█████████▎| 546/585 [28:11<00:13,  2.92it/s] 94%|█████████▎| 547/585 [28:11<00:12,  3.07it/s] 94%|█████████▎| 548/585 [28:11<00:11,  3.19it/s] 94%|█████████▍| 549/585 [28:12<00:10,  3.28it/s] 94%|█████████▍| 550/585 [28:12<00:10,  3.35it/s] 94%|█████████▍| 551/585 [28:12<00:10,  3.39it/s] 94%|█████████▍| 552/585 [28:13<00:09,  3.43it/s] 95%|█████████▍| 553/585 [28:13<00:09,  3.45it/s] 95%|█████████▍| 554/585 [28:14<00:13,  2.36it/s] 95%|█████████▍| 555/585 [28:14<00:11,  2.61it/s] 95%|█████████▌| 556/585 [28:14<00:10,  2.83it/s] 95%|█████████▌| 557/585 [28:14<00:09,  3.00it/s] 95%|█████████▌| 558/585 [28:15<00:08,  3.14it/s] 96%|█████████▌| 559/585 [28:15<00:08,  3.24it/s] 96%|█████████▌| 560/585 [28:15<00:07,  3.32it/s] 96%|█████████▌| 561/585 [28:16<00:07,  3.38it/s] 96%|█████████▌| 562/585 [28:16<00:06,  3.41it/s] 96%|█████████▌| 563/585 [28:17<00:08,  2.45it/s] 96%|█████████▋| 564/585 [28:17<00:07,  2.69it/s] 97%|█████████▋| 565/585 [28:17<00:06,  2.90it/s] 97%|█████████▋| 566/585 [28:17<00:06,  3.06it/s] 97%|█████████▋| 567/585 [28:18<00:05,  3.18it/s] 97%|█████████▋| 568/585 [28:18<00:05,  3.28it/s] 97%|█████████▋| 569/585 [28:18<00:04,  3.34it/s] 97%|█████████▋| 570/585 [28:19<00:04,  3.39it/s] 98%|█████████▊| 571/585 [28:19<00:05,  2.35it/s] 98%|█████████▊| 572/585 [28:20<00:04,  2.61it/s] 98%|█████████▊| 573/585 [28:20<00:04,  2.83it/s] 98%|█████████▊| 574/585 [28:20<00:03,  3.00it/s] 98%|█████████▊| 575/585 [28:20<00:03,  3.14it/s] 98%|█████████▊| 576/585 [28:21<00:02,  3.24it/s] 99%|█████████▊| 577/585 [28:21<00:02,  3.32it/s] 99%|█████████▉| 578/585 [28:21<00:02,  3.37it/s] 99%|█████████▉| 579/585 [28:22<00:01,  3.42it/s] 99%|█████████▉| 580/585 [28:22<00:02,  2.39it/s] 99%|█████████▉| 581/585 [28:23<00:01,  2.64it/s] 99%|█████████▉| 582/585 [28:23<00:01,  2.85it/s]100%|█████████▉| 583/585 [28:23<00:00,  3.02it/s]100%|█████████▉| 584/585 [28:23<00:00,  3.15it/s]100%|██████████| 585/585 [28:24<00:00,  3.25it/s][INFO|trainer.py:2140] 2023-08-28 20:16:26,724 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:16:26,724 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:16:26,724 >>   Batch size = 8
{'eval_loss': 1.057836890220642, 'eval_runtime': 11.9616, 'eval_samples_per_second': 291.935, 'eval_steps_per_second': 36.534, 'epoch': 4.0}
{'loss': 0.7267, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.56it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.78it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.80it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.97it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.45it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.09it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.86it/s][A
 10%|▉         | 42/437 [00:01<00:08, 44.77it/s][A
 11%|█         | 47/437 [00:01<00:14, 26.08it/s][A
 12%|█▏        | 52/437 [00:01<00:12, 29.98it/s][A
 13%|█▎        | 57/437 [00:01<00:11, 33.43it/s][A
 14%|█▍        | 62/437 [00:01<00:10, 36.32it/s][A
 15%|█▌        | 67/437 [00:01<00:09, 38.65it/s][A
 16%|█▋        | 72/437 [00:01<00:09, 40.47it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 41.80it/s][A
 19%|█▉        | 82/437 [00:02<00:08, 42.64it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 42.86it/s][A
 21%|██        | 92/437 [00:02<00:08, 43.07it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.43it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.80it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.26it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.54it/s][A
 27%|██▋       | 117/437 [00:03<00:13, 23.92it/s][A
 28%|██▊       | 122/437 [00:03<00:11, 27.99it/s][A
 29%|██▉       | 127/437 [00:03<00:09, 31.64it/s][A
 30%|███       | 132/437 [00:03<00:08, 34.76it/s][A
 31%|███▏      | 137/437 [00:03<00:08, 37.40it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 39.47it/s][A
 34%|███▎      | 147/437 [00:03<00:07, 41.05it/s][A
 35%|███▍      | 152/437 [00:04<00:06, 42.28it/s][A
 36%|███▌      | 157/437 [00:04<00:13, 20.38it/s][A
 37%|███▋      | 162/437 [00:04<00:11, 24.41it/s][A
 38%|███▊      | 167/437 [00:04<00:09, 28.33it/s][A
 39%|███▉      | 172/437 [00:04<00:08, 31.90it/s][A
 41%|████      | 177/437 [00:04<00:07, 35.01it/s][A
 42%|████▏     | 182/437 [00:05<00:06, 37.58it/s][A
 43%|████▎     | 187/437 [00:05<00:06, 39.61it/s][A
 44%|████▍     | 192/437 [00:05<00:05, 40.98it/s][A
 45%|████▌     | 197/437 [00:05<00:05, 41.65it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 42.27it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 42.83it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 43.41it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 43.93it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.37it/s][A
 52%|█████▏    | 227/437 [00:06<00:04, 44.67it/s][A
 53%|█████▎    | 232/437 [00:06<00:04, 44.81it/s][A
 54%|█████▍    | 237/437 [00:06<00:04, 44.76it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 44.54it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 44.23it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 44.35it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.38it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.46it/s][A
 61%|██████    | 267/437 [00:07<00:03, 44.64it/s][A
 62%|██████▏   | 272/437 [00:07<00:06, 25.82it/s][A
 63%|██████▎   | 277/437 [00:07<00:05, 29.65it/s][A
 65%|██████▍   | 282/437 [00:07<00:04, 33.08it/s][A
 66%|██████▌   | 287/437 [00:07<00:04, 35.99it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 38.38it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 40.23it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 41.64it/s][A
 70%|███████   | 307/437 [00:08<00:03, 42.48it/s][A
 71%|███████▏  | 312/437 [00:08<00:02, 42.67it/s][A
 73%|███████▎  | 317/437 [00:08<00:02, 42.85it/s][A
 74%|███████▎  | 322/437 [00:08<00:02, 43.24it/s][A
 75%|███████▍  | 327/437 [00:08<00:02, 43.71it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 44.10it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 44.37it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 44.65it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.75it/s][A
 81%|████████  | 352/437 [00:09<00:01, 44.68it/s][A
 82%|████████▏ | 357/437 [00:09<00:01, 44.40it/s][A
 83%|████████▎ | 362/437 [00:09<00:01, 44.12it/s][A
 84%|████████▍ | 367/437 [00:09<00:01, 44.09it/s][A
 85%|████████▌ | 372/437 [00:09<00:01, 44.17it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 44.34it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 44.60it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 44.74it/s][A
 90%|████████▉ | 392/437 [00:10<00:01, 44.78it/s][A
 91%|█████████ | 397/437 [00:10<00:01, 27.75it/s][A
 92%|█████████▏| 402/437 [00:10<00:01, 31.36it/s][A
 93%|█████████▎| 407/437 [00:10<00:00, 34.44it/s][A
 94%|█████████▍| 412/437 [00:10<00:00, 37.08it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 39.18it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 40.79it/s][A
 98%|█████████▊| 427/437 [00:11<00:00, 41.98it/s][A
 99%|█████████▉| 432/437 [00:11<00:00, 42.72it/s][A
100%|██████████| 437/437 [00:11<00:00, 42.84it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:11<00:00, 42.84it/s][A100%|██████████| 585/585 [28:35<00:00,  3.25it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 20:16:40,401 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 20:16:42,918 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:18:21,210 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:18:25,706 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:18:26,227 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 20:21:49,883 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 20:21:50,051 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117 (score: 1.0291342735290527).
                                                 100%|██████████| 585/585 [35:56<00:00,  3.25it/s]100%|██████████| 585/585 [35:56<00:00,  3.69s/it]
[INFO|trainer.py:1894] 2023-08-28 20:24:04,096 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 20:24:06,296 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 20:25:45,067 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 20:25:48,674 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 20:25:49,879 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:25:58,862 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:25:59,161 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:25:59,161 >>   train_loss               =     0.7218
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:25:59,161 >>   train_runtime            = 0:35:55.42
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:25:59,161 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:25:59,161 >>   train_samples_per_second =     17.398
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:25:59,161 >>   train_steps_per_second   =      0.271
{'eval_loss': 1.063089370727539, 'eval_runtime': 11.2604, 'eval_samples_per_second': 310.113, 'eval_steps_per_second': 38.809, 'epoch': 5.0}
{'train_runtime': 2155.4248, 'train_samples_per_second': 17.398, 'train_steps_per_second': 0.271, 'train_loss': 0.7217612633338342, 'epoch': 5.0}
08/28/2023 20:26:04 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 20:26:04,476 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 20:26:04,476 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 20:26:04,476 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:08, 50.48it/s]  3%|▎         | 12/437 [00:00<00:08, 47.42it/s]  4%|▍         | 17/437 [00:00<00:08, 46.81it/s]  5%|▌         | 22/437 [00:00<00:08, 46.35it/s]  6%|▌         | 27/437 [00:00<00:08, 46.15it/s]  7%|▋         | 32/437 [00:00<00:08, 45.93it/s]  8%|▊         | 37/437 [00:00<00:08, 45.85it/s] 10%|▉         | 42/437 [00:00<00:08, 45.66it/s] 11%|█         | 47/437 [00:01<00:08, 45.08it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.71it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.62it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.85it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.99it/s] 16%|█▋        | 72/437 [00:01<00:08, 45.14it/s] 18%|█▊        | 77/437 [00:01<00:07, 45.24it/s] 19%|█▉        | 82/437 [00:01<00:07, 45.34it/s] 20%|█▉        | 87/437 [00:01<00:07, 45.06it/s] 21%|██        | 92/437 [00:02<00:07, 44.68it/s] 22%|██▏       | 97/437 [00:02<00:07, 44.49it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.58it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.73it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.91it/s] 27%|██▋       | 117/437 [00:02<00:07, 45.08it/s] 28%|██▊       | 122/437 [00:02<00:06, 45.13it/s] 29%|██▉       | 127/437 [00:02<00:06, 45.23it/s] 30%|███       | 132/437 [00:03<00:24, 12.22it/s] 31%|███▏      | 137/437 [00:04<00:19, 15.59it/s] 32%|███▏      | 142/437 [00:04<00:15, 19.40it/s] 34%|███▎      | 147/437 [00:04<00:12, 23.42it/s] 35%|███▍      | 152/437 [00:04<00:10, 27.40it/s] 36%|███▌      | 157/437 [00:04<00:09, 31.08it/s] 37%|███▋      | 162/437 [00:04<00:08, 34.35it/s] 38%|███▊      | 167/437 [00:04<00:07, 37.02it/s] 39%|███▉      | 172/437 [00:04<00:06, 38.86it/s] 41%|████      | 177/437 [00:04<00:06, 40.33it/s] 42%|████▏     | 182/437 [00:05<00:06, 41.55it/s] 43%|████▎     | 187/437 [00:05<00:05, 42.56it/s] 44%|████▍     | 192/437 [00:05<00:05, 43.32it/s] 45%|████▌     | 197/437 [00:05<00:05, 43.88it/s] 46%|████▌     | 202/437 [00:05<00:05, 44.35it/s] 47%|████▋     | 207/437 [00:05<00:05, 44.68it/s] 49%|████▊     | 212/437 [00:05<00:05, 44.66it/s] 50%|████▉     | 217/437 [00:05<00:04, 44.47it/s] 51%|█████     | 222/437 [00:05<00:04, 44.45it/s] 52%|█████▏    | 227/437 [00:06<00:10, 19.67it/s] 53%|█████▎    | 232/437 [00:06<00:08, 23.70it/s] 54%|█████▍    | 237/437 [00:06<00:07, 27.68it/s] 55%|█████▌    | 242/437 [00:06<00:06, 31.28it/s] 57%|█████▋    | 247/437 [00:06<00:05, 34.53it/s] 58%|█████▊    | 252/437 [00:07<00:04, 37.19it/s] 59%|█████▉    | 257/437 [00:07<00:04, 39.39it/s] 60%|█████▉    | 262/437 [00:07<00:04, 40.97it/s] 61%|██████    | 267/437 [00:07<00:04, 41.73it/s] 62%|██████▏   | 272/437 [00:07<00:03, 42.40it/s] 63%|██████▎   | 277/437 [00:07<00:03, 43.00it/s] 65%|██████▍   | 282/437 [00:07<00:03, 43.62it/s] 66%|██████▌   | 287/437 [00:07<00:03, 44.08it/s] 67%|██████▋   | 292/437 [00:07<00:03, 44.40it/s] 68%|██████▊   | 297/437 [00:08<00:03, 44.69it/s] 69%|██████▉   | 302/437 [00:08<00:03, 44.98it/s] 70%|███████   | 307/437 [00:08<00:02, 44.96it/s] 71%|███████▏  | 312/437 [00:08<00:02, 44.63it/s] 73%|███████▎  | 317/437 [00:08<00:02, 44.56it/s] 74%|███████▎  | 322/437 [00:08<00:02, 44.51it/s] 75%|███████▍  | 327/437 [00:08<00:02, 44.70it/s] 76%|███████▌  | 332/437 [00:08<00:02, 44.86it/s] 77%|███████▋  | 337/437 [00:08<00:02, 44.97it/s] 78%|███████▊  | 342/437 [00:09<00:04, 23.64it/s] 79%|███████▉  | 347/437 [00:09<00:03, 27.60it/s] 81%|████████  | 352/437 [00:09<00:02, 31.24it/s] 82%|████████▏ | 357/437 [00:09<00:02, 34.53it/s] 83%|████████▎ | 362/437 [00:09<00:02, 37.25it/s] 84%|████████▍ | 367/437 [00:09<00:01, 39.41it/s] 85%|████████▌ | 372/437 [00:10<00:01, 41.05it/s] 86%|████████▋ | 377/437 [00:10<00:01, 42.22it/s] 87%|████████▋ | 382/437 [00:10<00:01, 42.61it/s] 89%|████████▊ | 387/437 [00:10<00:01, 42.89it/s] 90%|████████▉ | 392/437 [00:10<00:01, 43.31it/s] 91%|█████████ | 397/437 [00:10<00:00, 43.76it/s] 92%|█████████▏| 402/437 [00:10<00:00, 44.23it/s] 93%|█████████▎| 407/437 [00:10<00:00, 44.66it/s] 94%|█████████▍| 412/437 [00:10<00:00, 44.87it/s] 95%|█████████▌| 417/437 [00:11<00:00, 45.00it/s] 97%|█████████▋| 422/437 [00:11<00:00, 44.88it/s] 98%|█████████▊| 427/437 [00:11<00:00, 44.69it/s] 99%|█████████▉| 432/437 [00:11<00:00, 44.47it/s]100%|██████████| 437/437 [00:11<00:00, 44.37it/s]100%|██████████| 437/437 [00:11<00:00, 37.96it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 20:26:16,024 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:26:16,024 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:26:16,024 >>   eval_loss               =     1.0291
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:26:16,024 >>   eval_runtime            = 0:00:11.54
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:26:16,024 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:26:16,024 >>   eval_samples_per_second =    302.389
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:26:16,024 >>   eval_steps_per_second   =     37.842
[INFO|trainer_pt_utils.py:913] 2023-08-28 20:26:16,024 >>   perplexity              =     2.7986
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:15,208 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:15,395 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:15,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:15,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:36:15,396 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:36:22,178 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:36:22,944 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:36:24,527 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:38:01,419 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:38:01,575 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:15,126 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:15,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:15,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:15,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:38:15,395 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:38:22,070 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:38:22,481 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:38:24,776 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:38:47,655 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:38:48,033 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/checkpoint-468
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.67it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.64it/s]Extractor Predicting: 6it [00:04,  1.22it/s]Extractor Predicting: 7it [00:04,  1.32it/s]Extractor Predicting: 8it [00:05,  1.39it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:07,  1.14it/s]Extractor Predicting: 11it [00:08,  1.26it/s]Extractor Predicting: 12it [00:08,  1.34it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:10,  1.21it/s]Extractor Predicting: 15it [00:11,  1.30it/s]Extractor Predicting: 16it [00:11,  1.39it/s]Extractor Predicting: 17it [00:12,  1.42it/s]Extractor Predicting: 18it [00:13,  1.17it/s]Extractor Predicting: 19it [00:14,  1.31it/s]Extractor Predicting: 20it [00:14,  1.38it/s]Extractor Predicting: 21it [00:15,  1.44it/s]Extractor Predicting: 22it [00:16,  1.31it/s]Extractor Predicting: 23it [00:16,  1.41it/s]Extractor Predicting: 24it [00:17,  1.47it/s]Extractor Predicting: 25it [00:18,  1.51it/s]Extractor Predicting: 26it [00:18,  1.53it/s]Extractor Predicting: 27it [00:19,  1.30it/s]Extractor Predicting: 28it [00:20,  1.38it/s]Extractor Predicting: 29it [00:20,  1.46it/s]Extractor Predicting: 30it [00:21,  1.46it/s]Extractor Predicting: 31it [00:22,  1.51it/s]Extractor Predicting: 32it [00:23,  1.11it/s]Extractor Predicting: 33it [00:24,  1.23it/s]Extractor Predicting: 34it [00:24,  1.32it/s]Extractor Predicting: 35it [00:25,  1.39it/s]Extractor Predicting: 36it [00:26,  1.31it/s]Extractor Predicting: 37it [00:27,  1.39it/s]Extractor Predicting: 38it [00:27,  1.45it/s]Extractor Predicting: 39it [00:28,  1.50it/s]Extractor Predicting: 40it [00:28,  1.54it/s]Extractor Predicting: 41it [00:29,  1.32it/s]Extractor Predicting: 42it [00:30,  1.39it/s]Extractor Predicting: 43it [00:31,  1.46it/s]Extractor Predicting: 44it [00:32,  1.34it/s]Extractor Predicting: 45it [00:32,  1.39it/s]Extractor Predicting: 46it [00:33,  1.43it/s]Extractor Predicting: 47it [00:33,  1.49it/s]Extractor Predicting: 48it [00:34,  1.30it/s]Extractor Predicting: 49it [00:35,  1.38it/s]Extractor Predicting: 50it [00:36,  1.45it/s]Extractor Predicting: 51it [00:36,  1.46it/s]Extractor Predicting: 52it [00:37,  1.48it/s]Extractor Predicting: 53it [00:38,  1.44it/s]Extractor Predicting: 54it [00:38,  1.48it/s]Extractor Predicting: 55it [00:39,  1.52it/s]Extractor Predicting: 56it [00:40,  1.55it/s]Extractor Predicting: 57it [00:40,  1.56it/s]Extractor Predicting: 58it [00:41,  1.36it/s]Extractor Predicting: 59it [00:42,  1.41it/s]Extractor Predicting: 60it [00:42,  1.44it/s]Extractor Predicting: 61it [00:43,  1.46it/s]Extractor Predicting: 62it [00:44,  1.49it/s]Extractor Predicting: 63it [00:45,  1.17it/s]Extractor Predicting: 64it [00:46,  1.24it/s]Extractor Predicting: 65it [00:46,  1.32it/s]Extractor Predicting: 66it [00:47,  1.27it/s]Extractor Predicting: 67it [00:48,  1.22it/s]Extractor Predicting: 68it [00:49,  1.29it/s]Extractor Predicting: 69it [00:50,  1.34it/s]Extractor Predicting: 70it [00:50,  1.39it/s]Extractor Predicting: 71it [00:51,  1.41it/s]Extractor Predicting: 72it [00:52,  1.21it/s]Extractor Predicting: 73it [00:53,  1.29it/s]Extractor Predicting: 74it [00:53,  1.32it/s]Extractor Predicting: 75it [00:54,  1.40it/s]Extractor Predicting: 76it [00:55,  1.26it/s]Extractor Predicting: 77it [00:56,  1.31it/s]Extractor Predicting: 78it [00:56,  1.38it/s]Extractor Predicting: 79it [00:57,  1.40it/s]Extractor Predicting: 80it [00:58,  1.41it/s]Extractor Predicting: 81it [00:58,  1.35it/s]Extractor Predicting: 82it [00:59,  1.42it/s]Extractor Predicting: 83it [01:00,  1.45it/s]Extractor Predicting: 84it [01:00,  1.50it/s]Extractor Predicting: 85it [01:01,  1.50it/s]Extractor Predicting: 86it [01:02,  1.41it/s]Extractor Predicting: 87it [01:03,  1.21it/s]Extractor Predicting: 88it [01:04,  1.31it/s]Extractor Predicting: 89it [01:04,  1.42it/s]Extractor Predicting: 90it [01:05,  1.47it/s]Extractor Predicting: 91it [01:06,  1.34it/s]Extractor Predicting: 92it [01:06,  1.46it/s]Extractor Predicting: 93it [01:07,  1.56it/s]Extractor Predicting: 94it [01:07,  1.65it/s]Extractor Predicting: 95it [01:08,  1.62it/s]Extractor Predicting: 96it [01:09,  1.09it/s]Extractor Predicting: 97it [01:10,  1.21it/s]Extractor Predicting: 98it [01:11,  1.32it/s]Extractor Predicting: 99it [01:11,  1.47it/s]Extractor Predicting: 100it [01:12,  1.37it/s]Extractor Predicting: 101it [01:13,  1.45it/s]Extractor Predicting: 102it [01:13,  1.49it/s]Extractor Predicting: 103it [01:14,  1.53it/s]Extractor Predicting: 104it [01:15,  1.54it/s]Extractor Predicting: 105it [01:16,  1.20it/s]Extractor Predicting: 106it [01:16,  1.31it/s]Extractor Predicting: 107it [01:17,  1.38it/s]Extractor Predicting: 108it [01:18,  1.47it/s]Extractor Predicting: 109it [01:19,  1.29it/s]Extractor Predicting: 110it [01:19,  1.40it/s]Extractor Predicting: 111it [01:20,  1.49it/s]Extractor Predicting: 112it [01:20,  1.56it/s]Extractor Predicting: 113it [01:21,  1.62it/s]Extractor Predicting: 114it [01:22,  1.38it/s]Extractor Predicting: 115it [01:22,  1.48it/s]Extractor Predicting: 116it [01:23,  1.51it/s]Extractor Predicting: 117it [01:24,  1.53it/s]Extractor Predicting: 118it [01:24,  1.56it/s]Extractor Predicting: 119it [01:26,  1.20it/s]Extractor Predicting: 120it [01:26,  1.28it/s]Extractor Predicting: 121it [01:27,  1.36it/s]Extractor Predicting: 122it [01:27,  1.42it/s]Extractor Predicting: 123it [01:28,  1.33it/s]Extractor Predicting: 124it [01:29,  1.38it/s]Extractor Predicting: 125it [01:30,  1.43it/s]Extractor Predicting: 126it [01:30,  1.45it/s]Extractor Predicting: 127it [01:31,  1.47it/s]Extractor Predicting: 128it [01:32,  1.14it/s]Extractor Predicting: 129it [01:33,  1.24it/s]Extractor Predicting: 130it [01:34,  1.29it/s]Extractor Predicting: 131it [01:34,  1.34it/s]Extractor Predicting: 132it [01:35,  1.38it/s]Extractor Predicting: 133it [01:36,  1.42it/s]Extractor Predicting: 134it [01:37,  1.32it/s]Extractor Predicting: 135it [01:37,  1.39it/s]Extractor Predicting: 136it [01:38,  1.42it/s]Extractor Predicting: 137it [01:38,  1.45it/s]Extractor Predicting: 138it [01:39,  1.50it/s]Extractor Predicting: 139it [01:40,  1.39it/s]Extractor Predicting: 140it [01:41,  1.47it/s]Extractor Predicting: 141it [01:41,  1.47it/s]Extractor Predicting: 142it [01:42,  1.48it/s]Extractor Predicting: 143it [01:43,  1.52it/s]Extractor Predicting: 144it [01:44,  1.31it/s]Extractor Predicting: 144it [01:44,  1.38it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:42:53,153 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:42:53,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:42:53,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:42:53,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:42:53,312 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:42:57,986 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:42:58,331 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:42:59,438 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:43:01,117 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:43:01,809 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:43:04,879 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:43:05,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:43:05,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:43:05,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:43:05,244 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:43:07,707 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:43:07,708 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:43:08,895 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:43:10,188 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:43:10,188 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4854854854854855,
  "recall": 0.1388888888888889,
  "score": 0.21598753061678913,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.66it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:03,  1.45it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.53it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:07,  1.12it/s]Extractor Predicting: 11it [00:07,  1.24it/s]Extractor Predicting: 12it [00:08,  1.35it/s]Extractor Predicting: 13it [00:09,  1.41it/s]Extractor Predicting: 14it [00:10,  1.29it/s]Extractor Predicting: 15it [00:10,  1.40it/s]Extractor Predicting: 16it [00:11,  1.48it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.54it/s]Extractor Predicting: 19it [00:13,  1.45it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:14,  1.54it/s]Extractor Predicting: 22it [00:15,  1.51it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:16,  1.30it/s]Extractor Predicting: 25it [00:17,  1.39it/s]Extractor Predicting: 26it [00:18,  1.44it/s]Extractor Predicting: 27it [00:18,  1.48it/s]Extractor Predicting: 28it [00:19,  1.51it/s]Extractor Predicting: 29it [00:20,  1.43it/s]Extractor Predicting: 30it [00:20,  1.44it/s]Extractor Predicting: 31it [00:21,  1.48it/s]Extractor Predicting: 32it [00:22,  1.50it/s]Extractor Predicting: 33it [00:22,  1.55it/s]Extractor Predicting: 34it [00:23,  1.45it/s]Extractor Predicting: 35it [00:24,  1.50it/s]Extractor Predicting: 36it [00:24,  1.56it/s]Extractor Predicting: 37it [00:25,  1.63it/s]Extractor Predicting: 38it [00:25,  1.64it/s]Extractor Predicting: 39it [00:27,  1.18it/s]Extractor Predicting: 40it [00:27,  1.29it/s]Extractor Predicting: 41it [00:28,  1.39it/s]Extractor Predicting: 42it [00:29,  1.45it/s]Extractor Predicting: 43it [00:29,  1.32it/s]Extractor Predicting: 44it [00:30,  1.41it/s]Extractor Predicting: 45it [00:31,  1.46it/s]Extractor Predicting: 46it [00:31,  1.53it/s]Extractor Predicting: 47it [00:32,  1.57it/s]Extractor Predicting: 48it [00:33,  1.32it/s]Extractor Predicting: 49it [00:34,  1.32it/s]Extractor Predicting: 50it [00:34,  1.39it/s]Extractor Predicting: 51it [00:35,  1.46it/s]Extractor Predicting: 52it [00:36,  1.40it/s]Extractor Predicting: 53it [00:36,  1.46it/s]Extractor Predicting: 54it [00:37,  1.51it/s]Extractor Predicting: 55it [00:38,  1.52it/s]Extractor Predicting: 56it [00:38,  1.56it/s]Extractor Predicting: 57it [00:39,  1.44it/s]Extractor Predicting: 58it [00:40,  1.52it/s]Extractor Predicting: 59it [00:41,  1.13it/s]Extractor Predicting: 60it [00:42,  1.25it/s]Extractor Predicting: 61it [00:42,  1.34it/s]Extractor Predicting: 62it [00:43,  1.42it/s]Extractor Predicting: 63it [00:44,  1.17it/s]Extractor Predicting: 64it [00:45,  1.26it/s]Extractor Predicting: 65it [00:45,  1.36it/s]Extractor Predicting: 66it [00:46,  1.43it/s]Extractor Predicting: 67it [00:47,  1.34it/s]Extractor Predicting: 68it [00:47,  1.47it/s]Extractor Predicting: 69it [00:48,  1.50it/s]Extractor Predicting: 70it [00:48,  1.55it/s]Extractor Predicting: 71it [00:49,  1.57it/s]Extractor Predicting: 72it [00:50,  1.19it/s]Extractor Predicting: 73it [00:51,  1.28it/s]Extractor Predicting: 74it [00:52,  1.36it/s]Extractor Predicting: 75it [00:52,  1.44it/s]Extractor Predicting: 76it [00:53,  1.33it/s]Extractor Predicting: 77it [00:54,  1.39it/s]Extractor Predicting: 78it [00:54,  1.45it/s]Extractor Predicting: 79it [00:55,  1.50it/s]Extractor Predicting: 80it [00:56,  1.53it/s]Extractor Predicting: 81it [00:56,  1.52it/s]Extractor Predicting: 82it [00:57,  1.53it/s]Extractor Predicting: 83it [00:58,  1.56it/s]Extractor Predicting: 84it [00:58,  1.57it/s]Extractor Predicting: 85it [00:59,  1.62it/s]Extractor Predicting: 86it [01:00,  1.48it/s]Extractor Predicting: 87it [01:00,  1.52it/s]Extractor Predicting: 88it [01:01,  1.54it/s]Extractor Predicting: 89it [01:01,  1.59it/s]Extractor Predicting: 90it [01:02,  1.59it/s]Extractor Predicting: 91it [01:03,  1.20it/s]Extractor Predicting: 92it [01:04,  1.28it/s]Extractor Predicting: 93it [01:05,  1.39it/s]Extractor Predicting: 94it [01:05,  1.43it/s]Extractor Predicting: 95it [01:06,  1.32it/s]Extractor Predicting: 96it [01:07,  1.41it/s]Extractor Predicting: 97it [01:07,  1.47it/s]Extractor Predicting: 98it [01:08,  1.52it/s]Extractor Predicting: 99it [01:09,  1.57it/s]Extractor Predicting: 100it [01:10,  1.10it/s]Extractor Predicting: 101it [01:11,  1.23it/s]Extractor Predicting: 102it [01:11,  1.34it/s]Extractor Predicting: 103it [01:13,  1.03it/s]Extractor Predicting: 104it [01:13,  1.14it/s]Extractor Predicting: 105it [01:14,  1.26it/s]Extractor Predicting: 106it [01:15,  1.19it/s]Extractor Predicting: 107it [01:16,  1.29it/s]Extractor Predicting: 108it [01:16,  1.39it/s]Extractor Predicting: 109it [01:17,  1.43it/s]Extractor Predicting: 110it [01:17,  1.48it/s]Extractor Predicting: 111it [01:18,  1.39it/s]Extractor Predicting: 112it [01:19,  1.44it/s]Extractor Predicting: 113it [01:20,  1.46it/s]Extractor Predicting: 114it [01:20,  1.49it/s]Extractor Predicting: 115it [01:21,  1.52it/s]Extractor Predicting: 116it [01:22,  1.32it/s]Extractor Predicting: 117it [01:22,  1.40it/s]Extractor Predicting: 118it [01:23,  1.47it/s]Extractor Predicting: 119it [01:24,  1.51it/s]Extractor Predicting: 120it [01:24,  1.54it/s]Extractor Predicting: 121it [01:25,  1.31it/s]Extractor Predicting: 122it [01:26,  1.40it/s]Extractor Predicting: 123it [01:27,  1.49it/s]Extractor Predicting: 124it [01:27,  1.55it/s]Extractor Predicting: 125it [01:28,  1.59it/s]Extractor Predicting: 126it [01:29,  1.34it/s]Extractor Predicting: 127it [01:29,  1.43it/s]Extractor Predicting: 128it [01:30,  1.49it/s]Extractor Predicting: 129it [01:31,  1.51it/s]Extractor Predicting: 130it [01:31,  1.53it/s]Extractor Predicting: 131it [01:32,  1.31it/s]Extractor Predicting: 132it [01:33,  1.40it/s]Extractor Predicting: 133it [01:33,  1.48it/s]Extractor Predicting: 134it [01:34,  1.54it/s]Extractor Predicting: 135it [01:35,  1.57it/s]Extractor Predicting: 136it [01:35,  1.42it/s]Extractor Predicting: 137it [01:36,  1.49it/s]Extractor Predicting: 138it [01:37,  1.51it/s]Extractor Predicting: 139it [01:38,  1.41it/s]Extractor Predicting: 140it [01:38,  1.52it/s]Extractor Predicting: 141it [01:39,  1.48it/s]Extractor Predicting: 142it [01:39,  1.52it/s]Extractor Predicting: 143it [01:40,  1.54it/s]Extractor Predicting: 144it [01:41,  1.57it/s]Extractor Predicting: 145it [01:41,  1.62it/s]Extractor Predicting: 146it [01:42,  1.39it/s]Extractor Predicting: 147it [01:43,  1.48it/s]Extractor Predicting: 148it [01:44,  1.23it/s]Extractor Predicting: 149it [01:44,  1.34it/s]Extractor Predicting: 150it [01:45,  1.41it/s]Extractor Predicting: 151it [01:46,  1.48it/s]Extractor Predicting: 152it [01:47,  1.12it/s]Extractor Predicting: 153it [01:48,  1.23it/s]Extractor Predicting: 154it [01:48,  1.35it/s]Extractor Predicting: 155it [01:49,  1.43it/s]Extractor Predicting: 156it [01:51,  1.03s/it]Extractor Predicting: 157it [01:51,  1.12it/s]Extractor Predicting: 158it [01:52,  1.28it/s]Extractor Predicting: 159it [01:52,  1.36it/s]Extractor Predicting: 160it [01:54,  1.19it/s]Extractor Predicting: 161it [01:54,  1.29it/s]Extractor Predicting: 162it [01:55,  1.37it/s]Extractor Predicting: 163it [01:55,  1.46it/s]Extractor Predicting: 164it [01:56,  1.52it/s]Extractor Predicting: 165it [01:57,  1.11it/s]Extractor Predicting: 166it [01:58,  1.25it/s]Extractor Predicting: 167it [01:59,  1.35it/s]Extractor Predicting: 168it [01:59,  1.40it/s]Extractor Predicting: 169it [02:01,  1.11it/s]Extractor Predicting: 170it [02:01,  1.23it/s]Extractor Predicting: 171it [02:02,  1.34it/s]Extractor Predicting: 172it [02:02,  1.43it/s]Extractor Predicting: 173it [02:03,  1.25it/s]Extractor Predicting: 174it [02:04,  1.35it/s]Extractor Predicting: 175it [02:05,  1.40it/s]Extractor Predicting: 176it [02:05,  1.43it/s]Extractor Predicting: 177it [02:06,  1.52it/s]Extractor Predicting: 178it [02:07,  1.38it/s]Extractor Predicting: 179it [02:07,  1.47it/s]Extractor Predicting: 180it [02:08,  1.55it/s]Extractor Predicting: 181it [02:09,  1.55it/s]Extractor Predicting: 182it [02:09,  1.59it/s]Extractor Predicting: 183it [02:10,  1.38it/s]Extractor Predicting: 184it [02:11,  1.43it/s]Extractor Predicting: 185it [02:11,  1.48it/s]Extractor Predicting: 186it [02:12,  1.52it/s]Extractor Predicting: 187it [02:13,  1.56it/s]Extractor Predicting: 188it [02:14,  1.09it/s]Extractor Predicting: 189it [02:15,  1.02it/s]Extractor Predicting: 190it [02:16,  1.15it/s]Extractor Predicting: 191it [02:16,  1.28it/s]Extractor Predicting: 192it [02:17,  1.40it/s]Extractor Predicting: 193it [02:18,  1.19it/s]Extractor Predicting: 194it [02:19,  1.28it/s]Extractor Predicting: 195it [02:19,  1.39it/s]Extractor Predicting: 196it [02:20,  1.46it/s]Extractor Predicting: 197it [02:21,  1.51it/s]Extractor Predicting: 198it [02:21,  1.46it/s]Extractor Predicting: 199it [02:22,  1.53it/s]Extractor Predicting: 200it [02:22,  1.61it/s]Extractor Predicting: 201it [02:23,  1.66it/s]Extractor Predicting: 202it [02:24,  1.67it/s]Extractor Predicting: 203it [02:24,  1.58it/s]Extractor Predicting: 204it [02:25,  1.60it/s]Extractor Predicting: 205it [02:25,  1.63it/s]Extractor Predicting: 206it [02:26,  1.63it/s]Extractor Predicting: 207it [02:27,  1.64it/s]Extractor Predicting: 208it [02:28,  1.34it/s]Extractor Predicting: 209it [02:28,  1.42it/s]Extractor Predicting: 210it [02:29,  1.48it/s]Extractor Predicting: 211it [02:30,  1.52it/s]Extractor Predicting: 212it [02:30,  1.57it/s]Extractor Predicting: 213it [02:31,  1.20it/s]Extractor Predicting: 214it [02:32,  1.29it/s]Extractor Predicting: 215it [02:33,  1.37it/s]Extractor Predicting: 216it [02:33,  1.45it/s]Extractor Predicting: 217it [02:35,  1.14it/s]Extractor Predicting: 218it [02:35,  1.24it/s]Extractor Predicting: 219it [02:36,  1.33it/s]Extractor Predicting: 220it [02:36,  1.43it/s]Extractor Predicting: 221it [02:37,  1.39it/s]Extractor Predicting: 222it [02:38,  1.46it/s]Extractor Predicting: 223it [02:38,  1.52it/s]Extractor Predicting: 224it [02:39,  1.56it/s]Extractor Predicting: 225it [02:40,  1.57it/s]Extractor Predicting: 226it [02:41,  1.39it/s]Extractor Predicting: 227it [02:41,  1.47it/s]Extractor Predicting: 228it [02:42,  1.53it/s]Extractor Predicting: 229it [02:42,  1.59it/s]Extractor Predicting: 230it [02:43,  1.62it/s]Extractor Predicting: 231it [02:44,  1.33it/s]Extractor Predicting: 232it [02:45,  1.41it/s]Extractor Predicting: 233it [02:45,  1.49it/s]Extractor Predicting: 234it [02:47,  1.14it/s]Extractor Predicting: 235it [02:47,  1.27it/s]Extractor Predicting: 236it [02:48,  1.37it/s]Extractor Predicting: 237it [02:48,  1.45it/s]Extractor Predicting: 238it [02:49,  1.43it/s]Extractor Predicting: 239it [02:50,  1.48it/s]Extractor Predicting: 240it [02:50,  1.54it/s]Extractor Predicting: 241it [02:51,  1.58it/s]Extractor Predicting: 242it [02:51,  1.64it/s]Extractor Predicting: 243it [02:52,  1.37it/s]Extractor Predicting: 244it [02:53,  1.44it/s]Extractor Predicting: 245it [02:54,  1.50it/s]Extractor Predicting: 246it [02:54,  1.54it/s]Extractor Predicting: 247it [02:55,  1.58it/s]Extractor Predicting: 248it [02:56,  1.19it/s]Extractor Predicting: 249it [02:57,  1.31it/s]Extractor Predicting: 250it [02:58,  1.29it/s]Extractor Predicting: 251it [02:58,  1.36it/s]Extractor Predicting: 252it [03:00,  1.08it/s]Extractor Predicting: 253it [03:00,  1.23it/s]Extractor Predicting: 254it [03:01,  1.33it/s]Extractor Predicting: 255it [03:01,  1.39it/s]Extractor Predicting: 256it [03:02,  1.38it/s]Extractor Predicting: 257it [03:03,  1.47it/s]Extractor Predicting: 258it [03:03,  1.53it/s]Extractor Predicting: 259it [03:04,  1.59it/s]Extractor Predicting: 260it [03:04,  1.61it/s]Extractor Predicting: 261it [03:05,  1.52it/s]Extractor Predicting: 262it [03:06,  1.57it/s]Extractor Predicting: 263it [03:06,  1.60it/s]Extractor Predicting: 264it [03:07,  1.63it/s]Extractor Predicting: 265it [03:08,  1.66it/s]Extractor Predicting: 266it [03:09,  1.13it/s]Extractor Predicting: 267it [03:10,  1.23it/s]Extractor Predicting: 268it [03:10,  1.34it/s]Extractor Predicting: 269it [03:11,  1.43it/s]Extractor Predicting: 270it [03:12,  1.23it/s]Extractor Predicting: 271it [03:13,  1.36it/s]Extractor Predicting: 272it [03:13,  1.47it/s]Extractor Predicting: 273it [03:14,  1.51it/s]Extractor Predicting: 274it [03:14,  1.57it/s]Extractor Predicting: 275it [03:15,  1.42it/s]Extractor Predicting: 276it [03:16,  1.49it/s]Extractor Predicting: 277it [03:16,  1.55it/s]Extractor Predicting: 278it [03:17,  1.58it/s]Extractor Predicting: 279it [03:18,  1.59it/s]Extractor Predicting: 280it [03:18,  1.62it/s]Extractor Predicting: 281it [03:19,  1.62it/s]Extractor Predicting: 281it [03:19,  1.41it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:44,109 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:44,554 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:44,554 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:44,554 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:44,554 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:47:46,407 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:47:46,409 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:47:47,212 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:47:49,242 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:47:49,242 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:53,173 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:53,391 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:53,391 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:53,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:47:53,392 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:47:55,609 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:47:55,610 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:47:56,143 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:47:57,507 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:47:57,632 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.44150110375275936,
  "recall": 0.14830194275545008,
  "score": 0.2220248667850799,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.49it/s]Extractor Predicting: 4it [00:02,  1.51it/s]Extractor Predicting: 5it [00:03,  1.40it/s]Extractor Predicting: 6it [00:03,  1.88it/s]Extractor Predicting: 6it [00:03,  1.63it/s]
[INFO|configuration_utils.py:515] 2023-08-28 20:48:25,490 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:48:25,716 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 20:48:26,561 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:48:26,562 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 20:48:26,737 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 20:50:39,803 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 20:50:40,665 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 20:50:44,669 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 20:50:44,814 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_10_seed_0/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 20:50:46,246 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:50:46,414 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:50:46,414 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:50:46,414 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:50:46,414 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:50:46,415 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 20:50:46,415 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2727272727272727,
  "recall": 0.023346303501945526,
  "score": 0.04301075268817204,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 20:50:48,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:53,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:54,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:54,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:55,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:55,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:56,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:57,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:57,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:58,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:59,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:50:59,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:00,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:01,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:01,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:02,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:02,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:03,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:04,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:04,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:05,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:17<04:04, 17.45s/it][WARNING|generation_utils.py:914] 2023-08-28 20:51:07,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:07,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:08,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:09,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:09,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:11,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:11,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:12,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:13,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:13,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:14,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:15,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:16,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:17,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:17,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:18,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:19,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:19,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:20,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:21,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:22,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:22,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:34<03:46, 17.40s/it][WARNING|generation_utils.py:914] 2023-08-28 20:51:23,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:23,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:24,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:25,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:25,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:26,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:27,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:27,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:28,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:29,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:29,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:30,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:31,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:32,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:32,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:33,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:34,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:34,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:35,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:36,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:36,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:37,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:37,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:50<03:17, 16.42s/it][WARNING|generation_utils.py:914] 2023-08-28 20:51:38,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:39,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:39,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:40,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:40,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:41,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:41,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:42,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:42,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:43,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:44,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:44,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:45,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:45,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:46,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:47,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:47,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:48,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:48,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:49,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:50,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [01:02<02:41, 14.68s/it][WARNING|generation_utils.py:914] 2023-08-28 20:51:50,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:51,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:51,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:52,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:53,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:54,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:54,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:55,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:56,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:56,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:57,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:58,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:51:59,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:00,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:00,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:01,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:01,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:02,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:03,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:03,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:04,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:05,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:17<02:30, 15.07s/it][WARNING|generation_utils.py:914] 2023-08-28 20:52:06,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:06,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:07,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:08,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:08,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:09,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:10,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:11,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:11,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:12,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:12,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:13,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:14,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:15,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:15,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:16,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:16,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:17,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:18,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:18,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:19,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:19,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:20,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:20,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:32<02:15, 15.04s/it][WARNING|generation_utils.py:914] 2023-08-28 20:52:21,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:22,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:22,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:23,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:24,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:25,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:26,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:27,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:27,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:28,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:29,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:30,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:30,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:31,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:31,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:32,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:33,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:33,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:34,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:35,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:35,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:36,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:36,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:48<02:02, 15.37s/it][WARNING|generation_utils.py:914] 2023-08-28 20:52:37,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:37,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:38,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:39,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:40,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:40,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:41,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:41,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:42,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:43,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:44,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:44,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:45,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:45,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:46,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:46,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:47,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:48,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:49,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:49,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:50,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:51,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:51,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [02:03<01:47, 15.29s/it][WARNING|generation_utils.py:914] 2023-08-28 20:52:52,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:53,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:53,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:54,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:55,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:55,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:56,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:56,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:57,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:58,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:59,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:52:59,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:00,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:00,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:01,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:02,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:02,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:03,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:03,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:04,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:05,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:05,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:18<01:29, 14.90s/it][WARNING|generation_utils.py:914] 2023-08-28 20:53:06,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:07,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:07,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:08,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:09,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:09,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:10,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:10,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:10,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:12,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:12,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:13,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:13,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:14,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:14,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:15,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:15,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:16,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:17,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:17,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:18,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:30<01:09, 14.00s/it][WARNING|generation_utils.py:914] 2023-08-28 20:53:18,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:19,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:19,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:20,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:20,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:21,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:22,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:23,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:23,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:24,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:25,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:26,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:26,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:27,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:28,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:29,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:29,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:30,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:30,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:31,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:33,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:45<00:57, 14.30s/it][WARNING|generation_utils.py:914] 2023-08-28 20:53:33,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:34,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:34,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:36,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:36,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:37,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:38,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:39,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:39,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:40,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:41,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:41,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:43,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:44,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:44,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:45,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:46,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:46,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:47,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:48,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:48,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:49,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [03:02<00:45, 15.13s/it][WARNING|generation_utils.py:914] 2023-08-28 20:53:50,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:51,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:51,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:52,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:52,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:53,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:54,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:54,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:55,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:56,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:57,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:57,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:58,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:59,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:53:59,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:00,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:01,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:01,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:02,393 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:03,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:04,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:04,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:05,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:05,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:07,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [03:18<00:31, 15.66s/it][WARNING|generation_utils.py:914] 2023-08-28 20:54:07,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:07,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:08,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:09,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:10,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:10,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:11,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:11,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:12,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:12,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:13,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:13,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:14,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:15,078 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:15,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:16,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:17,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:17,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:18,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:19,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:19,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:20,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:21,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:21,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:22,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:22,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:23,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:35<00:15, 15.85s/it][WARNING|generation_utils.py:914] 2023-08-28 20:54:24,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:24,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:25,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:25,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:26,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:27,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:27,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:28,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:29,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:29,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:30,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:31,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:32,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:33,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:33,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:34,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:35,134 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:36,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:36,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:37,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:38,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:38,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 20:54:39,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:51<00:00, 15.96s/it]Generating: 100%|██████████| 15/15 [03:51<00:00, 15.43s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:03,889 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:04,135 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:04,135 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:04,135 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:04,136 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 20:55:08,207 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 20:55:08,559 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:55:09,746 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 20:55:11,376 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:55:11,857 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:17,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:17,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:17,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:17,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 20:55:17,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 20:55:20,140 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 20:55:20,141 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 20:55:21,333 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 20:55:22,017 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 20:55:22,018 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 356, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 443, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 591, 'raw': 640}
{'target': 600, 'success': 620, 'raw': 672}
{'prompt': 'Relation : composer .', 'success_rate': 0.9226190476190477, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8536931818181818, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 540, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8491847826086957, 'errors': {''}}
['Relation : platform . Context : The game is based on the Windows Defender operating system , and offers users a new way to protect themselves from unauthorized access to their systems . Head Entity : Windows Defender , Tail Entity : Windows Microsoft .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 313, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 397, 'raw': 448}
{'target': 600, 'success': 425, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 542, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.8943452380952381, 'errors': {'', "('Xbox Live Arcade', 'platform', '', 'The Xbox Live Arcade is a set of .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 322, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 458, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 515, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.8522727272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 209, 'raw': 256}
{'target': 600, 'success': 233, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 287, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 443, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 597, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8138020833333334, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 316, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 365, 'raw': 448}
{'target': 600, 'success': 395, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 476, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8233695652173914, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 324, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 547, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 624, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8478260869565217, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 573, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8551136363636364, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 415, 'raw': 448}
{'target': 600, 'success': 445, 'raw': 480}
{'target': 600, 'success': 473, 'raw': 512}
{'target': 600, 'success': 502, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 471, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 381, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 576, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8551136363636364, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 196, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 243, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 293, 'raw': 384}
{'target': 600, 'success': 314, 'raw': 416}
{'target': 600, 'success': 342, 'raw': 448}
{'target': 600, 'success': 364, 'raw': 480}
{'target': 600, 'success': 387, 'raw': 512}
{'target': 600, 'success': 412, 'raw': 544}
{'target': 600, 'success': 438, 'raw': 576}
{'target': 600, 'success': 465, 'raw': 608}
{'target': 600, 'success': 486, 'raw': 640}
{'target': 600, 'success': 516, 'raw': 672}
{'target': 600, 'success': 545, 'raw': 704}
{'target': 600, 'success': 566, 'raw': 736}
{'target': 600, 'success': 592, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : position held .', 'success_rate': 0.76875, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 122, 'raw': 160}
{'target': 600, 'success': 141, 'raw': 192}
{'target': 600, 'success': 162, 'raw': 224}
{'target': 600, 'success': 188, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 226, 'raw': 320}
{'target': 600, 'success': 251, 'raw': 352}
{'target': 600, 'success': 276, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 323, 'raw': 448}
{'target': 600, 'success': 343, 'raw': 480}
{'target': 600, 'success': 362, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 407, 'raw': 576}
{'target': 600, 'success': 428, 'raw': 608}
{'target': 600, 'success': 449, 'raw': 640}
{'target': 600, 'success': 474, 'raw': 672}
{'target': 600, 'success': 499, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 548, 'raw': 768}
{'target': 600, 'success': 568, 'raw': 800}
{'target': 600, 'success': 595, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.7118055555555556, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 431, 'raw': 512}
{'target': 600, 'success': 457, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 509, 'raw': 608}
{'target': 600, 'success': 538, 'raw': 640}
{'target': 600, 'success': 568, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 618, 'raw': 736}
{'prompt': 'Relation : religion .', 'success_rate': 0.8396739130434783, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/2_ext.jsonl'}}
estimate vocab size: 11905
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12005, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:01,  1.48s/it]Extractor Estimating: 2it [00:02,  1.01s/it]Extractor Estimating: 3it [00:02,  1.20it/s]Extractor Estimating: 4it [00:03,  1.28it/s]Extractor Estimating: 5it [00:04,  1.31it/s]Extractor Estimating: 6it [00:04,  1.48it/s]Extractor Estimating: 7it [00:05,  1.53it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:06,  1.59it/s]Extractor Estimating: 10it [00:07,  1.29it/s]Extractor Estimating: 11it [00:08,  1.39it/s]Extractor Estimating: 12it [00:08,  1.46it/s]Extractor Estimating: 13it [00:09,  1.55it/s]Extractor Estimating: 14it [00:10,  1.54it/s]Extractor Estimating: 15it [00:10,  1.40it/s]Extractor Estimating: 16it [00:11,  1.48it/s]Extractor Estimating: 17it [00:12,  1.56it/s]Extractor Estimating: 18it [00:12,  1.58it/s]Extractor Estimating: 19it [00:13,  1.58it/s]Extractor Estimating: 20it [00:14,  1.53it/s]Extractor Estimating: 21it [00:14,  1.54it/s]Extractor Estimating: 22it [00:15,  1.56it/s]Extractor Estimating: 23it [00:15,  1.60it/s]Extractor Estimating: 24it [00:16,  1.62it/s]Extractor Estimating: 25it [00:17,  1.19it/s]Extractor Estimating: 26it [00:18,  1.28it/s]Extractor Estimating: 27it [00:19,  1.32it/s]Extractor Estimating: 28it [00:19,  1.39it/s]Extractor Estimating: 29it [00:20,  1.23it/s]Extractor Estimating: 30it [00:21,  1.33it/s]Extractor Estimating: 31it [00:22,  1.41it/s]Extractor Estimating: 32it [00:22,  1.44it/s]Extractor Estimating: 33it [00:23,  1.44it/s]Extractor Estimating: 34it [00:24,  1.34it/s]Extractor Estimating: 35it [00:24,  1.38it/s]Extractor Estimating: 36it [00:25,  1.44it/s]Extractor Estimating: 37it [00:26,  1.43it/s]Extractor Estimating: 38it [00:26,  1.46it/s]Extractor Estimating: 39it [00:28,  1.07it/s]Extractor Estimating: 40it [00:29,  1.17it/s]Extractor Estimating: 41it [00:29,  1.29it/s]Extractor Estimating: 42it [00:30,  1.34it/s]Extractor Estimating: 43it [00:31,  1.26it/s]Extractor Estimating: 44it [00:31,  1.37it/s]Extractor Estimating: 45it [00:32,  1.20it/s]Extractor Estimating: 46it [00:33,  1.33it/s]Extractor Estimating: 47it [00:34,  1.39it/s]Extractor Estimating: 48it [00:34,  1.44it/s]Extractor Estimating: 49it [00:35,  1.34it/s]Extractor Estimating: 50it [00:36,  1.38it/s]Extractor Estimating: 51it [00:36,  1.47it/s]Extractor Estimating: 52it [00:37,  1.52it/s]Extractor Estimating: 53it [00:38,  1.54it/s]Extractor Estimating: 54it [00:39,  1.21it/s]Extractor Estimating: 55it [00:39,  1.35it/s]Extractor Estimating: 56it [00:40,  1.40it/s]Extractor Estimating: 57it [00:41,  1.45it/s]Extractor Estimating: 58it [00:41,  1.41it/s]Extractor Estimating: 59it [00:42,  1.51it/s]Extractor Estimating: 60it [00:43,  1.56it/s]Extractor Estimating: 61it [00:43,  1.62it/s]Extractor Estimating: 62it [00:44,  1.67it/s]Extractor Estimating: 63it [00:45,  1.20it/s]Extractor Estimating: 64it [00:46,  1.27it/s]Extractor Estimating: 65it [00:46,  1.37it/s]Extractor Estimating: 66it [00:47,  1.47it/s]Extractor Estimating: 67it [00:49,  1.01it/s]Extractor Estimating: 68it [00:49,  1.16it/s]Extractor Estimating: 69it [00:50,  1.25it/s]Extractor Estimating: 70it [00:50,  1.36it/s]Extractor Estimating: 71it [00:52,  1.20it/s]Extractor Estimating: 72it [00:52,  1.34it/s]Extractor Estimating: 73it [00:53,  1.46it/s]Extractor Estimating: 74it [00:53,  1.55it/s]Extractor Estimating: 75it [00:54,  1.49it/s]Extractor Estimating: 76it [00:55,  1.30it/s]Extractor Estimating: 77it [00:55,  1.45it/s]Extractor Estimating: 78it [00:56,  1.55it/s]Extractor Estimating: 79it [00:57,  1.59it/s]Extractor Estimating: 80it [00:57,  1.64it/s]Extractor Estimating: 81it [00:58,  1.42it/s]Extractor Estimating: 82it [00:59,  1.55it/s]Extractor Estimating: 83it [00:59,  1.66it/s]Extractor Estimating: 84it [01:00,  1.69it/s]Extractor Estimating: 85it [01:00,  1.68it/s]Extractor Estimating: 86it [01:02,  1.01s/it]Extractor Estimating: 87it [01:03,  1.16it/s]Extractor Estimating: 88it [01:04,  1.13it/s]Extractor Estimating: 89it [01:04,  1.28it/s]Extractor Estimating: 90it [01:05,  1.41it/s]Extractor Estimating: 91it [01:05,  1.50it/s]Extractor Estimating: 92it [01:06,  1.34it/s]Extractor Estimating: 93it [01:07,  1.45it/s]Extractor Estimating: 94it [01:07,  1.58it/s]Extractor Estimating: 95it [01:08,  1.69it/s]Extractor Estimating: 96it [01:08,  1.73it/s]Extractor Estimating: 97it [01:09,  1.62it/s]Extractor Estimating: 98it [01:09,  1.74it/s]Extractor Estimating: 99it [01:10,  1.72it/s]Extractor Estimating: 100it [01:11,  1.80it/s]Extractor Estimating: 101it [01:11,  1.76it/s]Extractor Estimating: 102it [01:12,  1.74it/s]Extractor Estimating: 103it [01:13,  1.19it/s]Extractor Estimating: 104it [01:14,  1.30it/s]Extractor Estimating: 105it [01:14,  1.42it/s]Extractor Estimating: 106it [01:15,  1.49it/s]Extractor Estimating: 107it [01:16,  1.41it/s]Extractor Estimating: 108it [01:16,  1.53it/s]Extractor Estimating: 109it [01:17,  1.61it/s]Extractor Estimating: 110it [01:17,  1.67it/s]Extractor Estimating: 111it [01:18,  1.60it/s]Extractor Estimating: 112it [01:19,  1.37it/s]Extractor Estimating: 113it [01:20,  1.48it/s]Extractor Estimating: 114it [01:20,  1.55it/s]Extractor Estimating: 115it [01:21,  1.60it/s]Extractor Estimating: 116it [01:21,  1.66it/s]Extractor Estimating: 117it [01:22,  1.58it/s]Extractor Estimating: 118it [01:23,  1.64it/s]Extractor Estimating: 119it [01:23,  1.65it/s]Extractor Estimating: 120it [01:24,  1.66it/s]Extractor Estimating: 121it [01:24,  1.62it/s]Extractor Estimating: 122it [01:25,  1.54it/s]Extractor Estimating: 123it [01:26,  1.42it/s]Extractor Estimating: 124it [01:27,  1.52it/s]Extractor Estimating: 125it [01:27,  1.58it/s]Extractor Estimating: 126it [01:28,  1.65it/s]Extractor Estimating: 127it [01:28,  1.57it/s]Extractor Estimating: 128it [01:29,  1.57it/s]Extractor Estimating: 129it [01:30,  1.62it/s]Extractor Estimating: 130it [01:30,  1.63it/s]Extractor Estimating: 131it [01:31,  1.76it/s]Extractor Estimating: 132it [01:31,  1.77it/s]Extractor Estimating: 133it [01:32,  1.69it/s]Extractor Estimating: 134it [01:32,  1.76it/s]Extractor Estimating: 135it [01:33,  1.80it/s]Extractor Estimating: 136it [01:33,  1.86it/s]Extractor Estimating: 137it [01:35,  1.24it/s]Extractor Estimating: 138it [01:35,  1.39it/s]Extractor Estimating: 139it [01:36,  1.47it/s]Extractor Estimating: 140it [01:36,  1.57it/s]Extractor Estimating: 141it [01:37,  1.34it/s]Extractor Estimating: 142it [01:38,  1.44it/s]Extractor Estimating: 143it [01:39,  1.51it/s]Extractor Estimating: 144it [01:39,  1.62it/s]Extractor Estimating: 145it [01:40,  1.70it/s]Extractor Estimating: 146it [01:41,  1.48it/s]Extractor Estimating: 147it [01:41,  1.58it/s]Extractor Estimating: 148it [01:42,  1.66it/s]Extractor Estimating: 149it [01:42,  1.72it/s]Extractor Estimating: 150it [01:43,  1.73it/s]Extractor Estimating: 151it [01:44,  1.51it/s]Extractor Estimating: 152it [01:44,  1.51it/s]Extractor Estimating: 153it [01:45,  1.51it/s]Extractor Estimating: 154it [01:46,  1.42it/s]Extractor Estimating: 155it [01:46,  1.54it/s]Extractor Estimating: 156it [01:47,  1.21it/s]Extractor Estimating: 157it [01:48,  1.21it/s]Extractor Estimating: 158it [01:49,  1.29it/s]Extractor Estimating: 159it [01:50,  1.36it/s]Extractor Estimating: 160it [01:51,  1.16it/s]Extractor Estimating: 161it [01:52,  1.19it/s]Extractor Estimating: 162it [01:52,  1.27it/s]Extractor Estimating: 163it [01:53,  1.35it/s]Extractor Estimating: 164it [01:54,  1.15it/s]Extractor Estimating: 165it [01:55,  1.23it/s]Extractor Estimating: 166it [01:55,  1.34it/s]Extractor Estimating: 167it [01:56,  1.45it/s]Extractor Estimating: 168it [01:57,  1.11it/s]Extractor Estimating: 169it [01:58,  1.22it/s]Extractor Estimating: 170it [01:58,  1.33it/s]Extractor Estimating: 171it [01:59,  1.43it/s]Extractor Estimating: 172it [02:00,  1.20it/s]Extractor Estimating: 173it [02:01,  1.32it/s]Extractor Estimating: 174it [02:01,  1.40it/s]Extractor Estimating: 175it [02:02,  1.46it/s]Extractor Estimating: 176it [02:03,  1.53it/s]Extractor Estimating: 177it [02:03,  1.43it/s]Extractor Estimating: 178it [02:04,  1.54it/s]Extractor Estimating: 179it [02:04,  1.61it/s]Extractor Estimating: 180it [02:05,  1.45it/s]Extractor Estimating: 181it [02:06,  1.56it/s]Extractor Estimating: 182it [02:06,  1.64it/s]Extractor Estimating: 183it [02:07,  1.70it/s]Extractor Estimating: 184it [02:07,  1.72it/s]Extractor Estimating: 185it [02:09,  1.18it/s]Extractor Estimating: 186it [02:09,  1.32it/s]Extractor Estimating: 187it [02:10,  1.41it/s]Extractor Estimating: 188it [02:11,  1.49it/s]Extractor Estimating: 189it [02:11,  1.50it/s]Extractor Estimating: 190it [02:12,  1.59it/s]Extractor Estimating: 191it [02:12,  1.63it/s]Extractor Estimating: 192it [02:13,  1.65it/s]Extractor Estimating: 193it [02:14,  1.63it/s]Extractor Estimating: 194it [02:14,  1.64it/s]Extractor Estimating: 195it [02:15,  1.44it/s]Extractor Estimating: 196it [02:16,  1.53it/s]Extractor Estimating: 197it [02:16,  1.63it/s]Extractor Estimating: 198it [02:17,  1.70it/s]Extractor Estimating: 199it [02:17,  1.66it/s]Extractor Estimating: 200it [02:19,  1.18it/s]Extractor Estimating: 201it [02:19,  1.25it/s]Extractor Estimating: 202it [02:20,  1.29it/s]Extractor Estimating: 203it [02:21,  1.38it/s]Extractor Estimating: 204it [02:22,  1.25it/s]Extractor Estimating: 205it [02:22,  1.31it/s]Extractor Estimating: 206it [02:23,  1.33it/s]Extractor Estimating: 207it [02:24,  1.39it/s]Extractor Estimating: 208it [02:25,  1.24it/s]Extractor Estimating: 209it [02:26,  1.28it/s]Extractor Estimating: 210it [02:26,  1.33it/s]Extractor Estimating: 211it [02:27,  1.37it/s]Extractor Estimating: 212it [02:28,  1.12it/s]Extractor Estimating: 213it [02:29,  1.23it/s]Extractor Estimating: 214it [02:29,  1.32it/s]Extractor Estimating: 215it [02:30,  1.41it/s]Extractor Estimating: 216it [02:32,  1.03s/it]Extractor Estimating: 217it [02:32,  1.11it/s]Extractor Estimating: 218it [02:33,  1.23it/s]Extractor Estimating: 219it [02:34,  1.26it/s]Extractor Estimating: 220it [02:35,  1.00s/it]Extractor Estimating: 221it [02:37,  1.23s/it]Extractor Estimating: 222it [02:38,  1.06s/it]Extractor Estimating: 223it [02:38,  1.06it/s]Extractor Estimating: 224it [02:39,  1.07it/s]Extractor Estimating: 225it [02:40,  1.18it/s]Extractor Estimating: 226it [02:40,  1.33it/s]Extractor Estimating: 227it [02:41,  1.51it/s]Extractor Estimating: 228it [02:41,  1.67it/s]Extractor Estimating: 229it [02:42,  1.64it/s]Extractor Estimating: 230it [02:43,  1.56it/s]Extractor Estimating: 231it [02:43,  1.76it/s]Extractor Estimating: 232it [02:44,  1.85it/s]Extractor Estimating: 233it [02:44,  1.88it/s]Extractor Estimating: 234it [02:45,  1.90it/s]Extractor Estimating: 235it [02:45,  1.99it/s]Extractor Estimating: 236it [02:46,  1.78it/s]Extractor Estimating: 237it [02:46,  1.82it/s]Extractor Estimating: 238it [02:47,  1.86it/s]Extractor Estimating: 239it [02:47,  1.95it/s]Extractor Estimating: 240it [02:48,  1.95it/s]Extractor Estimating: 241it [02:48,  2.01it/s]Extractor Estimating: 242it [02:49,  1.67it/s]Extractor Estimating: 243it [02:49,  1.79it/s]Extractor Estimating: 244it [02:50,  1.88it/s]Extractor Estimating: 245it [02:50,  1.92it/s]Extractor Estimating: 246it [02:51,  1.97it/s]Extractor Estimating: 247it [02:51,  1.97it/s]Extractor Estimating: 248it [02:52,  1.60it/s]Extractor Estimating: 249it [02:53,  1.74it/s]Extractor Estimating: 250it [02:53,  1.81it/s]Extractor Estimating: 251it [02:54,  1.73it/s]Extractor Estimating: 252it [02:55,  1.67it/s]Extractor Estimating: 253it [02:55,  1.52it/s]Extractor Estimating: 254it [02:56,  1.55it/s]Extractor Estimating: 255it [02:57,  1.58it/s]Extractor Estimating: 256it [02:57,  1.64it/s]Extractor Estimating: 257it [02:58,  1.70it/s]Extractor Estimating: 258it [02:59,  1.47it/s]Extractor Estimating: 259it [02:59,  1.47it/s]Extractor Estimating: 260it [03:00,  1.54it/s]Extractor Estimating: 261it [03:00,  1.58it/s]Extractor Estimating: 262it [03:01,  1.60it/s]Extractor Estimating: 263it [03:02,  1.38it/s]Extractor Estimating: 264it [03:03,  1.43it/s]Extractor Estimating: 265it [03:03,  1.52it/s]Extractor Estimating: 266it [03:04,  1.57it/s]Extractor Estimating: 267it [03:04,  1.53it/s]Extractor Estimating: 268it [03:05,  1.46it/s]Extractor Estimating: 269it [03:06,  1.47it/s]Extractor Estimating: 270it [03:07,  1.52it/s]Extractor Estimating: 271it [03:07,  1.56it/s]Extractor Estimating: 272it [03:08,  1.62it/s]Extractor Estimating: 273it [03:08,  1.58it/s]Extractor Estimating: 274it [03:09,  1.55it/s]Extractor Estimating: 275it [03:10,  1.57it/s]Extractor Estimating: 276it [03:10,  1.43it/s]Extractor Estimating: 277it [03:11,  1.47it/s]Extractor Estimating: 278it [03:12,  1.46it/s]Extractor Estimating: 279it [03:12,  1.54it/s]Extractor Estimating: 280it [03:13,  1.57it/s]Extractor Estimating: 281it [03:14,  1.51it/s]Extractor Estimating: 282it [03:14,  1.55it/s]Extractor Estimating: 283it [03:15,  1.61it/s]Extractor Estimating: 284it [03:15,  1.70it/s]Extractor Estimating: 285it [03:16,  1.68it/s]Extractor Estimating: 286it [03:18,  1.16it/s]Extractor Estimating: 287it [03:18,  1.25it/s]Extractor Estimating: 288it [03:19,  1.37it/s]Extractor Estimating: 289it [03:19,  1.42it/s]Extractor Estimating: 290it [03:21,  1.17it/s]Extractor Estimating: 291it [03:21,  1.26it/s]Extractor Estimating: 292it [03:22,  1.40it/s]Extractor Estimating: 293it [03:22,  1.45it/s]Extractor Estimating: 294it [03:23,  1.30it/s]Extractor Estimating: 295it [03:24,  1.42it/s]Extractor Estimating: 296it [03:24,  1.49it/s]Extractor Estimating: 297it [03:25,  1.52it/s]Extractor Estimating: 298it [03:26,  1.63it/s]Extractor Estimating: 299it [03:27,  1.35it/s]Extractor Estimating: 300it [03:27,  1.43it/s]Extractor Estimating: 301it [03:28,  1.53it/s]Extractor Estimating: 302it [03:28,  1.55it/s]Extractor Estimating: 303it [03:29,  1.59it/s]Extractor Estimating: 304it [03:31,  1.11it/s]Extractor Estimating: 305it [03:31,  1.25it/s]Extractor Estimating: 306it [03:32,  1.38it/s]Extractor Estimating: 307it [03:32,  1.46it/s]Extractor Estimating: 308it [03:33,  1.31it/s]Extractor Estimating: 309it [03:34,  1.40it/s]Extractor Estimating: 310it [03:34,  1.49it/s]Extractor Estimating: 311it [03:35,  1.55it/s]Extractor Estimating: 312it [03:36,  1.58it/s]Extractor Estimating: 313it [03:36,  1.46it/s]Extractor Estimating: 314it [03:37,  1.48it/s]Extractor Estimating: 315it [03:38,  1.53it/s]Extractor Estimating: 316it [03:39,  1.28it/s]Extractor Estimating: 317it [03:39,  1.40it/s]Extractor Estimating: 318it [03:40,  1.37it/s]Extractor Estimating: 319it [03:41,  1.46it/s]Extractor Estimating: 320it [03:42,  1.17it/s]Extractor Estimating: 321it [03:43,  1.27it/s]Extractor Estimating: 322it [03:43,  1.37it/s]Extractor Estimating: 323it [03:44,  1.45it/s]Extractor Estimating: 324it [03:45,  1.36it/s]Extractor Estimating: 325it [03:45,  1.47it/s]Extractor Estimating: 326it [03:46,  1.56it/s]Extractor Estimating: 327it [03:46,  1.61it/s]Extractor Estimating: 328it [03:47,  1.68it/s]Extractor Estimating: 329it [03:48,  1.14it/s]Extractor Estimating: 330it [03:49,  1.30it/s]Extractor Estimating: 331it [03:49,  1.46it/s]Extractor Estimating: 332it [03:50,  1.60it/s]Extractor Estimating: 333it [03:51,  1.34it/s]Extractor Estimating: 334it [03:51,  1.40it/s]Extractor Estimating: 335it [03:52,  1.54it/s]Extractor Estimating: 336it [03:52,  1.62it/s]Extractor Estimating: 337it [03:53,  1.68it/s]Extractor Estimating: 338it [03:54,  1.43it/s]Extractor Estimating: 339it [03:55,  1.51it/s]Extractor Estimating: 340it [03:55,  1.58it/s]Extractor Estimating: 341it [03:56,  1.61it/s]Extractor Estimating: 342it [03:56,  1.70it/s]Extractor Estimating: 343it [03:58,  1.10it/s]Extractor Estimating: 344it [03:58,  1.23it/s]Extractor Estimating: 345it [03:59,  1.41it/s]Extractor Estimating: 346it [03:59,  1.52it/s]Extractor Estimating: 347it [04:00,  1.47it/s]Extractor Estimating: 348it [04:01,  1.59it/s]Extractor Estimating: 349it [04:01,  1.64it/s]Extractor Estimating: 350it [04:02,  1.67it/s]Extractor Estimating: 351it [04:02,  1.65it/s]Extractor Estimating: 352it [04:04,  1.06it/s]Extractor Estimating: 353it [04:05,  1.20it/s]Extractor Estimating: 354it [04:05,  1.32it/s]Extractor Estimating: 355it [04:06,  1.44it/s]Extractor Estimating: 356it [04:07,  1.30it/s]Extractor Estimating: 357it [04:07,  1.39it/s]Extractor Estimating: 358it [04:08,  1.46it/s]Extractor Estimating: 359it [04:09,  1.54it/s]Extractor Estimating: 360it [04:11,  1.23s/it]Extractor Estimating: 361it [04:12,  1.06s/it]Extractor Estimating: 362it [04:13,  1.15s/it]Extractor Estimating: 363it [04:14,  1.03it/s]Extractor Estimating: 364it [04:14,  1.18it/s]Extractor Estimating: 365it [04:15,  1.30it/s]Extractor Estimating: 366it [04:16,  1.10it/s]Extractor Estimating: 367it [04:17,  1.23it/s]Extractor Estimating: 368it [04:17,  1.35it/s]Extractor Estimating: 369it [04:18,  1.42it/s]Extractor Estimating: 370it [04:19,  1.37it/s]Extractor Estimating: 371it [04:19,  1.42it/s]Extractor Estimating: 372it [04:20,  1.49it/s]Extractor Estimating: 373it [04:21,  1.58it/s]Extractor Estimating: 374it [04:21,  1.62it/s]Extractor Estimating: 375it [04:22,  1.27it/s]Extractor Estimating: 375it [04:22,  1.43it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:28,006 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:28,157 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:28,157 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:28,157 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:28,157 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 21:01:31,246 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 21:01:31,340 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:01:32,412 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 21:01:34,173 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:01:34,173 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:41,309 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:41,457 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:41,457 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:41,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 21:01:41,458 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 21:01:45,193 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 21:01:45,338 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 21:01:47,436 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 21:01:48,744 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 21:01:48,910 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 23:06:37,663 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 23:06:39,778 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4500, 'num_train': 3000}
num of filtered data: 7493 mean pseudo reward: 0.9278789011136784
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 22565
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22665, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22665, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.920, loss:871.4083
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.915, loss:783.8708
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 2.433, loss:850.2952
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.926, loss:804.4445
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.920, loss:804.1754
>> valid entity prec:0.5801, rec:0.6112, f1:0.5952
>> valid relation prec:0.3925, rec:0.1407, f1:0.2071
>> valid relation with NER prec:0.3925, rec:0.1407, f1:0.2071
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.399, loss:809.2697
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.911, loss:803.4048
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.913, loss:809.8739
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.926, loss:840.7821
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.925, loss:777.9153
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5508, rec:0.4921, f1:0.5198
>> valid relation prec:0.3739, rec:0.1011, f1:0.1592
>> valid relation with NER prec:0.3739, rec:0.1011, f1:0.1592
g_step 1100, step 161, avg_time 2.140, loss:798.1766
g_step 1200, step 261, avg_time 0.920, loss:793.8473
g_step 1300, step 48, avg_time 0.927, loss:752.8793
g_step 1400, step 148, avg_time 0.910, loss:746.3610
g_step 1500, step 248, avg_time 0.908, loss:764.8529
>> valid entity prec:0.5663, rec:0.6211, f1:0.5925
>> valid relation prec:0.2680, rec:0.1223, f1:0.1680
>> valid relation with NER prec:0.2680, rec:0.1223, f1:0.1680
g_step 1600, step 35, avg_time 2.127, loss:762.0409
g_step 1700, step 135, avg_time 0.928, loss:714.4454
g_step 1800, step 235, avg_time 0.929, loss:734.4864
g_step 1900, step 22, avg_time 0.908, loss:738.4095
g_step 2000, step 122, avg_time 0.933, loss:677.5406
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5670, rec:0.6221, f1:0.5933
>> valid relation prec:0.2856, rec:0.1063, f1:0.1549
>> valid relation with NER prec:0.2856, rec:0.1063, f1:0.1549
g_step 2100, step 222, avg_time 2.149, loss:670.7938
g_step 2200, step 9, avg_time 0.916, loss:701.2513
g_step 2300, step 109, avg_time 0.934, loss:641.8850
g_step 2400, step 209, avg_time 0.919, loss:683.6626
g_step 2500, step 309, avg_time 0.918, loss:640.6505
>> valid entity prec:0.6140, rec:0.5521, f1:0.5814
>> valid relation prec:0.2757, rec:0.1032, f1:0.1501
>> valid relation with NER prec:0.2757, rec:0.1032, f1:0.1501
g_step 2600, step 96, avg_time 2.122, loss:607.4226
g_step 2700, step 196, avg_time 0.932, loss:635.3090
g_step 2800, step 296, avg_time 0.929, loss:633.7619
g_step 2900, step 83, avg_time 0.920, loss:580.4100
g_step 3000, step 183, avg_time 0.934, loss:616.9263
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6230, rec:0.5538, f1:0.5864
>> valid relation prec:0.3160, rec:0.1235, f1:0.1776
>> valid relation with NER prec:0.3160, rec:0.1235, f1:0.1776
g_step 3100, step 283, avg_time 2.133, loss:613.1966
g_step 3200, step 70, avg_time 0.920, loss:568.8464
g_step 3300, step 170, avg_time 0.917, loss:560.3130
g_step 3400, step 270, avg_time 0.937, loss:573.3882
g_step 3500, step 57, avg_time 0.913, loss:556.3871
>> valid entity prec:0.5615, rec:0.5939, f1:0.5772
>> valid relation prec:0.2753, rec:0.1269, f1:0.1738
>> valid relation with NER prec:0.2753, rec:0.1269, f1:0.1738
g_step 3600, step 157, avg_time 2.141, loss:535.9707
g_step 3700, step 257, avg_time 0.937, loss:570.6123
g_step 3800, step 44, avg_time 0.920, loss:526.2431
g_step 3900, step 144, avg_time 0.926, loss:508.1334
g_step 4000, step 244, avg_time 0.929, loss:540.0745
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6085, rec:0.5345, f1:0.5691
>> valid relation prec:0.3213, rec:0.1178, f1:0.1724
>> valid relation with NER prec:0.3213, rec:0.1178, f1:0.1724
g_step 4100, step 31, avg_time 2.142, loss:526.5898
g_step 4200, step 131, avg_time 0.926, loss:473.0069
g_step 4300, step 231, avg_time 0.923, loss:515.7141
g_step 4400, step 18, avg_time 0.928, loss:515.6780
g_step 4500, step 118, avg_time 0.944, loss:468.9468
>> valid entity prec:0.5763, rec:0.5338, f1:0.5542
>> valid relation prec:0.2439, rec:0.1054, f1:0.1472
>> valid relation with NER prec:0.2439, rec:0.1054, f1:0.1472
g_step 4600, step 218, avg_time 2.144, loss:489.1236
g_step 4700, step 5, avg_time 0.914, loss:479.0858
g_step 4800, step 105, avg_time 0.931, loss:447.4592
g_step 4900, step 205, avg_time 0.926, loss:458.8817
g_step 5000, step 305, avg_time 0.939, loss:506.0713
learning rate was adjusted to 0.0008
>> valid entity prec:0.5762, rec:0.5576, f1:0.5667
>> valid relation prec:0.2652, rec:0.1186, f1:0.1639
>> valid relation with NER prec:0.2652, rec:0.1186, f1:0.1639
g_step 5100, step 92, avg_time 2.153, loss:434.6031
g_step 5200, step 192, avg_time 0.923, loss:444.7527
g_step 5300, step 292, avg_time 0.930, loss:444.3365
g_step 5400, step 79, avg_time 0.927, loss:432.4408
g_step 5500, step 179, avg_time 0.932, loss:428.7625
>> valid entity prec:0.6113, rec:0.5638, f1:0.5866
>> valid relation prec:0.2618, rec:0.1252, f1:0.1694
>> valid relation with NER prec:0.2618, rec:0.1252, f1:0.1694
g_step 5600, step 279, avg_time 2.160, loss:435.0472
g_step 5700, step 66, avg_time 0.927, loss:399.3439
g_step 5800, step 166, avg_time 0.934, loss:435.4148
g_step 5900, step 266, avg_time 0.931, loss:416.0610
g_step 6000, step 53, avg_time 0.948, loss:392.7665
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5826, rec:0.5459, f1:0.5636
>> valid relation prec:0.2399, rec:0.1238, f1:0.1633
>> valid relation with NER prec:0.2399, rec:0.1238, f1:0.1633
g_step 6100, step 153, avg_time 2.158, loss:390.9165
g_step 6200, step 253, avg_time 0.928, loss:401.2518
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 23:06:39 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 23:06:39 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_23-06-37_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 23:06:43 - WARNING - datasets.builder -   Using custom data configuration default-ba4055795f7e3aa6
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ba4055795f7e3aa6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 23:06:59,768 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:06:59,769 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:06:59,769 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:06:59,770 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:07:00,537 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:07:00,722 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:07:00,722 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:07:00,722 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:07:00,722 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:07:00,723 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:07:00,723 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 23:07:02,847 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:07:06,310 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 23:07:06,461 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ba4055795f7e3aa6/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:06,  1.07ba/s] 25%|██▌       | 2/8 [00:01<00:03,  1.99ba/s] 38%|███▊      | 3/8 [00:01<00:01,  2.74ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.32ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.76ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.08ba/s] 88%|████████▊ | 7/8 [00:02<00:00,  4.31ba/s]100%|██████████| 8/8 [00:02<00:00,  5.24ba/s]100%|██████████| 8/8 [00:02<00:00,  3.55ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.01ba/s] 50%|█████     | 2/4 [00:01<00:01,  1.87ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.59ba/s]100%|██████████| 4/4 [00:01<00:00,  3.64ba/s]100%|██████████| 4/4 [00:01<00:00,  2.20ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.27ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.10ba/s] 50%|█████     | 4/8 [00:00<00:00,  6.87ba/s] 75%|███████▌  | 6/8 [00:00<00:00,  8.63ba/s]100%|██████████| 8/8 [00:01<00:00, 10.57ba/s]100%|██████████| 8/8 [00:01<00:00,  7.90ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.31ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.63ba/s]100%|██████████| 4/4 [00:01<00:00,  5.44ba/s]100%|██████████| 4/4 [00:01<00:00,  3.96ba/s]
[INFO|trainer.py:414] 2023-08-28 23:07:26,929 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 23:07:27,643 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 23:07:27,644 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-28 23:07:27,644 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 23:07:27,644 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 23:07:27,644 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 23:07:27,644 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 23:07:27,644 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:04<46:57,  4.82s/it]  0%|          | 2/585 [00:06<29:33,  3.04s/it]  1%|          | 3/585 [00:08<23:06,  2.38s/it]  1%|          | 4/585 [00:09<17:46,  1.83s/it]  1%|          | 5/585 [00:09<13:28,  1.39s/it]  1%|          | 6/585 [00:10<11:20,  1.17s/it]  1%|          | 7/585 [00:13<15:36,  1.62s/it]  1%|▏         | 8/585 [00:13<12:53,  1.34s/it]  2%|▏         | 9/585 [00:14<09:41,  1.01s/it]  2%|▏         | 10/585 [00:14<07:31,  1.27it/s]  2%|▏         | 11/585 [00:14<06:03,  1.58it/s]  2%|▏         | 12/585 [00:15<07:09,  1.33it/s]  2%|▏         | 13/585 [00:16<05:48,  1.64it/s]  2%|▏         | 14/585 [00:16<04:51,  1.96it/s]  3%|▎         | 15/585 [00:16<04:12,  2.26it/s]  3%|▎         | 16/585 [00:16<03:44,  2.53it/s]  3%|▎         | 17/585 [00:17<03:25,  2.76it/s]  3%|▎         | 18/585 [00:17<03:12,  2.95it/s]  3%|▎         | 19/585 [00:17<03:02,  3.10it/s]  3%|▎         | 20/585 [00:18<03:33,  2.65it/s]  4%|▎         | 21/585 [00:18<03:17,  2.86it/s]  4%|▍         | 22/585 [00:18<03:05,  3.03it/s]  4%|▍         | 23/585 [00:19<02:58,  3.16it/s]  4%|▍         | 24/585 [00:19<02:52,  3.25it/s]  4%|▍         | 25/585 [00:19<02:48,  3.32it/s]  4%|▍         | 26/585 [00:19<02:45,  3.38it/s]  5%|▍         | 27/585 [00:20<02:43,  3.42it/s]  5%|▍         | 28/585 [00:20<02:41,  3.44it/s]  5%|▍         | 29/585 [00:20<02:40,  3.46it/s]  5%|▌         | 30/585 [00:21<04:23,  2.11it/s]  5%|▌         | 31/585 [00:21<03:51,  2.40it/s]  5%|▌         | 32/585 [00:22<03:28,  2.65it/s]  6%|▌         | 33/585 [00:22<03:13,  2.86it/s]  6%|▌         | 34/585 [00:22<03:02,  3.02it/s]  6%|▌         | 35/585 [00:23<02:54,  3.15it/s]  6%|▌         | 36/585 [00:23<02:49,  3.25it/s]  6%|▋         | 37/585 [00:23<02:45,  3.32it/s]  6%|▋         | 38/585 [00:23<02:42,  3.37it/s]  7%|▋         | 39/585 [00:24<03:13,  2.82it/s]  7%|▋         | 40/585 [00:24<03:01,  2.99it/s]  7%|▋         | 41/585 [00:25<02:53,  3.13it/s]  7%|▋         | 42/585 [00:25<02:47,  3.23it/s]  7%|▋         | 43/585 [00:25<02:43,  3.31it/s]  8%|▊         | 44/585 [00:25<02:41,  3.36it/s]  8%|▊         | 45/585 [00:26<02:38,  3.40it/s]  8%|▊         | 46/585 [00:26<02:37,  3.43it/s]  8%|▊         | 47/585 [00:26<02:36,  3.45it/s]  8%|▊         | 48/585 [00:27<02:35,  3.46it/s]  8%|▊         | 49/585 [00:27<03:17,  2.72it/s]  9%|▊         | 50/585 [00:27<03:03,  2.91it/s]  9%|▊         | 51/585 [00:28<02:54,  3.07it/s]  9%|▉         | 52/585 [00:28<02:47,  3.19it/s]  9%|▉         | 53/585 [00:28<02:42,  3.27it/s]  9%|▉         | 54/585 [00:29<02:38,  3.34it/s]  9%|▉         | 55/585 [00:29<02:36,  3.38it/s] 10%|▉         | 56/585 [00:29<02:34,  3.42it/s] 10%|▉         | 57/585 [00:29<02:33,  3.44it/s] 10%|▉         | 58/585 [00:30<02:32,  3.45it/s] 10%|█         | 59/585 [00:30<02:51,  3.06it/s] 10%|█         | 60/585 [00:30<02:45,  3.18it/s] 10%|█         | 61/585 [00:31<02:40,  3.27it/s] 11%|█         | 62/585 [00:31<02:36,  3.33it/s] 11%|█         | 63/585 [00:31<02:34,  3.38it/s] 11%|█         | 64/585 [00:32<02:33,  3.40it/s] 11%|█         | 65/585 [00:32<02:32,  3.41it/s] 11%|█▏        | 66/585 [00:32<02:31,  3.43it/s] 11%|█▏        | 67/585 [00:32<02:30,  3.45it/s] 12%|█▏        | 68/585 [00:33<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:33<02:52,  2.99it/s] 12%|█▏        | 70/585 [00:33<02:44,  3.13it/s] 12%|█▏        | 71/585 [00:34<02:39,  3.23it/s] 12%|█▏        | 72/585 [00:34<02:35,  3.31it/s] 12%|█▏        | 73/585 [00:34<02:32,  3.36it/s] 13%|█▎        | 74/585 [00:35<02:30,  3.40it/s] 13%|█▎        | 75/585 [00:35<02:28,  3.43it/s] 13%|█▎        | 76/585 [00:35<02:27,  3.45it/s] 13%|█▎        | 77/585 [00:35<02:26,  3.46it/s] 13%|█▎        | 78/585 [00:36<02:26,  3.47it/s] 14%|█▎        | 79/585 [00:36<03:10,  2.66it/s] 14%|█▎        | 80/585 [00:37<02:56,  2.87it/s] 14%|█▍        | 81/585 [00:37<02:46,  3.03it/s] 14%|█▍        | 82/585 [00:37<02:39,  3.15it/s] 14%|█▍        | 83/585 [00:37<02:34,  3.25it/s] 14%|█▍        | 84/585 [00:38<02:30,  3.32it/s] 15%|█▍        | 85/585 [00:38<02:28,  3.37it/s] 15%|█▍        | 86/585 [00:38<02:26,  3.41it/s] 15%|█▍        | 87/585 [00:39<02:24,  3.44it/s] 15%|█▌        | 88/585 [00:39<02:23,  3.46it/s] 15%|█▌        | 89/585 [00:39<03:00,  2.75it/s] 15%|█▌        | 90/585 [00:40<02:48,  2.93it/s] 16%|█▌        | 91/585 [00:40<02:40,  3.08it/s] 16%|█▌        | 92/585 [00:40<02:34,  3.20it/s] 16%|█▌        | 93/585 [00:41<02:30,  3.28it/s] 16%|█▌        | 94/585 [00:41<02:27,  3.34it/s] 16%|█▌        | 95/585 [00:41<02:24,  3.39it/s] 16%|█▋        | 96/585 [00:41<02:23,  3.42it/s] 17%|█▋        | 97/585 [00:42<02:21,  3.44it/s] 17%|█▋        | 98/585 [00:42<02:20,  3.46it/s] 17%|█▋        | 99/585 [00:43<03:33,  2.28it/s] 17%|█▋        | 100/585 [00:43<03:10,  2.54it/s] 17%|█▋        | 101/585 [00:43<02:54,  2.77it/s] 17%|█▋        | 102/585 [00:44<02:43,  2.95it/s] 18%|█▊        | 103/585 [00:44<02:35,  3.10it/s] 18%|█▊        | 104/585 [00:44<02:29,  3.21it/s] 18%|█▊        | 105/585 [00:44<02:26,  3.29it/s] 18%|█▊        | 106/585 [00:45<02:23,  3.34it/s] 18%|█▊        | 107/585 [00:45<02:21,  3.39it/s] 18%|█▊        | 108/585 [00:46<03:30,  2.26it/s] 19%|█▊        | 109/585 [00:46<03:08,  2.53it/s] 19%|█▉        | 110/585 [00:46<02:52,  2.76it/s] 19%|█▉        | 111/585 [00:47<02:41,  2.94it/s] 19%|█▉        | 112/585 [00:47<02:33,  3.09it/s] 19%|█▉        | 113/585 [00:47<02:27,  3.20it/s] 19%|█▉        | 114/585 [00:48<02:23,  3.28it/s] 20%|█▉        | 115/585 [00:48<02:20,  3.34it/s] 20%|█▉        | 116/585 [00:48<02:18,  3.39it/s] 20%|██        | 117/585 [00:48<02:16,  3.42it/s][INFO|trainer.py:2140] 2023-08-28 23:08:16,853 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:08:16,853 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 23:08:16,853 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.95it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.98it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.51it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.57it/s][A
  6%|▌         | 27/437 [00:00<00:08, 46.25it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.91it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.54it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.80it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.32it/s][A
 12%|█▏        | 52/437 [00:01<00:12, 30.77it/s][A
 13%|█▎        | 57/437 [00:01<00:17, 21.53it/s][A
 14%|█▍        | 62/437 [00:01<00:14, 25.83it/s][A
 15%|█▌        | 67/437 [00:01<00:12, 29.66it/s][A
 16%|█▋        | 72/437 [00:02<00:11, 33.08it/s][A
 18%|█▊        | 77/437 [00:02<00:10, 35.97it/s][A
 19%|█▉        | 82/437 [00:02<00:09, 38.33it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 40.16it/s][A
 21%|██        | 92/437 [00:02<00:08, 41.50it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 42.32it/s][A
 23%|██▎       | 102/437 [00:03<00:07, 42.63it/s][A
 24%|██▍       | 107/437 [00:03<00:14, 23.45it/s][A
 26%|██▌       | 112/437 [00:03<00:11, 27.39it/s][A
 27%|██▋       | 117/437 [00:03<00:10, 31.04it/s][A
 28%|██▊       | 122/437 [00:03<00:09, 34.27it/s][A
 29%|██▉       | 127/437 [00:03<00:08, 36.95it/s][A
 30%|███       | 132/437 [00:03<00:07, 39.11it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 40.81it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 41.85it/s][A
 34%|███▎      | 147/437 [00:04<00:06, 42.27it/s][A
 35%|███▍      | 152/437 [00:04<00:06, 42.58it/s][A
 36%|███▌      | 157/437 [00:04<00:06, 43.05it/s][A
 37%|███▋      | 162/437 [00:04<00:06, 43.62it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 44.01it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.31it/s][A
 41%|████      | 177/437 [00:04<00:09, 27.07it/s][A
 42%|████▏     | 182/437 [00:05<00:09, 27.80it/s][A
 43%|████▎     | 187/437 [00:05<00:07, 31.87it/s][A
 44%|████▍     | 192/437 [00:05<00:07, 34.90it/s][A
 45%|████▌     | 197/437 [00:05<00:06, 37.50it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 39.50it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 41.08it/s][A
 49%|████▊     | 212/437 [00:06<00:05, 42.17it/s][A
 50%|████▉     | 217/437 [00:06<00:11, 19.77it/s][A
 51%|█████     | 222/437 [00:06<00:09, 23.77it/s][A
 52%|█████▏    | 227/437 [00:06<00:07, 27.69it/s][A
 53%|█████▎    | 232/437 [00:06<00:06, 31.31it/s][A
 54%|█████▍    | 237/437 [00:06<00:05, 34.53it/s][A
 55%|█████▌    | 242/437 [00:06<00:05, 37.14it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 39.29it/s][A
 58%|█████▊    | 252/437 [00:07<00:04, 40.76it/s][A
 59%|█████▉    | 257/437 [00:07<00:04, 41.49it/s][A
 60%|█████▉    | 262/437 [00:07<00:04, 42.06it/s][A
 61%|██████    | 267/437 [00:07<00:03, 42.77it/s][A
 62%|██████▏   | 272/437 [00:07<00:03, 43.32it/s][A
 63%|██████▎   | 277/437 [00:07<00:03, 43.91it/s][A
 65%|██████▍   | 282/437 [00:07<00:03, 44.25it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 44.58it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 44.72it/s][A
 68%|██████▊   | 297/437 [00:08<00:03, 44.52it/s][A
 69%|██████▉   | 302/437 [00:08<00:04, 29.83it/s][A
 70%|███████   | 306/437 [00:08<00:06, 20.77it/s][A
 71%|███████   | 309/437 [00:09<00:07, 18.25it/s][A
 72%|███████▏  | 314/437 [00:09<00:05, 22.91it/s][A
 73%|███████▎  | 319/437 [00:09<00:04, 27.30it/s][A
 74%|███████▍  | 324/437 [00:09<00:03, 31.18it/s][A
 75%|███████▌  | 329/437 [00:09<00:03, 34.44it/s][A
 76%|███████▋  | 334/437 [00:09<00:02, 37.17it/s][A
 78%|███████▊  | 339/437 [00:09<00:02, 39.37it/s][A
 79%|███████▊  | 344/437 [00:09<00:02, 40.91it/s][A
 80%|███████▉  | 349/437 [00:09<00:02, 41.66it/s][A
 81%|████████  | 354/437 [00:10<00:01, 42.14it/s][A
 82%|████████▏ | 359/437 [00:10<00:01, 42.78it/s][A
 83%|████████▎ | 364/437 [00:10<00:01, 43.23it/s][A
 84%|████████▍ | 369/437 [00:10<00:01, 43.75it/s][A
 86%|████████▌ | 374/437 [00:10<00:01, 44.08it/s][A
 87%|████████▋ | 379/437 [00:10<00:01, 44.28it/s][A
 88%|████████▊ | 384/437 [00:10<00:01, 44.54it/s][A
 89%|████████▉ | 389/437 [00:10<00:01, 44.71it/s][A
 90%|█████████ | 394/437 [00:10<00:00, 44.57it/s][A
 91%|█████████▏| 399/437 [00:11<00:00, 44.25it/s][A
 92%|█████████▏| 404/437 [00:11<00:00, 44.05it/s][A
 94%|█████████▎| 409/437 [00:11<00:00, 44.31it/s][A
 95%|█████████▍| 414/437 [00:11<00:00, 44.44it/s][A
 96%|█████████▌| 419/437 [00:11<00:00, 44.64it/s][A
 97%|█████████▋| 424/437 [00:11<00:00, 44.74it/s][A
 98%|█████████▊| 429/437 [00:11<00:00, 44.74it/s][A
 99%|█████████▉| 434/437 [00:12<00:00, 44.80it/s][A                                                 
                                                 [A 20%|██        | 117/585 [01:01<02:16,  3.42it/s]
100%|██████████| 437/437 [00:12<00:00, 44.80it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:08:32,117 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-28 23:08:33,929 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:09:35,287 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:09:37,730 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:09:38,421 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [04:45<9:12:59, 71.05s/it] 20%|██        | 119/585 [04:45<6:27:26, 49.88s/it] 21%|██        | 120/585 [04:45<4:31:17, 35.01s/it] 21%|██        | 121/585 [04:46<3:10:10, 24.59s/it] 21%|██        | 122/585 [04:46<2:13:30, 17.30s/it] 21%|██        | 123/585 [04:46<1:33:55, 12.20s/it] 21%|██        | 124/585 [04:46<1:06:16,  8.62s/it] 21%|██▏       | 125/585 [04:47<46:57,  6.12s/it]   22%|██▏       | 126/585 [04:47<33:27,  4.37s/it] 22%|██▏       | 127/585 [04:47<24:02,  3.15s/it] 22%|██▏       | 128/585 [04:48<17:26,  2.29s/it] 22%|██▏       | 129/585 [04:48<13:25,  1.77s/it] 22%|██▏       | 130/585 [04:48<10:02,  1.32s/it] 22%|██▏       | 131/585 [04:49<07:40,  1.01s/it] 23%|██▎       | 132/585 [04:49<06:00,  1.25it/s] 23%|██▎       | 133/585 [04:49<04:51,  1.55it/s] 23%|██▎       | 134/585 [04:50<04:02,  1.86it/s] 23%|██▎       | 135/585 [04:50<03:28,  2.15it/s] 23%|██▎       | 136/585 [04:50<03:05,  2.42it/s] 23%|██▎       | 137/585 [04:51<02:48,  2.66it/s] 24%|██▎       | 138/585 [04:51<02:36,  2.85it/s] 24%|██▍       | 139/585 [04:51<02:52,  2.58it/s] 24%|██▍       | 140/585 [04:52<02:39,  2.79it/s] 24%|██▍       | 141/585 [04:52<02:30,  2.96it/s] 24%|██▍       | 142/585 [04:52<02:23,  3.09it/s] 24%|██▍       | 143/585 [04:52<02:18,  3.19it/s] 25%|██▍       | 144/585 [04:53<02:15,  3.26it/s] 25%|██▍       | 145/585 [04:53<02:13,  3.31it/s] 25%|██▍       | 146/585 [04:53<02:11,  3.35it/s] 25%|██▌       | 147/585 [04:54<02:09,  3.37it/s] 25%|██▌       | 148/585 [04:54<02:08,  3.39it/s] 25%|██▌       | 149/585 [04:54<02:26,  2.97it/s] 26%|██▌       | 150/585 [04:55<02:20,  3.09it/s] 26%|██▌       | 151/585 [04:55<02:16,  3.19it/s] 26%|██▌       | 152/585 [04:55<02:12,  3.26it/s] 26%|██▌       | 153/585 [04:55<02:10,  3.31it/s] 26%|██▋       | 154/585 [04:56<02:08,  3.34it/s] 26%|██▋       | 155/585 [04:56<02:07,  3.37it/s] 27%|██▋       | 156/585 [04:56<02:06,  3.40it/s] 27%|██▋       | 157/585 [04:57<02:05,  3.42it/s] 27%|██▋       | 158/585 [04:57<02:04,  3.43it/s] 27%|██▋       | 159/585 [04:57<02:33,  2.77it/s] 27%|██▋       | 160/585 [04:58<02:24,  2.95it/s] 28%|██▊       | 161/585 [04:58<02:17,  3.08it/s] 28%|██▊       | 162/585 [04:58<02:12,  3.18it/s] 28%|██▊       | 163/585 [04:59<02:09,  3.26it/s] 28%|██▊       | 164/585 [04:59<02:07,  3.31it/s] 28%|██▊       | 165/585 [04:59<02:05,  3.35it/s] 28%|██▊       | 166/585 [04:59<02:04,  3.38it/s] 29%|██▊       | 167/585 [05:00<02:03,  3.40it/s] 29%|██▊       | 168/585 [05:00<02:02,  3.41it/s] 29%|██▉       | 169/585 [05:01<02:48,  2.46it/s] 29%|██▉       | 170/585 [05:01<02:34,  2.69it/s] 29%|██▉       | 171/585 [05:01<02:23,  2.88it/s] 29%|██▉       | 172/585 [05:02<02:16,  3.03it/s] 30%|██▉       | 173/585 [05:02<02:11,  3.15it/s] 30%|██▉       | 174/585 [05:02<02:07,  3.23it/s] 30%|██▉       | 175/585 [05:02<02:04,  3.29it/s] 30%|███       | 176/585 [05:03<02:02,  3.33it/s] 30%|███       | 177/585 [05:03<02:01,  3.37it/s] 30%|███       | 178/585 [05:03<01:59,  3.39it/s] 31%|███       | 179/585 [05:04<02:25,  2.80it/s] 31%|███       | 180/585 [05:04<02:16,  2.96it/s] 31%|███       | 181/585 [05:04<02:10,  3.09it/s] 31%|███       | 182/585 [05:05<02:06,  3.19it/s] 31%|███▏      | 183/585 [05:05<02:03,  3.26it/s] 31%|███▏      | 184/585 [05:05<02:00,  3.31it/s] 32%|███▏      | 185/585 [05:06<01:59,  3.35it/s] 32%|███▏      | 186/585 [05:06<01:58,  3.38it/s] 32%|███▏      | 187/585 [05:06<01:57,  3.40it/s] 32%|███▏      | 188/585 [05:06<01:56,  3.41it/s] 32%|███▏      | 189/585 [05:07<02:08,  3.08it/s] 32%|███▏      | 190/585 [05:07<02:04,  3.18it/s] 33%|███▎      | 191/585 [05:07<02:00,  3.26it/s] 33%|███▎      | 192/585 [05:08<01:58,  3.32it/s] 33%|███▎      | 193/585 [05:08<01:56,  3.35it/s] 33%|███▎      | 194/585 [05:08<01:55,  3.38it/s] 33%|███▎      | 195/585 [05:09<01:54,  3.40it/s] 34%|███▎      | 196/585 [05:09<01:53,  3.42it/s] 34%|███▎      | 197/585 [05:09<01:53,  3.43it/s] 34%|███▍      | 198/585 [05:09<01:52,  3.43it/s] 34%|███▍      | 199/585 [05:10<02:46,  2.32it/s] 34%|███▍      | 200/585 [05:11<02:29,  2.58it/s] 34%|███▍      | 201/585 [05:11<02:17,  2.79it/s] 35%|███▍      | 202/585 [05:11<02:09,  2.95it/s] 35%|███▍      | 203/585 [05:11<02:03,  3.08it/s] 35%|███▍      | 204/585 [05:12<01:59,  3.18it/s] 35%|███▌      | 205/585 [05:12<01:56,  3.25it/s] 35%|███▌      | 206/585 [05:12<01:54,  3.31it/s] 35%|███▌      | 207/585 [05:13<01:53,  3.34it/s] 36%|███▌      | 208/585 [05:13<02:17,  2.75it/s] 36%|███▌      | 209/585 [05:13<02:08,  2.93it/s] 36%|███▌      | 210/585 [05:14<02:02,  3.06it/s] 36%|███▌      | 211/585 [05:14<01:58,  3.17it/s] 36%|███▌      | 212/585 [05:14<01:54,  3.25it/s] 36%|███▋      | 213/585 [05:15<01:52,  3.30it/s] 37%|███▋      | 214/585 [05:15<01:51,  3.34it/s] 37%|███▋      | 215/585 [05:15<01:49,  3.37it/s] 37%|███▋      | 216/585 [05:15<01:48,  3.39it/s] 37%|███▋      | 217/585 [05:16<01:48,  3.41it/s] 37%|███▋      | 218/585 [05:16<02:42,  2.25it/s] 37%|███▋      | 219/585 [05:17<02:25,  2.51it/s] 38%|███▊      | 220/585 [05:18<03:20,  1.82it/s] 38%|███▊      | 221/585 [05:18<02:52,  2.12it/s] 38%|███▊      | 222/585 [05:18<02:31,  2.39it/s] 38%|███▊      | 223/585 [05:19<02:17,  2.63it/s] 38%|███▊      | 224/585 [05:19<02:07,  2.83it/s] 38%|███▊      | 225/585 [05:19<02:00,  2.99it/s] 39%|███▊      | 226/585 [05:19<01:55,  3.11it/s] 39%|███▉      | 227/585 [05:20<01:51,  3.21it/s] 39%|███▉      | 228/585 [05:20<01:49,  3.27it/s] 39%|███▉      | 229/585 [05:20<02:05,  2.83it/s] 39%|███▉      | 230/585 [05:21<01:58,  2.98it/s] 39%|███▉      | 231/585 [05:21<01:53,  3.11it/s] 40%|███▉      | 232/585 [05:21<01:50,  3.20it/s] 40%|███▉      | 233/585 [05:22<01:47,  3.27it/s] 40%|████      | 234/585 [05:22<01:45,  3.32it/s][INFO|trainer.py:2140] 2023-08-28 23:12:50,111 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:12:50,112 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 23:12:50,112 >>   Batch size = 8
{'eval_loss': 1.0482137203216553, 'eval_runtime': 12.2948, 'eval_samples_per_second': 284.023, 'eval_steps_per_second': 35.544, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  2%|▏         | 7/437 [00:00<00:07, 58.28it/s][A
  3%|▎         | 13/437 [00:00<00:08, 50.31it/s][A
  4%|▍         | 19/437 [00:00<00:08, 47.79it/s][A
  5%|▌         | 24/437 [00:00<00:08, 46.78it/s][A
  7%|▋         | 29/437 [00:00<00:08, 46.21it/s][A
  8%|▊         | 34/437 [00:00<00:08, 45.83it/s][A
  9%|▉         | 39/437 [00:01<00:08, 45.54it/s][A
 10%|█         | 44/437 [00:01<00:22, 17.54it/s][A
 11%|█         | 49/437 [00:01<00:17, 21.58it/s][A
 12%|█▏        | 54/437 [00:01<00:14, 25.66it/s][A
 14%|█▎        | 59/437 [00:01<00:12, 29.51it/s][A
 15%|█▍        | 64/437 [00:01<00:11, 32.96it/s][A
 16%|█▌        | 69/437 [00:02<00:10, 35.84it/s][A
 17%|█▋        | 74/437 [00:02<00:09, 38.22it/s][A
 18%|█▊        | 79/437 [00:02<00:08, 39.92it/s][A
 19%|█▉        | 84/437 [00:02<00:08, 40.89it/s][A
 20%|██        | 89/437 [00:02<00:08, 41.69it/s][A
 22%|██▏       | 94/437 [00:02<00:08, 42.36it/s][A
 23%|██▎       | 99/437 [00:02<00:07, 43.02it/s][A
 24%|██▍       | 104/437 [00:02<00:07, 43.66it/s][A
 25%|██▍       | 109/437 [00:02<00:07, 44.06it/s][A
 26%|██▌       | 114/437 [00:03<00:07, 44.39it/s][A
 27%|██▋       | 119/437 [00:03<00:07, 44.56it/s][A
 28%|██▊       | 124/437 [00:03<00:07, 44.36it/s][A
 30%|██▉       | 129/437 [00:03<00:06, 44.19it/s][A
 31%|███       | 134/437 [00:03<00:06, 44.14it/s][A
 32%|███▏      | 139/437 [00:03<00:06, 44.09it/s][A
 33%|███▎      | 144/437 [00:03<00:06, 44.33it/s][A
 34%|███▍      | 149/437 [00:04<00:06, 44.46it/s][A
 35%|███▌      | 154/437 [00:04<00:11, 23.99it/s][A
 36%|███▋      | 159/437 [00:04<00:09, 27.93it/s][A
 38%|███▊      | 164/437 [00:04<00:08, 31.50it/s][A
 39%|███▊      | 169/437 [00:04<00:07, 34.64it/s][A
 40%|███▉      | 174/437 [00:04<00:07, 37.19it/s][A
 41%|████      | 179/437 [00:04<00:06, 39.29it/s][A
 42%|████▏     | 184/437 [00:04<00:06, 40.83it/s][A
 43%|████▎     | 189/437 [00:05<00:05, 41.89it/s][A
 44%|████▍     | 194/437 [00:05<00:05, 42.30it/s][A
 46%|████▌     | 199/437 [00:05<00:05, 42.67it/s][A
 47%|████▋     | 204/437 [00:05<00:05, 43.09it/s][A
 48%|████▊     | 209/437 [00:05<00:05, 43.61it/s][A
 49%|████▉     | 214/437 [00:05<00:05, 43.94it/s][A
 50%|█████     | 219/437 [00:05<00:04, 44.27it/s][A
 51%|█████▏    | 224/437 [00:05<00:04, 44.52it/s][A
 52%|█████▏    | 229/437 [00:05<00:04, 44.60it/s][A
 54%|█████▎    | 234/437 [00:06<00:04, 44.45it/s][A
 55%|█████▍    | 239/437 [00:06<00:04, 44.24it/s][A
 56%|█████▌    | 244/437 [00:06<00:04, 44.18it/s][A
 57%|█████▋    | 249/437 [00:06<00:04, 44.22it/s][A
 58%|█████▊    | 254/437 [00:06<00:04, 44.26it/s][A
 59%|█████▉    | 259/437 [00:06<00:04, 44.45it/s][A
 60%|██████    | 264/437 [00:06<00:03, 44.56it/s][A
 62%|██████▏   | 269/437 [00:07<00:03, 44.65it/s][A
 63%|██████▎   | 274/437 [00:07<00:06, 24.96it/s][A
 64%|██████▍   | 279/437 [00:07<00:05, 28.81it/s][A
 65%|██████▍   | 284/437 [00:07<00:04, 32.31it/s][A
 66%|██████▌   | 289/437 [00:07<00:04, 35.33it/s][A
 67%|██████▋   | 294/437 [00:07<00:03, 37.79it/s][A
 68%|██████▊   | 299/437 [00:07<00:03, 39.70it/s][A
 70%|██████▉   | 304/437 [00:07<00:03, 41.19it/s][A
 71%|███████   | 309/437 [00:08<00:03, 42.03it/s][A
 72%|███████▏  | 314/437 [00:08<00:02, 42.36it/s][A
 73%|███████▎  | 319/437 [00:08<00:02, 42.61it/s][A
 74%|███████▍  | 324/437 [00:08<00:02, 43.13it/s][A
 75%|███████▌  | 329/437 [00:08<00:02, 43.61it/s][A
 76%|███████▋  | 334/437 [00:08<00:02, 44.00it/s][A
 78%|███████▊  | 339/437 [00:08<00:02, 44.31it/s][A
 79%|███████▊  | 344/437 [00:08<00:02, 44.55it/s][A
 80%|███████▉  | 349/437 [00:08<00:01, 44.68it/s][A
 81%|████████  | 354/437 [00:09<00:01, 44.50it/s][A
 82%|████████▏ | 359/437 [00:09<00:01, 44.13it/s][A
 83%|████████▎ | 364/437 [00:09<00:01, 43.99it/s][A
 84%|████████▍ | 369/437 [00:09<00:01, 44.05it/s][A
 86%|████████▌ | 374/437 [00:09<00:01, 44.27it/s][A
 87%|████████▋ | 379/437 [00:09<00:01, 44.46it/s][A
 88%|████████▊ | 384/437 [00:09<00:01, 44.61it/s][A
 89%|████████▉ | 389/437 [00:09<00:01, 44.74it/s][A
 90%|█████████ | 394/437 [00:10<00:00, 44.72it/s][A
 91%|█████████▏| 399/437 [00:10<00:01, 21.11it/s][A
 92%|█████████▏| 404/437 [00:10<00:01, 25.10it/s][A
 94%|█████████▎| 409/437 [00:10<00:00, 28.90it/s][A
 95%|█████████▍| 414/437 [00:10<00:00, 32.38it/s][A
 96%|█████████▌| 419/437 [00:10<00:00, 35.39it/s][A
 97%|█████████▋| 424/437 [00:11<00:00, 37.80it/s][A
 98%|█████████▊| 429/437 [00:11<00:00, 39.75it/s][A
 99%|█████████▉| 434/437 [00:11<00:00, 41.06it/s][A                                                 
                                                 [A 40%|████      | 234/585 [05:34<01:45,  3.32it/s]
100%|██████████| 437/437 [00:11<00:00, 41.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:13:02,435 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 23:13:04,587 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:14:10,531 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:14:12,571 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:14:12,857 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [08:59<6:20:16, 65.19s/it] 40%|████      | 236/585 [08:59<4:26:11, 45.76s/it] 41%|████      | 237/585 [08:59<3:06:18, 32.12s/it] 41%|████      | 238/585 [09:00<2:10:32, 22.57s/it] 41%|████      | 239/585 [09:00<1:31:37, 15.89s/it] 41%|████      | 240/585 [09:00<1:04:27, 11.21s/it] 41%|████      | 241/585 [09:00<45:29,  7.93s/it]   41%|████▏     | 242/585 [09:01<32:14,  5.64s/it] 42%|████▏     | 243/585 [09:01<22:59,  4.03s/it] 42%|████▏     | 244/585 [09:01<16:32,  2.91s/it] 42%|████▏     | 245/585 [09:02<12:01,  2.12s/it] 42%|████▏     | 246/585 [09:02<09:03,  1.60s/it] 42%|████▏     | 247/585 [09:02<06:48,  1.21s/it] 42%|████▏     | 248/585 [09:03<05:14,  1.07it/s] 43%|████▎     | 249/585 [09:03<04:08,  1.35it/s] 43%|████▎     | 250/585 [09:03<03:21,  1.66it/s] 43%|████▎     | 251/585 [09:03<02:49,  1.97it/s] 43%|████▎     | 252/585 [09:04<02:26,  2.27it/s] 43%|████▎     | 253/585 [09:04<02:11,  2.53it/s] 43%|████▎     | 254/585 [09:04<01:59,  2.76it/s] 44%|████▎     | 255/585 [09:05<01:52,  2.95it/s] 44%|████▍     | 256/585 [09:05<01:46,  3.09it/s] 44%|████▍     | 257/585 [09:05<02:06,  2.59it/s] 44%|████▍     | 258/585 [09:06<01:56,  2.81it/s] 44%|████▍     | 259/585 [09:06<01:49,  2.99it/s] 44%|████▍     | 260/585 [09:06<01:44,  3.12it/s] 45%|████▍     | 261/585 [09:07<01:40,  3.23it/s] 45%|████▍     | 262/585 [09:07<01:37,  3.30it/s] 45%|████▍     | 263/585 [09:07<01:35,  3.36it/s] 45%|████▌     | 264/585 [09:07<01:34,  3.39it/s] 45%|████▌     | 265/585 [09:08<01:33,  3.42it/s] 45%|████▌     | 266/585 [09:08<01:32,  3.44it/s] 46%|████▌     | 267/585 [09:08<01:50,  2.87it/s] 46%|████▌     | 268/585 [09:09<01:44,  3.04it/s] 46%|████▌     | 269/585 [09:09<01:39,  3.16it/s] 46%|████▌     | 270/585 [09:09<01:36,  3.25it/s] 46%|████▋     | 271/585 [09:10<01:34,  3.32it/s] 46%|████▋     | 272/585 [09:10<01:32,  3.37it/s] 47%|████▋     | 273/585 [09:10<01:31,  3.41it/s] 47%|████▋     | 274/585 [09:10<01:30,  3.43it/s] 47%|████▋     | 275/585 [09:11<01:29,  3.45it/s] 47%|████▋     | 276/585 [09:11<01:29,  3.47it/s] 47%|████▋     | 277/585 [09:11<01:37,  3.15it/s] 48%|████▊     | 278/585 [09:12<01:34,  3.24it/s] 48%|████▊     | 279/585 [09:12<01:32,  3.31it/s] 48%|████▊     | 280/585 [09:12<01:30,  3.37it/s] 48%|████▊     | 281/585 [09:13<01:29,  3.41it/s] 48%|████▊     | 282/585 [09:13<01:28,  3.43it/s] 48%|████▊     | 283/585 [09:13<01:27,  3.45it/s] 49%|████▊     | 284/585 [09:13<01:26,  3.46it/s] 49%|████▊     | 285/585 [09:14<01:26,  3.47it/s] 49%|████▉     | 286/585 [09:14<01:25,  3.48it/s] 49%|████▉     | 287/585 [09:14<01:25,  3.48it/s] 49%|████▉     | 288/585 [09:15<01:46,  2.78it/s] 49%|████▉     | 289/585 [09:15<01:39,  2.96it/s] 50%|████▉     | 290/585 [09:15<01:35,  3.10it/s] 50%|████▉     | 291/585 [09:16<01:31,  3.21it/s] 50%|████▉     | 292/585 [09:16<01:29,  3.29it/s] 50%|█████     | 293/585 [09:16<01:27,  3.35it/s] 50%|█████     | 294/585 [09:16<01:25,  3.39it/s] 50%|█████     | 295/585 [09:17<01:24,  3.42it/s] 51%|█████     | 296/585 [09:17<01:24,  3.44it/s] 51%|█████     | 297/585 [09:17<01:23,  3.46it/s] 51%|█████     | 298/585 [09:18<01:46,  2.71it/s] 51%|█████     | 299/585 [09:18<01:38,  2.90it/s] 51%|█████▏    | 300/585 [09:18<01:33,  3.05it/s] 51%|█████▏    | 301/585 [09:19<01:29,  3.17it/s] 52%|█████▏    | 302/585 [09:19<01:26,  3.26it/s] 52%|█████▏    | 303/585 [09:19<01:24,  3.33it/s] 52%|█████▏    | 304/585 [09:20<01:23,  3.38it/s] 52%|█████▏    | 305/585 [09:20<01:22,  3.41it/s] 52%|█████▏    | 306/585 [09:20<01:21,  3.44it/s] 52%|█████▏    | 307/585 [09:20<01:20,  3.46it/s] 53%|█████▎    | 308/585 [09:21<01:33,  2.95it/s] 53%|█████▎    | 309/585 [09:21<01:29,  3.09it/s] 53%|█████▎    | 310/585 [09:22<01:26,  3.19it/s] 53%|█████▎    | 311/585 [09:22<01:24,  3.26it/s] 53%|█████▎    | 312/585 [09:22<01:22,  3.31it/s] 54%|█████▎    | 313/585 [09:22<01:21,  3.34it/s] 54%|█████▎    | 314/585 [09:23<01:20,  3.37it/s] 54%|█████▍    | 315/585 [09:23<01:19,  3.40it/s] 54%|█████▍    | 316/585 [09:23<01:28,  3.02it/s] 54%|█████▍    | 317/585 [09:24<01:25,  3.14it/s] 54%|█████▍    | 318/585 [09:24<01:40,  2.67it/s] 55%|█████▍    | 319/585 [09:24<01:32,  2.86it/s] 55%|█████▍    | 320/585 [09:25<01:27,  3.01it/s] 55%|█████▍    | 321/585 [09:25<01:24,  3.13it/s] 55%|█████▌    | 322/585 [09:25<01:21,  3.22it/s] 55%|█████▌    | 323/585 [09:26<01:19,  3.30it/s] 55%|█████▌    | 324/585 [09:26<01:17,  3.36it/s] 56%|█████▌    | 325/585 [09:26<01:16,  3.40it/s] 56%|█████▌    | 326/585 [09:26<01:15,  3.43it/s] 56%|█████▌    | 327/585 [09:27<01:14,  3.45it/s] 56%|█████▌    | 328/585 [09:27<01:33,  2.76it/s] 56%|█████▌    | 329/585 [09:28<01:26,  2.94it/s] 56%|█████▋    | 330/585 [09:28<01:22,  3.09it/s] 57%|█████▋    | 331/585 [09:28<01:19,  3.20it/s] 57%|█████▋    | 332/585 [09:28<01:16,  3.29it/s] 57%|█████▋    | 333/585 [09:29<01:15,  3.35it/s] 57%|█████▋    | 334/585 [09:29<01:13,  3.40it/s] 57%|█████▋    | 335/585 [09:29<01:12,  3.43it/s] 57%|█████▋    | 336/585 [09:30<01:12,  3.45it/s] 58%|█████▊    | 337/585 [09:30<01:11,  3.46it/s] 58%|█████▊    | 338/585 [09:30<01:27,  2.82it/s] 58%|█████▊    | 339/585 [09:31<01:22,  3.00it/s] 58%|█████▊    | 340/585 [09:31<01:18,  3.13it/s] 58%|█████▊    | 341/585 [09:31<01:15,  3.23it/s] 58%|█████▊    | 342/585 [09:32<01:13,  3.31it/s] 59%|█████▊    | 343/585 [09:32<01:11,  3.36it/s] 59%|█████▉    | 344/585 [09:32<01:10,  3.40it/s] 59%|█████▉    | 345/585 [09:32<01:09,  3.43it/s] 59%|█████▉    | 346/585 [09:33<01:09,  3.45it/s] 59%|█████▉    | 347/585 [09:33<01:08,  3.46it/s] 59%|█████▉    | 348/585 [09:34<01:28,  2.68it/s] 60%|█████▉    | 349/585 [09:34<01:21,  2.89it/s] 60%|█████▉    | 350/585 [09:34<01:17,  3.05it/s] 60%|██████    | 351/585 [09:34<01:13,  3.17it/s][INFO|trainer.py:2140] 2023-08-28 23:17:02,558 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:17:02,559 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 23:17:02,559 >>   Batch size = 8
{'eval_loss': 1.0567421913146973, 'eval_runtime': 11.6368, 'eval_samples_per_second': 300.083, 'eval_steps_per_second': 37.553, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.19it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.72it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.89it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.95it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.38it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.11it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.91it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.73it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.78it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.88it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.98it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.86it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.73it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.76it/s][A
 18%|█▊        | 77/437 [00:01<00:13, 26.36it/s][A
 19%|█▉        | 82/437 [00:02<00:11, 30.14it/s][A
 20%|█▉        | 87/437 [00:02<00:10, 33.49it/s][A
 21%|██        | 92/437 [00:02<00:09, 36.36it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 38.66it/s][A
 23%|██▎       | 102/437 [00:02<00:08, 40.46it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 41.82it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 42.61it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 42.78it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 42.97it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 43.53it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.94it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.34it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.59it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.83it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.99it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.76it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.43it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.18it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.18it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.46it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.60it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.86it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.99it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 45.00it/s][A
 46%|████▌     | 202/437 [00:04<00:07, 32.74it/s][A
 47%|████▋     | 207/437 [00:04<00:06, 35.83it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 38.24it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 40.09it/s][A
 51%|█████     | 222/437 [00:05<00:05, 41.51it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 42.61it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.37it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.72it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.59it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.44it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 43.74it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 44.19it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.42it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.67it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.88it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.97it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.83it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.44it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.22it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.23it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.33it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.58it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.76it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.88it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.97it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.76it/s][A
 76%|███████▌  | 332/437 [00:08<00:04, 25.40it/s][A
 77%|███████▋  | 337/437 [00:08<00:03, 29.18it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 32.76it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 35.72it/s][A
 81%|████████  | 352/437 [00:08<00:02, 38.19it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 40.02it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 41.47it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 42.37it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 42.67it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 42.85it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 43.35it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.77it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.27it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.47it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.74it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.84it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.50it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.28it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 44.22it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.18it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.48it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.60it/s][A                                                 
                                                 [A 60%|██████    | 351/585 [09:45<01:13,  3.17it/s]
100%|██████████| 437/437 [00:10<00:00, 44.60it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:17:13,235 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-28 23:17:14,881 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:18:11,829 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:18:15,405 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:18:15,960 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [12:46<3:43:53, 57.65s/it] 60%|██████    | 353/585 [12:46<2:36:36, 40.50s/it] 61%|██████    | 354/585 [12:47<1:49:29, 28.44s/it] 61%|██████    | 355/585 [12:47<1:16:38, 19.99s/it] 61%|██████    | 356/585 [12:47<53:45, 14.08s/it]   61%|██████    | 357/585 [12:47<37:47,  9.95s/it] 61%|██████    | 358/585 [12:48<26:40,  7.05s/it] 61%|██████▏   | 359/585 [12:48<18:54,  5.02s/it] 62%|██████▏   | 360/585 [12:48<13:30,  3.60s/it] 62%|██████▏   | 361/585 [12:49<09:44,  2.61s/it] 62%|██████▏   | 362/585 [12:49<07:06,  1.91s/it] 62%|██████▏   | 363/585 [12:50<05:37,  1.52s/it] 62%|██████▏   | 364/585 [12:50<04:14,  1.15s/it] 62%|██████▏   | 365/585 [12:50<03:16,  1.12it/s] 63%|██████▎   | 366/585 [12:50<02:36,  1.40it/s] 63%|██████▎   | 367/585 [12:51<02:07,  1.71it/s] 63%|██████▎   | 368/585 [12:51<01:47,  2.01it/s] 63%|██████▎   | 369/585 [12:51<01:34,  2.30it/s] 63%|██████▎   | 370/585 [12:52<01:24,  2.55it/s] 63%|██████▎   | 371/585 [12:52<01:17,  2.76it/s] 64%|██████▎   | 372/585 [12:52<01:12,  2.94it/s] 64%|██████▍   | 373/585 [12:53<01:35,  2.23it/s] 64%|██████▍   | 374/585 [12:53<01:24,  2.49it/s] 64%|██████▍   | 375/585 [12:53<01:17,  2.71it/s] 64%|██████▍   | 376/585 [12:54<01:12,  2.90it/s] 64%|██████▍   | 377/585 [12:54<01:08,  3.04it/s] 65%|██████▍   | 378/585 [12:54<01:05,  3.15it/s] 65%|██████▍   | 379/585 [12:55<01:03,  3.24it/s] 65%|██████▍   | 380/585 [12:55<01:01,  3.31it/s] 65%|██████▌   | 381/585 [12:55<01:00,  3.36it/s] 65%|██████▌   | 382/585 [12:56<01:07,  2.99it/s] 65%|██████▌   | 383/585 [12:56<01:04,  3.13it/s] 66%|██████▌   | 384/585 [12:56<01:02,  3.23it/s] 66%|██████▌   | 385/585 [12:56<01:00,  3.30it/s] 66%|██████▌   | 386/585 [12:57<00:59,  3.36it/s] 66%|██████▌   | 387/585 [12:57<00:58,  3.40it/s] 66%|██████▋   | 388/585 [12:57<00:57,  3.43it/s] 66%|██████▋   | 389/585 [12:58<00:56,  3.45it/s] 67%|██████▋   | 390/585 [12:58<00:56,  3.46it/s] 67%|██████▋   | 391/585 [12:58<00:55,  3.47it/s] 67%|██████▋   | 392/585 [12:58<00:55,  3.48it/s] 67%|██████▋   | 393/585 [12:59<01:03,  3.03it/s] 67%|██████▋   | 394/585 [12:59<01:00,  3.16it/s] 68%|██████▊   | 395/585 [12:59<00:58,  3.24it/s] 68%|██████▊   | 396/585 [13:00<00:57,  3.31it/s] 68%|██████▊   | 397/585 [13:00<00:55,  3.36it/s] 68%|██████▊   | 398/585 [13:00<00:55,  3.40it/s] 68%|██████▊   | 399/585 [13:01<00:54,  3.43it/s] 68%|██████▊   | 400/585 [13:01<00:53,  3.45it/s] 69%|██████▊   | 401/585 [13:01<00:53,  3.46it/s] 69%|██████▊   | 402/585 [13:01<00:52,  3.47it/s] 69%|██████▉   | 403/585 [13:02<01:00,  3.01it/s] 69%|██████▉   | 404/585 [13:02<00:57,  3.14it/s] 69%|██████▉   | 405/585 [13:02<00:55,  3.24it/s] 69%|██████▉   | 406/585 [13:03<00:54,  3.31it/s] 70%|██████▉   | 407/585 [13:03<00:52,  3.36it/s] 70%|██████▉   | 408/585 [13:03<00:51,  3.40it/s] 70%|██████▉   | 409/585 [13:04<00:51,  3.44it/s] 70%|███████   | 410/585 [13:04<00:50,  3.46it/s] 70%|███████   | 411/585 [13:04<00:50,  3.47it/s] 70%|███████   | 412/585 [13:04<00:49,  3.48it/s] 71%|███████   | 413/585 [13:05<01:04,  2.66it/s] 71%|███████   | 414/585 [13:05<00:59,  2.87it/s] 71%|███████   | 415/585 [13:06<00:56,  3.04it/s] 71%|███████   | 416/585 [13:06<00:53,  3.16it/s] 71%|███████▏  | 417/585 [13:06<00:51,  3.26it/s] 71%|███████▏  | 418/585 [13:06<00:50,  3.33it/s] 72%|███████▏  | 419/585 [13:07<00:49,  3.38it/s] 72%|███████▏  | 420/585 [13:07<00:48,  3.41it/s] 72%|███████▏  | 421/585 [13:07<00:47,  3.44it/s] 72%|███████▏  | 422/585 [13:08<00:47,  3.46it/s] 72%|███████▏  | 423/585 [13:08<01:04,  2.50it/s] 72%|███████▏  | 424/585 [13:09<00:58,  2.73it/s] 73%|███████▎  | 425/585 [13:09<00:54,  2.93it/s] 73%|███████▎  | 426/585 [13:09<00:51,  3.08it/s] 73%|███████▎  | 427/585 [13:09<00:49,  3.19it/s] 73%|███████▎  | 428/585 [13:10<00:47,  3.28it/s] 73%|███████▎  | 429/585 [13:10<00:46,  3.34it/s] 74%|███████▎  | 430/585 [13:10<00:45,  3.39it/s] 74%|███████▎  | 431/585 [13:11<00:44,  3.42it/s] 74%|███████▍  | 432/585 [13:11<00:44,  3.45it/s] 74%|███████▍  | 433/585 [13:11<00:54,  2.78it/s] 74%|███████▍  | 434/585 [13:12<00:50,  2.96it/s] 74%|███████▍  | 435/585 [13:12<00:48,  3.11it/s] 75%|███████▍  | 436/585 [13:12<00:46,  3.22it/s] 75%|███████▍  | 437/585 [13:12<00:44,  3.30it/s] 75%|███████▍  | 438/585 [13:13<00:43,  3.36it/s] 75%|███████▌  | 439/585 [13:13<00:42,  3.40it/s] 75%|███████▌  | 440/585 [13:13<00:42,  3.43it/s] 75%|███████▌  | 441/585 [13:14<00:41,  3.45it/s] 76%|███████▌  | 442/585 [13:14<00:41,  3.46it/s] 76%|███████▌  | 443/585 [13:14<00:52,  2.68it/s] 76%|███████▌  | 444/585 [13:15<00:48,  2.88it/s] 76%|███████▌  | 445/585 [13:15<00:46,  3.04it/s] 76%|███████▌  | 446/585 [13:15<00:43,  3.17it/s] 76%|███████▋  | 447/585 [13:16<00:42,  3.26it/s] 77%|███████▋  | 448/585 [13:16<00:41,  3.32it/s] 77%|███████▋  | 449/585 [13:16<00:40,  3.38it/s] 77%|███████▋  | 450/585 [13:16<00:39,  3.41it/s] 77%|███████▋  | 451/585 [13:17<00:38,  3.44it/s] 77%|███████▋  | 452/585 [13:17<00:38,  3.46it/s] 77%|███████▋  | 453/585 [13:18<00:45,  2.89it/s] 78%|███████▊  | 454/585 [13:18<00:42,  3.05it/s] 78%|███████▊  | 455/585 [13:18<00:40,  3.17it/s] 78%|███████▊  | 456/585 [13:18<00:39,  3.26it/s] 78%|███████▊  | 457/585 [13:19<00:38,  3.33it/s] 78%|███████▊  | 458/585 [13:19<00:37,  3.38it/s] 78%|███████▊  | 459/585 [13:19<00:40,  3.09it/s] 79%|███████▊  | 460/585 [13:20<00:39,  3.20it/s] 79%|███████▉  | 461/585 [13:20<00:37,  3.29it/s] 79%|███████▉  | 462/585 [13:20<00:36,  3.35it/s] 79%|███████▉  | 463/585 [13:21<00:35,  3.39it/s] 79%|███████▉  | 464/585 [13:21<00:35,  3.42it/s] 79%|███████▉  | 465/585 [13:21<00:34,  3.45it/s] 80%|███████▉  | 466/585 [13:21<00:34,  3.46it/s] 80%|███████▉  | 467/585 [13:22<00:33,  3.48it/s] 80%|████████  | 468/585 [13:22<00:33,  3.48it/s][INFO|trainer.py:2140] 2023-08-28 23:20:50,293 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:20:50,293 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 23:20:50,293 >>   Batch size = 8
{'eval_loss': 1.0732208490371704, 'eval_runtime': 10.4567, 'eval_samples_per_second': 333.949, 'eval_steps_per_second': 41.791, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.12it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.33it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.63it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.76it/s][A
  6%|▌         | 27/437 [00:00<00:08, 46.20it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.91it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.45it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.67it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.31it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.19it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.45it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.68it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.78it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.94it/s][A
 18%|█▊        | 77/437 [00:01<00:13, 27.10it/s][A
 19%|█▉        | 82/437 [00:02<00:11, 31.22it/s][A
 20%|█▉        | 87/437 [00:02<00:10, 34.42it/s][A
 21%|██        | 92/437 [00:02<00:09, 37.07it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 39.19it/s][A
 23%|██▎       | 102/437 [00:02<00:08, 40.85it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 42.12it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.06it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.32it/s][A
 28%|██▊       | 122/437 [00:03<00:09, 34.16it/s][A
 29%|██▉       | 127/437 [00:03<00:08, 36.91it/s][A
 30%|███       | 132/437 [00:03<00:07, 39.02it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 40.70it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 41.95it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 42.92it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.56it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.86it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.58it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 43.58it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 43.69it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.06it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.31it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.66it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.78it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.90it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.77it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.31it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 44.13it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.11it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.25it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.51it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.66it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.76it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.86it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 44.72it/s][A
 58%|█████▊    | 252/437 [00:06<00:07, 23.90it/s][A
 59%|█████▉    | 257/437 [00:06<00:06, 27.83it/s][A
 60%|█████▉    | 262/437 [00:06<00:05, 31.42it/s][A
 61%|██████    | 267/437 [00:06<00:04, 34.62it/s][A
 62%|██████▏   | 272/437 [00:06<00:04, 37.24it/s][A
 63%|██████▎   | 277/437 [00:06<00:04, 39.33it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 40.84it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 41.86it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 42.19it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 42.62it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 43.04it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.44it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.85it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.25it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.53it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.75it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 44.68it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 44.36it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 44.21it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.18it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.24it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.51it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.62it/s][A
 84%|████████▍ | 367/437 [00:09<00:01, 44.88it/s][A
 85%|████████▌ | 372/437 [00:09<00:02, 23.92it/s][A
 86%|████████▋ | 377/437 [00:09<00:02, 27.84it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 31.43it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 34.56it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 37.24it/s][A
 91%|█████████ | 397/437 [00:09<00:01, 39.29it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 40.98it/s][A
 93%|█████████▎| 407/437 [00:10<00:00, 41.99it/s][A
 94%|█████████▍| 412/437 [00:10<00:00, 42.27it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 42.52it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 43.02it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 43.47it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 43.96it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.35it/s][A                                                 
                                                 [A 80%|████████  | 468/585 [13:33<00:33,  3.48it/s]
100%|██████████| 437/437 [00:10<00:00, 44.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:21:01,366 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-28 23:21:03,238 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:22:09,962 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:22:13,915 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:22:15,127 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [16:55<2:03:48, 64.04s/it] 80%|████████  | 470/585 [16:55<1:26:13, 44.99s/it] 81%|████████  | 471/585 [16:56<1:00:00, 31.58s/it] 81%|████████  | 472/585 [16:56<41:47, 22.19s/it]   81%|████████  | 473/585 [16:56<29:09, 15.62s/it] 81%|████████  | 474/585 [16:56<20:23, 11.02s/it] 81%|████████  | 475/585 [16:57<14:18,  7.80s/it] 81%|████████▏ | 476/585 [16:57<10:04,  5.55s/it] 82%|████████▏ | 477/585 [16:57<07:08,  3.97s/it] 82%|████████▏ | 478/585 [16:58<05:06,  2.87s/it] 82%|████████▏ | 479/585 [16:58<03:42,  2.09s/it] 82%|████████▏ | 480/585 [16:58<02:47,  1.60s/it] 82%|████████▏ | 481/585 [16:59<02:05,  1.21s/it] 82%|████████▏ | 482/585 [16:59<01:35,  1.07it/s] 83%|████████▎ | 483/585 [16:59<01:15,  1.35it/s] 83%|████████▎ | 484/585 [16:59<01:01,  1.65it/s] 83%|████████▎ | 485/585 [17:00<00:51,  1.96it/s] 83%|████████▎ | 486/585 [17:00<00:44,  2.25it/s] 83%|████████▎ | 487/585 [17:00<00:39,  2.51it/s] 83%|████████▎ | 488/585 [17:01<00:35,  2.73it/s] 84%|████████▎ | 489/585 [17:01<00:33,  2.91it/s] 84%|████████▍ | 490/585 [17:02<00:39,  2.43it/s] 84%|████████▍ | 491/585 [17:02<00:35,  2.67it/s] 84%|████████▍ | 492/585 [17:02<00:32,  2.86it/s] 84%|████████▍ | 493/585 [17:02<00:30,  3.01it/s] 84%|████████▍ | 494/585 [17:03<00:29,  3.13it/s] 85%|████████▍ | 495/585 [17:03<00:27,  3.23it/s] 85%|████████▍ | 496/585 [17:03<00:26,  3.30it/s] 85%|████████▍ | 497/585 [17:04<00:26,  3.36it/s] 85%|████████▌ | 498/585 [17:04<00:25,  3.40it/s] 85%|████████▌ | 499/585 [17:04<00:25,  3.43it/s] 85%|████████▌ | 500/585 [17:05<00:29,  2.85it/s]                                                  85%|████████▌ | 500/585 [17:05<00:29,  2.85it/s] 86%|████████▌ | 501/585 [17:05<00:27,  3.02it/s] 86%|████████▌ | 502/585 [17:05<00:26,  3.15it/s] 86%|████████▌ | 503/585 [17:05<00:25,  3.25it/s] 86%|████████▌ | 504/585 [17:06<00:24,  3.32it/s] 86%|████████▋ | 505/585 [17:06<00:23,  3.38it/s] 86%|████████▋ | 506/585 [17:06<00:23,  3.42it/s] 87%|████████▋ | 507/585 [17:07<00:22,  3.44it/s] 87%|████████▋ | 508/585 [17:07<00:22,  3.46it/s] 87%|████████▋ | 509/585 [17:07<00:21,  3.48it/s] 87%|████████▋ | 510/585 [17:08<00:26,  2.79it/s] 87%|████████▋ | 511/585 [17:08<00:24,  2.98it/s] 88%|████████▊ | 512/585 [17:08<00:23,  3.12it/s] 88%|████████▊ | 513/585 [17:09<00:22,  3.23it/s] 88%|████████▊ | 514/585 [17:09<00:21,  3.31it/s] 88%|████████▊ | 515/585 [17:09<00:20,  3.36it/s] 88%|████████▊ | 516/585 [17:09<00:20,  3.41it/s] 88%|████████▊ | 517/585 [17:10<00:19,  3.44it/s] 89%|████████▊ | 518/585 [17:10<00:19,  3.46it/s] 89%|████████▊ | 519/585 [17:10<00:18,  3.48it/s] 89%|████████▉ | 520/585 [17:11<00:20,  3.11it/s] 89%|████████▉ | 521/585 [17:11<00:19,  3.22it/s] 89%|████████▉ | 522/585 [17:11<00:19,  3.30it/s] 89%|████████▉ | 523/585 [17:12<00:18,  3.36it/s] 90%|████████▉ | 524/585 [17:12<00:17,  3.41it/s] 90%|████████▉ | 525/585 [17:12<00:17,  3.44it/s] 90%|████████▉ | 526/585 [17:12<00:17,  3.46it/s] 90%|█████████ | 527/585 [17:13<00:16,  3.47it/s] 90%|█████████ | 528/585 [17:13<00:16,  3.48it/s] 90%|█████████ | 529/585 [17:13<00:16,  3.49it/s] 91%|█████████ | 530/585 [17:13<00:15,  3.50it/s] 91%|█████████ | 531/585 [17:14<00:19,  2.81it/s] 91%|█████████ | 532/585 [17:14<00:17,  2.98it/s] 91%|█████████ | 533/585 [17:15<00:16,  3.12it/s] 91%|█████████▏| 534/585 [17:15<00:15,  3.23it/s] 91%|█████████▏| 535/585 [17:15<00:15,  3.31it/s] 92%|█████████▏| 536/585 [17:15<00:14,  3.36it/s] 92%|█████████▏| 537/585 [17:16<00:14,  3.41it/s] 92%|█████████▏| 538/585 [17:16<00:13,  3.44it/s] 92%|█████████▏| 539/585 [17:16<00:13,  3.46it/s] 92%|█████████▏| 540/585 [17:17<00:12,  3.47it/s] 92%|█████████▏| 541/585 [17:17<00:17,  2.56it/s] 93%|█████████▎| 542/585 [17:17<00:15,  2.78it/s] 93%|█████████▎| 543/585 [17:18<00:14,  2.97it/s] 93%|█████████▎| 544/585 [17:18<00:13,  3.11it/s] 93%|█████████▎| 545/585 [17:18<00:12,  3.22it/s] 93%|█████████▎| 546/585 [17:19<00:11,  3.30it/s] 94%|█████████▎| 547/585 [17:19<00:11,  3.36it/s] 94%|█████████▎| 548/585 [17:19<00:10,  3.40it/s] 94%|█████████▍| 549/585 [17:19<00:10,  3.43it/s] 94%|█████████▍| 550/585 [17:20<00:10,  3.45it/s] 94%|█████████▍| 551/585 [17:20<00:11,  2.84it/s] 94%|█████████▍| 552/585 [17:21<00:10,  3.01it/s] 95%|█████████▍| 553/585 [17:21<00:10,  3.14it/s] 95%|█████████▍| 554/585 [17:21<00:09,  3.24it/s] 95%|█████████▍| 555/585 [17:21<00:09,  3.31it/s] 95%|█████████▌| 556/585 [17:22<00:08,  3.37it/s] 95%|█████████▌| 557/585 [17:22<00:08,  3.41it/s] 95%|█████████▌| 558/585 [17:22<00:07,  3.44it/s] 96%|█████████▌| 559/585 [17:23<00:07,  3.46it/s] 96%|█████████▌| 560/585 [17:23<00:07,  3.47it/s] 96%|█████████▌| 561/585 [17:23<00:07,  3.28it/s] 96%|█████████▌| 562/585 [17:23<00:06,  3.34it/s] 96%|█████████▌| 563/585 [17:24<00:06,  3.39it/s] 96%|█████████▋| 564/585 [17:24<00:06,  3.42it/s] 97%|█████████▋| 565/585 [17:24<00:05,  3.45it/s] 97%|█████████▋| 566/585 [17:25<00:06,  2.99it/s] 97%|█████████▋| 567/585 [17:25<00:05,  3.13it/s] 97%|█████████▋| 568/585 [17:25<00:05,  3.23it/s] 97%|█████████▋| 569/585 [17:26<00:04,  3.31it/s] 97%|█████████▋| 570/585 [17:26<00:04,  3.36it/s] 98%|█████████▊| 571/585 [17:26<00:04,  3.12it/s] 98%|█████████▊| 572/585 [17:27<00:04,  3.23it/s] 98%|█████████▊| 573/585 [17:27<00:03,  3.30it/s] 98%|█████████▊| 574/585 [17:27<00:03,  3.36it/s] 98%|█████████▊| 575/585 [17:27<00:02,  3.40it/s] 98%|█████████▊| 576/585 [17:28<00:02,  3.43it/s] 99%|█████████▊| 577/585 [17:28<00:02,  3.45it/s] 99%|█████████▉| 578/585 [17:28<00:02,  3.46it/s] 99%|█████████▉| 579/585 [17:29<00:01,  3.48it/s] 99%|█████████▉| 580/585 [17:29<00:01,  3.48it/s] 99%|█████████▉| 581/585 [17:29<00:01,  3.49it/s] 99%|█████████▉| 582/585 [17:30<00:01,  2.94it/s]100%|█████████▉| 583/585 [17:30<00:00,  3.08it/s]100%|█████████▉| 584/585 [17:30<00:00,  3.20it/s]100%|██████████| 585/585 [17:31<00:00,  2.49it/s][INFO|trainer.py:2140] 2023-08-28 23:24:58,945 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:24:58,945 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 23:24:58,945 >>   Batch size = 8
{'eval_loss': 1.0793920755386353, 'eval_runtime': 10.7717, 'eval_samples_per_second': 324.184, 'eval_steps_per_second': 40.569, 'epoch': 4.0}
{'loss': 0.6338, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.37it/s][A
  3%|▎         | 12/437 [00:00<00:08, 49.11it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.38it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.66it/s][A
  6%|▌         | 27/437 [00:00<00:08, 46.02it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.04it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.56it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.39it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.46it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.59it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.81it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.94it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 45.02it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.89it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.58it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.31it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.22it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.38it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.48it/s][A
 23%|██▎       | 102/437 [00:02<00:12, 26.99it/s][A
 24%|██▍       | 107/437 [00:02<00:10, 30.73it/s][A
 26%|██▌       | 112/437 [00:02<00:09, 33.97it/s][A
 27%|██▋       | 117/437 [00:02<00:08, 36.68it/s][A
 28%|██▊       | 122/437 [00:02<00:08, 38.82it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 40.56it/s][A
 30%|███       | 132/437 [00:03<00:07, 41.85it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 42.66it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 42.88it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.09it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 43.44it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.87it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.23it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.54it/s][A
 39%|███▉      | 172/437 [00:04<00:05, 44.75it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.85it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.59it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.36it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.21it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.28it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.32it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.71it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.82it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.88it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.83it/s][A
 52%|█████▏    | 227/437 [00:05<00:06, 34.23it/s][A
 53%|█████▎    | 232/437 [00:05<00:05, 36.86it/s][A
 54%|█████▍    | 237/437 [00:05<00:05, 39.02it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 40.71it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 42.02it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 42.89it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 43.58it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 43.85it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.70it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.53it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.73it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.10it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.47it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.67it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.79it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.87it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.67it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.37it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.06it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.06it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.27it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.49it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.77it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.90it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.89it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.63it/s][A
 82%|████████▏ | 357/437 [00:08<00:02, 26.74it/s][A
 83%|████████▎ | 362/437 [00:08<00:02, 30.46it/s][A
 84%|████████▍ | 367/437 [00:08<00:02, 33.77it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 36.51it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 38.72it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 40.46it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 41.77it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 42.53it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 42.73it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 42.91it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 43.19it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 43.72it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.09it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 44.45it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.68it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.78it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.49it/s][A                                                 
                                                 [A100%|██████████| 585/585 [17:41<00:00,  2.49it/s]
100%|██████████| 437/437 [00:10<00:00, 44.49it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 23:25:09,566 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-28 23:25:11,302 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:25:57,315 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:26:00,016 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:26:00,345 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 23:27:41,764 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 23:27:41,843 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117 (score: 1.0482137203216553).
                                                 100%|██████████| 585/585 [21:22<00:00,  2.49it/s]100%|██████████| 585/585 [21:22<00:00,  2.19s/it]
[INFO|trainer.py:1894] 2023-08-28 23:28:53,856 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 23:28:54,827 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 23:32:34,147 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 23:32:36,313 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 23:32:36,717 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:32:41,718 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:41,827 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:41,827 >>   train_loss               =     0.6301
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:41,827 >>   train_runtime            = 0:21:22.86
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:41,827 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:41,827 >>   train_samples_per_second =     29.232
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:41,827 >>   train_steps_per_second   =      0.456
{'eval_loss': 1.0856763124465942, 'eval_runtime': 10.3817, 'eval_samples_per_second': 336.361, 'eval_steps_per_second': 42.093, 'epoch': 5.0}
{'train_runtime': 1282.8614, 'train_samples_per_second': 29.232, 'train_steps_per_second': 0.456, 'train_loss': 0.6301053886739617, 'epoch': 5.0}
08/28/2023 23:32:43 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 23:32:43,611 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 23:32:43,612 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-28 23:32:43,612 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:08, 50.91it/s]  3%|▎         | 12/437 [00:00<00:08, 48.24it/s]  4%|▍         | 17/437 [00:00<00:08, 47.47it/s]  5%|▌         | 22/437 [00:00<00:08, 46.89it/s]  6%|▌         | 27/437 [00:00<00:08, 46.55it/s]  7%|▋         | 32/437 [00:00<00:08, 46.41it/s]  8%|▊         | 37/437 [00:00<00:08, 46.37it/s] 10%|▉         | 42/437 [00:00<00:08, 45.97it/s] 11%|█         | 47/437 [00:01<00:08, 45.49it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.83it/s] 13%|█▎        | 57/437 [00:01<00:14, 26.78it/s] 14%|█▍        | 62/437 [00:01<00:12, 30.61it/s] 15%|█▌        | 67/437 [00:01<00:10, 34.02it/s] 16%|█▋        | 72/437 [00:01<00:09, 36.82it/s] 18%|█▊        | 77/437 [00:01<00:09, 39.11it/s] 19%|█▉        | 82/437 [00:02<00:08, 40.84it/s] 20%|█▉        | 87/437 [00:02<00:08, 42.19it/s] 21%|██        | 92/437 [00:02<00:08, 43.01it/s] 22%|██▏       | 97/437 [00:02<00:07, 43.27it/s] 23%|██▎       | 102/437 [00:02<00:07, 43.36it/s] 24%|██▍       | 107/437 [00:02<00:07, 43.82it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.25it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.61it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.81it/s] 29%|██▉       | 127/437 [00:03<00:06, 45.03it/s] 30%|███       | 132/437 [00:03<00:06, 45.25it/s] 31%|███▏      | 137/437 [00:03<00:06, 45.08it/s] 32%|███▏      | 142/437 [00:03<00:06, 44.80it/s] 34%|███▎      | 147/437 [00:03<00:06, 44.51it/s] 35%|███▍      | 152/437 [00:03<00:06, 44.59it/s] 36%|███▌      | 157/437 [00:03<00:06, 44.71it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.89it/s] 38%|███▊      | 167/437 [00:03<00:05, 45.04it/s] 39%|███▉      | 172/437 [00:04<00:05, 45.19it/s] 41%|████      | 177/437 [00:04<00:05, 45.25it/s] 42%|████▏     | 182/437 [00:04<00:09, 27.48it/s] 43%|████▎     | 187/437 [00:04<00:08, 31.20it/s] 44%|████▍     | 192/437 [00:04<00:07, 34.47it/s] 45%|████▌     | 197/437 [00:04<00:06, 37.13it/s] 46%|████▌     | 202/437 [00:04<00:05, 39.35it/s] 47%|████▋     | 207/437 [00:05<00:05, 40.99it/s] 49%|████▊     | 212/437 [00:05<00:05, 42.27it/s] 50%|████▉     | 217/437 [00:05<00:05, 43.06it/s] 51%|█████     | 222/437 [00:05<00:04, 43.07it/s] 52%|█████▏    | 227/437 [00:05<00:04, 43.35it/s] 53%|█████▎    | 232/437 [00:05<00:04, 43.82it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.24it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.62it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.87it/s] 58%|█████▊    | 252/437 [00:06<00:04, 44.90it/s] 59%|█████▉    | 257/437 [00:06<00:04, 44.91it/s] 60%|█████▉    | 262/437 [00:06<00:03, 44.78it/s] 61%|██████    | 267/437 [00:06<00:03, 44.54it/s] 62%|██████▏   | 272/437 [00:06<00:03, 44.36it/s] 63%|██████▎   | 277/437 [00:06<00:03, 44.63it/s] 65%|██████▍   | 282/437 [00:06<00:03, 44.83it/s] 66%|██████▌   | 287/437 [00:06<00:03, 45.09it/s] 67%|██████▋   | 292/437 [00:06<00:03, 45.13it/s] 68%|██████▊   | 297/437 [00:07<00:03, 45.15it/s] 69%|██████▉   | 302/437 [00:07<00:03, 45.00it/s] 70%|███████   | 307/437 [00:07<00:03, 32.63it/s] 71%|███████▏  | 312/437 [00:07<00:03, 35.64it/s] 73%|███████▎  | 317/437 [00:07<00:03, 38.15it/s] 74%|███████▎  | 322/437 [00:07<00:02, 40.06it/s] 75%|███████▍  | 327/437 [00:07<00:02, 41.59it/s] 76%|███████▌  | 332/437 [00:07<00:02, 42.75it/s] 77%|███████▋  | 337/437 [00:08<00:02, 43.55it/s] 78%|███████▊  | 342/437 [00:08<00:02, 43.97it/s] 79%|███████▉  | 347/437 [00:08<00:02, 43.90it/s] 81%|████████  | 352/437 [00:08<00:01, 43.77it/s] 82%|████████▏ | 357/437 [00:08<00:01, 43.91it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.22it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.56it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.89it/s] 86%|████████▋ | 377/437 [00:08<00:01, 45.07it/s] 87%|████████▋ | 382/437 [00:09<00:01, 45.17it/s] 89%|████████▊ | 387/437 [00:09<00:01, 45.20it/s] 90%|████████▉ | 392/437 [00:09<00:01, 44.75it/s] 91%|█████████ | 397/437 [00:09<00:00, 44.52it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.45it/s] 93%|█████████▎| 407/437 [00:09<00:00, 44.52it/s] 94%|█████████▍| 412/437 [00:09<00:00, 44.75it/s] 95%|█████████▌| 417/437 [00:09<00:00, 45.01it/s] 97%|█████████▋| 422/437 [00:09<00:00, 45.14it/s] 98%|█████████▊| 427/437 [00:10<00:00, 45.22it/s] 99%|█████████▉| 432/437 [00:10<00:00, 45.08it/s]100%|██████████| 437/437 [00:10<00:00, 31.49it/s]100%|██████████| 437/437 [00:10<00:00, 41.76it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 23:32:54,114 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:54,114 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:54,114 >>   eval_loss               =     1.0482
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:54,114 >>   eval_runtime            = 0:00:10.50
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:54,114 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:54,114 >>   eval_samples_per_second =    332.494
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:54,114 >>   eval_steps_per_second   =     41.609
[INFO|trainer_pt_utils.py:913] 2023-08-28 23:32:54,114 >>   perplexity              =     2.8526
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:34,529 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:34,619 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:34,619 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:34,619 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:34,619 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:33:35,594 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:33:35,595 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:33:36,426 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:33:37,639 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:33:37,715 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:40,536 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:40,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:40,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:40,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:33:40,538 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:33:41,537 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:33:41,576 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:33:42,179 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:33:42,421 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:33:42,422 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/checkpoint-351
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.62it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.61it/s]Extractor Predicting: 15it [00:09,  1.61it/s]Extractor Predicting: 16it [00:09,  1.61it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.58it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.61it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:13,  1.55it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:17,  1.56it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:18,  1.53it/s]Extractor Predicting: 31it [00:19,  1.49it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:20,  1.55it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:22,  1.56it/s]Extractor Predicting: 36it [00:22,  1.46it/s]Extractor Predicting: 37it [00:23,  1.49it/s]Extractor Predicting: 38it [00:24,  1.51it/s]Extractor Predicting: 39it [00:24,  1.53it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.51it/s]Extractor Predicting: 43it [00:27,  1.54it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:28,  1.54it/s]Extractor Predicting: 46it [00:29,  1.43it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:30,  1.52it/s]Extractor Predicting: 49it [00:31,  1.55it/s]Extractor Predicting: 50it [00:32,  1.58it/s]Extractor Predicting: 51it [00:32,  1.44it/s]Extractor Predicting: 52it [00:33,  1.36it/s]Extractor Predicting: 53it [00:34,  1.42it/s]Extractor Predicting: 54it [00:35,  1.45it/s]Extractor Predicting: 55it [00:35,  1.49it/s]Extractor Predicting: 56it [00:36,  1.43it/s]Extractor Predicting: 57it [00:37,  1.47it/s]Extractor Predicting: 58it [00:37,  1.51it/s]Extractor Predicting: 59it [00:38,  1.51it/s]Extractor Predicting: 60it [00:38,  1.51it/s]Extractor Predicting: 61it [00:39,  1.42it/s]Extractor Predicting: 62it [00:40,  1.45it/s]Extractor Predicting: 63it [00:41,  1.48it/s]Extractor Predicting: 64it [00:41,  1.46it/s]Extractor Predicting: 65it [00:42,  1.49it/s]Extractor Predicting: 66it [00:43,  1.31it/s]Extractor Predicting: 67it [00:44,  1.36it/s]Extractor Predicting: 68it [00:44,  1.39it/s]Extractor Predicting: 69it [00:45,  1.42it/s]Extractor Predicting: 70it [00:46,  1.43it/s]Extractor Predicting: 71it [00:47,  1.30it/s]Extractor Predicting: 72it [00:47,  1.33it/s]Extractor Predicting: 73it [00:48,  1.39it/s]Extractor Predicting: 74it [00:49,  1.39it/s]Extractor Predicting: 75it [00:49,  1.39it/s]Extractor Predicting: 76it [00:50,  1.42it/s]Extractor Predicting: 77it [00:51,  1.42it/s]Extractor Predicting: 78it [00:51,  1.46it/s]Extractor Predicting: 79it [00:52,  1.44it/s]Extractor Predicting: 80it [00:53,  1.36it/s]Extractor Predicting: 81it [00:54,  1.33it/s]Extractor Predicting: 82it [00:54,  1.39it/s]Extractor Predicting: 83it [00:55,  1.43it/s]Extractor Predicting: 84it [00:56,  1.47it/s]Extractor Predicting: 85it [00:56,  1.47it/s]Extractor Predicting: 86it [00:57,  1.37it/s]Extractor Predicting: 87it [00:58,  1.42it/s]Extractor Predicting: 88it [00:58,  1.47it/s]Extractor Predicting: 89it [00:59,  1.55it/s]Extractor Predicting: 90it [01:00,  1.55it/s]Extractor Predicting: 91it [01:00,  1.49it/s]Extractor Predicting: 92it [01:01,  1.58it/s]Extractor Predicting: 93it [01:01,  1.64it/s]Extractor Predicting: 94it [01:02,  1.70it/s]Extractor Predicting: 95it [01:03,  1.66it/s]Extractor Predicting: 96it [01:03,  1.58it/s]Extractor Predicting: 97it [01:04,  1.59it/s]Extractor Predicting: 98it [01:05,  1.60it/s]Extractor Predicting: 99it [01:05,  1.68it/s]Extractor Predicting: 100it [01:06,  1.70it/s]Extractor Predicting: 101it [01:07,  1.50it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:08,  1.55it/s]Extractor Predicting: 104it [01:08,  1.55it/s]Extractor Predicting: 105it [01:09,  1.57it/s]Extractor Predicting: 106it [01:10,  1.47it/s]Extractor Predicting: 107it [01:10,  1.49it/s]Extractor Predicting: 108it [01:11,  1.56it/s]Extractor Predicting: 109it [01:12,  1.61it/s]Extractor Predicting: 110it [01:12,  1.63it/s]Extractor Predicting: 111it [01:13,  1.59it/s]Extractor Predicting: 112it [01:13,  1.63it/s]Extractor Predicting: 113it [01:14,  1.67it/s]Extractor Predicting: 114it [01:15,  1.65it/s]Extractor Predicting: 115it [01:15,  1.69it/s]Extractor Predicting: 116it [01:16,  1.55it/s]Extractor Predicting: 117it [01:17,  1.55it/s]Extractor Predicting: 118it [01:17,  1.57it/s]Extractor Predicting: 119it [01:18,  1.57it/s]Extractor Predicting: 120it [01:19,  1.55it/s]Extractor Predicting: 121it [01:19,  1.42it/s]Extractor Predicting: 122it [01:20,  1.47it/s]Extractor Predicting: 123it [01:21,  1.48it/s]Extractor Predicting: 124it [01:21,  1.48it/s]Extractor Predicting: 125it [01:22,  1.50it/s]Extractor Predicting: 126it [01:23,  1.36it/s]Extractor Predicting: 127it [01:24,  1.40it/s]Extractor Predicting: 128it [01:24,  1.40it/s]Extractor Predicting: 129it [01:25,  1.45it/s]Extractor Predicting: 130it [01:25,  1.51it/s]Extractor Predicting: 131it [01:26,  1.49it/s]Extractor Predicting: 132it [01:27,  1.49it/s]Extractor Predicting: 133it [01:28,  1.43it/s]Extractor Predicting: 134it [01:28,  1.46it/s]Extractor Predicting: 135it [01:29,  1.49it/s]Extractor Predicting: 136it [01:30,  1.49it/s]Extractor Predicting: 137it [01:30,  1.50it/s]Extractor Predicting: 138it [01:31,  1.49it/s]Extractor Predicting: 139it [01:32,  1.39it/s]Extractor Predicting: 140it [01:32,  1.45it/s]Extractor Predicting: 141it [01:33,  1.45it/s]Extractor Predicting: 142it [01:34,  1.46it/s]Extractor Predicting: 143it [01:34,  1.43it/s]Extractor Predicting: 144it [01:35,  1.48it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:58,390 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:58,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:58,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:58,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:37:58,602 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:38:00,947 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:38:00,948 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:38:01,718 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:38:03,485 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:38:03,485 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:06,086 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:06,257 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:06,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:06,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:38:06,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:38:07,863 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:38:07,864 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:38:08,840 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:38:09,415 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:38:09,619 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5444915254237288,
  "recall": 0.14719358533791524,
  "score": 0.23174030658250674,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.71it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.55it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:05,  1.48it/s]Extractor Predicting: 10it [00:06,  1.51it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.59it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.58it/s]Extractor Predicting: 16it [00:10,  1.61it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:12,  1.53it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.54it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:18,  1.48it/s]Extractor Predicting: 30it [00:19,  1.36it/s]Extractor Predicting: 31it [00:20,  1.41it/s]Extractor Predicting: 32it [00:21,  1.44it/s]Extractor Predicting: 33it [00:21,  1.43it/s]Extractor Predicting: 34it [00:22,  1.48it/s]Extractor Predicting: 35it [00:23,  1.51it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:24,  1.61it/s]Extractor Predicting: 38it [00:58, 10.83s/it]Extractor Predicting: 39it [00:59,  7.78s/it]Extractor Predicting: 40it [01:00,  5.63s/it]Extractor Predicting: 41it [01:00,  4.15s/it]Extractor Predicting: 42it [01:01,  3.09s/it]Extractor Predicting: 43it [01:02,  2.35s/it]Extractor Predicting: 44it [01:02,  1.83s/it]Extractor Predicting: 45it [01:03,  1.48s/it]Extractor Predicting: 46it [01:04,  1.25s/it]Extractor Predicting: 47it [01:04,  1.06s/it]Extractor Predicting: 48it [01:05,  1.08it/s]Extractor Predicting: 49it [01:05,  1.19it/s]Extractor Predicting: 50it [01:06,  1.28it/s]Extractor Predicting: 51it [01:07,  1.33it/s]Extractor Predicting: 52it [01:07,  1.40it/s]Extractor Predicting: 53it [01:08,  1.45it/s]Extractor Predicting: 54it [01:09,  1.50it/s]Extractor Predicting: 55it [01:09,  1.50it/s]Extractor Predicting: 56it [01:10,  1.50it/s]Extractor Predicting: 57it [01:11,  1.56it/s]Extractor Predicting: 58it [01:11,  1.60it/s]Extractor Predicting: 59it [01:12,  1.64it/s]Extractor Predicting: 60it [01:12,  1.63it/s]Extractor Predicting: 61it [01:13,  1.58it/s]Extractor Predicting: 62it [01:14,  1.60it/s]Extractor Predicting: 63it [01:14,  1.62it/s]Extractor Predicting: 64it [01:15,  1.59it/s]Extractor Predicting: 65it [01:16,  1.60it/s]Extractor Predicting: 66it [01:16,  1.61it/s]Extractor Predicting: 67it [01:17,  1.62it/s]Extractor Predicting: 68it [01:17,  1.68it/s]Extractor Predicting: 69it [01:18,  1.66it/s]Extractor Predicting: 70it [01:19,  1.65it/s]Extractor Predicting: 71it [01:19,  1.64it/s]Extractor Predicting: 72it [01:20,  1.61it/s]Extractor Predicting: 73it [01:20,  1.58it/s]Extractor Predicting: 74it [01:21,  1.59it/s]Extractor Predicting: 75it [01:22,  1.59it/s]Extractor Predicting: 76it [01:22,  1.60it/s]Extractor Predicting: 77it [01:23,  1.53it/s]Extractor Predicting: 78it [01:24,  1.56it/s]Extractor Predicting: 79it [01:24,  1.56it/s]Extractor Predicting: 80it [01:25,  1.58it/s]Extractor Predicting: 81it [01:26,  1.58it/s]Extractor Predicting: 82it [01:26,  1.48it/s]Extractor Predicting: 83it [01:27,  1.52it/s]Extractor Predicting: 84it [01:28,  1.54it/s]Extractor Predicting: 85it [01:28,  1.59it/s]Extractor Predicting: 86it [01:29,  1.58it/s]Extractor Predicting: 87it [01:30,  1.49it/s]Extractor Predicting: 88it [01:30,  1.51it/s]Extractor Predicting: 89it [01:31,  1.56it/s]Extractor Predicting: 90it [01:31,  1.57it/s]Extractor Predicting: 91it [01:32,  1.58it/s]Extractor Predicting: 92it [01:33,  1.55it/s]Extractor Predicting: 93it [01:33,  1.60it/s]Extractor Predicting: 94it [01:34,  1.57it/s]Extractor Predicting: 95it [01:35,  1.43it/s]Extractor Predicting: 96it [01:35,  1.49it/s]Extractor Predicting: 97it [01:36,  1.53it/s]Extractor Predicting: 98it [01:37,  1.55it/s]Extractor Predicting: 99it [01:37,  1.57it/s]Extractor Predicting: 100it [01:38,  1.45it/s]Extractor Predicting: 101it [01:39,  1.50it/s]Extractor Predicting: 102it [01:39,  1.54it/s]Extractor Predicting: 103it [01:40,  1.52it/s]Extractor Predicting: 104it [01:41,  1.52it/s]Extractor Predicting: 105it [01:41,  1.48it/s]Extractor Predicting: 106it [01:42,  1.51it/s]Extractor Predicting: 107it [01:43,  1.53it/s]Extractor Predicting: 108it [01:43,  1.55it/s]Extractor Predicting: 109it [01:44,  1.55it/s]Extractor Predicting: 110it [01:45,  1.49it/s]Extractor Predicting: 111it [01:45,  1.50it/s]Extractor Predicting: 112it [01:46,  1.51it/s]Extractor Predicting: 113it [01:47,  1.52it/s]Extractor Predicting: 114it [01:47,  1.38it/s]Extractor Predicting: 115it [01:48,  1.39it/s]Extractor Predicting: 116it [01:49,  1.46it/s]Extractor Predicting: 117it [01:49,  1.51it/s]Extractor Predicting: 118it [01:50,  1.53it/s]Extractor Predicting: 119it [01:51,  1.55it/s]Extractor Predicting: 120it [01:51,  1.43it/s]Extractor Predicting: 121it [01:52,  1.49it/s]Extractor Predicting: 122it [01:53,  1.53it/s]Extractor Predicting: 123it [01:53,  1.58it/s]Extractor Predicting: 124it [01:54,  1.61it/s]Extractor Predicting: 125it [01:55,  1.50it/s]Extractor Predicting: 126it [01:55,  1.53it/s]Extractor Predicting: 127it [01:56,  1.57it/s]Extractor Predicting: 128it [01:56,  1.57it/s]Extractor Predicting: 129it [01:57,  1.55it/s]Extractor Predicting: 130it [01:58,  1.49it/s]Extractor Predicting: 131it [01:58,  1.55it/s]Extractor Predicting: 132it [01:59,  1.57it/s]Extractor Predicting: 133it [02:00,  1.59it/s]Extractor Predicting: 134it [02:00,  1.62it/s]Extractor Predicting: 135it [02:01,  1.53it/s]Extractor Predicting: 136it [02:02,  1.56it/s]Extractor Predicting: 137it [02:02,  1.58it/s]Extractor Predicting: 138it [02:03,  1.57it/s]Extractor Predicting: 139it [02:03,  1.60it/s]Extractor Predicting: 140it [02:04,  1.66it/s]Extractor Predicting: 141it [02:05,  1.67it/s]Extractor Predicting: 142it [02:05,  1.48it/s]Extractor Predicting: 143it [02:06,  1.51it/s]Extractor Predicting: 144it [02:07,  1.55it/s]Extractor Predicting: 145it [02:07,  1.59it/s]Extractor Predicting: 146it [02:08,  1.63it/s]Extractor Predicting: 147it [02:09,  1.56it/s]Extractor Predicting: 148it [02:09,  1.55it/s]Extractor Predicting: 149it [02:10,  1.59it/s]Extractor Predicting: 150it [02:10,  1.60it/s]Extractor Predicting: 151it [02:11,  1.61it/s]Extractor Predicting: 152it [02:12,  1.50it/s]Extractor Predicting: 153it [02:12,  1.51it/s]Extractor Predicting: 154it [02:13,  1.56it/s]Extractor Predicting: 155it [02:14,  1.59it/s]Extractor Predicting: 156it [02:14,  1.64it/s]Extractor Predicting: 157it [02:15,  1.57it/s]Extractor Predicting: 158it [02:15,  1.64it/s]Extractor Predicting: 159it [02:16,  1.63it/s]Extractor Predicting: 160it [02:17,  1.61it/s]Extractor Predicting: 161it [02:17,  1.59it/s]Extractor Predicting: 162it [02:18,  1.52it/s]Extractor Predicting: 163it [02:19,  1.57it/s]Extractor Predicting: 164it [02:19,  1.59it/s]Extractor Predicting: 165it [02:20,  1.63it/s]Extractor Predicting: 166it [02:20,  1.66it/s]Extractor Predicting: 167it [02:21,  1.48it/s]Extractor Predicting: 168it [02:22,  1.48it/s]Extractor Predicting: 169it [02:23,  1.52it/s]Extractor Predicting: 170it [02:23,  1.54it/s]Extractor Predicting: 171it [02:24,  1.57it/s]Extractor Predicting: 172it [02:25,  1.52it/s]Extractor Predicting: 173it [02:25,  1.53it/s]Extractor Predicting: 174it [02:26,  1.56it/s]Extractor Predicting: 175it [02:26,  1.54it/s]Extractor Predicting: 176it [02:27,  1.53it/s]Extractor Predicting: 177it [02:28,  1.49it/s]Extractor Predicting: 178it [02:28,  1.52it/s]Extractor Predicting: 179it [02:29,  1.57it/s]Extractor Predicting: 180it [02:30,  1.61it/s]Extractor Predicting: 181it [02:30,  1.59it/s]Extractor Predicting: 182it [02:31,  1.52it/s]Extractor Predicting: 183it [02:32,  1.54it/s]Extractor Predicting: 184it [02:32,  1.54it/s]Extractor Predicting: 185it [02:33,  1.55it/s]Extractor Predicting: 186it [02:34,  1.56it/s]Extractor Predicting: 187it [02:34,  1.57it/s]Extractor Predicting: 188it [02:35,  1.55it/s]Extractor Predicting: 189it [02:36,  1.55it/s]Extractor Predicting: 190it [02:36,  1.51it/s]Extractor Predicting: 191it [02:37,  1.56it/s]Extractor Predicting: 192it [02:37,  1.60it/s]Extractor Predicting: 193it [02:38,  1.55it/s]Extractor Predicting: 194it [02:39,  1.54it/s]Extractor Predicting: 195it [02:40,  1.43it/s]Extractor Predicting: 196it [02:40,  1.48it/s]Extractor Predicting: 197it [02:41,  1.52it/s]Extractor Predicting: 198it [02:41,  1.52it/s]Extractor Predicting: 199it [02:42,  1.56it/s]Extractor Predicting: 200it [02:43,  1.55it/s]Extractor Predicting: 201it [02:43,  1.60it/s]Extractor Predicting: 202it [02:44,  1.62it/s]Extractor Predicting: 203it [02:45,  1.61it/s]Extractor Predicting: 204it [02:45,  1.61it/s]Extractor Predicting: 205it [02:46,  1.52it/s]Extractor Predicting: 206it [02:47,  1.54it/s]Extractor Predicting: 207it [02:47,  1.57it/s]Extractor Predicting: 208it [02:48,  1.56it/s]Extractor Predicting: 209it [02:48,  1.59it/s]Extractor Predicting: 210it [02:49,  1.37it/s]Extractor Predicting: 211it [02:50,  1.43it/s]Extractor Predicting: 212it [02:51,  1.49it/s]Extractor Predicting: 213it [02:51,  1.52it/s]Extractor Predicting: 214it [02:52,  1.52it/s]Extractor Predicting: 215it [02:53,  1.48it/s]Extractor Predicting: 216it [02:53,  1.53it/s]Extractor Predicting: 217it [02:54,  1.57it/s]Extractor Predicting: 218it [02:54,  1.57it/s]Extractor Predicting: 219it [02:55,  1.57it/s]Extractor Predicting: 220it [02:56,  1.42it/s]Extractor Predicting: 221it [02:57,  1.47it/s]Extractor Predicting: 222it [02:57,  1.50it/s]Extractor Predicting: 223it [02:58,  1.55it/s]Extractor Predicting: 224it [02:58,  1.57it/s]Extractor Predicting: 225it [02:59,  1.50it/s]Extractor Predicting: 226it [03:00,  1.51it/s]Extractor Predicting: 227it [03:00,  1.55it/s]Extractor Predicting: 228it [03:01,  1.57it/s]Extractor Predicting: 229it [03:02,  1.60it/s]Extractor Predicting: 230it [03:02,  1.52it/s]Extractor Predicting: 231it [03:03,  1.54it/s]Extractor Predicting: 232it [03:04,  1.35it/s]Extractor Predicting: 233it [03:05,  1.42it/s]Extractor Predicting: 234it [03:05,  1.49it/s]Extractor Predicting: 235it [03:06,  1.53it/s]Extractor Predicting: 236it [03:06,  1.56it/s]Extractor Predicting: 237it [03:07,  1.50it/s]Extractor Predicting: 238it [03:08,  1.53it/s]Extractor Predicting: 239it [03:08,  1.54it/s]Extractor Predicting: 240it [03:09,  1.59it/s]Extractor Predicting: 241it [03:10,  1.61it/s]Extractor Predicting: 242it [03:10,  1.54it/s]Extractor Predicting: 243it [03:11,  1.57it/s]Extractor Predicting: 244it [03:11,  1.58it/s]Extractor Predicting: 245it [03:12,  1.59it/s]Extractor Predicting: 246it [03:13,  1.59it/s]Extractor Predicting: 247it [03:13,  1.53it/s]Extractor Predicting: 248it [03:14,  1.56it/s]Extractor Predicting: 249it [03:15,  1.59it/s]Extractor Predicting: 250it [03:15,  1.62it/s]Extractor Predicting: 251it [03:16,  1.59it/s]Extractor Predicting: 252it [03:17,  1.52it/s]Extractor Predicting: 253it [03:17,  1.61it/s]Extractor Predicting: 254it [03:18,  1.60it/s]Extractor Predicting: 255it [03:18,  1.59it/s]Extractor Predicting: 256it [03:19,  1.58it/s]Extractor Predicting: 257it [03:20,  1.55it/s]Extractor Predicting: 258it [03:20,  1.60it/s]Extractor Predicting: 259it [03:21,  1.63it/s]Extractor Predicting: 260it [03:21,  1.65it/s]Extractor Predicting: 261it [03:22,  1.62it/s]Extractor Predicting: 262it [03:23,  1.50it/s]Extractor Predicting: 263it [03:24,  1.55it/s]Extractor Predicting: 264it [03:24,  1.59it/s]Extractor Predicting: 265it [03:25,  1.62it/s]Extractor Predicting: 266it [03:25,  1.67it/s]Extractor Predicting: 267it [03:26,  1.59it/s]Extractor Predicting: 268it [03:27,  1.60it/s]Extractor Predicting: 269it [03:27,  1.63it/s]Extractor Predicting: 270it [03:28,  1.64it/s]Extractor Predicting: 271it [03:28,  1.68it/s]Extractor Predicting: 272it [03:29,  1.63it/s]Extractor Predicting: 273it [03:30,  1.62it/s]Extractor Predicting: 274it [03:30,  1.64it/s]Extractor Predicting: 275it [03:31,  1.64it/s]Extractor Predicting: 276it [03:31,  1.64it/s]Extractor Predicting: 277it [03:32,  1.56it/s]Extractor Predicting: 278it [03:33,  1.59it/s]Extractor Predicting: 279it [03:33,  1.58it/s]Extractor Predicting: 280it [03:34,  1.61it/s]Extractor Predicting: 281it [03:35,  1.52it/s]Extractor Predicting: 281it [03:35,  1.31it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:23,780 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:23,877 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:23,877 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:23,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:23,878 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:42:25,686 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:42:25,687 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:42:26,617 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:42:28,030 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:42:28,153 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:32,096 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:32,275 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:32,275 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:32,275 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:42:32,275 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:42:33,692 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:42:33,694 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:42:34,536 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:42:35,133 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:42:35,280 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4859312192943278,
  "recall": 0.16135251371792972,
  "score": 0.2422623023825429,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.37it/s]Extractor Predicting: 2it [00:01,  1.42it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.50it/s]Extractor Predicting: 6it [00:03,  1.81it/s]Extractor Predicting: 6it [00:03,  1.62it/s]
[INFO|configuration_utils.py:515] 2023-08-28 23:42:47,222 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:42:47,334 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 23:42:47,526 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:42:47,527 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 23:42:47,608 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 23:43:37,637 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 23:43:37,766 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 23:43:39,582 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 23:43:39,583 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 23:43:40,033 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:43:40,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:43:40,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:43:40,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:43:40,374 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:43:40,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 23:43:40,374 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.23809523809523808,
  "recall": 0.019455252918287938,
  "score": 0.03597122302158274,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 23:43:41,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:42,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:43,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:43,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:44,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:44,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:45,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:46,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:46,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:47,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:47,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:48,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:49,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:49,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:50,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:50,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:51,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:52,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:52,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:53,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:11<02:47, 11.98s/it][WARNING|generation_utils.py:914] 2023-08-28 23:43:53,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:54,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:55,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:56,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:56,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:57,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:58,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:59,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:43:59,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:00,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:01,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:02,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:02,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:03,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:03,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:04,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:05,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:06,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:06,604 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:07,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:07,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:08,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:27<03:01, 13.94s/it][WARNING|generation_utils.py:914] 2023-08-28 23:44:09,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:09,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:10,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:11,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:11,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:12,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:12,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:13,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:13,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:14,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:15,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:16,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:16,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:17,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:17,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:18,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:19,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:19,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:20,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:20,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:21,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:21,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:40<02:43, 13.60s/it][WARNING|generation_utils.py:914] 2023-08-28 23:44:22,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:22,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:23,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:23,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:24,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:25,077 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:25,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:26,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:26,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:27,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:27,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:28,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:28,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:29,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:29,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:30,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:30,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:31,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:32,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:32,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:33,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:33,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:52<02:22, 12.96s/it][WARNING|generation_utils.py:914] 2023-08-28 23:44:34,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:35,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:35,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:36,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:36,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:37,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:37,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:38,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:39,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:39,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:40,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:41,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:41,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:42,264 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:43,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:44,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:44,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:45,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:45,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:46,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:46,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:47,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:06<02:11, 13.20s/it][WARNING|generation_utils.py:914] 2023-08-28 23:44:48,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:48,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:49,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:50,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:50,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:51,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:51,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:52,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:53,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:53,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:54,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:54,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:55,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:55,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:56,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:56,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:57,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:57,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:58,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:59,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:44:59,792 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:00,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:18<01:57, 13.04s/it][WARNING|generation_utils.py:914] 2023-08-28 23:45:00,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:01,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:02,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:02,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:03,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:04,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:04,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:05,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:05,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:06,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:07,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:08,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:08,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:09,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:10,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:10,747 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:11,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:12,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:12,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:13,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:14,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:14,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:15,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:34<01:50, 13.83s/it][WARNING|generation_utils.py:914] 2023-08-28 23:45:16,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:16,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:17,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:18,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:18,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:19,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:19,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:20,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:21,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:22,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:22,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:23,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:23,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:24,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:25,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:25,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:26,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:26,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:27,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:28,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:28,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:47<01:35, 13.63s/it][WARNING|generation_utils.py:914] 2023-08-28 23:45:29,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:30,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:30,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:31,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:32,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:32,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:33,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:33,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:34,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:35,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:35,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:36,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:37,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:37,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:38,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:39,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:39,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:40,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:41,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:41,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:42,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:42,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:43,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [02:02<01:23, 13.96s/it][WARNING|generation_utils.py:914] 2023-08-28 23:45:44,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:44,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:45,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:45,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:46,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:46,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:47,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:47,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:48,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:48,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:49,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:49,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:49,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:50,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:51,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:51,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:52,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:52,786 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:53,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:53,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:54,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:12<01:04, 12.94s/it][WARNING|generation_utils.py:914] 2023-08-28 23:45:54,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:55,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:55,920 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:56,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:57,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:57,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:58,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:59,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:45:59,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:00,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:00,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:01,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:02,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:02,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:03,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:04,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:04,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:05,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:05,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:06,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:07,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:25<00:51, 12.99s/it][WARNING|generation_utils.py:914] 2023-08-28 23:46:07,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:08,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:08,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:09,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:09,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:10,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:11,285 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:11,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:12,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:13,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:13,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:14,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:14,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:15,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:15,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:16,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:17,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:17,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:18,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:19,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:19,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:20,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:38<00:38, 12.95s/it][WARNING|generation_utils.py:914] 2023-08-28 23:46:20,737 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:21,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:22,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:22,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:23,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:23,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:24,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:24,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:25,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:26,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:26,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:27,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:28,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:29,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:29,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:30,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:30,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:31,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:32,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:32,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:33,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:33,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:52<00:26, 13.18s/it][WARNING|generation_utils.py:914] 2023-08-28 23:46:34,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:35,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:35,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:36,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:36,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:37,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:38,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:38,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:39,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:39,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:40,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:40,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:41,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:41,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:42,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:43,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:43,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:44,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:44,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:45,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:45,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:46,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:47,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:47,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:48,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [03:06<00:13, 13.50s/it][WARNING|generation_utils.py:914] 2023-08-28 23:46:48,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:49,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:50,013 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:51,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:51,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:52,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:53,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:53,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:54,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:54,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:55,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:55,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:56,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:57,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:57,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:58,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:58,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:59,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:46:59,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:47:00,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:47:01,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 23:47:01,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:20<00:00, 13.62s/it]Generating: 100%|██████████| 15/15 [03:20<00:00, 13.38s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:19,888 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:20,030 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:20,031 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:20,031 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:20,031 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:47:21,604 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:47:21,605 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:47:22,177 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:47:23,475 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:47:23,575 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:26,090 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:26,091 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:26,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:26,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:47:26,092 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:47:27,019 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:47:27,180 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:47:27,616 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:47:28,051 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:47:28,051 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 272, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 362, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 602, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.940625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 143, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 597, 'raw': 672}
{'target': 600, 'success': 626, 'raw': 704}
{'prompt': 'Relation : main subject .', 'success_rate': 0.8892045454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 327, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 410, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8622159090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 593, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : platform .', 'success_rate': 0.8835227272727273, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 588, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.8764204545454546, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 373, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 566, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8806818181818182, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 422, 'raw': 512}
{'target': 600, 'success': 453, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 610, 'raw': 736}
{'prompt': 'Relation : location .', 'success_rate': 0.8288043478260869, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 604, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8988095238095238, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 317, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 399, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 480, 'raw': 576}
{'target': 600, 'success': 507, 'raw': 608}
{'target': 600, 'success': 536, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 595, 'raw': 704}
{'target': 600, 'success': 625, 'raw': 736}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.8491847826086957, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 509, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 597, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9345238095238095, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 521, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 608, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9047619047619048, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 432, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 570, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 441, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : position held .', 'success_rate': 0.8522727272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position played on team / speciality . Context : On 31 March 2014 , the club announced that he would be joining Ligue 1 at the end of the current season . Head Entity : Ligue 1 , Tail Entity : manager .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 224, 'raw': 288}
{'target': 600, 'success': 246, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 287, 'raw': 384}
{'target': 600, 'success': 310, 'raw': 416}
{'target': 600, 'success': 337, 'raw': 448}
{'target': 600, 'success': 366, 'raw': 480}
{'target': 600, 'success': 386, 'raw': 512}
{'target': 600, 'success': 409, 'raw': 544}
{'target': 600, 'success': 436, 'raw': 576}
{'target': 600, 'success': 460, 'raw': 608}
{'target': 600, 'success': 487, 'raw': 640}
{'target': 600, 'success': 512, 'raw': 672}
{'target': 600, 'success': 536, 'raw': 704}
{'target': 600, 'success': 560, 'raw': 736}
{'target': 600, 'success': 587, 'raw': 768}
{'target': 600, 'success': 614, 'raw': 800}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.7675, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 167, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 221, 'raw': 256}
{'target': 600, 'success': 252, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 366, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : religion .', 'success_rate': 0.8821022727272727, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/3_ext.jsonl'}}
estimate vocab size: 10203
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10303, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.34it/s]Extractor Estimating: 2it [00:01,  1.48it/s]Extractor Estimating: 3it [00:01,  1.55it/s]Extractor Estimating: 4it [00:02,  1.63it/s]Extractor Estimating: 5it [00:03,  1.54it/s]Extractor Estimating: 6it [00:03,  1.47it/s]Extractor Estimating: 7it [00:04,  1.56it/s]Extractor Estimating: 8it [00:05,  1.56it/s]Extractor Estimating: 9it [00:05,  1.63it/s]Extractor Estimating: 10it [00:06,  1.63it/s]Extractor Estimating: 11it [00:07,  1.57it/s]Extractor Estimating: 12it [00:07,  1.62it/s]Extractor Estimating: 13it [00:08,  1.62it/s]Extractor Estimating: 14it [00:08,  1.56it/s]Extractor Estimating: 15it [00:09,  1.64it/s]Extractor Estimating: 16it [00:10,  1.59it/s]Extractor Estimating: 17it [00:10,  1.67it/s]Extractor Estimating: 18it [00:11,  1.67it/s]Extractor Estimating: 19it [00:11,  1.66it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:13,  1.51it/s]Extractor Estimating: 22it [00:13,  1.53it/s]Extractor Estimating: 23it [00:14,  1.55it/s]Extractor Estimating: 24it [00:15,  1.60it/s]Extractor Estimating: 25it [00:15,  1.60it/s]Extractor Estimating: 26it [00:16,  1.53it/s]Extractor Estimating: 27it [00:17,  1.54it/s]Extractor Estimating: 28it [00:17,  1.60it/s]Extractor Estimating: 29it [00:18,  1.60it/s]Extractor Estimating: 30it [00:19,  1.56it/s]Extractor Estimating: 31it [00:19,  1.44it/s]Extractor Estimating: 32it [00:20,  1.47it/s]Extractor Estimating: 33it [00:21,  1.53it/s]Extractor Estimating: 34it [00:21,  1.54it/s]Extractor Estimating: 35it [00:22,  1.58it/s]Extractor Estimating: 36it [00:23,  1.48it/s]Extractor Estimating: 37it [00:23,  1.52it/s]Extractor Estimating: 38it [00:24,  1.53it/s]Extractor Estimating: 39it [00:24,  1.52it/s]Extractor Estimating: 40it [00:25,  1.52it/s]Extractor Estimating: 41it [00:26,  1.43it/s]Extractor Estimating: 42it [00:27,  1.48it/s]Extractor Estimating: 43it [00:27,  1.46it/s]Extractor Estimating: 44it [00:28,  1.48it/s]Extractor Estimating: 45it [00:29,  1.45it/s]Extractor Estimating: 46it [00:29,  1.38it/s]Extractor Estimating: 47it [00:30,  1.44it/s]Extractor Estimating: 48it [00:31,  1.45it/s]Extractor Estimating: 49it [00:31,  1.48it/s]Extractor Estimating: 50it [00:32,  1.44it/s]Extractor Estimating: 51it [00:33,  1.36it/s]Extractor Estimating: 52it [00:34,  1.48it/s]Extractor Estimating: 53it [00:34,  1.50it/s]Extractor Estimating: 54it [00:35,  1.59it/s]Extractor Estimating: 55it [00:35,  1.66it/s]Extractor Estimating: 56it [00:36,  1.62it/s]Extractor Estimating: 57it [00:36,  1.70it/s]Extractor Estimating: 58it [00:37,  1.67it/s]Extractor Estimating: 59it [00:38,  1.71it/s]Extractor Estimating: 60it [00:38,  1.71it/s]Extractor Estimating: 61it [00:39,  1.73it/s]Extractor Estimating: 62it [00:39,  1.72it/s]Extractor Estimating: 63it [00:40,  1.64it/s]Extractor Estimating: 64it [00:41,  1.67it/s]Extractor Estimating: 65it [00:41,  1.75it/s]Extractor Estimating: 66it [00:42,  1.74it/s]Extractor Estimating: 67it [00:42,  1.75it/s]Extractor Estimating: 68it [00:43,  1.71it/s]Extractor Estimating: 69it [00:44,  1.60it/s]Extractor Estimating: 70it [00:44,  1.68it/s]Extractor Estimating: 71it [00:45,  1.74it/s]Extractor Estimating: 72it [00:45,  1.75it/s]Extractor Estimating: 73it [00:46,  1.73it/s]Extractor Estimating: 74it [00:46,  1.73it/s]Extractor Estimating: 75it [00:47,  1.70it/s]Extractor Estimating: 76it [00:47,  1.74it/s]Extractor Estimating: 77it [00:48,  1.77it/s]Extractor Estimating: 78it [00:49,  1.77it/s]Extractor Estimating: 79it [00:49,  1.79it/s]Extractor Estimating: 80it [00:50,  1.82it/s]Extractor Estimating: 81it [00:50,  1.79it/s]Extractor Estimating: 82it [00:51,  1.88it/s]Extractor Estimating: 83it [00:51,  1.89it/s]Extractor Estimating: 84it [00:52,  1.77it/s]Extractor Estimating: 85it [00:52,  1.79it/s]Extractor Estimating: 86it [00:53,  1.88it/s]Extractor Estimating: 87it [00:54,  1.78it/s]Extractor Estimating: 88it [00:54,  1.87it/s]Extractor Estimating: 89it [00:55,  1.89it/s]Extractor Estimating: 90it [00:55,  1.78it/s]Extractor Estimating: 91it [00:56,  1.77it/s]Extractor Estimating: 92it [00:56,  1.80it/s]Extractor Estimating: 93it [00:57,  1.73it/s]Extractor Estimating: 94it [00:57,  1.79it/s]Extractor Estimating: 95it [00:58,  1.87it/s]Extractor Estimating: 96it [00:58,  1.87it/s]Extractor Estimating: 97it [00:59,  1.88it/s]Extractor Estimating: 98it [00:59,  1.91it/s]Extractor Estimating: 99it [01:00,  1.76it/s]Extractor Estimating: 100it [01:01,  1.84it/s]Extractor Estimating: 101it [01:01,  1.78it/s]Extractor Estimating: 102it [01:02,  1.81it/s]Extractor Estimating: 103it [01:02,  1.91it/s]Extractor Estimating: 104it [01:03,  1.90it/s]Extractor Estimating: 105it [01:03,  1.75it/s]Extractor Estimating: 106it [01:04,  1.79it/s]Extractor Estimating: 107it [01:05,  1.78it/s]Extractor Estimating: 108it [01:05,  1.79it/s]Extractor Estimating: 109it [01:06,  1.82it/s]Extractor Estimating: 110it [01:06,  1.81it/s]Extractor Estimating: 111it [01:07,  1.67it/s]Extractor Estimating: 112it [01:07,  1.75it/s]Extractor Estimating: 113it [01:08,  1.74it/s]Extractor Estimating: 114it [01:09,  1.72it/s]Extractor Estimating: 115it [01:09,  1.71it/s]Extractor Estimating: 116it [01:10,  1.72it/s]Extractor Estimating: 117it [01:10,  1.76it/s]Extractor Estimating: 118it [01:11,  1.69it/s]Extractor Estimating: 119it [01:11,  1.74it/s]Extractor Estimating: 120it [01:12,  1.81it/s]Extractor Estimating: 121it [01:12,  1.81it/s]Extractor Estimating: 122it [01:13,  1.83it/s]Extractor Estimating: 123it [01:14,  1.85it/s]Extractor Estimating: 124it [01:14,  1.65it/s]Extractor Estimating: 125it [01:15,  1.65it/s]Extractor Estimating: 126it [01:16,  1.64it/s]Extractor Estimating: 127it [01:16,  1.72it/s]Extractor Estimating: 128it [01:17,  1.68it/s]Extractor Estimating: 129it [01:18,  1.45it/s]Extractor Estimating: 130it [01:18,  1.56it/s]Extractor Estimating: 131it [01:19,  1.70it/s]Extractor Estimating: 132it [01:19,  1.75it/s]Extractor Estimating: 133it [01:20,  1.74it/s]Extractor Estimating: 134it [01:20,  1.72it/s]Extractor Estimating: 135it [01:21,  1.70it/s]Extractor Estimating: 136it [01:21,  1.75it/s]Extractor Estimating: 137it [01:22,  1.75it/s]Extractor Estimating: 138it [01:22,  1.88it/s]Extractor Estimating: 139it [01:23,  1.91it/s]Extractor Estimating: 140it [01:24,  1.80it/s]Extractor Estimating: 141it [01:24,  1.78it/s]Extractor Estimating: 142it [01:25,  1.82it/s]Extractor Estimating: 143it [01:25,  1.87it/s]Extractor Estimating: 144it [01:26,  1.88it/s]Extractor Estimating: 145it [01:26,  1.98it/s]Extractor Estimating: 146it [01:27,  1.77it/s]Extractor Estimating: 147it [01:27,  1.80it/s]Extractor Estimating: 148it [01:28,  1.81it/s]Extractor Estimating: 149it [01:28,  1.87it/s]Extractor Estimating: 150it [01:29,  1.88it/s]Extractor Estimating: 151it [01:30,  1.77it/s]Extractor Estimating: 152it [01:30,  1.53it/s]Extractor Estimating: 153it [01:31,  1.52it/s]Extractor Estimating: 154it [01:32,  1.48it/s]Extractor Estimating: 155it [01:32,  1.53it/s]Extractor Estimating: 156it [01:33,  1.56it/s]Extractor Estimating: 157it [01:34,  1.45it/s]Extractor Estimating: 158it [01:34,  1.50it/s]Extractor Estimating: 159it [01:35,  1.53it/s]Extractor Estimating: 160it [01:36,  1.53it/s]Extractor Estimating: 161it [01:36,  1.53it/s]Extractor Estimating: 162it [01:37,  1.42it/s]Extractor Estimating: 163it [01:38,  1.46it/s]Extractor Estimating: 164it [01:38,  1.52it/s]Extractor Estimating: 165it [01:39,  1.56it/s]Extractor Estimating: 166it [01:40,  1.61it/s]Extractor Estimating: 167it [01:40,  1.64it/s]Extractor Estimating: 168it [01:41,  1.56it/s]Extractor Estimating: 169it [01:42,  1.55it/s]Extractor Estimating: 170it [01:42,  1.43it/s]Extractor Estimating: 171it [01:43,  1.47it/s]Extractor Estimating: 172it [01:44,  1.47it/s]Extractor Estimating: 173it [01:44,  1.51it/s]Extractor Estimating: 174it [01:45,  1.55it/s]Extractor Estimating: 175it [01:46,  1.48it/s]Extractor Estimating: 176it [01:46,  1.52it/s]Extractor Estimating: 177it [01:47,  1.54it/s]Extractor Estimating: 178it [01:48,  1.55it/s]Extractor Estimating: 179it [01:48,  1.60it/s]Extractor Estimating: 180it [01:49,  1.49it/s]Extractor Estimating: 181it [01:49,  1.57it/s]Extractor Estimating: 182it [01:50,  1.65it/s]Extractor Estimating: 183it [01:51,  1.68it/s]Extractor Estimating: 184it [01:51,  1.67it/s]Extractor Estimating: 185it [01:52,  1.63it/s]Extractor Estimating: 186it [01:52,  1.71it/s]Extractor Estimating: 187it [01:53,  1.75it/s]Extractor Estimating: 188it [01:53,  1.77it/s]Extractor Estimating: 189it [01:54,  1.75it/s]Extractor Estimating: 190it [01:55,  1.70it/s]Extractor Estimating: 191it [01:56,  1.50it/s]Extractor Estimating: 192it [01:56,  1.55it/s]Extractor Estimating: 193it [01:57,  1.61it/s]Extractor Estimating: 194it [01:57,  1.74it/s]Extractor Estimating: 195it [01:58,  1.73it/s]Extractor Estimating: 196it [01:58,  1.58it/s]Extractor Estimating: 197it [01:59,  1.65it/s]Extractor Estimating: 198it [02:00,  1.70it/s]Extractor Estimating: 199it [02:00,  1.74it/s]Extractor Estimating: 200it [02:01,  1.72it/s]Extractor Estimating: 201it [02:02,  1.53it/s]Extractor Estimating: 202it [02:02,  1.36it/s]Extractor Estimating: 203it [02:03,  1.32it/s]Extractor Estimating: 204it [02:04,  1.40it/s]Extractor Estimating: 205it [02:05,  1.44it/s]Extractor Estimating: 206it [02:05,  1.34it/s]Extractor Estimating: 207it [02:06,  1.39it/s]Extractor Estimating: 208it [02:07,  1.33it/s]Extractor Estimating: 209it [02:08,  1.35it/s]Extractor Estimating: 210it [02:08,  1.31it/s]Extractor Estimating: 211it [02:09,  1.37it/s]Extractor Estimating: 212it [02:10,  1.21it/s]Extractor Estimating: 213it [02:11,  1.30it/s]Extractor Estimating: 214it [02:11,  1.31it/s]Extractor Estimating: 215it [02:12,  1.40it/s]Extractor Estimating: 216it [02:13,  1.28it/s]Extractor Estimating: 217it [02:14,  1.36it/s]Extractor Estimating: 218it [02:14,  1.34it/s]Extractor Estimating: 219it [02:15,  1.38it/s]Extractor Estimating: 220it [02:16,  1.38it/s]Extractor Estimating: 221it [02:17,  1.39it/s]Extractor Estimating: 222it [02:17,  1.42it/s]Extractor Estimating: 223it [02:18,  1.47it/s]Extractor Estimating: 224it [02:18,  1.51it/s]Extractor Estimating: 225it [02:19,  1.44it/s]Extractor Estimating: 226it [02:20,  1.60it/s]Extractor Estimating: 227it [02:20,  1.70it/s]Extractor Estimating: 228it [02:21,  1.81it/s]Extractor Estimating: 229it [02:21,  1.86it/s]Extractor Estimating: 230it [02:22,  1.94it/s]Extractor Estimating: 231it [02:22,  1.82it/s]Extractor Estimating: 232it [02:23,  1.96it/s]Extractor Estimating: 233it [02:23,  1.95it/s]Extractor Estimating: 234it [02:24,  2.03it/s]Extractor Estimating: 235it [02:24,  2.14it/s]Extractor Estimating: 236it [02:25,  2.14it/s]Extractor Estimating: 237it [02:25,  2.18it/s]Extractor Estimating: 238it [02:26,  1.99it/s]Extractor Estimating: 239it [02:26,  2.05it/s]Extractor Estimating: 240it [02:26,  2.06it/s]Extractor Estimating: 241it [02:27,  2.06it/s]Extractor Estimating: 242it [02:27,  2.02it/s]Extractor Estimating: 243it [02:28,  1.99it/s]Extractor Estimating: 244it [02:29,  1.78it/s]Extractor Estimating: 245it [02:29,  1.84it/s]Extractor Estimating: 246it [02:30,  1.95it/s]Extractor Estimating: 247it [02:30,  2.00it/s]Extractor Estimating: 248it [02:31,  2.02it/s]Extractor Estimating: 249it [02:31,  2.09it/s]Extractor Estimating: 250it [02:32,  1.86it/s]Extractor Estimating: 251it [02:32,  1.80it/s]Extractor Estimating: 252it [02:33,  1.74it/s]Extractor Estimating: 253it [02:34,  1.70it/s]Extractor Estimating: 254it [02:34,  1.63it/s]Extractor Estimating: 255it [02:35,  1.51it/s]Extractor Estimating: 256it [02:36,  1.59it/s]Extractor Estimating: 257it [02:36,  1.60it/s]Extractor Estimating: 258it [02:37,  1.61it/s]Extractor Estimating: 259it [02:37,  1.60it/s]Extractor Estimating: 260it [02:38,  1.56it/s]Extractor Estimating: 261it [02:39,  1.55it/s]Extractor Estimating: 262it [02:39,  1.55it/s]Extractor Estimating: 263it [02:40,  1.60it/s]Extractor Estimating: 264it [02:41,  1.38it/s]Extractor Estimating: 265it [02:42,  1.45it/s]Extractor Estimating: 266it [02:42,  1.47it/s]Extractor Estimating: 267it [02:43,  1.50it/s]Extractor Estimating: 268it [02:43,  1.59it/s]Extractor Estimating: 269it [02:44,  1.53it/s]Extractor Estimating: 270it [02:45,  1.58it/s]Extractor Estimating: 271it [02:45,  1.58it/s]Extractor Estimating: 272it [02:46,  1.64it/s]Extractor Estimating: 273it [02:46,  1.63it/s]Extractor Estimating: 274it [02:47,  1.51it/s]Extractor Estimating: 275it [02:48,  1.52it/s]Extractor Estimating: 276it [02:48,  1.60it/s]Extractor Estimating: 277it [02:49,  1.65it/s]Extractor Estimating: 278it [02:50,  1.73it/s]Extractor Estimating: 279it [02:50,  1.59it/s]Extractor Estimating: 280it [02:51,  1.66it/s]Extractor Estimating: 281it [02:51,  1.67it/s]Extractor Estimating: 282it [02:52,  1.74it/s]Extractor Estimating: 283it [02:53,  1.75it/s]Extractor Estimating: 284it [02:53,  1.77it/s]Extractor Estimating: 285it [02:54,  1.71it/s]Extractor Estimating: 286it [02:54,  1.67it/s]Extractor Estimating: 287it [02:55,  1.72it/s]Extractor Estimating: 288it [02:55,  1.76it/s]Extractor Estimating: 289it [02:56,  1.80it/s]Extractor Estimating: 290it [02:57,  1.64it/s]Extractor Estimating: 291it [02:57,  1.60it/s]Extractor Estimating: 292it [02:58,  1.64it/s]Extractor Estimating: 293it [02:58,  1.72it/s]Extractor Estimating: 294it [02:59,  1.74it/s]Extractor Estimating: 295it [03:00,  1.70it/s]Extractor Estimating: 296it [03:00,  1.71it/s]Extractor Estimating: 297it [03:01,  1.56it/s]Extractor Estimating: 298it [03:01,  1.63it/s]Extractor Estimating: 299it [03:02,  1.67it/s]Extractor Estimating: 300it [03:03,  1.71it/s]Extractor Estimating: 301it [03:03,  1.72it/s]Extractor Estimating: 302it [03:04,  1.59it/s]Extractor Estimating: 303it [03:04,  1.69it/s]Extractor Estimating: 304it [03:05,  1.78it/s]Extractor Estimating: 305it [03:06,  1.75it/s]Extractor Estimating: 306it [03:06,  1.79it/s]Extractor Estimating: 307it [03:07,  1.76it/s]Extractor Estimating: 308it [03:07,  1.68it/s]Extractor Estimating: 309it [03:08,  1.70it/s]Extractor Estimating: 310it [03:08,  1.66it/s]Extractor Estimating: 311it [03:09,  1.69it/s]Extractor Estimating: 312it [03:10,  1.74it/s]Extractor Estimating: 313it [03:10,  1.71it/s]Extractor Estimating: 314it [03:11,  1.62it/s]Extractor Estimating: 315it [03:12,  1.57it/s]Extractor Estimating: 316it [03:12,  1.63it/s]Extractor Estimating: 317it [03:13,  1.68it/s]Extractor Estimating: 318it [03:13,  1.65it/s]Extractor Estimating: 319it [03:14,  1.71it/s]Extractor Estimating: 320it [03:14,  1.74it/s]Extractor Estimating: 321it [03:15,  1.67it/s]Extractor Estimating: 322it [03:16,  1.68it/s]Extractor Estimating: 323it [03:16,  1.73it/s]Extractor Estimating: 324it [03:17,  1.74it/s]Extractor Estimating: 325it [03:17,  1.67it/s]Extractor Estimating: 326it [03:18,  1.57it/s]Extractor Estimating: 327it [03:19,  1.58it/s]Extractor Estimating: 328it [03:19,  1.65it/s]Extractor Estimating: 329it [03:20,  1.78it/s]Extractor Estimating: 330it [03:20,  1.81it/s]Extractor Estimating: 331it [03:21,  1.87it/s]Extractor Estimating: 332it [03:21,  1.77it/s]Extractor Estimating: 333it [03:22,  1.78it/s]Extractor Estimating: 334it [03:23,  1.80it/s]Extractor Estimating: 335it [03:23,  1.80it/s]Extractor Estimating: 336it [03:24,  1.90it/s]Extractor Estimating: 337it [03:24,  1.87it/s]Extractor Estimating: 338it [03:25,  1.77it/s]Extractor Estimating: 339it [03:25,  1.78it/s]Extractor Estimating: 340it [03:26,  1.84it/s]Extractor Estimating: 341it [03:26,  1.86it/s]Extractor Estimating: 342it [03:27,  1.87it/s]Extractor Estimating: 343it [03:27,  1.85it/s]Extractor Estimating: 344it [03:28,  1.70it/s]Extractor Estimating: 345it [03:29,  1.76it/s]Extractor Estimating: 346it [03:29,  1.79it/s]Extractor Estimating: 347it [03:30,  1.80it/s]Extractor Estimating: 348it [03:30,  1.78it/s]Extractor Estimating: 349it [03:31,  1.81it/s]Extractor Estimating: 350it [03:31,  1.68it/s]Extractor Estimating: 351it [03:32,  1.73it/s]Extractor Estimating: 352it [03:33,  1.65it/s]Extractor Estimating: 353it [03:33,  1.67it/s]Extractor Estimating: 354it [03:34,  1.72it/s]Extractor Estimating: 355it [03:35,  1.56it/s]Extractor Estimating: 356it [03:35,  1.57it/s]Extractor Estimating: 357it [03:36,  1.58it/s]Extractor Estimating: 358it [03:36,  1.65it/s]Extractor Estimating: 359it [03:37,  1.62it/s]Extractor Estimating: 360it [03:38,  1.50it/s]Extractor Estimating: 361it [03:38,  1.59it/s]Extractor Estimating: 362it [03:39,  1.60it/s]Extractor Estimating: 363it [03:40,  1.65it/s]Extractor Estimating: 364it [03:40,  1.66it/s]Extractor Estimating: 365it [03:41,  1.53it/s]Extractor Estimating: 366it [03:42,  1.58it/s]Extractor Estimating: 367it [03:42,  1.67it/s]Extractor Estimating: 368it [03:43,  1.71it/s]Extractor Estimating: 369it [03:43,  1.73it/s]Extractor Estimating: 370it [03:44,  1.79it/s]Extractor Estimating: 371it [03:44,  1.76it/s]Extractor Estimating: 372it [03:45,  1.73it/s]Extractor Estimating: 373it [03:45,  1.67it/s]Extractor Estimating: 374it [03:46,  1.71it/s]Extractor Estimating: 375it [03:47,  1.79it/s]Extractor Estimating: 375it [03:47,  1.65it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:51:56,896 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:51:56,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:51:56,983 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:51:56,983 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:51:56,983 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 23:51:58,401 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 23:51:58,403 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:51:58,849 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 23:52:00,059 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:52:00,123 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:52:02,560 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:52:02,561 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:52:02,561 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:52:02,561 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 23:52:02,561 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 23:52:04,012 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 23:52:04,094 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 23:52:04,586 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 23:52:04,948 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 23:52:04,948 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:58:34,672 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:58:36,108 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 1499}
num of filtered data: 7495 mean pseudo reward: 0.9226395984294533
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 19405
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19505, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19505, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.956, loss:801.9382
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.968, loss:794.6507
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.956, loss:767.3868
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.975, loss:730.5053
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.953, loss:765.4590
>> valid entity prec:0.5944, rec:0.5676, f1:0.5807
>> valid relation prec:0.4005, rec:0.1372, f1:0.2044
>> valid relation with NER prec:0.4005, rec:0.1372, f1:0.2044
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.274, loss:742.4945
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.968, loss:741.1485
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.963, loss:760.3285
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.985, loss:753.6619
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.954, loss:728.5272
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5971, rec:0.5280, f1:0.5604
>> valid relation prec:0.4192, rec:0.1040, f1:0.1667
>> valid relation with NER prec:0.4192, rec:0.1040, f1:0.1667
g_step 1100, step 161, avg_time 2.220, loss:744.4981
g_step 1200, step 261, avg_time 0.976, loss:742.6257
g_step 1300, step 48, avg_time 0.953, loss:681.9874
g_step 1400, step 148, avg_time 0.963, loss:703.9320
g_step 1500, step 248, avg_time 0.992, loss:697.2351
>> valid entity prec:0.6301, rec:0.4957, f1:0.5549
>> valid relation prec:0.4043, rec:0.1029, f1:0.1640
>> valid relation with NER prec:0.4043, rec:0.1029, f1:0.1640
g_step 1600, step 35, avg_time 2.215, loss:666.2323
g_step 1700, step 135, avg_time 0.960, loss:662.2540
g_step 1800, step 235, avg_time 0.965, loss:657.6899
g_step 1900, step 22, avg_time 0.965, loss:644.7938
g_step 2000, step 122, avg_time 0.981, loss:637.2030
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5799, rec:0.5604, f1:0.5700
>> valid relation prec:0.3560, rec:0.1244, f1:0.1843
>> valid relation with NER prec:0.3560, rec:0.1244, f1:0.1843
g_step 2100, step 222, avg_time 2.235, loss:622.0915
g_step 2200, step 9, avg_time 0.953, loss:609.8799
g_step 2300, step 109, avg_time 0.969, loss:583.4971
g_step 2400, step 209, avg_time 0.980, loss:566.5237
g_step 2500, step 309, avg_time 0.968, loss:611.7058
>> valid entity prec:0.5691, rec:0.5871, f1:0.5780
>> valid relation prec:0.3448, rec:0.1261, f1:0.1846
>> valid relation with NER prec:0.3448, rec:0.1261, f1:0.1846
g_step 2600, step 96, avg_time 2.225, loss:543.7103
g_step 2700, step 196, avg_time 0.964, loss:560.9950
g_step 2800, step 296, avg_time 0.970, loss:580.2456
g_step 2900, step 83, avg_time 0.987, loss:507.0947
g_step 3000, step 183, avg_time 0.964, loss:557.1291
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5843, rec:0.5270, f1:0.5542
>> valid relation prec:0.2876, rec:0.1201, f1:0.1694
>> valid relation with NER prec:0.2876, rec:0.1201, f1:0.1694
g_step 3100, step 283, avg_time 2.218, loss:543.7038
g_step 3200, step 70, avg_time 0.979, loss:495.7934
g_step 3300, step 170, avg_time 0.974, loss:495.5680
g_step 3400, step 270, avg_time 0.981, loss:505.2711
g_step 3500, step 57, avg_time 0.953, loss:512.0823
>> valid entity prec:0.6008, rec:0.5094, f1:0.5514
>> valid relation prec:0.2921, rec:0.1103, f1:0.1601
>> valid relation with NER prec:0.2921, rec:0.1103, f1:0.1601
g_step 3600, step 157, avg_time 2.210, loss:478.2509
g_step 3700, step 257, avg_time 0.981, loss:494.0265
g_step 3800, step 44, avg_time 0.966, loss:475.3749
g_step 3900, step 144, avg_time 0.972, loss:445.0526
g_step 4000, step 244, avg_time 0.984, loss:478.5125
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5969, rec:0.5573, f1:0.5764
>> valid relation prec:0.2703, rec:0.1221, f1:0.1682
>> valid relation with NER prec:0.2703, rec:0.1221, f1:0.1682
g_step 4100, step 31, avg_time 2.207, loss:439.3351
g_step 4200, step 131, avg_time 0.988, loss:439.8148
g_step 4300, step 231, avg_time 0.961, loss:445.5212
g_step 4400, step 18, avg_time 0.955, loss:457.0539
g_step 4500, step 118, avg_time 0.976, loss:406.7892
>> valid entity prec:0.5462, rec:0.5289, f1:0.5374
>> valid relation prec:0.3301, rec:0.1352, f1:0.1919
>> valid relation with NER prec:0.3301, rec:0.1352, f1:0.1919
g_step 4600, step 218, avg_time 2.231, loss:435.7755
g_step 4700, step 5, avg_time 0.956, loss:439.6142
g_step 4800, step 105, avg_time 0.970, loss:399.0096
g_step 4900, step 205, avg_time 0.973, loss:411.9197
g_step 5000, step 305, avg_time 0.969, loss:426.5032
learning rate was adjusted to 0.0008
>> valid entity prec:0.5592, rec:0.5704, f1:0.5647
>> valid relation prec:0.3297, rec:0.1375, f1:0.1941
>> valid relation with NER prec:0.3297, rec:0.1375, f1:0.1941
g_step 5100, step 92, avg_time 2.213, loss:388.4068
g_step 5200, step 192, avg_time 0.972, loss:380.6110
g_step 5300, step 292, avg_time 0.967, loss:408.9749
g_step 5400, step 79, avg_time 0.979, loss:384.4375
g_step 5500, step 179, avg_time 0.959, loss:356.4437
>> valid entity prec:0.5756, rec:0.5277, f1:0.5506
>> valid relation prec:0.2800, rec:0.1023, f1:0.1498
>> valid relation with NER prec:0.2800, rec:0.1023, f1:0.1498
g_step 5600, step 279, avg_time 2.227, loss:362.5675
g_step 5700, step 66, avg_time 0.968, loss:355.5541
g_step 5800, step 166, avg_time 0.971, loss:355.0880
g_step 5900, step 266, avg_time 0.978, loss:359.6585
g_step 6000, step 53, avg_time 0.966, loss:359.8497
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5847, rec:0.5175, f1:0.5491
>> valid relation prec:0.2992, rec:0.1221, f1:0.1734
>> valid relation with NER prec:0.2992, rec:0.1221, f1:0.1734
g_step 6100, step 153, avg_time 2.227, loss:344.2289
g_step 6200, step 253, avg_time 0.976, loss:368.2568
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:58:36 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:58:36 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-58-34_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:58:38 - WARNING - datasets.builder -   Using custom data configuration default-49f3340beff07c7a
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-49f3340beff07c7a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:58:47,747 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:58:47,822 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:58:47,822 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:58:47,823 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:58:48,123 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:58:48,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:58:48,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:58:48,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:58:48,309 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:58:48,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:58:48,309 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:58:49,346 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:58:52,545 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:58:52,637 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-49f3340beff07c7a/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  1.81ba/s] 25%|██▌       | 2/8 [00:00<00:02,  2.93ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.64ba/s] 50%|█████     | 4/8 [00:01<00:01,  3.43ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.88ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.18ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.42ba/s]100%|██████████| 8/8 [00:01<00:00,  5.34ba/s]100%|██████████| 8/8 [00:01<00:00,  4.07ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.06ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.63ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.27ba/s]100%|██████████| 4/4 [00:01<00:00,  4.40ba/s]100%|██████████| 4/4 [00:01<00:00,  3.58ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.30ba/s] 25%|██▌       | 2/8 [00:00<00:01,  4.14ba/s] 50%|█████     | 4/8 [00:00<00:00,  6.79ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  6.11ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  7.86ba/s]100%|██████████| 8/8 [00:01<00:00,  7.03ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.12ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.89ba/s]100%|██████████| 4/4 [00:00<00:00,  7.22ba/s]100%|██████████| 4/4 [00:00<00:00,  5.60ba/s]
[INFO|trainer.py:414] 2023-08-29 01:59:04,358 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:59:04,727 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:59:04,728 >>   Num examples = 7499
[INFO|trainer.py:1149] 2023-08-29 01:59:04,728 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:59:04,728 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:59:04,728 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:59:04,728 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:59:04,728 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:56,  3.31it/s]  0%|          | 2/585 [00:00<02:52,  3.39it/s]  1%|          | 3/585 [00:00<02:49,  3.43it/s]  1%|          | 4/585 [00:01<02:47,  3.46it/s]  1%|          | 5/585 [00:01<02:47,  3.47it/s]  1%|          | 6/585 [00:01<02:46,  3.48it/s]  1%|          | 7/585 [00:02<02:45,  3.49it/s]  1%|▏         | 8/585 [00:02<02:45,  3.50it/s]  2%|▏         | 9/585 [00:02<02:44,  3.50it/s]  2%|▏         | 10/585 [00:02<02:44,  3.50it/s]  2%|▏         | 11/585 [00:03<03:04,  3.11it/s]  2%|▏         | 12/585 [00:03<02:58,  3.22it/s]  2%|▏         | 13/585 [00:03<02:53,  3.30it/s]  2%|▏         | 14/585 [00:04<02:50,  3.35it/s]  3%|▎         | 15/585 [00:04<02:47,  3.40it/s]  3%|▎         | 16/585 [00:04<02:46,  3.43it/s]  3%|▎         | 17/585 [00:04<02:44,  3.45it/s]  3%|▎         | 18/585 [00:05<03:35,  2.63it/s]  3%|▎         | 19/585 [00:05<03:19,  2.84it/s]  3%|▎         | 20/585 [00:06<03:07,  3.01it/s]  4%|▎         | 21/585 [00:06<02:59,  3.14it/s]  4%|▍         | 22/585 [00:06<02:53,  3.24it/s]  4%|▍         | 23/585 [00:07<02:49,  3.32it/s]  4%|▍         | 24/585 [00:07<02:46,  3.37it/s]  4%|▍         | 25/585 [00:07<02:44,  3.41it/s]  4%|▍         | 26/585 [00:07<02:43,  3.42it/s]  5%|▍         | 27/585 [00:08<02:42,  3.44it/s]  5%|▍         | 28/585 [00:08<02:58,  3.13it/s]  5%|▍         | 29/585 [00:08<02:52,  3.23it/s]  5%|▌         | 30/585 [00:09<02:47,  3.30it/s]  5%|▌         | 31/585 [00:09<02:44,  3.36it/s]  5%|▌         | 32/585 [00:09<02:42,  3.40it/s]  6%|▌         | 33/585 [00:09<02:41,  3.43it/s]  6%|▌         | 34/585 [00:10<02:39,  3.45it/s]  6%|▌         | 35/585 [00:10<02:38,  3.46it/s]  6%|▌         | 36/585 [00:10<02:38,  3.47it/s]  6%|▋         | 37/585 [00:11<02:37,  3.48it/s]  6%|▋         | 38/585 [00:11<02:37,  3.48it/s]  7%|▋         | 39/585 [00:11<02:53,  3.15it/s]  7%|▋         | 40/585 [00:12<02:48,  3.24it/s]  7%|▋         | 41/585 [00:12<02:44,  3.31it/s]  7%|▋         | 42/585 [00:12<02:41,  3.36it/s]  7%|▋         | 43/585 [00:12<02:39,  3.40it/s]  8%|▊         | 44/585 [00:13<02:37,  3.43it/s]  8%|▊         | 45/585 [00:13<02:36,  3.44it/s]  8%|▊         | 46/585 [00:13<02:36,  3.45it/s]  8%|▊         | 47/585 [00:14<02:35,  3.46it/s]  8%|▊         | 48/585 [00:14<02:34,  3.47it/s]  8%|▊         | 49/585 [00:14<02:34,  3.47it/s]  9%|▊         | 50/585 [00:15<02:42,  3.29it/s]  9%|▊         | 51/585 [00:15<02:39,  3.35it/s]  9%|▉         | 52/585 [00:15<02:37,  3.39it/s]  9%|▉         | 53/585 [00:15<02:35,  3.42it/s]  9%|▉         | 54/585 [00:16<02:34,  3.44it/s]  9%|▉         | 55/585 [00:16<02:33,  3.45it/s] 10%|▉         | 56/585 [00:16<02:32,  3.46it/s] 10%|▉         | 57/585 [00:17<02:32,  3.47it/s] 10%|▉         | 58/585 [00:17<02:31,  3.47it/s] 10%|█         | 59/585 [00:17<02:31,  3.48it/s] 10%|█         | 60/585 [00:17<02:30,  3.48it/s] 10%|█         | 61/585 [00:18<02:42,  3.22it/s] 11%|█         | 62/585 [00:18<02:38,  3.29it/s] 11%|█         | 63/585 [00:18<02:35,  3.35it/s] 11%|█         | 64/585 [00:19<02:33,  3.39it/s] 11%|█         | 65/585 [00:19<02:32,  3.42it/s] 11%|█▏        | 66/585 [00:19<02:30,  3.44it/s] 11%|█▏        | 67/585 [00:19<02:30,  3.45it/s] 12%|█▏        | 68/585 [00:20<02:29,  3.46it/s] 12%|█▏        | 69/585 [00:20<02:28,  3.47it/s] 12%|█▏        | 70/585 [00:20<02:28,  3.48it/s] 12%|█▏        | 71/585 [00:21<02:27,  3.48it/s] 12%|█▏        | 72/585 [00:21<02:41,  3.17it/s] 12%|█▏        | 73/585 [00:21<02:37,  3.26it/s] 13%|█▎        | 74/585 [00:22<02:33,  3.32it/s] 13%|█▎        | 75/585 [00:22<02:31,  3.37it/s] 13%|█▎        | 76/585 [00:22<02:29,  3.40it/s] 13%|█▎        | 77/585 [00:22<02:28,  3.43it/s] 13%|█▎        | 78/585 [00:23<02:27,  3.44it/s] 14%|█▎        | 79/585 [00:23<02:26,  3.45it/s] 14%|█▎        | 80/585 [00:23<02:25,  3.46it/s] 14%|█▍        | 81/585 [00:24<02:25,  3.47it/s] 14%|█▍        | 82/585 [00:24<02:24,  3.47it/s] 14%|█▍        | 83/585 [00:24<02:42,  3.08it/s] 14%|█▍        | 84/585 [00:25<02:36,  3.19it/s] 15%|█▍        | 85/585 [00:25<02:32,  3.28it/s] 15%|█▍        | 86/585 [00:25<02:42,  3.07it/s] 15%|█▍        | 87/585 [00:26<02:36,  3.19it/s] 15%|█▌        | 88/585 [00:26<02:31,  3.27it/s] 15%|█▌        | 89/585 [00:26<02:28,  3.33it/s] 15%|█▌        | 90/585 [00:26<02:26,  3.38it/s] 16%|█▌        | 91/585 [00:27<02:25,  3.41it/s] 16%|█▌        | 92/585 [00:27<02:23,  3.43it/s] 16%|█▌        | 93/585 [00:27<02:41,  3.04it/s] 16%|█▌        | 94/585 [00:28<02:35,  3.16it/s] 16%|█▌        | 95/585 [00:28<02:30,  3.25it/s] 16%|█▋        | 96/585 [00:28<02:27,  3.32it/s] 17%|█▋        | 97/585 [00:29<02:25,  3.36it/s] 17%|█▋        | 98/585 [00:29<02:23,  3.40it/s] 17%|█▋        | 99/585 [00:29<02:21,  3.42it/s] 17%|█▋        | 100/585 [00:29<02:20,  3.44it/s] 17%|█▋        | 101/585 [00:30<02:52,  2.80it/s] 17%|█▋        | 102/585 [00:31<04:39,  1.73it/s] 18%|█▊        | 103/585 [00:31<04:19,  1.86it/s] 18%|█▊        | 104/585 [00:32<03:42,  2.16it/s] 18%|█▊        | 105/585 [00:32<03:16,  2.44it/s] 18%|█▊        | 106/585 [00:32<02:58,  2.68it/s] 18%|█▊        | 107/585 [00:33<02:45,  2.88it/s] 18%|█▊        | 108/585 [00:33<02:37,  3.04it/s] 19%|█▊        | 109/585 [00:33<02:30,  3.16it/s] 19%|█▉        | 110/585 [00:33<02:26,  3.25it/s] 19%|█▉        | 111/585 [00:34<02:22,  3.31it/s] 19%|█▉        | 112/585 [00:34<02:20,  3.36it/s] 19%|█▉        | 113/585 [00:34<02:31,  3.11it/s] 19%|█▉        | 114/585 [00:35<02:26,  3.21it/s] 20%|█▉        | 115/585 [00:35<02:22,  3.29it/s] 20%|█▉        | 116/585 [00:35<02:20,  3.35it/s] 20%|██        | 117/585 [00:36<02:18,  3.39it/s][INFO|trainer.py:2140] 2023-08-29 01:59:40,935 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:59:40,936 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 01:59:40,936 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 56.38it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.89it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.00it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.27it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.82it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.53it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.14it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.50it/s][A
 11%|█         | 47/437 [00:01<00:08, 43.99it/s][A
 12%|█▏        | 52/437 [00:01<00:12, 30.18it/s][A
 13%|█▎        | 57/437 [00:01<00:13, 27.33it/s][A
 14%|█▍        | 62/437 [00:01<00:11, 31.57it/s][A
 15%|█▌        | 67/437 [00:01<00:10, 34.74it/s][A
 16%|█▋        | 72/437 [00:01<00:09, 37.28it/s][A
 18%|█▊        | 77/437 [00:01<00:09, 39.38it/s][A
 19%|█▉        | 82/437 [00:02<00:08, 40.91it/s][A
 20%|█▉        | 87/437 [00:02<00:08, 42.07it/s][A
 21%|██        | 92/437 [00:02<00:08, 42.91it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.19it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.15it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.08it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.43it/s][A
 27%|██▋       | 117/437 [00:03<00:10, 29.86it/s][A
 28%|██▊       | 122/437 [00:03<00:09, 33.20it/s][A
 29%|██▉       | 127/437 [00:03<00:08, 36.00it/s][A
 30%|███       | 132/437 [00:03<00:07, 38.30it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 40.00it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 41.39it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 42.44it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 42.86it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 42.92it/s][A
 37%|███▋      | 162/437 [00:04<00:06, 42.97it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 43.32it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 43.73it/s][A
 41%|████      | 177/437 [00:04<00:07, 33.11it/s][A
 42%|████▏     | 182/437 [00:04<00:08, 31.59it/s][A
 43%|████▎     | 187/437 [00:04<00:07, 35.15it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 37.63it/s][A
 45%|████▌     | 197/437 [00:05<00:06, 39.53it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 40.95it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 42.12it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 42.84it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 43.24it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.52it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.39it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.50it/s][A
 54%|█████▍    | 237/437 [00:06<00:06, 31.51it/s][A
 55%|█████▌    | 242/437 [00:06<00:05, 34.63it/s][A
 57%|█████▋    | 247/437 [00:06<00:05, 37.17it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 39.13it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 40.70it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 41.86it/s][A
 61%|██████    | 267/437 [00:06<00:03, 42.65it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 43.07it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 42.95it/s][A
 65%|██████▍   | 282/437 [00:07<00:03, 43.18it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 43.54it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 43.83it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.18it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 35.06it/s][A
 70%|███████   | 307/437 [00:07<00:03, 37.50it/s][A
 71%|███████▏  | 312/437 [00:07<00:03, 39.44it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 40.89it/s][A
 74%|███████▎  | 322/437 [00:08<00:02, 41.99it/s][A
 75%|███████▍  | 327/437 [00:08<00:02, 42.84it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 43.44it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 43.75it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 43.49it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.26it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.49it/s][A
 82%|████████▏ | 357/437 [00:09<00:01, 43.80it/s][A
 83%|████████▎ | 362/437 [00:09<00:02, 34.10it/s][A
 84%|████████▍ | 367/437 [00:09<00:01, 36.73it/s][A
 85%|████████▌ | 372/437 [00:09<00:01, 38.85it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 40.44it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 41.68it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 42.58it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.15it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.46it/s][A
 92%|█████████▏| 402/437 [00:10<00:00, 43.31it/s][A
 93%|█████████▎| 407/437 [00:10<00:00, 43.30it/s][A
 94%|█████████▍| 412/437 [00:10<00:00, 43.57it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 43.94it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 44.14it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.33it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.51it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.55it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.55it/s][A 20%|██        | 117/585 [00:47<02:18,  3.39it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:59:52,653 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 01:59:53,230 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:00:26,291 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:00:27,253 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:00:27,646 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [02:17<3:57:50, 30.56s/it] 20%|██        | 119/585 [02:17<2:47:05, 21.51s/it] 21%|██        | 120/585 [02:17<1:57:23, 15.15s/it] 21%|██        | 121/585 [02:18<1:22:40, 10.69s/it] 21%|██        | 122/585 [02:18<58:25,  7.57s/it]   21%|██        | 123/585 [02:18<41:28,  5.39s/it] 21%|██        | 124/585 [02:19<29:38,  3.86s/it] 21%|██▏       | 125/585 [02:19<21:22,  2.79s/it] 22%|██▏       | 126/585 [02:19<15:35,  2.04s/it] 22%|██▏       | 127/585 [02:19<11:33,  1.51s/it] 22%|██▏       | 128/585 [02:20<08:43,  1.15s/it] 22%|██▏       | 129/585 [02:20<07:00,  1.08it/s] 22%|██▏       | 130/585 [02:20<05:33,  1.36it/s] 22%|██▏       | 131/585 [02:21<04:32,  1.67it/s] 23%|██▎       | 132/585 [02:21<03:49,  1.97it/s] 23%|██▎       | 133/585 [02:21<03:19,  2.26it/s] 23%|██▎       | 134/585 [02:22<02:58,  2.52it/s] 23%|██▎       | 135/585 [02:22<02:44,  2.74it/s] 23%|██▎       | 136/585 [02:22<02:33,  2.92it/s] 23%|██▎       | 137/585 [02:22<02:26,  3.06it/s] 24%|██▎       | 138/585 [02:23<02:20,  3.18it/s] 24%|██▍       | 139/585 [02:23<02:33,  2.91it/s] 24%|██▍       | 140/585 [02:23<02:25,  3.07it/s] 24%|██▍       | 141/585 [02:24<02:19,  3.19it/s] 24%|██▍       | 142/585 [02:24<02:15,  3.27it/s] 24%|██▍       | 143/585 [02:24<02:12,  3.34it/s] 25%|██▍       | 144/585 [02:25<02:10,  3.38it/s] 25%|██▍       | 145/585 [02:25<02:08,  3.42it/s] 25%|██▍       | 146/585 [02:25<02:07,  3.44it/s] 25%|██▌       | 147/585 [02:25<02:06,  3.46it/s] 25%|██▌       | 148/585 [02:26<02:06,  3.47it/s] 25%|██▌       | 149/585 [02:26<02:05,  3.47it/s] 26%|██▌       | 150/585 [02:26<02:16,  3.18it/s] 26%|██▌       | 151/585 [02:27<02:12,  3.27it/s] 26%|██▌       | 152/585 [02:27<02:09,  3.34it/s] 26%|██▌       | 153/585 [02:27<02:07,  3.38it/s] 26%|██▋       | 154/585 [02:28<02:06,  3.42it/s] 26%|██▋       | 155/585 [02:28<02:04,  3.44it/s] 27%|██▋       | 156/585 [02:28<02:04,  3.46it/s] 27%|██▋       | 157/585 [02:28<02:03,  3.47it/s] 27%|██▋       | 158/585 [02:29<02:02,  3.48it/s] 27%|██▋       | 159/585 [02:29<02:02,  3.48it/s] 27%|██▋       | 160/585 [02:29<02:01,  3.48it/s] 28%|██▊       | 161/585 [02:30<02:20,  3.02it/s] 28%|██▊       | 162/585 [02:30<02:14,  3.15it/s] 28%|██▊       | 163/585 [02:30<02:09,  3.25it/s] 28%|██▊       | 164/585 [02:31<02:06,  3.32it/s] 28%|██▊       | 165/585 [02:31<02:04,  3.37it/s] 28%|██▊       | 166/585 [02:31<02:02,  3.41it/s] 29%|██▊       | 167/585 [02:31<02:01,  3.43it/s] 29%|██▊       | 168/585 [02:32<02:00,  3.45it/s] 29%|██▉       | 169/585 [02:32<02:00,  3.47it/s] 29%|██▉       | 170/585 [02:32<01:59,  3.47it/s] 29%|██▉       | 171/585 [02:33<02:11,  3.15it/s] 29%|██▉       | 172/585 [02:33<02:07,  3.24it/s] 30%|██▉       | 173/585 [02:33<02:04,  3.32it/s] 30%|██▉       | 174/585 [02:33<02:02,  3.37it/s] 30%|██▉       | 175/585 [02:34<02:00,  3.41it/s] 30%|███       | 176/585 [02:34<01:59,  3.43it/s] 30%|███       | 177/585 [02:34<01:58,  3.45it/s] 30%|███       | 178/585 [02:35<01:57,  3.46it/s] 31%|███       | 179/585 [02:35<01:56,  3.47it/s] 31%|███       | 180/585 [02:35<01:56,  3.48it/s] 31%|███       | 181/585 [02:35<01:55,  3.49it/s] 31%|███       | 182/585 [02:36<02:09,  3.11it/s] 31%|███▏      | 183/585 [02:36<02:04,  3.22it/s] 31%|███▏      | 184/585 [02:36<02:01,  3.30it/s] 32%|███▏      | 185/585 [02:37<01:59,  3.35it/s] 32%|███▏      | 186/585 [02:37<01:57,  3.40it/s] 32%|███▏      | 187/585 [02:37<01:56,  3.42it/s] 32%|███▏      | 188/585 [02:38<01:55,  3.44it/s] 32%|███▏      | 189/585 [02:38<01:54,  3.46it/s] 32%|███▏      | 190/585 [02:38<01:53,  3.47it/s] 33%|███▎      | 191/585 [02:38<01:53,  3.48it/s] 33%|███▎      | 192/585 [02:39<01:52,  3.48it/s] 33%|███▎      | 193/585 [02:39<02:11,  2.99it/s] 33%|███▎      | 194/585 [02:39<02:05,  3.12it/s] 33%|███▎      | 195/585 [02:40<02:00,  3.22it/s] 34%|███▎      | 196/585 [02:40<01:57,  3.30it/s] 34%|███▎      | 197/585 [02:40<01:55,  3.35it/s] 34%|███▍      | 198/585 [02:41<01:54,  3.39it/s] 34%|███▍      | 199/585 [02:41<01:52,  3.42it/s] 34%|███▍      | 200/585 [02:41<01:51,  3.44it/s] 34%|███▍      | 201/585 [02:41<01:51,  3.45it/s] 35%|███▍      | 202/585 [02:42<01:50,  3.47it/s] 35%|███▍      | 203/585 [02:42<02:02,  3.11it/s] 35%|███▍      | 204/585 [02:42<01:58,  3.21it/s] 35%|███▌      | 205/585 [02:43<01:55,  3.29it/s] 35%|███▌      | 206/585 [02:43<01:53,  3.35it/s] 35%|███▌      | 207/585 [02:43<01:51,  3.39it/s] 36%|███▌      | 208/585 [02:44<01:50,  3.42it/s] 36%|███▌      | 209/585 [02:44<01:49,  3.44it/s] 36%|███▌      | 210/585 [02:44<01:48,  3.45it/s] 36%|███▌      | 211/585 [02:44<01:48,  3.46it/s] 36%|███▌      | 212/585 [02:45<01:47,  3.47it/s] 36%|███▋      | 213/585 [02:45<01:47,  3.48it/s] 37%|███▋      | 214/585 [02:45<02:01,  3.07it/s] 37%|███▋      | 215/585 [02:46<01:56,  3.18it/s] 37%|███▋      | 216/585 [02:46<01:53,  3.26it/s] 37%|███▋      | 217/585 [02:46<01:50,  3.33it/s] 37%|███▋      | 218/585 [02:47<01:48,  3.37it/s] 37%|███▋      | 219/585 [02:47<01:47,  3.40it/s] 38%|███▊      | 220/585 [02:47<01:46,  3.43it/s] 38%|███▊      | 221/585 [02:47<01:45,  3.44it/s] 38%|███▊      | 222/585 [02:48<01:45,  3.45it/s] 38%|███▊      | 223/585 [02:48<01:44,  3.46it/s] 38%|███▊      | 224/585 [02:48<01:55,  3.11it/s] 38%|███▊      | 225/585 [02:49<01:51,  3.22it/s] 39%|███▊      | 226/585 [02:49<01:49,  3.29it/s] 39%|███▉      | 227/585 [02:49<01:52,  3.18it/s] 39%|███▉      | 228/585 [02:50<01:49,  3.27it/s] 39%|███▉      | 229/585 [02:50<01:46,  3.33it/s] 39%|███▉      | 230/585 [02:50<01:45,  3.38it/s] 39%|███▉      | 231/585 [02:51<01:43,  3.41it/s] 40%|███▉      | 232/585 [02:51<01:42,  3.43it/s] 40%|███▉      | 233/585 [02:51<01:42,  3.45it/s] 40%|████      | 234/585 [02:51<01:41,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 02:01:56,637 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:01:56,637 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 02:01:56,637 >>   Batch size = 8
{'eval_loss': 1.076441764831543, 'eval_runtime': 10.8279, 'eval_samples_per_second': 322.5, 'eval_steps_per_second': 40.359, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.25it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.87it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.17it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.60it/s][A
  6%|▌         | 27/437 [00:00<00:12, 33.07it/s][A
  7%|▋         | 32/437 [00:00<00:11, 36.24it/s][A
  8%|▊         | 37/437 [00:00<00:10, 38.62it/s][A
 10%|▉         | 42/437 [00:01<00:09, 40.48it/s][A
 11%|█         | 47/437 [00:01<00:09, 41.79it/s][A
 12%|█▏        | 52/437 [00:01<00:09, 42.70it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 43.40it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 43.65it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 43.51it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.47it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.70it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.94it/s][A
 20%|█▉        | 87/437 [00:02<00:07, 44.27it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.44it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.74it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.76it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.47it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.06it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.87it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.89it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 44.14it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.36it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.52it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.69it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.72it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.56it/s][A
 36%|███▌      | 157/437 [00:03<00:07, 35.36it/s][A
 37%|███▋      | 162/437 [00:03<00:07, 37.79it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 39.68it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 41.15it/s][A
 41%|████      | 177/437 [00:04<00:06, 42.20it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.03it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.57it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.89it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.62it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.50it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.73it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.95it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.27it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.43it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.68it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.66it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.49it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.19it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.01it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.07it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.15it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.34it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.54it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.72it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.65it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.35it/s][A
 66%|██████▌   | 287/437 [00:06<00:04, 34.42it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 37.05it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 39.15it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 40.73it/s][A
 70%|███████   | 307/437 [00:07<00:03, 41.92it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 42.81it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.44it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.64it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 43.41it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.33it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.49it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.87it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.17it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.44it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.47it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.60it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.45it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.05it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.86it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.93it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 44.19it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.42it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.61it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.60it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.56it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.34it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 34.54it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 37.14it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 39.18it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 40.70it/s][A
100%|██████████| 437/437 [00:10<00:00, 41.93it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 41.93it/s][A 40%|████      | 234/585 [03:02<01:41,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:02:07,182 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 02:02:07,909 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:02:41,348 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:02:42,769 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:02:43,076 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [04:36<3:03:51, 31.52s/it] 40%|████      | 236/585 [04:36<2:09:00, 22.18s/it] 41%|████      | 237/585 [04:37<1:30:44, 15.65s/it] 41%|████      | 238/585 [04:37<1:03:50, 11.04s/it] 41%|████      | 239/585 [04:37<45:03,  7.81s/it]   41%|████      | 240/585 [04:37<31:57,  5.56s/it] 41%|████      | 241/585 [04:38<22:47,  3.98s/it] 41%|████▏     | 242/585 [04:38<16:24,  2.87s/it] 42%|████▏     | 243/585 [04:38<11:56,  2.09s/it] 42%|████▏     | 244/585 [04:39<08:49,  1.55s/it] 42%|████▏     | 245/585 [04:39<06:38,  1.17s/it] 42%|████▏     | 246/585 [04:39<05:16,  1.07it/s] 42%|████▏     | 247/585 [04:40<04:09,  1.35it/s] 42%|████▏     | 248/585 [04:41<06:02,  1.07s/it] 43%|████▎     | 249/585 [04:42<04:41,  1.19it/s] 43%|████▎     | 250/585 [04:42<03:45,  1.48it/s] 43%|████▎     | 251/585 [04:42<03:25,  1.63it/s] 43%|████▎     | 252/585 [04:43<02:52,  1.93it/s] 43%|████▎     | 253/585 [04:43<02:28,  2.23it/s] 43%|████▎     | 254/585 [04:43<02:12,  2.50it/s] 44%|████▎     | 255/585 [04:44<02:00,  2.74it/s] 44%|████▍     | 256/585 [04:44<01:52,  2.93it/s] 44%|████▍     | 257/585 [04:44<01:46,  3.08it/s] 44%|████▍     | 258/585 [04:44<01:42,  3.19it/s] 44%|████▍     | 259/585 [04:45<01:39,  3.28it/s] 44%|████▍     | 260/585 [04:45<01:46,  3.06it/s] 45%|████▍     | 261/585 [04:45<01:41,  3.18it/s] 45%|████▍     | 262/585 [04:46<01:38,  3.27it/s] 45%|████▍     | 263/585 [04:46<01:36,  3.34it/s] 45%|████▌     | 264/585 [04:46<01:34,  3.38it/s] 45%|████▌     | 265/585 [04:47<01:33,  3.42it/s] 45%|████▌     | 266/585 [04:47<01:32,  3.44it/s] 46%|████▌     | 267/585 [04:47<01:31,  3.46it/s] 46%|████▌     | 268/585 [04:47<01:31,  3.47it/s] 46%|████▌     | 269/585 [04:48<01:30,  3.48it/s] 46%|████▌     | 270/585 [04:48<01:30,  3.48it/s] 46%|████▋     | 271/585 [04:48<01:40,  3.13it/s] 46%|████▋     | 272/585 [04:49<01:36,  3.23it/s] 47%|████▋     | 273/585 [04:49<01:34,  3.31it/s] 47%|████▋     | 274/585 [04:49<01:32,  3.36it/s] 47%|████▋     | 275/585 [04:49<01:31,  3.40it/s] 47%|████▋     | 276/585 [04:50<01:30,  3.43it/s] 47%|████▋     | 277/585 [04:50<01:29,  3.45it/s] 48%|████▊     | 278/585 [04:50<01:28,  3.46it/s] 48%|████▊     | 279/585 [04:51<01:28,  3.47it/s] 48%|████▊     | 280/585 [04:51<01:27,  3.48it/s] 48%|████▊     | 281/585 [04:51<01:27,  3.48it/s] 48%|████▊     | 282/585 [04:52<01:37,  3.11it/s] 48%|████▊     | 283/585 [04:52<01:33,  3.22it/s] 49%|████▊     | 284/585 [04:52<01:31,  3.30it/s] 49%|████▊     | 285/585 [04:52<01:29,  3.35it/s] 49%|████▉     | 286/585 [04:53<01:28,  3.39it/s] 49%|████▉     | 287/585 [04:53<01:27,  3.42it/s] 49%|████▉     | 288/585 [04:53<01:26,  3.44it/s] 49%|████▉     | 289/585 [04:54<01:25,  3.46it/s] 50%|████▉     | 290/585 [04:54<01:25,  3.47it/s] 50%|████▉     | 291/585 [04:54<01:24,  3.48it/s] 50%|████▉     | 292/585 [04:54<01:24,  3.48it/s] 50%|█████     | 293/585 [04:55<01:31,  3.21it/s] 50%|█████     | 294/585 [04:55<01:28,  3.29it/s] 50%|█████     | 295/585 [04:55<01:26,  3.35it/s] 51%|█████     | 296/585 [04:56<01:25,  3.39it/s] 51%|█████     | 297/585 [04:56<01:24,  3.42it/s] 51%|█████     | 298/585 [04:56<01:23,  3.44it/s] 51%|█████     | 299/585 [04:57<01:22,  3.46it/s] 51%|█████▏    | 300/585 [04:57<01:22,  3.47it/s] 51%|█████▏    | 301/585 [04:57<01:21,  3.48it/s] 52%|█████▏    | 302/585 [04:57<01:21,  3.48it/s] 52%|█████▏    | 303/585 [04:58<01:20,  3.49it/s] 52%|█████▏    | 304/585 [04:58<01:31,  3.06it/s] 52%|█████▏    | 305/585 [04:58<01:28,  3.18it/s] 52%|█████▏    | 306/585 [04:59<01:25,  3.27it/s] 52%|█████▏    | 307/585 [04:59<01:23,  3.33it/s] 53%|█████▎    | 308/585 [04:59<01:21,  3.38it/s] 53%|█████▎    | 309/585 [05:00<01:20,  3.41it/s] 53%|█████▎    | 310/585 [05:00<01:19,  3.44it/s] 53%|█████▎    | 311/585 [05:00<01:19,  3.46it/s] 53%|█████▎    | 312/585 [05:00<01:18,  3.47it/s] 54%|█████▎    | 313/585 [05:01<01:18,  3.48it/s] 54%|█████▎    | 314/585 [05:01<01:17,  3.48it/s] 54%|█████▍    | 315/585 [05:01<01:26,  3.11it/s] 54%|█████▍    | 316/585 [05:02<01:23,  3.21it/s] 54%|█████▍    | 317/585 [05:02<01:21,  3.29it/s] 54%|█████▍    | 318/585 [05:02<01:19,  3.35it/s] 55%|█████▍    | 319/585 [05:03<01:18,  3.39it/s] 55%|█████▍    | 320/585 [05:03<01:17,  3.42it/s] 55%|█████▍    | 321/585 [05:03<01:16,  3.44it/s] 55%|█████▌    | 322/585 [05:03<01:16,  3.45it/s] 55%|█████▌    | 323/585 [05:04<01:15,  3.46it/s] 55%|█████▌    | 324/585 [05:04<01:15,  3.47it/s] 56%|█████▌    | 325/585 [05:04<01:14,  3.47it/s] 56%|█████▌    | 326/585 [05:05<01:20,  3.21it/s] 56%|█████▌    | 327/585 [05:05<01:18,  3.29it/s] 56%|█████▌    | 328/585 [05:05<01:16,  3.35it/s] 56%|█████▌    | 329/585 [05:05<01:15,  3.39it/s] 56%|█████▋    | 330/585 [05:06<01:14,  3.42it/s] 57%|█████▋    | 331/585 [05:06<01:13,  3.44it/s] 57%|█████▋    | 332/585 [05:06<01:13,  3.45it/s] 57%|█████▋    | 333/585 [05:07<01:12,  3.46it/s] 57%|█████▋    | 334/585 [05:07<01:12,  3.47it/s] 57%|█████▋    | 335/585 [05:07<01:11,  3.47it/s] 57%|█████▋    | 336/585 [05:07<01:11,  3.47it/s] 58%|█████▊    | 337/585 [05:08<01:16,  3.24it/s] 58%|█████▊    | 338/585 [05:08<01:14,  3.31it/s] 58%|█████▊    | 339/585 [05:08<01:13,  3.36it/s] 58%|█████▊    | 340/585 [05:09<01:12,  3.39it/s] 58%|█████▊    | 341/585 [05:09<01:11,  3.42it/s] 58%|█████▊    | 342/585 [05:09<01:10,  3.44it/s] 59%|█████▊    | 343/585 [05:10<01:10,  3.45it/s] 59%|█████▉    | 344/585 [05:10<01:09,  3.46it/s] 59%|█████▉    | 345/585 [05:10<01:09,  3.47it/s] 59%|█████▉    | 346/585 [05:10<01:08,  3.47it/s] 59%|█████▉    | 347/585 [05:11<01:08,  3.47it/s] 59%|█████▉    | 348/585 [05:11<01:16,  3.11it/s] 60%|█████▉    | 349/585 [05:11<01:13,  3.22it/s] 60%|█████▉    | 350/585 [05:12<01:11,  3.29it/s] 60%|██████    | 351/585 [05:12<01:09,  3.35it/s][INFO|trainer.py:2140] 2023-08-29 02:04:17,243 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:04:17,243 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 02:04:17,243 >>   Batch size = 8
{'eval_loss': 1.0926727056503296, 'eval_runtime': 10.262, 'eval_samples_per_second': 340.284, 'eval_steps_per_second': 42.584, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.25it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.11it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.47it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.67it/s][A
  6%|▌         | 27/437 [00:00<00:09, 43.26it/s][A
  7%|▋         | 32/437 [00:00<00:09, 43.67it/s][A
  8%|▊         | 37/437 [00:00<00:09, 43.92it/s][A
 10%|▉         | 42/437 [00:00<00:08, 43.97it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.23it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.45it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.49it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.43it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.22it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.36it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.48it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.41it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.41it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.38it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.53it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.53it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.43it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.29it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.33it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.45it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.46it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.45it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.42it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.44it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.46it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.42it/s][A
 36%|███▌      | 157/437 [00:03<00:07, 37.82it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 39.77it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 41.19it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 42.23it/s][A
 41%|████      | 177/437 [00:04<00:06, 43.02it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.64it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.02it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.20it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 43.78it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 43.67it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 43.89it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.11it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.42it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.53it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.68it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.72it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.46it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.93it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.94it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.92it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.24it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.42it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.60it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.63it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.78it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.52it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.20it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 37.20it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 39.27it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 40.85it/s][A
 70%|███████   | 307/437 [00:07<00:03, 42.00it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 42.85it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.52it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.95it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.02it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 43.69it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 43.49it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 43.69it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.02it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.28it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.54it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.68it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.70it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.40it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.01it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.84it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.99it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.21it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.44it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.64it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.43it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.48it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.24it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.97it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 35.52it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 37.94it/s][A
100%|██████████| 437/437 [00:10<00:00, 39.82it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 39.82it/s][A 60%|██████    | 351/585 [05:22<01:09,  3.35it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:04:27,848 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 02:04:28,457 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:05:02,859 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:05:04,162 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:05:04,492 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [06:57<2:02:53, 31.64s/it] 60%|██████    | 353/585 [06:57<1:26:05, 22.26s/it] 61%|██████    | 354/585 [06:57<1:00:19, 15.67s/it] 61%|██████    | 355/585 [06:58<42:22, 11.06s/it]   61%|██████    | 356/585 [06:58<29:58,  7.85s/it] 61%|██████    | 357/585 [06:58<21:13,  5.58s/it] 61%|██████    | 358/585 [06:59<15:06,  3.99s/it] 61%|██████▏   | 359/585 [06:59<10:51,  2.88s/it] 62%|██████▏   | 360/585 [06:59<07:53,  2.10s/it] 62%|██████▏   | 361/585 [07:00<05:49,  1.56s/it] 62%|██████▏   | 362/585 [07:00<04:22,  1.18s/it] 62%|██████▏   | 363/585 [07:00<03:27,  1.07it/s] 62%|██████▏   | 364/585 [07:00<02:43,  1.35it/s] 62%|██████▏   | 365/585 [07:01<02:12,  1.66it/s] 63%|██████▎   | 366/585 [07:02<03:04,  1.19it/s] 63%|██████▎   | 367/585 [07:02<02:27,  1.48it/s] 63%|██████▎   | 368/585 [07:03<02:01,  1.79it/s] 63%|██████▎   | 369/585 [07:03<01:43,  2.09it/s] 63%|██████▎   | 370/585 [07:03<01:37,  2.21it/s] 63%|██████▎   | 371/585 [07:04<01:26,  2.48it/s] 64%|██████▎   | 372/585 [07:04<01:18,  2.72it/s] 64%|██████▍   | 373/585 [07:04<01:12,  2.91it/s] 64%|██████▍   | 374/585 [07:05<01:08,  3.07it/s] 64%|██████▍   | 375/585 [07:05<01:05,  3.18it/s] 64%|██████▍   | 376/585 [07:05<01:03,  3.27it/s] 64%|██████▍   | 377/585 [07:05<01:02,  3.34it/s] 65%|██████▍   | 378/585 [07:06<01:01,  3.38it/s] 65%|██████▍   | 379/585 [07:06<01:00,  3.42it/s] 65%|██████▍   | 380/585 [07:06<00:59,  3.44it/s] 65%|██████▌   | 381/585 [07:07<01:05,  3.13it/s] 65%|██████▌   | 382/585 [07:07<01:02,  3.23it/s] 65%|██████▌   | 383/585 [07:07<01:01,  3.31it/s] 66%|██████▌   | 384/585 [07:08<00:59,  3.36it/s] 66%|██████▌   | 385/585 [07:08<00:58,  3.39it/s] 66%|██████▌   | 386/585 [07:08<00:58,  3.42it/s] 66%|██████▌   | 387/585 [07:08<00:57,  3.44it/s] 66%|██████▋   | 388/585 [07:09<00:56,  3.46it/s] 66%|██████▋   | 389/585 [07:09<00:56,  3.47it/s] 67%|██████▋   | 390/585 [07:09<00:56,  3.48it/s] 67%|██████▋   | 391/585 [07:10<00:55,  3.48it/s] 67%|██████▋   | 392/585 [07:10<00:57,  3.37it/s] 67%|██████▋   | 393/585 [07:10<00:56,  3.41it/s] 67%|██████▋   | 394/585 [07:10<00:55,  3.43it/s] 68%|██████▊   | 395/585 [07:11<00:55,  3.45it/s] 68%|██████▊   | 396/585 [07:11<00:54,  3.47it/s] 68%|██████▊   | 397/585 [07:11<00:54,  3.47it/s] 68%|██████▊   | 398/585 [07:12<00:53,  3.48it/s] 68%|██████▊   | 399/585 [07:12<00:53,  3.48it/s] 68%|██████▊   | 400/585 [07:12<00:53,  3.49it/s] 69%|██████▊   | 401/585 [07:12<00:52,  3.49it/s] 69%|██████▊   | 402/585 [07:13<00:52,  3.49it/s] 69%|██████▉   | 403/585 [07:13<00:55,  3.29it/s] 69%|██████▉   | 404/585 [07:13<00:54,  3.35it/s] 69%|██████▉   | 405/585 [07:14<00:53,  3.39it/s] 69%|██████▉   | 406/585 [07:14<00:52,  3.42it/s] 70%|██████▉   | 407/585 [07:14<00:51,  3.44it/s] 70%|██████▉   | 408/585 [07:14<00:51,  3.46it/s] 70%|██████▉   | 409/585 [07:15<00:50,  3.47it/s] 70%|███████   | 410/585 [07:15<00:50,  3.48it/s] 70%|███████   | 411/585 [07:15<00:49,  3.48it/s] 70%|███████   | 412/585 [07:16<00:49,  3.49it/s] 71%|███████   | 413/585 [07:16<00:49,  3.49it/s] 71%|███████   | 414/585 [07:16<00:52,  3.25it/s] 71%|███████   | 415/585 [07:17<00:51,  3.32it/s] 71%|███████   | 416/585 [07:17<00:50,  3.37it/s] 71%|███████▏  | 417/585 [07:17<00:49,  3.40it/s] 71%|███████▏  | 418/585 [07:17<00:48,  3.42it/s] 72%|███████▏  | 419/585 [07:18<00:48,  3.45it/s] 72%|███████▏  | 420/585 [07:18<00:47,  3.46it/s] 72%|███████▏  | 421/585 [07:18<00:47,  3.47it/s] 72%|███████▏  | 422/585 [07:19<00:46,  3.48it/s] 72%|███████▏  | 423/585 [07:19<00:46,  3.48it/s] 72%|███████▏  | 424/585 [07:19<00:46,  3.48it/s] 73%|███████▎  | 425/585 [07:20<00:51,  3.08it/s] 73%|███████▎  | 426/585 [07:20<00:49,  3.19it/s] 73%|███████▎  | 427/585 [07:20<00:48,  3.28it/s] 73%|███████▎  | 428/585 [07:20<00:47,  3.34it/s] 73%|███████▎  | 429/585 [07:21<00:46,  3.38it/s] 74%|███████▎  | 430/585 [07:21<00:45,  3.41it/s] 74%|███████▎  | 431/585 [07:21<00:44,  3.42it/s] 74%|███████▍  | 432/585 [07:22<00:44,  3.44it/s] 74%|███████▍  | 433/585 [07:22<00:43,  3.46it/s] 74%|███████▍  | 434/585 [07:22<00:43,  3.46it/s] 74%|███████▍  | 435/585 [07:22<00:43,  3.47it/s] 75%|███████▍  | 436/585 [07:23<00:48,  3.10it/s] 75%|███████▍  | 437/585 [07:23<00:46,  3.20it/s] 75%|███████▍  | 438/585 [07:23<00:44,  3.28it/s] 75%|███████▌  | 439/585 [07:24<00:43,  3.34it/s] 75%|███████▌  | 440/585 [07:24<00:42,  3.38it/s] 75%|███████▌  | 441/585 [07:24<00:42,  3.41it/s] 76%|███████▌  | 442/585 [07:25<00:41,  3.43it/s] 76%|███████▌  | 443/585 [07:25<00:41,  3.45it/s] 76%|███████▌  | 444/585 [07:25<00:40,  3.46it/s] 76%|███████▌  | 445/585 [07:25<00:40,  3.47it/s] 76%|███████▌  | 446/585 [07:26<00:40,  3.47it/s] 76%|███████▋  | 447/585 [07:26<00:45,  3.03it/s] 77%|███████▋  | 448/585 [07:26<00:43,  3.15it/s] 77%|███████▋  | 449/585 [07:27<00:41,  3.24it/s] 77%|███████▋  | 450/585 [07:27<00:40,  3.31it/s] 77%|███████▋  | 451/585 [07:27<00:39,  3.36it/s] 77%|███████▋  | 452/585 [07:28<00:39,  3.40it/s] 77%|███████▋  | 453/585 [07:28<00:38,  3.42it/s] 78%|███████▊  | 454/585 [07:28<00:38,  3.44it/s] 78%|███████▊  | 455/585 [07:28<00:37,  3.45it/s] 78%|███████▊  | 456/585 [07:29<00:37,  3.46it/s] 78%|███████▊  | 457/585 [07:29<00:39,  3.21it/s] 78%|███████▊  | 458/585 [07:29<00:38,  3.29it/s] 78%|███████▊  | 459/585 [07:30<00:37,  3.34it/s] 79%|███████▊  | 460/585 [07:30<00:36,  3.38it/s] 79%|███████▉  | 461/585 [07:30<00:36,  3.41it/s] 79%|███████▉  | 462/585 [07:31<00:35,  3.44it/s] 79%|███████▉  | 463/585 [07:31<00:35,  3.45it/s] 79%|███████▉  | 464/585 [07:31<00:36,  3.30it/s] 79%|███████▉  | 465/585 [07:31<00:35,  3.35it/s] 80%|███████▉  | 466/585 [07:32<00:35,  3.39it/s] 80%|███████▉  | 467/585 [07:32<00:34,  3.42it/s] 80%|████████  | 468/585 [07:32<00:34,  3.43it/s][INFO|trainer.py:2140] 2023-08-29 02:06:37,537 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:06:37,537 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 02:06:37,537 >>   Batch size = 8
{'eval_loss': 1.1042248010635376, 'eval_runtime': 10.0824, 'eval_samples_per_second': 346.346, 'eval_steps_per_second': 43.343, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.42it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.44it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.69it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.72it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.12it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.90it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.75it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.57it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.62it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.74it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.73it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.58it/s][A
 15%|█▌        | 67/437 [00:01<00:09, 39.98it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 41.45it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 42.42it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 43.03it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 43.47it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.89it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.13it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.20it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.90it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.92it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.12it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.39it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.49it/s][A
 30%|███       | 132/437 [00:02<00:06, 44.57it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.64it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.62it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.33it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.13it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.06it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.26it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.46it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.57it/s][A
 41%|████      | 177/437 [00:03<00:05, 44.62it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.64it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.53it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.38it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.11it/s][A
 46%|████▌     | 202/437 [00:04<00:06, 37.27it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 39.28it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 40.85it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 41.98it/s][A
 51%|█████     | 222/437 [00:05<00:05, 42.74it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.39it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.90it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.07it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.82it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.64it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.82it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.96it/s][A
 60%|█████▉    | 262/437 [00:05<00:03, 44.29it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.44it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.63it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.64it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.59it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.27it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.11it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.10it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.11it/s][A
 70%|███████   | 307/437 [00:06<00:02, 44.36it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.40it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.51it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.61it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.59it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.44it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 38.34it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 40.16it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 41.50it/s][A
 81%|████████  | 352/437 [00:08<00:02, 42.50it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.24it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.75it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.08it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.09it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.82it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.61it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.79it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.05it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.38it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.54it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.67it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.75it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.49it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.08it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.00it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.97it/s][A
100%|██████████| 437/437 [00:09<00:00, 44.26it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:09<00:00, 44.26it/s][A 80%|████████  | 468/585 [07:42<00:34,  3.43it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:06:47,852 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 02:06:48,683 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:07:18,822 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:07:20,004 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:07:20,319 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [08:41<40:02, 20.71s/it] 80%|████████  | 470/585 [08:41<28:00, 14.61s/it] 81%|████████  | 471/585 [08:41<19:36, 10.32s/it] 81%|████████  | 472/585 [08:42<13:45,  7.31s/it] 81%|████████  | 473/585 [08:42<09:42,  5.20s/it] 81%|████████  | 474/585 [08:42<06:53,  3.73s/it] 81%|████████  | 475/585 [08:42<04:56,  2.70s/it] 81%|████████▏ | 476/585 [08:43<03:35,  1.97s/it] 82%|████████▏ | 477/585 [08:43<02:38,  1.47s/it] 82%|████████▏ | 478/585 [08:43<01:59,  1.11s/it] 82%|████████▏ | 479/585 [08:44<01:31,  1.16it/s] 82%|████████▏ | 480/585 [08:44<01:12,  1.45it/s] 82%|████████▏ | 481/585 [08:44<01:02,  1.66it/s] 82%|████████▏ | 482/585 [08:45<00:52,  1.97it/s] 83%|████████▎ | 483/585 [08:45<00:45,  2.27it/s] 83%|████████▎ | 484/585 [08:45<00:39,  2.53it/s] 83%|████████▎ | 485/585 [08:45<00:36,  2.76it/s] 83%|████████▎ | 486/585 [08:46<00:33,  2.95it/s] 83%|████████▎ | 487/585 [08:46<00:31,  3.09it/s] 83%|████████▎ | 488/585 [08:46<00:30,  3.21it/s] 84%|████████▎ | 489/585 [08:47<00:29,  3.29it/s] 84%|████████▍ | 490/585 [08:47<00:28,  3.35it/s] 84%|████████▍ | 491/585 [08:47<00:27,  3.39it/s] 84%|████████▍ | 492/585 [08:48<00:31,  2.99it/s] 84%|████████▍ | 493/585 [08:48<00:29,  3.12it/s] 84%|████████▍ | 494/585 [08:48<00:28,  3.23it/s] 85%|████████▍ | 495/585 [08:48<00:27,  3.30it/s] 85%|████████▍ | 496/585 [08:49<00:26,  3.36it/s] 85%|████████▍ | 497/585 [08:49<00:25,  3.40it/s] 85%|████████▌ | 498/585 [08:49<00:25,  3.43it/s] 85%|████████▌ | 499/585 [08:50<00:24,  3.45it/s] 85%|████████▌ | 500/585 [08:50<00:24,  3.47it/s]                                                  85%|████████▌ | 500/585 [08:50<00:24,  3.47it/s] 86%|████████▌ | 501/585 [08:50<00:24,  3.47it/s] 86%|████████▌ | 502/585 [08:51<00:27,  3.03it/s] 86%|████████▌ | 503/585 [08:51<00:25,  3.16it/s] 86%|████████▌ | 504/585 [08:51<00:24,  3.25it/s] 86%|████████▋ | 505/585 [08:51<00:24,  3.32it/s] 86%|████████▋ | 506/585 [08:52<00:23,  3.37it/s] 87%|████████▋ | 507/585 [08:52<00:22,  3.41it/s] 87%|████████▋ | 508/585 [08:52<00:22,  3.43it/s] 87%|████████▋ | 509/585 [08:53<00:22,  3.44it/s] 87%|████████▋ | 510/585 [08:53<00:21,  3.46it/s] 87%|████████▋ | 511/585 [08:53<00:21,  3.47it/s] 88%|████████▊ | 512/585 [08:54<00:24,  3.04it/s] 88%|████████▊ | 513/585 [08:54<00:22,  3.15it/s] 88%|████████▊ | 514/585 [08:54<00:21,  3.24it/s] 88%|████████▊ | 515/585 [08:54<00:21,  3.29it/s] 88%|████████▊ | 516/585 [08:55<00:20,  3.34it/s] 88%|████████▊ | 517/585 [08:55<00:20,  3.37it/s] 89%|████████▊ | 518/585 [08:55<00:19,  3.39it/s] 89%|████████▊ | 519/585 [08:56<00:19,  3.41it/s] 89%|████████▉ | 520/585 [08:56<00:19,  3.42it/s] 89%|████████▉ | 521/585 [08:56<00:18,  3.42it/s] 89%|████████▉ | 522/585 [08:57<00:19,  3.20it/s] 89%|████████▉ | 523/585 [08:57<00:18,  3.27it/s] 90%|████████▉ | 524/585 [08:57<00:18,  3.32it/s] 90%|████████▉ | 525/585 [08:57<00:17,  3.35it/s] 90%|████████▉ | 526/585 [08:58<00:17,  3.38it/s] 90%|█████████ | 527/585 [08:58<00:17,  3.39it/s] 90%|█████████ | 528/585 [08:58<00:16,  3.40it/s] 90%|█████████ | 529/585 [08:59<00:16,  3.41it/s] 91%|█████████ | 530/585 [08:59<00:16,  3.42it/s] 91%|█████████ | 531/585 [08:59<00:15,  3.42it/s] 91%|█████████ | 532/585 [08:59<00:15,  3.43it/s] 91%|█████████ | 533/585 [09:00<00:16,  3.18it/s] 91%|█████████▏| 534/585 [09:00<00:15,  3.26it/s] 91%|█████████▏| 535/585 [09:00<00:15,  3.31it/s] 92%|█████████▏| 536/585 [09:01<00:14,  3.34it/s] 92%|█████████▏| 537/585 [09:01<00:14,  3.37it/s] 92%|█████████▏| 538/585 [09:01<00:13,  3.39it/s] 92%|█████████▏| 539/585 [09:02<00:13,  3.40it/s] 92%|█████████▏| 540/585 [09:02<00:13,  3.41it/s] 92%|█████████▏| 541/585 [09:02<00:12,  3.42it/s] 93%|█████████▎| 542/585 [09:02<00:12,  3.42it/s] 93%|█████████▎| 543/585 [09:03<00:12,  3.43it/s] 93%|█████████▎| 544/585 [09:03<00:14,  2.80it/s] 93%|█████████▎| 545/585 [09:04<00:13,  2.97it/s] 93%|█████████▎| 546/585 [09:04<00:12,  3.09it/s] 94%|█████████▎| 547/585 [09:04<00:11,  3.19it/s] 94%|█████████▎| 548/585 [09:04<00:11,  3.26it/s] 94%|█████████▍| 549/585 [09:05<00:10,  3.31it/s] 94%|█████████▍| 550/585 [09:05<00:10,  3.36it/s] 94%|█████████▍| 551/585 [09:05<00:10,  3.40it/s] 94%|█████████▍| 552/585 [09:06<00:09,  3.42it/s] 95%|█████████▍| 553/585 [09:06<00:09,  3.44it/s] 95%|█████████▍| 554/585 [09:06<00:09,  3.22it/s] 95%|█████████▍| 555/585 [09:07<00:09,  3.30it/s] 95%|█████████▌| 556/585 [09:07<00:08,  3.35it/s] 95%|█████████▌| 557/585 [09:07<00:08,  3.39it/s] 95%|█████████▌| 558/585 [09:07<00:07,  3.41it/s] 96%|█████████▌| 559/585 [09:08<00:07,  3.43it/s] 96%|█████████▌| 560/585 [09:08<00:07,  3.45it/s] 96%|█████████▌| 561/585 [09:08<00:06,  3.46it/s] 96%|█████████▌| 562/585 [09:09<00:06,  3.46it/s] 96%|█████████▌| 563/585 [09:09<00:06,  3.47it/s] 96%|█████████▋| 564/585 [09:09<00:06,  3.47it/s] 97%|█████████▋| 565/585 [09:09<00:06,  3.16it/s] 97%|█████████▋| 566/585 [09:10<00:05,  3.25it/s] 97%|█████████▋| 567/585 [09:10<00:05,  3.30it/s] 97%|█████████▋| 568/585 [09:10<00:05,  3.34it/s] 97%|█████████▋| 569/585 [09:11<00:04,  3.36it/s] 97%|█████████▋| 570/585 [09:11<00:04,  3.39it/s] 98%|█████████▊| 571/585 [09:11<00:04,  3.40it/s] 98%|█████████▊| 572/585 [09:12<00:03,  3.41it/s] 98%|█████████▊| 573/585 [09:12<00:03,  3.41it/s] 98%|█████████▊| 574/585 [09:12<00:03,  3.42it/s] 98%|█████████▊| 575/585 [09:12<00:03,  3.16it/s] 98%|█████████▊| 576/585 [09:13<00:02,  3.25it/s] 99%|█████████▊| 577/585 [09:13<00:02,  3.32it/s] 99%|█████████▉| 578/585 [09:13<00:02,  3.36it/s] 99%|█████████▉| 579/585 [09:14<00:01,  3.39it/s] 99%|█████████▉| 580/585 [09:14<00:01,  3.13it/s] 99%|█████████▉| 581/585 [09:14<00:01,  3.23it/s] 99%|█████████▉| 582/585 [09:15<00:00,  3.30it/s]100%|█████████▉| 583/585 [09:15<00:00,  3.36it/s]100%|█████████▉| 584/585 [09:15<00:00,  3.39it/s]100%|██████████| 585/585 [09:16<00:00,  2.97it/s][INFO|trainer.py:2140] 2023-08-29 02:08:20,809 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:08:20,809 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 02:08:20,809 >>   Batch size = 8
{'eval_loss': 1.1113871335983276, 'eval_runtime': 10.0084, 'eval_samples_per_second': 348.906, 'eval_steps_per_second': 43.663, 'epoch': 4.0}
{'loss': 0.5205, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.85it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.29it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.90it/s][A
  5%|▌         | 22/437 [00:00<00:09, 46.06it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.60it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.86it/s][A
  8%|▊         | 37/437 [00:00<00:09, 44.39it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.20it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.30it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.45it/s][A
 13%|█▎        | 57/437 [00:02<00:27, 13.75it/s][A
 14%|█▍        | 62/437 [00:02<00:21, 17.54it/s][A
 15%|█▌        | 67/437 [00:02<00:17, 21.51it/s][A
 16%|█▋        | 72/437 [00:02<00:14, 25.52it/s][A
 18%|█▊        | 77/437 [00:02<00:12, 29.36it/s][A
 19%|█▉        | 82/437 [00:02<00:10, 32.78it/s][A
 20%|█▉        | 87/437 [00:02<00:11, 31.65it/s][A
 21%|██        | 92/437 [00:02<00:09, 34.78it/s][A
 22%|██▏       | 97/437 [00:03<00:09, 37.12it/s][A
 23%|██▎       | 102/437 [00:03<00:08, 39.06it/s][A
 24%|██▍       | 107/437 [00:03<00:08, 40.60it/s][A
 26%|██▌       | 112/437 [00:03<00:07, 41.84it/s][A
 27%|██▋       | 117/437 [00:03<00:07, 42.65it/s][A
 28%|██▊       | 122/437 [00:03<00:07, 43.20it/s][A
 29%|██▉       | 127/437 [00:03<00:07, 43.17it/s][A
 30%|███       | 132/437 [00:03<00:07, 43.47it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 43.80it/s][A
 32%|███▏      | 142/437 [00:04<00:06, 43.98it/s][A
 34%|███▎      | 147/437 [00:04<00:06, 44.20it/s][A
 35%|███▍      | 152/437 [00:04<00:06, 44.36it/s][A
 36%|███▌      | 157/437 [00:04<00:06, 44.31it/s][A
 37%|███▋      | 162/437 [00:04<00:06, 44.47it/s][A
 38%|███▊      | 167/437 [00:04<00:06, 44.23it/s][A
 39%|███▉      | 172/437 [00:04<00:06, 44.15it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.14it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.20it/s][A
 43%|████▎     | 187/437 [00:05<00:05, 44.27it/s][A
 44%|████▍     | 192/437 [00:05<00:05, 44.50it/s][A
 45%|████▌     | 197/437 [00:05<00:05, 44.55it/s][A
 46%|████▌     | 202/437 [00:05<00:05, 44.53it/s][A
 47%|████▋     | 207/437 [00:05<00:05, 44.47it/s][A
 49%|████▊     | 212/437 [00:05<00:05, 44.25it/s][A
 50%|████▉     | 217/437 [00:05<00:04, 44.23it/s][A
 51%|█████     | 222/437 [00:05<00:05, 36.90it/s][A
 52%|█████▏    | 227/437 [00:06<00:05, 39.09it/s][A
 53%|█████▎    | 232/437 [00:06<00:05, 40.69it/s][A
 54%|█████▍    | 237/437 [00:06<00:04, 41.91it/s][A
 55%|█████▌    | 242/437 [00:06<00:04, 42.78it/s][A
 57%|█████▋    | 247/437 [00:06<00:04, 43.42it/s][A
 58%|█████▊    | 252/437 [00:06<00:04, 43.80it/s][A
 59%|█████▉    | 257/437 [00:06<00:04, 43.95it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.68it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.42it/s][A
 62%|██████▏   | 272/437 [00:07<00:03, 43.66it/s][A
 63%|██████▎   | 277/437 [00:07<00:03, 43.99it/s][A
 65%|██████▍   | 282/437 [00:07<00:03, 44.22it/s][A
 66%|██████▌   | 287/437 [00:07<00:03, 44.47it/s][A
 67%|██████▋   | 292/437 [00:07<00:03, 44.62it/s][A
 68%|██████▊   | 297/437 [00:07<00:03, 44.45it/s][A
 69%|██████▉   | 302/437 [00:07<00:03, 44.35it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.13it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.80it/s][A
 73%|███████▎  | 317/437 [00:08<00:02, 43.78it/s][A
 74%|███████▎  | 322/437 [00:08<00:02, 44.06it/s][A
 75%|███████▍  | 327/437 [00:08<00:02, 44.25it/s][A
 76%|███████▌  | 332/437 [00:08<00:02, 44.46it/s][A
 77%|███████▋  | 337/437 [00:08<00:02, 44.68it/s][A
 78%|███████▊  | 342/437 [00:08<00:02, 44.68it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 44.55it/s][A
 81%|████████  | 352/437 [00:08<00:02, 35.97it/s][A
 82%|████████▏ | 357/437 [00:09<00:02, 38.23it/s][A
 83%|████████▎ | 362/437 [00:09<00:01, 40.05it/s][A
 84%|████████▍ | 367/437 [00:09<00:01, 41.43it/s][A
 85%|████████▌ | 372/437 [00:09<00:01, 42.46it/s][A
 86%|████████▋ | 377/437 [00:09<00:01, 43.18it/s][A
 87%|████████▋ | 382/437 [00:09<00:01, 43.65it/s][A
 89%|████████▊ | 387/437 [00:09<00:01, 43.83it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.68it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.47it/s][A
 92%|█████████▏| 402/437 [00:10<00:00, 43.60it/s][A
 93%|█████████▎| 407/437 [00:10<00:00, 43.96it/s][A
 94%|█████████▍| 412/437 [00:10<00:00, 44.21it/s][A
 95%|█████████▌| 417/437 [00:10<00:00, 44.46it/s][A
 97%|█████████▋| 422/437 [00:10<00:00, 44.58it/s][A
 98%|█████████▊| 427/437 [00:10<00:00, 44.69it/s][A
 99%|█████████▉| 432/437 [00:10<00:00, 44.43it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.15it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.15it/s][A100%|██████████| 585/585 [09:26<00:00,  2.97it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 02:08:32,449 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 02:08:32,945 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:08:44,374 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:08:44,672 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:08:44,851 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 02:08:54,677 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 02:08:54,747 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117 (score: 1.076441764831543).
                                                 100%|██████████| 585/585 [09:59<00:00,  2.97it/s]100%|██████████| 585/585 [09:59<00:00,  1.03s/it]
[INFO|trainer.py:1894] 2023-08-29 02:09:04,570 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 02:09:04,712 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 02:09:15,286 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 02:09:15,912 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 02:09:16,200 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:09:17,655 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:17,656 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:17,656 >>   train_loss               =     0.5166
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:17,656 >>   train_runtime            = 0:09:59.77
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:17,656 >>   train_samples            =       7499
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:17,656 >>   train_samples_per_second =     62.515
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:17,656 >>   train_steps_per_second   =      0.975
{'eval_loss': 1.1187241077423096, 'eval_runtime': 10.8614, 'eval_samples_per_second': 321.505, 'eval_steps_per_second': 40.234, 'epoch': 5.0}
{'train_runtime': 599.7788, 'train_samples_per_second': 62.515, 'train_steps_per_second': 0.975, 'train_loss': 0.516620349068927, 'epoch': 5.0}
08/29/2023 02:09:18 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 02:09:18,839 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 02:09:18,839 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 02:09:18,839 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.81it/s]  3%|▎         | 12/437 [00:00<00:08, 49.20it/s]  4%|▍         | 17/437 [00:00<00:11, 35.57it/s]  5%|▌         | 22/437 [00:00<00:10, 38.76it/s]  6%|▌         | 27/437 [00:00<00:10, 40.92it/s]  7%|▋         | 32/437 [00:00<00:09, 42.28it/s]  8%|▊         | 37/437 [00:00<00:09, 43.26it/s] 10%|▉         | 42/437 [00:00<00:09, 43.85it/s] 11%|█         | 47/437 [00:01<00:08, 44.31it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.69it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.33it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.08it/s] 15%|█▌        | 67/437 [00:01<00:08, 44.04it/s] 16%|█▋        | 72/437 [00:01<00:08, 44.33it/s] 18%|█▊        | 77/437 [00:01<00:08, 44.58it/s] 19%|█▉        | 82/437 [00:01<00:07, 44.80it/s] 20%|█▉        | 87/437 [00:01<00:07, 44.98it/s] 21%|██        | 92/437 [00:02<00:07, 45.11it/s] 22%|██▏       | 97/437 [00:02<00:07, 45.11it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.69it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.38it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.30it/s] 27%|██▋       | 117/437 [00:02<00:07, 44.43it/s] 28%|██▊       | 122/437 [00:02<00:07, 44.58it/s] 29%|██▉       | 127/437 [00:02<00:06, 44.74it/s] 30%|███       | 132/437 [00:03<00:08, 37.68it/s] 31%|███▏      | 137/437 [00:03<00:07, 39.71it/s] 32%|███▏      | 142/437 [00:04<00:24, 11.93it/s] 34%|███▎      | 147/437 [00:04<00:18, 15.31it/s] 35%|███▍      | 152/437 [00:04<00:14, 19.11it/s] 36%|███▌      | 157/437 [00:04<00:12, 23.11it/s] 37%|███▋      | 162/437 [00:04<00:10, 27.11it/s] 38%|███▊      | 167/437 [00:04<00:08, 30.81it/s] 39%|███▉      | 172/437 [00:04<00:07, 34.09it/s] 41%|████      | 177/437 [00:05<00:07, 36.76it/s] 42%|████▏     | 182/437 [00:05<00:06, 38.52it/s] 43%|████▎     | 187/437 [00:05<00:06, 39.92it/s] 44%|████▍     | 192/437 [00:05<00:05, 41.10it/s] 45%|████▌     | 197/437 [00:05<00:05, 42.18it/s] 46%|████▌     | 202/437 [00:05<00:05, 42.97it/s] 47%|████▋     | 207/437 [00:05<00:05, 43.66it/s] 49%|████▊     | 212/437 [00:05<00:05, 44.12it/s] 50%|████▉     | 217/437 [00:05<00:04, 44.53it/s] 51%|█████     | 222/437 [00:06<00:05, 38.57it/s] 52%|█████▏    | 227/437 [00:06<00:05, 40.40it/s] 53%|█████▎    | 232/437 [00:06<00:04, 41.75it/s] 54%|█████▍    | 237/437 [00:06<00:04, 42.73it/s] 55%|█████▌    | 242/437 [00:06<00:04, 43.49it/s] 57%|█████▋    | 247/437 [00:06<00:04, 44.01it/s] 58%|█████▊    | 252/437 [00:06<00:04, 44.29it/s] 59%|█████▉    | 257/437 [00:06<00:04, 44.44it/s] 60%|█████▉    | 262/437 [00:07<00:03, 44.11it/s] 61%|██████    | 267/437 [00:07<00:03, 43.97it/s] 62%|██████▏   | 272/437 [00:07<00:03, 44.02it/s] 63%|██████▎   | 277/437 [00:07<00:03, 44.37it/s] 65%|██████▍   | 282/437 [00:07<00:03, 44.53it/s] 66%|██████▌   | 287/437 [00:07<00:03, 44.76it/s] 67%|██████▋   | 292/437 [00:07<00:03, 44.90it/s] 68%|██████▊   | 297/437 [00:07<00:03, 45.07it/s] 69%|██████▉   | 302/437 [00:07<00:03, 44.93it/s] 70%|███████   | 307/437 [00:08<00:02, 44.57it/s] 71%|███████▏  | 312/437 [00:08<00:02, 44.36it/s] 73%|███████▎  | 317/437 [00:08<00:02, 44.35it/s] 74%|███████▎  | 322/437 [00:08<00:02, 44.45it/s] 75%|███████▍  | 327/437 [00:08<00:02, 44.60it/s] 76%|███████▌  | 332/437 [00:08<00:02, 44.52it/s] 77%|███████▋  | 337/437 [00:08<00:02, 44.91it/s] 78%|███████▊  | 342/437 [00:08<00:02, 44.98it/s] 79%|███████▉  | 347/437 [00:08<00:02, 44.92it/s] 81%|████████  | 352/437 [00:09<00:01, 44.55it/s] 82%|████████▏ | 357/437 [00:09<00:02, 36.58it/s] 83%|████████▎ | 362/437 [00:09<00:01, 38.85it/s] 84%|████████▍ | 367/437 [00:09<00:01, 40.56it/s] 85%|████████▌ | 372/437 [00:09<00:01, 41.93it/s] 86%|████████▋ | 377/437 [00:09<00:01, 42.81it/s] 87%|████████▋ | 382/437 [00:09<00:01, 43.52it/s] 89%|████████▊ | 387/437 [00:09<00:01, 44.06it/s] 90%|████████▉ | 392/437 [00:09<00:01, 44.18it/s] 91%|█████████ | 397/437 [00:10<00:00, 43.93it/s] 92%|█████████▏| 402/437 [00:10<00:00, 43.88it/s] 93%|█████████▎| 407/437 [00:10<00:00, 43.92it/s] 94%|█████████▍| 412/437 [00:10<00:00, 44.24it/s] 95%|█████████▌| 417/437 [00:10<00:00, 44.52it/s] 97%|█████████▋| 422/437 [00:10<00:00, 44.72it/s] 98%|█████████▊| 427/437 [00:10<00:00, 44.90it/s] 99%|█████████▉| 432/437 [00:10<00:00, 45.02it/s]100%|██████████| 437/437 [00:11<00:00, 44.92it/s]100%|██████████| 437/437 [00:11<00:00, 39.69it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 02:09:29,868 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:29,868 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:29,868 >>   eval_loss               =     1.0764
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:29,868 >>   eval_runtime            = 0:00:11.02
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:29,868 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:29,868 >>   eval_samples_per_second =    316.625
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:29,868 >>   eval_steps_per_second   =     39.623
[INFO|trainer_pt_utils.py:913] 2023-08-29 02:09:29,868 >>   perplexity              =     2.9342
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:51,156 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:51,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:51,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:51,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:51,233 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:09:52,620 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:09:52,621 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:09:53,066 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:09:54,291 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:09:54,504 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:57,982 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:58,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:58,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:58,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:09:58,130 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:09:59,165 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:09:59,166 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:10:00,076 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:10:00,450 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:10:00,450 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-117
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:05,  1.49it/s]Extractor Predicting: 10it [00:06,  1.52it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.48it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.48it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.55it/s]Extractor Predicting: 21it [00:13,  1.49it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:17,  1.56it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:18,  1.57it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.52it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:23,  1.52it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.54it/s]Extractor Predicting: 38it [00:24,  1.55it/s]Extractor Predicting: 39it [00:25,  1.50it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:26,  1.55it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:28,  1.51it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:31,  1.54it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:32,  1.53it/s]Extractor Predicting: 51it [00:33,  1.50it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:34,  1.52it/s]Extractor Predicting: 54it [00:35,  1.45it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:38,  1.56it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:39,  1.49it/s]Extractor Predicting: 61it [00:40,  1.49it/s]Extractor Predicting: 62it [00:40,  1.50it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.43it/s]Extractor Predicting: 65it [00:42,  1.46it/s]Extractor Predicting: 66it [00:43,  1.45it/s]Extractor Predicting: 67it [00:44,  1.46it/s]Extractor Predicting: 68it [00:44,  1.46it/s]Extractor Predicting: 69it [00:45,  1.39it/s]Extractor Predicting: 70it [00:46,  1.41it/s]Extractor Predicting: 71it [00:47,  1.43it/s]Extractor Predicting: 72it [00:47,  1.42it/s]Extractor Predicting: 73it [00:48,  1.45it/s]Extractor Predicting: 74it [00:49,  1.42it/s]Extractor Predicting: 75it [00:49,  1.43it/s]Extractor Predicting: 76it [00:50,  1.45it/s]Extractor Predicting: 77it [00:51,  1.44it/s]Extractor Predicting: 78it [00:51,  1.48it/s]Extractor Predicting: 79it [00:52,  1.34it/s]Extractor Predicting: 80it [00:53,  1.32it/s]Extractor Predicting: 81it [00:54,  1.36it/s]Extractor Predicting: 82it [00:54,  1.41it/s]Extractor Predicting: 83it [00:55,  1.44it/s]Extractor Predicting: 84it [00:56,  1.48it/s]Extractor Predicting: 85it [00:57,  1.43it/s]Extractor Predicting: 86it [00:57,  1.41it/s]Extractor Predicting: 87it [00:58,  1.44it/s]Extractor Predicting: 88it [00:59,  1.49it/s]Extractor Predicting: 89it [00:59,  1.55it/s]Extractor Predicting: 90it [01:00,  1.50it/s]Extractor Predicting: 91it [01:00,  1.57it/s]Extractor Predicting: 92it [01:01,  1.63it/s]Extractor Predicting: 93it [01:01,  1.68it/s]Extractor Predicting: 94it [01:02,  1.72it/s]Extractor Predicting: 95it [01:03,  1.66it/s]Extractor Predicting: 96it [01:03,  1.64it/s]Extractor Predicting: 97it [01:04,  1.62it/s]Extractor Predicting: 98it [01:05,  1.62it/s]Extractor Predicting: 99it [01:05,  1.69it/s]Extractor Predicting: 100it [01:06,  1.69it/s]Extractor Predicting: 101it [01:06,  1.63it/s]Extractor Predicting: 102it [01:07,  1.60it/s]Extractor Predicting: 103it [01:08,  1.60it/s]Extractor Predicting: 104it [01:08,  1.57it/s]Extractor Predicting: 105it [01:09,  1.58it/s]Extractor Predicting: 106it [01:10,  1.55it/s]Extractor Predicting: 107it [01:10,  1.54it/s]Extractor Predicting: 108it [01:11,  1.60it/s]Extractor Predicting: 109it [01:11,  1.63it/s]Extractor Predicting: 110it [01:12,  1.65it/s]Extractor Predicting: 111it [01:13,  1.61it/s]Extractor Predicting: 112it [01:13,  1.63it/s]Extractor Predicting: 113it [01:14,  1.67it/s]Extractor Predicting: 114it [01:14,  1.64it/s]Extractor Predicting: 115it [01:15,  1.67it/s]Extractor Predicting: 116it [01:16,  1.55it/s]Extractor Predicting: 117it [01:16,  1.51it/s]Extractor Predicting: 118it [01:17,  1.53it/s]Extractor Predicting: 119it [01:18,  1.52it/s]Extractor Predicting: 120it [01:18,  1.50it/s]Extractor Predicting: 121it [01:19,  1.52it/s]Extractor Predicting: 122it [01:20,  1.48it/s]Extractor Predicting: 123it [01:21,  1.48it/s]Extractor Predicting: 124it [01:21,  1.47it/s]Extractor Predicting: 125it [01:22,  1.49it/s]Extractor Predicting: 126it [01:23,  1.48it/s]Extractor Predicting: 127it [01:23,  1.43it/s]Extractor Predicting: 128it [01:24,  1.46it/s]Extractor Predicting: 129it [01:25,  1.49it/s]Extractor Predicting: 130it [01:25,  1.53it/s]Extractor Predicting: 131it [01:26,  1.50it/s]Extractor Predicting: 132it [01:27,  1.44it/s]Extractor Predicting: 133it [01:27,  1.45it/s]Extractor Predicting: 134it [01:28,  1.47it/s]Extractor Predicting: 135it [01:29,  1.49it/s]Extractor Predicting: 136it [01:29,  1.49it/s]Extractor Predicting: 137it [01:30,  1.44it/s]Extractor Predicting: 138it [01:31,  1.48it/s]Extractor Predicting: 139it [01:31,  1.49it/s]Extractor Predicting: 140it [01:32,  1.53it/s]Extractor Predicting: 141it [01:33,  1.50it/s]Extractor Predicting: 142it [01:33,  1.45it/s]Extractor Predicting: 143it [01:34,  1.48it/s]Extractor Predicting: 144it [01:35,  1.52it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:15,397 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:15,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:15,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:15,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:15,512 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:14:17,046 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:14:17,047 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:14:17,826 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:14:19,111 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:14:19,193 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:22,698 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:22,782 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:22,783 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:22,783 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:14:22,783 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:14:23,942 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:14:23,943 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:14:24,707 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:14:25,145 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:14:25,145 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5464601769911505,
  "recall": 0.14146620847651775,
  "score": 0.22474977252047312,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.69it/s]Extractor Predicting: 2it [00:01,  1.61it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.60it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.51it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.59it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.46it/s]Extractor Predicting: 32it [00:20,  1.47it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.54it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:25,  1.61it/s]Extractor Predicting: 40it [00:25,  1.62it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:27,  1.55it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:29,  1.58it/s]Extractor Predicting: 46it [00:29,  1.50it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:31,  1.56it/s]Extractor Predicting: 50it [00:32,  1.56it/s]Extractor Predicting: 51it [00:33,  1.49it/s]Extractor Predicting: 52it [00:33,  1.52it/s]Extractor Predicting: 53it [00:34,  1.54it/s]Extractor Predicting: 54it [00:34,  1.56it/s]Extractor Predicting: 55it [00:35,  1.54it/s]Extractor Predicting: 56it [00:36,  1.51it/s]Extractor Predicting: 57it [00:36,  1.56it/s]Extractor Predicting: 58it [00:37,  1.60it/s]Extractor Predicting: 59it [00:38,  1.64it/s]Extractor Predicting: 60it [00:38,  1.63it/s]Extractor Predicting: 61it [00:39,  1.55it/s]Extractor Predicting: 62it [00:39,  1.57it/s]Extractor Predicting: 63it [00:40,  1.60it/s]Extractor Predicting: 64it [00:41,  1.56it/s]Extractor Predicting: 65it [00:41,  1.58it/s]Extractor Predicting: 66it [00:42,  1.52it/s]Extractor Predicting: 67it [00:43,  1.55it/s]Extractor Predicting: 68it [00:43,  1.62it/s]Extractor Predicting: 69it [00:44,  1.60it/s]Extractor Predicting: 70it [00:44,  1.61it/s]Extractor Predicting: 71it [00:45,  1.61it/s]Extractor Predicting: 72it [00:46,  1.61it/s]Extractor Predicting: 73it [00:46,  1.58it/s]Extractor Predicting: 74it [00:47,  1.51it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:48,  1.56it/s]Extractor Predicting: 77it [00:49,  1.54it/s]Extractor Predicting: 78it [00:50,  1.41it/s]Extractor Predicting: 79it [00:51,  1.41it/s]Extractor Predicting: 80it [00:51,  1.45it/s]Extractor Predicting: 81it [00:52,  1.48it/s]Extractor Predicting: 82it [00:53,  1.49it/s]Extractor Predicting: 83it [00:53,  1.52it/s]Extractor Predicting: 84it [00:54,  1.47it/s]Extractor Predicting: 85it [00:54,  1.53it/s]Extractor Predicting: 86it [00:55,  1.52it/s]Extractor Predicting: 87it [00:56,  1.54it/s]Extractor Predicting: 88it [00:56,  1.54it/s]Extractor Predicting: 89it [00:57,  1.50it/s]Extractor Predicting: 90it [00:58,  1.52it/s]Extractor Predicting: 91it [00:58,  1.53it/s]Extractor Predicting: 92it [00:59,  1.52it/s]Extractor Predicting: 93it [01:00,  1.57it/s]Extractor Predicting: 94it [01:00,  1.45it/s]Extractor Predicting: 95it [01:01,  1.48it/s]Extractor Predicting: 96it [01:02,  1.52it/s]Extractor Predicting: 97it [01:02,  1.55it/s]Extractor Predicting: 98it [01:03,  1.56it/s]Extractor Predicting: 99it [01:04,  1.52it/s]Extractor Predicting: 100it [01:04,  1.56it/s]Extractor Predicting: 101it [01:05,  1.58it/s]Extractor Predicting: 102it [01:06,  1.59it/s]Extractor Predicting: 103it [01:06,  1.55it/s]Extractor Predicting: 104it [01:07,  1.48it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:08,  1.52it/s]Extractor Predicting: 107it [01:09,  1.53it/s]Extractor Predicting: 108it [01:10,  1.55it/s]Extractor Predicting: 109it [01:10,  1.49it/s]Extractor Predicting: 110it [01:11,  1.52it/s]Extractor Predicting: 111it [01:12,  1.52it/s]Extractor Predicting: 112it [01:12,  1.51it/s]Extractor Predicting: 113it [01:13,  1.52it/s]Extractor Predicting: 114it [01:14,  1.46it/s]Extractor Predicting: 115it [01:14,  1.49it/s]Extractor Predicting: 116it [01:15,  1.55it/s]Extractor Predicting: 117it [01:15,  1.57it/s]Extractor Predicting: 118it [01:16,  1.58it/s]Extractor Predicting: 119it [01:17,  1.58it/s]Extractor Predicting: 120it [01:17,  1.58it/s]Extractor Predicting: 121it [01:18,  1.54it/s]Extractor Predicting: 122it [01:19,  1.58it/s]Extractor Predicting: 123it [01:19,  1.61it/s]Extractor Predicting: 124it [01:20,  1.63it/s]Extractor Predicting: 125it [01:20,  1.63it/s]Extractor Predicting: 126it [01:21,  1.55it/s]Extractor Predicting: 127it [01:22,  1.59it/s]Extractor Predicting: 128it [01:22,  1.58it/s]Extractor Predicting: 129it [01:23,  1.57it/s]Extractor Predicting: 130it [01:24,  1.56it/s]Extractor Predicting: 131it [01:24,  1.54it/s]Extractor Predicting: 132it [01:25,  1.56it/s]Extractor Predicting: 133it [01:26,  1.58it/s]Extractor Predicting: 134it [01:26,  1.61it/s]Extractor Predicting: 135it [01:27,  1.62it/s]Extractor Predicting: 136it [01:27,  1.54it/s]Extractor Predicting: 137it [01:28,  1.57it/s]Extractor Predicting: 138it [01:29,  1.56it/s]Extractor Predicting: 139it [01:29,  1.59it/s]Extractor Predicting: 140it [01:30,  1.65it/s]Extractor Predicting: 141it [01:31,  1.58it/s]Extractor Predicting: 142it [01:31,  1.58it/s]Extractor Predicting: 143it [01:32,  1.58it/s]Extractor Predicting: 144it [01:32,  1.59it/s]Extractor Predicting: 145it [01:33,  1.62it/s]Extractor Predicting: 146it [01:34,  1.59it/s]Extractor Predicting: 147it [01:34,  1.63it/s]Extractor Predicting: 148it [01:35,  1.59it/s]Extractor Predicting: 149it [01:36,  1.61it/s]Extractor Predicting: 150it [01:36,  1.61it/s]Extractor Predicting: 151it [01:37,  1.56it/s]Extractor Predicting: 152it [01:37,  1.59it/s]Extractor Predicting: 153it [01:38,  1.57it/s]Extractor Predicting: 154it [01:39,  1.60it/s]Extractor Predicting: 155it [01:39,  1.62it/s]Extractor Predicting: 156it [01:40,  1.58it/s]Extractor Predicting: 157it [01:41,  1.61it/s]Extractor Predicting: 158it [01:41,  1.67it/s]Extractor Predicting: 159it [01:42,  1.65it/s]Extractor Predicting: 160it [01:42,  1.62it/s]Extractor Predicting: 161it [01:43,  1.55it/s]Extractor Predicting: 162it [01:44,  1.56it/s]Extractor Predicting: 163it [01:44,  1.59it/s]Extractor Predicting: 164it [01:45,  1.60it/s]Extractor Predicting: 165it [01:46,  1.63it/s]Extractor Predicting: 166it [01:46,  1.65it/s]Extractor Predicting: 167it [01:47,  1.65it/s]Extractor Predicting: 168it [01:48,  1.44it/s]Extractor Predicting: 169it [01:48,  1.43it/s]Extractor Predicting: 170it [01:49,  1.47it/s]Extractor Predicting: 171it [01:50,  1.52it/s]Extractor Predicting: 172it [01:50,  1.54it/s]Extractor Predicting: 173it [01:51,  1.54it/s]Extractor Predicting: 174it [01:52,  1.49it/s]Extractor Predicting: 175it [01:52,  1.49it/s]Extractor Predicting: 176it [01:53,  1.48it/s]Extractor Predicting: 177it [01:54,  1.54it/s]Extractor Predicting: 178it [01:54,  1.54it/s]Extractor Predicting: 179it [01:55,  1.53it/s]Extractor Predicting: 180it [01:55,  1.58it/s]Extractor Predicting: 181it [01:56,  1.55it/s]Extractor Predicting: 182it [01:57,  1.57it/s]Extractor Predicting: 183it [01:57,  1.57it/s]Extractor Predicting: 184it [01:58,  1.52it/s]Extractor Predicting: 185it [01:59,  1.53it/s]Extractor Predicting: 186it [01:59,  1.54it/s]Extractor Predicting: 187it [02:00,  1.56it/s]Extractor Predicting: 188it [02:01,  1.54it/s]Extractor Predicting: 189it [02:01,  1.49it/s]Extractor Predicting: 190it [02:02,  1.51it/s]Extractor Predicting: 191it [02:03,  1.56it/s]Extractor Predicting: 192it [02:03,  1.60it/s]Extractor Predicting: 193it [02:04,  1.54it/s]Extractor Predicting: 194it [02:05,  1.47it/s]Extractor Predicting: 195it [02:05,  1.52it/s]Extractor Predicting: 196it [02:06,  1.54it/s]Extractor Predicting: 197it [02:07,  1.56it/s]Extractor Predicting: 198it [02:07,  1.55it/s]Extractor Predicting: 199it [02:08,  1.52it/s]Extractor Predicting: 200it [02:08,  1.59it/s]Extractor Predicting: 201it [02:09,  1.63it/s]Extractor Predicting: 202it [02:10,  1.63it/s]Extractor Predicting: 203it [02:10,  1.61it/s]Extractor Predicting: 204it [02:11,  1.55it/s]Extractor Predicting: 205it [02:12,  1.57it/s]Extractor Predicting: 206it [02:12,  1.57it/s]Extractor Predicting: 207it [02:13,  1.58it/s]Extractor Predicting: 208it [02:13,  1.56it/s]Extractor Predicting: 209it [02:14,  1.50it/s]Extractor Predicting: 210it [02:15,  1.52it/s]Extractor Predicting: 211it [02:15,  1.54it/s]Extractor Predicting: 212it [02:16,  1.57it/s]Extractor Predicting: 213it [02:17,  1.57it/s]Extractor Predicting: 214it [02:17,  1.55it/s]Extractor Predicting: 215it [02:18,  1.56it/s]Extractor Predicting: 216it [02:19,  1.59it/s]Extractor Predicting: 217it [02:19,  1.54it/s]Extractor Predicting: 218it [02:20,  1.54it/s]Extractor Predicting: 219it [02:21,  1.54it/s]Extractor Predicting: 220it [02:21,  1.57it/s]Extractor Predicting: 221it [02:22,  1.58it/s]Extractor Predicting: 222it [02:23,  1.50it/s]Extractor Predicting: 223it [02:23,  1.56it/s]Extractor Predicting: 224it [02:24,  1.57it/s]Extractor Predicting: 225it [02:24,  1.57it/s]Extractor Predicting: 226it [02:25,  1.56it/s]Extractor Predicting: 227it [02:26,  1.52it/s]Extractor Predicting: 228it [02:26,  1.55it/s]Extractor Predicting: 229it [02:27,  1.58it/s]Extractor Predicting: 230it [02:28,  1.61it/s]Extractor Predicting: 231it [02:28,  1.60it/s]Extractor Predicting: 232it [02:29,  1.49it/s]Extractor Predicting: 233it [02:30,  1.53it/s]Extractor Predicting: 234it [02:30,  1.57it/s]Extractor Predicting: 235it [02:31,  1.59it/s]Extractor Predicting: 236it [02:31,  1.60it/s]Extractor Predicting: 237it [02:32,  1.55it/s]Extractor Predicting: 238it [02:33,  1.56it/s]Extractor Predicting: 239it [02:33,  1.56it/s]Extractor Predicting: 240it [02:34,  1.60it/s]Extractor Predicting: 241it [02:35,  1.61it/s]Extractor Predicting: 242it [02:35,  1.55it/s]Extractor Predicting: 243it [02:36,  1.57it/s]Extractor Predicting: 244it [02:37,  1.59it/s]Extractor Predicting: 245it [02:37,  1.59it/s]Extractor Predicting: 246it [02:38,  1.60it/s]Extractor Predicting: 247it [02:38,  1.54it/s]Extractor Predicting: 248it [02:39,  1.56it/s]Extractor Predicting: 249it [02:40,  1.59it/s]Extractor Predicting: 250it [02:40,  1.62it/s]Extractor Predicting: 251it [02:41,  1.58it/s]Extractor Predicting: 252it [02:42,  1.48it/s]Extractor Predicting: 253it [02:42,  1.57it/s]Extractor Predicting: 254it [02:43,  1.58it/s]Extractor Predicting: 255it [02:44,  1.57it/s]Extractor Predicting: 256it [02:44,  1.57it/s]Extractor Predicting: 257it [02:45,  1.53it/s]Extractor Predicting: 258it [02:45,  1.58it/s]Extractor Predicting: 259it [02:46,  1.62it/s]Extractor Predicting: 260it [02:47,  1.61it/s]Extractor Predicting: 261it [02:47,  1.59it/s]Extractor Predicting: 262it [02:48,  1.62it/s]Extractor Predicting: 263it [02:49,  1.62it/s]Extractor Predicting: 264it [02:49,  1.64it/s]Extractor Predicting: 265it [02:50,  1.57it/s]Extractor Predicting: 266it [02:50,  1.62it/s]Extractor Predicting: 267it [02:51,  1.59it/s]Extractor Predicting: 268it [02:52,  1.44it/s]Extractor Predicting: 269it [02:52,  1.50it/s]Extractor Predicting: 270it [02:53,  1.48it/s]Extractor Predicting: 271it [02:54,  1.56it/s]Extractor Predicting: 272it [02:54,  1.62it/s]Extractor Predicting: 273it [02:55,  1.61it/s]Extractor Predicting: 274it [02:56,  1.62it/s]Extractor Predicting: 275it [02:56,  1.56it/s]Extractor Predicting: 276it [02:57,  1.58it/s]Extractor Predicting: 277it [02:57,  1.60it/s]Extractor Predicting: 278it [02:58,  1.61it/s]Extractor Predicting: 279it [02:59,  1.59it/s]Extractor Predicting: 280it [02:59,  1.54it/s]Extractor Predicting: 281it [03:00,  1.53it/s]Extractor Predicting: 281it [03:00,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:50,303 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:50,404 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:50,404 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:50,404 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:50,404 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:17:51,941 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:17:51,942 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:17:52,630 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:17:53,812 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:17:53,874 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:56,309 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:56,311 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:56,311 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:56,311 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:17:56,311 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:17:57,583 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:17:57,716 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:17:58,204 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:17:58,628 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:17:58,628 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.48809934460158677,
  "recall": 0.2098472489989619,
  "score": 0.293507571043352,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.35it/s]Extractor Predicting: 2it [00:01,  1.40it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.51it/s]Extractor Predicting: 6it [00:03,  1.72it/s]Extractor Predicting: 6it [00:03,  1.58it/s]
[INFO|configuration_utils.py:515] 2023-08-29 02:18:12,182 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:18:12,441 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 02:18:12,884 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:18:12,885 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 02:18:13,037 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 02:18:42,345 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 02:18:42,431 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 02:18:43,442 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 02:18:43,444 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 02:18:43,744 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:43,932 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:43,932 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:43,932 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:43,932 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:43,932 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 02:18:43,932 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.11904761904761904,
  "recall": 0.019455252918287938,
  "score": 0.033444816053511704,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/15 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 02:18:44,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:45,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:45,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:46,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:47,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:47,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:48,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:48,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:49,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:50,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:50,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:51,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:51,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:52,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:52,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:53,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:54,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:54,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:55,271 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:55,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   7%|▋         | 1/15 [00:11<02:45, 11.85s/it][WARNING|generation_utils.py:914] 2023-08-29 02:18:56,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:57,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:57,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:58,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:59,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:18:59,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:00,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:01,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:01,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:02,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:03,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:04,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:04,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:05,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:05,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:06,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:07,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:07,835 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:08,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:09,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:09,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  13%|█▎        | 2/15 [00:25<02:50, 13.12s/it][WARNING|generation_utils.py:914] 2023-08-29 02:19:10,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:10,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:11,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:12,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:12,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:13,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:14,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:14,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:15,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:15,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:16,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:17,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:17,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:18,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:18,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:19,371 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:19,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:20,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:20,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:21,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:21,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 3/15 [00:37<02:31, 12.64s/it][WARNING|generation_utils.py:914] 2023-08-29 02:19:22,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:23,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:23,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:24,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:24,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:25,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:25,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:26,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:26,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:27,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:28,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:28,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:29,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:29,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:30,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:30,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:31,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:31,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:32,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:33,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:33,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  27%|██▋       | 4/15 [00:49<02:14, 12.27s/it][WARNING|generation_utils.py:914] 2023-08-29 02:19:34,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:34,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:35,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:36,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:36,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:37,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:37,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:38,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:38,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:39,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:40,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:40,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:41,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:42,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:42,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:43,130 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:43,662 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:44,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:44,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:45,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:46,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  33%|███▎      | 5/15 [01:01<02:02, 12.26s/it][WARNING|generation_utils.py:914] 2023-08-29 02:19:46,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:47,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:47,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:48,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:48,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:49,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:49,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:50,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:51,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:51,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:52,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:52,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:53,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:54,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:54,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:55,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:55,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:56,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:56,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:57,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:58,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 6/15 [01:14<01:49, 12.22s/it][WARNING|generation_utils.py:914] 2023-08-29 02:19:58,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:59,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:19:59,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:00,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:01,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:01,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:02,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:03,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:03,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:04,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:04,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:05,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:06,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:07,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:07,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:08,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:08,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:09,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:10,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:10,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:11,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:11,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  47%|████▋     | 7/15 [01:27<01:42, 12.77s/it][WARNING|generation_utils.py:914] 2023-08-29 02:20:12,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:13,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:13,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:14,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:14,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:15,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:15,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:16,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:17,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:17,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:18,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:18,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:19,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:19,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:20,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:21,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:21,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:22,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:23,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:23,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:24,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  53%|█████▎    | 8/15 [01:40<01:28, 12.63s/it][WARNING|generation_utils.py:914] 2023-08-29 02:20:24,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:25,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:26,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:26,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:27,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:28,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:28,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:29,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:29,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:30,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:31,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:31,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:32,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:33,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:33,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:34,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:35,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:35,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:36,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:37,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:37,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 9/15 [01:53<01:17, 12.92s/it][WARNING|generation_utils.py:914] 2023-08-29 02:20:38,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:38,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:39,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:39,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:40,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:40,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:41,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:41,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:42,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:42,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:43,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:43,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:44,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:45,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:45,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:46,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:46,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:46,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:47,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:47,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:48,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  67%|██████▋   | 10/15 [02:04<01:00, 12.20s/it][WARNING|generation_utils.py:914] 2023-08-29 02:20:49,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:49,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:50,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:50,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:51,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:52,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:53,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:53,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:54,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:55,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:55,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:56,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:56,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:57,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:58,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:58,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:20:59,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:00,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:00,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:01,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:02,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  73%|███████▎  | 11/15 [02:18<00:50, 12.72s/it][WARNING|generation_utils.py:914] 2023-08-29 02:21:02,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:03,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:04,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:04,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:05,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:05,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:06,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:07,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:07,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:08,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:08,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:09,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:10,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:10,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:11,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:11,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:12,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:12,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:13,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:14,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:14,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:15,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 12/15 [02:31<00:38, 12.80s/it][WARNING|generation_utils.py:914] 2023-08-29 02:21:15,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:16,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:17,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:17,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:18,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:18,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:19,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:19,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:20,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:20,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:21,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:22,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:22,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:23,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:23,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:24,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:24,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:25,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:26,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:26,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:27,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:27,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  87%|████████▋ | 13/15 [02:43<00:25, 12.65s/it][WARNING|generation_utils.py:914] 2023-08-29 02:21:28,235 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:28,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:29,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:30,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:30,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:31,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:31,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:32,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:32,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:33,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:33,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:34,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:34,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:35,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:36,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:36,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:37,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:37,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:38,368 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:38,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:39,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:40,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  93%|█████████▎| 14/15 [02:55<00:12, 12.57s/it][WARNING|generation_utils.py:914] 2023-08-29 02:21:40,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:41,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:41,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:42,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:43,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:43,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:44,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:44,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:45,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:46,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:46,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:47,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:48,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:48,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:49,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:50,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:50,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:51,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:51,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:52,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 02:21:53,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 15/15 [03:09<00:00, 12.75s/it]Generating: 100%|██████████| 15/15 [03:09<00:00, 12.61s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:07,672 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:07,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:07,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:07,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:07,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:22:09,143 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:22:09,144 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:22:09,835 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:22:11,183 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:22:11,267 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:15,013 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:15,132 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:15,132 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:15,133 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:22:15,133 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:22:16,194 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:22:16,195 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:22:16,937 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:22:17,340 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:22:17,340 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 608, 'raw': 640}
{'prompt': 'Relation : composer .', 'success_rate': 0.95, 'errors': {''}}
['Relation : main subject . Context : Later in the year ( October 1887 ) , he wrote a study of the philosopher John Locke , which was published in 1889 . Head Entity : John Locke , Tail Entity : philosophical .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 416, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 622, 'raw': 672}
{'prompt': 'Relation : main subject .', 'success_rate': 0.9255952380952381, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 345, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 433, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 485, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 600, 'raw': 672}
{'prompt': 'Relation : participant in .', 'success_rate': 0.8928571428571429, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 494, 'raw': 544}
{'target': 600, 'success': 524, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : platform .', 'success_rate': 0.9107142857142857, 'errors': {'', '(\'Sonic the Hedgehog 2\', \'platform\', \'\', \'" Sonic the Hedgehog 2 " is a spin - off to " Sonic the Hedgehog 3 " and features a 3D racing gameplay .\')'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 526, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : taxon rank .', 'success_rate': 0.9151785714285714, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 462, 'raw': 512}
{'target': 600, 'success': 490, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : competition class .', 'success_rate': 0.8943452380952381, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : location .', 'success_rate': 0.8522727272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 318, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 401, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 602, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8958333333333334, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 552, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : nominated for .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 269, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 478, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9285714285714286, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 353, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 436, 'raw': 512}
{'target': 600, 'success': 465, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : owned by .', 'success_rate': 0.8579545454545454, 'errors': {''}}
['Relation : position held . Context : Later in the year , the leader of the Labour Party , Owen Smith , made a public denunciation of George Osborne on 24 June 2014 , following a meeting in London with Smith , who had been Labour leader in 2010 . Head Entity : Owen Smith , Tail Entity : Leader of Labour Party .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 253, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 339, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 515, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 571, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 629, 'raw': 704}
{'prompt': 'Relation : position held .', 'success_rate': 0.8934659090909091, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 468, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 606, 'raw': 704}
{'prompt': 'Relation : position played on team / speciality .', 'success_rate': 0.8607954545454546, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 474, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 593, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : religion .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/4_ext.jsonl'}}
estimate vocab size: 9032
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9132, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.44it/s]Extractor Estimating: 2it [00:01,  1.46it/s]Extractor Estimating: 3it [00:02,  1.51it/s]Extractor Estimating: 4it [00:02,  1.59it/s]Extractor Estimating: 5it [00:03,  1.61it/s]Extractor Estimating: 6it [00:03,  1.54it/s]Extractor Estimating: 7it [00:04,  1.55it/s]Extractor Estimating: 8it [00:05,  1.58it/s]Extractor Estimating: 9it [00:05,  1.56it/s]Extractor Estimating: 10it [00:06,  1.58it/s]Extractor Estimating: 11it [00:07,  1.51it/s]Extractor Estimating: 12it [00:07,  1.47it/s]Extractor Estimating: 13it [00:08,  1.51it/s]Extractor Estimating: 14it [00:09,  1.53it/s]Extractor Estimating: 15it [00:09,  1.55it/s]Extractor Estimating: 16it [00:10,  1.60it/s]Extractor Estimating: 17it [00:10,  1.58it/s]Extractor Estimating: 18it [00:11,  1.55it/s]Extractor Estimating: 19it [00:12,  1.57it/s]Extractor Estimating: 20it [00:12,  1.61it/s]Extractor Estimating: 21it [00:13,  1.67it/s]Extractor Estimating: 22it [00:14,  1.65it/s]Extractor Estimating: 23it [00:14,  1.57it/s]Extractor Estimating: 24it [00:15,  1.61it/s]Extractor Estimating: 25it [00:15,  1.60it/s]Extractor Estimating: 26it [00:16,  1.54it/s]Extractor Estimating: 27it [00:17,  1.42it/s]Extractor Estimating: 28it [00:18,  1.40it/s]Extractor Estimating: 29it [00:18,  1.45it/s]Extractor Estimating: 30it [00:19,  1.50it/s]Extractor Estimating: 31it [00:20,  1.52it/s]Extractor Estimating: 32it [00:20,  1.47it/s]Extractor Estimating: 33it [00:21,  1.46it/s]Extractor Estimating: 34it [00:22,  1.47it/s]Extractor Estimating: 35it [00:22,  1.51it/s]Extractor Estimating: 36it [00:23,  1.48it/s]Extractor Estimating: 37it [00:24,  1.55it/s]Extractor Estimating: 38it [00:24,  1.41it/s]Extractor Estimating: 39it [00:25,  1.49it/s]Extractor Estimating: 40it [00:26,  1.53it/s]Extractor Estimating: 41it [00:26,  1.52it/s]Extractor Estimating: 42it [00:27,  1.58it/s]Extractor Estimating: 43it [00:28,  1.54it/s]Extractor Estimating: 44it [00:28,  1.55it/s]Extractor Estimating: 45it [00:29,  1.55it/s]Extractor Estimating: 46it [00:29,  1.57it/s]Extractor Estimating: 47it [00:30,  1.56it/s]Extractor Estimating: 48it [00:31,  1.46it/s]Extractor Estimating: 49it [00:32,  1.46it/s]Extractor Estimating: 50it [00:32,  1.50it/s]Extractor Estimating: 51it [00:33,  1.59it/s]Extractor Estimating: 52it [00:33,  1.64it/s]Extractor Estimating: 53it [00:34,  1.55it/s]Extractor Estimating: 54it [00:35,  1.63it/s]Extractor Estimating: 55it [00:35,  1.64it/s]Extractor Estimating: 56it [00:36,  1.69it/s]Extractor Estimating: 57it [00:36,  1.71it/s]Extractor Estimating: 58it [00:37,  1.78it/s]Extractor Estimating: 59it [00:38,  1.62it/s]Extractor Estimating: 60it [00:38,  1.75it/s]Extractor Estimating: 61it [00:39,  1.74it/s]Extractor Estimating: 62it [00:39,  1.77it/s]Extractor Estimating: 63it [00:40,  1.84it/s]Extractor Estimating: 64it [00:40,  1.81it/s]Extractor Estimating: 65it [00:41,  1.80it/s]Extractor Estimating: 66it [00:41,  1.71it/s]Extractor Estimating: 67it [00:42,  1.72it/s]Extractor Estimating: 68it [00:43,  1.77it/s]Extractor Estimating: 69it [00:43,  1.84it/s]Extractor Estimating: 70it [00:44,  1.84it/s]Extractor Estimating: 71it [00:44,  1.85it/s]Extractor Estimating: 72it [00:45,  1.72it/s]Extractor Estimating: 73it [00:45,  1.77it/s]Extractor Estimating: 74it [00:46,  1.79it/s]Extractor Estimating: 75it [00:46,  1.84it/s]Extractor Estimating: 76it [00:47,  1.81it/s]Extractor Estimating: 77it [00:47,  1.86it/s]Extractor Estimating: 78it [00:48,  1.76it/s]Extractor Estimating: 79it [00:49,  1.78it/s]Extractor Estimating: 80it [00:49,  1.82it/s]Extractor Estimating: 81it [00:50,  1.86it/s]Extractor Estimating: 82it [00:50,  1.82it/s]Extractor Estimating: 83it [00:51,  1.84it/s]Extractor Estimating: 84it [00:51,  1.72it/s]Extractor Estimating: 85it [00:52,  1.77it/s]Extractor Estimating: 86it [00:53,  1.65it/s]Extractor Estimating: 87it [00:53,  1.63it/s]Extractor Estimating: 88it [00:54,  1.64it/s]Extractor Estimating: 89it [00:55,  1.62it/s]Extractor Estimating: 90it [00:55,  1.63it/s]Extractor Estimating: 91it [00:56,  1.73it/s]Extractor Estimating: 92it [00:56,  1.83it/s]Extractor Estimating: 93it [00:57,  1.82it/s]Extractor Estimating: 94it [00:57,  1.88it/s]Extractor Estimating: 95it [00:58,  1.79it/s]Extractor Estimating: 96it [00:58,  1.66it/s]Extractor Estimating: 97it [00:59,  1.71it/s]Extractor Estimating: 98it [01:00,  1.69it/s]Extractor Estimating: 99it [01:00,  1.65it/s]Extractor Estimating: 100it [01:01,  1.61it/s]Extractor Estimating: 101it [01:01,  1.67it/s]Extractor Estimating: 102it [01:02,  1.67it/s]Extractor Estimating: 103it [01:03,  1.74it/s]Extractor Estimating: 104it [01:03,  1.71it/s]Extractor Estimating: 105it [01:04,  1.77it/s]Extractor Estimating: 106it [01:04,  1.73it/s]Extractor Estimating: 107it [01:05,  1.75it/s]Extractor Estimating: 108it [01:05,  1.77it/s]Extractor Estimating: 109it [01:06,  1.81it/s]Extractor Estimating: 110it [01:06,  1.85it/s]Extractor Estimating: 111it [01:07,  1.83it/s]Extractor Estimating: 112it [01:08,  1.72it/s]Extractor Estimating: 113it [01:08,  1.80it/s]Extractor Estimating: 114it [01:09,  1.82it/s]Extractor Estimating: 115it [01:09,  1.89it/s]Extractor Estimating: 116it [01:10,  1.84it/s]Extractor Estimating: 117it [01:10,  1.92it/s]Extractor Estimating: 118it [01:11,  1.95it/s]Extractor Estimating: 119it [01:11,  1.97it/s]Extractor Estimating: 120it [01:12,  1.96it/s]Extractor Estimating: 121it [01:12,  1.89it/s]Extractor Estimating: 122it [01:13,  1.86it/s]Extractor Estimating: 123it [01:13,  1.86it/s]Extractor Estimating: 124it [01:14,  1.89it/s]Extractor Estimating: 125it [01:15,  1.84it/s]Extractor Estimating: 126it [01:15,  1.85it/s]Extractor Estimating: 127it [01:16,  1.74it/s]Extractor Estimating: 128it [01:16,  1.84it/s]Extractor Estimating: 129it [01:17,  1.90it/s]Extractor Estimating: 130it [01:17,  1.92it/s]Extractor Estimating: 131it [01:18,  1.94it/s]Extractor Estimating: 132it [01:18,  1.96it/s]Extractor Estimating: 133it [01:19,  1.91it/s]Extractor Estimating: 134it [01:19,  1.88it/s]Extractor Estimating: 135it [01:20,  1.88it/s]Extractor Estimating: 136it [01:20,  1.92it/s]Extractor Estimating: 137it [01:21,  1.93it/s]Extractor Estimating: 138it [01:21,  1.95it/s]Extractor Estimating: 139it [01:22,  1.81it/s]Extractor Estimating: 140it [01:23,  1.75it/s]Extractor Estimating: 141it [01:23,  1.86it/s]Extractor Estimating: 142it [01:24,  1.84it/s]Extractor Estimating: 143it [01:24,  1.87it/s]Extractor Estimating: 144it [01:25,  1.91it/s]Extractor Estimating: 145it [01:25,  1.82it/s]Extractor Estimating: 146it [01:26,  1.88it/s]Extractor Estimating: 147it [01:26,  1.92it/s]Extractor Estimating: 148it [01:27,  1.96it/s]Extractor Estimating: 149it [01:27,  1.93it/s]Extractor Estimating: 150it [01:28,  1.88it/s]Extractor Estimating: 151it [01:29,  1.62it/s]Extractor Estimating: 152it [01:29,  1.58it/s]Extractor Estimating: 153it [01:30,  1.58it/s]Extractor Estimating: 154it [01:31,  1.55it/s]Extractor Estimating: 155it [01:31,  1.62it/s]Extractor Estimating: 156it [01:32,  1.57it/s]Extractor Estimating: 157it [01:32,  1.57it/s]Extractor Estimating: 158it [01:33,  1.59it/s]Extractor Estimating: 159it [01:34,  1.61it/s]Extractor Estimating: 160it [01:34,  1.65it/s]Extractor Estimating: 161it [01:35,  1.57it/s]Extractor Estimating: 162it [01:36,  1.56it/s]Extractor Estimating: 163it [01:36,  1.52it/s]Extractor Estimating: 164it [01:37,  1.53it/s]Extractor Estimating: 165it [01:38,  1.54it/s]Extractor Estimating: 166it [01:38,  1.47it/s]Extractor Estimating: 167it [01:39,  1.53it/s]Extractor Estimating: 168it [01:40,  1.49it/s]Extractor Estimating: 169it [01:40,  1.55it/s]Extractor Estimating: 170it [01:41,  1.57it/s]Extractor Estimating: 171it [01:41,  1.58it/s]Extractor Estimating: 172it [01:42,  1.61it/s]Extractor Estimating: 173it [01:43,  1.59it/s]Extractor Estimating: 174it [01:44,  1.47it/s]Extractor Estimating: 175it [01:44,  1.49it/s]Extractor Estimating: 176it [01:45,  1.60it/s]Extractor Estimating: 177it [01:45,  1.63it/s]Extractor Estimating: 178it [01:46,  1.67it/s]Extractor Estimating: 179it [01:46,  1.64it/s]Extractor Estimating: 180it [01:47,  1.67it/s]Extractor Estimating: 181it [01:48,  1.55it/s]Extractor Estimating: 182it [01:48,  1.62it/s]Extractor Estimating: 183it [01:49,  1.63it/s]Extractor Estimating: 184it [01:50,  1.58it/s]Extractor Estimating: 185it [01:50,  1.60it/s]Extractor Estimating: 186it [01:51,  1.74it/s]Extractor Estimating: 187it [01:51,  1.74it/s]Extractor Estimating: 188it [01:52,  1.79it/s]Extractor Estimating: 189it [01:52,  1.86it/s]Extractor Estimating: 190it [01:53,  1.62it/s]Extractor Estimating: 191it [01:54,  1.71it/s]Extractor Estimating: 192it [01:54,  1.73it/s]Extractor Estimating: 193it [01:55,  1.80it/s]Extractor Estimating: 194it [01:55,  1.81it/s]Extractor Estimating: 195it [01:56,  1.78it/s]Extractor Estimating: 196it [01:57,  1.58it/s]Extractor Estimating: 197it [01:57,  1.64it/s]Extractor Estimating: 198it [01:58,  1.71it/s]Extractor Estimating: 199it [01:58,  1.67it/s]Extractor Estimating: 200it [01:59,  1.65it/s]Extractor Estimating: 201it [02:00,  1.45it/s]Extractor Estimating: 202it [02:01,  1.44it/s]Extractor Estimating: 203it [02:01,  1.48it/s]Extractor Estimating: 204it [02:02,  1.55it/s]Extractor Estimating: 205it [02:02,  1.56it/s]Extractor Estimating: 206it [02:03,  1.44it/s]Extractor Estimating: 207it [02:04,  1.44it/s]Extractor Estimating: 208it [02:05,  1.46it/s]Extractor Estimating: 209it [02:05,  1.44it/s]Extractor Estimating: 210it [02:06,  1.48it/s]Extractor Estimating: 211it [02:07,  1.36it/s]Extractor Estimating: 212it [02:07,  1.39it/s]Extractor Estimating: 213it [02:08,  1.42it/s]Extractor Estimating: 214it [02:09,  1.48it/s]Extractor Estimating: 215it [02:09,  1.46it/s]Extractor Estimating: 216it [02:10,  1.34it/s]Extractor Estimating: 217it [02:11,  1.34it/s]Extractor Estimating: 218it [02:12,  1.36it/s]Extractor Estimating: 219it [02:12,  1.39it/s]Extractor Estimating: 220it [02:13,  1.40it/s]Extractor Estimating: 221it [02:14,  1.28it/s]Extractor Estimating: 222it [02:15,  1.32it/s]Extractor Estimating: 223it [02:15,  1.35it/s]Extractor Estimating: 224it [02:16,  1.38it/s]Extractor Estimating: 225it [02:17,  1.38it/s]Extractor Estimating: 226it [02:17,  1.57it/s]Extractor Estimating: 227it [02:18,  1.78it/s]Extractor Estimating: 228it [02:18,  1.91it/s]Extractor Estimating: 229it [02:19,  1.98it/s]Extractor Estimating: 230it [02:19,  2.10it/s]Extractor Estimating: 231it [02:20,  2.06it/s]Extractor Estimating: 232it [02:20,  2.03it/s]Extractor Estimating: 233it [02:20,  2.08it/s]Extractor Estimating: 234it [02:21,  2.16it/s]Extractor Estimating: 235it [02:21,  2.20it/s]Extractor Estimating: 236it [02:22,  2.18it/s]Extractor Estimating: 237it [02:22,  2.18it/s]Extractor Estimating: 238it [02:23,  2.17it/s]Extractor Estimating: 239it [02:23,  1.99it/s]Extractor Estimating: 240it [02:24,  2.03it/s]Extractor Estimating: 241it [02:24,  2.13it/s]Extractor Estimating: 242it [02:25,  2.06it/s]Extractor Estimating: 243it [02:25,  2.05it/s]Extractor Estimating: 244it [02:26,  2.01it/s]Extractor Estimating: 245it [02:26,  1.88it/s]Extractor Estimating: 246it [02:27,  2.00it/s]Extractor Estimating: 247it [02:27,  2.17it/s]Extractor Estimating: 248it [02:28,  2.16it/s]Extractor Estimating: 249it [02:28,  2.17it/s]Extractor Estimating: 250it [02:29,  2.20it/s]Extractor Estimating: 251it [02:29,  1.97it/s]Extractor Estimating: 252it [02:30,  1.73it/s]Extractor Estimating: 253it [02:31,  1.62it/s]Extractor Estimating: 254it [02:31,  1.59it/s]Extractor Estimating: 255it [02:32,  1.60it/s]Extractor Estimating: 256it [02:32,  1.64it/s]Extractor Estimating: 257it [02:33,  1.58it/s]Extractor Estimating: 258it [02:34,  1.55it/s]Extractor Estimating: 259it [02:34,  1.62it/s]Extractor Estimating: 260it [02:35,  1.61it/s]Extractor Estimating: 261it [02:36,  1.63it/s]Extractor Estimating: 262it [02:36,  1.56it/s]Extractor Estimating: 263it [02:37,  1.61it/s]Extractor Estimating: 264it [02:37,  1.64it/s]Extractor Estimating: 265it [02:38,  1.62it/s]Extractor Estimating: 266it [02:39,  1.62it/s]Extractor Estimating: 267it [02:39,  1.55it/s]Extractor Estimating: 268it [02:40,  1.58it/s]Extractor Estimating: 269it [02:41,  1.63it/s]Extractor Estimating: 270it [02:41,  1.48it/s]Extractor Estimating: 271it [02:42,  1.53it/s]Extractor Estimating: 272it [02:43,  1.55it/s]Extractor Estimating: 273it [02:43,  1.48it/s]Extractor Estimating: 274it [02:44,  1.49it/s]Extractor Estimating: 275it [02:45,  1.44it/s]Extractor Estimating: 276it [02:45,  1.50it/s]Extractor Estimating: 277it [02:46,  1.57it/s]Extractor Estimating: 278it [02:47,  1.46it/s]Extractor Estimating: 279it [02:47,  1.55it/s]Extractor Estimating: 280it [02:48,  1.52it/s]Extractor Estimating: 281it [02:49,  1.57it/s]Extractor Estimating: 282it [02:49,  1.63it/s]Extractor Estimating: 283it [02:50,  1.66it/s]Extractor Estimating: 284it [02:50,  1.64it/s]Extractor Estimating: 285it [02:51,  1.57it/s]Extractor Estimating: 286it [02:52,  1.62it/s]Extractor Estimating: 287it [02:52,  1.72it/s]Extractor Estimating: 288it [02:53,  1.76it/s]Extractor Estimating: 289it [02:53,  1.83it/s]Extractor Estimating: 290it [02:54,  1.87it/s]Extractor Estimating: 291it [02:55,  1.56it/s]Extractor Estimating: 292it [02:55,  1.64it/s]Extractor Estimating: 293it [02:56,  1.75it/s]Extractor Estimating: 294it [02:56,  1.81it/s]Extractor Estimating: 295it [02:57,  1.85it/s]Extractor Estimating: 296it [02:57,  1.88it/s]Extractor Estimating: 297it [02:58,  1.80it/s]Extractor Estimating: 298it [02:58,  1.80it/s]Extractor Estimating: 299it [02:59,  1.81it/s]Extractor Estimating: 300it [02:59,  1.77it/s]Extractor Estimating: 301it [03:00,  1.75it/s]Extractor Estimating: 302it [03:01,  1.78it/s]Extractor Estimating: 303it [03:01,  1.80it/s]Extractor Estimating: 304it [03:02,  1.82it/s]Extractor Estimating: 305it [03:02,  1.83it/s]Extractor Estimating: 306it [03:03,  1.87it/s]Extractor Estimating: 307it [03:03,  1.88it/s]Extractor Estimating: 308it [03:04,  1.92it/s]Extractor Estimating: 309it [03:04,  1.79it/s]Extractor Estimating: 310it [03:05,  1.79it/s]Extractor Estimating: 311it [03:06,  1.71it/s]Extractor Estimating: 312it [03:06,  1.80it/s]Extractor Estimating: 313it [03:07,  1.79it/s]Extractor Estimating: 314it [03:07,  1.84it/s]Extractor Estimating: 315it [03:08,  1.79it/s]Extractor Estimating: 316it [03:08,  1.78it/s]Extractor Estimating: 317it [03:09,  1.78it/s]Extractor Estimating: 318it [03:09,  1.81it/s]Extractor Estimating: 319it [03:10,  1.75it/s]Extractor Estimating: 320it [03:11,  1.79it/s]Extractor Estimating: 321it [03:11,  1.70it/s]Extractor Estimating: 322it [03:12,  1.77it/s]Extractor Estimating: 323it [03:12,  1.77it/s]Extractor Estimating: 324it [03:13,  1.80it/s]Extractor Estimating: 325it [03:13,  1.78it/s]Extractor Estimating: 326it [03:14,  1.80it/s]Extractor Estimating: 327it [03:14,  1.80it/s]Extractor Estimating: 328it [03:15,  1.73it/s]Extractor Estimating: 329it [03:16,  1.77it/s]Extractor Estimating: 330it [03:16,  1.78it/s]Extractor Estimating: 331it [03:17,  1.78it/s]Extractor Estimating: 332it [03:17,  1.83it/s]Extractor Estimating: 333it [03:18,  1.85it/s]Extractor Estimating: 334it [03:18,  1.72it/s]Extractor Estimating: 335it [03:19,  1.73it/s]Extractor Estimating: 336it [03:19,  1.83it/s]Extractor Estimating: 337it [03:20,  1.89it/s]Extractor Estimating: 338it [03:20,  1.92it/s]Extractor Estimating: 339it [03:21,  1.92it/s]Extractor Estimating: 340it [03:22,  1.77it/s]Extractor Estimating: 341it [03:22,  1.76it/s]Extractor Estimating: 342it [03:23,  1.81it/s]Extractor Estimating: 343it [03:23,  1.88it/s]Extractor Estimating: 344it [03:24,  1.90it/s]Extractor Estimating: 345it [03:24,  1.92it/s]Extractor Estimating: 346it [03:25,  1.88it/s]Extractor Estimating: 347it [03:25,  1.90it/s]Extractor Estimating: 348it [03:26,  1.90it/s]Extractor Estimating: 349it [03:26,  1.89it/s]Extractor Estimating: 350it [03:27,  1.84it/s]Extractor Estimating: 351it [03:28,  1.84it/s]Extractor Estimating: 352it [03:28,  1.68it/s]Extractor Estimating: 353it [03:29,  1.69it/s]Extractor Estimating: 354it [03:29,  1.72it/s]Extractor Estimating: 355it [03:30,  1.74it/s]Extractor Estimating: 356it [03:31,  1.74it/s]Extractor Estimating: 357it [03:31,  1.75it/s]Extractor Estimating: 358it [03:32,  1.63it/s]Extractor Estimating: 359it [03:32,  1.66it/s]Extractor Estimating: 360it [03:33,  1.68it/s]Extractor Estimating: 361it [03:34,  1.68it/s]Extractor Estimating: 362it [03:34,  1.71it/s]Extractor Estimating: 363it [03:35,  1.66it/s]Extractor Estimating: 364it [03:35,  1.60it/s]Extractor Estimating: 365it [03:36,  1.62it/s]Extractor Estimating: 366it [03:37,  1.52it/s]Extractor Estimating: 367it [03:37,  1.58it/s]Extractor Estimating: 368it [03:38,  1.53it/s]Extractor Estimating: 369it [03:39,  1.58it/s]Extractor Estimating: 370it [03:39,  1.61it/s]Extractor Estimating: 371it [03:40,  1.66it/s]Extractor Estimating: 372it [03:40,  1.71it/s]Extractor Estimating: 373it [03:41,  1.76it/s]Extractor Estimating: 374it [03:42,  1.66it/s]Extractor Estimating: 375it [03:42,  1.64it/s]Extractor Estimating: 375it [03:42,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:30,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:30,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:30,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:30,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:30,141 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 02:26:31,092 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 02:26:31,094 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:26:31,926 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 02:26:33,181 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:26:33,252 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:35,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:35,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:35,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:35,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 02:26:35,632 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 02:26:36,633 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 02:26:36,784 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 02:26:37,285 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 02:26:37,689 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 02:26:37,690 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 04:34:13,679 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 04:34:14,479 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_10_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 7500, 'num_train': 0}
num of filtered data: 7499 mean pseudo reward: 0.9164346629042331
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl'}
train vocab size: 16028
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 16128, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=16128, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.975, loss:731.5784
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.978, loss:678.3967
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.987, loss:708.0650
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 87, avg_time 0.975, loss:672.3818
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 187, avg_time 0.970, loss:651.7949
>> valid entity prec:0.5564, rec:0.5803, f1:0.5681
>> valid relation prec:0.3594, rec:0.1527, f1:0.2144
>> valid relation with NER prec:0.3594, rec:0.1527, f1:0.2144
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 287, avg_time 2.270, loss:646.2545
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 74, avg_time 0.989, loss:602.6246
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 174, avg_time 0.976, loss:640.7515
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 274, avg_time 0.974, loss:642.9445
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 61, avg_time 0.981, loss:636.7190
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5681, rec:0.6162, f1:0.5912
>> valid relation prec:0.3915, rec:0.1716, f1:0.2386
>> valid relation with NER prec:0.3915, rec:0.1716, f1:0.2386
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 161, avg_time 2.296, loss:607.8686
g_step 1200, step 261, avg_time 0.983, loss:628.8958
g_step 1300, step 48, avg_time 0.956, loss:591.0333
g_step 1400, step 148, avg_time 0.992, loss:581.6193
g_step 1500, step 248, avg_time 0.973, loss:578.9710
>> valid entity prec:0.5791, rec:0.6112, f1:0.5947
>> valid relation prec:0.4016, rec:0.1860, f1:0.2542
>> valid relation with NER prec:0.4016, rec:0.1860, f1:0.2542
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 35, avg_time 2.287, loss:571.9524
g_step 1700, step 135, avg_time 0.978, loss:573.1896
g_step 1800, step 235, avg_time 0.964, loss:550.0398
g_step 1900, step 22, avg_time 0.967, loss:518.5929
g_step 2000, step 122, avg_time 0.986, loss:513.6093
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5786, rec:0.5455, f1:0.5616
>> valid relation prec:0.3996, rec:0.1705, f1:0.2390
>> valid relation with NER prec:0.3996, rec:0.1705, f1:0.2390
g_step 2100, step 222, avg_time 2.222, loss:535.8501
g_step 2200, step 9, avg_time 0.974, loss:543.0178
g_step 2300, step 109, avg_time 0.976, loss:470.2923
g_step 2400, step 209, avg_time 0.986, loss:506.8761
g_step 2500, step 309, avg_time 0.979, loss:524.9801
>> valid entity prec:0.5755, rec:0.5506, f1:0.5628
>> valid relation prec:0.4254, rec:0.1487, f1:0.2204
>> valid relation with NER prec:0.4254, rec:0.1487, f1:0.2204
g_step 2600, step 96, avg_time 2.219, loss:452.6372
g_step 2700, step 196, avg_time 0.985, loss:477.3617
g_step 2800, step 296, avg_time 0.988, loss:482.1670
g_step 2900, step 83, avg_time 0.980, loss:434.9499
g_step 3000, step 183, avg_time 0.998, loss:443.2318
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6123, rec:0.5079, f1:0.5552
>> valid relation prec:0.4266, rec:0.1407, f1:0.2116
>> valid relation with NER prec:0.4266, rec:0.1407, f1:0.2116
g_step 3100, step 283, avg_time 2.222, loss:472.3328
g_step 3200, step 70, avg_time 0.972, loss:425.8653
g_step 3300, step 170, avg_time 0.986, loss:437.4537
g_step 3400, step 270, avg_time 0.969, loss:445.2719
g_step 3500, step 57, avg_time 0.965, loss:394.6408
>> valid entity prec:0.5972, rec:0.5627, f1:0.5794
>> valid relation prec:0.3275, rec:0.1716, f1:0.2252
>> valid relation with NER prec:0.3275, rec:0.1716, f1:0.2252
g_step 3600, step 157, avg_time 2.220, loss:417.2889
g_step 3700, step 257, avg_time 0.997, loss:419.6176
g_step 3800, step 44, avg_time 0.969, loss:407.8244
g_step 3900, step 144, avg_time 0.968, loss:383.5103
g_step 4000, step 244, avg_time 0.994, loss:418.4395
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5972, rec:0.5215, f1:0.5568
>> valid relation prec:0.3414, rec:0.1579, f1:0.2159
>> valid relation with NER prec:0.3414, rec:0.1579, f1:0.2159
g_step 4100, step 31, avg_time 2.227, loss:405.3978
g_step 4200, step 131, avg_time 0.961, loss:363.8023
g_step 4300, step 231, avg_time 0.968, loss:378.9860
g_step 4400, step 18, avg_time 0.977, loss:411.2937
g_step 4500, step 118, avg_time 0.975, loss:348.8399
>> valid entity prec:0.5683, rec:0.5123, f1:0.5389
>> valid relation prec:0.3225, rec:0.1513, f1:0.2060
>> valid relation with NER prec:0.3225, rec:0.1513, f1:0.2060
g_step 4600, step 218, avg_time 2.247, loss:364.9214
g_step 4700, step 5, avg_time 0.973, loss:375.1072
g_step 4800, step 105, avg_time 0.981, loss:349.4214
g_step 4900, step 205, avg_time 0.989, loss:347.4847
g_step 5000, step 305, avg_time 0.971, loss:375.7168
learning rate was adjusted to 0.0008
>> valid entity prec:0.5717, rec:0.5679, f1:0.5698
>> valid relation prec:0.2930, rec:0.1679, f1:0.2135
>> valid relation with NER prec:0.2930, rec:0.1679, f1:0.2135
g_step 5100, step 92, avg_time 2.242, loss:323.0412
g_step 5200, step 192, avg_time 0.972, loss:318.8340
g_step 5300, step 292, avg_time 0.980, loss:337.6380
g_step 5400, step 79, avg_time 0.975, loss:307.2730
g_step 5500, step 179, avg_time 0.980, loss:316.9531
>> valid entity prec:0.5961, rec:0.5348, f1:0.5638
>> valid relation prec:0.3413, rec:0.1559, f1:0.2140
>> valid relation with NER prec:0.3413, rec:0.1559, f1:0.2140
g_step 5600, step 279, avg_time 2.234, loss:332.7982
g_step 5700, step 66, avg_time 0.964, loss:310.3392
g_step 5800, step 166, avg_time 0.972, loss:302.8008
g_step 5900, step 266, avg_time 0.992, loss:336.4118
g_step 6000, step 53, avg_time 1.001, loss:299.7660
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5996, rec:0.5148, f1:0.5540
>> valid relation prec:0.3589, rec:0.1490, f1:0.2106
>> valid relation with NER prec:0.3589, rec:0.1490, f1:0.2106
g_step 6100, step 153, avg_time 2.232, loss:287.0591
g_step 6200, step 253, avg_time 0.975, loss:310.5021
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 04:34:14 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 04:34:14 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_04-34-13_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 04:34:16 - WARNING - datasets.builder -   Using custom data configuration default-e576fa8293520ff0
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e576fa8293520ff0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 04:34:23,630 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:34:23,742 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 04:34:23,743 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 04:34:23,744 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 04:34:24,071 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:34:24,284 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:34:24,284 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:34:24,284 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:34:24,284 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:34:24,284 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 04:34:24,284 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 04:34:25,387 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 04:34:28,720 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 04:34:28,839 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e576fa8293520ff0/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:03,  2.00ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.13ba/s] 38%|███▊      | 3/8 [00:00<00:01,  3.82ba/s] 50%|█████     | 4/8 [00:01<00:00,  4.26ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.56ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  3.98ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  4.29ba/s]100%|██████████| 8/8 [00:01<00:00,  4.26ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.98ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.99ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.55ba/s]100%|██████████| 4/4 [00:01<00:00,  4.70ba/s]100%|██████████| 4/4 [00:01<00:00,  3.82ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.59ba/s] 38%|███▊      | 3/8 [00:00<00:00,  6.10ba/s] 62%|██████▎   | 5/8 [00:00<00:00,  8.12ba/s] 88%|████████▊ | 7/8 [00:00<00:00,  9.35ba/s]100%|██████████| 8/8 [00:00<00:00,  8.49ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.60ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.47ba/s]100%|██████████| 4/4 [00:00<00:00,  7.90ba/s]100%|██████████| 4/4 [00:00<00:00,  6.32ba/s]
[INFO|trainer.py:414] 2023-08-29 04:34:38,145 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 04:34:38,462 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 04:34:38,463 >>   Num examples = 7500
[INFO|trainer.py:1149] 2023-08-29 04:34:38,463 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 04:34:38,463 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 04:34:38,463 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 04:34:38,463 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 04:34:38,463 >>   Total optimization steps = 585
  0%|          | 0/585 [00:00<?, ?it/s]  0%|          | 1/585 [00:00<02:55,  3.33it/s]  0%|          | 2/585 [00:00<02:51,  3.40it/s]  1%|          | 3/585 [00:00<02:50,  3.42it/s]  1%|          | 4/585 [00:01<03:18,  2.93it/s]  1%|          | 5/585 [00:01<03:07,  3.09it/s]  1%|          | 6/585 [00:01<03:00,  3.20it/s]  1%|          | 7/585 [00:02<02:56,  3.27it/s]  1%|▏         | 8/585 [00:02<02:53,  3.32it/s]  2%|▏         | 9/585 [00:02<02:51,  3.36it/s]  2%|▏         | 10/585 [00:03<02:50,  3.38it/s]  2%|▏         | 11/585 [00:03<02:49,  3.40it/s]  2%|▏         | 12/585 [00:03<02:48,  3.41it/s]  2%|▏         | 13/585 [00:03<02:47,  3.42it/s]  2%|▏         | 14/585 [00:04<03:07,  3.04it/s]  3%|▎         | 15/585 [00:04<03:01,  3.15it/s]  3%|▎         | 16/585 [00:04<02:56,  3.23it/s]  3%|▎         | 17/585 [00:05<02:52,  3.29it/s]  3%|▎         | 18/585 [00:05<02:50,  3.33it/s]  3%|▎         | 19/585 [00:05<02:48,  3.36it/s]  3%|▎         | 20/585 [00:06<02:46,  3.39it/s]  4%|▎         | 21/585 [00:06<02:46,  3.40it/s]  4%|▍         | 22/585 [00:06<02:45,  3.41it/s]  4%|▍         | 23/585 [00:06<02:44,  3.42it/s]  4%|▍         | 24/585 [00:07<02:58,  3.15it/s]  4%|▍         | 25/585 [00:07<02:53,  3.23it/s]  4%|▍         | 26/585 [00:07<02:49,  3.29it/s]  5%|▍         | 27/585 [00:08<02:47,  3.33it/s]  5%|▍         | 28/585 [00:08<02:45,  3.36it/s]  5%|▍         | 29/585 [00:08<02:44,  3.39it/s]  5%|▌         | 30/585 [00:09<02:43,  3.40it/s]  5%|▌         | 31/585 [00:09<02:42,  3.41it/s]  5%|▌         | 32/585 [00:09<02:41,  3.42it/s]  6%|▌         | 33/585 [00:09<02:41,  3.42it/s]  6%|▌         | 34/585 [00:10<02:40,  3.43it/s]  6%|▌         | 35/585 [00:10<02:56,  3.12it/s]  6%|▌         | 36/585 [00:10<02:50,  3.22it/s]  6%|▋         | 37/585 [00:11<02:46,  3.30it/s]  6%|▋         | 38/585 [00:11<02:43,  3.35it/s]  7%|▋         | 39/585 [00:11<02:41,  3.39it/s]  7%|▋         | 40/585 [00:12<02:39,  3.42it/s]  7%|▋         | 41/585 [00:12<02:38,  3.44it/s]  7%|▋         | 42/585 [00:12<02:37,  3.45it/s]  7%|▋         | 43/585 [00:12<02:36,  3.47it/s]  8%|▊         | 44/585 [00:13<02:35,  3.47it/s]  8%|▊         | 45/585 [00:13<02:35,  3.48it/s]  8%|▊         | 46/585 [00:13<02:50,  3.16it/s]  8%|▊         | 47/585 [00:14<02:45,  3.25it/s]  8%|▊         | 48/585 [00:14<02:41,  3.32it/s]  8%|▊         | 49/585 [00:14<02:39,  3.37it/s]  9%|▊         | 50/585 [00:15<02:37,  3.40it/s]  9%|▊         | 51/585 [00:15<02:36,  3.42it/s]  9%|▉         | 52/585 [00:15<02:34,  3.44it/s]  9%|▉         | 53/585 [00:15<02:34,  3.45it/s]  9%|▉         | 54/585 [00:16<02:33,  3.46it/s]  9%|▉         | 55/585 [00:16<02:32,  3.47it/s] 10%|▉         | 56/585 [00:16<02:32,  3.47it/s] 10%|▉         | 57/585 [00:17<02:46,  3.18it/s] 10%|▉         | 58/585 [00:17<02:41,  3.26it/s] 10%|█         | 59/585 [00:17<02:38,  3.33it/s] 10%|█         | 60/585 [00:17<02:35,  3.37it/s] 10%|█         | 61/585 [00:18<02:33,  3.41it/s] 11%|█         | 62/585 [00:18<02:32,  3.43it/s] 11%|█         | 63/585 [00:18<02:31,  3.45it/s] 11%|█         | 64/585 [00:19<02:30,  3.46it/s] 11%|█         | 65/585 [00:19<02:30,  3.46it/s] 11%|█▏        | 66/585 [00:19<02:29,  3.47it/s] 11%|█▏        | 67/585 [00:19<02:29,  3.47it/s] 12%|█▏        | 68/585 [00:20<02:46,  3.10it/s] 12%|█▏        | 69/585 [00:20<02:41,  3.20it/s] 12%|█▏        | 70/585 [00:20<02:36,  3.29it/s] 12%|█▏        | 71/585 [00:21<02:33,  3.34it/s] 12%|█▏        | 72/585 [00:21<02:31,  3.38it/s] 12%|█▏        | 73/585 [00:21<02:29,  3.41it/s] 13%|█▎        | 74/585 [00:22<02:28,  3.43it/s] 13%|█▎        | 75/585 [00:22<02:27,  3.45it/s] 13%|█▎        | 76/585 [00:22<02:27,  3.46it/s] 13%|█▎        | 77/585 [00:22<02:26,  3.47it/s] 13%|█▎        | 78/585 [00:23<02:25,  3.47it/s] 14%|█▎        | 79/585 [00:23<02:45,  3.05it/s] 14%|█▎        | 80/585 [00:23<02:39,  3.17it/s] 14%|█▍        | 81/585 [00:24<02:34,  3.25it/s] 14%|█▍        | 82/585 [00:24<02:31,  3.32it/s] 14%|█▍        | 83/585 [00:24<02:29,  3.37it/s] 14%|█▍        | 84/585 [00:25<02:27,  3.40it/s] 15%|█▍        | 85/585 [00:25<02:26,  3.42it/s] 15%|█▍        | 86/585 [00:25<02:24,  3.44it/s] 15%|█▍        | 87/585 [00:25<02:24,  3.46it/s] 15%|█▌        | 88/585 [00:26<02:23,  3.47it/s] 15%|█▌        | 89/585 [00:26<02:40,  3.08it/s] 15%|█▌        | 90/585 [00:26<02:35,  3.19it/s] 16%|█▌        | 91/585 [00:27<02:30,  3.27it/s] 16%|█▌        | 92/585 [00:27<02:28,  3.33it/s] 16%|█▌        | 93/585 [00:27<02:25,  3.37it/s] 16%|█▌        | 94/585 [00:28<02:24,  3.40it/s] 16%|█▌        | 95/585 [00:28<02:23,  3.43it/s] 16%|█▋        | 96/585 [00:28<02:21,  3.44it/s] 17%|█▋        | 97/585 [00:28<02:21,  3.46it/s] 17%|█▋        | 98/585 [00:29<02:20,  3.47it/s] 17%|█▋        | 99/585 [00:29<02:20,  3.47it/s] 17%|█▋        | 100/585 [00:29<02:32,  3.17it/s] 17%|█▋        | 101/585 [00:30<02:28,  3.26it/s] 17%|█▋        | 102/585 [00:30<02:25,  3.32it/s] 18%|█▊        | 103/585 [00:30<02:23,  3.36it/s] 18%|█▊        | 104/585 [00:31<02:22,  3.39it/s] 18%|█▊        | 105/585 [00:31<02:20,  3.41it/s] 18%|█▊        | 106/585 [00:31<02:20,  3.42it/s] 18%|█▊        | 107/585 [00:32<02:34,  3.10it/s] 18%|█▊        | 108/585 [00:32<02:28,  3.21it/s] 19%|█▊        | 109/585 [00:32<02:24,  3.28it/s] 19%|█▉        | 110/585 [00:33<02:35,  3.05it/s] 19%|█▉        | 111/585 [00:33<02:29,  3.17it/s] 19%|█▉        | 112/585 [00:33<02:25,  3.26it/s] 19%|█▉        | 113/585 [00:33<02:22,  3.32it/s] 19%|█▉        | 114/585 [00:34<02:20,  3.36it/s] 20%|█▉        | 115/585 [00:34<02:18,  3.40it/s] 20%|█▉        | 116/585 [00:34<02:17,  3.42it/s] 20%|██        | 117/585 [00:35<02:16,  3.44it/s][INFO|trainer.py:2140] 2023-08-29 04:35:13,527 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:35:13,527 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 04:35:13,527 >>   Batch size = 8

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.92it/s][A
  3%|▎         | 12/437 [00:00<00:33, 12.80it/s][A
  3%|▎         | 15/437 [00:01<00:30, 13.76it/s][A
  5%|▍         | 20/437 [00:01<00:21, 19.38it/s][A
  6%|▌         | 25/437 [00:01<00:16, 24.60it/s][A
  7%|▋         | 30/437 [00:01<00:13, 29.16it/s][A
  8%|▊         | 35/437 [00:01<00:12, 33.09it/s][A
  9%|▉         | 40/437 [00:01<00:10, 36.15it/s][A
 10%|█         | 45/437 [00:01<00:10, 38.55it/s][A
 11%|█▏        | 50/437 [00:01<00:09, 40.32it/s][A
 13%|█▎        | 55/437 [00:01<00:09, 41.18it/s][A
 14%|█▎        | 60/437 [00:02<00:08, 41.92it/s][A
 15%|█▍        | 65/437 [00:02<00:08, 42.44it/s][A
 16%|█▌        | 70/437 [00:02<00:08, 43.09it/s][A
 17%|█▋        | 75/437 [00:02<00:08, 43.67it/s][A
 18%|█▊        | 80/437 [00:02<00:08, 44.12it/s][A
 19%|█▉        | 85/437 [00:02<00:07, 44.38it/s][A
 21%|██        | 90/437 [00:02<00:07, 44.54it/s][A
 22%|██▏       | 95/437 [00:02<00:07, 44.53it/s][A
 23%|██▎       | 100/437 [00:02<00:07, 44.39it/s][A
 24%|██▍       | 105/437 [00:03<00:07, 44.18it/s][A
 25%|██▌       | 110/437 [00:03<00:07, 44.06it/s][A
 26%|██▋       | 115/437 [00:03<00:07, 44.20it/s][A
 27%|██▋       | 120/437 [00:03<00:07, 44.46it/s][A
 29%|██▊       | 125/437 [00:03<00:06, 44.66it/s][A
 30%|██▉       | 130/437 [00:03<00:06, 44.78it/s][A
 31%|███       | 135/437 [00:03<00:06, 44.80it/s][A
 32%|███▏      | 140/437 [00:03<00:06, 44.61it/s][A
 33%|███▎      | 145/437 [00:04<00:06, 44.42it/s][A
 34%|███▍      | 150/437 [00:04<00:07, 36.62it/s][A
 35%|███▌      | 155/437 [00:04<00:07, 38.81it/s][A
 37%|███▋      | 160/437 [00:04<00:06, 40.50it/s][A
 38%|███▊      | 165/437 [00:04<00:06, 41.82it/s][A
 39%|███▉      | 170/437 [00:04<00:06, 42.76it/s][A
 40%|████      | 175/437 [00:04<00:06, 43.41it/s][A
 41%|████      | 180/437 [00:04<00:05, 43.92it/s][A
 42%|████▏     | 185/437 [00:04<00:05, 44.12it/s][A
 43%|████▎     | 190/437 [00:05<00:05, 43.81it/s][A
 45%|████▍     | 195/437 [00:05<00:05, 43.72it/s][A
 46%|████▌     | 200/437 [00:05<00:05, 43.82it/s][A
 47%|████▋     | 205/437 [00:05<00:05, 44.11it/s][A
 48%|████▊     | 210/437 [00:05<00:05, 44.29it/s][A
 49%|████▉     | 215/437 [00:05<00:04, 44.56it/s][A
 50%|█████     | 220/437 [00:05<00:04, 44.63it/s][A
 51%|█████▏    | 225/437 [00:05<00:04, 44.77it/s][A
 53%|█████▎    | 230/437 [00:05<00:04, 44.81it/s][A
 54%|█████▍    | 235/437 [00:06<00:04, 44.40it/s][A
 55%|█████▍    | 240/437 [00:06<00:08, 24.10it/s][A
 56%|█████▌    | 245/437 [00:06<00:06, 28.02it/s][A
 57%|█████▋    | 250/437 [00:06<00:05, 31.49it/s][A
 58%|█████▊    | 255/437 [00:06<00:05, 34.68it/s][A
 59%|█████▉    | 260/437 [00:06<00:04, 37.30it/s][A
 61%|██████    | 265/437 [00:07<00:04, 39.32it/s][A
 62%|██████▏   | 270/437 [00:07<00:04, 40.90it/s][A
 63%|██████▎   | 275/437 [00:07<00:03, 41.88it/s][A
 64%|██████▍   | 280/437 [00:07<00:03, 42.19it/s][A
 65%|██████▌   | 285/437 [00:07<00:03, 42.60it/s][A
 66%|██████▋   | 290/437 [00:07<00:03, 43.18it/s][A
 68%|██████▊   | 295/437 [00:07<00:03, 43.61it/s][A
 69%|██████▊   | 300/437 [00:07<00:03, 44.07it/s][A
 70%|██████▉   | 305/437 [00:07<00:02, 44.32it/s][A
 71%|███████   | 310/437 [00:08<00:02, 44.55it/s][A
 72%|███████▏  | 315/437 [00:08<00:02, 44.67it/s][A
 73%|███████▎  | 320/437 [00:08<00:02, 44.45it/s][A
 74%|███████▍  | 325/437 [00:08<00:02, 44.15it/s][A
 76%|███████▌  | 330/437 [00:08<00:02, 44.05it/s][A
 77%|███████▋  | 335/437 [00:08<00:02, 44.13it/s][A
 78%|███████▊  | 340/437 [00:08<00:02, 44.34it/s][A
 79%|███████▉  | 345/437 [00:08<00:02, 44.50it/s][A
 80%|████████  | 350/437 [00:08<00:01, 44.68it/s][A
 81%|████████  | 355/437 [00:09<00:01, 44.82it/s][A
 82%|████████▏ | 360/437 [00:09<00:02, 29.37it/s][A
 84%|████████▎ | 365/437 [00:09<00:02, 32.81it/s][A
 85%|████████▍ | 370/437 [00:09<00:01, 35.73it/s][A
 86%|████████▌ | 375/437 [00:09<00:01, 38.10it/s][A
 87%|████████▋ | 380/437 [00:09<00:01, 39.92it/s][A
 88%|████████▊ | 385/437 [00:09<00:01, 41.36it/s][A
 89%|████████▉ | 390/437 [00:09<00:01, 42.41it/s][A
 90%|█████████ | 395/437 [00:10<00:00, 42.94it/s][A
 92%|█████████▏| 400/437 [00:10<00:00, 43.01it/s][A
 93%|█████████▎| 405/437 [00:10<00:00, 43.13it/s][A
 94%|█████████▍| 410/437 [00:10<00:00, 43.46it/s][A
 95%|█████████▍| 415/437 [00:10<00:00, 43.92it/s][A
 96%|█████████▌| 420/437 [00:10<00:00, 44.02it/s][A
 97%|█████████▋| 425/437 [00:10<00:00, 44.32it/s][A
 98%|█████████▊| 430/437 [00:10<00:00, 44.56it/s][A
100%|█████████▉| 435/437 [00:11<00:00, 44.68it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:11<00:00, 44.68it/s][A 20%|██        | 117/585 [00:46<02:16,  3.44it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:35:25,663 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
[INFO|configuration_utils.py:351] 2023-08-29 04:35:26,280 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:35:54,055 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:35:54,588 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:35:54,881 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117/special_tokens_map.json
 20%|██        | 118/585 [02:19<4:06:19, 31.65s/it] 20%|██        | 119/585 [02:20<2:52:59, 22.27s/it] 21%|██        | 120/585 [02:20<2:01:31, 15.68s/it] 21%|██        | 121/585 [02:20<1:25:33, 11.06s/it] 21%|██        | 122/585 [02:21<1:00:25,  7.83s/it] 21%|██        | 123/585 [02:21<42:52,  5.57s/it]   21%|██        | 124/585 [02:21<30:36,  3.98s/it] 21%|██▏       | 125/585 [02:21<22:02,  2.88s/it] 22%|██▏       | 126/585 [02:22<16:03,  2.10s/it] 22%|██▏       | 127/585 [02:22<11:53,  1.56s/it] 22%|██▏       | 128/585 [02:22<08:57,  1.18s/it] 22%|██▏       | 129/585 [02:23<07:10,  1.06it/s] 22%|██▏       | 130/585 [02:23<05:39,  1.34it/s] 22%|██▏       | 131/585 [02:23<04:36,  1.64it/s] 23%|██▎       | 132/585 [02:24<03:51,  1.95it/s] 23%|██▎       | 133/585 [02:24<03:20,  2.25it/s] 23%|██▎       | 134/585 [02:24<02:58,  2.52it/s] 23%|██▎       | 135/585 [02:24<02:43,  2.75it/s] 23%|██▎       | 136/585 [02:25<02:32,  2.95it/s] 23%|██▎       | 137/585 [02:25<02:24,  3.09it/s] 24%|██▎       | 138/585 [02:25<02:19,  3.20it/s] 24%|██▍       | 139/585 [02:26<02:15,  3.29it/s] 24%|██▍       | 140/585 [02:26<02:23,  3.10it/s] 24%|██▍       | 141/585 [02:26<02:18,  3.21it/s] 24%|██▍       | 142/585 [02:27<02:14,  3.30it/s] 24%|██▍       | 143/585 [02:27<02:11,  3.36it/s] 25%|██▍       | 144/585 [02:27<02:09,  3.40it/s] 25%|██▍       | 145/585 [02:27<02:08,  3.43it/s] 25%|██▍       | 146/585 [02:28<02:07,  3.45it/s] 25%|██▌       | 147/585 [02:28<02:06,  3.46it/s] 25%|██▌       | 148/585 [02:28<02:05,  3.47it/s] 25%|██▌       | 149/585 [02:29<02:05,  3.48it/s] 26%|██▌       | 150/585 [02:29<02:04,  3.49it/s] 26%|██▌       | 151/585 [02:29<02:19,  3.12it/s] 26%|██▌       | 152/585 [02:30<02:14,  3.22it/s] 26%|██▌       | 153/585 [02:30<02:10,  3.30it/s] 26%|██▋       | 154/585 [02:30<02:08,  3.36it/s] 26%|██▋       | 155/585 [02:30<02:06,  3.40it/s] 27%|██▋       | 156/585 [02:31<02:05,  3.43it/s] 27%|██▋       | 157/585 [02:31<02:04,  3.45it/s] 27%|██▋       | 158/585 [02:31<02:03,  3.47it/s] 27%|██▋       | 159/585 [02:32<02:02,  3.48it/s] 27%|██▋       | 160/585 [02:32<02:02,  3.48it/s] 28%|██▊       | 161/585 [02:32<02:01,  3.49it/s] 28%|██▊       | 162/585 [02:33<02:17,  3.07it/s] 28%|██▊       | 163/585 [02:33<02:12,  3.19it/s] 28%|██▊       | 164/585 [02:33<02:08,  3.27it/s] 28%|██▊       | 165/585 [02:33<02:05,  3.34it/s] 28%|██▊       | 166/585 [02:34<02:03,  3.38it/s] 29%|██▊       | 167/585 [02:34<02:02,  3.42it/s] 29%|██▊       | 168/585 [02:34<02:01,  3.44it/s] 29%|██▉       | 169/585 [02:35<02:00,  3.46it/s] 29%|██▉       | 170/585 [02:35<01:59,  3.47it/s] 29%|██▉       | 171/585 [02:35<01:58,  3.48it/s] 29%|██▉       | 172/585 [02:35<01:58,  3.49it/s] 30%|██▉       | 173/585 [02:36<02:10,  3.16it/s] 30%|██▉       | 174/585 [02:36<02:06,  3.25it/s] 30%|██▉       | 175/585 [02:36<02:03,  3.32it/s] 30%|███       | 176/585 [02:37<02:01,  3.37it/s] 30%|███       | 177/585 [02:37<01:59,  3.41it/s] 30%|███       | 178/585 [02:37<01:58,  3.43it/s] 31%|███       | 179/585 [02:37<01:57,  3.45it/s] 31%|███       | 180/585 [02:38<01:57,  3.46it/s] 31%|███       | 181/585 [02:38<01:56,  3.47it/s] 31%|███       | 182/585 [02:38<01:55,  3.48it/s] 31%|███▏      | 183/585 [02:39<01:55,  3.48it/s] 31%|███▏      | 184/585 [02:39<02:07,  3.14it/s] 32%|███▏      | 185/585 [02:39<02:03,  3.24it/s] 32%|███▏      | 186/585 [02:40<02:00,  3.31it/s] 32%|███▏      | 187/585 [02:40<01:58,  3.36it/s] 32%|███▏      | 188/585 [02:40<01:57,  3.38it/s] 32%|███▏      | 189/585 [02:40<01:55,  3.41it/s] 32%|███▏      | 190/585 [02:41<01:54,  3.44it/s] 33%|███▎      | 191/585 [02:41<01:54,  3.45it/s] 33%|███▎      | 192/585 [02:41<01:53,  3.46it/s] 33%|███▎      | 193/585 [02:42<01:52,  3.47it/s] 33%|███▎      | 194/585 [02:42<01:52,  3.48it/s] 33%|███▎      | 195/585 [02:42<02:05,  3.11it/s] 34%|███▎      | 196/585 [02:43<02:01,  3.21it/s] 34%|███▎      | 197/585 [02:43<01:57,  3.29it/s] 34%|███▍      | 198/585 [02:43<01:55,  3.35it/s] 34%|███▍      | 199/585 [02:43<01:53,  3.39it/s] 34%|███▍      | 200/585 [02:44<01:52,  3.42it/s] 34%|███▍      | 201/585 [02:44<01:51,  3.44it/s] 35%|███▍      | 202/585 [02:44<01:50,  3.46it/s] 35%|███▍      | 203/585 [02:45<01:50,  3.47it/s] 35%|███▍      | 204/585 [02:45<01:49,  3.47it/s] 35%|███▌      | 205/585 [02:45<01:49,  3.48it/s] 35%|███▌      | 206/585 [02:46<02:07,  2.98it/s] 35%|███▌      | 207/585 [02:46<02:01,  3.12it/s] 36%|███▌      | 208/585 [02:46<01:57,  3.22it/s] 36%|███▌      | 209/585 [02:46<01:53,  3.30it/s] 36%|███▌      | 210/585 [02:47<01:51,  3.35it/s] 36%|███▌      | 211/585 [02:47<01:50,  3.39it/s] 36%|███▌      | 212/585 [02:47<01:48,  3.42it/s] 36%|███▋      | 213/585 [02:48<01:48,  3.44it/s] 37%|███▋      | 214/585 [02:48<01:47,  3.46it/s] 37%|███▋      | 215/585 [02:48<01:46,  3.47it/s] 37%|███▋      | 216/585 [02:49<01:55,  3.18it/s] 37%|███▋      | 217/585 [02:49<01:52,  3.27it/s] 37%|███▋      | 218/585 [02:49<01:50,  3.33it/s] 37%|███▋      | 219/585 [02:49<01:48,  3.37it/s] 38%|███▊      | 220/585 [02:50<01:47,  3.40it/s] 38%|███▊      | 221/585 [02:50<01:46,  3.42it/s] 38%|███▊      | 222/585 [02:50<01:45,  3.44it/s] 38%|███▊      | 223/585 [02:51<01:44,  3.46it/s] 38%|███▊      | 224/585 [02:51<01:44,  3.47it/s] 38%|███▊      | 225/585 [02:51<01:43,  3.47it/s] 39%|███▊      | 226/585 [02:51<01:43,  3.48it/s] 39%|███▉      | 227/585 [02:52<01:51,  3.21it/s] 39%|███▉      | 228/585 [02:52<01:48,  3.29it/s] 39%|███▉      | 229/585 [02:52<01:46,  3.34it/s] 39%|███▉      | 230/585 [02:53<01:44,  3.38it/s] 39%|███▉      | 231/585 [02:53<01:43,  3.41it/s] 40%|███▉      | 232/585 [02:53<01:42,  3.43it/s] 40%|███▉      | 233/585 [02:54<01:42,  3.44it/s] 40%|████      | 234/585 [02:54<01:41,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 04:37:32,903 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:37:32,903 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 04:37:32,903 >>   Batch size = 8
{'eval_loss': 1.1281837224960327, 'eval_runtime': 11.0918, 'eval_samples_per_second': 314.826, 'eval_steps_per_second': 39.398, 'epoch': 1.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.88it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.52it/s][A
  4%|▍         | 17/437 [00:00<00:08, 46.91it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.18it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.75it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.54it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.20it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.52it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.13it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.12it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.24it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.37it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.47it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.68it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.79it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.58it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.28it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.04it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.02it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.14it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.31it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.39it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.53it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.71it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.67it/s][A
 30%|███       | 132/437 [00:03<00:08, 36.86it/s][A
 31%|███▏      | 137/437 [00:03<00:07, 38.92it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 40.52it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 41.80it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 42.74it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 43.29it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 43.81it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.02it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.68it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.55it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.73it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.91it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.27it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.47it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.62it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.72it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.47it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.71it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.95it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 44.03it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 44.17it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.36it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.53it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.77it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.73it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.47it/s][A
 60%|█████▉    | 262/437 [00:06<00:05, 34.18it/s][A
 61%|██████    | 267/437 [00:06<00:04, 36.82it/s][A
 62%|██████▏   | 272/437 [00:06<00:04, 38.89it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 40.55it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 41.83it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 42.70it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.47it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.72it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.57it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.34it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.40it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.73it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.11it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.34it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.59it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.68it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.62it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.22it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.94it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.84it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.04it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.26it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.52it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.65it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.64it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.56it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 35.21it/s][A
 91%|█████████ | 397/437 [00:09<00:01, 37.65it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 39.51it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 40.93it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 42.08it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 42.87it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 43.50it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 43.84it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.55it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.44it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 43.44it/s][A 40%|████      | 234/585 [03:04<01:41,  3.45it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:37:43,135 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-29 04:37:43,798 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:38:10,718 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:38:11,981 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:38:12,342 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234/special_tokens_map.json
 40%|████      | 235/585 [04:29<2:48:34, 28.90s/it] 40%|████      | 236/585 [04:30<1:58:20, 20.34s/it] 41%|████      | 237/585 [04:30<1:23:06, 14.33s/it] 41%|████      | 238/585 [04:30<58:30, 10.12s/it]   41%|████      | 239/585 [04:31<41:20,  7.17s/it] 41%|████      | 240/585 [04:31<29:20,  5.10s/it] 41%|████      | 241/585 [04:31<20:58,  3.66s/it] 41%|████▏     | 242/585 [04:32<15:07,  2.65s/it] 42%|████▏     | 243/585 [04:32<11:02,  1.94s/it] 42%|████▏     | 244/585 [04:32<08:24,  1.48s/it] 42%|████▏     | 245/585 [04:33<06:20,  1.12s/it] 42%|████▏     | 246/585 [04:33<05:03,  1.12it/s] 42%|████▏     | 247/585 [04:33<04:00,  1.40it/s] 42%|████▏     | 248/585 [04:33<03:16,  1.71it/s] 43%|████▎     | 249/585 [04:34<02:46,  2.02it/s] 43%|████▎     | 250/585 [04:34<02:24,  2.31it/s] 43%|████▎     | 251/585 [04:34<02:09,  2.57it/s] 43%|████▎     | 252/585 [04:35<01:59,  2.80it/s] 43%|████▎     | 253/585 [04:35<01:51,  2.97it/s] 43%|████▎     | 254/585 [04:35<01:46,  3.11it/s] 44%|████▎     | 255/585 [04:36<02:27,  2.24it/s] 44%|████▍     | 256/585 [04:37<03:00,  1.82it/s] 44%|████▍     | 257/585 [04:37<02:34,  2.13it/s] 44%|████▍     | 258/585 [04:37<02:15,  2.41it/s] 44%|████▍     | 259/585 [04:38<02:02,  2.66it/s] 44%|████▍     | 260/585 [04:38<01:53,  2.87it/s] 45%|████▍     | 261/585 [04:38<01:46,  3.03it/s] 45%|████▍     | 262/585 [04:38<01:42,  3.15it/s] 45%|████▍     | 263/585 [04:39<01:39,  3.25it/s] 45%|████▌     | 264/585 [04:39<01:36,  3.32it/s] 45%|████▌     | 265/585 [04:39<01:47,  2.99it/s] 45%|████▌     | 266/585 [04:40<01:42,  3.12it/s] 46%|████▌     | 267/585 [04:40<01:38,  3.23it/s] 46%|████▌     | 268/585 [04:40<01:35,  3.30it/s] 46%|████▌     | 269/585 [04:41<01:34,  3.36it/s] 46%|████▌     | 270/585 [04:41<01:32,  3.40it/s] 46%|████▋     | 271/585 [04:41<01:31,  3.43it/s] 46%|████▋     | 272/585 [04:41<01:30,  3.45it/s] 47%|████▋     | 273/585 [04:42<01:30,  3.46it/s] 47%|████▋     | 274/585 [04:42<01:29,  3.47it/s] 47%|████▋     | 275/585 [04:42<01:29,  3.48it/s] 47%|████▋     | 276/585 [04:43<01:38,  3.15it/s] 47%|████▋     | 277/585 [04:43<01:34,  3.25it/s] 48%|████▊     | 278/585 [04:43<01:32,  3.32it/s] 48%|████▊     | 279/585 [04:44<01:30,  3.37it/s] 48%|████▊     | 280/585 [04:44<01:29,  3.41it/s] 48%|████▊     | 281/585 [04:44<01:28,  3.43it/s] 48%|████▊     | 282/585 [04:44<01:27,  3.45it/s] 48%|████▊     | 283/585 [04:45<01:27,  3.46it/s] 49%|████▊     | 284/585 [04:45<01:26,  3.47it/s] 49%|████▊     | 285/585 [04:45<01:26,  3.48it/s] 49%|████▉     | 286/585 [04:46<01:25,  3.49it/s] 49%|████▉     | 287/585 [04:46<01:45,  2.83it/s] 49%|████▉     | 288/585 [04:46<01:38,  3.00it/s] 49%|████▉     | 289/585 [04:47<01:34,  3.14it/s] 50%|████▉     | 290/585 [04:47<01:31,  3.24it/s] 50%|████▉     | 291/585 [04:47<01:28,  3.31it/s] 50%|████▉     | 292/585 [04:47<01:27,  3.36it/s] 50%|█████     | 293/585 [04:48<01:25,  3.40it/s] 50%|█████     | 294/585 [04:48<01:24,  3.43it/s] 50%|█████     | 295/585 [04:48<01:24,  3.44it/s] 51%|█████     | 296/585 [04:49<01:23,  3.46it/s] 51%|█████     | 297/585 [04:49<01:28,  3.25it/s] 51%|█████     | 298/585 [04:49<01:26,  3.32it/s] 51%|█████     | 299/585 [04:50<01:24,  3.37it/s] 51%|█████▏    | 300/585 [04:50<01:23,  3.40it/s] 51%|█████▏    | 301/585 [04:50<01:22,  3.43it/s] 52%|█████▏    | 302/585 [04:50<01:22,  3.45it/s] 52%|█████▏    | 303/585 [04:51<01:21,  3.46it/s] 52%|█████▏    | 304/585 [04:51<01:20,  3.47it/s] 52%|█████▏    | 305/585 [04:51<01:20,  3.48it/s] 52%|█████▏    | 306/585 [04:52<01:20,  3.48it/s] 52%|█████▏    | 307/585 [04:52<01:19,  3.49it/s] 53%|█████▎    | 308/585 [04:52<01:26,  3.21it/s] 53%|█████▎    | 309/585 [04:52<01:23,  3.29it/s] 53%|█████▎    | 310/585 [04:53<01:22,  3.35it/s] 53%|█████▎    | 311/585 [04:53<01:20,  3.39it/s] 53%|█████▎    | 312/585 [04:53<01:19,  3.42it/s] 54%|█████▎    | 313/585 [04:54<01:19,  3.44it/s] 54%|█████▎    | 314/585 [04:54<01:18,  3.46it/s] 54%|█████▍    | 315/585 [04:54<01:17,  3.47it/s] 54%|█████▍    | 316/585 [04:54<01:17,  3.47it/s] 54%|█████▍    | 317/585 [04:55<01:17,  3.48it/s] 54%|█████▍    | 318/585 [04:55<01:16,  3.48it/s] 55%|█████▍    | 319/585 [04:55<01:22,  3.22it/s] 55%|█████▍    | 320/585 [04:56<01:20,  3.30it/s] 55%|█████▍    | 321/585 [04:56<01:18,  3.36it/s] 55%|█████▌    | 322/585 [04:56<01:17,  3.39it/s] 55%|█████▌    | 323/585 [04:57<01:16,  3.42it/s] 55%|█████▌    | 324/585 [04:57<01:15,  3.44it/s] 56%|█████▌    | 325/585 [04:57<01:15,  3.46it/s] 56%|█████▌    | 326/585 [04:57<01:14,  3.46it/s] 56%|█████▌    | 327/585 [04:58<01:14,  3.47it/s] 56%|█████▌    | 328/585 [04:58<01:13,  3.47it/s] 56%|█████▌    | 329/585 [04:58<01:13,  3.48it/s] 56%|█████▋    | 330/585 [04:59<01:18,  3.26it/s] 57%|█████▋    | 331/585 [04:59<01:16,  3.32it/s] 57%|█████▋    | 332/585 [04:59<01:15,  3.37it/s] 57%|█████▋    | 333/585 [04:59<01:14,  3.40it/s] 57%|█████▋    | 334/585 [05:00<01:13,  3.43it/s] 57%|█████▋    | 335/585 [05:00<01:12,  3.44it/s] 57%|█████▋    | 336/585 [05:00<01:12,  3.46it/s] 58%|█████▊    | 337/585 [05:01<01:11,  3.46it/s] 58%|█████▊    | 338/585 [05:01<01:11,  3.47it/s] 58%|█████▊    | 339/585 [05:01<01:10,  3.48it/s] 58%|█████▊    | 340/585 [05:02<01:10,  3.48it/s] 58%|█████▊    | 341/585 [05:02<01:17,  3.16it/s] 58%|█████▊    | 342/585 [05:02<01:16,  3.18it/s] 59%|█████▊    | 343/585 [05:02<01:14,  3.26it/s] 59%|█████▉    | 344/585 [05:03<01:12,  3.33it/s] 59%|█████▉    | 345/585 [05:03<01:11,  3.37it/s] 59%|█████▉    | 346/585 [05:03<01:10,  3.41it/s] 59%|█████▉    | 347/585 [05:04<01:09,  3.43it/s] 59%|█████▉    | 348/585 [05:04<01:08,  3.44it/s] 60%|█████▉    | 349/585 [05:04<01:08,  3.45it/s] 60%|█████▉    | 350/585 [05:04<01:07,  3.46it/s] 60%|██████    | 351/585 [05:05<01:07,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 04:39:43,863 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:39:43,863 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 04:39:43,863 >>   Batch size = 8
{'eval_loss': 1.1482046842575073, 'eval_runtime': 10.1294, 'eval_samples_per_second': 344.738, 'eval_steps_per_second': 43.142, 'epoch': 2.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.85it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.78it/s][A
  4%|▍         | 17/437 [00:00<00:08, 47.20it/s][A
  5%|▌         | 22/437 [00:00<00:08, 46.31it/s][A
  6%|▌         | 27/437 [00:00<00:08, 45.91it/s][A
  7%|▋         | 32/437 [00:00<00:08, 45.56it/s][A
  8%|▊         | 37/437 [00:00<00:08, 45.19it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.49it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.14it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.07it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.17it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.38it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.49it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.68it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.78it/s][A
 19%|█▉        | 82/437 [00:01<00:07, 44.61it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.34it/s][A
 21%|██        | 92/437 [00:02<00:07, 44.09it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 44.03it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 44.22it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 44.43it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.56it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.66it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.69it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.51it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.30it/s][A
 31%|███▏      | 137/437 [00:03<00:08, 35.24it/s][A
 32%|███▏      | 142/437 [00:03<00:07, 37.68it/s][A
 34%|███▎      | 147/437 [00:03<00:07, 39.55it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 41.05it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 42.09it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 42.94it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 43.59it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.80it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.58it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 43.42it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 43.55it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 43.93it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.23it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.50it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.49it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.72it/s][A
 50%|████▉     | 217/437 [00:04<00:04, 44.65it/s][A
 51%|█████     | 222/437 [00:05<00:04, 44.20it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.90it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.95it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 44.25it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 44.47it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.67it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.67it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.74it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.53it/s][A
 61%|██████    | 267/437 [00:06<00:04, 37.76it/s][A
 62%|██████▏   | 272/437 [00:06<00:04, 39.59it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 40.98it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 42.11it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 42.95it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 43.34it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 43.79it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.96it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.70it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.59it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 43.71it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 43.93it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.13it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.43it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.53it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.63it/s][A
 79%|███████▉  | 347/437 [00:07<00:02, 44.50it/s][A
 81%|████████  | 352/437 [00:08<00:01, 44.24it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 44.09it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 44.10it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 44.11it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 44.34it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.53it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.58it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.69it/s][A
 90%|████████▉ | 392/437 [00:08<00:01, 44.45it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.27it/s][A
 92%|█████████▏| 402/437 [00:09<00:01, 34.45it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 37.02it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 39.00it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 40.68it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 41.87it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 42.82it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 43.49it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.75it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 43.75it/s][A 60%|██████    | 351/585 [05:15<01:07,  3.47it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:39:54,669 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
[INFO|configuration_utils.py:351] 2023-08-29 04:39:55,398 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:40:11,458 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:40:11,901 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:40:12,095 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351/special_tokens_map.json
 60%|██████    | 352/585 [06:31<1:41:33, 26.15s/it] 60%|██████    | 353/585 [06:32<1:11:12, 18.42s/it] 61%|██████    | 354/585 [06:32<49:58, 12.98s/it]   61%|██████    | 355/585 [06:32<35:09,  9.17s/it] 61%|██████    | 356/585 [06:33<24:50,  6.51s/it] 61%|██████    | 357/585 [06:33<17:44,  4.67s/it] 61%|██████    | 358/585 [06:33<12:41,  3.35s/it] 61%|██████▏   | 359/585 [06:33<09:10,  2.44s/it] 62%|██████▏   | 360/585 [06:34<06:43,  1.79s/it] 62%|██████▏   | 361/585 [06:34<05:00,  1.34s/it] 62%|██████▏   | 362/585 [06:34<03:48,  1.03s/it] 62%|██████▏   | 363/585 [06:35<03:04,  1.21it/s] 62%|██████▏   | 364/585 [06:35<02:27,  1.50it/s] 62%|██████▏   | 365/585 [06:35<02:02,  1.80it/s] 63%|██████▎   | 366/585 [06:36<01:44,  2.10it/s] 63%|██████▎   | 367/585 [06:36<01:31,  2.38it/s] 63%|██████▎   | 368/585 [06:36<01:22,  2.62it/s] 63%|██████▎   | 369/585 [06:36<01:16,  2.83it/s] 63%|██████▎   | 370/585 [06:37<01:11,  2.99it/s] 63%|██████▎   | 371/585 [06:37<01:08,  3.11it/s] 64%|██████▎   | 372/585 [06:37<01:06,  3.20it/s] 64%|██████▍   | 373/585 [06:38<01:04,  3.27it/s] 64%|██████▍   | 374/585 [06:38<01:09,  3.02it/s] 64%|██████▍   | 375/585 [06:38<01:06,  3.13it/s] 64%|██████▍   | 376/585 [06:39<01:04,  3.22it/s] 64%|██████▍   | 377/585 [06:39<01:03,  3.28it/s] 65%|██████▍   | 378/585 [06:39<01:02,  3.33it/s] 65%|██████▍   | 379/585 [06:39<01:01,  3.36it/s] 65%|██████▍   | 380/585 [06:40<01:00,  3.38it/s] 65%|██████▌   | 381/585 [06:40<01:00,  3.40it/s] 65%|██████▌   | 382/585 [06:40<00:59,  3.41it/s] 65%|██████▌   | 383/585 [06:41<00:59,  3.42it/s] 66%|██████▌   | 384/585 [06:41<01:05,  3.08it/s] 66%|██████▌   | 385/585 [06:41<01:02,  3.18it/s] 66%|██████▌   | 386/585 [06:42<01:01,  3.25it/s] 66%|██████▌   | 387/585 [06:42<00:59,  3.32it/s] 66%|██████▋   | 388/585 [06:42<00:58,  3.37it/s] 66%|██████▋   | 389/585 [06:42<00:57,  3.40it/s] 67%|██████▋   | 390/585 [06:43<00:56,  3.43it/s] 67%|██████▋   | 391/585 [06:43<00:56,  3.45it/s] 67%|██████▋   | 392/585 [06:43<00:55,  3.46it/s] 67%|██████▋   | 393/585 [06:44<00:55,  3.47it/s] 67%|██████▋   | 394/585 [06:44<00:54,  3.48it/s] 68%|██████▊   | 395/585 [06:44<00:59,  3.18it/s] 68%|██████▊   | 396/585 [06:45<00:57,  3.27it/s] 68%|██████▊   | 397/585 [06:45<00:56,  3.33it/s] 68%|██████▊   | 398/585 [06:45<00:55,  3.38it/s] 68%|██████▊   | 399/585 [06:45<00:54,  3.41it/s] 68%|██████▊   | 400/585 [06:46<00:53,  3.44it/s] 69%|██████▊   | 401/585 [06:46<00:53,  3.46it/s] 69%|██████▊   | 402/585 [06:46<00:52,  3.47it/s] 69%|██████▉   | 403/585 [06:47<00:52,  3.47it/s] 69%|██████▉   | 404/585 [06:47<00:52,  3.48it/s] 69%|██████▉   | 405/585 [06:47<00:51,  3.48it/s] 69%|██████▉   | 406/585 [06:48<00:57,  3.13it/s] 70%|██████▉   | 407/585 [06:48<00:55,  3.23it/s] 70%|██████▉   | 408/585 [06:48<00:53,  3.31it/s] 70%|██████▉   | 409/585 [06:48<00:52,  3.36it/s] 70%|███████   | 410/585 [06:49<00:51,  3.40it/s] 70%|███████   | 411/585 [06:49<00:50,  3.42it/s] 70%|███████   | 412/585 [06:49<00:58,  2.96it/s] 71%|███████   | 413/585 [06:50<00:55,  3.10it/s] 71%|███████   | 414/585 [06:50<00:53,  3.21it/s] 71%|███████   | 415/585 [06:50<00:51,  3.29it/s] 71%|███████   | 416/585 [06:51<00:56,  3.01it/s] 71%|███████▏  | 417/585 [06:51<00:53,  3.14it/s] 71%|███████▏  | 418/585 [06:51<00:51,  3.24it/s] 72%|███████▏  | 419/585 [06:52<00:50,  3.31it/s] 72%|███████▏  | 420/585 [06:52<00:49,  3.36it/s] 72%|███████▏  | 421/585 [06:52<00:48,  3.40it/s] 72%|███████▏  | 422/585 [06:52<00:47,  3.43it/s] 72%|███████▏  | 423/585 [06:53<00:46,  3.45it/s] 72%|███████▏  | 424/585 [06:53<00:46,  3.46it/s] 73%|███████▎  | 425/585 [06:53<00:46,  3.47it/s] 73%|███████▎  | 426/585 [06:54<00:45,  3.48it/s] 73%|███████▎  | 427/585 [06:54<00:50,  3.15it/s] 73%|███████▎  | 428/585 [06:54<00:48,  3.24it/s] 73%|███████▎  | 429/585 [06:55<00:47,  3.31it/s] 74%|███████▎  | 430/585 [06:55<00:46,  3.37it/s] 74%|███████▎  | 431/585 [06:55<00:45,  3.40it/s] 74%|███████▍  | 432/585 [06:55<00:44,  3.43it/s] 74%|███████▍  | 433/585 [06:56<00:44,  3.45it/s] 74%|███████▍  | 434/585 [06:56<00:43,  3.46it/s] 74%|███████▍  | 435/585 [06:56<00:43,  3.47it/s] 75%|███████▍  | 436/585 [06:57<00:42,  3.48it/s] 75%|███████▍  | 437/585 [06:57<00:42,  3.48it/s] 75%|███████▍  | 438/585 [06:57<00:45,  3.25it/s] 75%|███████▌  | 439/585 [06:57<00:44,  3.32it/s] 75%|███████▌  | 440/585 [06:58<00:43,  3.37it/s] 75%|███████▌  | 441/585 [06:58<00:42,  3.40it/s] 76%|███████▌  | 442/585 [06:58<00:41,  3.43it/s] 76%|███████▌  | 443/585 [06:59<00:41,  3.45it/s] 76%|███████▌  | 444/585 [06:59<00:40,  3.46it/s] 76%|███████▌  | 445/585 [06:59<00:40,  3.47it/s] 76%|███████▌  | 446/585 [06:59<00:40,  3.47it/s] 76%|███████▋  | 447/585 [07:00<00:39,  3.48it/s] 77%|███████▋  | 448/585 [07:00<00:39,  3.48it/s] 77%|███████▋  | 449/585 [07:00<00:41,  3.25it/s] 77%|███████▋  | 450/585 [07:01<00:40,  3.32it/s] 77%|███████▋  | 451/585 [07:01<00:39,  3.37it/s] 77%|███████▋  | 452/585 [07:01<00:39,  3.40it/s] 77%|███████▋  | 453/585 [07:02<00:38,  3.43it/s] 78%|███████▊  | 454/585 [07:02<00:38,  3.45it/s] 78%|███████▊  | 455/585 [07:02<00:37,  3.46it/s] 78%|███████▊  | 456/585 [07:02<00:37,  3.47it/s] 78%|███████▊  | 457/585 [07:03<00:36,  3.47it/s] 78%|███████▊  | 458/585 [07:03<00:36,  3.48it/s] 78%|███████▊  | 459/585 [07:03<00:36,  3.48it/s] 79%|███████▊  | 460/585 [07:04<00:37,  3.30it/s] 79%|███████▉  | 461/585 [07:04<00:36,  3.35it/s] 79%|███████▉  | 462/585 [07:04<00:36,  3.39it/s] 79%|███████▉  | 463/585 [07:04<00:35,  3.42it/s] 79%|███████▉  | 464/585 [07:05<00:35,  3.44it/s] 79%|███████▉  | 465/585 [07:05<00:35,  3.39it/s] 80%|███████▉  | 466/585 [07:05<00:34,  3.42it/s] 80%|███████▉  | 467/585 [07:06<00:34,  3.44it/s] 80%|████████  | 468/585 [07:06<00:33,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 04:41:44,889 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:41:44,889 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 04:41:44,889 >>   Batch size = 8
{'eval_loss': 1.1651020050048828, 'eval_runtime': 10.1033, 'eval_samples_per_second': 345.629, 'eval_steps_per_second': 43.253, 'epoch': 3.0}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.08it/s][A
  3%|▎         | 12/437 [00:00<00:08, 47.87it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.24it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.39it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.11it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.80it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.61it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.56it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.61it/s][A
 12%|█▏        | 52/437 [00:01<00:08, 44.59it/s][A
 13%|█▎        | 57/437 [00:01<00:08, 44.46it/s][A
 14%|█▍        | 62/437 [00:01<00:08, 44.37it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 44.28it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 44.36it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 44.40it/s][A
 19%|█▉        | 82/437 [00:01<00:09, 38.00it/s][A
 20%|█▉        | 87/437 [00:01<00:08, 39.89it/s][A
 21%|██        | 92/437 [00:02<00:08, 41.30it/s][A
 22%|██▏       | 97/437 [00:02<00:08, 42.39it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.13it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.70it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 43.96it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 43.95it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 43.72it/s][A
 29%|██▉       | 127/437 [00:02<00:07, 43.56it/s][A
 30%|███       | 132/437 [00:03<00:06, 43.79it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.06it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 44.31it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 44.54it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.67it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.59it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.32it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.11it/s][A
 39%|███▉      | 172/437 [00:03<00:06, 43.94it/s][A
 41%|████      | 177/437 [00:04<00:05, 43.96it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.21it/s][A
 43%|████▎     | 187/437 [00:04<00:05, 44.42it/s][A
 44%|████▍     | 192/437 [00:04<00:05, 44.55it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 44.72it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 44.61it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 44.43it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 44.15it/s][A
 50%|████▉     | 217/437 [00:05<00:05, 36.90it/s][A
 51%|█████     | 222/437 [00:05<00:05, 39.03it/s][A
 52%|█████▏    | 227/437 [00:05<00:05, 40.65it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 41.84it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 42.76it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.36it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 43.86it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 43.98it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 43.60it/s][A
 60%|█████▉    | 262/437 [00:06<00:04, 43.46it/s][A
 61%|██████    | 267/437 [00:06<00:03, 43.68it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.07it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 44.26it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 44.55it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.62it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.68it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.45it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 43.99it/s][A
 70%|███████   | 307/437 [00:07<00:02, 43.78it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 43.94it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.19it/s][A
 74%|███████▎  | 322/437 [00:07<00:02, 44.31it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 44.43it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 44.52it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 44.70it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 44.47it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 36.45it/s][A
 81%|████████  | 352/437 [00:08<00:02, 38.67it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 40.38it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 41.67it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 42.63it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.27it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 43.75it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 43.90it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 43.66it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 43.38it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 43.61it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 43.85it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.15it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.31it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.53it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.66it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.46it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.21it/s][A
100%|██████████| 437/437 [00:10<00:00, 43.93it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 43.93it/s][A 80%|████████  | 468/585 [07:16<00:33,  3.46it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:41:55,166 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 04:41:55,923 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:42:23,490 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:42:24,736 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:42:24,990 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468/special_tokens_map.json
 80%|████████  | 469/585 [08:44<57:18, 29.64s/it] 80%|████████  | 470/585 [08:44<39:58, 20.85s/it] 81%|████████  | 471/585 [08:45<27:56, 14.71s/it] 81%|████████  | 472/585 [08:45<19:33, 10.38s/it] 81%|████████  | 473/585 [08:45<13:43,  7.36s/it] 81%|████████  | 474/585 [08:46<09:41,  5.24s/it] 81%|████████  | 475/585 [08:46<06:52,  3.75s/it] 81%|████████▏ | 476/585 [08:46<04:55,  2.71s/it] 82%|████████▏ | 477/585 [08:46<03:34,  1.98s/it] 82%|████████▏ | 478/585 [08:47<02:37,  1.47s/it] 82%|████████▏ | 479/585 [08:47<01:58,  1.12s/it] 82%|████████▏ | 480/585 [08:47<01:33,  1.12it/s] 82%|████████▏ | 481/585 [08:48<01:14,  1.40it/s] 82%|████████▏ | 482/585 [08:49<01:29,  1.15it/s] 83%|████████▎ | 483/585 [08:49<01:10,  1.44it/s] 83%|████████▎ | 484/585 [08:50<01:01,  1.65it/s] 83%|████████▎ | 485/585 [08:50<00:50,  1.96it/s] 83%|████████▎ | 486/585 [08:50<00:43,  2.26it/s] 83%|████████▎ | 487/585 [08:51<00:41,  2.35it/s] 83%|████████▎ | 488/585 [08:51<00:37,  2.61it/s] 84%|████████▎ | 489/585 [08:51<00:33,  2.82it/s] 84%|████████▍ | 490/585 [08:51<00:31,  3.00it/s] 84%|████████▍ | 491/585 [08:52<00:30,  3.13it/s] 84%|████████▍ | 492/585 [08:52<00:28,  3.23it/s] 84%|████████▍ | 493/585 [08:52<00:27,  3.31it/s] 84%|████████▍ | 494/585 [08:53<00:27,  3.36it/s] 85%|████████▍ | 495/585 [08:53<00:26,  3.40it/s] 85%|████████▍ | 496/585 [08:53<00:25,  3.42it/s] 85%|████████▍ | 497/585 [08:53<00:25,  3.44it/s] 85%|████████▌ | 498/585 [08:54<00:30,  2.84it/s] 85%|████████▌ | 499/585 [08:54<00:28,  3.01it/s] 85%|████████▌ | 500/585 [08:55<00:27,  3.14it/s]                                                  85%|████████▌ | 500/585 [08:55<00:27,  3.14it/s] 86%|████████▌ | 501/585 [08:55<00:25,  3.24it/s] 86%|████████▌ | 502/585 [08:55<00:25,  3.31it/s] 86%|████████▌ | 503/585 [08:55<00:24,  3.37it/s] 86%|████████▌ | 504/585 [08:56<00:23,  3.41it/s] 86%|████████▋ | 505/585 [08:56<00:23,  3.44it/s] 86%|████████▋ | 506/585 [08:56<00:22,  3.46it/s] 87%|████████▋ | 507/585 [08:57<00:22,  3.47it/s] 87%|████████▋ | 508/585 [08:57<00:28,  2.70it/s] 87%|████████▋ | 509/585 [08:57<00:26,  2.90it/s] 87%|████████▋ | 510/585 [08:58<00:24,  3.06it/s] 87%|████████▋ | 511/585 [08:58<00:23,  3.18it/s] 88%|████████▊ | 512/585 [08:58<00:22,  3.27it/s] 88%|████████▊ | 513/585 [08:58<00:21,  3.33it/s] 88%|████████▊ | 514/585 [08:59<00:21,  3.38it/s] 88%|████████▊ | 515/585 [08:59<00:20,  3.41it/s] 88%|████████▊ | 516/585 [08:59<00:20,  3.44it/s] 88%|████████▊ | 517/585 [09:00<00:19,  3.45it/s] 89%|████████▊ | 518/585 [09:00<00:20,  3.21it/s] 89%|████████▊ | 519/585 [09:00<00:20,  3.29it/s] 89%|████████▉ | 520/585 [09:01<00:19,  3.35it/s] 89%|████████▉ | 521/585 [09:01<00:18,  3.39it/s] 89%|████████▉ | 522/585 [09:01<00:18,  3.42it/s] 89%|████████▉ | 523/585 [09:01<00:18,  3.44it/s] 90%|████████▉ | 524/585 [09:02<00:17,  3.46it/s] 90%|████████▉ | 525/585 [09:02<00:17,  3.47it/s] 90%|████████▉ | 526/585 [09:02<00:16,  3.48it/s] 90%|█████████ | 527/585 [09:03<00:16,  3.48it/s] 90%|█████████ | 528/585 [09:03<00:16,  3.48it/s] 90%|█████████ | 529/585 [09:03<00:17,  3.25it/s] 91%|█████████ | 530/585 [09:04<00:16,  3.31it/s] 91%|█████████ | 531/585 [09:04<00:16,  3.36it/s] 91%|█████████ | 532/585 [09:04<00:15,  3.40it/s] 91%|█████████ | 533/585 [09:04<00:15,  3.43it/s] 91%|█████████▏| 534/585 [09:05<00:14,  3.45it/s] 91%|█████████▏| 535/585 [09:05<00:14,  3.46it/s] 92%|█████████▏| 536/585 [09:05<00:14,  3.46it/s] 92%|█████████▏| 537/585 [09:06<00:13,  3.47it/s] 92%|█████████▏| 538/585 [09:06<00:13,  3.48it/s] 92%|█████████▏| 539/585 [09:06<00:13,  3.48it/s] 92%|█████████▏| 540/585 [09:06<00:14,  3.13it/s] 92%|█████████▏| 541/585 [09:07<00:13,  3.23it/s] 93%|█████████▎| 542/585 [09:07<00:13,  3.30it/s] 93%|█████████▎| 543/585 [09:07<00:12,  3.36it/s] 93%|█████████▎| 544/585 [09:08<00:12,  3.40it/s] 93%|█████████▎| 545/585 [09:08<00:11,  3.42it/s] 93%|█████████▎| 546/585 [09:08<00:11,  3.45it/s] 94%|█████████▎| 547/585 [09:08<00:10,  3.46it/s] 94%|█████████▎| 548/585 [09:09<00:10,  3.47it/s] 94%|█████████▍| 549/585 [09:09<00:10,  3.47it/s] 94%|█████████▍| 550/585 [09:09<00:10,  3.48it/s] 94%|█████████▍| 551/585 [09:10<00:10,  3.17it/s] 94%|█████████▍| 552/585 [09:10<00:10,  3.26it/s] 95%|█████████▍| 553/585 [09:10<00:09,  3.33it/s] 95%|█████████▍| 554/585 [09:11<00:09,  3.37it/s] 95%|█████████▍| 555/585 [09:11<00:08,  3.41it/s] 95%|█████████▌| 556/585 [09:11<00:08,  3.43it/s] 95%|█████████▌| 557/585 [09:11<00:08,  3.45it/s] 95%|█████████▌| 558/585 [09:12<00:07,  3.46it/s] 96%|█████████▌| 559/585 [09:12<00:07,  3.47it/s] 96%|█████████▌| 560/585 [09:12<00:07,  3.48it/s] 96%|█████████▌| 561/585 [09:13<00:06,  3.48it/s] 96%|█████████▌| 562/585 [09:13<00:07,  3.15it/s] 96%|█████████▌| 563/585 [09:13<00:06,  3.24it/s] 96%|█████████▋| 564/585 [09:14<00:06,  3.31it/s] 97%|█████████▋| 565/585 [09:14<00:05,  3.37it/s] 97%|█████████▋| 566/585 [09:14<00:05,  3.40it/s] 97%|█████████▋| 567/585 [09:14<00:05,  3.43it/s] 97%|█████████▋| 568/585 [09:15<00:04,  3.45it/s] 97%|█████████▋| 569/585 [09:15<00:04,  3.46it/s] 97%|█████████▋| 570/585 [09:15<00:04,  3.47it/s] 98%|█████████▊| 571/585 [09:16<00:04,  3.47it/s] 98%|█████████▊| 572/585 [09:16<00:03,  3.48it/s] 98%|█████████▊| 573/585 [09:16<00:03,  3.14it/s] 98%|█████████▊| 574/585 [09:17<00:03,  3.23it/s] 98%|█████████▊| 575/585 [09:17<00:03,  3.31it/s] 98%|█████████▊| 576/585 [09:17<00:02,  3.36it/s] 99%|█████████▊| 577/585 [09:17<00:02,  3.40it/s] 99%|█████████▉| 578/585 [09:18<00:02,  3.43it/s] 99%|█████████▉| 579/585 [09:18<00:01,  3.44it/s] 99%|█████████▉| 580/585 [09:18<00:01,  3.46it/s] 99%|█████████▉| 581/585 [09:19<00:01,  3.47it/s] 99%|█████████▉| 582/585 [09:19<00:00,  3.47it/s]100%|█████████▉| 583/585 [09:19<00:00,  3.47it/s]100%|█████████▉| 584/585 [09:19<00:00,  3.48it/s]100%|██████████| 585/585 [09:20<00:00,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 04:43:58,651 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:43:58,651 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 04:43:58,651 >>   Batch size = 8
{'eval_loss': 1.180245041847229, 'eval_runtime': 10.0738, 'eval_samples_per_second': 346.643, 'eval_steps_per_second': 43.38, 'epoch': 4.0}
{'loss': 0.4036, 'learning_rate': 5.448717948717949e-06, 'epoch': 4.27}

  0%|          | 0/437 [00:00<?, ?it/s][A
  1%|▏         | 6/437 [00:00<00:07, 55.05it/s][A
  3%|▎         | 12/437 [00:00<00:08, 48.13it/s][A
  4%|▍         | 17/437 [00:00<00:09, 46.40it/s][A
  5%|▌         | 22/437 [00:00<00:09, 45.61it/s][A
  6%|▌         | 27/437 [00:00<00:09, 45.17it/s][A
  7%|▋         | 32/437 [00:00<00:09, 44.94it/s][A
  8%|▊         | 37/437 [00:00<00:08, 44.70it/s][A
 10%|▉         | 42/437 [00:00<00:08, 44.56it/s][A
 11%|█         | 47/437 [00:01<00:08, 44.60it/s][A
 12%|█▏        | 52/437 [00:01<00:10, 37.71it/s][A
 13%|█▎        | 57/437 [00:01<00:09, 39.72it/s][A
 14%|█▍        | 62/437 [00:01<00:09, 41.20it/s][A
 15%|█▌        | 67/437 [00:01<00:08, 42.23it/s][A
 16%|█▋        | 72/437 [00:01<00:08, 43.07it/s][A
 18%|█▊        | 77/437 [00:01<00:08, 43.65it/s][A
 19%|█▉        | 82/437 [00:01<00:08, 44.02it/s][A
 20%|█▉        | 87/437 [00:01<00:07, 44.14it/s][A
 21%|██        | 92/437 [00:02<00:07, 43.75it/s][A
 22%|██▏       | 97/437 [00:02<00:07, 43.57it/s][A
 23%|██▎       | 102/437 [00:02<00:07, 43.73it/s][A
 24%|██▍       | 107/437 [00:02<00:07, 43.99it/s][A
 26%|██▌       | 112/437 [00:02<00:07, 44.43it/s][A
 27%|██▋       | 117/437 [00:02<00:07, 44.66it/s][A
 28%|██▊       | 122/437 [00:02<00:07, 44.75it/s][A
 29%|██▉       | 127/437 [00:02<00:06, 44.70it/s][A
 30%|███       | 132/437 [00:03<00:06, 44.38it/s][A
 31%|███▏      | 137/437 [00:03<00:06, 44.01it/s][A
 32%|███▏      | 142/437 [00:03<00:06, 43.80it/s][A
 34%|███▎      | 147/437 [00:03<00:06, 43.84it/s][A
 35%|███▍      | 152/437 [00:03<00:06, 44.03it/s][A
 36%|███▌      | 157/437 [00:03<00:06, 44.20it/s][A
 37%|███▋      | 162/437 [00:03<00:06, 44.44it/s][A
 38%|███▊      | 167/437 [00:03<00:06, 44.65it/s][A
 39%|███▉      | 172/437 [00:03<00:05, 44.48it/s][A
 41%|████      | 177/437 [00:04<00:05, 44.41it/s][A
 42%|████▏     | 182/437 [00:04<00:05, 44.21it/s][A
 43%|████▎     | 187/437 [00:04<00:06, 36.94it/s][A
 44%|████▍     | 192/437 [00:04<00:06, 39.07it/s][A
 45%|████▌     | 197/437 [00:04<00:05, 40.68it/s][A
 46%|████▌     | 202/437 [00:04<00:05, 41.88it/s][A
 47%|████▋     | 207/437 [00:04<00:05, 42.80it/s][A
 49%|████▊     | 212/437 [00:04<00:05, 43.45it/s][A
 50%|████▉     | 217/437 [00:04<00:05, 43.89it/s][A
 51%|█████     | 222/437 [00:05<00:04, 43.98it/s][A
 52%|█████▏    | 227/437 [00:05<00:04, 43.71it/s][A
 53%|█████▎    | 232/437 [00:05<00:04, 43.52it/s][A
 54%|█████▍    | 237/437 [00:05<00:04, 43.66it/s][A
 55%|█████▌    | 242/437 [00:05<00:04, 43.90it/s][A
 57%|█████▋    | 247/437 [00:05<00:04, 44.26it/s][A
 58%|█████▊    | 252/437 [00:05<00:04, 44.50it/s][A
 59%|█████▉    | 257/437 [00:05<00:04, 44.64it/s][A
 60%|█████▉    | 262/437 [00:06<00:03, 44.79it/s][A
 61%|██████    | 267/437 [00:06<00:03, 44.52it/s][A
 62%|██████▏   | 272/437 [00:06<00:03, 44.20it/s][A
 63%|██████▎   | 277/437 [00:06<00:03, 43.92it/s][A
 65%|██████▍   | 282/437 [00:06<00:03, 43.91it/s][A
 66%|██████▌   | 287/437 [00:06<00:03, 44.14it/s][A
 67%|██████▋   | 292/437 [00:06<00:03, 44.39it/s][A
 68%|██████▊   | 297/437 [00:06<00:03, 44.51it/s][A
 69%|██████▉   | 302/437 [00:06<00:03, 44.49it/s][A
 70%|███████   | 307/437 [00:07<00:02, 44.76it/s][A
 71%|███████▏  | 312/437 [00:07<00:02, 44.53it/s][A
 73%|███████▎  | 317/437 [00:07<00:02, 44.21it/s][A
 74%|███████▎  | 322/437 [00:07<00:03, 35.43it/s][A
 75%|███████▍  | 327/437 [00:07<00:02, 37.80it/s][A
 76%|███████▌  | 332/437 [00:07<00:02, 39.60it/s][A
 77%|███████▋  | 337/437 [00:07<00:02, 41.12it/s][A
 78%|███████▊  | 342/437 [00:07<00:02, 42.18it/s][A
 79%|███████▉  | 347/437 [00:08<00:02, 43.03it/s][A
 81%|████████  | 352/437 [00:08<00:01, 43.62it/s][A
 82%|████████▏ | 357/437 [00:08<00:01, 43.90it/s][A
 83%|████████▎ | 362/437 [00:08<00:01, 43.72it/s][A
 84%|████████▍ | 367/437 [00:08<00:01, 43.68it/s][A
 85%|████████▌ | 372/437 [00:08<00:01, 43.87it/s][A
 86%|████████▋ | 377/437 [00:08<00:01, 44.16it/s][A
 87%|████████▋ | 382/437 [00:08<00:01, 44.43it/s][A
 89%|████████▊ | 387/437 [00:08<00:01, 44.68it/s][A
 90%|████████▉ | 392/437 [00:09<00:01, 44.82it/s][A
 91%|█████████ | 397/437 [00:09<00:00, 44.91it/s][A
 92%|█████████▏| 402/437 [00:09<00:00, 44.72it/s][A
 93%|█████████▎| 407/437 [00:09<00:00, 44.40it/s][A
 94%|█████████▍| 412/437 [00:09<00:00, 44.25it/s][A
 95%|█████████▌| 417/437 [00:09<00:00, 44.27it/s][A
 97%|█████████▋| 422/437 [00:09<00:00, 44.41it/s][A
 98%|█████████▊| 427/437 [00:09<00:00, 44.58it/s][A
 99%|█████████▉| 432/437 [00:09<00:00, 44.73it/s][A
100%|██████████| 437/437 [00:10<00:00, 44.83it/s][A
                                                 [A                                                 
100%|██████████| 437/437 [00:10<00:00, 44.83it/s][A100%|██████████| 585/585 [09:30<00:00,  3.48it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 04:44:09,338 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
[INFO|configuration_utils.py:351] 2023-08-29 04:44:10,033 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:44:37,248 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:44:37,974 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:44:38,315 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 04:45:08,228 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 04:45:08,273 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117 (score: 1.1281837224960327).
                                                 100%|██████████| 585/585 [10:46<00:00,  3.48it/s]100%|██████████| 585/585 [10:46<00:00,  1.10s/it]
[INFO|trainer.py:1894] 2023-08-29 04:45:24,835 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 04:45:24,980 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 04:45:38,632 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 04:45:39,136 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 04:45:39,372 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:45:41,398 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:41,398 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:41,398 >>   train_loss               =     0.4004
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:41,399 >>   train_runtime            = 0:10:46.13
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:41,399 >>   train_samples            =       7500
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:41,399 >>   train_samples_per_second =     58.037
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:41,399 >>   train_steps_per_second   =      0.905
{'eval_loss': 1.187245488166809, 'eval_runtime': 10.0507, 'eval_samples_per_second': 347.437, 'eval_steps_per_second': 43.479, 'epoch': 5.0}
{'train_runtime': 646.139, 'train_samples_per_second': 58.037, 'train_steps_per_second': 0.905, 'train_loss': 0.400425048567291, 'epoch': 5.0}
08/29/2023 04:45:42 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 04:45:42,370 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 04:45:42,370 >>   Num examples = 3492
[INFO|trainer.py:2145] 2023-08-29 04:45:42,370 >>   Batch size = 8
  0%|          | 0/437 [00:00<?, ?it/s]  1%|▏         | 6/437 [00:00<00:07, 55.46it/s]  3%|▎         | 12/437 [00:00<00:08, 49.37it/s]  4%|▍         | 17/437 [00:00<00:08, 47.35it/s]  5%|▌         | 22/437 [00:00<00:08, 46.72it/s]  6%|▌         | 27/437 [00:00<00:08, 46.22it/s]  7%|▋         | 32/437 [00:00<00:08, 46.08it/s]  8%|▊         | 37/437 [00:00<00:08, 45.90it/s] 10%|▉         | 42/437 [00:00<00:08, 45.45it/s] 11%|█         | 47/437 [00:01<00:08, 44.83it/s] 12%|█▏        | 52/437 [00:01<00:08, 44.65it/s] 13%|█▎        | 57/437 [00:01<00:08, 44.68it/s] 14%|█▍        | 62/437 [00:01<00:08, 44.91it/s] 15%|█▌        | 67/437 [00:01<00:08, 45.05it/s] 16%|█▋        | 72/437 [00:01<00:08, 45.25it/s] 18%|█▊        | 77/437 [00:01<00:07, 45.25it/s] 19%|█▉        | 82/437 [00:01<00:07, 45.28it/s] 20%|█▉        | 87/437 [00:01<00:07, 45.01it/s] 21%|██        | 92/437 [00:02<00:07, 44.72it/s] 22%|██▏       | 97/437 [00:02<00:07, 44.58it/s] 23%|██▎       | 102/437 [00:02<00:07, 44.67it/s] 24%|██▍       | 107/437 [00:02<00:07, 44.81it/s] 26%|██▌       | 112/437 [00:02<00:07, 44.98it/s] 27%|██▋       | 117/437 [00:02<00:07, 45.16it/s] 28%|██▊       | 122/437 [00:02<00:06, 45.18it/s] 29%|██▉       | 127/437 [00:02<00:06, 45.15it/s] 30%|███       | 132/437 [00:02<00:06, 44.92it/s] 31%|███▏      | 137/437 [00:03<00:07, 39.34it/s] 32%|███▏      | 142/437 [00:03<00:07, 40.92it/s] 34%|███▎      | 147/437 [00:03<00:06, 42.23it/s] 35%|███▍      | 152/437 [00:03<00:06, 42.99it/s] 36%|███▌      | 157/437 [00:03<00:06, 43.67it/s] 37%|███▋      | 162/437 [00:03<00:06, 44.18it/s] 38%|███▊      | 167/437 [00:03<00:06, 44.56it/s] 39%|███▉      | 172/437 [00:03<00:05, 44.82it/s] 41%|████      | 177/437 [00:03<00:05, 44.44it/s] 42%|████▏     | 182/437 [00:04<00:05, 44.30it/s] 43%|████▎     | 187/437 [00:04<00:05, 44.28it/s] 44%|████▍     | 192/437 [00:04<00:05, 44.59it/s] 45%|████▌     | 197/437 [00:04<00:05, 44.78it/s] 46%|████▌     | 202/437 [00:04<00:05, 44.96it/s] 47%|████▋     | 207/437 [00:04<00:05, 45.10it/s] 49%|████▊     | 212/437 [00:04<00:04, 45.20it/s] 50%|████▉     | 217/437 [00:04<00:04, 45.08it/s] 51%|█████     | 222/437 [00:04<00:04, 44.74it/s] 52%|█████▏    | 227/437 [00:05<00:04, 44.58it/s] 53%|█████▎    | 232/437 [00:05<00:04, 44.56it/s] 54%|█████▍    | 237/437 [00:05<00:04, 44.75it/s] 55%|█████▌    | 242/437 [00:05<00:04, 44.74it/s] 57%|█████▋    | 247/437 [00:05<00:04, 44.83it/s] 58%|█████▊    | 252/437 [00:05<00:04, 45.02it/s] 59%|█████▉    | 257/437 [00:05<00:03, 45.09it/s] 60%|█████▉    | 262/437 [00:05<00:03, 45.08it/s] 61%|██████    | 267/437 [00:05<00:03, 44.89it/s] 62%|██████▏   | 272/437 [00:06<00:04, 39.15it/s] 63%|██████▎   | 277/437 [00:06<00:03, 40.86it/s] 65%|██████▍   | 282/437 [00:06<00:03, 42.18it/s] 66%|██████▌   | 287/437 [00:06<00:03, 43.10it/s] 67%|██████▋   | 292/437 [00:06<00:03, 43.69it/s] 68%|██████▊   | 297/437 [00:06<00:03, 44.10it/s] 69%|██████▉   | 302/437 [00:06<00:03, 44.43it/s] 70%|███████   | 307/437 [00:06<00:02, 44.35it/s] 71%|███████▏  | 312/437 [00:07<00:02, 44.00it/s] 73%|███████▎  | 317/437 [00:07<00:02, 43.95it/s] 74%|███████▎  | 322/437 [00:07<00:02, 44.14it/s] 75%|███████▍  | 327/437 [00:07<00:02, 44.42it/s] 76%|███████▌  | 332/437 [00:07<00:02, 44.67it/s] 77%|███████▋  | 337/437 [00:07<00:02, 44.81it/s] 78%|███████▊  | 342/437 [00:07<00:02, 44.93it/s] 79%|███████▉  | 347/437 [00:07<00:02, 44.92it/s] 81%|████████  | 352/437 [00:07<00:01, 44.67it/s] 82%|████████▏ | 357/437 [00:08<00:01, 44.32it/s] 83%|████████▎ | 362/437 [00:08<00:01, 44.13it/s] 84%|████████▍ | 367/437 [00:08<00:01, 44.30it/s] 85%|████████▌ | 372/437 [00:08<00:01, 44.56it/s] 86%|████████▋ | 377/437 [00:08<00:01, 44.79it/s] 87%|████████▋ | 382/437 [00:08<00:01, 44.96it/s] 89%|████████▊ | 387/437 [00:08<00:01, 45.03it/s] 90%|████████▉ | 392/437 [00:08<00:01, 44.95it/s] 91%|█████████ | 397/437 [00:08<00:00, 44.64it/s] 92%|█████████▏| 402/437 [00:09<00:00, 44.47it/s] 93%|█████████▎| 407/437 [00:09<00:00, 36.96it/s] 94%|█████████▍| 412/437 [00:09<00:00, 39.08it/s] 95%|█████████▌| 417/437 [00:09<00:00, 40.70it/s] 97%|█████████▋| 422/437 [00:09<00:00, 42.02it/s] 98%|█████████▊| 427/437 [00:09<00:00, 42.90it/s] 99%|█████████▉| 432/437 [00:09<00:00, 43.66it/s]100%|██████████| 437/437 [00:09<00:00, 44.10it/s]100%|██████████| 437/437 [00:09<00:00, 44.18it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 04:45:52,284 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:52,284 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:52,284 >>   eval_loss               =     1.1282
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:52,284 >>   eval_runtime            = 0:00:09.91
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:52,284 >>   eval_samples            =       3492
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:52,284 >>   eval_samples_per_second =    352.243
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:52,284 >>   eval_steps_per_second   =     44.081
[INFO|trainer_pt_utils.py:913] 2023-08-29 04:45:52,284 >>   perplexity              =       3.09
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:09,200 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:09,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:09,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:09,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:09,284 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:46:10,146 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:46:10,148 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:46:10,855 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:46:12,087 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:46:12,087 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:15,835 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:15,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:15,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:15,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:46:15,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:46:17,119 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:46:17,121 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:46:17,838 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:46:18,172 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:46:18,172 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-585
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-351
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/generator/iter5/model/checkpoint-117
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/dev.jsonl', 'labels': ['composer', 'main subject', 'participant in', 'platform', 'taxon rank'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11742
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11842, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.53it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.43it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.47it/s]Extractor Predicting: 15it [00:10,  1.47it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.46it/s]Extractor Predicting: 18it [00:12,  1.46it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.52it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.54it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:21,  1.53it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.50it/s]Extractor Predicting: 35it [00:23,  1.52it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.52it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:25,  1.51it/s]Extractor Predicting: 40it [00:26,  1.54it/s]Extractor Predicting: 41it [00:27,  1.55it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.55it/s]Extractor Predicting: 44it [00:29,  1.49it/s]Extractor Predicting: 45it [00:29,  1.49it/s]Extractor Predicting: 46it [00:30,  1.50it/s]Extractor Predicting: 47it [00:31,  1.52it/s]Extractor Predicting: 48it [00:31,  1.52it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:33,  1.53it/s]Extractor Predicting: 51it [00:33,  1.51it/s]Extractor Predicting: 52it [00:34,  1.51it/s]Extractor Predicting: 53it [00:35,  1.52it/s]Extractor Predicting: 54it [00:35,  1.47it/s]Extractor Predicting: 55it [00:36,  1.49it/s]Extractor Predicting: 56it [00:37,  1.52it/s]Extractor Predicting: 57it [00:37,  1.52it/s]Extractor Predicting: 58it [00:38,  1.56it/s]Extractor Predicting: 59it [00:39,  1.49it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:40,  1.48it/s]Extractor Predicting: 62it [00:41,  1.49it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.43it/s]Extractor Predicting: 65it [00:43,  1.45it/s]Extractor Predicting: 66it [00:43,  1.45it/s]Extractor Predicting: 67it [00:44,  1.45it/s]Extractor Predicting: 68it [00:45,  1.45it/s]Extractor Predicting: 69it [00:45,  1.46it/s]Extractor Predicting: 70it [00:46,  1.46it/s]Extractor Predicting: 71it [00:47,  1.40it/s]Extractor Predicting: 72it [00:48,  1.41it/s]Extractor Predicting: 73it [00:48,  1.44it/s]Extractor Predicting: 74it [00:49,  1.42it/s]Extractor Predicting: 75it [00:50,  1.47it/s]Extractor Predicting: 76it [00:50,  1.43it/s]Extractor Predicting: 77it [00:51,  1.43it/s]Extractor Predicting: 78it [00:52,  1.46it/s]Extractor Predicting: 79it [00:52,  1.44it/s]Extractor Predicting: 80it [00:53,  1.43it/s]Extractor Predicting: 81it [00:54,  1.38it/s]Extractor Predicting: 82it [00:55,  1.43it/s]Extractor Predicting: 83it [00:55,  1.46it/s]Extractor Predicting: 84it [00:56,  1.49it/s]Extractor Predicting: 85it [00:57,  1.37it/s]Extractor Predicting: 86it [00:58,  1.32it/s]Extractor Predicting: 87it [00:58,  1.38it/s]Extractor Predicting: 88it [00:59,  1.43it/s]Extractor Predicting: 89it [00:59,  1.50it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:01,  1.55it/s]Extractor Predicting: 92it [01:01,  1.62it/s]Extractor Predicting: 93it [01:02,  1.66it/s]Extractor Predicting: 94it [01:02,  1.70it/s]Extractor Predicting: 95it [01:03,  1.65it/s]Extractor Predicting: 96it [01:04,  1.68it/s]Extractor Predicting: 97it [01:04,  1.60it/s]Extractor Predicting: 98it [01:05,  1.60it/s]Extractor Predicting: 99it [01:05,  1.67it/s]Extractor Predicting: 100it [01:06,  1.68it/s]Extractor Predicting: 101it [01:07,  1.66it/s]Extractor Predicting: 102it [01:07,  1.57it/s]Extractor Predicting: 103it [01:08,  1.59it/s]Extractor Predicting: 104it [01:09,  1.56it/s]Extractor Predicting: 105it [01:09,  1.58it/s]Extractor Predicting: 106it [01:10,  1.61it/s]Extractor Predicting: 107it [01:11,  1.49it/s]Extractor Predicting: 108it [01:11,  1.55it/s]Extractor Predicting: 109it [01:12,  1.59it/s]Extractor Predicting: 110it [01:12,  1.62it/s]Extractor Predicting: 111it [01:13,  1.65it/s]Extractor Predicting: 112it [01:14,  1.55it/s]Extractor Predicting: 113it [01:14,  1.61it/s]Extractor Predicting: 114it [01:15,  1.59it/s]Extractor Predicting: 115it [01:15,  1.63it/s]Extractor Predicting: 116it [01:16,  1.60it/s]Extractor Predicting: 117it [01:17,  1.58it/s]Extractor Predicting: 118it [01:17,  1.52it/s]Extractor Predicting: 119it [01:18,  1.51it/s]Extractor Predicting: 120it [01:19,  1.49it/s]Extractor Predicting: 121it [01:19,  1.51it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:21,  1.44it/s]Extractor Predicting: 124it [01:22,  1.44it/s]Extractor Predicting: 125it [01:22,  1.46it/s]Extractor Predicting: 126it [01:23,  1.46it/s]Extractor Predicting: 127it [01:24,  1.47it/s]Extractor Predicting: 128it [01:24,  1.42it/s]Extractor Predicting: 129it [01:25,  1.46it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:26,  1.48it/s]Extractor Predicting: 132it [01:27,  1.48it/s]Extractor Predicting: 133it [01:28,  1.44it/s]Extractor Predicting: 134it [01:28,  1.46it/s]Extractor Predicting: 135it [01:29,  1.48it/s]Extractor Predicting: 136it [01:30,  1.47it/s]Extractor Predicting: 137it [01:30,  1.48it/s]Extractor Predicting: 138it [01:31,  1.46it/s]Extractor Predicting: 139it [01:32,  1.48it/s]Extractor Predicting: 140it [01:32,  1.52it/s]Extractor Predicting: 141it [01:33,  1.49it/s]Extractor Predicting: 142it [01:34,  1.49it/s]Extractor Predicting: 143it [01:35,  1.45it/s]Extractor Predicting: 144it [01:35,  1.49it/s]Extractor Predicting: 144it [01:35,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:47,700 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:47,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:47,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:47,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:47,786 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:49:48,916 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:49:48,917 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:49:49,598 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:49:50,814 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:49:50,929 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:54,386 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:54,464 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:54,464 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:54,464 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:49:54,464 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:49:55,797 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:49:55,798 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:49:56,528 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:49:56,852 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:49:56,852 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5003843197540354,
  "recall": 0.18642611683848798,
  "score": 0.2716461506363447,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 19669
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19769, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.68it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.56it/s]Extractor Predicting: 5it [00:03,  1.54it/s]Extractor Predicting: 6it [00:03,  1.49it/s]Extractor Predicting: 7it [00:04,  1.52it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.51it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.41it/s]Extractor Predicting: 22it [00:14,  1.41it/s]Extractor Predicting: 23it [00:15,  1.46it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:17,  1.48it/s]Extractor Predicting: 28it [00:18,  1.49it/s]Extractor Predicting: 29it [00:19,  1.49it/s]Extractor Predicting: 30it [00:19,  1.46it/s]Extractor Predicting: 31it [00:20,  1.44it/s]Extractor Predicting: 32it [00:21,  1.46it/s]Extractor Predicting: 33it [00:21,  1.50it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:23,  1.54it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.59it/s]Extractor Predicting: 38it [00:25,  1.60it/s]Extractor Predicting: 39it [00:25,  1.60it/s]Extractor Predicting: 40it [00:26,  1.60it/s]Extractor Predicting: 41it [00:26,  1.56it/s]Extractor Predicting: 42it [00:27,  1.56it/s]Extractor Predicting: 43it [00:28,  1.57it/s]Extractor Predicting: 44it [00:28,  1.58it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.57it/s]Extractor Predicting: 49it [00:32,  1.57it/s]Extractor Predicting: 50it [00:32,  1.57it/s]Extractor Predicting: 51it [00:33,  1.51it/s]Extractor Predicting: 52it [00:34,  1.54it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:35,  1.55it/s]Extractor Predicting: 56it [00:36,  1.48it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:37,  1.60it/s]Extractor Predicting: 59it [00:38,  1.64it/s]Extractor Predicting: 60it [00:39,  1.62it/s]Extractor Predicting: 61it [00:39,  1.53it/s]Extractor Predicting: 62it [00:40,  1.56it/s]Extractor Predicting: 63it [00:40,  1.59it/s]Extractor Predicting: 64it [00:41,  1.56it/s]Extractor Predicting: 65it [00:42,  1.57it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:43,  1.54it/s]Extractor Predicting: 68it [00:44,  1.62it/s]Extractor Predicting: 69it [00:44,  1.60it/s]Extractor Predicting: 70it [00:45,  1.60it/s]Extractor Predicting: 71it [00:46,  1.60it/s]Extractor Predicting: 72it [00:46,  1.61it/s]Extractor Predicting: 73it [00:47,  1.58it/s]Extractor Predicting: 74it [00:48,  1.52it/s]Extractor Predicting: 75it [00:48,  1.55it/s]Extractor Predicting: 76it [00:49,  1.57it/s]Extractor Predicting: 77it [00:49,  1.55it/s]Extractor Predicting: 78it [00:50,  1.57it/s]Extractor Predicting: 79it [00:51,  1.52it/s]Extractor Predicting: 80it [00:51,  1.53it/s]Extractor Predicting: 81it [00:52,  1.54it/s]Extractor Predicting: 82it [00:53,  1.54it/s]Extractor Predicting: 83it [00:53,  1.56it/s]Extractor Predicting: 84it [00:54,  1.51it/s]Extractor Predicting: 85it [00:55,  1.41it/s]Extractor Predicting: 86it [00:56,  1.45it/s]Extractor Predicting: 87it [00:56,  1.48it/s]Extractor Predicting: 88it [00:57,  1.50it/s]Extractor Predicting: 89it [00:57,  1.50it/s]Extractor Predicting: 90it [00:58,  1.52it/s]Extractor Predicting: 91it [00:59,  1.52it/s]Extractor Predicting: 92it [00:59,  1.50it/s]Extractor Predicting: 93it [01:00,  1.55it/s]Extractor Predicting: 94it [01:01,  1.47it/s]Extractor Predicting: 95it [01:01,  1.49it/s]Extractor Predicting: 96it [01:02,  1.52it/s]Extractor Predicting: 97it [01:03,  1.54it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:04,  1.53it/s]Extractor Predicting: 100it [01:05,  1.56it/s]Extractor Predicting: 101it [01:05,  1.58it/s]Extractor Predicting: 102it [01:06,  1.59it/s]Extractor Predicting: 103it [01:07,  1.54it/s]Extractor Predicting: 104it [01:07,  1.50it/s]Extractor Predicting: 105it [01:08,  1.52it/s]Extractor Predicting: 106it [01:09,  1.52it/s]Extractor Predicting: 107it [01:09,  1.52it/s]Extractor Predicting: 108it [01:10,  1.55it/s]Extractor Predicting: 109it [01:11,  1.50it/s]Extractor Predicting: 110it [01:11,  1.53it/s]Extractor Predicting: 111it [01:12,  1.52it/s]Extractor Predicting: 112it [01:13,  1.51it/s]Extractor Predicting: 113it [01:13,  1.51it/s]Extractor Predicting: 114it [01:14,  1.47it/s]Extractor Predicting: 115it [01:15,  1.49it/s]Extractor Predicting: 116it [01:15,  1.54it/s]Extractor Predicting: 117it [01:16,  1.55it/s]Extractor Predicting: 118it [01:16,  1.57it/s]Extractor Predicting: 119it [01:17,  1.57it/s]Extractor Predicting: 120it [01:18,  1.51it/s]Extractor Predicting: 121it [01:18,  1.55it/s]Extractor Predicting: 122it [01:19,  1.57it/s]Extractor Predicting: 123it [01:20,  1.60it/s]Extractor Predicting: 124it [01:20,  1.62it/s]Extractor Predicting: 125it [01:21,  1.57it/s]Extractor Predicting: 126it [01:21,  1.59it/s]Extractor Predicting: 127it [01:22,  1.61it/s]Extractor Predicting: 128it [01:23,  1.60it/s]Extractor Predicting: 129it [01:23,  1.57it/s]Extractor Predicting: 130it [01:24,  1.54it/s]Extractor Predicting: 131it [01:25,  1.58it/s]Extractor Predicting: 132it [01:25,  1.59it/s]Extractor Predicting: 133it [01:26,  1.61it/s]Extractor Predicting: 134it [01:26,  1.63it/s]Extractor Predicting: 135it [01:27,  1.60it/s]Extractor Predicting: 136it [01:28,  1.60it/s]Extractor Predicting: 137it [01:28,  1.61it/s]Extractor Predicting: 138it [01:29,  1.59it/s]Extractor Predicting: 139it [01:30,  1.61it/s]Extractor Predicting: 140it [01:30,  1.63it/s]Extractor Predicting: 141it [01:31,  1.65it/s]Extractor Predicting: 142it [01:31,  1.63it/s]Extractor Predicting: 143it [01:32,  1.61it/s]Extractor Predicting: 144it [01:33,  1.62it/s]Extractor Predicting: 145it [01:33,  1.60it/s]Extractor Predicting: 146it [01:34,  1.63it/s]Extractor Predicting: 147it [01:34,  1.65it/s]Extractor Predicting: 148it [01:35,  1.60it/s]Extractor Predicting: 149it [01:36,  1.63it/s]Extractor Predicting: 150it [01:36,  1.56it/s]Extractor Predicting: 151it [01:37,  1.57it/s]Extractor Predicting: 152it [01:38,  1.60it/s]Extractor Predicting: 153it [01:38,  1.58it/s]Extractor Predicting: 154it [01:39,  1.62it/s]Extractor Predicting: 155it [01:40,  1.58it/s]Extractor Predicting: 156it [01:40,  1.62it/s]Extractor Predicting: 157it [01:41,  1.64it/s]Extractor Predicting: 158it [01:41,  1.71it/s]Extractor Predicting: 159it [01:42,  1.68it/s]Extractor Predicting: 160it [01:43,  1.66it/s]Extractor Predicting: 161it [01:43,  1.65it/s]Extractor Predicting: 162it [01:44,  1.64it/s]Extractor Predicting: 163it [01:44,  1.67it/s]Extractor Predicting: 164it [01:45,  1.68it/s]Extractor Predicting: 165it [01:45,  1.71it/s]Extractor Predicting: 166it [01:46,  1.72it/s]Extractor Predicting: 167it [01:47,  1.70it/s]Extractor Predicting: 168it [01:47,  1.64it/s]Extractor Predicting: 169it [01:48,  1.64it/s]Extractor Predicting: 170it [01:49,  1.56it/s]Extractor Predicting: 171it [01:49,  1.57it/s]Extractor Predicting: 172it [01:50,  1.59it/s]Extractor Predicting: 173it [01:51,  1.57it/s]Extractor Predicting: 174it [01:51,  1.58it/s]Extractor Predicting: 175it [01:52,  1.37it/s]Extractor Predicting: 176it [01:53,  1.39it/s]Extractor Predicting: 177it [01:53,  1.47it/s]Extractor Predicting: 178it [01:54,  1.49it/s]Extractor Predicting: 179it [01:55,  1.54it/s]Extractor Predicting: 180it [01:55,  1.53it/s]Extractor Predicting: 181it [01:56,  1.52it/s]Extractor Predicting: 182it [01:57,  1.55it/s]Extractor Predicting: 183it [01:57,  1.56it/s]Extractor Predicting: 184it [01:58,  1.55it/s]Extractor Predicting: 185it [01:59,  1.48it/s]Extractor Predicting: 186it [01:59,  1.50it/s]Extractor Predicting: 187it [02:00,  1.52it/s]Extractor Predicting: 188it [02:01,  1.50it/s]Extractor Predicting: 189it [02:01,  1.50it/s]Extractor Predicting: 190it [02:02,  1.47it/s]Extractor Predicting: 191it [02:03,  1.52it/s]Extractor Predicting: 192it [02:03,  1.57it/s]Extractor Predicting: 193it [02:04,  1.51it/s]Extractor Predicting: 194it [02:05,  1.50it/s]Extractor Predicting: 195it [02:05,  1.50it/s]Extractor Predicting: 196it [02:06,  1.52it/s]Extractor Predicting: 197it [02:06,  1.55it/s]Extractor Predicting: 198it [02:07,  1.54it/s]Extractor Predicting: 199it [02:08,  1.57it/s]Extractor Predicting: 200it [02:08,  1.56it/s]Extractor Predicting: 201it [02:09,  1.60it/s]Extractor Predicting: 202it [02:10,  1.61it/s]Extractor Predicting: 203it [02:10,  1.59it/s]Extractor Predicting: 204it [02:11,  1.58it/s]Extractor Predicting: 205it [02:12,  1.55it/s]Extractor Predicting: 206it [02:12,  1.55it/s]Extractor Predicting: 207it [02:13,  1.57it/s]Extractor Predicting: 208it [02:13,  1.55it/s]Extractor Predicting: 209it [02:14,  1.56it/s]Extractor Predicting: 210it [02:15,  1.52it/s]Extractor Predicting: 211it [02:15,  1.54it/s]Extractor Predicting: 212it [02:16,  1.42it/s]Extractor Predicting: 213it [02:17,  1.46it/s]Extractor Predicting: 214it [02:18,  1.47it/s]Extractor Predicting: 215it [02:18,  1.50it/s]Extractor Predicting: 216it [02:19,  1.54it/s]Extractor Predicting: 217it [02:20,  1.52it/s]Extractor Predicting: 218it [02:20,  1.52it/s]Extractor Predicting: 219it [02:21,  1.52it/s]Extractor Predicting: 220it [02:21,  1.55it/s]Extractor Predicting: 221it [02:22,  1.56it/s]Extractor Predicting: 222it [02:23,  1.53it/s]Extractor Predicting: 223it [02:23,  1.57it/s]Extractor Predicting: 224it [02:24,  1.57it/s]Extractor Predicting: 225it [02:25,  1.57it/s]Extractor Predicting: 226it [02:25,  1.56it/s]Extractor Predicting: 227it [02:26,  1.52it/s]Extractor Predicting: 228it [02:27,  1.54it/s]Extractor Predicting: 229it [02:27,  1.57it/s]Extractor Predicting: 230it [02:28,  1.59it/s]Extractor Predicting: 231it [02:28,  1.58it/s]Extractor Predicting: 232it [02:29,  1.53it/s]Extractor Predicting: 233it [02:30,  1.55it/s]Extractor Predicting: 234it [02:30,  1.57it/s]Extractor Predicting: 235it [02:31,  1.59it/s]Extractor Predicting: 236it [02:32,  1.60it/s]Extractor Predicting: 237it [02:32,  1.55it/s]Extractor Predicting: 238it [02:33,  1.56it/s]Extractor Predicting: 239it [02:34,  1.55it/s]Extractor Predicting: 240it [02:34,  1.59it/s]Extractor Predicting: 241it [02:35,  1.60it/s]Extractor Predicting: 242it [02:35,  1.58it/s]Extractor Predicting: 243it [02:36,  1.59it/s]Extractor Predicting: 244it [02:37,  1.60it/s]Extractor Predicting: 245it [02:37,  1.60it/s]Extractor Predicting: 246it [02:38,  1.60it/s]Extractor Predicting: 247it [02:39,  1.59it/s]Extractor Predicting: 248it [02:39,  1.59it/s]Extractor Predicting: 249it [02:40,  1.61it/s]Extractor Predicting: 250it [02:40,  1.63it/s]Extractor Predicting: 251it [02:41,  1.59it/s]Extractor Predicting: 252it [02:42,  1.54it/s]Extractor Predicting: 253it [02:42,  1.61it/s]Extractor Predicting: 254it [02:43,  1.44it/s]Extractor Predicting: 255it [02:44,  1.47it/s]Extractor Predicting: 256it [02:44,  1.50it/s]Extractor Predicting: 257it [02:45,  1.49it/s]Extractor Predicting: 258it [02:46,  1.54it/s]Extractor Predicting: 259it [02:46,  1.58it/s]Extractor Predicting: 260it [02:47,  1.59it/s]Extractor Predicting: 261it [02:48,  1.57it/s]Extractor Predicting: 262it [02:48,  1.59it/s]Extractor Predicting: 263it [02:49,  1.59it/s]Extractor Predicting: 264it [02:49,  1.61it/s]Extractor Predicting: 265it [02:50,  1.56it/s]Extractor Predicting: 266it [02:51,  1.61it/s]Extractor Predicting: 267it [02:51,  1.58it/s]Extractor Predicting: 268it [02:52,  1.58it/s]Extractor Predicting: 269it [02:53,  1.61it/s]Extractor Predicting: 270it [02:53,  1.55it/s]Extractor Predicting: 271it [02:54,  1.60it/s]Extractor Predicting: 272it [02:54,  1.65it/s]Extractor Predicting: 273it [02:55,  1.62it/s]Extractor Predicting: 274it [02:56,  1.63it/s]Extractor Predicting: 275it [02:56,  1.54it/s]Extractor Predicting: 276it [02:57,  1.56it/s]Extractor Predicting: 277it [02:58,  1.58it/s]Extractor Predicting: 278it [02:58,  1.58it/s]Extractor Predicting: 279it [02:59,  1.57it/s]Extractor Predicting: 280it [03:00,  1.52it/s]Extractor Predicting: 281it [03:00,  1.51it/s]Extractor Predicting: 281it [03:00,  1.55it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:18,883 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:18,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:18,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:18,928 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:18,929 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 04:53:20,062 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 04:53:20,063 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:53:20,765 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 04:53:21,902 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:53:21,932 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:25,153 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:25,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:25,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:25,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 04:53:25,155 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 04:53:26,182 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 04:53:26,183 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 04:53:26,894 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 04:53:27,271 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 04:53:27,271 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.42339544513457555,
  "recall": 0.18196648376093727,
  "score": 0.2545379110050825,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1121
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1221, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.41it/s]Extractor Predicting: 5it [00:03,  1.45it/s]Extractor Predicting: 6it [00:03,  1.84it/s]Extractor Predicting: 6it [00:03,  1.60it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2222222222222222,
  "recall": 0.038910505836575876,
  "score": 0.06622516556291393,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_10_seed_0/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/', 'labels': ['competition class', 'location', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'religion'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_10_seed_0/extractor/iter1/results_single_is_eval_True_limit5000.json'
