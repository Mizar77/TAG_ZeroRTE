Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_15_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': False, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:20<06:28, 20.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:39<05:48, 19.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:52<04:39, 16.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [01:08<04:22, 16.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:23<04:01, 16.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:40<03:48, 16.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:56<03:28, 16.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [02:13<03:16, 16.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:32<03:08, 17.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:46<02:43, 16.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [03:02<02:24, 16.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [03:17<02:07, 15.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [03:32<01:48, 15.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:48<01:33, 15.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [04:04<01:18, 15.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [04:21<01:05, 16.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [04:38<00:48, 16.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:52<00:31, 15.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [05:12<00:17, 17.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [05:29<00:00, 16.88s/it]Generating: 100%|██████████| 20/20 [05:29<00:00, 16.46s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 205, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 304, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 462, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 560, 'raw': 704}
{'target': 600, 'success': 585, 'raw': 736}
{'target': 600, 'success': 605, 'raw': 768}
{'prompt': 'Relation : followed by .', 'success_rate': 0.7877604166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : military rank . Context : On 31 March 1859 , his unit , called " The Three Knights " , reached the Rhine , having previously crossed the Danube in May . Head Entity : Three Knights , Tail Entity : cavalry .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 137, 'raw': 192}
{'target': 600, 'success': 158, 'raw': 224}
{'target': 600, 'success': 179, 'raw': 256}
{'target': 600, 'success': 201, 'raw': 288}
{'target': 600, 'success': 225, 'raw': 320}
{'target': 600, 'success': 247, 'raw': 352}
{'target': 600, 'success': 268, 'raw': 384}
{'target': 600, 'success': 290, 'raw': 416}
{'target': 600, 'success': 312, 'raw': 448}
{'target': 600, 'success': 336, 'raw': 480}
{'target': 600, 'success': 361, 'raw': 512}
{'target': 600, 'success': 382, 'raw': 544}
{'target': 600, 'success': 409, 'raw': 576}
{'target': 600, 'success': 436, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 479, 'raw': 672}
{'target': 600, 'success': 502, 'raw': 704}
{'target': 600, 'success': 522, 'raw': 736}
{'target': 600, 'success': 545, 'raw': 768}
{'target': 600, 'success': 567, 'raw': 800}
{'target': 600, 'success': 589, 'raw': 832}
{'target': 600, 'success': 612, 'raw': 864}
{'prompt': 'Relation : military rank .', 'success_rate': 0.7083333333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('Tamerlane', 'military rank', '', 'During his time on active duty in the British navy , Tamerlane won a bronze medal in the 1st Squadron of the Royal Navy , in World War II .')"}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 164, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 248, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 329, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 383, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 558, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : operating system .', 'success_rate': 0.8678977272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)', "('American Airlines', 'operating system', '', 'The company was founded in 1969 by American Airlines engineer Neil Armstrong , who had retired from service in 1967 after retiring from the USAF as early as 1958 .')"}}
['Relation : record label . Context : Later in 2008 , the band became a major draw for their debut solo album entitled " The Perfect Day " , released on June 30 , 2008 . Head Entity : The Perfect Day , Tail Entity : single label .\n']
['Relation : record label . Context : Later in 2008 , the band became a major draw for their debut solo album entitled " The Perfect Day " , released on June 30 , 2008 . Head Entity : The Perfect Day , Tail Entity : single label .\n', 'Relation : record label . Context : After the death of The Church and the release of the album , Sommers began publishing his own book on Sommers . Head Entity : The Church and the album , Tail Entity : Sommers .\n']
{'target': 600, 'success': 21, 'raw': 32}
{'target': 600, 'success': 47, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 125, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 178, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 356, 'raw': 448}
{'target': 600, 'success': 381, 'raw': 480}
{'target': 600, 'success': 407, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 459, 'raw': 576}
{'target': 600, 'success': 483, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 530, 'raw': 672}
{'target': 600, 'success': 554, 'raw': 704}
{'target': 600, 'success': 577, 'raw': 736}
{'target': 600, 'success': 604, 'raw': 768}
{'prompt': 'Relation : record label .', 'success_rate': 0.7864583333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 488, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 574, 'raw': 672}
{'target': 600, 'success': 600, 'raw': 704}
{'prompt': 'Relation : tributary .', 'success_rate': 0.8522727272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 121, 'raw': 160}
{'target': 600, 'success': 146, 'raw': 192}
{'target': 600, 'success': 173, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 308, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 357, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 410, 'raw': 512}
{'target': 600, 'success': 438, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 514, 'raw': 640}
{'target': 600, 'success': 541, 'raw': 672}
{'target': 600, 'success': 568, 'raw': 704}
{'target': 600, 'success': 593, 'raw': 736}
{'target': 600, 'success': 614, 'raw': 768}
{'prompt': 'Relation : architect .', 'success_rate': 0.7994791666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : constellation . Context : Later in the year ( 1143–1230 ) , at the conclusion of a series of conflicts , the Kingdom of France lost control in the battle between the Ottoman Empire and the Greek Republic . Head Entity : Kingdom of France , Tail Entity : constellation .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 375, 'raw': 448}
{'target': 600, 'success': 402, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 505, 'raw': 608}
{'target': 600, 'success': 530, 'raw': 640}
{'target': 600, 'success': 555, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 606, 'raw': 736}
{'prompt': 'Relation : constellation .', 'success_rate': 0.8233695652173914, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 261, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 332, 'raw': 416}
{'target': 600, 'success': 361, 'raw': 448}
{'target': 600, 'success': 390, 'raw': 480}
{'target': 600, 'success': 412, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 516, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 72, 'raw': 96}
{'target': 600, 'success': 97, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 148, 'raw': 192}
{'target': 600, 'success': 171, 'raw': 224}
{'target': 600, 'success': 197, 'raw': 256}
{'target': 600, 'success': 219, 'raw': 288}
{'target': 600, 'success': 245, 'raw': 320}
{'target': 600, 'success': 270, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 317, 'raw': 416}
{'target': 600, 'success': 334, 'raw': 448}
{'target': 600, 'success': 354, 'raw': 480}
{'target': 600, 'success': 379, 'raw': 512}
{'target': 600, 'success': 403, 'raw': 544}
{'target': 600, 'success': 421, 'raw': 576}
{'target': 600, 'success': 443, 'raw': 608}
{'target': 600, 'success': 467, 'raw': 640}
{'target': 600, 'success': 491, 'raw': 672}
{'target': 600, 'success': 514, 'raw': 704}
{'target': 600, 'success': 535, 'raw': 736}
{'target': 600, 'success': 554, 'raw': 768}
{'target': 600, 'success': 577, 'raw': 800}
{'target': 600, 'success': 596, 'raw': 832}
{'target': 600, 'success': 616, 'raw': 864}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.7129629629629629, 'errors': {'', '(\'The Encyclopedia of Mammals and Reptiles , Vol .\', \'country of origin\', \'\', \'Gipsy was awarded the Distinguished Award in 2009 by the International Committee for the Study of the Nature of Species for his work in " The Encyclopedia of Mammals and Reptiles , Vol. 1 " .\')', 'not enough values to unpack (expected 2, got 1)', '(\'Munich\', \'country of origin\', \'\', \'" All of Us " is the first song the band have released since they released " A Very Happy Birthday " at the 2009 International Music Festival in Munich .\')'}}
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 183, 'raw': 224}
{'target': 600, 'success': 208, 'raw': 256}
{'target': 600, 'success': 235, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 312, 'raw': 384}
{'target': 600, 'success': 333, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 385, 'raw': 480}
{'target': 600, 'success': 411, 'raw': 512}
{'target': 600, 'success': 435, 'raw': 544}
{'target': 600, 'success': 461, 'raw': 576}
{'target': 600, 'success': 486, 'raw': 608}
{'target': 600, 'success': 510, 'raw': 640}
{'target': 600, 'success': 533, 'raw': 672}
{'target': 600, 'success': 556, 'raw': 704}
{'target': 600, 'success': 582, 'raw': 736}
{'target': 600, 'success': 610, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.7942708333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 339, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 424, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 475, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 585, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : follows .', 'success_rate': 0.8328804347826086, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : league . Context : On 31 occasions in all competitions in the 1974 FIFA World Cup , he represented Australia at the 1958 FIFA Confederations Cup , 1966 FIFA World Cup , 1960 FIFA World Cup , 1971 FIFA World Cup . Head Entity : 1958 FIFA World Cup , Tail Entity : Argentina .\n']
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 69, 'raw': 96}
{'target': 600, 'success': 95, 'raw': 128}
{'target': 600, 'success': 120, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 169, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 242, 'raw': 320}
{'target': 600, 'success': 268, 'raw': 352}
{'target': 600, 'success': 294, 'raw': 384}
{'target': 600, 'success': 319, 'raw': 416}
{'target': 600, 'success': 347, 'raw': 448}
{'target': 600, 'success': 374, 'raw': 480}
{'target': 600, 'success': 397, 'raw': 512}
{'target': 600, 'success': 420, 'raw': 544}
{'target': 600, 'success': 446, 'raw': 576}
{'target': 600, 'success': 471, 'raw': 608}
{'target': 600, 'success': 494, 'raw': 640}
{'target': 600, 'success': 513, 'raw': 672}
{'target': 600, 'success': 538, 'raw': 704}
{'target': 600, 'success': 563, 'raw': 736}
{'target': 600, 'success': 589, 'raw': 768}
{'target': 600, 'success': 615, 'raw': 800}
{'prompt': 'Relation : league .', 'success_rate': 0.76875, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 370, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 479, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 532, 'raw': 608}
{'target': 600, 'success': 561, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8735795454545454, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : member of . Context : Later in the year ( October 1887 ) , a deal was announced that the band would perform in front of many of their members ; in spite of this , the band was unable to perform . Head Entity : band , Tail Entity : Sigmund Kaempfer .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 260, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 447, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 583, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : member of .', 'success_rate': 0.8328804347826086, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 73, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 153, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 203, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 253, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 379, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 434, 'raw': 544}
{'target': 600, 'success': 460, 'raw': 576}
{'target': 600, 'success': 484, 'raw': 608}
{'target': 600, 'success': 505, 'raw': 640}
{'target': 600, 'success': 528, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 576, 'raw': 736}
{'target': 600, 'success': 601, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.7825520833333334, 'errors': {'', "('election of 1937', 'member of political party', '', 'It was defeated in the election of 1937 with 17 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 100, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 147, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 200, 'raw': 256}
{'target': 600, 'success': 226, 'raw': 288}
{'target': 600, 'success': 248, 'raw': 320}
{'target': 600, 'success': 273, 'raw': 352}
{'target': 600, 'success': 298, 'raw': 384}
{'target': 600, 'success': 322, 'raw': 416}
{'target': 600, 'success': 349, 'raw': 448}
{'target': 600, 'success': 375, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 422, 'raw': 544}
{'target': 600, 'success': 450, 'raw': 576}
{'target': 600, 'success': 474, 'raw': 608}
{'target': 600, 'success': 499, 'raw': 640}
{'target': 600, 'success': 525, 'raw': 672}
{'target': 600, 'success': 551, 'raw': 704}
{'target': 600, 'success': 575, 'raw': 736}
{'target': 600, 'success': 604, 'raw': 768}
{'prompt': 'Relation : notable work .', 'success_rate': 0.7864583333333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : operator . Context : Later in the year , the department built a substation to serve the town of Baskerville near Baskerville Station to serve the area outside the city , and then used Baskerville Airfield as a landing base . Head Entity : Baskerville Airfield , Tail Entity : Baskerville Group .\n']
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 349, 'raw': 416}
{'target': 600, 'success': 374, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 506, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 557, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8274456521739131, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 185, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 352, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 461, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 545, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 602, 'raw': 704}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.8551136363636364, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : On 31 March 2014 , the Armenian government appointed him a Vice President of the Armenian Community . Head Entity : Armenian President , Tail Entity : Vice President .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 38, 'raw': 64}
{'target': 600, 'success': 56, 'raw': 96}
{'target': 600, 'success': 72, 'raw': 128}
{'target': 600, 'success': 91, 'raw': 160}
{'target': 600, 'success': 111, 'raw': 192}
{'target': 600, 'success': 133, 'raw': 224}
{'target': 600, 'success': 153, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 196, 'raw': 320}
{'target': 600, 'success': 212, 'raw': 352}
{'target': 600, 'success': 232, 'raw': 384}
{'target': 600, 'success': 249, 'raw': 416}
{'target': 600, 'success': 266, 'raw': 448}
{'target': 600, 'success': 286, 'raw': 480}
{'target': 600, 'success': 309, 'raw': 512}
{'target': 600, 'success': 330, 'raw': 544}
{'target': 600, 'success': 348, 'raw': 576}
{'target': 600, 'success': 369, 'raw': 608}
{'target': 600, 'success': 390, 'raw': 640}
{'target': 600, 'success': 411, 'raw': 672}
{'target': 600, 'success': 426, 'raw': 704}
{'target': 600, 'success': 451, 'raw': 736}
{'target': 600, 'success': 471, 'raw': 768}
{'target': 600, 'success': 491, 'raw': 800}
{'target': 600, 'success': 511, 'raw': 832}
{'target': 600, 'success': 532, 'raw': 864}
{'target': 600, 'success': 554, 'raw': 896}
{'target': 600, 'success': 577, 'raw': 928}
{'target': 600, 'success': 594, 'raw': 960}
{'target': 600, 'success': 616, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.6209677419354839, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 246, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 398, 'raw': 480}
{'target': 600, 'success': 421, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 477, 'raw': 576}
{'target': 600, 'success': 504, 'raw': 608}
{'target': 600, 'success': 531, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 587, 'raw': 704}
{'target': 600, 'success': 613, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8328804347826086, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 16923
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17023, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:21, 21.75s/it]Extractor Estimating: 2it [00:23, 10.06s/it]Extractor Estimating: 3it [00:24,  5.79s/it]Extractor Estimating: 4it [00:24,  3.75s/it]Extractor Estimating: 5it [00:26,  2.86s/it]Extractor Estimating: 6it [00:26,  2.10s/it]Extractor Estimating: 7it [00:27,  1.65s/it]Extractor Estimating: 8it [00:30,  2.07s/it]Extractor Estimating: 9it [00:31,  1.66s/it]Extractor Estimating: 10it [00:31,  1.35s/it]Extractor Estimating: 11it [00:33,  1.35s/it]Extractor Estimating: 12it [00:34,  1.15s/it]Extractor Estimating: 13it [00:34,  1.00it/s]Extractor Estimating: 14it [00:35,  1.13it/s]Extractor Estimating: 15it [00:35,  1.21it/s]Extractor Estimating: 16it [00:36,  1.29it/s]Extractor Estimating: 17it [00:37,  1.36it/s]Extractor Estimating: 18it [00:39,  1.10s/it]Extractor Estimating: 19it [00:39,  1.03it/s]Extractor Estimating: 20it [00:40,  1.11it/s]Extractor Estimating: 21it [00:41,  1.18it/s]Extractor Estimating: 22it [00:42,  1.24it/s]Extractor Estimating: 23it [00:42,  1.35it/s]Extractor Estimating: 24it [00:43,  1.40it/s]Extractor Estimating: 25it [00:43,  1.43it/s]Extractor Estimating: 26it [00:44,  1.49it/s]Extractor Estimating: 27it [00:45,  1.51it/s]Extractor Estimating: 28it [00:45,  1.55it/s]Extractor Estimating: 29it [00:46,  1.58it/s]Extractor Estimating: 30it [00:47,  1.60it/s]Extractor Estimating: 31it [00:47,  1.54it/s]Extractor Estimating: 32it [00:48,  1.55it/s]Extractor Estimating: 33it [00:49,  1.55it/s]Extractor Estimating: 34it [00:49,  1.55it/s]Extractor Estimating: 35it [00:50,  1.53it/s]Extractor Estimating: 36it [00:50,  1.54it/s]Extractor Estimating: 37it [00:51,  1.56it/s]Extractor Estimating: 38it [00:52,  1.61it/s]Extractor Estimating: 39it [00:52,  1.58it/s]Extractor Estimating: 40it [00:53,  1.57it/s]Extractor Estimating: 41it [00:54,  1.59it/s]Extractor Estimating: 42it [00:54,  1.58it/s]Extractor Estimating: 43it [00:55,  1.57it/s]Extractor Estimating: 44it [00:56,  1.58it/s]Extractor Estimating: 45it [00:56,  1.62it/s]Extractor Estimating: 46it [00:57,  1.60it/s]Extractor Estimating: 47it [00:57,  1.57it/s]Extractor Estimating: 48it [00:58,  1.63it/s]Extractor Estimating: 49it [00:59,  1.61it/s]Extractor Estimating: 50it [00:59,  1.62it/s]Extractor Estimating: 51it [01:00,  1.63it/s]Extractor Estimating: 52it [01:00,  1.67it/s]Extractor Estimating: 53it [01:01,  1.72it/s]Extractor Estimating: 54it [01:02,  1.65it/s]Extractor Estimating: 55it [01:02,  1.68it/s]Extractor Estimating: 56it [01:03,  1.60it/s]Extractor Estimating: 57it [01:03,  1.62it/s]Extractor Estimating: 58it [01:04,  1.59it/s]Extractor Estimating: 59it [01:05,  1.62it/s]Extractor Estimating: 60it [01:05,  1.62it/s]Extractor Estimating: 61it [01:06,  1.65it/s]Extractor Estimating: 62it [01:06,  1.69it/s]Extractor Estimating: 63it [01:07,  1.70it/s]Extractor Estimating: 64it [01:08,  1.66it/s]Extractor Estimating: 65it [01:08,  1.52it/s]Extractor Estimating: 66it [01:09,  1.55it/s]Extractor Estimating: 67it [01:10,  1.55it/s]Extractor Estimating: 68it [01:10,  1.60it/s]Extractor Estimating: 69it [01:11,  1.66it/s]Extractor Estimating: 70it [01:11,  1.68it/s]Extractor Estimating: 71it [01:12,  1.65it/s]Extractor Estimating: 72it [01:13,  1.67it/s]Extractor Estimating: 73it [01:13,  1.60it/s]Extractor Estimating: 74it [01:14,  1.58it/s]Extractor Estimating: 75it [01:15,  1.62it/s]Extractor Estimating: 76it [01:15,  1.61it/s]Extractor Estimating: 77it [01:16,  1.59it/s]Extractor Estimating: 78it [01:16,  1.59it/s]Extractor Estimating: 79it [01:17,  1.59it/s]Extractor Estimating: 80it [01:18,  1.62it/s]Extractor Estimating: 81it [01:18,  1.60it/s]Extractor Estimating: 82it [01:19,  1.54it/s]Extractor Estimating: 83it [01:20,  1.57it/s]Extractor Estimating: 84it [01:20,  1.54it/s]Extractor Estimating: 85it [01:21,  1.46it/s]Extractor Estimating: 86it [01:22,  1.46it/s]Extractor Estimating: 87it [01:22,  1.45it/s]Extractor Estimating: 88it [01:23,  1.43it/s]Extractor Estimating: 89it [01:24,  1.41it/s]Extractor Estimating: 90it [01:25,  1.45it/s]Extractor Estimating: 91it [01:25,  1.45it/s]Extractor Estimating: 92it [01:26,  1.41it/s]Extractor Estimating: 93it [01:27,  1.43it/s]Extractor Estimating: 94it [01:27,  1.48it/s]Extractor Estimating: 95it [01:28,  1.46it/s]Extractor Estimating: 96it [01:29,  1.48it/s]Extractor Estimating: 97it [01:29,  1.45it/s]Extractor Estimating: 98it [01:30,  1.44it/s]Extractor Estimating: 99it [01:31,  1.44it/s]Extractor Estimating: 100it [01:31,  1.49it/s]Extractor Estimating: 101it [01:32,  1.52it/s]Extractor Estimating: 102it [01:33,  1.57it/s]Extractor Estimating: 103it [01:33,  1.61it/s]Extractor Estimating: 104it [01:34,  1.60it/s]Extractor Estimating: 105it [01:35,  1.57it/s]Extractor Estimating: 106it [01:35,  1.60it/s]Extractor Estimating: 107it [01:36,  1.43it/s]Extractor Estimating: 108it [01:37,  1.51it/s]Extractor Estimating: 109it [01:37,  1.55it/s]Extractor Estimating: 110it [01:38,  1.59it/s]Extractor Estimating: 111it [01:38,  1.59it/s]Extractor Estimating: 112it [01:39,  1.58it/s]Extractor Estimating: 113it [01:40,  1.54it/s]Extractor Estimating: 114it [01:40,  1.54it/s]Extractor Estimating: 115it [01:41,  1.57it/s]Extractor Estimating: 116it [01:42,  1.57it/s]Extractor Estimating: 117it [01:42,  1.57it/s]Extractor Estimating: 118it [01:43,  1.61it/s]Extractor Estimating: 119it [01:43,  1.62it/s]Extractor Estimating: 120it [01:44,  1.65it/s]Extractor Estimating: 121it [01:45,  1.63it/s]Extractor Estimating: 122it [01:45,  1.60it/s]Extractor Estimating: 123it [01:46,  1.57it/s]Extractor Estimating: 124it [01:47,  1.58it/s]Extractor Estimating: 125it [01:47,  1.63it/s]Extractor Estimating: 126it [01:48,  1.62it/s]Extractor Estimating: 127it [01:48,  1.60it/s]Extractor Estimating: 128it [01:49,  1.65it/s]Extractor Estimating: 129it [01:50,  1.61it/s]Extractor Estimating: 130it [01:50,  1.63it/s]Extractor Estimating: 131it [01:51,  1.57it/s]Extractor Estimating: 132it [01:52,  1.59it/s]Extractor Estimating: 133it [01:52,  1.55it/s]Extractor Estimating: 134it [01:53,  1.55it/s]Extractor Estimating: 135it [01:54,  1.54it/s]Extractor Estimating: 136it [01:54,  1.56it/s]Extractor Estimating: 137it [01:55,  1.54it/s]Extractor Estimating: 138it [01:55,  1.55it/s]Extractor Estimating: 139it [01:56,  1.53it/s]Extractor Estimating: 140it [01:57,  1.51it/s]Extractor Estimating: 141it [01:57,  1.53it/s]Extractor Estimating: 142it [01:58,  1.48it/s]Extractor Estimating: 143it [01:59,  1.52it/s]Extractor Estimating: 144it [01:59,  1.54it/s]Extractor Estimating: 145it [02:00,  1.59it/s]Extractor Estimating: 146it [02:01,  1.53it/s]Extractor Estimating: 147it [02:01,  1.62it/s]Extractor Estimating: 148it [02:02,  1.63it/s]Extractor Estimating: 149it [02:02,  1.64it/s]Extractor Estimating: 150it [02:03,  1.61it/s]Extractor Estimating: 151it [02:04,  1.52it/s]Extractor Estimating: 152it [02:04,  1.55it/s]Extractor Estimating: 153it [02:05,  1.56it/s]Extractor Estimating: 154it [02:06,  1.59it/s]Extractor Estimating: 155it [02:06,  1.60it/s]Extractor Estimating: 156it [02:07,  1.60it/s]Extractor Estimating: 157it [02:07,  1.65it/s]Extractor Estimating: 158it [02:08,  1.63it/s]Extractor Estimating: 159it [02:09,  1.63it/s]Extractor Estimating: 160it [02:09,  1.60it/s]Extractor Estimating: 161it [02:10,  1.55it/s]Extractor Estimating: 162it [02:11,  1.54it/s]Extractor Estimating: 163it [02:11,  1.59it/s]Extractor Estimating: 164it [02:12,  1.60it/s]Extractor Estimating: 165it [02:13,  1.61it/s]Extractor Estimating: 166it [02:13,  1.62it/s]Extractor Estimating: 167it [02:14,  1.64it/s]Extractor Estimating: 168it [02:14,  1.63it/s]Extractor Estimating: 169it [02:15,  1.61it/s]Extractor Estimating: 170it [02:16,  1.55it/s]Extractor Estimating: 171it [02:16,  1.58it/s]Extractor Estimating: 172it [02:17,  1.60it/s]Extractor Estimating: 173it [02:18,  1.59it/s]Extractor Estimating: 174it [02:18,  1.56it/s]Extractor Estimating: 175it [02:19,  1.57it/s]Extractor Estimating: 176it [02:19,  1.58it/s]Extractor Estimating: 177it [02:20,  1.58it/s]Extractor Estimating: 178it [02:21,  1.63it/s]Extractor Estimating: 179it [02:21,  1.62it/s]Extractor Estimating: 180it [02:22,  1.59it/s]Extractor Estimating: 181it [02:23,  1.61it/s]Extractor Estimating: 182it [02:23,  1.58it/s]Extractor Estimating: 183it [02:24,  1.56it/s]Extractor Estimating: 184it [02:25,  1.55it/s]Extractor Estimating: 185it [02:25,  1.55it/s]Extractor Estimating: 186it [02:26,  1.55it/s]Extractor Estimating: 187it [02:27,  1.44it/s]Extractor Estimating: 188it [02:27,  1.53it/s]Extractor Estimating: 189it [02:28,  1.54it/s]Extractor Estimating: 190it [02:28,  1.59it/s]Extractor Estimating: 191it [02:29,  1.56it/s]Extractor Estimating: 192it [02:30,  1.56it/s]Extractor Estimating: 193it [02:30,  1.59it/s]Extractor Estimating: 194it [02:31,  1.57it/s]Extractor Estimating: 195it [02:32,  1.60it/s]Extractor Estimating: 196it [02:32,  1.56it/s]Extractor Estimating: 197it [02:33,  1.61it/s]Extractor Estimating: 198it [02:33,  1.61it/s]Extractor Estimating: 199it [02:34,  1.63it/s]Extractor Estimating: 200it [02:35,  1.64it/s]Extractor Estimating: 201it [02:35,  1.67it/s]Extractor Estimating: 202it [02:36,  1.64it/s]Extractor Estimating: 203it [02:36,  1.66it/s]Extractor Estimating: 204it [02:37,  1.65it/s]Extractor Estimating: 205it [02:38,  1.63it/s]Extractor Estimating: 206it [02:38,  1.53it/s]Extractor Estimating: 207it [02:39,  1.57it/s]Extractor Estimating: 208it [02:40,  1.56it/s]Extractor Estimating: 209it [02:40,  1.56it/s]Extractor Estimating: 210it [02:41,  1.59it/s]Extractor Estimating: 211it [02:42,  1.61it/s]Extractor Estimating: 212it [02:42,  1.62it/s]Extractor Estimating: 213it [02:43,  1.67it/s]Extractor Estimating: 214it [02:43,  1.71it/s]Extractor Estimating: 215it [02:44,  1.68it/s]Extractor Estimating: 216it [02:45,  1.65it/s]Extractor Estimating: 217it [02:45,  1.60it/s]Extractor Estimating: 218it [02:46,  1.62it/s]Extractor Estimating: 219it [02:46,  1.64it/s]Extractor Estimating: 220it [02:47,  1.65it/s]Extractor Estimating: 221it [02:48,  1.63it/s]Extractor Estimating: 222it [02:48,  1.59it/s]Extractor Estimating: 223it [02:49,  1.58it/s]Extractor Estimating: 224it [02:50,  1.54it/s]Extractor Estimating: 225it [02:50,  1.57it/s]Extractor Estimating: 226it [02:51,  1.57it/s]Extractor Estimating: 227it [02:51,  1.58it/s]Extractor Estimating: 228it [02:52,  1.58it/s]Extractor Estimating: 229it [02:53,  1.58it/s]Extractor Estimating: 230it [02:53,  1.63it/s]Extractor Estimating: 231it [02:54,  1.59it/s]Extractor Estimating: 232it [02:55,  1.58it/s]Extractor Estimating: 233it [02:55,  1.61it/s]Extractor Estimating: 234it [02:56,  1.56it/s]Extractor Estimating: 235it [02:57,  1.53it/s]Extractor Estimating: 236it [02:57,  1.56it/s]Extractor Estimating: 237it [02:58,  1.54it/s]Extractor Estimating: 238it [02:58,  1.57it/s]Extractor Estimating: 239it [02:59,  1.49it/s]Extractor Estimating: 240it [03:00,  1.47it/s]Extractor Estimating: 241it [03:00,  1.53it/s]Extractor Estimating: 242it [03:01,  1.53it/s]Extractor Estimating: 243it [03:02,  1.56it/s]Extractor Estimating: 244it [03:02,  1.59it/s]Extractor Estimating: 245it [03:03,  1.56it/s]Extractor Estimating: 246it [03:04,  1.61it/s]Extractor Estimating: 247it [03:04,  1.56it/s]Extractor Estimating: 248it [03:05,  1.53it/s]Extractor Estimating: 249it [03:06,  1.53it/s]Extractor Estimating: 250it [03:06,  1.51it/s]Extractor Estimating: 251it [03:07,  1.51it/s]Extractor Estimating: 252it [03:08,  1.50it/s]Extractor Estimating: 253it [03:08,  1.48it/s]Extractor Estimating: 254it [03:09,  1.39it/s]Extractor Estimating: 255it [03:10,  1.41it/s]Extractor Estimating: 256it [03:11,  1.43it/s]Extractor Estimating: 257it [03:11,  1.43it/s]Extractor Estimating: 258it [03:12,  1.40it/s]Extractor Estimating: 259it [03:13,  1.40it/s]Extractor Estimating: 260it [03:13,  1.42it/s]Extractor Estimating: 261it [03:14,  1.45it/s]Extractor Estimating: 262it [03:15,  1.53it/s]Extractor Estimating: 263it [03:15,  1.51it/s]Extractor Estimating: 264it [03:16,  1.50it/s]Extractor Estimating: 265it [03:17,  1.50it/s]Extractor Estimating: 266it [03:17,  1.52it/s]Extractor Estimating: 267it [03:18,  1.45it/s]Extractor Estimating: 268it [03:19,  1.31it/s]Extractor Estimating: 269it [03:20,  1.40it/s]Extractor Estimating: 270it [03:20,  1.45it/s]Extractor Estimating: 271it [03:21,  1.49it/s]Extractor Estimating: 272it [03:21,  1.49it/s]Extractor Estimating: 273it [03:22,  1.51it/s]Extractor Estimating: 274it [03:23,  1.51it/s]Extractor Estimating: 275it [03:23,  1.54it/s]Extractor Estimating: 276it [03:24,  1.61it/s]Extractor Estimating: 277it [03:25,  1.60it/s]Extractor Estimating: 278it [03:25,  1.61it/s]Extractor Estimating: 279it [03:26,  1.61it/s]Extractor Estimating: 280it [03:26,  1.64it/s]Extractor Estimating: 281it [03:27,  1.61it/s]Extractor Estimating: 282it [03:28,  1.65it/s]Extractor Estimating: 283it [03:28,  1.65it/s]Extractor Estimating: 284it [03:29,  1.63it/s]Extractor Estimating: 285it [03:29,  1.64it/s]Extractor Estimating: 286it [03:30,  1.60it/s]Extractor Estimating: 287it [03:31,  1.61it/s]Extractor Estimating: 288it [03:31,  1.62it/s]Extractor Estimating: 289it [03:32,  1.61it/s]Extractor Estimating: 290it [03:33,  1.61it/s]Extractor Estimating: 291it [03:33,  1.63it/s]Extractor Estimating: 292it [03:34,  1.59it/s]Extractor Estimating: 293it [03:34,  1.58it/s]Extractor Estimating: 294it [03:35,  1.58it/s]Extractor Estimating: 295it [03:36,  1.59it/s]Extractor Estimating: 296it [03:36,  1.54it/s]Extractor Estimating: 297it [03:37,  1.55it/s]Extractor Estimating: 298it [03:38,  1.57it/s]Extractor Estimating: 299it [03:38,  1.61it/s]Extractor Estimating: 300it [03:39,  1.61it/s]Extractor Estimating: 301it [03:39,  1.66it/s]Extractor Estimating: 302it [03:40,  1.70it/s]Extractor Estimating: 303it [03:41,  1.74it/s]Extractor Estimating: 304it [03:41,  1.77it/s]Extractor Estimating: 305it [03:42,  1.78it/s]Extractor Estimating: 306it [03:42,  1.78it/s]Extractor Estimating: 307it [03:43,  1.76it/s]Extractor Estimating: 308it [03:43,  1.75it/s]Extractor Estimating: 309it [03:44,  1.69it/s]Extractor Estimating: 310it [03:45,  1.71it/s]Extractor Estimating: 311it [03:45,  1.75it/s]Extractor Estimating: 312it [03:46,  1.78it/s]Extractor Estimating: 313it [03:46,  1.79it/s]Extractor Estimating: 314it [03:47,  1.74it/s]Extractor Estimating: 315it [03:47,  1.79it/s]Extractor Estimating: 316it [03:48,  1.80it/s]Extractor Estimating: 317it [03:48,  1.78it/s]Extractor Estimating: 318it [03:49,  1.75it/s]Extractor Estimating: 319it [03:50,  1.77it/s]Extractor Estimating: 320it [03:50,  1.80it/s]Extractor Estimating: 321it [03:51,  1.73it/s]Extractor Estimating: 322it [03:51,  1.78it/s]Extractor Estimating: 323it [03:52,  1.73it/s]Extractor Estimating: 324it [03:53,  1.70it/s]Extractor Estimating: 325it [03:53,  1.69it/s]Extractor Estimating: 326it [03:54,  1.59it/s]Extractor Estimating: 327it [03:54,  1.62it/s]Extractor Estimating: 328it [03:55,  1.60it/s]Extractor Estimating: 329it [03:56,  1.59it/s]Extractor Estimating: 330it [03:56,  1.61it/s]Extractor Estimating: 331it [03:57,  1.63it/s]Extractor Estimating: 332it [03:58,  1.55it/s]Extractor Estimating: 333it [03:58,  1.54it/s]Extractor Estimating: 334it [03:59,  1.53it/s]Extractor Estimating: 335it [04:00,  1.45it/s]Extractor Estimating: 336it [04:00,  1.47it/s]Extractor Estimating: 337it [04:01,  1.53it/s]Extractor Estimating: 338it [04:02,  1.50it/s]Extractor Estimating: 339it [04:02,  1.52it/s]Extractor Estimating: 340it [04:03,  1.50it/s]Extractor Estimating: 341it [04:04,  1.51it/s]Extractor Estimating: 342it [04:04,  1.51it/s]Extractor Estimating: 343it [04:05,  1.54it/s]Extractor Estimating: 344it [04:06,  1.57it/s]Extractor Estimating: 345it [04:06,  1.57it/s]Extractor Estimating: 346it [04:07,  1.58it/s]Extractor Estimating: 347it [04:08,  1.53it/s]Extractor Estimating: 348it [04:08,  1.51it/s]Extractor Estimating: 349it [04:09,  1.55it/s]Extractor Estimating: 350it [04:09,  1.56it/s]Extractor Estimating: 351it [04:10,  1.54it/s]Extractor Estimating: 352it [04:11,  1.59it/s]Extractor Estimating: 353it [04:11,  1.63it/s]Extractor Estimating: 354it [04:12,  1.70it/s]Extractor Estimating: 355it [04:12,  1.69it/s]Extractor Estimating: 356it [04:13,  1.62it/s]Extractor Estimating: 357it [04:14,  1.70it/s]Extractor Estimating: 358it [04:14,  1.68it/s]Extractor Estimating: 359it [04:15,  1.52it/s]Extractor Estimating: 360it [04:16,  1.53it/s]Extractor Estimating: 361it [04:16,  1.51it/s]Extractor Estimating: 362it [04:17,  1.55it/s]Extractor Estimating: 363it [04:18,  1.56it/s]Extractor Estimating: 364it [04:18,  1.61it/s]Extractor Estimating: 365it [04:19,  1.65it/s]Extractor Estimating: 366it [04:19,  1.67it/s]Extractor Estimating: 367it [04:20,  1.59it/s]Extractor Estimating: 368it [04:21,  1.60it/s]Extractor Estimating: 369it [04:21,  1.58it/s]Extractor Estimating: 370it [04:22,  1.61it/s]Extractor Estimating: 371it [04:23,  1.56it/s]Extractor Estimating: 372it [04:23,  1.53it/s]Extractor Estimating: 373it [04:24,  1.53it/s]Extractor Estimating: 374it [04:25,  1.55it/s]Extractor Estimating: 375it [04:25,  1.52it/s]Extractor Estimating: 376it [04:26,  1.49it/s]Extractor Estimating: 377it [04:27,  1.45it/s]Extractor Estimating: 378it [04:27,  1.48it/s]Extractor Estimating: 379it [04:28,  1.50it/s]Extractor Estimating: 380it [04:29,  1.47it/s]Extractor Estimating: 381it [04:29,  1.46it/s]Extractor Estimating: 382it [04:30,  1.46it/s]Extractor Estimating: 383it [04:31,  1.48it/s]Extractor Estimating: 384it [04:31,  1.48it/s]Extractor Estimating: 385it [04:32,  1.49it/s]Extractor Estimating: 386it [04:33,  1.47it/s]Extractor Estimating: 387it [04:33,  1.43it/s]Extractor Estimating: 388it [04:34,  1.44it/s]Extractor Estimating: 389it [04:35,  1.36it/s]Extractor Estimating: 390it [04:36,  1.37it/s]Extractor Estimating: 391it [04:36,  1.45it/s]Extractor Estimating: 392it [04:37,  1.43it/s]Extractor Estimating: 393it [04:38,  1.39it/s]Extractor Estimating: 394it [04:39,  1.36it/s]Extractor Estimating: 395it [04:39,  1.30it/s]Extractor Estimating: 396it [04:40,  1.41it/s]Extractor Estimating: 397it [04:41,  1.44it/s]Extractor Estimating: 398it [04:41,  1.41it/s]Extractor Estimating: 399it [04:42,  1.39it/s]Extractor Estimating: 400it [04:43,  1.44it/s]Extractor Estimating: 401it [04:43,  1.49it/s]Extractor Estimating: 402it [04:44,  1.48it/s]Extractor Estimating: 403it [04:45,  1.44it/s]Extractor Estimating: 404it [04:45,  1.46it/s]Extractor Estimating: 405it [04:46,  1.43it/s]Extractor Estimating: 406it [04:47,  1.49it/s]Extractor Estimating: 407it [04:47,  1.52it/s]Extractor Estimating: 408it [04:48,  1.54it/s]Extractor Estimating: 409it [04:49,  1.54it/s]Extractor Estimating: 410it [04:49,  1.57it/s]Extractor Estimating: 411it [04:50,  1.55it/s]Extractor Estimating: 412it [04:51,  1.56it/s]Extractor Estimating: 413it [04:51,  1.55it/s]Extractor Estimating: 414it [04:52,  1.53it/s]Extractor Estimating: 415it [04:53,  1.51it/s]Extractor Estimating: 416it [04:53,  1.51it/s]Extractor Estimating: 417it [04:54,  1.56it/s]Extractor Estimating: 418it [04:55,  1.51it/s]Extractor Estimating: 419it [04:55,  1.56it/s]Extractor Estimating: 420it [04:56,  1.60it/s]Extractor Estimating: 421it [04:56,  1.62it/s]Extractor Estimating: 422it [04:57,  1.62it/s]Extractor Estimating: 423it [04:58,  1.55it/s]Extractor Estimating: 424it [04:58,  1.55it/s]Extractor Estimating: 425it [04:59,  1.55it/s]Extractor Estimating: 426it [05:00,  1.45it/s]Extractor Estimating: 427it [05:00,  1.51it/s]Extractor Estimating: 428it [05:01,  1.54it/s]Extractor Estimating: 429it [05:02,  1.52it/s]Extractor Estimating: 430it [05:02,  1.53it/s]Extractor Estimating: 431it [05:03,  1.55it/s]Extractor Estimating: 432it [05:04,  1.55it/s]Extractor Estimating: 433it [05:04,  1.57it/s]Extractor Estimating: 434it [05:05,  1.56it/s]Extractor Estimating: 435it [05:06,  1.42it/s]Extractor Estimating: 436it [05:06,  1.46it/s]Extractor Estimating: 437it [05:07,  1.50it/s]Extractor Estimating: 438it [05:08,  1.49it/s]Extractor Estimating: 439it [05:08,  1.51it/s]Extractor Estimating: 440it [05:09,  1.55it/s]Extractor Estimating: 441it [05:09,  1.58it/s]Extractor Estimating: 442it [05:10,  1.53it/s]Extractor Estimating: 443it [05:11,  1.50it/s]Extractor Estimating: 444it [05:12,  1.46it/s]Extractor Estimating: 445it [05:12,  1.51it/s]Extractor Estimating: 446it [05:13,  1.51it/s]Extractor Estimating: 447it [05:13,  1.57it/s]Extractor Estimating: 448it [05:14,  1.55it/s]Extractor Estimating: 449it [05:15,  1.51it/s]Extractor Estimating: 450it [05:15,  1.53it/s]Extractor Estimating: 451it [05:16,  1.55it/s]Extractor Estimating: 452it [05:17,  1.53it/s]Extractor Estimating: 453it [05:17,  1.58it/s]Extractor Estimating: 454it [05:18,  1.61it/s]Extractor Estimating: 455it [05:19,  1.64it/s]Extractor Estimating: 456it [05:19,  1.64it/s]Extractor Estimating: 457it [05:20,  1.66it/s]Extractor Estimating: 458it [05:20,  1.60it/s]Extractor Estimating: 459it [05:21,  1.58it/s]Extractor Estimating: 460it [05:22,  1.58it/s]Extractor Estimating: 461it [05:22,  1.56it/s]Extractor Estimating: 462it [05:23,  1.58it/s]Extractor Estimating: 463it [05:24,  1.52it/s]Extractor Estimating: 464it [05:24,  1.54it/s]Extractor Estimating: 465it [05:25,  1.54it/s]Extractor Estimating: 466it [05:26,  1.55it/s]Extractor Estimating: 467it [05:26,  1.55it/s]Extractor Estimating: 468it [05:27,  1.60it/s]Extractor Estimating: 469it [05:27,  1.60it/s]Extractor Estimating: 470it [05:28,  1.65it/s]Extractor Estimating: 471it [05:29,  1.64it/s]Extractor Estimating: 472it [05:29,  1.69it/s]Extractor Estimating: 473it [05:30,  1.69it/s]Extractor Estimating: 474it [05:30,  1.68it/s]Extractor Estimating: 475it [05:31,  1.63it/s]Extractor Estimating: 476it [05:32,  1.66it/s]Extractor Estimating: 477it [05:32,  1.63it/s]Extractor Estimating: 478it [05:33,  1.61it/s]Extractor Estimating: 479it [05:33,  1.60it/s]Extractor Estimating: 480it [05:34,  1.59it/s]Extractor Estimating: 481it [05:35,  1.56it/s]Extractor Estimating: 482it [05:35,  1.53it/s]Extractor Estimating: 483it [05:36,  1.54it/s]Extractor Estimating: 484it [05:37,  1.59it/s]Extractor Estimating: 485it [05:37,  1.57it/s]Extractor Estimating: 486it [05:38,  1.56it/s]Extractor Estimating: 487it [05:39,  1.54it/s]Extractor Estimating: 488it [05:39,  1.57it/s]Extractor Estimating: 489it [05:40,  1.60it/s]Extractor Estimating: 490it [05:41,  1.58it/s]Extractor Estimating: 491it [05:41,  1.53it/s]Extractor Estimating: 492it [05:42,  1.53it/s]Extractor Estimating: 493it [05:43,  1.53it/s]Extractor Estimating: 494it [05:43,  1.55it/s]Extractor Estimating: 495it [05:44,  1.47it/s]Extractor Estimating: 496it [05:45,  1.51it/s]Extractor Estimating: 497it [05:45,  1.53it/s]Extractor Estimating: 498it [05:46,  1.60it/s]Extractor Estimating: 499it [05:46,  1.61it/s]Extractor Estimating: 500it [05:47,  1.64it/s]Extractor Estimating: 500it [05:47,  1.44it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 8000}
num of filtered data: 2090 mean pseudo reward: 0.9670724210574336
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 13798
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13898, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=13898, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 12, avg_time 1.352, loss:263.7383
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 24, avg_time 1.033, loss:196.0977
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 36, avg_time 1.032, loss:173.0608
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 48, avg_time 1.042, loss:148.5125
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 60, avg_time 1.042, loss:142.4356
>> valid entity prec:0.5565, rec:0.6135, f1:0.5836
>> valid relation prec:0.1888, rec:0.1306, f1:0.1544
>> valid relation with NER prec:0.1888, rec:0.1306, f1:0.1544
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 72, avg_time 2.269, loss:130.1466
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 84, avg_time 1.032, loss:132.6625
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 8, avg_time 1.033, loss:131.4252
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 20, avg_time 1.035, loss:124.0830
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 32, avg_time 1.036, loss:137.6455
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5420, rec:0.5739, f1:0.5575
>> valid relation prec:0.1842, rec:0.1263, f1:0.1498
>> valid relation with NER prec:0.1842, rec:0.1263, f1:0.1498
g_step 1100, step 44, avg_time 2.257, loss:144.4127
g_step 1200, step 56, avg_time 1.042, loss:132.0851
g_step 1300, step 68, avg_time 1.032, loss:115.1637
g_step 1400, step 80, avg_time 1.032, loss:125.8881
g_step 1500, step 4, avg_time 1.029, loss:120.7454
>> valid entity prec:0.5860, rec:0.5591, f1:0.5722
>> valid relation prec:0.2109, rec:0.1388, f1:0.1674
>> valid relation with NER prec:0.2109, rec:0.1388, f1:0.1674
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 16, avg_time 2.255, loss:97.3445
g_step 1700, step 28, avg_time 1.050, loss:107.9073
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 00:09:17 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 00:09:17 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_00-09-17_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 00:09:18 - WARNING - datasets.builder -   Using custom data configuration default-e578e9413fa7b290
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-e578e9413fa7b290/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 00:09:18,708 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:09:18,710 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:09:18,710 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:09:18,711 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:09:18,718 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:09:18,723 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:09:18,723 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:09:18,723 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:09:18,723 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:09:18,724 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:09:18,724 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 00:09:18,838 >> loading weights file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:09:22,045 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 00:09:22,047 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_15_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-e578e9413fa7b290/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/29/2023 00:09:22 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14641216b050> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  2.97ba/s] 67%|██████▋   | 2/3 [00:00<00:00,  3.84ba/s]100%|██████████| 3/3 [00:00<00:00,  5.28ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.44ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.55ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.59ba/s]100%|██████████| 4/4 [00:00<00:00,  5.82ba/s]100%|██████████| 4/4 [00:00<00:00,  5.28ba/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  9.00ba/s]100%|██████████| 3/3 [00:00<00:00, 14.49ba/s]100%|██████████| 3/3 [00:00<00:00, 13.63ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  8.90ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.02ba/s]100%|██████████| 4/4 [00:00<00:00, 11.47ba/s]
[INFO|trainer.py:414] 2023-08-29 00:09:24,279 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 00:09:24,293 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 00:09:24,293 >>   Num examples = 2090
[INFO|trainer.py:1149] 2023-08-29 00:09:24,293 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 00:09:24,293 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 00:09:24,293 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 00:09:24,293 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 00:09:24,293 >>   Total optimization steps = 165
  0%|          | 0/165 [00:00<?, ?it/s]  1%|          | 1/165 [00:00<00:48,  3.35it/s]  1%|          | 2/165 [00:00<00:47,  3.42it/s]  2%|▏         | 3/165 [00:00<00:46,  3.45it/s]  2%|▏         | 4/165 [00:01<00:46,  3.44it/s]  3%|▎         | 5/165 [00:01<00:46,  3.46it/s]  4%|▎         | 6/165 [00:01<00:45,  3.46it/s]  4%|▍         | 7/165 [00:02<00:45,  3.47it/s]  5%|▍         | 8/165 [00:02<00:45,  3.47it/s]  5%|▌         | 9/165 [00:02<00:44,  3.47it/s]  6%|▌         | 10/165 [00:02<00:44,  3.47it/s]  7%|▋         | 11/165 [00:03<00:44,  3.47it/s]  7%|▋         | 12/165 [00:03<00:44,  3.47it/s]  8%|▊         | 13/165 [00:03<00:43,  3.47it/s]  8%|▊         | 14/165 [00:04<00:43,  3.47it/s]  9%|▉         | 15/165 [00:04<00:43,  3.46it/s] 10%|▉         | 16/165 [00:04<00:42,  3.47it/s] 10%|█         | 17/165 [00:04<00:42,  3.47it/s] 11%|█         | 18/165 [00:05<00:42,  3.47it/s] 12%|█▏        | 19/165 [00:05<00:42,  3.47it/s] 12%|█▏        | 20/165 [00:05<00:41,  3.47it/s] 13%|█▎        | 21/165 [00:06<00:41,  3.47it/s] 13%|█▎        | 22/165 [00:06<00:41,  3.47it/s] 14%|█▍        | 23/165 [00:06<00:40,  3.47it/s] 15%|█▍        | 24/165 [00:06<00:40,  3.47it/s] 15%|█▌        | 25/165 [00:07<00:40,  3.47it/s] 16%|█▌        | 26/165 [00:07<00:40,  3.47it/s] 16%|█▋        | 27/165 [00:07<00:39,  3.47it/s] 17%|█▋        | 28/165 [00:08<00:39,  3.47it/s] 18%|█▊        | 29/165 [00:08<00:39,  3.47it/s] 18%|█▊        | 30/165 [00:08<00:38,  3.48it/s] 19%|█▉        | 31/165 [00:08<00:38,  3.47it/s] 19%|█▉        | 32/165 [00:09<00:38,  3.47it/s] 20%|██        | 33/165 [00:09<00:34,  3.80it/s][INFO|trainer.py:2140] 2023-08-29 00:09:33,729 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:09:33,729 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 00:09:33,729 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.47it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.06it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.11it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.19it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.84it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.57it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.38it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.13it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.16it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.27it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.24it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.09it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.14it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.13it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.14it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 45.07it/s][A
 20%|██        | 87/431 [00:01<00:07, 45.06it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 45.04it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.04it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.12it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 44.95it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.10it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 44.98it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.06it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.85it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.86it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 45.08it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.20it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.05it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.10it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.08it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 44.99it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.02it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 45.03it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.90it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 45.04it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.12it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.14it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.08it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.09it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.02it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.07it/s][A
 50%|█████     | 217/431 [00:04<00:04, 45.00it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.89it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 45.01it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 45.12it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.19it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.12it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.14it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 44.93it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.12it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.98it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 45.05it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 45.12it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.13it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.07it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.10it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.03it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 44.90it/s][A
 70%|███████   | 302/431 [00:06<00:02, 44.91it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.90it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 45.01it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 45.02it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.06it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.18it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.10it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.07it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 44.91it/s][A
 81%|████████  | 347/431 [00:07<00:01, 44.93it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 44.96it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.96it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 45.04it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 45.05it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.11it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.08it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.11it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 44.85it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 44.81it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 44.90it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 45.01it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 45.02it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.03it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.04it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.05it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.06it/s][A                                                
                                                 [A 20%|██        | 33/165 [00:19<00:34,  3.80it/s]
100%|██████████| 431/431 [00:09<00:00, 45.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:09:43,309 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-33
[INFO|configuration_utils.py:351] 2023-08-29 00:09:43,328 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-33/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:09:45,353 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-33/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:09:45,377 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-33/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:09:45,385 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-33/special_tokens_map.json
 21%|██        | 34/165 [00:25<11:00,  5.04s/it] 21%|██        | 35/165 [00:25<07:50,  3.62s/it] 22%|██▏       | 36/165 [00:26<05:37,  2.62s/it] 22%|██▏       | 37/165 [00:26<04:05,  1.92s/it] 23%|██▎       | 38/165 [00:26<03:01,  1.43s/it] 24%|██▎       | 39/165 [00:27<02:17,  1.09s/it] 24%|██▍       | 40/165 [00:27<01:46,  1.17it/s] 25%|██▍       | 41/165 [00:27<01:24,  1.46it/s] 25%|██▌       | 42/165 [00:27<01:09,  1.77it/s] 26%|██▌       | 43/165 [00:28<00:59,  2.07it/s] 27%|██▋       | 44/165 [00:28<00:51,  2.35it/s] 27%|██▋       | 45/165 [00:28<00:46,  2.59it/s] 28%|██▊       | 46/165 [00:29<00:42,  2.80it/s] 28%|██▊       | 47/165 [00:29<00:39,  2.96it/s] 29%|██▉       | 48/165 [00:29<00:37,  3.08it/s] 30%|██▉       | 49/165 [00:30<00:36,  3.18it/s] 30%|███       | 50/165 [00:30<00:35,  3.23it/s] 31%|███       | 51/165 [00:30<00:34,  3.29it/s] 32%|███▏      | 52/165 [00:30<00:33,  3.33it/s] 32%|███▏      | 53/165 [00:31<00:33,  3.35it/s] 33%|███▎      | 54/165 [00:31<00:32,  3.37it/s] 33%|███▎      | 55/165 [00:31<00:32,  3.39it/s] 34%|███▍      | 56/165 [00:32<00:32,  3.40it/s] 35%|███▍      | 57/165 [00:32<00:31,  3.41it/s] 35%|███▌      | 58/165 [00:32<00:31,  3.41it/s] 36%|███▌      | 59/165 [00:32<00:31,  3.41it/s] 36%|███▋      | 60/165 [00:33<00:30,  3.42it/s] 37%|███▋      | 61/165 [00:33<00:30,  3.41it/s] 38%|███▊      | 62/165 [00:33<00:30,  3.41it/s] 38%|███▊      | 63/165 [00:34<00:29,  3.42it/s] 39%|███▉      | 64/165 [00:34<00:29,  3.42it/s] 39%|███▉      | 65/165 [00:34<00:29,  3.42it/s] 40%|████      | 66/165 [00:34<00:26,  3.75it/s][INFO|trainer.py:2140] 2023-08-29 00:09:59,191 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:09:59,191 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 00:09:59,191 >>   Batch size = 8
{'eval_loss': 0.9834004044532776, 'eval_runtime': 9.5669, 'eval_samples_per_second': 360.2, 'eval_steps_per_second': 45.051, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.54it/s][A
  3%|▎         | 12/431 [00:00<00:08, 48.91it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.10it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.19it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.74it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.49it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.22it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.01it/s][A
 11%|█         | 47/431 [00:01<00:08, 44.95it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.05it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.30it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.18it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.08it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.01it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.03it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.80it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.91it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.99it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.08it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.10it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.03it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 44.86it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 44.91it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 44.97it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.73it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.80it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.94it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.16it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.19it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.11it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 44.96it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 44.95it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 44.96it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.77it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.75it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.98it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.16it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.16it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 44.97it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 44.96it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.00it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 44.94it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.90it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.77it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.85it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 44.96it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.10it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.07it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.07it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.05it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 44.89it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.84it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.61it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.76it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 44.96it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.10it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.05it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.13it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 44.89it/s][A
 70%|███████   | 302/431 [00:06<00:02, 44.96it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.87it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.70it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.78it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 44.86it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.03it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 44.94it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 43.41it/s][A
 79%|███████▉  | 342/431 [00:07<00:02, 44.08it/s][A
 81%|████████  | 347/431 [00:07<00:01, 44.23it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 44.34it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.53it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.48it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.61it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 44.93it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 44.84it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.00it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.02it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.03it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 44.92it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 44.87it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.86it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 44.79it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 44.98it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 44.90it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.00it/s][A                                                
                                                 [A 40%|████      | 66/165 [00:44<00:26,  3.75it/s]
100%|██████████| 431/431 [00:09<00:00, 45.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:10:08,798 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-66
[INFO|configuration_utils.py:351] 2023-08-29 00:10:08,837 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-66/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:10:11,107 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-66/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:10:11,123 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-66/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:10:11,133 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-66/special_tokens_map.json
 41%|████      | 67/165 [00:51<08:29,  5.20s/it] 41%|████      | 68/165 [00:51<06:01,  3.73s/it] 42%|████▏     | 69/165 [00:52<04:18,  2.70s/it] 42%|████▏     | 70/165 [00:52<03:07,  1.98s/it] 43%|████▎     | 71/165 [00:52<02:18,  1.47s/it] 44%|████▎     | 72/165 [00:53<01:44,  1.12s/it] 44%|████▍     | 73/165 [00:53<01:20,  1.14it/s] 45%|████▍     | 74/165 [00:53<01:03,  1.42it/s] 45%|████▌     | 75/165 [00:53<00:52,  1.72it/s] 46%|████▌     | 76/165 [00:54<00:43,  2.02it/s] 47%|████▋     | 77/165 [00:54<00:38,  2.31it/s] 47%|████▋     | 78/165 [00:54<00:34,  2.56it/s] 48%|████▊     | 79/165 [00:55<00:31,  2.77it/s] 48%|████▊     | 80/165 [00:55<00:28,  2.94it/s] 49%|████▉     | 81/165 [00:55<00:27,  3.07it/s] 50%|████▉     | 82/165 [00:56<00:26,  3.16it/s] 50%|█████     | 83/165 [00:56<00:25,  3.24it/s] 51%|█████     | 84/165 [00:56<00:24,  3.29it/s] 52%|█████▏    | 85/165 [00:56<00:24,  3.32it/s] 52%|█████▏    | 86/165 [00:57<00:23,  3.35it/s] 53%|█████▎    | 87/165 [00:57<00:23,  3.37it/s] 53%|█████▎    | 88/165 [00:57<00:22,  3.37it/s] 54%|█████▍    | 89/165 [00:58<00:22,  3.39it/s] 55%|█████▍    | 90/165 [00:58<00:22,  3.39it/s] 55%|█████▌    | 91/165 [00:58<00:21,  3.40it/s] 56%|█████▌    | 92/165 [00:58<00:21,  3.41it/s] 56%|█████▋    | 93/165 [00:59<00:21,  3.43it/s] 57%|█████▋    | 94/165 [00:59<00:20,  3.44it/s] 58%|█████▊    | 95/165 [00:59<00:20,  3.45it/s] 58%|█████▊    | 96/165 [01:00<00:19,  3.45it/s] 59%|█████▉    | 97/165 [01:00<00:19,  3.46it/s] 59%|█████▉    | 98/165 [01:00<00:19,  3.46it/s] 60%|██████    | 99/165 [01:00<00:17,  3.76it/s][INFO|trainer.py:2140] 2023-08-29 00:10:25,195 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:10:25,195 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 00:10:25,195 >>   Batch size = 8
{'eval_loss': 0.9863485097885132, 'eval_runtime': 9.5983, 'eval_samples_per_second': 359.024, 'eval_steps_per_second': 44.904, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.77it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.45it/s][A
  4%|▍         | 18/431 [00:00<00:08, 47.51it/s][A
  5%|▌         | 23/431 [00:00<00:08, 46.53it/s][A
  6%|▋         | 28/431 [00:00<00:08, 45.83it/s][A
  8%|▊         | 33/431 [00:00<00:08, 45.52it/s][A
  9%|▉         | 38/431 [00:00<00:08, 45.36it/s][A
 10%|▉         | 43/431 [00:00<00:08, 45.13it/s][A
 11%|█         | 48/431 [00:01<00:08, 45.12it/s][A
 12%|█▏        | 53/431 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 58/431 [00:01<00:08, 45.43it/s][A
 15%|█▍        | 63/431 [00:01<00:08, 45.46it/s][A
 16%|█▌        | 68/431 [00:01<00:07, 45.42it/s][A
 17%|█▋        | 73/431 [00:01<00:07, 45.22it/s][A
 18%|█▊        | 78/431 [00:01<00:07, 45.15it/s][A
 19%|█▉        | 83/431 [00:01<00:07, 45.08it/s][A
 20%|██        | 88/431 [00:01<00:07, 44.87it/s][A
 22%|██▏       | 93/431 [00:02<00:07, 45.05it/s][A
 23%|██▎       | 98/431 [00:02<00:07, 45.06it/s][A
 24%|██▍       | 103/431 [00:02<00:07, 45.25it/s][A
 25%|██▌       | 108/431 [00:02<00:07, 45.28it/s][A
 26%|██▌       | 113/431 [00:02<00:07, 45.30it/s][A
 27%|██▋       | 118/431 [00:02<00:06, 45.21it/s][A
 29%|██▊       | 123/431 [00:02<00:06, 45.14it/s][A
 30%|██▉       | 128/431 [00:02<00:06, 45.14it/s][A
 31%|███       | 133/431 [00:02<00:06, 45.02it/s][A
 32%|███▏      | 138/431 [00:03<00:06, 45.06it/s][A
 33%|███▎      | 143/431 [00:03<00:06, 45.17it/s][A
 34%|███▍      | 148/431 [00:03<00:06, 45.22it/s][A
 35%|███▌      | 153/431 [00:03<00:06, 45.21it/s][A
 37%|███▋      | 158/431 [00:03<00:06, 45.35it/s][A
 38%|███▊      | 163/431 [00:03<00:05, 45.18it/s][A
 39%|███▉      | 168/431 [00:03<00:05, 45.22it/s][A
 40%|████      | 173/431 [00:03<00:05, 45.11it/s][A
 41%|████▏     | 178/431 [00:03<00:05, 45.06it/s][A
 42%|████▏     | 183/431 [00:04<00:05, 45.04it/s][A
 44%|████▎     | 188/431 [00:04<00:05, 45.13it/s][A
 45%|████▍     | 193/431 [00:04<00:05, 45.20it/s][A
 46%|████▌     | 198/431 [00:04<00:05, 45.22it/s][A
 47%|████▋     | 203/431 [00:04<00:05, 45.30it/s][A
 48%|████▊     | 208/431 [00:04<00:04, 45.18it/s][A
 49%|████▉     | 213/431 [00:04<00:04, 45.17it/s][A
 51%|█████     | 218/431 [00:04<00:04, 45.12it/s][A
 52%|█████▏    | 223/431 [00:04<00:04, 45.05it/s][A
 53%|█████▎    | 228/431 [00:05<00:04, 44.99it/s][A
 54%|█████▍    | 233/431 [00:05<00:04, 45.20it/s][A
 55%|█████▌    | 238/431 [00:05<00:04, 45.23it/s][A
 56%|█████▋    | 243/431 [00:05<00:04, 45.26it/s][A
 58%|█████▊    | 248/431 [00:05<00:04, 45.28it/s][A
 59%|█████▊    | 253/431 [00:05<00:03, 45.10it/s][A
 60%|█████▉    | 258/431 [00:05<00:03, 45.11it/s][A
 61%|██████    | 263/431 [00:05<00:03, 45.11it/s][A
 62%|██████▏   | 268/431 [00:05<00:03, 45.05it/s][A
 63%|██████▎   | 273/431 [00:06<00:03, 45.07it/s][A
 65%|██████▍   | 278/431 [00:06<00:03, 45.16it/s][A
 66%|██████▌   | 283/431 [00:06<00:03, 45.17it/s][A
 67%|██████▋   | 288/431 [00:06<00:03, 45.27it/s][A
 68%|██████▊   | 293/431 [00:06<00:03, 45.19it/s][A
 69%|██████▉   | 298/431 [00:06<00:02, 45.07it/s][A
 70%|███████   | 303/431 [00:06<00:02, 45.09it/s][A
 71%|███████▏  | 308/431 [00:06<00:02, 45.18it/s][A
 73%|███████▎  | 313/431 [00:06<00:02, 45.16it/s][A
 74%|███████▍  | 318/431 [00:07<00:02, 45.10it/s][A
 75%|███████▍  | 323/431 [00:07<00:02, 45.09it/s][A
 76%|███████▌  | 328/431 [00:07<00:02, 45.24it/s][A
 77%|███████▋  | 333/431 [00:07<00:02, 45.26it/s][A
 78%|███████▊  | 338/431 [00:07<00:02, 45.23it/s][A
 80%|███████▉  | 343/431 [00:07<00:01, 45.19it/s][A
 81%|████████  | 348/431 [00:07<00:01, 45.06it/s][A
 82%|████████▏ | 353/431 [00:07<00:01, 45.18it/s][A
 83%|████████▎ | 358/431 [00:07<00:01, 45.17it/s][A
 84%|████████▍ | 363/431 [00:08<00:01, 45.12it/s][A
 85%|████████▌ | 368/431 [00:08<00:01, 45.08it/s][A
 87%|████████▋ | 373/431 [00:08<00:01, 45.26it/s][A
 88%|████████▊ | 378/431 [00:08<00:01, 45.31it/s][A
 89%|████████▉ | 383/431 [00:08<00:01, 45.24it/s][A
 90%|█████████ | 388/431 [00:08<00:00, 45.12it/s][A
 91%|█████████ | 393/431 [00:08<00:00, 44.99it/s][A
 92%|█████████▏| 398/431 [00:08<00:00, 45.11it/s][A
 94%|█████████▎| 403/431 [00:08<00:00, 45.05it/s][A
 95%|█████████▍| 408/431 [00:09<00:00, 45.18it/s][A
 96%|█████████▌| 413/431 [00:09<00:00, 44.99it/s][A
 97%|█████████▋| 418/431 [00:09<00:00, 45.20it/s][A
 98%|█████████▊| 423/431 [00:09<00:00, 45.29it/s][A
 99%|█████████▉| 428/431 [00:09<00:00, 45.20it/s][A                                                
                                                 [A 60%|██████    | 99/165 [01:10<00:17,  3.76it/s]
100%|██████████| 431/431 [00:09<00:00, 45.20it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:10:34,760 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-99
[INFO|configuration_utils.py:351] 2023-08-29 00:10:34,785 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-99/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:10:36,289 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-99/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:10:36,301 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-99/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:10:36,311 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-99/special_tokens_map.json
 61%|██████    | 100/165 [01:16<05:08,  4.75s/it] 61%|██████    | 101/165 [01:16<03:38,  3.41s/it] 62%|██████▏   | 102/165 [01:16<02:36,  2.48s/it] 62%|██████▏   | 103/165 [01:16<01:52,  1.82s/it] 63%|██████▎   | 104/165 [01:17<01:23,  1.36s/it] 64%|██████▎   | 105/165 [01:17<01:02,  1.04s/it] 64%|██████▍   | 106/165 [01:17<00:48,  1.22it/s] 65%|██████▍   | 107/165 [01:18<00:38,  1.52it/s] 65%|██████▌   | 108/165 [01:18<00:31,  1.82it/s] 66%|██████▌   | 109/165 [01:18<00:26,  2.12it/s] 67%|██████▋   | 110/165 [01:19<00:23,  2.39it/s] 67%|██████▋   | 111/165 [01:19<00:20,  2.63it/s] 68%|██████▊   | 112/165 [01:19<00:18,  2.81it/s] 68%|██████▊   | 113/165 [01:19<00:17,  2.97it/s] 69%|██████▉   | 114/165 [01:20<00:16,  3.09it/s] 70%|██████▉   | 115/165 [01:20<00:15,  3.18it/s] 70%|███████   | 116/165 [01:20<00:15,  3.25it/s] 71%|███████   | 117/165 [01:21<00:14,  3.30it/s] 72%|███████▏  | 118/165 [01:21<00:14,  3.33it/s] 72%|███████▏  | 119/165 [01:21<00:13,  3.36it/s] 73%|███████▎  | 120/165 [01:21<00:13,  3.37it/s] 73%|███████▎  | 121/165 [01:22<00:12,  3.39it/s] 74%|███████▍  | 122/165 [01:22<00:12,  3.39it/s] 75%|███████▍  | 123/165 [01:22<00:12,  3.39it/s] 75%|███████▌  | 124/165 [01:23<00:12,  3.40it/s] 76%|███████▌  | 125/165 [01:23<00:11,  3.40it/s] 76%|███████▋  | 126/165 [01:23<00:11,  3.41it/s] 77%|███████▋  | 127/165 [01:24<00:11,  3.41it/s] 78%|███████▊  | 128/165 [01:24<00:10,  3.41it/s] 78%|███████▊  | 129/165 [01:24<00:10,  3.41it/s] 79%|███████▉  | 130/165 [01:24<00:10,  3.41it/s] 79%|███████▉  | 131/165 [01:25<00:09,  3.42it/s] 80%|████████  | 132/165 [01:25<00:08,  3.76it/s][INFO|trainer.py:2140] 2023-08-29 00:10:49,699 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:10:49,699 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 00:10:49,699 >>   Batch size = 8
{'eval_loss': 1.0003145933151245, 'eval_runtime': 9.5408, 'eval_samples_per_second': 361.186, 'eval_steps_per_second': 45.174, 'epoch': 3.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 57.11it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.61it/s][A
  4%|▍         | 18/431 [00:00<00:08, 47.39it/s][A
  5%|▌         | 23/431 [00:00<00:08, 46.35it/s][A
  6%|▋         | 28/431 [00:00<00:08, 45.69it/s][A
  8%|▊         | 33/431 [00:00<00:08, 45.38it/s][A
  9%|▉         | 38/431 [00:00<00:08, 45.27it/s][A
 10%|▉         | 43/431 [00:00<00:08, 45.21it/s][A
 11%|█         | 48/431 [00:01<00:08, 45.18it/s][A
 12%|█▏        | 53/431 [00:01<00:08, 45.27it/s][A
 13%|█▎        | 58/431 [00:01<00:08, 45.33it/s][A
 15%|█▍        | 63/431 [00:01<00:08, 45.36it/s][A
 16%|█▌        | 68/431 [00:01<00:08, 45.36it/s][A
 17%|█▋        | 73/431 [00:01<00:07, 45.18it/s][A
 18%|█▊        | 78/431 [00:01<00:07, 44.98it/s][A
 19%|█▉        | 83/431 [00:01<00:07, 44.89it/s][A
 20%|██        | 88/431 [00:01<00:07, 44.96it/s][A
 22%|██▏       | 93/431 [00:02<00:07, 44.99it/s][A
 23%|██▎       | 98/431 [00:02<00:07, 45.17it/s][A
 24%|██▍       | 103/431 [00:02<00:07, 45.22it/s][A
 25%|██▌       | 108/431 [00:02<00:07, 45.35it/s][A
 26%|██▌       | 113/431 [00:02<00:07, 45.31it/s][A
 27%|██▋       | 118/431 [00:02<00:06, 45.21it/s][A
 29%|██▊       | 123/431 [00:02<00:06, 45.08it/s][A
 30%|██▉       | 128/431 [00:02<00:06, 45.02it/s][A
 31%|███       | 133/431 [00:02<00:06, 45.09it/s][A
 32%|███▏      | 138/431 [00:03<00:06, 45.09it/s][A
 33%|███▎      | 143/431 [00:03<00:06, 45.23it/s][A
 34%|███▍      | 148/431 [00:03<00:06, 45.17it/s][A
 35%|███▌      | 153/431 [00:03<00:06, 45.14it/s][A
 37%|███▋      | 158/431 [00:03<00:06, 45.24it/s][A
 38%|███▊      | 163/431 [00:03<00:05, 45.16it/s][A
 39%|███▉      | 168/431 [00:03<00:05, 45.05it/s][A
 40%|████      | 173/431 [00:03<00:05, 45.09it/s][A
 41%|████▏     | 178/431 [00:03<00:05, 45.05it/s][A
 42%|████▏     | 183/431 [00:04<00:05, 45.18it/s][A
 44%|████▎     | 188/431 [00:04<00:05, 45.20it/s][A
 45%|████▍     | 193/431 [00:04<00:05, 45.19it/s][A
 46%|████▌     | 198/431 [00:04<00:05, 45.21it/s][A
 47%|████▋     | 203/431 [00:04<00:05, 45.18it/s][A
 48%|████▊     | 208/431 [00:04<00:04, 45.20it/s][A
 49%|████▉     | 213/431 [00:04<00:04, 45.06it/s][A
 51%|█████     | 218/431 [00:04<00:04, 44.99it/s][A
 52%|█████▏    | 223/431 [00:04<00:04, 45.14it/s][A
 53%|█████▎    | 228/431 [00:05<00:04, 45.20it/s][A
 54%|█████▍    | 233/431 [00:05<00:04, 45.19it/s][A
 55%|█████▌    | 238/431 [00:05<00:04, 45.25it/s][A
 56%|█████▋    | 243/431 [00:05<00:04, 45.15it/s][A
 58%|█████▊    | 248/431 [00:05<00:04, 45.28it/s][A
 59%|█████▊    | 253/431 [00:05<00:03, 45.26it/s][A
 60%|█████▉    | 258/431 [00:05<00:03, 45.19it/s][A
 61%|██████    | 263/431 [00:05<00:03, 45.05it/s][A
 62%|██████▏   | 268/431 [00:05<00:03, 45.08it/s][A
 63%|██████▎   | 273/431 [00:06<00:03, 45.13it/s][A
 65%|██████▍   | 278/431 [00:06<00:03, 45.16it/s][A
 66%|██████▌   | 283/431 [00:06<00:03, 45.12it/s][A
 67%|██████▋   | 288/431 [00:06<00:03, 45.21it/s][A
 68%|██████▊   | 293/431 [00:06<00:03, 45.23it/s][A
 69%|██████▉   | 298/431 [00:06<00:02, 45.22it/s][A
 70%|███████   | 303/431 [00:06<00:02, 45.01it/s][A
 71%|███████▏  | 308/431 [00:06<00:02, 45.08it/s][A
 73%|███████▎  | 313/431 [00:06<00:02, 45.09it/s][A
 74%|███████▍  | 318/431 [00:07<00:02, 45.17it/s][A
 75%|███████▍  | 323/431 [00:07<00:02, 45.17it/s][A
 76%|███████▌  | 328/431 [00:07<00:02, 45.12it/s][A
 77%|███████▋  | 333/431 [00:07<00:02, 45.15it/s][A
 78%|███████▊  | 338/431 [00:07<00:02, 45.17it/s][A
 80%|███████▉  | 343/431 [00:07<00:01, 45.21it/s][A
 81%|████████  | 348/431 [00:07<00:01, 45.05it/s][A
 82%|████████▏ | 353/431 [00:07<00:01, 45.13it/s][A
 83%|████████▎ | 358/431 [00:07<00:01, 45.07it/s][A
 84%|████████▍ | 363/431 [00:08<00:01, 45.13it/s][A
 85%|████████▌ | 368/431 [00:08<00:01, 45.14it/s][A
 87%|████████▋ | 373/431 [00:08<00:01, 45.17it/s][A
 88%|████████▊ | 378/431 [00:08<00:01, 45.26it/s][A
 89%|████████▉ | 383/431 [00:08<00:01, 45.00it/s][A
 90%|█████████ | 388/431 [00:08<00:00, 45.21it/s][A
 91%|█████████ | 393/431 [00:08<00:00, 45.18it/s][A
 92%|█████████▏| 398/431 [00:08<00:00, 45.01it/s][A
 94%|█████████▎| 403/431 [00:08<00:00, 45.10it/s][A
 95%|█████████▍| 408/431 [00:09<00:00, 45.10it/s][A
 96%|█████████▌| 413/431 [00:09<00:00, 45.13it/s][A
 97%|█████████▋| 418/431 [00:09<00:00, 45.23it/s][A
 98%|█████████▊| 423/431 [00:09<00:00, 45.27it/s][A
 99%|█████████▉| 428/431 [00:09<00:00, 45.05it/s][A                                                 
                                                 [A 80%|████████  | 132/165 [01:34<00:08,  3.76it/s]
100%|██████████| 431/431 [00:09<00:00, 45.05it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:10:59,247 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-132
[INFO|configuration_utils.py:351] 2023-08-29 00:10:59,272 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-132/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:11:01,167 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-132/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:11:01,185 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-132/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:11:01,196 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-132/special_tokens_map.json
 81%|████████  | 133/165 [01:41<02:38,  4.94s/it] 81%|████████  | 134/165 [01:41<01:49,  3.55s/it] 82%|████████▏ | 135/165 [01:41<01:17,  2.57s/it] 82%|████████▏ | 136/165 [01:42<00:54,  1.89s/it] 83%|████████▎ | 137/165 [01:42<00:39,  1.41s/it] 84%|████████▎ | 138/165 [01:42<00:28,  1.07s/it] 84%|████████▍ | 139/165 [01:43<00:21,  1.19it/s] 85%|████████▍ | 140/165 [01:43<00:16,  1.48it/s] 85%|████████▌ | 141/165 [01:43<00:13,  1.78it/s] 86%|████████▌ | 142/165 [01:43<00:11,  2.08it/s] 87%|████████▋ | 143/165 [01:44<00:09,  2.36it/s] 87%|████████▋ | 144/165 [01:44<00:08,  2.60it/s] 88%|████████▊ | 145/165 [01:44<00:07,  2.79it/s] 88%|████████▊ | 146/165 [01:45<00:06,  2.96it/s] 89%|████████▉ | 147/165 [01:45<00:05,  3.10it/s] 90%|████████▉ | 148/165 [01:45<00:05,  3.20it/s] 90%|█████████ | 149/165 [01:45<00:04,  3.28it/s] 91%|█████████ | 150/165 [01:46<00:04,  3.33it/s] 92%|█████████▏| 151/165 [01:46<00:04,  3.37it/s] 92%|█████████▏| 152/165 [01:46<00:03,  3.40it/s] 93%|█████████▎| 153/165 [01:47<00:03,  3.42it/s] 93%|█████████▎| 154/165 [01:47<00:03,  3.43it/s] 94%|█████████▍| 155/165 [01:47<00:02,  3.44it/s] 95%|█████████▍| 156/165 [01:47<00:02,  3.44it/s] 95%|█████████▌| 157/165 [01:48<00:02,  3.45it/s] 96%|█████████▌| 158/165 [01:48<00:02,  3.45it/s] 96%|█████████▋| 159/165 [01:48<00:01,  3.46it/s] 97%|█████████▋| 160/165 [01:49<00:01,  3.46it/s] 98%|█████████▊| 161/165 [01:49<00:01,  3.46it/s] 98%|█████████▊| 162/165 [01:49<00:00,  3.46it/s] 99%|█████████▉| 163/165 [01:49<00:00,  3.46it/s] 99%|█████████▉| 164/165 [01:50<00:00,  3.46it/s]100%|██████████| 165/165 [01:50<00:00,  3.80it/s][INFO|trainer.py:2140] 2023-08-29 00:11:14,748 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:11:14,749 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 00:11:14,749 >>   Batch size = 8
{'eval_loss': 1.0067391395568848, 'eval_runtime': 9.5428, 'eval_samples_per_second': 361.109, 'eval_steps_per_second': 45.165, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.66it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.63it/s][A
  4%|▍         | 18/431 [00:00<00:08, 47.47it/s][A
  5%|▌         | 23/431 [00:00<00:08, 46.66it/s][A
  6%|▋         | 28/431 [00:00<00:08, 46.02it/s][A
  8%|▊         | 33/431 [00:00<00:08, 45.52it/s][A
  9%|▉         | 38/431 [00:00<00:08, 45.23it/s][A
 10%|▉         | 43/431 [00:00<00:08, 45.15it/s][A
 11%|█         | 48/431 [00:01<00:08, 45.21it/s][A
 12%|█▏        | 53/431 [00:01<00:08, 45.30it/s][A
 13%|█▎        | 58/431 [00:01<00:08, 45.22it/s][A
 15%|█▍        | 63/431 [00:01<00:08, 45.37it/s][A
 16%|█▌        | 68/431 [00:01<00:07, 45.39it/s][A
 17%|█▋        | 73/431 [00:01<00:07, 45.29it/s][A
 18%|█▊        | 78/431 [00:01<00:07, 45.07it/s][A
 19%|█▉        | 83/431 [00:01<00:07, 44.90it/s][A
 20%|██        | 88/431 [00:01<00:07, 44.97it/s][A
 22%|██▏       | 93/431 [00:02<00:07, 44.98it/s][A
 23%|██▎       | 98/431 [00:02<00:07, 45.21it/s][A
 24%|██▍       | 103/431 [00:02<00:07, 45.16it/s][A
 25%|██▌       | 108/431 [00:02<00:07, 45.34it/s][A
 26%|██▌       | 113/431 [00:02<00:07, 45.37it/s][A
 27%|██▋       | 118/431 [00:02<00:06, 45.29it/s][A
 29%|██▊       | 123/431 [00:02<00:06, 45.03it/s][A
 30%|██▉       | 128/431 [00:02<00:06, 44.90it/s][A
 31%|███       | 133/431 [00:02<00:06, 43.36it/s][A
 32%|███▏      | 138/431 [00:03<00:06, 44.29it/s][A
 33%|███▎      | 143/431 [00:03<00:06, 44.72it/s][A
 34%|███▍      | 148/431 [00:03<00:06, 44.81it/s][A
 35%|███▌      | 153/431 [00:03<00:06, 45.10it/s][A
 37%|███▋      | 158/431 [00:03<00:06, 45.13it/s][A
 38%|███▊      | 163/431 [00:03<00:05, 45.10it/s][A
 39%|███▉      | 168/431 [00:03<00:05, 44.93it/s][A
 40%|████      | 173/431 [00:03<00:05, 44.85it/s][A
 41%|████▏     | 178/431 [00:03<00:05, 44.73it/s][A
 42%|████▏     | 183/431 [00:04<00:05, 44.96it/s][A
 44%|████▎     | 188/431 [00:04<00:05, 45.14it/s][A
 45%|████▍     | 193/431 [00:04<00:05, 45.25it/s][A
 46%|████▌     | 198/431 [00:04<00:05, 45.35it/s][A
 47%|████▋     | 203/431 [00:04<00:05, 45.36it/s][A
 48%|████▊     | 208/431 [00:04<00:04, 45.26it/s][A
 49%|████▉     | 213/431 [00:04<00:04, 45.06it/s][A
 51%|█████     | 218/431 [00:04<00:04, 44.66it/s][A
 52%|█████▏    | 223/431 [00:04<00:04, 45.02it/s][A
 53%|█████▎    | 228/431 [00:05<00:04, 44.95it/s][A
 54%|█████▍    | 233/431 [00:05<00:04, 45.02it/s][A
 55%|█████▌    | 238/431 [00:05<00:04, 45.13it/s][A
 56%|█████▋    | 243/431 [00:05<00:04, 45.17it/s][A
 58%|█████▊    | 248/431 [00:05<00:04, 45.29it/s][A
 59%|█████▊    | 253/431 [00:05<00:03, 45.37it/s][A
 60%|█████▉    | 258/431 [00:05<00:03, 45.26it/s][A
 61%|██████    | 263/431 [00:05<00:03, 45.05it/s][A
 62%|██████▏   | 268/431 [00:05<00:03, 45.05it/s][A
 63%|██████▎   | 273/431 [00:06<00:03, 44.96it/s][A
 65%|██████▍   | 278/431 [00:06<00:03, 45.08it/s][A
 66%|██████▌   | 283/431 [00:06<00:03, 45.22it/s][A
 67%|██████▋   | 288/431 [00:06<00:03, 45.32it/s][A
 68%|██████▊   | 293/431 [00:06<00:03, 45.23it/s][A
 69%|██████▉   | 298/431 [00:06<00:02, 45.30it/s][A
 70%|███████   | 303/431 [00:06<00:02, 45.15it/s][A
 71%|███████▏  | 308/431 [00:06<00:02, 44.97it/s][A
 73%|███████▎  | 313/431 [00:06<00:02, 44.97it/s][A
 74%|███████▍  | 318/431 [00:07<00:02, 44.88it/s][A
 75%|███████▍  | 323/431 [00:07<00:02, 45.05it/s][A
 76%|███████▌  | 328/431 [00:07<00:02, 45.08it/s][A
 77%|███████▋  | 333/431 [00:07<00:02, 45.04it/s][A
 78%|███████▊  | 338/431 [00:07<00:02, 45.20it/s][A
 80%|███████▉  | 343/431 [00:07<00:01, 45.32it/s][A
 81%|████████  | 348/431 [00:07<00:01, 45.18it/s][A
 82%|████████▏ | 353/431 [00:07<00:01, 45.09it/s][A
 83%|████████▎ | 358/431 [00:07<00:01, 44.88it/s][A
 84%|████████▍ | 363/431 [00:08<00:01, 44.92it/s][A
 85%|████████▌ | 368/431 [00:08<00:01, 44.91it/s][A
 87%|████████▋ | 373/431 [00:08<00:01, 45.11it/s][A
 88%|████████▊ | 378/431 [00:08<00:01, 45.27it/s][A
 89%|████████▉ | 383/431 [00:08<00:01, 45.34it/s][A
 90%|█████████ | 388/431 [00:08<00:00, 45.18it/s][A
 91%|█████████ | 393/431 [00:08<00:00, 45.22it/s][A
 92%|█████████▏| 398/431 [00:08<00:00, 45.14it/s][A
 94%|█████████▎| 403/431 [00:08<00:00, 45.04it/s][A
 95%|█████████▍| 408/431 [00:09<00:00, 44.97it/s][A
 96%|█████████▌| 413/431 [00:09<00:00, 44.93it/s][A
 97%|█████████▋| 418/431 [00:09<00:00, 45.19it/s][A
 98%|█████████▊| 423/431 [00:09<00:00, 45.21it/s][A
 99%|█████████▉| 428/431 [00:09<00:00, 45.21it/s][A                                                 
                                                 [A100%|██████████| 165/165 [02:00<00:00,  3.80it/s]
100%|██████████| 431/431 [00:09<00:00, 45.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 00:11:24,315 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-165
[INFO|configuration_utils.py:351] 2023-08-29 00:11:24,343 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-165/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:11:26,403 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-165/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:11:26,427 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-165/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:11:26,437 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-165/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 00:11:30,397 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 00:11:30,401 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-33 (score: 0.9834004044532776).
                                                 100%|██████████| 165/165 [02:08<00:00,  3.80it/s]100%|██████████| 165/165 [02:08<00:00,  1.29it/s]
[INFO|trainer.py:1894] 2023-08-29 00:11:32,498 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-29 00:11:32,514 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 00:11:34,459 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 00:11:34,478 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 00:11:34,490 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:11:34,710 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:34,710 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:34,710 >>   train_loss               =     0.5791
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:34,710 >>   train_runtime            = 0:02:08.19
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:34,710 >>   train_samples            =       2090
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:34,710 >>   train_samples_per_second =     81.513
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:34,710 >>   train_steps_per_second   =      1.287
{'eval_loss': 1.0085853338241577, 'eval_runtime': 9.5563, 'eval_samples_per_second': 360.601, 'eval_steps_per_second': 45.101, 'epoch': 5.0}
{'train_runtime': 128.2, 'train_samples_per_second': 81.513, 'train_steps_per_second': 1.287, 'train_loss': 0.5790559710878315, 'epoch': 5.0}
08/29/2023 00:11:34 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 00:11:34,768 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 00:11:34,768 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 00:11:34,768 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 56.40it/s]  3%|▎         | 12/431 [00:00<00:08, 49.23it/s]  4%|▍         | 17/431 [00:00<00:08, 47.63it/s]  5%|▌         | 22/431 [00:00<00:08, 46.85it/s]  6%|▋         | 27/431 [00:00<00:08, 46.30it/s]  7%|▋         | 32/431 [00:00<00:08, 46.14it/s]  9%|▊         | 37/431 [00:00<00:08, 46.05it/s] 10%|▉         | 42/431 [00:00<00:08, 45.77it/s] 11%|█         | 47/431 [00:01<00:08, 45.01it/s] 12%|█▏        | 52/431 [00:01<00:08, 44.79it/s] 13%|█▎        | 57/431 [00:01<00:08, 44.85it/s] 14%|█▍        | 62/431 [00:01<00:08, 44.89it/s] 16%|█▌        | 67/431 [00:01<00:08, 45.11it/s] 17%|█▋        | 72/431 [00:01<00:07, 45.23it/s] 18%|█▊        | 77/431 [00:01<00:07, 45.37it/s] 19%|█▉        | 82/431 [00:01<00:07, 45.39it/s] 20%|██        | 87/431 [00:01<00:07, 45.28it/s] 21%|██▏       | 92/431 [00:02<00:07, 44.90it/s] 23%|██▎       | 97/431 [00:02<00:07, 44.73it/s] 24%|██▎       | 102/431 [00:02<00:07, 44.52it/s] 25%|██▍       | 107/431 [00:02<00:07, 44.79it/s] 26%|██▌       | 112/431 [00:02<00:07, 45.08it/s] 27%|██▋       | 117/431 [00:02<00:06, 45.16it/s] 28%|██▊       | 122/431 [00:02<00:06, 45.30it/s] 29%|██▉       | 127/431 [00:02<00:06, 45.40it/s] 31%|███       | 132/431 [00:02<00:06, 45.29it/s] 32%|███▏      | 137/431 [00:03<00:06, 44.99it/s] 33%|███▎      | 142/431 [00:03<00:06, 44.78it/s] 34%|███▍      | 147/431 [00:03<00:06, 44.78it/s] 35%|███▌      | 152/431 [00:03<00:06, 44.86it/s] 36%|███▋      | 157/431 [00:03<00:06, 44.97it/s] 38%|███▊      | 162/431 [00:03<00:05, 44.93it/s] 39%|███▊      | 167/431 [00:03<00:05, 45.22it/s] 40%|███▉      | 172/431 [00:03<00:05, 45.25it/s] 41%|████      | 177/431 [00:03<00:05, 45.38it/s] 42%|████▏     | 182/431 [00:04<00:05, 45.08it/s] 43%|████▎     | 187/431 [00:04<00:05, 44.91it/s] 45%|████▍     | 192/431 [00:04<00:05, 44.79it/s] 46%|████▌     | 197/431 [00:04<00:05, 44.82it/s] 47%|████▋     | 202/431 [00:04<00:05, 44.97it/s] 48%|████▊     | 207/431 [00:04<00:04, 45.05it/s] 49%|████▉     | 212/431 [00:04<00:04, 45.13it/s] 50%|█████     | 217/431 [00:04<00:04, 45.18it/s] 52%|█████▏    | 222/431 [00:04<00:04, 45.32it/s] 53%|█████▎    | 227/431 [00:05<00:04, 45.12it/s] 54%|█████▍    | 232/431 [00:05<00:04, 44.97it/s] 55%|█████▍    | 237/431 [00:05<00:04, 44.71it/s] 56%|█████▌    | 242/431 [00:05<00:04, 44.77it/s] 57%|█████▋    | 247/431 [00:05<00:04, 44.90it/s] 58%|█████▊    | 252/431 [00:05<00:03, 44.96it/s] 60%|█████▉    | 257/431 [00:05<00:03, 45.17it/s] 61%|██████    | 262/431 [00:05<00:03, 45.22it/s] 62%|██████▏   | 267/431 [00:05<00:03, 45.34it/s] 63%|██████▎   | 272/431 [00:06<00:03, 45.10it/s] 64%|██████▍   | 277/431 [00:06<00:03, 44.96it/s] 65%|██████▌   | 282/431 [00:06<00:03, 44.72it/s] 67%|██████▋   | 287/431 [00:06<00:03, 44.72it/s] 68%|██████▊   | 292/431 [00:06<00:03, 44.86it/s] 69%|██████▉   | 297/431 [00:06<00:02, 44.97it/s] 70%|███████   | 302/431 [00:06<00:02, 45.14it/s] 71%|███████   | 307/431 [00:06<00:02, 45.21it/s] 72%|███████▏  | 312/431 [00:06<00:02, 45.27it/s] 74%|███████▎  | 317/431 [00:07<00:02, 45.04it/s] 75%|███████▍  | 322/431 [00:07<00:02, 45.02it/s] 76%|███████▌  | 327/431 [00:07<00:02, 44.77it/s] 77%|███████▋  | 332/431 [00:07<00:02, 44.71it/s] 78%|███████▊  | 337/431 [00:07<00:02, 44.89it/s] 79%|███████▉  | 342/431 [00:07<00:01, 45.01it/s] 81%|████████  | 347/431 [00:07<00:01, 45.19it/s] 82%|████████▏ | 352/431 [00:07<00:01, 45.19it/s] 83%|████████▎ | 357/431 [00:07<00:01, 45.23it/s] 84%|████████▍ | 362/431 [00:08<00:01, 45.15it/s] 85%|████████▌ | 367/431 [00:08<00:01, 44.93it/s] 86%|████████▋ | 372/431 [00:08<00:01, 44.89it/s] 87%|████████▋ | 377/431 [00:08<00:01, 44.76it/s] 89%|████████▊ | 382/431 [00:08<00:01, 44.81it/s] 90%|████████▉ | 387/431 [00:08<00:00, 44.94it/s] 91%|█████████ | 392/431 [00:08<00:00, 45.01it/s] 92%|█████████▏| 397/431 [00:08<00:00, 45.14it/s] 93%|█████████▎| 402/431 [00:08<00:00, 45.14it/s] 94%|█████████▍| 407/431 [00:09<00:00, 45.02it/s] 96%|█████████▌| 412/431 [00:09<00:00, 44.94it/s] 97%|█████████▋| 417/431 [00:09<00:00, 44.69it/s] 98%|█████████▊| 422/431 [00:09<00:00, 44.80it/s] 99%|█████████▉| 427/431 [00:09<00:00, 44.83it/s]100%|██████████| 431/431 [00:09<00:00, 45.11it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 00:11:44,340 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:44,340 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:44,340 >>   eval_loss               =     0.9834
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:44,340 >>   eval_runtime            = 0:00:09.57
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:44,340 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:44,340 >>   eval_samples_per_second =    360.017
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:44,340 >>   eval_steps_per_second   =     45.028
[INFO|trainer_pt_utils.py:913] 2023-08-29 00:11:44,341 >>   perplexity              =     2.6735
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:51,102 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:51,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:51,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:51,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:51,105 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:11:51,707 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:11:51,707 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:11:52,281 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:11:53,350 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:11:53,351 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:56,413 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:56,418 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:56,418 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:56,418 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:11:56,418 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:11:57,072 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:11:57,073 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:11:57,632 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:11:57,794 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:11:57,794 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-99
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-165
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-33
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-66
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/checkpoint-132
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.45it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.52it/s]Extractor Predicting: 9it [00:05,  1.50it/s]Extractor Predicting: 10it [00:06,  1.50it/s]Extractor Predicting: 11it [00:07,  1.54it/s]Extractor Predicting: 12it [00:07,  1.54it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.49it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.51it/s]Extractor Predicting: 22it [00:14,  1.52it/s]Extractor Predicting: 23it [00:15,  1.54it/s]Extractor Predicting: 24it [00:15,  1.55it/s]Extractor Predicting: 25it [00:16,  1.50it/s]Extractor Predicting: 26it [00:17,  1.48it/s]Extractor Predicting: 27it [00:17,  1.45it/s]Extractor Predicting: 28it [00:18,  1.48it/s]Extractor Predicting: 29it [00:19,  1.48it/s]Extractor Predicting: 30it [00:19,  1.48it/s]Extractor Predicting: 31it [00:20,  1.47it/s]Extractor Predicting: 32it [00:21,  1.49it/s]Extractor Predicting: 33it [00:21,  1.52it/s]Extractor Predicting: 34it [00:22,  1.57it/s]Extractor Predicting: 35it [00:23,  1.58it/s]Extractor Predicting: 36it [00:23,  1.60it/s]Extractor Predicting: 37it [00:24,  1.61it/s]Extractor Predicting: 38it [00:24,  1.57it/s]Extractor Predicting: 39it [00:25,  1.60it/s]Extractor Predicting: 40it [00:26,  1.60it/s]Extractor Predicting: 41it [00:26,  1.57it/s]Extractor Predicting: 42it [00:27,  1.62it/s]Extractor Predicting: 43it [00:28,  1.59it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:29,  1.59it/s]Extractor Predicting: 46it [00:29,  1.61it/s]Extractor Predicting: 47it [00:30,  1.65it/s]Extractor Predicting: 48it [00:31,  1.66it/s]Extractor Predicting: 49it [00:31,  1.65it/s]Extractor Predicting: 50it [00:32,  1.63it/s]Extractor Predicting: 51it [00:32,  1.61it/s]Extractor Predicting: 52it [00:33,  1.59it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:34,  1.60it/s]Extractor Predicting: 55it [00:35,  1.60it/s]Extractor Predicting: 56it [00:36,  1.62it/s]Extractor Predicting: 57it [00:36,  1.65it/s]Extractor Predicting: 58it [00:37,  1.62it/s]Extractor Predicting: 59it [00:37,  1.66it/s]Extractor Predicting: 60it [00:38,  1.61it/s]Extractor Predicting: 61it [00:39,  1.62it/s]Extractor Predicting: 62it [00:39,  1.68it/s]Extractor Predicting: 63it [00:40,  1.68it/s]Extractor Predicting: 64it [00:40,  1.72it/s]Extractor Predicting: 65it [00:41,  1.73it/s]Extractor Predicting: 66it [00:41,  1.74it/s]Extractor Predicting: 67it [00:42,  1.72it/s]Extractor Predicting: 68it [00:43,  1.72it/s]Extractor Predicting: 69it [00:43,  1.60it/s]Extractor Predicting: 70it [00:44,  1.62it/s]Extractor Predicting: 71it [00:45,  1.62it/s]Extractor Predicting: 72it [00:45,  1.67it/s]Extractor Predicting: 73it [00:46,  1.63it/s]Extractor Predicting: 74it [00:46,  1.64it/s]Extractor Predicting: 75it [00:47,  1.68it/s]Extractor Predicting: 76it [00:48,  1.69it/s]Extractor Predicting: 77it [00:48,  1.71it/s]Extractor Predicting: 78it [00:49,  1.73it/s]Extractor Predicting: 79it [00:49,  1.73it/s]Extractor Predicting: 80it [00:50,  1.71it/s]Extractor Predicting: 81it [00:50,  1.67it/s]Extractor Predicting: 82it [00:51,  1.63it/s]Extractor Predicting: 83it [00:52,  1.66it/s]Extractor Predicting: 84it [00:52,  1.65it/s]Extractor Predicting: 85it [00:53,  1.60it/s]Extractor Predicting: 86it [00:54,  1.59it/s]Extractor Predicting: 87it [00:54,  1.61it/s]Extractor Predicting: 88it [00:55,  1.56it/s]Extractor Predicting: 89it [00:56,  1.56it/s]Extractor Predicting: 90it [00:56,  1.56it/s]Extractor Predicting: 91it [00:57,  1.58it/s]Extractor Predicting: 92it [00:57,  1.56it/s]Extractor Predicting: 93it [00:58,  1.55it/s]Extractor Predicting: 94it [00:59,  1.56it/s]Extractor Predicting: 95it [00:59,  1.55it/s]Extractor Predicting: 96it [01:00,  1.56it/s]Extractor Predicting: 97it [01:01,  1.55it/s]Extractor Predicting: 98it [01:01,  1.55it/s]Extractor Predicting: 99it [01:02,  1.57it/s]Extractor Predicting: 100it [01:03,  1.58it/s]Extractor Predicting: 101it [01:03,  1.57it/s]Extractor Predicting: 102it [01:04,  1.58it/s]Extractor Predicting: 103it [01:04,  1.58it/s]Extractor Predicting: 104it [01:05,  1.61it/s]Extractor Predicting: 105it [01:06,  1.63it/s]Extractor Predicting: 106it [01:06,  1.60it/s]Extractor Predicting: 107it [01:07,  1.59it/s]Extractor Predicting: 108it [01:08,  1.58it/s]Extractor Predicting: 109it [01:08,  1.57it/s]Extractor Predicting: 110it [01:09,  1.56it/s]Extractor Predicting: 111it [01:10,  1.54it/s]Extractor Predicting: 112it [01:10,  1.57it/s]Extractor Predicting: 113it [01:11,  1.58it/s]Extractor Predicting: 114it [01:11,  1.58it/s]Extractor Predicting: 115it [01:12,  1.58it/s]Extractor Predicting: 116it [01:13,  1.60it/s]Extractor Predicting: 117it [01:13,  1.60it/s]Extractor Predicting: 118it [01:14,  1.58it/s]Extractor Predicting: 119it [01:15,  1.59it/s]Extractor Predicting: 120it [01:15,  1.58it/s]Extractor Predicting: 121it [01:16,  1.57it/s]Extractor Predicting: 122it [01:17,  1.56it/s]Extractor Predicting: 123it [01:17,  1.56it/s]Extractor Predicting: 124it [01:18,  1.53it/s]Extractor Predicting: 125it [01:18,  1.56it/s]Extractor Predicting: 126it [01:19,  1.55it/s]Extractor Predicting: 127it [01:20,  1.54it/s]Extractor Predicting: 128it [01:20,  1.52it/s]Extractor Predicting: 129it [01:21,  1.54it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:22,  1.54it/s]Extractor Predicting: 132it [01:23,  1.57it/s]Extractor Predicting: 133it [01:24,  1.59it/s]Extractor Predicting: 134it [01:24,  1.57it/s]Extractor Predicting: 135it [01:25,  1.55it/s]Extractor Predicting: 136it [01:26,  1.56it/s]Extractor Predicting: 137it [01:26,  1.58it/s]Extractor Predicting: 138it [01:27,  1.60it/s]Extractor Predicting: 139it [01:27,  1.59it/s]Extractor Predicting: 140it [01:28,  1.61it/s]Extractor Predicting: 141it [01:29,  1.67it/s]Extractor Predicting: 141it [01:29,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:34,766 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:34,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:34,771 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:34,772 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:34,772 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:13:35,079 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:13:35,080 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:13:35,335 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:13:36,376 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:13:36,376 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:38,848 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:38,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:38,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:38,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:13:38,853 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:13:39,581 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:13:39,582 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:13:40,256 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:13:40,418 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:13:40,418 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2835570469798658,
  "recall": 0.14712710388856645,
  "score": 0.19373328238440962,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.65it/s]Extractor Predicting: 3it [00:01,  1.62it/s]Extractor Predicting: 4it [00:02,  1.69it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.65it/s]Extractor Predicting: 10it [00:06,  1.70it/s]Extractor Predicting: 11it [00:06,  1.66it/s]Extractor Predicting: 12it [00:07,  1.67it/s]Extractor Predicting: 13it [00:07,  1.62it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.65it/s]Extractor Predicting: 16it [00:09,  1.60it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:11,  1.59it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.60it/s]Extractor Predicting: 21it [00:12,  1.61it/s]Extractor Predicting: 22it [00:13,  1.62it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.61it/s]Extractor Predicting: 29it [00:17,  1.57it/s]Extractor Predicting: 30it [00:18,  1.53it/s]Extractor Predicting: 31it [00:19,  1.52it/s]Extractor Predicting: 32it [00:19,  1.51it/s]Extractor Predicting: 33it [00:20,  1.52it/s]Extractor Predicting: 34it [00:21,  1.54it/s]Extractor Predicting: 35it [00:21,  1.55it/s]Extractor Predicting: 36it [00:22,  1.57it/s]Extractor Predicting: 37it [00:23,  1.62it/s]Extractor Predicting: 38it [00:23,  1.61it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:25,  1.60it/s]Extractor Predicting: 42it [00:26,  1.59it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:27,  1.58it/s]Extractor Predicting: 45it [00:28,  1.56it/s]Extractor Predicting: 46it [00:28,  1.59it/s]Extractor Predicting: 47it [00:29,  1.59it/s]Extractor Predicting: 48it [00:30,  1.59it/s]Extractor Predicting: 49it [00:30,  1.58it/s]Extractor Predicting: 50it [00:31,  1.55it/s]Extractor Predicting: 51it [00:31,  1.56it/s]Extractor Predicting: 52it [00:32,  1.57it/s]Extractor Predicting: 53it [00:33,  1.57it/s]Extractor Predicting: 54it [00:33,  1.58it/s]Extractor Predicting: 55it [00:34,  1.55it/s]Extractor Predicting: 56it [00:35,  1.42it/s]Extractor Predicting: 57it [00:35,  1.49it/s]Extractor Predicting: 58it [00:36,  1.54it/s]Extractor Predicting: 59it [00:37,  1.57it/s]Extractor Predicting: 60it [00:37,  1.58it/s]Extractor Predicting: 61it [00:38,  1.58it/s]Extractor Predicting: 62it [00:39,  1.59it/s]Extractor Predicting: 63it [00:39,  1.56it/s]Extractor Predicting: 64it [00:40,  1.59it/s]Extractor Predicting: 65it [00:41,  1.57it/s]Extractor Predicting: 66it [00:41,  1.54it/s]Extractor Predicting: 67it [00:42,  1.59it/s]Extractor Predicting: 68it [00:42,  1.60it/s]Extractor Predicting: 69it [00:43,  1.63it/s]Extractor Predicting: 70it [00:44,  1.61it/s]Extractor Predicting: 71it [00:44,  1.60it/s]Extractor Predicting: 72it [00:45,  1.61it/s]Extractor Predicting: 73it [00:46,  1.56it/s]Extractor Predicting: 74it [00:46,  1.57it/s]Extractor Predicting: 75it [00:47,  1.57it/s]Extractor Predicting: 76it [00:47,  1.59it/s]Extractor Predicting: 77it [00:48,  1.55it/s]Extractor Predicting: 78it [00:49,  1.54it/s]Extractor Predicting: 79it [00:49,  1.56it/s]Extractor Predicting: 80it [00:50,  1.56it/s]Extractor Predicting: 81it [00:51,  1.56it/s]Extractor Predicting: 82it [00:51,  1.56it/s]Extractor Predicting: 83it [00:52,  1.54it/s]Extractor Predicting: 84it [00:53,  1.55it/s]Extractor Predicting: 85it [00:53,  1.56it/s]Extractor Predicting: 86it [00:54,  1.56it/s]Extractor Predicting: 87it [00:55,  1.55it/s]Extractor Predicting: 88it [00:55,  1.55it/s]Extractor Predicting: 89it [00:56,  1.53it/s]Extractor Predicting: 90it [00:57,  1.53it/s]Extractor Predicting: 91it [00:57,  1.54it/s]Extractor Predicting: 92it [00:58,  1.54it/s]Extractor Predicting: 93it [00:58,  1.53it/s]Extractor Predicting: 94it [00:59,  1.50it/s]Extractor Predicting: 95it [01:00,  1.49it/s]Extractor Predicting: 96it [01:00,  1.51it/s]Extractor Predicting: 97it [01:01,  1.51it/s]Extractor Predicting: 98it [01:02,  1.54it/s]Extractor Predicting: 99it [01:02,  1.52it/s]Extractor Predicting: 100it [01:03,  1.52it/s]Extractor Predicting: 101it [01:04,  1.54it/s]Extractor Predicting: 102it [01:04,  1.54it/s]Extractor Predicting: 103it [01:05,  1.53it/s]Extractor Predicting: 104it [01:06,  1.52it/s]Extractor Predicting: 105it [01:06,  1.52it/s]Extractor Predicting: 106it [01:07,  1.53it/s]Extractor Predicting: 107it [01:08,  1.49it/s]Extractor Predicting: 108it [01:08,  1.50it/s]Extractor Predicting: 109it [01:09,  1.51it/s]Extractor Predicting: 110it [01:10,  1.49it/s]Extractor Predicting: 111it [01:10,  1.50it/s]Extractor Predicting: 112it [01:11,  1.51it/s]Extractor Predicting: 113it [01:12,  1.55it/s]Extractor Predicting: 114it [01:12,  1.57it/s]Extractor Predicting: 115it [01:13,  1.55it/s]Extractor Predicting: 116it [01:14,  1.56it/s]Extractor Predicting: 117it [01:14,  1.60it/s]Extractor Predicting: 118it [01:15,  1.58it/s]Extractor Predicting: 119it [01:15,  1.60it/s]Extractor Predicting: 120it [01:16,  1.59it/s]Extractor Predicting: 121it [01:17,  1.60it/s]Extractor Predicting: 122it [01:17,  1.64it/s]Extractor Predicting: 123it [01:18,  1.63it/s]Extractor Predicting: 124it [01:18,  1.62it/s]Extractor Predicting: 125it [01:19,  1.60it/s]Extractor Predicting: 126it [01:20,  1.58it/s]Extractor Predicting: 127it [01:20,  1.60it/s]Extractor Predicting: 128it [01:21,  1.63it/s]Extractor Predicting: 129it [01:22,  1.58it/s]Extractor Predicting: 130it [01:22,  1.58it/s]Extractor Predicting: 131it [01:23,  1.58it/s]Extractor Predicting: 132it [01:24,  1.58it/s]Extractor Predicting: 133it [01:24,  1.57it/s]Extractor Predicting: 134it [01:25,  1.58it/s]Extractor Predicting: 135it [01:25,  1.59it/s]Extractor Predicting: 136it [01:26,  1.60it/s]Extractor Predicting: 137it [01:27,  1.60it/s]Extractor Predicting: 138it [01:27,  1.59it/s]Extractor Predicting: 139it [01:28,  1.63it/s]Extractor Predicting: 140it [01:29,  1.61it/s]Extractor Predicting: 141it [01:29,  1.58it/s]Extractor Predicting: 142it [01:30,  1.60it/s]Extractor Predicting: 143it [01:30,  1.60it/s]Extractor Predicting: 144it [01:31,  1.57it/s]Extractor Predicting: 145it [01:32,  1.58it/s]Extractor Predicting: 146it [01:32,  1.58it/s]Extractor Predicting: 147it [01:33,  1.57it/s]Extractor Predicting: 148it [01:34,  1.55it/s]Extractor Predicting: 149it [01:34,  1.56it/s]Extractor Predicting: 150it [01:35,  1.58it/s]Extractor Predicting: 151it [01:35,  1.61it/s]Extractor Predicting: 152it [01:36,  1.62it/s]Extractor Predicting: 153it [01:37,  1.62it/s]Extractor Predicting: 154it [01:38,  1.41it/s]Extractor Predicting: 155it [01:38,  1.46it/s]Extractor Predicting: 156it [01:39,  1.48it/s]Extractor Predicting: 157it [01:40,  1.49it/s]Extractor Predicting: 158it [01:40,  1.49it/s]Extractor Predicting: 159it [01:41,  1.51it/s]Extractor Predicting: 160it [01:42,  1.52it/s]Extractor Predicting: 161it [01:42,  1.55it/s]Extractor Predicting: 162it [01:43,  1.58it/s]Extractor Predicting: 163it [01:43,  1.58it/s]Extractor Predicting: 164it [01:44,  1.58it/s]Extractor Predicting: 165it [01:45,  1.56it/s]Extractor Predicting: 166it [01:45,  1.57it/s]Extractor Predicting: 167it [01:46,  1.60it/s]Extractor Predicting: 168it [01:47,  1.58it/s]Extractor Predicting: 169it [01:47,  1.57it/s]Extractor Predicting: 170it [01:48,  1.56it/s]Extractor Predicting: 171it [01:49,  1.53it/s]Extractor Predicting: 172it [01:49,  1.51it/s]Extractor Predicting: 173it [01:50,  1.50it/s]Extractor Predicting: 174it [01:51,  1.47it/s]Extractor Predicting: 175it [01:51,  1.44it/s]Extractor Predicting: 176it [01:52,  1.47it/s]Extractor Predicting: 177it [01:53,  1.50it/s]Extractor Predicting: 178it [01:53,  1.51it/s]Extractor Predicting: 179it [01:54,  1.53it/s]Extractor Predicting: 180it [01:55,  1.52it/s]Extractor Predicting: 181it [01:55,  1.55it/s]Extractor Predicting: 182it [01:56,  1.56it/s]Extractor Predicting: 183it [01:56,  1.55it/s]Extractor Predicting: 184it [01:57,  1.57it/s]Extractor Predicting: 185it [01:58,  1.59it/s]Extractor Predicting: 186it [01:58,  1.62it/s]Extractor Predicting: 187it [01:59,  1.60it/s]Extractor Predicting: 188it [02:00,  1.61it/s]Extractor Predicting: 189it [02:00,  1.61it/s]Extractor Predicting: 190it [02:01,  1.61it/s]Extractor Predicting: 191it [02:01,  1.57it/s]Extractor Predicting: 192it [02:02,  1.55it/s]Extractor Predicting: 193it [02:03,  1.56it/s]Extractor Predicting: 194it [02:03,  1.60it/s]Extractor Predicting: 195it [02:04,  1.60it/s]Extractor Predicting: 196it [02:05,  1.61it/s]Extractor Predicting: 197it [02:05,  1.62it/s]Extractor Predicting: 198it [02:06,  1.62it/s]Extractor Predicting: 199it [02:06,  1.60it/s]Extractor Predicting: 200it [02:07,  1.59it/s]Extractor Predicting: 201it [02:08,  1.59it/s]Extractor Predicting: 202it [02:08,  1.61it/s]Extractor Predicting: 203it [02:09,  1.63it/s]Extractor Predicting: 204it [02:10,  1.63it/s]Extractor Predicting: 205it [02:10,  1.63it/s]Extractor Predicting: 206it [02:11,  1.62it/s]Extractor Predicting: 207it [02:11,  1.57it/s]Extractor Predicting: 208it [02:12,  1.58it/s]Extractor Predicting: 209it [02:13,  1.58it/s]Extractor Predicting: 210it [02:13,  1.58it/s]Extractor Predicting: 211it [02:14,  1.60it/s]Extractor Predicting: 212it [02:15,  1.58it/s]Extractor Predicting: 213it [02:15,  1.61it/s]Extractor Predicting: 214it [02:16,  1.60it/s]Extractor Predicting: 215it [02:16,  1.63it/s]Extractor Predicting: 216it [02:17,  1.63it/s]Extractor Predicting: 217it [02:18,  1.55it/s]Extractor Predicting: 218it [02:18,  1.57it/s]Extractor Predicting: 219it [02:19,  1.60it/s]Extractor Predicting: 220it [02:20,  1.62it/s]Extractor Predicting: 221it [02:20,  1.60it/s]Extractor Predicting: 222it [02:21,  1.61it/s]Extractor Predicting: 223it [02:21,  1.63it/s]Extractor Predicting: 224it [02:22,  1.63it/s]Extractor Predicting: 225it [02:23,  1.61it/s]Extractor Predicting: 226it [02:23,  1.56it/s]Extractor Predicting: 227it [02:24,  1.55it/s]Extractor Predicting: 228it [02:25,  1.52it/s]Extractor Predicting: 229it [02:25,  1.53it/s]Extractor Predicting: 230it [02:26,  1.56it/s]Extractor Predicting: 231it [02:27,  1.54it/s]Extractor Predicting: 232it [02:27,  1.54it/s]Extractor Predicting: 233it [02:28,  1.56it/s]Extractor Predicting: 234it [02:29,  1.56it/s]Extractor Predicting: 235it [02:29,  1.52it/s]Extractor Predicting: 236it [02:30,  1.56it/s]Extractor Predicting: 237it [02:30,  1.60it/s]Extractor Predicting: 238it [02:31,  1.57it/s]Extractor Predicting: 239it [02:32,  1.54it/s]Extractor Predicting: 240it [02:32,  1.53it/s]Extractor Predicting: 241it [02:33,  1.53it/s]Extractor Predicting: 242it [02:34,  1.57it/s]Extractor Predicting: 243it [02:35,  1.39it/s]Extractor Predicting: 244it [02:35,  1.41it/s]Extractor Predicting: 245it [02:36,  1.46it/s]Extractor Predicting: 246it [02:37,  1.50it/s]Extractor Predicting: 247it [02:37,  1.53it/s]Extractor Predicting: 248it [02:38,  1.48it/s]Extractor Predicting: 249it [02:38,  1.51it/s]Extractor Predicting: 250it [02:39,  1.52it/s]Extractor Predicting: 251it [02:40,  1.50it/s]Extractor Predicting: 252it [02:40,  1.50it/s]Extractor Predicting: 253it [02:41,  1.51it/s]Extractor Predicting: 254it [02:42,  1.54it/s]Extractor Predicting: 255it [02:42,  1.55it/s]Extractor Predicting: 256it [02:43,  1.59it/s]Extractor Predicting: 257it [02:44,  1.61it/s]Extractor Predicting: 258it [02:44,  1.59it/s]Extractor Predicting: 259it [02:45,  1.58it/s]Extractor Predicting: 260it [02:46,  1.56it/s]Extractor Predicting: 261it [02:46,  1.56it/s]Extractor Predicting: 262it [02:47,  1.55it/s]Extractor Predicting: 263it [02:48,  1.50it/s]Extractor Predicting: 264it [02:48,  1.51it/s]Extractor Predicting: 265it [02:49,  1.54it/s]Extractor Predicting: 266it [02:49,  1.55it/s]Extractor Predicting: 267it [02:50,  1.56it/s]Extractor Predicting: 268it [02:51,  1.58it/s]Extractor Predicting: 269it [02:51,  1.61it/s]Extractor Predicting: 270it [02:52,  1.63it/s]Extractor Predicting: 271it [02:53,  1.59it/s]Extractor Predicting: 272it [02:53,  1.61it/s]Extractor Predicting: 273it [02:54,  1.59it/s]Extractor Predicting: 274it [02:54,  1.58it/s]Extractor Predicting: 275it [02:55,  1.60it/s]Extractor Predicting: 276it [02:56,  1.59it/s]Extractor Predicting: 277it [02:56,  1.57it/s]Extractor Predicting: 278it [02:57,  1.60it/s]Extractor Predicting: 279it [02:58,  1.56it/s]Extractor Predicting: 280it [02:58,  1.55it/s]Extractor Predicting: 281it [02:59,  1.56it/s]Extractor Predicting: 282it [03:00,  1.58it/s]Extractor Predicting: 283it [03:00,  1.58it/s]Extractor Predicting: 284it [03:01,  1.55it/s]Extractor Predicting: 285it [03:01,  1.56it/s]Extractor Predicting: 286it [03:02,  1.57it/s]Extractor Predicting: 287it [03:03,  1.60it/s]Extractor Predicting: 288it [03:03,  1.58it/s]Extractor Predicting: 289it [03:04,  1.54it/s]Extractor Predicting: 290it [03:05,  1.60it/s]Extractor Predicting: 291it [03:05,  1.58it/s]Extractor Predicting: 292it [03:06,  1.57it/s]Extractor Predicting: 293it [03:07,  1.54it/s]Extractor Predicting: 294it [03:07,  1.50it/s]Extractor Predicting: 295it [03:08,  1.55it/s]Extractor Predicting: 296it [03:09,  1.53it/s]Extractor Predicting: 297it [03:09,  1.56it/s]Extractor Predicting: 298it [03:10,  1.51it/s]Extractor Predicting: 299it [03:11,  1.52it/s]Extractor Predicting: 300it [03:11,  1.53it/s]Extractor Predicting: 301it [03:12,  1.54it/s]Extractor Predicting: 302it [03:12,  1.53it/s]Extractor Predicting: 303it [03:13,  1.52it/s]Extractor Predicting: 304it [03:14,  1.49it/s]Extractor Predicting: 305it [03:14,  1.50it/s]Extractor Predicting: 306it [03:15,  1.49it/s]Extractor Predicting: 307it [03:16,  1.48it/s]Extractor Predicting: 308it [03:17,  1.50it/s]Extractor Predicting: 309it [03:17,  1.50it/s]Extractor Predicting: 310it [03:18,  1.48it/s]Extractor Predicting: 311it [03:19,  1.47it/s]Extractor Predicting: 312it [03:19,  1.49it/s]Extractor Predicting: 313it [03:20,  1.49it/s]Extractor Predicting: 314it [03:21,  1.46it/s]Extractor Predicting: 315it [03:21,  1.51it/s]Extractor Predicting: 316it [03:22,  1.50it/s]Extractor Predicting: 317it [03:23,  1.48it/s]Extractor Predicting: 318it [03:23,  1.49it/s]Extractor Predicting: 319it [03:24,  1.49it/s]Extractor Predicting: 320it [03:24,  1.58it/s]Extractor Predicting: 321it [03:25,  1.61it/s]Extractor Predicting: 322it [03:26,  1.68it/s]Extractor Predicting: 323it [03:26,  1.72it/s]Extractor Predicting: 324it [03:27,  1.73it/s]Extractor Predicting: 325it [03:27,  1.75it/s]Extractor Predicting: 326it [03:28,  1.75it/s]Extractor Predicting: 327it [03:28,  1.74it/s]Extractor Predicting: 328it [03:29,  1.76it/s]Extractor Predicting: 329it [03:29,  1.79it/s]Extractor Predicting: 330it [03:30,  1.79it/s]Extractor Predicting: 331it [03:31,  1.83it/s]Extractor Predicting: 332it [03:31,  1.80it/s]Extractor Predicting: 333it [03:32,  1.83it/s]Extractor Predicting: 334it [03:32,  1.82it/s]Extractor Predicting: 335it [03:33,  1.82it/s]Extractor Predicting: 336it [03:33,  1.78it/s]Extractor Predicting: 337it [03:34,  1.83it/s]Extractor Predicting: 338it [03:34,  1.76it/s]Extractor Predicting: 339it [03:35,  1.75it/s]Extractor Predicting: 340it [03:36,  1.81it/s]Extractor Predicting: 341it [03:36,  1.79it/s]Extractor Predicting: 342it [03:37,  1.81it/s]Extractor Predicting: 343it [03:37,  1.80it/s]Extractor Predicting: 344it [03:38,  1.77it/s]Extractor Predicting: 345it [03:38,  1.79it/s]Extractor Predicting: 346it [03:39,  1.78it/s]Extractor Predicting: 347it [03:40,  1.76it/s]Extractor Predicting: 348it [03:40,  1.71it/s]Extractor Predicting: 349it [03:41,  1.67it/s]Extractor Predicting: 350it [03:41,  1.62it/s]Extractor Predicting: 351it [03:42,  1.60it/s]Extractor Predicting: 352it [03:43,  1.60it/s]Extractor Predicting: 353it [03:43,  1.62it/s]Extractor Predicting: 354it [03:44,  1.60it/s]Extractor Predicting: 355it [03:45,  1.58it/s]Extractor Predicting: 356it [03:45,  1.55it/s]Extractor Predicting: 357it [03:46,  1.53it/s]Extractor Predicting: 358it [03:47,  1.55it/s]Extractor Predicting: 359it [03:47,  1.55it/s]Extractor Predicting: 360it [03:48,  1.58it/s]Extractor Predicting: 361it [03:48,  1.57it/s]Extractor Predicting: 362it [03:49,  1.60it/s]Extractor Predicting: 363it [03:50,  1.57it/s]Extractor Predicting: 364it [03:50,  1.56it/s]Extractor Predicting: 365it [03:51,  1.56it/s]Extractor Predicting: 366it [03:52,  1.56it/s]Extractor Predicting: 367it [03:53,  1.38it/s]Extractor Predicting: 368it [03:53,  1.44it/s]Extractor Predicting: 369it [03:54,  1.47it/s]Extractor Predicting: 370it [03:55,  1.48it/s]Extractor Predicting: 371it [03:55,  1.51it/s]Extractor Predicting: 372it [03:56,  1.53it/s]Extractor Predicting: 373it [03:56,  1.52it/s]Extractor Predicting: 374it [03:57,  1.54it/s]Extractor Predicting: 375it [03:58,  1.56it/s]Extractor Predicting: 376it [03:58,  1.54it/s]Extractor Predicting: 377it [03:59,  1.55it/s]Extractor Predicting: 378it [04:00,  1.52it/s]Extractor Predicting: 379it [04:00,  1.52it/s]Extractor Predicting: 380it [04:01,  1.52it/s]Extractor Predicting: 381it [04:02,  1.50it/s]Extractor Predicting: 382it [04:02,  1.49it/s]Extractor Predicting: 383it [04:03,  1.50it/s]Extractor Predicting: 384it [04:04,  1.49it/s]Extractor Predicting: 385it [04:04,  1.50it/s]Extractor Predicting: 386it [04:05,  1.51it/s]Extractor Predicting: 387it [04:06,  1.50it/s]Extractor Predicting: 388it [04:06,  1.48it/s]Extractor Predicting: 389it [04:07,  1.52it/s]Extractor Predicting: 390it [04:08,  1.53it/s]Extractor Predicting: 391it [04:08,  1.53it/s]Extractor Predicting: 392it [04:09,  1.49it/s]Extractor Predicting: 393it [04:10,  1.53it/s]Extractor Predicting: 394it [04:10,  1.52it/s]Extractor Predicting: 395it [04:11,  1.49it/s]Extractor Predicting: 396it [04:12,  1.49it/s]Extractor Predicting: 397it [04:12,  1.47it/s]Extractor Predicting: 398it [04:13,  1.48it/s]Extractor Predicting: 399it [04:14,  1.45it/s]Extractor Predicting: 400it [04:14,  1.45it/s]Extractor Predicting: 401it [04:15,  1.46it/s]Extractor Predicting: 402it [04:16,  1.48it/s]Extractor Predicting: 403it [04:16,  1.48it/s]Extractor Predicting: 404it [04:17,  1.54it/s]Extractor Predicting: 405it [04:18,  1.52it/s]Extractor Predicting: 406it [04:18,  1.56it/s]Extractor Predicting: 407it [04:19,  1.55it/s]Extractor Predicting: 408it [04:20,  1.53it/s]Extractor Predicting: 409it [04:20,  1.57it/s]Extractor Predicting: 410it [04:21,  1.56it/s]Extractor Predicting: 411it [04:22,  1.59it/s]Extractor Predicting: 412it [04:22,  1.56it/s]Extractor Predicting: 413it [04:23,  1.59it/s]Extractor Predicting: 414it [04:23,  1.59it/s]Extractor Predicting: 415it [04:24,  1.58it/s]Extractor Predicting: 416it [04:25,  1.54it/s]Extractor Predicting: 417it [04:25,  1.57it/s]Extractor Predicting: 418it [04:26,  1.56it/s]Extractor Predicting: 419it [04:27,  1.55it/s]Extractor Predicting: 420it [04:27,  1.55it/s]Extractor Predicting: 421it [04:28,  1.56it/s]Extractor Predicting: 422it [04:29,  1.54it/s]Extractor Predicting: 423it [04:29,  1.55it/s]Extractor Predicting: 424it [04:30,  1.51it/s]Extractor Predicting: 425it [04:31,  1.53it/s]Extractor Predicting: 426it [04:31,  1.51it/s]Extractor Predicting: 427it [04:32,  1.54it/s]Extractor Predicting: 428it [04:33,  1.53it/s]Extractor Predicting: 429it [04:33,  1.54it/s]Extractor Predicting: 430it [04:33,  1.93it/s]Extractor Predicting: 430it [04:33,  1.57it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:22,335 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:22,339 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:22,339 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:22,339 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:22,339 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:18:23,054 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:18:23,054 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:18:23,315 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:18:24,411 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:18:24,411 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:26,902 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:26,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:26,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:26,906 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:18:26,907 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:18:27,239 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:18:27,241 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:18:27,917 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:18:28,090 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:18:28,090 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.27101368332867914,
  "recall": 0.1884832006214799,
  "score": 0.2223367697594502,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:02,  1.46it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.32it/s]Extractor Predicting: 5it [00:03,  1.37it/s]
[INFO|configuration_utils.py:515] 2023-08-29 00:18:32,120 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:18:32,121 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 00:18:32,126 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:18:32,126 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 00:18:32,128 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 00:18:35,369 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 00:18:35,371 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 00:18:35,396 >> loading configuration file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 00:18:35,397 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 00:18:35,414 >> Didn't find file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:18:35,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:18:35,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:18:35,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:18:35,422 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:18:35,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 00:18:35,422 >> loading file outputs/wrapper/fewrel/unseen_15_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.39655172413793105,
  "recall": 0.11386138613861387,
  "score": 0.17692307692307693,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_15_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 00:18:35,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:36,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:37,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:37,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:38,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:39,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:39,688 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:40,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:40,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:41,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:42,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:43,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:43,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:44,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:44,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:45,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:46,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:47,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:47,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:48,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:49,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:14<04:29, 14.19s/it][WARNING|generation_utils.py:914] 2023-08-29 00:18:49,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:50,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:51,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:51,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:52,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:53,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:53,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:54,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:54,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:55,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:56,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:57,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:57,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:58,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:59,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:18:59,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:00,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:00,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:01,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:02,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:02,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:03,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:28<04:12, 14.02s/it][WARNING|generation_utils.py:914] 2023-08-29 00:19:03,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:04,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:04,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:05,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:06,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:06,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:07,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:07,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:08,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:08,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:09,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:09,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:10,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:10,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:11,538 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:12,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:12,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:13,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:13,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:14,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:39<03:37, 12.80s/it][WARNING|generation_utils.py:914] 2023-08-29 00:19:15,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:15,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:16,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:17,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:17,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:18,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:18,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:19,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:20,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:20,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:21,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:22,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:22,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:23,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:24,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:24,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:25,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:26,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:26,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:27,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:27,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:53<03:30, 13.14s/it][WARNING|generation_utils.py:914] 2023-08-29 00:19:28,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:29,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:29,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:30,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:31,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:31,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:32,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:33,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:33,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:34,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:35,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:35,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:36,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:37,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:37,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:38,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:38,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:39,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:40,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:40,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [01:05<03:14, 12.97s/it][WARNING|generation_utils.py:914] 2023-08-29 00:19:41,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:42,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:42,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:43,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:44,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:44,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:45,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:46,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:46,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:47,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:48,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:48,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:49,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:50,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:51,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:51,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:52,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:53,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:53,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:54,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:55,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:56,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:21<03:13, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-29 00:19:56,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:57,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:58,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:58,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:59,344 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:19:59,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:00,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:01,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:01,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:02,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:02,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:03,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:04,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:04,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:05,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:05,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:06,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:07,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:07,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:08,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:33<02:52, 13.26s/it][WARNING|generation_utils.py:914] 2023-08-29 00:20:09,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:09,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:10,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:11,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:11,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:12,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:13,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:13,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:14,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:14,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:15,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:16,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:17,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:17,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:18,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:19,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:19,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:20,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:21,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:21,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:22,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:47<02:42, 13.54s/it][WARNING|generation_utils.py:914] 2023-08-29 00:20:23,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:23,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:24,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:24,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:25,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:26,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:26,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:27,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:28,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:28,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:29,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:30,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:30,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:31,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:31,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:32,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:33,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:33,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:34,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:35,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:36,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:36,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:37,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:38,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [02:03<02:36, 14.18s/it][WARNING|generation_utils.py:914] 2023-08-29 00:20:38,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:39,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:39,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:40,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:40,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:41,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:42,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:42,650 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:43,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:43,854 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:44,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:45,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:45,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:46,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:46,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:47,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:48,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:48,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:49,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:49,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:50,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:50,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:15<02:17, 13.78s/it][WARNING|generation_utils.py:914] 2023-08-29 00:20:51,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:52,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:53,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:53,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:54,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:54,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:55,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:56,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:57,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:58,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:58,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:20:59,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:00,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:00,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:01,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:02,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:02,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:03,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:04,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:05,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:05,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:30<02:07, 14.13s/it][WARNING|generation_utils.py:914] 2023-08-29 00:21:06,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:07,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:07,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:08,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:09,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:09,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:10,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:10,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:11,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:11,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:12,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:13,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:13,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:14,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:15,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:15,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:16,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:16,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:17,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:17,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:18,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:19,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:44<01:51, 13.91s/it][WARNING|generation_utils.py:914] 2023-08-29 00:21:19,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:20,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:21,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:21,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:22,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:23,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:23,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:24,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:24,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:25,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:26,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:26,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:27,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:28,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:28,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:29,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:30,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:30,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:31,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:32,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:57<01:34, 13.55s/it][WARNING|generation_utils.py:914] 2023-08-29 00:21:32,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:33,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:33,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:34,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:35,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:35,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:36,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:37,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:37,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:38,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:39,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:39,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:40,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:41,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:41,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:42,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:43,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:43,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:44,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:45,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:45,889 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [03:10<01:21, 13.65s/it][WARNING|generation_utils.py:914] 2023-08-29 00:21:46,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:47,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:47,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:48,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:49,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:49,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:50,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:51,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:51,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:52,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:53,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:53,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:54,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:55,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:55,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:56,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:57,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:57,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:58,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:59,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:21:59,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:00,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [03:25<01:09, 13.91s/it][WARNING|generation_utils.py:914] 2023-08-29 00:22:01,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:01,815 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:02,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:03,580 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:04,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:04,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:05,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:06,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:06,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:07,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:08,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:08,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:09,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:10,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:11,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:11,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:12,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:13,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:13,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:14,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:15,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:15,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:41<00:57, 14.43s/it][WARNING|generation_utils.py:914] 2023-08-29 00:22:16,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:17,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:18,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:18,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:19,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:20,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:21,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:22,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:22,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:23,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:24,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:24,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:25,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:26,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:26,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:27,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:28,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:28,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:29,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:30,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:30,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:55<00:43, 14.51s/it][WARNING|generation_utils.py:914] 2023-08-29 00:22:31,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:32,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:32,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:33,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:33,779 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:34,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:35,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:35,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:36,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:37,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:37,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:38,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:39,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:39,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:40,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:41,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:41,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:42,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:42,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:43,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:44,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [04:09<00:28, 14.29s/it][WARNING|generation_utils.py:914] 2023-08-29 00:22:45,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:45,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:46,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:47,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:48,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:48,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:49,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:50,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:50,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:51,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:52,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:52,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:53,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:54,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:54,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:55,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:56,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:56,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:57,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:58,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:59,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:22:59,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:00,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:00,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:01,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:02,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [04:26<00:15, 15.23s/it][WARNING|generation_utils.py:914] 2023-08-29 00:23:02,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:03,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:03,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:04,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:05,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:05,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:06,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:07,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:07,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:08,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:09,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:09,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:10,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:11,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:11,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:12,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:13,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:14,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:14,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:15,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:15,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 00:23:16,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:41<00:00, 15.13s/it]Generating: 100%|██████████| 20/20 [04:41<00:00, 14.09s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:23,440 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:23,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:23,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:23,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:23,446 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:23:23,726 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:23:23,727 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:23:24,380 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:23:25,457 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:23:25,457 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:26,830 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:26,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:26,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:26,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:23:26,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:23:27,160 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:23:27,161 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:23:27,425 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:23:27,573 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:23:27,573 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 580, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 222, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 467, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 605, 'raw': 704}
{'prompt': 'Relation : military rank .', 'success_rate': 0.859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 576, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.946875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9166666666666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : tributary .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 477, 'raw': 544}
{'target': 600, 'success': 506, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 565, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : architect .', 'success_rate': 0.8821022727272727, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : constellation . Context : The Cianostro system is a part of a cluster of planets in the order Leo . Head Entity : Leo , Tail Entity : constellation .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 124, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : constellation .', 'success_rate': 0.953125, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 150, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 391, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 627, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9330357142857143, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 129, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 426, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 500, 'raw': 608}
{'target': 600, 'success': 523, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 574, 'raw': 704}
{'target': 600, 'success': 598, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8138020833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 422, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 591, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8835227272727273, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 265, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 384, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 475, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 565, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 624, 'raw': 672}
{'prompt': 'Relation : follows .', 'success_rate': 0.9285714285714286, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 255, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 483, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 539, 'raw': 608}
{'target': 600, 'success': 569, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : league .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 487, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 547, 'raw': 576}
{'target': 600, 'success': 578, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 382, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 500, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 242, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 439, 'raw': 512}
{'target': 600, 'success': 470, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 615, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8735795454545454, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 289, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 407, 'raw': 448}
{'target': 600, 'success': 434, 'raw': 480}
{'target': 600, 'success': 459, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 543, 'raw': 608}
{'target': 600, 'success': 573, 'raw': 640}
{'target': 600, 'success': 599, 'raw': 672}
{'target': 600, 'success': 628, 'raw': 704}
{'prompt': 'Relation : notable work .', 'success_rate': 0.8920454545454546, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 430, 'raw': 480}
{'target': 600, 'success': 458, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 518, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 578, 'raw': 640}
{'target': 600, 'success': 607, 'raw': 672}
{'prompt': 'Relation : operator .', 'success_rate': 0.9032738095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 418, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 540, 'raw': 576}
{'target': 600, 'success': 569, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9360119047619048, 'errors': {''}}
['Relation : position held . Context : On 31 March 2014 , the Romanian national football team won their first match of the 2014 Asian Cup , losing 2–0 to eventual champion Malaysia . Head Entity : Malaysian , Tail Entity : national football team .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 68, 'raw': 96}
{'target': 600, 'success': 92, 'raw': 128}
{'target': 600, 'success': 111, 'raw': 160}
{'target': 600, 'success': 134, 'raw': 192}
{'target': 600, 'success': 159, 'raw': 224}
{'target': 600, 'success': 182, 'raw': 256}
{'target': 600, 'success': 206, 'raw': 288}
{'target': 600, 'success': 229, 'raw': 320}
{'target': 600, 'success': 250, 'raw': 352}
{'target': 600, 'success': 277, 'raw': 384}
{'target': 600, 'success': 303, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 349, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 391, 'raw': 544}
{'target': 600, 'success': 413, 'raw': 576}
{'target': 600, 'success': 438, 'raw': 608}
{'target': 600, 'success': 463, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 507, 'raw': 704}
{'target': 600, 'success': 528, 'raw': 736}
{'target': 600, 'success': 551, 'raw': 768}
{'target': 600, 'success': 576, 'raw': 800}
{'target': 600, 'success': 601, 'raw': 832}
{'prompt': 'Relation : position held .', 'success_rate': 0.7223557692307693, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 85, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 478, 'raw': 544}
{'target': 600, 'success': 507, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 595, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 12578
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12678, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.39it/s]Extractor Estimating: 2it [00:01,  1.35it/s]Extractor Estimating: 3it [00:02,  1.40it/s]Extractor Estimating: 4it [00:02,  1.37it/s]Extractor Estimating: 5it [00:03,  1.38it/s]Extractor Estimating: 6it [00:04,  1.39it/s]Extractor Estimating: 7it [00:04,  1.43it/s]Extractor Estimating: 8it [00:05,  1.46it/s]Extractor Estimating: 9it [00:06,  1.49it/s]Extractor Estimating: 10it [00:07,  1.46it/s]Extractor Estimating: 11it [00:07,  1.42it/s]Extractor Estimating: 12it [00:08,  1.47it/s]Extractor Estimating: 13it [00:09,  1.48it/s]Extractor Estimating: 14it [00:09,  1.49it/s]Extractor Estimating: 15it [00:10,  1.50it/s]Extractor Estimating: 16it [00:10,  1.53it/s]Extractor Estimating: 17it [00:11,  1.51it/s]Extractor Estimating: 18it [00:12,  1.49it/s]Extractor Estimating: 19it [00:13,  1.48it/s]Extractor Estimating: 20it [00:13,  1.48it/s]Extractor Estimating: 21it [00:14,  1.49it/s]Extractor Estimating: 22it [00:15,  1.49it/s]Extractor Estimating: 23it [00:15,  1.52it/s]Extractor Estimating: 24it [00:16,  1.51it/s]Extractor Estimating: 25it [00:17,  1.43it/s]Extractor Estimating: 26it [00:17,  1.51it/s]Extractor Estimating: 27it [00:18,  1.53it/s]Extractor Estimating: 28it [00:18,  1.53it/s]Extractor Estimating: 29it [00:19,  1.59it/s]Extractor Estimating: 30it [00:20,  1.56it/s]Extractor Estimating: 31it [00:20,  1.53it/s]Extractor Estimating: 32it [00:21,  1.56it/s]Extractor Estimating: 33it [00:22,  1.61it/s]Extractor Estimating: 34it [00:22,  1.64it/s]Extractor Estimating: 35it [00:23,  1.68it/s]Extractor Estimating: 36it [00:23,  1.70it/s]Extractor Estimating: 37it [00:24,  1.72it/s]Extractor Estimating: 38it [00:24,  1.75it/s]Extractor Estimating: 39it [00:25,  1.73it/s]Extractor Estimating: 40it [00:26,  1.78it/s]Extractor Estimating: 41it [00:26,  1.70it/s]Extractor Estimating: 42it [00:27,  1.74it/s]Extractor Estimating: 43it [00:27,  1.72it/s]Extractor Estimating: 44it [00:28,  1.72it/s]Extractor Estimating: 45it [00:28,  1.74it/s]Extractor Estimating: 46it [00:29,  1.76it/s]Extractor Estimating: 47it [00:30,  1.67it/s]Extractor Estimating: 48it [00:30,  1.72it/s]Extractor Estimating: 49it [00:31,  1.71it/s]Extractor Estimating: 50it [00:31,  1.72it/s]Extractor Estimating: 51it [00:32,  1.71it/s]Extractor Estimating: 52it [00:33,  1.71it/s]Extractor Estimating: 53it [00:33,  1.73it/s]Extractor Estimating: 54it [00:34,  1.76it/s]Extractor Estimating: 55it [00:34,  1.73it/s]Extractor Estimating: 56it [00:35,  1.75it/s]Extractor Estimating: 57it [00:35,  1.74it/s]Extractor Estimating: 58it [00:36,  1.72it/s]Extractor Estimating: 59it [00:37,  1.71it/s]Extractor Estimating: 60it [00:37,  1.73it/s]Extractor Estimating: 61it [00:38,  1.75it/s]Extractor Estimating: 62it [00:38,  1.73it/s]Extractor Estimating: 63it [00:39,  1.78it/s]Extractor Estimating: 64it [00:39,  1.73it/s]Extractor Estimating: 65it [00:40,  1.70it/s]Extractor Estimating: 66it [00:41,  1.76it/s]Extractor Estimating: 67it [00:41,  1.70it/s]Extractor Estimating: 68it [00:42,  1.78it/s]Extractor Estimating: 69it [00:42,  1.79it/s]Extractor Estimating: 70it [00:43,  1.75it/s]Extractor Estimating: 71it [00:43,  1.73it/s]Extractor Estimating: 72it [00:44,  1.70it/s]Extractor Estimating: 73it [00:45,  1.68it/s]Extractor Estimating: 74it [00:45,  1.67it/s]Extractor Estimating: 75it [00:46,  1.56it/s]Extractor Estimating: 76it [00:47,  1.52it/s]Extractor Estimating: 77it [00:47,  1.55it/s]Extractor Estimating: 78it [00:48,  1.53it/s]Extractor Estimating: 79it [00:49,  1.55it/s]Extractor Estimating: 80it [00:49,  1.55it/s]Extractor Estimating: 81it [00:50,  1.54it/s]Extractor Estimating: 82it [00:51,  1.54it/s]Extractor Estimating: 83it [00:51,  1.53it/s]Extractor Estimating: 84it [00:52,  1.53it/s]Extractor Estimating: 85it [00:53,  1.56it/s]Extractor Estimating: 86it [00:53,  1.52it/s]Extractor Estimating: 87it [00:54,  1.56it/s]Extractor Estimating: 88it [00:55,  1.52it/s]Extractor Estimating: 89it [00:55,  1.48it/s]Extractor Estimating: 90it [00:56,  1.49it/s]Extractor Estimating: 91it [00:57,  1.54it/s]Extractor Estimating: 92it [00:57,  1.56it/s]Extractor Estimating: 93it [00:58,  1.51it/s]Extractor Estimating: 94it [00:58,  1.56it/s]Extractor Estimating: 95it [00:59,  1.56it/s]Extractor Estimating: 96it [01:00,  1.57it/s]Extractor Estimating: 97it [01:00,  1.60it/s]Extractor Estimating: 98it [01:01,  1.45it/s]Extractor Estimating: 99it [01:02,  1.43it/s]Extractor Estimating: 100it [01:03,  1.47it/s]Extractor Estimating: 101it [01:03,  1.58it/s]Extractor Estimating: 102it [01:03,  1.72it/s]Extractor Estimating: 103it [01:04,  1.77it/s]Extractor Estimating: 104it [01:05,  1.82it/s]Extractor Estimating: 105it [01:05,  1.81it/s]Extractor Estimating: 106it [01:06,  1.84it/s]Extractor Estimating: 107it [01:06,  1.82it/s]Extractor Estimating: 108it [01:07,  1.85it/s]Extractor Estimating: 109it [01:07,  1.88it/s]Extractor Estimating: 110it [01:08,  1.90it/s]Extractor Estimating: 111it [01:08,  1.84it/s]Extractor Estimating: 112it [01:09,  1.82it/s]Extractor Estimating: 113it [01:09,  1.88it/s]Extractor Estimating: 114it [01:10,  1.83it/s]Extractor Estimating: 115it [01:11,  1.81it/s]Extractor Estimating: 116it [01:11,  1.80it/s]Extractor Estimating: 117it [01:12,  1.77it/s]Extractor Estimating: 118it [01:12,  1.78it/s]Extractor Estimating: 119it [01:13,  1.80it/s]Extractor Estimating: 120it [01:13,  1.82it/s]Extractor Estimating: 121it [01:14,  1.83it/s]Extractor Estimating: 122it [01:14,  1.90it/s]Extractor Estimating: 123it [01:15,  1.90it/s]Extractor Estimating: 124it [01:15,  1.84it/s]Extractor Estimating: 125it [01:16,  1.84it/s]Extractor Estimating: 126it [01:17,  1.77it/s]Extractor Estimating: 127it [01:17,  1.77it/s]Extractor Estimating: 128it [01:18,  1.73it/s]Extractor Estimating: 129it [01:18,  1.72it/s]Extractor Estimating: 130it [01:19,  1.67it/s]Extractor Estimating: 131it [01:20,  1.65it/s]Extractor Estimating: 132it [01:20,  1.67it/s]Extractor Estimating: 133it [01:21,  1.71it/s]Extractor Estimating: 134it [01:21,  1.65it/s]Extractor Estimating: 135it [01:22,  1.65it/s]Extractor Estimating: 136it [01:23,  1.66it/s]Extractor Estimating: 137it [01:23,  1.65it/s]Extractor Estimating: 138it [01:24,  1.61it/s]Extractor Estimating: 139it [01:24,  1.61it/s]Extractor Estimating: 140it [01:25,  1.63it/s]Extractor Estimating: 141it [01:26,  1.62it/s]Extractor Estimating: 142it [01:26,  1.63it/s]Extractor Estimating: 143it [01:27,  1.61it/s]Extractor Estimating: 144it [01:28,  1.58it/s]Extractor Estimating: 145it [01:28,  1.59it/s]Extractor Estimating: 146it [01:29,  1.63it/s]Extractor Estimating: 147it [01:29,  1.63it/s]Extractor Estimating: 148it [01:30,  1.61it/s]Extractor Estimating: 149it [01:31,  1.65it/s]Extractor Estimating: 150it [01:31,  1.60it/s]Extractor Estimating: 151it [01:32,  1.62it/s]Extractor Estimating: 152it [01:33,  1.62it/s]Extractor Estimating: 153it [01:33,  1.58it/s]Extractor Estimating: 154it [01:34,  1.65it/s]Extractor Estimating: 155it [01:34,  1.69it/s]Extractor Estimating: 156it [01:35,  1.76it/s]Extractor Estimating: 157it [01:35,  1.78it/s]Extractor Estimating: 158it [01:36,  1.79it/s]Extractor Estimating: 159it [01:37,  1.68it/s]Extractor Estimating: 160it [01:37,  1.67it/s]Extractor Estimating: 161it [01:38,  1.69it/s]Extractor Estimating: 162it [01:38,  1.73it/s]Extractor Estimating: 163it [01:39,  1.73it/s]Extractor Estimating: 164it [01:39,  1.75it/s]Extractor Estimating: 165it [01:40,  1.76it/s]Extractor Estimating: 166it [01:41,  1.76it/s]Extractor Estimating: 167it [01:41,  1.75it/s]Extractor Estimating: 168it [01:42,  1.69it/s]Extractor Estimating: 169it [01:42,  1.70it/s]Extractor Estimating: 170it [01:43,  1.69it/s]Extractor Estimating: 171it [01:44,  1.70it/s]Extractor Estimating: 172it [01:44,  1.71it/s]Extractor Estimating: 173it [01:45,  1.70it/s]Extractor Estimating: 174it [01:45,  1.75it/s]Extractor Estimating: 175it [01:46,  1.78it/s]Extractor Estimating: 176it [01:46,  1.74it/s]Extractor Estimating: 177it [01:47,  1.73it/s]Extractor Estimating: 178it [01:48,  1.67it/s]Extractor Estimating: 179it [01:48,  1.65it/s]Extractor Estimating: 180it [01:49,  1.69it/s]Extractor Estimating: 181it [01:49,  1.73it/s]Extractor Estimating: 182it [01:50,  1.75it/s]Extractor Estimating: 183it [01:50,  1.78it/s]Extractor Estimating: 184it [01:51,  1.82it/s]Extractor Estimating: 185it [01:52,  1.82it/s]Extractor Estimating: 186it [01:52,  1.81it/s]Extractor Estimating: 187it [01:53,  1.78it/s]Extractor Estimating: 188it [01:53,  1.78it/s]Extractor Estimating: 189it [01:54,  1.75it/s]Extractor Estimating: 190it [01:54,  1.72it/s]Extractor Estimating: 191it [01:55,  1.79it/s]Extractor Estimating: 192it [01:56,  1.73it/s]Extractor Estimating: 193it [01:56,  1.74it/s]Extractor Estimating: 194it [01:57,  1.69it/s]Extractor Estimating: 195it [01:58,  1.54it/s]Extractor Estimating: 196it [01:58,  1.62it/s]Extractor Estimating: 197it [01:59,  1.66it/s]Extractor Estimating: 198it [01:59,  1.65it/s]Extractor Estimating: 199it [02:00,  1.67it/s]Extractor Estimating: 200it [02:00,  1.71it/s]Extractor Estimating: 201it [02:01,  1.65it/s]Extractor Estimating: 202it [02:02,  1.67it/s]Extractor Estimating: 203it [02:02,  1.74it/s]Extractor Estimating: 204it [02:03,  1.72it/s]Extractor Estimating: 205it [02:03,  1.70it/s]Extractor Estimating: 206it [02:04,  1.69it/s]Extractor Estimating: 207it [02:04,  1.73it/s]Extractor Estimating: 208it [02:05,  1.79it/s]Extractor Estimating: 209it [02:06,  1.67it/s]Extractor Estimating: 210it [02:06,  1.67it/s]Extractor Estimating: 211it [02:07,  1.63it/s]Extractor Estimating: 212it [02:08,  1.63it/s]Extractor Estimating: 213it [02:08,  1.63it/s]Extractor Estimating: 214it [02:09,  1.62it/s]Extractor Estimating: 215it [02:09,  1.64it/s]Extractor Estimating: 216it [02:10,  1.68it/s]Extractor Estimating: 217it [02:11,  1.69it/s]Extractor Estimating: 218it [02:11,  1.65it/s]Extractor Estimating: 219it [02:12,  1.64it/s]Extractor Estimating: 220it [02:12,  1.64it/s]Extractor Estimating: 221it [02:13,  1.70it/s]Extractor Estimating: 222it [02:14,  1.71it/s]Extractor Estimating: 223it [02:14,  1.74it/s]Extractor Estimating: 224it [02:15,  1.76it/s]Extractor Estimating: 225it [02:15,  1.65it/s]Extractor Estimating: 226it [02:16,  1.62it/s]Extractor Estimating: 227it [02:17,  1.58it/s]Extractor Estimating: 228it [02:17,  1.57it/s]Extractor Estimating: 229it [02:18,  1.58it/s]Extractor Estimating: 230it [02:18,  1.63it/s]Extractor Estimating: 231it [02:19,  1.64it/s]Extractor Estimating: 232it [02:20,  1.60it/s]Extractor Estimating: 233it [02:20,  1.64it/s]Extractor Estimating: 234it [02:21,  1.60it/s]Extractor Estimating: 235it [02:22,  1.58it/s]Extractor Estimating: 236it [02:22,  1.60it/s]Extractor Estimating: 237it [02:23,  1.54it/s]Extractor Estimating: 238it [02:24,  1.55it/s]Extractor Estimating: 239it [02:24,  1.58it/s]Extractor Estimating: 240it [02:25,  1.57it/s]Extractor Estimating: 241it [02:25,  1.59it/s]Extractor Estimating: 242it [02:26,  1.63it/s]Extractor Estimating: 243it [02:27,  1.61it/s]Extractor Estimating: 244it [02:27,  1.63it/s]Extractor Estimating: 245it [02:28,  1.63it/s]Extractor Estimating: 246it [02:29,  1.59it/s]Extractor Estimating: 247it [02:29,  1.60it/s]Extractor Estimating: 248it [02:30,  1.62it/s]Extractor Estimating: 249it [02:30,  1.57it/s]Extractor Estimating: 250it [02:31,  1.55it/s]Extractor Estimating: 251it [02:32,  1.51it/s]Extractor Estimating: 252it [02:33,  1.46it/s]Extractor Estimating: 253it [02:33,  1.47it/s]Extractor Estimating: 254it [02:34,  1.50it/s]Extractor Estimating: 255it [02:35,  1.49it/s]Extractor Estimating: 256it [02:35,  1.50it/s]Extractor Estimating: 257it [02:36,  1.51it/s]Extractor Estimating: 258it [02:36,  1.50it/s]Extractor Estimating: 259it [02:37,  1.47it/s]Extractor Estimating: 260it [02:38,  1.50it/s]Extractor Estimating: 261it [02:39,  1.42it/s]Extractor Estimating: 262it [02:39,  1.44it/s]Extractor Estimating: 263it [02:40,  1.40it/s]Extractor Estimating: 264it [02:41,  1.49it/s]Extractor Estimating: 265it [02:41,  1.49it/s]Extractor Estimating: 266it [02:42,  1.36it/s]Extractor Estimating: 267it [02:43,  1.39it/s]Extractor Estimating: 268it [02:44,  1.40it/s]Extractor Estimating: 269it [02:44,  1.39it/s]Extractor Estimating: 270it [02:45,  1.42it/s]Extractor Estimating: 271it [02:46,  1.42it/s]Extractor Estimating: 272it [02:46,  1.40it/s]Extractor Estimating: 273it [02:47,  1.41it/s]Extractor Estimating: 274it [02:48,  1.39it/s]Extractor Estimating: 275it [02:49,  1.41it/s]Extractor Estimating: 276it [02:49,  1.44it/s]Extractor Estimating: 277it [02:50,  1.50it/s]Extractor Estimating: 278it [02:50,  1.55it/s]Extractor Estimating: 279it [02:51,  1.60it/s]Extractor Estimating: 280it [02:52,  1.59it/s]Extractor Estimating: 281it [02:52,  1.55it/s]Extractor Estimating: 282it [02:53,  1.54it/s]Extractor Estimating: 283it [02:54,  1.59it/s]Extractor Estimating: 284it [02:54,  1.64it/s]Extractor Estimating: 285it [02:55,  1.66it/s]Extractor Estimating: 286it [02:55,  1.65it/s]Extractor Estimating: 287it [02:56,  1.68it/s]Extractor Estimating: 288it [02:56,  1.69it/s]Extractor Estimating: 289it [02:57,  1.67it/s]Extractor Estimating: 290it [02:58,  1.63it/s]Extractor Estimating: 291it [02:58,  1.58it/s]Extractor Estimating: 292it [02:59,  1.58it/s]Extractor Estimating: 293it [03:00,  1.61it/s]Extractor Estimating: 294it [03:00,  1.65it/s]Extractor Estimating: 295it [03:01,  1.68it/s]Extractor Estimating: 296it [03:01,  1.71it/s]Extractor Estimating: 297it [03:02,  1.70it/s]Extractor Estimating: 298it [03:03,  1.64it/s]Extractor Estimating: 299it [03:03,  1.67it/s]Extractor Estimating: 300it [03:04,  1.59it/s]Extractor Estimating: 301it [03:04,  1.63it/s]Extractor Estimating: 302it [03:05,  1.66it/s]Extractor Estimating: 303it [03:06,  1.73it/s]Extractor Estimating: 304it [03:06,  1.82it/s]Extractor Estimating: 305it [03:07,  1.84it/s]Extractor Estimating: 306it [03:07,  1.90it/s]Extractor Estimating: 307it [03:08,  1.90it/s]Extractor Estimating: 308it [03:08,  1.92it/s]Extractor Estimating: 309it [03:09,  1.94it/s]Extractor Estimating: 310it [03:09,  1.96it/s]Extractor Estimating: 311it [03:10,  1.88it/s]Extractor Estimating: 312it [03:10,  1.86it/s]Extractor Estimating: 313it [03:11,  1.87it/s]Extractor Estimating: 314it [03:11,  1.91it/s]Extractor Estimating: 315it [03:12,  1.93it/s]Extractor Estimating: 316it [03:12,  1.93it/s]Extractor Estimating: 317it [03:13,  1.93it/s]Extractor Estimating: 318it [03:13,  1.97it/s]Extractor Estimating: 319it [03:14,  1.90it/s]Extractor Estimating: 320it [03:14,  1.83it/s]Extractor Estimating: 321it [03:15,  1.86it/s]Extractor Estimating: 322it [03:15,  1.90it/s]Extractor Estimating: 323it [03:16,  1.90it/s]Extractor Estimating: 324it [03:16,  1.92it/s]Extractor Estimating: 325it [03:17,  1.86it/s]Extractor Estimating: 326it [03:18,  1.76it/s]Extractor Estimating: 327it [03:18,  1.70it/s]Extractor Estimating: 328it [03:19,  1.71it/s]Extractor Estimating: 329it [03:20,  1.67it/s]Extractor Estimating: 330it [03:20,  1.62it/s]Extractor Estimating: 331it [03:21,  1.65it/s]Extractor Estimating: 332it [03:21,  1.61it/s]Extractor Estimating: 333it [03:22,  1.65it/s]Extractor Estimating: 334it [03:23,  1.59it/s]Extractor Estimating: 335it [03:23,  1.51it/s]Extractor Estimating: 336it [03:24,  1.53it/s]Extractor Estimating: 337it [03:25,  1.56it/s]Extractor Estimating: 338it [03:25,  1.60it/s]Extractor Estimating: 339it [03:26,  1.57it/s]Extractor Estimating: 340it [03:27,  1.59it/s]Extractor Estimating: 341it [03:27,  1.66it/s]Extractor Estimating: 342it [03:28,  1.58it/s]Extractor Estimating: 343it [03:28,  1.61it/s]Extractor Estimating: 344it [03:29,  1.64it/s]Extractor Estimating: 345it [03:30,  1.64it/s]Extractor Estimating: 346it [03:30,  1.64it/s]Extractor Estimating: 347it [03:31,  1.55it/s]Extractor Estimating: 348it [03:32,  1.56it/s]Extractor Estimating: 349it [03:32,  1.54it/s]Extractor Estimating: 350it [03:33,  1.55it/s]Extractor Estimating: 351it [03:33,  1.62it/s]Extractor Estimating: 352it [03:34,  1.63it/s]Extractor Estimating: 353it [03:35,  1.59it/s]Extractor Estimating: 354it [03:35,  1.60it/s]Extractor Estimating: 355it [03:36,  1.60it/s]Extractor Estimating: 356it [03:37,  1.59it/s]Extractor Estimating: 357it [03:37,  1.58it/s]Extractor Estimating: 358it [03:38,  1.58it/s]Extractor Estimating: 359it [03:38,  1.65it/s]Extractor Estimating: 360it [03:39,  1.56it/s]Extractor Estimating: 361it [03:40,  1.64it/s]Extractor Estimating: 362it [03:40,  1.68it/s]Extractor Estimating: 363it [03:41,  1.70it/s]Extractor Estimating: 364it [03:41,  1.68it/s]Extractor Estimating: 365it [03:42,  1.66it/s]Extractor Estimating: 366it [03:43,  1.66it/s]Extractor Estimating: 367it [03:43,  1.69it/s]Extractor Estimating: 368it [03:44,  1.63it/s]Extractor Estimating: 369it [03:44,  1.64it/s]Extractor Estimating: 370it [03:45,  1.63it/s]Extractor Estimating: 371it [03:46,  1.61it/s]Extractor Estimating: 372it [03:46,  1.62it/s]Extractor Estimating: 373it [03:47,  1.64it/s]Extractor Estimating: 374it [03:48,  1.48it/s]Extractor Estimating: 375it [03:48,  1.56it/s]Extractor Estimating: 376it [03:49,  1.51it/s]Extractor Estimating: 377it [03:50,  1.28it/s]Extractor Estimating: 378it [03:51,  1.31it/s]Extractor Estimating: 379it [03:51,  1.38it/s]Extractor Estimating: 380it [03:52,  1.41it/s]Extractor Estimating: 381it [03:53,  1.47it/s]Extractor Estimating: 382it [03:53,  1.48it/s]Extractor Estimating: 383it [03:54,  1.47it/s]Extractor Estimating: 384it [03:55,  1.48it/s]Extractor Estimating: 385it [03:55,  1.45it/s]Extractor Estimating: 386it [03:56,  1.48it/s]Extractor Estimating: 387it [03:57,  1.44it/s]Extractor Estimating: 388it [03:57,  1.47it/s]Extractor Estimating: 389it [03:58,  1.48it/s]Extractor Estimating: 390it [03:59,  1.49it/s]Extractor Estimating: 391it [04:00,  1.42it/s]Extractor Estimating: 392it [04:00,  1.47it/s]Extractor Estimating: 393it [04:01,  1.47it/s]Extractor Estimating: 394it [04:01,  1.51it/s]Extractor Estimating: 395it [04:02,  1.53it/s]Extractor Estimating: 396it [04:03,  1.53it/s]Extractor Estimating: 397it [04:03,  1.48it/s]Extractor Estimating: 398it [04:04,  1.42it/s]Extractor Estimating: 399it [04:05,  1.46it/s]Extractor Estimating: 400it [04:06,  1.50it/s]Extractor Estimating: 401it [04:06,  1.53it/s]Extractor Estimating: 402it [04:07,  1.52it/s]Extractor Estimating: 403it [04:07,  1.56it/s]Extractor Estimating: 404it [04:08,  1.51it/s]Extractor Estimating: 405it [04:09,  1.51it/s]Extractor Estimating: 406it [04:09,  1.51it/s]Extractor Estimating: 407it [04:10,  1.55it/s]Extractor Estimating: 408it [04:11,  1.59it/s]Extractor Estimating: 409it [04:11,  1.64it/s]Extractor Estimating: 410it [04:12,  1.62it/s]Extractor Estimating: 411it [04:12,  1.63it/s]Extractor Estimating: 412it [04:13,  1.58it/s]Extractor Estimating: 413it [04:14,  1.55it/s]Extractor Estimating: 414it [04:14,  1.55it/s]Extractor Estimating: 415it [04:15,  1.57it/s]Extractor Estimating: 416it [04:16,  1.60it/s]Extractor Estimating: 417it [04:16,  1.64it/s]Extractor Estimating: 418it [04:17,  1.65it/s]Extractor Estimating: 419it [04:17,  1.61it/s]Extractor Estimating: 420it [04:18,  1.57it/s]Extractor Estimating: 421it [04:19,  1.59it/s]Extractor Estimating: 422it [04:19,  1.65it/s]Extractor Estimating: 423it [04:20,  1.65it/s]Extractor Estimating: 424it [04:21,  1.61it/s]Extractor Estimating: 425it [04:21,  1.54it/s]Extractor Estimating: 426it [04:22,  1.53it/s]Extractor Estimating: 427it [04:23,  1.54it/s]Extractor Estimating: 428it [04:23,  1.57it/s]Extractor Estimating: 429it [04:24,  1.58it/s]Extractor Estimating: 430it [04:24,  1.58it/s]Extractor Estimating: 431it [04:25,  1.57it/s]Extractor Estimating: 432it [04:26,  1.55it/s]Extractor Estimating: 433it [04:26,  1.54it/s]Extractor Estimating: 434it [04:27,  1.50it/s]Extractor Estimating: 435it [04:28,  1.37it/s]Extractor Estimating: 436it [04:29,  1.43it/s]Extractor Estimating: 437it [04:29,  1.46it/s]Extractor Estimating: 438it [04:30,  1.49it/s]Extractor Estimating: 439it [04:31,  1.53it/s]Extractor Estimating: 440it [04:31,  1.57it/s]Extractor Estimating: 441it [04:32,  1.58it/s]Extractor Estimating: 442it [04:32,  1.58it/s]Extractor Estimating: 443it [04:33,  1.55it/s]Extractor Estimating: 444it [04:34,  1.56it/s]Extractor Estimating: 445it [04:34,  1.57it/s]Extractor Estimating: 446it [04:35,  1.58it/s]Extractor Estimating: 447it [04:36,  1.63it/s]Extractor Estimating: 448it [04:36,  1.57it/s]Extractor Estimating: 449it [04:37,  1.61it/s]Extractor Estimating: 450it [04:37,  1.58it/s]Extractor Estimating: 451it [04:38,  1.58it/s]Extractor Estimating: 452it [04:39,  1.62it/s]Extractor Estimating: 453it [04:39,  1.65it/s]Extractor Estimating: 454it [04:40,  1.66it/s]Extractor Estimating: 455it [04:40,  1.66it/s]Extractor Estimating: 456it [04:41,  1.68it/s]Extractor Estimating: 457it [04:42,  1.66it/s]Extractor Estimating: 458it [04:42,  1.68it/s]Extractor Estimating: 459it [04:43,  1.69it/s]Extractor Estimating: 460it [04:43,  1.69it/s]Extractor Estimating: 461it [04:44,  1.66it/s]Extractor Estimating: 462it [04:45,  1.72it/s]Extractor Estimating: 463it [04:45,  1.73it/s]Extractor Estimating: 464it [04:46,  1.77it/s]Extractor Estimating: 465it [04:46,  1.72it/s]Extractor Estimating: 466it [04:47,  1.71it/s]Extractor Estimating: 467it [04:48,  1.67it/s]Extractor Estimating: 468it [04:48,  1.67it/s]Extractor Estimating: 469it [04:49,  1.51it/s]Extractor Estimating: 470it [04:50,  1.56it/s]Extractor Estimating: 471it [04:50,  1.60it/s]Extractor Estimating: 472it [04:51,  1.59it/s]Extractor Estimating: 473it [04:51,  1.61it/s]Extractor Estimating: 474it [04:52,  1.64it/s]Extractor Estimating: 475it [04:53,  1.63it/s]Extractor Estimating: 476it [04:53,  1.65it/s]Extractor Estimating: 477it [04:54,  1.63it/s]Extractor Estimating: 478it [04:54,  1.59it/s]Extractor Estimating: 479it [04:55,  1.63it/s]Extractor Estimating: 480it [04:56,  1.66it/s]Extractor Estimating: 481it [04:56,  1.63it/s]Extractor Estimating: 482it [04:57,  1.67it/s]Extractor Estimating: 483it [04:57,  1.64it/s]Extractor Estimating: 484it [04:58,  1.56it/s]Extractor Estimating: 485it [04:59,  1.55it/s]Extractor Estimating: 486it [04:59,  1.58it/s]Extractor Estimating: 487it [05:00,  1.62it/s]Extractor Estimating: 488it [05:01,  1.67it/s]Extractor Estimating: 489it [05:01,  1.60it/s]Extractor Estimating: 490it [05:02,  1.60it/s]Extractor Estimating: 491it [05:02,  1.63it/s]Extractor Estimating: 492it [05:03,  1.62it/s]Extractor Estimating: 493it [05:04,  1.60it/s]Extractor Estimating: 494it [05:04,  1.65it/s]Extractor Estimating: 495it [05:05,  1.62it/s]Extractor Estimating: 496it [05:06,  1.64it/s]Extractor Estimating: 497it [05:06,  1.63it/s]Extractor Estimating: 498it [05:07,  1.62it/s]Extractor Estimating: 499it [05:07,  1.63it/s]Extractor Estimating: 500it [05:08,  1.74it/s]Extractor Estimating: 500it [05:08,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:51,379 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:51,384 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:51,384 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:51,384 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:51,384 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 00:28:52,004 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 00:28:52,005 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:28:52,568 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 00:28:53,663 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:28:53,663 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:56,491 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:56,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:56,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:56,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 00:28:56,499 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 00:28:57,138 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 00:28:57,138 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 00:28:57,708 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 00:28:57,885 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 00:28:57,885 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 01:37:33,142 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 01:37:33,179 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 6000}
num of filtered data: 4021 mean pseudo reward: 0.988411120592366
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 14517
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14617, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14617, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.032, loss:324.5038
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 32, avg_time 1.011, loss:283.8774
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 132, avg_time 1.027, loss:250.1649
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 64, avg_time 1.023, loss:219.5829
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 164, avg_time 1.020, loss:222.3422
>> valid entity prec:0.5774, rec:0.5866, f1:0.5820
>> valid relation prec:0.2270, rec:0.1466, f1:0.1781
>> valid relation with NER prec:0.2270, rec:0.1466, f1:0.1781
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 96, avg_time 2.257, loss:182.0621
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 28, avg_time 1.025, loss:199.0281
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 128, avg_time 1.025, loss:181.1916
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 60, avg_time 1.021, loss:165.6842
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 160, avg_time 1.019, loss:193.1867
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5506, rec:0.5471, f1:0.5488
>> valid relation prec:0.1840, rec:0.1286, f1:0.1514
>> valid relation with NER prec:0.1840, rec:0.1286, f1:0.1514
g_step 1100, step 92, avg_time 2.223, loss:196.5179
g_step 1200, step 24, avg_time 1.017, loss:162.7739
g_step 1300, step 124, avg_time 1.022, loss:160.1402
g_step 1400, step 56, avg_time 1.015, loss:161.5834
g_step 1500, step 156, avg_time 1.036, loss:149.1716
>> valid entity prec:0.5792, rec:0.5594, f1:0.5691
>> valid relation prec:0.1885, rec:0.1312, f1:0.1547
>> valid relation with NER prec:0.1885, rec:0.1312, f1:0.1547
g_step 1600, step 88, avg_time 2.236, loss:126.7238
g_step 1700, step 20, avg_time 1.017, loss:135.3257
g_step 1800, step 120, avg_time 1.036, loss:122.2537
g_step 1900, step 52, avg_time 0.997, loss:128.4409
g_step 2000, step 152, avg_time 1.015, loss:141.7185
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5744, rec:0.5308, f1:0.5517
>> valid relation prec:0.1810, rec:0.1347, f1:0.1545
>> valid relation with NER prec:0.1810, rec:0.1347, f1:0.1545
g_step 2100, step 84, avg_time 2.231, loss:119.1096
g_step 2200, step 16, avg_time 1.008, loss:114.4525
g_step 2300, step 116, avg_time 1.013, loss:100.9467
g_step 2400, step 48, avg_time 0.996, loss:102.7721
g_step 2500, step 148, avg_time 1.024, loss:101.0559
>> valid entity prec:0.5725, rec:0.5531, f1:0.5626
>> valid relation prec:0.1651, rec:0.1292, f1:0.1450
>> valid relation with NER prec:0.1651, rec:0.1292, f1:0.1450
g_step 2600, step 80, avg_time 2.163, loss:98.5119
g_step 2700, step 12, avg_time 0.987, loss:97.2290
g_step 2800, step 112, avg_time 0.984, loss:98.0416
g_step 2900, step 44, avg_time 0.969, loss:86.9569
g_step 3000, step 144, avg_time 0.988, loss:93.1624
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5429, rec:0.5534, f1:0.5481
>> valid relation prec:0.1671, rec:0.1271, f1:0.1444
>> valid relation with NER prec:0.1671, rec:0.1271, f1:0.1444
g_step 3100, step 76, avg_time 2.173, loss:91.1386
g_step 3200, step 8, avg_time 0.968, loss:85.7972
g_step 3300, step 108, avg_time 0.974, loss:71.2816
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 01:37:33 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 01:37:33 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_01-37-33_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 01:37:34 - WARNING - datasets.builder -   Using custom data configuration default-f6d579b44fc06a56
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-f6d579b44fc06a56/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:00,  3.61 tables/s]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 01:37:34,823 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:37:34,824 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:37:34,825 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:37:34,826 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:37:34,836 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:37:34,840 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:37:34,840 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:37:34,840 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:37:34,840 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:37:34,840 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:37:34,840 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 01:37:35,039 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:37:38,086 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 01:37:38,091 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-f6d579b44fc06a56/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  3.17ba/s] 40%|████      | 2/5 [00:00<00:00,  4.09ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.49ba/s] 80%|████████  | 4/5 [00:00<00:00,  4.70ba/s]100%|██████████| 5/5 [00:00<00:00,  5.47ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.49ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.67ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.71ba/s]100%|██████████| 4/4 [00:00<00:00,  4.73ba/s]100%|██████████| 4/4 [00:00<00:00,  4.70ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  8.91ba/s] 60%|██████    | 3/5 [00:00<00:00, 10.52ba/s]100%|██████████| 5/5 [00:00<00:00, 13.07ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.25ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.55ba/s]100%|██████████| 4/4 [00:00<00:00, 12.16ba/s]
[INFO|trainer.py:414] 2023-08-29 01:37:40,919 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 01:37:40,935 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 01:37:40,936 >>   Num examples = 4021
[INFO|trainer.py:1149] 2023-08-29 01:37:40,936 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 01:37:40,936 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 01:37:40,936 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 01:37:40,936 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 01:37:40,936 >>   Total optimization steps = 315
  0%|          | 0/315 [00:00<?, ?it/s]  0%|          | 1/315 [00:00<02:00,  2.60it/s]  1%|          | 2/315 [00:00<01:42,  3.05it/s]  1%|          | 3/315 [00:00<01:36,  3.24it/s]  1%|▏         | 4/315 [00:01<01:33,  3.34it/s]  2%|▏         | 5/315 [00:01<01:31,  3.39it/s]  2%|▏         | 6/315 [00:01<01:30,  3.43it/s]  2%|▏         | 7/315 [00:02<01:29,  3.44it/s]  3%|▎         | 8/315 [00:02<01:28,  3.47it/s]  3%|▎         | 9/315 [00:02<01:28,  3.48it/s]  3%|▎         | 10/315 [00:02<01:27,  3.48it/s]  3%|▎         | 11/315 [00:03<01:27,  3.49it/s]  4%|▍         | 12/315 [00:03<01:26,  3.49it/s]  4%|▍         | 13/315 [00:03<01:26,  3.48it/s]  4%|▍         | 14/315 [00:04<01:26,  3.49it/s]  5%|▍         | 15/315 [00:04<01:25,  3.49it/s]  5%|▌         | 16/315 [00:04<01:25,  3.49it/s]  5%|▌         | 17/315 [00:04<01:25,  3.49it/s]  6%|▌         | 18/315 [00:05<01:25,  3.49it/s]  6%|▌         | 19/315 [00:05<01:24,  3.49it/s]  6%|▋         | 20/315 [00:05<01:24,  3.50it/s]  7%|▋         | 21/315 [00:06<01:24,  3.49it/s]  7%|▋         | 22/315 [00:06<01:23,  3.49it/s]  7%|▋         | 23/315 [00:06<01:23,  3.49it/s]  8%|▊         | 24/315 [00:06<01:23,  3.49it/s]  8%|▊         | 25/315 [00:07<01:23,  3.49it/s]  8%|▊         | 26/315 [00:07<01:22,  3.49it/s]  9%|▊         | 27/315 [00:07<01:22,  3.49it/s]  9%|▉         | 28/315 [00:08<01:22,  3.49it/s]  9%|▉         | 29/315 [00:08<01:22,  3.49it/s] 10%|▉         | 30/315 [00:08<01:21,  3.49it/s] 10%|▉         | 31/315 [00:08<01:21,  3.49it/s] 10%|█         | 32/315 [00:09<01:21,  3.49it/s] 10%|█         | 33/315 [00:09<01:20,  3.49it/s] 11%|█         | 34/315 [00:09<01:20,  3.49it/s] 11%|█         | 35/315 [00:10<01:20,  3.48it/s] 11%|█▏        | 36/315 [00:10<01:20,  3.48it/s] 12%|█▏        | 37/315 [00:10<01:19,  3.48it/s] 12%|█▏        | 38/315 [00:10<01:19,  3.48it/s] 12%|█▏        | 39/315 [00:11<01:19,  3.48it/s] 13%|█▎        | 40/315 [00:11<01:18,  3.49it/s] 13%|█▎        | 41/315 [00:11<01:18,  3.49it/s] 13%|█▎        | 42/315 [00:12<01:18,  3.49it/s] 14%|█▎        | 43/315 [00:12<01:17,  3.49it/s] 14%|█▍        | 44/315 [00:12<01:17,  3.49it/s] 14%|█▍        | 45/315 [00:12<01:17,  3.49it/s] 15%|█▍        | 46/315 [00:13<01:17,  3.48it/s] 15%|█▍        | 47/315 [00:13<01:16,  3.49it/s] 15%|█▌        | 48/315 [00:13<01:16,  3.49it/s] 16%|█▌        | 49/315 [00:14<01:16,  3.49it/s] 16%|█▌        | 50/315 [00:14<01:15,  3.49it/s] 16%|█▌        | 51/315 [00:14<01:15,  3.49it/s] 17%|█▋        | 52/315 [00:14<01:15,  3.49it/s] 17%|█▋        | 53/315 [00:15<01:15,  3.49it/s] 17%|█▋        | 54/315 [00:15<01:14,  3.49it/s] 17%|█▋        | 55/315 [00:15<01:14,  3.49it/s] 18%|█▊        | 56/315 [00:16<01:14,  3.49it/s] 18%|█▊        | 57/315 [00:16<01:14,  3.47it/s] 18%|█▊        | 58/315 [00:16<01:13,  3.48it/s] 19%|█▊        | 59/315 [00:17<01:13,  3.49it/s] 19%|█▉        | 60/315 [00:17<01:13,  3.49it/s] 19%|█▉        | 61/315 [00:17<01:12,  3.49it/s] 20%|█▉        | 62/315 [00:17<01:12,  3.49it/s] 20%|██        | 63/315 [00:18<01:08,  3.65it/s][INFO|trainer.py:2140] 2023-08-29 01:37:59,047 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:37:59,047 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 01:37:59,048 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.84it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.27it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.49it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.52it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.95it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.53it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.31it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.10it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.09it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.35it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.35it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.36it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.21it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.04it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.95it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.93it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.97it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.11it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.22it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.29it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.30it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.17it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.01it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.98it/s][A
 31%|███       | 132/431 [00:02<00:06, 45.01it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.94it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.07it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.19it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.31it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.32it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.09it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.00it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.91it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.98it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.73it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.10it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.25it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.34it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.32it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.04it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.04it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.92it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.97it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.93it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 45.10it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.24it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.31it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.28it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.20it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.08it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.88it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.91it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.94it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.06it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.14it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.21it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.25it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.20it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.10it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.98it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.91it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.84it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.10it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.14it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.31it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.28it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.06it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.01it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 44.89it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.92it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 45.03it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.99it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.01it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.26it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.28it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.25it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.13it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.01it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 44.93it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.95it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 44.96it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.10it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.25it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.27it/s][A
                                                 [A                                                
100%|██████████| 431/431 [00:09<00:00, 45.27it/s][A 20%|██        | 63/315 [00:27<01:08,  3.65it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:38:08,608 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-63
[INFO|configuration_utils.py:351] 2023-08-29 01:38:08,636 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-63/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:38:11,032 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-63/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:38:11,083 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-63/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:38:11,130 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-63/special_tokens_map.json
 20%|██        | 64/315 [00:36<23:45,  5.68s/it] 21%|██        | 65/315 [00:36<16:56,  4.06s/it] 21%|██        | 66/315 [00:36<12:10,  2.93s/it] 21%|██▏       | 67/315 [00:37<08:50,  2.14s/it] 22%|██▏       | 68/315 [00:37<06:31,  1.59s/it] 22%|██▏       | 69/315 [00:37<04:54,  1.20s/it] 22%|██▏       | 70/315 [00:38<03:46,  1.08it/s] 23%|██▎       | 71/315 [00:38<02:59,  1.36it/s] 23%|██▎       | 72/315 [00:38<02:25,  1.67it/s] 23%|██▎       | 73/315 [00:39<02:02,  1.98it/s] 23%|██▎       | 74/315 [00:39<01:46,  2.27it/s] 24%|██▍       | 75/315 [00:39<01:34,  2.54it/s] 24%|██▍       | 76/315 [00:39<01:27,  2.72it/s] 24%|██▍       | 77/315 [00:40<01:21,  2.91it/s] 25%|██▍       | 78/315 [00:40<01:17,  3.07it/s] 25%|██▌       | 79/315 [00:40<01:14,  3.18it/s] 25%|██▌       | 80/315 [00:41<01:11,  3.27it/s] 26%|██▌       | 81/315 [00:41<01:10,  3.33it/s] 26%|██▌       | 82/315 [00:41<01:08,  3.38it/s] 26%|██▋       | 83/315 [00:41<01:07,  3.42it/s] 27%|██▋       | 84/315 [00:42<01:07,  3.44it/s] 27%|██▋       | 85/315 [00:42<01:06,  3.45it/s] 27%|██▋       | 86/315 [00:42<01:06,  3.47it/s] 28%|██▊       | 87/315 [00:43<01:06,  3.45it/s] 28%|██▊       | 88/315 [00:43<01:05,  3.46it/s] 28%|██▊       | 89/315 [00:43<01:05,  3.47it/s] 29%|██▊       | 90/315 [00:43<01:04,  3.48it/s] 29%|██▉       | 91/315 [00:44<01:04,  3.48it/s] 29%|██▉       | 92/315 [00:44<01:03,  3.49it/s] 30%|██▉       | 93/315 [00:44<01:03,  3.49it/s] 30%|██▉       | 94/315 [00:45<01:03,  3.49it/s] 30%|███       | 95/315 [00:45<01:02,  3.49it/s] 30%|███       | 96/315 [00:45<01:02,  3.49it/s] 31%|███       | 97/315 [00:45<01:02,  3.49it/s] 31%|███       | 98/315 [00:46<01:02,  3.49it/s] 31%|███▏      | 99/315 [00:46<01:02,  3.48it/s] 32%|███▏      | 100/315 [00:46<01:01,  3.49it/s] 32%|███▏      | 101/315 [00:47<01:01,  3.49it/s] 32%|███▏      | 102/315 [00:47<01:01,  3.49it/s] 33%|███▎      | 103/315 [00:47<01:00,  3.49it/s] 33%|███▎      | 104/315 [00:47<01:00,  3.49it/s] 33%|███▎      | 105/315 [00:48<01:00,  3.50it/s] 34%|███▎      | 106/315 [00:48<00:59,  3.50it/s] 34%|███▍      | 107/315 [00:48<00:59,  3.50it/s] 34%|███▍      | 108/315 [00:49<00:59,  3.50it/s] 35%|███▍      | 109/315 [00:49<00:59,  3.46it/s] 35%|███▍      | 110/315 [00:49<00:59,  3.47it/s] 35%|███▌      | 111/315 [00:49<00:58,  3.47it/s] 36%|███▌      | 112/315 [00:50<00:58,  3.48it/s] 36%|███▌      | 113/315 [00:50<00:58,  3.48it/s] 36%|███▌      | 114/315 [00:50<00:57,  3.49it/s] 37%|███▋      | 115/315 [00:51<00:57,  3.49it/s] 37%|███▋      | 116/315 [00:51<00:56,  3.49it/s] 37%|███▋      | 117/315 [00:51<00:56,  3.48it/s] 37%|███▋      | 118/315 [00:51<00:56,  3.48it/s] 38%|███▊      | 119/315 [00:52<00:57,  3.38it/s] 38%|███▊      | 120/315 [00:52<00:57,  3.41it/s] 38%|███▊      | 121/315 [00:52<00:56,  3.43it/s] 39%|███▊      | 122/315 [00:53<00:55,  3.45it/s] 39%|███▉      | 123/315 [00:53<00:55,  3.46it/s] 39%|███▉      | 124/315 [00:53<00:55,  3.47it/s] 40%|███▉      | 125/315 [00:53<00:54,  3.47it/s] 40%|████      | 126/315 [00:54<00:51,  3.64it/s][INFO|trainer.py:2140] 2023-08-29 01:38:35,158 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:38:35,158 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 01:38:35,158 >>   Batch size = 8
{'eval_loss': 1.0289695262908936, 'eval_runtime': 9.552, 'eval_samples_per_second': 360.763, 'eval_steps_per_second': 45.121, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.97it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.44it/s][A
  4%|▍         | 18/431 [00:00<00:08, 47.09it/s][A
  5%|▌         | 23/431 [00:00<00:08, 46.24it/s][A
  6%|▋         | 28/431 [00:00<00:08, 45.93it/s][A
  8%|▊         | 33/431 [00:00<00:08, 45.44it/s][A
  9%|▉         | 38/431 [00:00<00:08, 45.19it/s][A
 10%|▉         | 43/431 [00:00<00:08, 45.11it/s][A
 11%|█         | 48/431 [00:01<00:08, 45.03it/s][A
 12%|█▏        | 53/431 [00:01<00:08, 45.21it/s][A
 13%|█▎        | 58/431 [00:01<00:08, 45.28it/s][A
 15%|█▍        | 63/431 [00:01<00:08, 45.18it/s][A
 16%|█▌        | 68/431 [00:01<00:08, 45.22it/s][A
 17%|█▋        | 73/431 [00:01<00:07, 45.17it/s][A
 18%|█▊        | 78/431 [00:01<00:07, 44.98it/s][A
 19%|█▉        | 83/431 [00:01<00:07, 44.96it/s][A
 20%|██        | 88/431 [00:01<00:07, 44.94it/s][A
 22%|██▏       | 93/431 [00:02<00:07, 45.04it/s][A
 23%|██▎       | 98/431 [00:02<00:07, 45.08it/s][A
 24%|██▍       | 103/431 [00:02<00:07, 45.20it/s][A
 25%|██▌       | 108/431 [00:02<00:07, 45.14it/s][A
 26%|██▌       | 113/431 [00:02<00:07, 45.14it/s][A
 27%|██▋       | 118/431 [00:02<00:06, 45.09it/s][A
 29%|██▊       | 123/431 [00:02<00:06, 44.96it/s][A
 30%|██▉       | 128/431 [00:02<00:06, 44.99it/s][A
 31%|███       | 133/431 [00:02<00:06, 44.87it/s][A
 32%|███▏      | 138/431 [00:03<00:06, 44.97it/s][A
 33%|███▎      | 143/431 [00:03<00:06, 45.13it/s][A
 34%|███▍      | 148/431 [00:03<00:06, 45.19it/s][A
 35%|███▌      | 153/431 [00:03<00:06, 45.20it/s][A
 37%|███▋      | 158/431 [00:03<00:06, 45.11it/s][A
 38%|███▊      | 163/431 [00:03<00:05, 45.07it/s][A
 39%|███▉      | 168/431 [00:03<00:05, 44.92it/s][A
 40%|████      | 173/431 [00:03<00:05, 44.91it/s][A
 41%|████▏     | 178/431 [00:03<00:05, 44.95it/s][A
 42%|████▏     | 183/431 [00:04<00:05, 45.04it/s][A
 44%|████▎     | 188/431 [00:04<00:05, 45.18it/s][A
 45%|████▍     | 193/431 [00:04<00:05, 45.19it/s][A
 46%|████▌     | 198/431 [00:04<00:05, 45.17it/s][A
 47%|████▋     | 203/431 [00:04<00:05, 45.15it/s][A
 48%|████▊     | 208/431 [00:04<00:04, 45.08it/s][A
 49%|████▉     | 213/431 [00:04<00:04, 44.79it/s][A
 51%|█████     | 218/431 [00:04<00:04, 44.92it/s][A
 52%|█████▏    | 223/431 [00:04<00:04, 44.97it/s][A
 53%|█████▎    | 228/431 [00:05<00:04, 45.10it/s][A
 54%|█████▍    | 233/431 [00:05<00:04, 45.15it/s][A
 55%|█████▌    | 238/431 [00:05<00:04, 45.23it/s][A
 56%|█████▋    | 243/431 [00:05<00:04, 45.17it/s][A
 58%|█████▊    | 248/431 [00:05<00:04, 45.07it/s][A
 59%|█████▊    | 253/431 [00:05<00:03, 45.08it/s][A
 60%|█████▉    | 258/431 [00:05<00:03, 44.94it/s][A
 61%|██████    | 263/431 [00:05<00:03, 44.94it/s][A
 62%|██████▏   | 268/431 [00:05<00:03, 44.89it/s][A
 63%|██████▎   | 273/431 [00:06<00:03, 45.08it/s][A
 65%|██████▍   | 278/431 [00:06<00:03, 45.07it/s][A
 66%|██████▌   | 283/431 [00:06<00:03, 45.23it/s][A
 67%|██████▋   | 288/431 [00:06<00:03, 45.02it/s][A
 68%|██████▊   | 293/431 [00:06<00:03, 44.92it/s][A
 69%|██████▉   | 298/431 [00:06<00:02, 45.06it/s][A
 70%|███████   | 303/431 [00:06<00:02, 44.97it/s][A
 71%|███████▏  | 308/431 [00:06<00:02, 44.94it/s][A
 73%|███████▎  | 313/431 [00:06<00:02, 44.97it/s][A
 74%|███████▍  | 318/431 [00:07<00:02, 45.06it/s][A
 75%|███████▍  | 323/431 [00:07<00:02, 45.10it/s][A
 76%|███████▌  | 328/431 [00:07<00:02, 45.18it/s][A
 77%|███████▋  | 333/431 [00:07<00:02, 44.99it/s][A
 78%|███████▊  | 338/431 [00:07<00:02, 45.08it/s][A
 80%|███████▉  | 343/431 [00:07<00:01, 45.03it/s][A
 81%|████████  | 348/431 [00:07<00:01, 44.91it/s][A
 82%|████████▏ | 353/431 [00:07<00:01, 44.93it/s][A
 83%|████████▎ | 358/431 [00:07<00:01, 44.93it/s][A
 84%|████████▍ | 363/431 [00:08<00:01, 45.13it/s][A
 85%|████████▌ | 368/431 [00:08<00:01, 45.10it/s][A
 87%|████████▋ | 373/431 [00:08<00:01, 45.12it/s][A
 88%|████████▊ | 378/431 [00:08<00:01, 45.09it/s][A
 89%|████████▉ | 383/431 [00:08<00:01, 45.07it/s][A
 90%|█████████ | 388/431 [00:08<00:00, 45.03it/s][A
 91%|█████████ | 393/431 [00:08<00:00, 44.91it/s][A
 92%|█████████▏| 398/431 [00:08<00:00, 44.88it/s][A
 94%|█████████▎| 403/431 [00:08<00:00, 44.93it/s][A
 95%|█████████▍| 408/431 [00:09<00:00, 45.10it/s][A
 96%|█████████▌| 413/431 [00:09<00:00, 45.03it/s][A
 97%|█████████▋| 418/431 [00:09<00:00, 45.08it/s][A
 98%|█████████▊| 423/431 [00:09<00:00, 45.09it/s][A
 99%|█████████▉| 428/431 [00:09<00:00, 45.09it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 45.09it/s][A 40%|████      | 126/315 [01:03<00:51,  3.64it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:38:44,755 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-126
[INFO|configuration_utils.py:351] 2023-08-29 01:38:44,785 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-126/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:38:46,778 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-126/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:38:46,835 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-126/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:38:46,848 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-126/special_tokens_map.json
 40%|████      | 127/315 [01:09<15:22,  4.91s/it] 41%|████      | 128/315 [01:10<10:58,  3.52s/it] 41%|████      | 129/315 [01:10<07:54,  2.55s/it] 41%|████▏     | 130/315 [01:10<05:46,  1.87s/it] 42%|████▏     | 131/315 [01:11<04:17,  1.40s/it] 42%|████▏     | 132/315 [01:11<03:15,  1.07s/it] 42%|████▏     | 133/315 [01:11<02:31,  1.20it/s] 43%|████▎     | 134/315 [01:11<02:01,  1.49it/s] 43%|████▎     | 135/315 [01:12<01:40,  1.80it/s] 43%|████▎     | 136/315 [01:12<01:25,  2.10it/s] 43%|████▎     | 137/315 [01:12<01:14,  2.37it/s] 44%|████▍     | 138/315 [01:13<01:07,  2.62it/s] 44%|████▍     | 139/315 [01:13<01:03,  2.79it/s] 44%|████▍     | 140/315 [01:13<00:59,  2.96it/s] 45%|████▍     | 141/315 [01:14<00:56,  3.11it/s] 45%|████▌     | 142/315 [01:14<00:53,  3.22it/s] 45%|████▌     | 143/315 [01:14<00:52,  3.29it/s] 46%|████▌     | 144/315 [01:14<00:51,  3.35it/s] 46%|████▌     | 145/315 [01:15<00:50,  3.39it/s] 46%|████▋     | 146/315 [01:15<00:49,  3.42it/s] 47%|████▋     | 147/315 [01:15<00:48,  3.44it/s] 47%|████▋     | 148/315 [01:16<00:48,  3.46it/s] 47%|████▋     | 149/315 [01:16<00:47,  3.47it/s] 48%|████▊     | 150/315 [01:16<00:47,  3.47it/s] 48%|████▊     | 151/315 [01:16<00:47,  3.47it/s] 48%|████▊     | 152/315 [01:17<00:46,  3.48it/s] 49%|████▊     | 153/315 [01:17<00:46,  3.48it/s] 49%|████▉     | 154/315 [01:17<00:46,  3.49it/s] 49%|████▉     | 155/315 [01:18<00:45,  3.49it/s] 50%|████▉     | 156/315 [01:18<00:45,  3.49it/s] 50%|████▉     | 157/315 [01:18<00:45,  3.49it/s] 50%|█████     | 158/315 [01:18<00:45,  3.49it/s] 50%|█████     | 159/315 [01:19<00:44,  3.48it/s] 51%|█████     | 160/315 [01:19<00:44,  3.49it/s] 51%|█████     | 161/315 [01:19<00:44,  3.48it/s] 51%|█████▏    | 162/315 [01:20<00:43,  3.49it/s] 52%|█████▏    | 163/315 [01:20<00:43,  3.49it/s] 52%|█████▏    | 164/315 [01:20<00:43,  3.49it/s] 52%|█████▏    | 165/315 [01:20<00:42,  3.49it/s] 53%|█████▎    | 166/315 [01:21<00:42,  3.49it/s] 53%|█████▎    | 167/315 [01:21<00:42,  3.49it/s] 53%|█████▎    | 168/315 [01:21<00:42,  3.49it/s] 54%|█████▎    | 169/315 [01:22<00:41,  3.49it/s] 54%|█████▍    | 170/315 [01:22<00:41,  3.49it/s] 54%|█████▍    | 171/315 [01:22<00:41,  3.49it/s] 55%|█████▍    | 172/315 [01:22<00:41,  3.48it/s] 55%|█████▍    | 173/315 [01:23<00:40,  3.49it/s] 55%|█████▌    | 174/315 [01:23<00:40,  3.49it/s] 56%|█████▌    | 175/315 [01:23<00:40,  3.49it/s] 56%|█████▌    | 176/315 [01:24<00:39,  3.49it/s] 56%|█████▌    | 177/315 [01:24<00:39,  3.49it/s] 57%|█████▋    | 178/315 [01:24<00:39,  3.49it/s] 57%|█████▋    | 179/315 [01:24<00:39,  3.49it/s] 57%|█████▋    | 180/315 [01:25<00:38,  3.49it/s] 57%|█████▋    | 181/315 [01:25<00:38,  3.48it/s] 58%|█████▊    | 182/315 [01:25<00:38,  3.48it/s] 58%|█████▊    | 183/315 [01:26<00:38,  3.47it/s] 58%|█████▊    | 184/315 [01:26<00:37,  3.48it/s] 59%|█████▊    | 185/315 [01:26<00:37,  3.48it/s] 59%|█████▉    | 186/315 [01:26<00:37,  3.48it/s] 59%|█████▉    | 187/315 [01:27<00:36,  3.48it/s] 60%|█████▉    | 188/315 [01:27<00:36,  3.49it/s] 60%|██████    | 189/315 [01:27<00:34,  3.65it/s][INFO|trainer.py:2140] 2023-08-29 01:39:08,671 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:39:08,672 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 01:39:08,672 >>   Batch size = 8
{'eval_loss': 1.029698133468628, 'eval_runtime': 9.5639, 'eval_samples_per_second': 360.315, 'eval_steps_per_second': 45.065, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.60it/s][A
  3%|▎         | 12/431 [00:00<00:08, 48.99it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.34it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.24it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.96it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.67it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.54it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.32it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.31it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.24it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.36it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.23it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.14it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.20it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.22it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 45.21it/s][A
 20%|██        | 87/431 [00:01<00:07, 45.13it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 45.19it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.25it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.34it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.24it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.25it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.12it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.21it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 45.22it/s][A
 31%|███       | 132/431 [00:02<00:06, 45.17it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 45.17it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.21it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.29it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.18it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.26it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.27it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.11it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 45.21it/s][A
 41%|████      | 177/431 [00:03<00:05, 45.28it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 45.23it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.26it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.10it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.23it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.24it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.28it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.29it/s][A
 50%|█████     | 217/431 [00:04<00:04, 45.23it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 45.07it/s][A
 53%|█████▎    | 227/431 [00:04<00:04, 45.20it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 45.20it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.13it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.23it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.24it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.20it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.21it/s][A
 61%|██████    | 262/431 [00:05<00:03, 45.25it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 45.20it/s][A
 63%|██████▎   | 272/431 [00:05<00:03, 45.13it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.22it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.14it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.25it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.28it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.22it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.24it/s][A
 71%|███████   | 307/431 [00:06<00:02, 45.25it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 45.19it/s][A
 74%|███████▎  | 317/431 [00:06<00:02, 45.22it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.13it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.09it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.21it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.28it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.20it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.23it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 45.25it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 45.22it/s][A
 84%|████████▍ | 362/431 [00:07<00:01, 45.18it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 45.12it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.05it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.22it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.30it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.22it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.21it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.28it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 45.15it/s][A
 94%|█████████▍| 407/431 [00:08<00:00, 45.21it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.23it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.16it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.23it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.18it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 45.18it/s][A 60%|██████    | 189/315 [01:37<00:34,  3.65it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:39:18,218 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-189
[INFO|configuration_utils.py:351] 2023-08-29 01:39:18,238 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-189/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:39:20,014 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-189/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:39:20,031 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-189/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:39:20,039 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-189/special_tokens_map.json
 60%|██████    | 190/315 [01:42<09:42,  4.66s/it] 61%|██████    | 191/315 [01:42<06:55,  3.35s/it] 61%|██████    | 192/315 [01:43<04:59,  2.43s/it] 61%|██████▏   | 193/315 [01:43<03:38,  1.79s/it] 62%|██████▏   | 194/315 [01:43<02:42,  1.34s/it] 62%|██████▏   | 195/315 [01:44<02:03,  1.03s/it] 62%|██████▏   | 196/315 [01:44<01:35,  1.24it/s] 63%|██████▎   | 197/315 [01:44<01:16,  1.54it/s] 63%|██████▎   | 198/315 [01:44<01:03,  1.84it/s] 63%|██████▎   | 199/315 [01:45<00:54,  2.14it/s] 63%|██████▎   | 200/315 [01:45<00:47,  2.41it/s] 64%|██████▍   | 201/315 [01:45<00:43,  2.65it/s] 64%|██████▍   | 202/315 [01:46<00:39,  2.83it/s] 64%|██████▍   | 203/315 [01:46<00:37,  2.99it/s] 65%|██████▍   | 204/315 [01:46<00:35,  3.11it/s] 65%|██████▌   | 205/315 [01:46<00:34,  3.20it/s] 65%|██████▌   | 206/315 [01:47<00:33,  3.26it/s] 66%|██████▌   | 207/315 [01:47<00:32,  3.31it/s] 66%|██████▌   | 208/315 [01:47<00:31,  3.35it/s] 66%|██████▋   | 209/315 [01:48<00:31,  3.37it/s] 67%|██████▋   | 210/315 [01:48<00:30,  3.39it/s] 67%|██████▋   | 211/315 [01:48<00:30,  3.40it/s] 67%|██████▋   | 212/315 [01:49<00:30,  3.41it/s] 68%|██████▊   | 213/315 [01:49<00:29,  3.40it/s] 68%|██████▊   | 214/315 [01:49<00:29,  3.41it/s] 68%|██████▊   | 215/315 [01:49<00:29,  3.42it/s] 69%|██████▊   | 216/315 [01:50<00:28,  3.42it/s] 69%|██████▉   | 217/315 [01:50<00:28,  3.42it/s] 69%|██████▉   | 218/315 [01:50<00:28,  3.42it/s] 70%|██████▉   | 219/315 [01:51<00:28,  3.42it/s] 70%|██████▉   | 220/315 [01:51<00:27,  3.43it/s] 70%|███████   | 221/315 [01:51<00:27,  3.42it/s] 70%|███████   | 222/315 [01:51<00:27,  3.42it/s] 71%|███████   | 223/315 [01:52<00:27,  3.34it/s] 71%|███████   | 224/315 [01:52<00:27,  3.36it/s] 71%|███████▏  | 225/315 [01:52<00:26,  3.37it/s] 72%|███████▏  | 226/315 [01:53<00:26,  3.39it/s] 72%|███████▏  | 227/315 [01:53<00:25,  3.40it/s] 72%|███████▏  | 228/315 [01:53<00:25,  3.41it/s] 73%|███████▎  | 229/315 [01:54<00:25,  3.41it/s] 73%|███████▎  | 230/315 [01:54<00:24,  3.42it/s] 73%|███████▎  | 231/315 [01:54<00:24,  3.42it/s] 74%|███████▎  | 232/315 [01:54<00:24,  3.42it/s] 74%|███████▍  | 233/315 [01:55<00:23,  3.42it/s] 74%|███████▍  | 234/315 [01:55<00:23,  3.42it/s] 75%|███████▍  | 235/315 [01:55<00:23,  3.40it/s] 75%|███████▍  | 236/315 [01:56<00:23,  3.41it/s] 75%|███████▌  | 237/315 [01:56<00:22,  3.41it/s] 76%|███████▌  | 238/315 [01:56<00:22,  3.42it/s] 76%|███████▌  | 239/315 [01:56<00:22,  3.44it/s] 76%|███████▌  | 240/315 [01:57<00:21,  3.45it/s] 77%|███████▋  | 241/315 [01:57<00:21,  3.46it/s] 77%|███████▋  | 242/315 [01:57<00:21,  3.46it/s] 77%|███████▋  | 243/315 [01:58<00:20,  3.47it/s] 77%|███████▋  | 244/315 [01:58<00:20,  3.47it/s] 78%|███████▊  | 245/315 [01:58<00:20,  3.47it/s] 78%|███████▊  | 246/315 [01:58<00:19,  3.47it/s] 78%|███████▊  | 247/315 [01:59<00:19,  3.47it/s] 79%|███████▊  | 248/315 [01:59<00:19,  3.47it/s] 79%|███████▉  | 249/315 [01:59<00:18,  3.48it/s] 79%|███████▉  | 250/315 [02:00<00:18,  3.48it/s] 80%|███████▉  | 251/315 [02:00<00:18,  3.48it/s] 80%|████████  | 252/315 [02:00<00:17,  3.64it/s][INFO|trainer.py:2140] 2023-08-29 01:39:41,588 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:39:41,589 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 01:39:41,589 >>   Batch size = 8
{'eval_loss': 1.0410571098327637, 'eval_runtime': 9.5314, 'eval_samples_per_second': 361.541, 'eval_steps_per_second': 45.219, 'epoch': 3.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.96it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.43it/s][A
  4%|▍         | 18/431 [00:00<00:08, 47.36it/s][A
  5%|▌         | 23/431 [00:00<00:08, 46.27it/s][A
  6%|▋         | 28/431 [00:00<00:08, 45.91it/s][A
  8%|▊         | 33/431 [00:00<00:08, 45.44it/s][A
  9%|▉         | 38/431 [00:00<00:08, 45.36it/s][A
 10%|▉         | 43/431 [00:00<00:08, 45.32it/s][A
 11%|█         | 48/431 [00:01<00:08, 45.34it/s][A
 12%|█▏        | 53/431 [00:01<00:08, 45.41it/s][A
 13%|█▎        | 58/431 [00:01<00:08, 45.48it/s][A
 15%|█▍        | 63/431 [00:01<00:08, 45.43it/s][A
 16%|█▌        | 68/431 [00:01<00:08, 45.31it/s][A
 17%|█▋        | 73/431 [00:01<00:07, 45.12it/s][A
 18%|█▊        | 78/431 [00:01<00:07, 45.11it/s][A
 19%|█▉        | 83/431 [00:01<00:07, 45.10it/s][A
 20%|██        | 88/431 [00:01<00:07, 45.15it/s][A
 22%|██▏       | 93/431 [00:02<00:07, 45.10it/s][A
 23%|██▎       | 98/431 [00:02<00:07, 45.26it/s][A
 24%|██▍       | 103/431 [00:02<00:07, 45.33it/s][A
 25%|██▌       | 108/431 [00:02<00:07, 45.35it/s][A
 26%|██▌       | 113/431 [00:02<00:07, 45.26it/s][A
 27%|██▋       | 118/431 [00:02<00:06, 45.11it/s][A
 29%|██▊       | 123/431 [00:02<00:06, 45.11it/s][A
 30%|██▉       | 128/431 [00:02<00:06, 45.07it/s][A
 31%|███       | 133/431 [00:02<00:06, 45.10it/s][A
 32%|███▏      | 138/431 [00:03<00:06, 45.10it/s][A
 33%|███▎      | 143/431 [00:03<00:06, 45.22it/s][A
 34%|███▍      | 148/431 [00:03<00:06, 45.32it/s][A
 35%|███▌      | 153/431 [00:03<00:06, 45.29it/s][A
 37%|███▋      | 158/431 [00:03<00:06, 45.27it/s][A
 38%|███▊      | 163/431 [00:03<00:05, 45.16it/s][A
 39%|███▉      | 168/431 [00:03<00:05, 45.03it/s][A
 40%|████      | 173/431 [00:03<00:05, 45.13it/s][A
 41%|████▏     | 178/431 [00:03<00:05, 45.09it/s][A
 42%|████▏     | 183/431 [00:04<00:05, 45.08it/s][A
 44%|████▎     | 188/431 [00:04<00:05, 45.15it/s][A
 45%|████▍     | 193/431 [00:04<00:05, 45.31it/s][A
 46%|████▌     | 198/431 [00:04<00:05, 45.35it/s][A
 47%|████▋     | 203/431 [00:04<00:05, 45.32it/s][A
 48%|████▊     | 208/431 [00:04<00:04, 45.25it/s][A
 49%|████▉     | 213/431 [00:04<00:04, 45.21it/s][A
 51%|█████     | 218/431 [00:04<00:04, 45.05it/s][A
 52%|█████▏    | 223/431 [00:04<00:04, 45.13it/s][A
 53%|█████▎    | 228/431 [00:05<00:04, 45.14it/s][A
 54%|█████▍    | 233/431 [00:05<00:04, 45.14it/s][A
 55%|█████▌    | 238/431 [00:05<00:04, 45.26it/s][A
 56%|█████▋    | 243/431 [00:05<00:04, 45.34it/s][A
 58%|█████▊    | 248/431 [00:05<00:04, 45.31it/s][A
 59%|█████▊    | 253/431 [00:05<00:03, 45.30it/s][A
 60%|█████▉    | 258/431 [00:05<00:03, 45.15it/s][A
 61%|██████    | 263/431 [00:05<00:03, 45.12it/s][A
 62%|██████▏   | 268/431 [00:05<00:03, 45.09it/s][A
 63%|██████▎   | 273/431 [00:06<00:03, 45.17it/s][A
 65%|██████▍   | 278/431 [00:06<00:03, 45.19it/s][A
 66%|██████▌   | 283/431 [00:06<00:03, 45.18it/s][A
 67%|██████▋   | 288/431 [00:06<00:03, 45.20it/s][A
 68%|██████▊   | 293/431 [00:06<00:03, 45.25it/s][A
 69%|██████▉   | 298/431 [00:06<00:02, 45.31it/s][A
 70%|███████   | 303/431 [00:06<00:02, 45.24it/s][A
 71%|███████▏  | 308/431 [00:06<00:02, 45.17it/s][A
 73%|███████▎  | 313/431 [00:06<00:02, 45.15it/s][A
 74%|███████▍  | 318/431 [00:07<00:02, 45.10it/s][A
 75%|███████▍  | 323/431 [00:07<00:02, 45.22it/s][A
 76%|███████▌  | 328/431 [00:07<00:02, 45.22it/s][A
 77%|███████▋  | 333/431 [00:07<00:02, 45.24it/s][A
 78%|███████▊  | 338/431 [00:07<00:02, 45.28it/s][A
 80%|███████▉  | 343/431 [00:07<00:01, 45.24it/s][A
 81%|████████  | 348/431 [00:07<00:01, 45.23it/s][A
 82%|████████▏ | 353/431 [00:07<00:01, 45.27it/s][A
 83%|████████▎ | 358/431 [00:07<00:01, 45.19it/s][A
 84%|████████▍ | 363/431 [00:08<00:01, 45.21it/s][A
 85%|████████▌ | 368/431 [00:08<00:01, 45.17it/s][A
 87%|████████▋ | 373/431 [00:08<00:01, 45.17it/s][A
 88%|████████▊ | 378/431 [00:08<00:01, 45.08it/s][A
 89%|████████▉ | 383/431 [00:08<00:01, 45.23it/s][A
 90%|█████████ | 388/431 [00:08<00:00, 45.23it/s][A
 91%|█████████ | 393/431 [00:08<00:00, 45.19it/s][A
 92%|█████████▏| 398/431 [00:08<00:00, 45.26it/s][A
 94%|█████████▎| 403/431 [00:08<00:00, 45.30it/s][A
 95%|█████████▍| 408/431 [00:09<00:00, 45.30it/s][A
 96%|█████████▌| 413/431 [00:09<00:00, 45.22it/s][A
 97%|█████████▋| 418/431 [00:09<00:00, 45.06it/s][A
 98%|█████████▊| 423/431 [00:09<00:00, 45.09it/s][A
 99%|█████████▉| 428/431 [00:09<00:00, 45.16it/s][A                                                 
                                                 [A 80%|████████  | 252/315 [02:10<00:17,  3.64it/s]
100%|██████████| 431/431 [00:09<00:00, 45.16it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:39:51,141 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-252
[INFO|configuration_utils.py:351] 2023-08-29 01:39:51,175 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-252/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:39:52,813 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-252/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:39:52,830 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-252/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:39:52,850 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-252/special_tokens_map.json
 80%|████████  | 253/315 [02:15<04:52,  4.72s/it] 81%|████████  | 254/315 [02:16<03:27,  3.39s/it] 81%|████████  | 255/315 [02:16<02:27,  2.46s/it] 81%|████████▏ | 256/315 [02:16<01:46,  1.81s/it] 82%|████████▏ | 257/315 [02:16<01:18,  1.36s/it] 82%|████████▏ | 258/315 [02:17<00:59,  1.04s/it] 82%|████████▏ | 259/315 [02:17<00:45,  1.23it/s] 83%|████████▎ | 260/315 [02:17<00:36,  1.52it/s] 83%|████████▎ | 261/315 [02:18<00:29,  1.83it/s] 83%|████████▎ | 262/315 [02:18<00:24,  2.13it/s] 83%|████████▎ | 263/315 [02:18<00:21,  2.40it/s] 84%|████████▍ | 264/315 [02:18<00:19,  2.64it/s] 84%|████████▍ | 265/315 [02:19<00:17,  2.83it/s] 84%|████████▍ | 266/315 [02:19<00:16,  2.99it/s] 85%|████████▍ | 267/315 [02:19<00:15,  3.11it/s] 85%|████████▌ | 268/315 [02:20<00:14,  3.20it/s] 85%|████████▌ | 269/315 [02:20<00:14,  3.27it/s] 86%|████████▌ | 270/315 [02:20<00:13,  3.31it/s] 86%|████████▌ | 271/315 [02:20<00:13,  3.35it/s] 86%|████████▋ | 272/315 [02:21<00:12,  3.38it/s] 87%|████████▋ | 273/315 [02:21<00:12,  3.40it/s] 87%|████████▋ | 274/315 [02:21<00:12,  3.41it/s] 87%|████████▋ | 275/315 [02:22<00:11,  3.42it/s] 88%|████████▊ | 276/315 [02:22<00:11,  3.41it/s] 88%|████████▊ | 277/315 [02:22<00:11,  3.41it/s] 88%|████████▊ | 278/315 [02:23<00:10,  3.42it/s] 89%|████████▊ | 279/315 [02:23<00:10,  3.42it/s] 89%|████████▉ | 280/315 [02:23<00:10,  3.43it/s] 89%|████████▉ | 281/315 [02:23<00:09,  3.43it/s] 90%|████████▉ | 282/315 [02:24<00:09,  3.43it/s] 90%|████████▉ | 283/315 [02:24<00:09,  3.44it/s] 90%|█████████ | 284/315 [02:24<00:09,  3.44it/s] 90%|█████████ | 285/315 [02:25<00:08,  3.44it/s] 91%|█████████ | 286/315 [02:25<00:08,  3.43it/s] 91%|█████████ | 287/315 [02:25<00:08,  3.42it/s] 91%|█████████▏| 288/315 [02:25<00:07,  3.43it/s] 92%|█████████▏| 289/315 [02:26<00:07,  3.43it/s] 92%|█████████▏| 290/315 [02:26<00:07,  3.43it/s] 92%|█████████▏| 291/315 [02:26<00:07,  3.42it/s] 93%|█████████▎| 292/315 [02:27<00:06,  3.43it/s] 93%|█████████▎| 293/315 [02:27<00:06,  3.43it/s] 93%|█████████▎| 294/315 [02:27<00:06,  3.43it/s] 94%|█████████▎| 295/315 [02:27<00:05,  3.43it/s] 94%|█████████▍| 296/315 [02:28<00:05,  3.43it/s] 94%|█████████▍| 297/315 [02:28<00:05,  3.43it/s] 95%|█████████▍| 298/315 [02:28<00:04,  3.42it/s] 95%|█████████▍| 299/315 [02:29<00:04,  3.42it/s] 95%|█████████▌| 300/315 [02:29<00:04,  3.43it/s] 96%|█████████▌| 301/315 [02:29<00:04,  3.43it/s] 96%|█████████▌| 302/315 [02:30<00:03,  3.42it/s] 96%|█████████▌| 303/315 [02:30<00:03,  3.42it/s] 97%|█████████▋| 304/315 [02:30<00:03,  3.42it/s] 97%|█████████▋| 305/315 [02:30<00:02,  3.42it/s] 97%|█████████▋| 306/315 [02:31<00:02,  3.42it/s] 97%|█████████▋| 307/315 [02:31<00:02,  3.42it/s] 98%|█████████▊| 308/315 [02:31<00:02,  3.43it/s] 98%|█████████▊| 309/315 [02:32<00:01,  3.42it/s] 98%|█████████▊| 310/315 [02:32<00:01,  3.42it/s] 99%|█████████▊| 311/315 [02:32<00:01,  3.43it/s] 99%|█████████▉| 312/315 [02:32<00:00,  3.43it/s] 99%|█████████▉| 313/315 [02:33<00:00,  3.43it/s]100%|█████████▉| 314/315 [02:33<00:00,  3.43it/s]100%|██████████| 315/315 [02:33<00:00,  3.59it/s][INFO|trainer.py:2140] 2023-08-29 01:40:14,725 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:40:14,725 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 01:40:14,725 >>   Batch size = 8
{'eval_loss': 1.0506213903427124, 'eval_runtime': 9.532, 'eval_samples_per_second': 361.52, 'eval_steps_per_second': 45.216, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.43it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.05it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.44it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.58it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.93it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.55it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.27it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.09it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.15it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.22it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.36it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.27it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.39it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.29it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.12it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 45.00it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.92it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.96it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.12it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.17it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.30it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.42it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.34it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.14it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.97it/s][A
 31%|███       | 132/431 [00:02<00:06, 45.05it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 45.09it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.05it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.19it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.16it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.36it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.27it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.22it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 45.07it/s][A
 41%|████      | 177/431 [00:03<00:05, 45.06it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.99it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.10it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.19it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.30it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.38it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.30it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.27it/s][A
 50%|█████     | 217/431 [00:04<00:04, 45.03it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 45.11it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 45.08it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 45.11it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.19it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.28it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.36it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.34it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.27it/s][A
 61%|██████    | 262/431 [00:05<00:03, 45.13it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 45.10it/s][A
 63%|██████▎   | 272/431 [00:05<00:03, 45.08it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.05it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.13it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.24it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.33it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.24it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.18it/s][A
 71%|███████   | 307/431 [00:06<00:02, 45.25it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.97it/s][A
 74%|███████▎  | 317/431 [00:06<00:02, 45.07it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.07it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.22it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.23it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.30it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.16it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.28it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 44.80it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 45.17it/s][A
 84%|████████▍ | 362/431 [00:07<00:01, 45.20it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 45.27it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.24it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.27it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.17it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.18it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.27it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.26it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 45.07it/s][A
 94%|█████████▍| 407/431 [00:08<00:00, 45.15it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.20it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.24it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.16it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.15it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 45.15it/s][A100%|██████████| 315/315 [02:43<00:00,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 01:40:24,277 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-315
[INFO|configuration_utils.py:351] 2023-08-29 01:40:24,331 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-315/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:40:26,075 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-315/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:40:26,095 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-315/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:40:26,102 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-315/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 01:40:30,268 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 01:40:30,271 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-63 (score: 1.0289695262908936).
                                                 100%|██████████| 315/315 [02:51<00:00,  3.59it/s]100%|██████████| 315/315 [02:51<00:00,  1.84it/s]
[INFO|trainer.py:1894] 2023-08-29 01:40:32,038 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-29 01:40:32,069 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 01:40:34,152 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 01:40:34,178 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 01:40:34,190 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:40:34,418 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,419 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,419 >>   train_loss               =     0.4649
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,419 >>   train_runtime            = 0:02:51.09
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,419 >>   train_samples            =       4021
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,419 >>   train_samples_per_second =    117.508
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:34,419 >>   train_steps_per_second   =      1.841
{'eval_loss': 1.055071234703064, 'eval_runtime': 9.5398, 'eval_samples_per_second': 361.222, 'eval_steps_per_second': 45.179, 'epoch': 5.0}
{'train_runtime': 171.0942, 'train_samples_per_second': 117.508, 'train_steps_per_second': 1.841, 'train_loss': 0.46486094641307046, 'epoch': 5.0}
08/29/2023 01:40:34 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 01:40:34,480 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 01:40:34,480 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 01:40:34,480 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 55.96it/s]  3%|▎         | 12/431 [00:00<00:08, 48.99it/s]  4%|▍         | 17/431 [00:00<00:08, 47.39it/s]  5%|▌         | 22/431 [00:00<00:08, 46.92it/s]  6%|▋         | 27/431 [00:00<00:08, 46.41it/s]  7%|▋         | 32/431 [00:00<00:08, 46.09it/s]  9%|▊         | 37/431 [00:00<00:08, 46.03it/s] 10%|▉         | 42/431 [00:00<00:08, 45.79it/s] 11%|█         | 47/431 [00:01<00:08, 45.25it/s] 12%|█▏        | 52/431 [00:01<00:08, 45.05it/s] 13%|█▎        | 57/431 [00:01<00:08, 45.04it/s] 14%|█▍        | 62/431 [00:01<00:08, 45.06it/s] 16%|█▌        | 67/431 [00:01<00:08, 45.25it/s] 17%|█▋        | 72/431 [00:01<00:07, 45.29it/s] 18%|█▊        | 77/431 [00:01<00:07, 45.25it/s] 19%|█▉        | 82/431 [00:01<00:07, 45.32it/s] 20%|██        | 87/431 [00:01<00:07, 45.39it/s] 21%|██▏       | 92/431 [00:02<00:07, 45.23it/s] 23%|██▎       | 97/431 [00:02<00:07, 44.89it/s] 24%|██▎       | 102/431 [00:02<00:07, 44.93it/s] 25%|██▍       | 107/431 [00:02<00:07, 45.00it/s] 26%|██▌       | 112/431 [00:02<00:07, 45.14it/s] 27%|██▋       | 117/431 [00:02<00:06, 45.02it/s] 28%|██▊       | 122/431 [00:02<00:06, 45.25it/s] 29%|██▉       | 127/431 [00:02<00:06, 45.23it/s] 31%|███       | 132/431 [00:02<00:06, 45.36it/s] 32%|███▏      | 137/431 [00:03<00:06, 45.23it/s] 33%|███▎      | 142/431 [00:03<00:06, 45.08it/s] 34%|███▍      | 147/431 [00:03<00:06, 45.09it/s] 35%|███▌      | 152/431 [00:03<00:06, 45.06it/s] 36%|███▋      | 157/431 [00:03<00:06, 45.03it/s] 38%|███▊      | 162/431 [00:03<00:05, 45.27it/s] 39%|███▊      | 167/431 [00:03<00:05, 45.20it/s] 40%|███▉      | 172/431 [00:03<00:05, 45.20it/s] 41%|████      | 177/431 [00:03<00:05, 45.25it/s] 42%|████▏     | 182/431 [00:04<00:05, 45.17it/s] 43%|████▎     | 187/431 [00:04<00:05, 44.98it/s] 45%|████▍     | 192/431 [00:04<00:05, 44.94it/s] 46%|████▌     | 197/431 [00:04<00:05, 45.04it/s] 47%|████▋     | 202/431 [00:04<00:05, 45.19it/s] 48%|████▊     | 207/431 [00:04<00:04, 45.20it/s] 49%|████▉     | 212/431 [00:04<00:04, 45.14it/s] 50%|█████     | 217/431 [00:04<00:04, 45.10it/s] 52%|█████▏    | 222/431 [00:04<00:04, 45.22it/s] 53%|█████▎    | 227/431 [00:04<00:04, 45.12it/s] 54%|█████▍    | 232/431 [00:05<00:04, 44.87it/s] 55%|█████▍    | 237/431 [00:05<00:04, 45.03it/s] 56%|█████▌    | 242/431 [00:05<00:04, 45.11it/s] 57%|█████▋    | 247/431 [00:05<00:04, 45.21it/s] 58%|█████▊    | 252/431 [00:05<00:03, 45.25it/s] 60%|█████▉    | 257/431 [00:05<00:03, 45.21it/s] 61%|██████    | 262/431 [00:05<00:03, 45.26it/s] 62%|██████▏   | 267/431 [00:05<00:03, 45.28it/s] 63%|██████▎   | 272/431 [00:05<00:03, 45.12it/s] 64%|██████▍   | 277/431 [00:06<00:03, 45.03it/s] 65%|██████▌   | 282/431 [00:06<00:03, 44.93it/s] 67%|██████▋   | 287/431 [00:06<00:03, 45.04it/s] 68%|██████▊   | 292/431 [00:06<00:03, 45.14it/s] 69%|██████▉   | 297/431 [00:06<00:02, 45.11it/s] 70%|███████   | 302/431 [00:06<00:02, 45.18it/s] 71%|███████   | 307/431 [00:06<00:02, 45.25it/s] 72%|███████▏  | 312/431 [00:06<00:02, 45.23it/s] 74%|███████▎  | 317/431 [00:06<00:02, 45.17it/s] 75%|███████▍  | 322/431 [00:07<00:02, 44.97it/s] 76%|███████▌  | 327/431 [00:07<00:02, 45.02it/s] 77%|███████▋  | 332/431 [00:07<00:02, 45.07it/s] 78%|███████▊  | 337/431 [00:07<00:02, 45.07it/s] 79%|███████▉  | 342/431 [00:07<00:01, 45.19it/s] 81%|████████  | 347/431 [00:07<00:01, 45.21it/s] 82%|████████▏ | 352/431 [00:07<00:01, 45.29it/s] 83%|████████▎ | 357/431 [00:07<00:01, 45.23it/s] 84%|████████▍ | 362/431 [00:07<00:01, 45.15it/s] 85%|████████▌ | 367/431 [00:08<00:01, 45.07it/s] 86%|████████▋ | 372/431 [00:08<00:01, 45.06it/s] 87%|████████▋ | 377/431 [00:08<00:01, 45.01it/s] 89%|████████▊ | 382/431 [00:08<00:01, 45.10it/s] 90%|████████▉ | 387/431 [00:08<00:00, 45.17it/s] 91%|█████████ | 392/431 [00:08<00:00, 45.20it/s] 92%|█████████▏| 397/431 [00:08<00:00, 45.17it/s] 93%|█████████▎| 402/431 [00:08<00:00, 45.24it/s] 94%|█████████▍| 407/431 [00:08<00:00, 45.15it/s] 96%|█████████▌| 412/431 [00:09<00:00, 45.04it/s] 97%|█████████▋| 417/431 [00:09<00:00, 44.93it/s] 98%|█████████▊| 422/431 [00:09<00:00, 44.98it/s] 99%|█████████▉| 427/431 [00:09<00:00, 45.08it/s]100%|██████████| 431/431 [00:09<00:00, 45.23it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 01:40:44,028 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:44,028 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:44,028 >>   eval_loss               =      1.029
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:44,028 >>   eval_runtime            = 0:00:09.54
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:44,028 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:44,028 >>   eval_samples_per_second =    360.942
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:44,028 >>   eval_steps_per_second   =     45.144
[INFO|trainer_pt_utils.py:913] 2023-08-29 01:40:44,028 >>   perplexity              =     2.7982
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:50,985 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:50,989 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:50,989 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:50,989 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:50,989 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:40:51,606 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:40:51,607 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:40:52,195 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:40:53,270 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:40:53,271 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:56,100 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:56,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:56,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:56,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:40:56,104 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:40:56,755 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:40:56,756 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:40:57,331 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:40:57,505 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:40:57,505 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-189
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-252
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-126
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-63
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/checkpoint-315
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.39it/s]Extractor Predicting: 2it [00:01,  1.49it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.55it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.54it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.58it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.52it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.57it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:15,  1.59it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.52it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:21,  1.60it/s]Extractor Predicting: 35it [00:22,  1.62it/s]Extractor Predicting: 36it [00:23,  1.64it/s]Extractor Predicting: 37it [00:23,  1.65it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:24,  1.64it/s]Extractor Predicting: 40it [00:25,  1.64it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:26,  1.58it/s]Extractor Predicting: 43it [00:27,  1.57it/s]Extractor Predicting: 44it [00:28,  1.59it/s]Extractor Predicting: 45it [00:28,  1.60it/s]Extractor Predicting: 46it [00:29,  1.62it/s]Extractor Predicting: 47it [00:29,  1.67it/s]Extractor Predicting: 48it [00:30,  1.68it/s]Extractor Predicting: 49it [00:31,  1.68it/s]Extractor Predicting: 50it [00:31,  1.65it/s]Extractor Predicting: 51it [00:32,  1.63it/s]Extractor Predicting: 52it [00:32,  1.62it/s]Extractor Predicting: 53it [00:33,  1.59it/s]Extractor Predicting: 54it [00:34,  1.63it/s]Extractor Predicting: 55it [00:34,  1.63it/s]Extractor Predicting: 56it [00:35,  1.65it/s]Extractor Predicting: 57it [00:35,  1.68it/s]Extractor Predicting: 58it [00:36,  1.66it/s]Extractor Predicting: 59it [00:37,  1.70it/s]Extractor Predicting: 60it [00:37,  1.66it/s]Extractor Predicting: 61it [00:38,  1.66it/s]Extractor Predicting: 62it [00:38,  1.72it/s]Extractor Predicting: 63it [00:39,  1.73it/s]Extractor Predicting: 64it [00:40,  1.76it/s]Extractor Predicting: 65it [00:40,  1.77it/s]Extractor Predicting: 66it [00:41,  1.79it/s]Extractor Predicting: 67it [00:41,  1.77it/s]Extractor Predicting: 68it [00:42,  1.77it/s]Extractor Predicting: 69it [00:42,  1.80it/s]Extractor Predicting: 70it [00:43,  1.78it/s]Extractor Predicting: 71it [00:44,  1.74it/s]Extractor Predicting: 72it [00:44,  1.77it/s]Extractor Predicting: 73it [00:45,  1.71it/s]Extractor Predicting: 74it [00:45,  1.70it/s]Extractor Predicting: 75it [00:46,  1.75it/s]Extractor Predicting: 76it [00:46,  1.75it/s]Extractor Predicting: 77it [00:47,  1.77it/s]Extractor Predicting: 78it [00:47,  1.77it/s]Extractor Predicting: 79it [00:48,  1.78it/s]Extractor Predicting: 80it [00:49,  1.75it/s]Extractor Predicting: 81it [00:49,  1.71it/s]Extractor Predicting: 82it [00:50,  1.67it/s]Extractor Predicting: 83it [00:50,  1.69it/s]Extractor Predicting: 84it [00:51,  1.69it/s]Extractor Predicting: 85it [00:52,  1.64it/s]Extractor Predicting: 86it [00:52,  1.63it/s]Extractor Predicting: 87it [00:53,  1.63it/s]Extractor Predicting: 88it [00:54,  1.60it/s]Extractor Predicting: 89it [00:54,  1.60it/s]Extractor Predicting: 90it [00:55,  1.59it/s]Extractor Predicting: 91it [00:55,  1.61it/s]Extractor Predicting: 92it [00:56,  1.59it/s]Extractor Predicting: 93it [00:57,  1.58it/s]Extractor Predicting: 94it [00:57,  1.60it/s]Extractor Predicting: 95it [00:58,  1.59it/s]Extractor Predicting: 96it [00:59,  1.60it/s]Extractor Predicting: 97it [00:59,  1.59it/s]Extractor Predicting: 98it [01:00,  1.59it/s]Extractor Predicting: 99it [01:00,  1.61it/s]Extractor Predicting: 100it [01:01,  1.62it/s]Extractor Predicting: 101it [01:02,  1.61it/s]Extractor Predicting: 102it [01:02,  1.62it/s]Extractor Predicting: 103it [01:03,  1.62it/s]Extractor Predicting: 104it [01:04,  1.65it/s]Extractor Predicting: 105it [01:04,  1.67it/s]Extractor Predicting: 106it [01:05,  1.64it/s]Extractor Predicting: 107it [01:05,  1.63it/s]Extractor Predicting: 108it [01:06,  1.63it/s]Extractor Predicting: 109it [01:07,  1.61it/s]Extractor Predicting: 110it [01:07,  1.60it/s]Extractor Predicting: 111it [01:08,  1.58it/s]Extractor Predicting: 112it [01:09,  1.60it/s]Extractor Predicting: 113it [01:09,  1.63it/s]Extractor Predicting: 114it [01:10,  1.63it/s]Extractor Predicting: 115it [01:10,  1.62it/s]Extractor Predicting: 116it [01:11,  1.65it/s]Extractor Predicting: 117it [01:12,  1.64it/s]Extractor Predicting: 118it [01:12,  1.63it/s]Extractor Predicting: 119it [01:13,  1.63it/s]Extractor Predicting: 120it [01:13,  1.62it/s]Extractor Predicting: 121it [01:14,  1.61it/s]Extractor Predicting: 122it [01:15,  1.60it/s]Extractor Predicting: 123it [01:15,  1.59it/s]Extractor Predicting: 124it [01:16,  1.57it/s]Extractor Predicting: 125it [01:17,  1.59it/s]Extractor Predicting: 126it [01:17,  1.59it/s]Extractor Predicting: 127it [01:18,  1.58it/s]Extractor Predicting: 128it [01:18,  1.56it/s]Extractor Predicting: 129it [01:19,  1.58it/s]Extractor Predicting: 130it [01:20,  1.61it/s]Extractor Predicting: 131it [01:20,  1.59it/s]Extractor Predicting: 132it [01:21,  1.60it/s]Extractor Predicting: 133it [01:22,  1.50it/s]Extractor Predicting: 134it [01:22,  1.52it/s]Extractor Predicting: 135it [01:23,  1.52it/s]Extractor Predicting: 136it [01:24,  1.54it/s]Extractor Predicting: 137it [01:24,  1.58it/s]Extractor Predicting: 138it [01:25,  1.61it/s]Extractor Predicting: 139it [01:25,  1.62it/s]Extractor Predicting: 140it [01:26,  1.63it/s]Extractor Predicting: 141it [01:27,  1.70it/s]Extractor Predicting: 141it [01:27,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:32,896 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:32,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:32,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:32,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:32,902 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:42:33,500 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:42:33,501 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:42:34,090 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:42:35,135 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:42:35,135 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:38,018 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:38,023 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:38,024 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:38,024 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:42:38,024 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:42:38,660 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:42:38,661 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:42:39,220 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:42:39,403 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:42:39,403 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2879027997886952,
  "recall": 0.15815438189204875,
  "score": 0.2041580820378348,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.60it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.70it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.69it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.74it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.71it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.70it/s]Extractor Predicting: 15it [00:08,  1.69it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.65it/s]Extractor Predicting: 18it [00:10,  1.64it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:11,  1.65it/s]Extractor Predicting: 21it [00:12,  1.54it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:13,  1.58it/s]Extractor Predicting: 24it [00:14,  1.58it/s]Extractor Predicting: 25it [00:15,  1.61it/s]Extractor Predicting: 26it [00:15,  1.62it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:16,  1.64it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.56it/s]Extractor Predicting: 31it [00:18,  1.56it/s]Extractor Predicting: 32it [00:19,  1.54it/s]Extractor Predicting: 33it [00:20,  1.56it/s]Extractor Predicting: 34it [00:20,  1.57it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.65it/s]Extractor Predicting: 41it [00:25,  1.64it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:26,  1.62it/s]Extractor Predicting: 44it [00:26,  1.62it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:28,  1.63it/s]Extractor Predicting: 48it [00:29,  1.63it/s]Extractor Predicting: 49it [00:30,  1.62it/s]Extractor Predicting: 50it [00:30,  1.60it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:31,  1.61it/s]Extractor Predicting: 53it [00:32,  1.62it/s]Extractor Predicting: 54it [00:33,  1.63it/s]Extractor Predicting: 55it [00:33,  1.59it/s]Extractor Predicting: 56it [00:34,  1.60it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:36,  1.66it/s]Extractor Predicting: 60it [00:36,  1.66it/s]Extractor Predicting: 61it [00:37,  1.65it/s]Extractor Predicting: 62it [00:37,  1.64it/s]Extractor Predicting: 63it [00:38,  1.62it/s]Extractor Predicting: 64it [00:39,  1.64it/s]Extractor Predicting: 65it [00:39,  1.61it/s]Extractor Predicting: 66it [00:40,  1.58it/s]Extractor Predicting: 67it [00:41,  1.63it/s]Extractor Predicting: 68it [00:41,  1.64it/s]Extractor Predicting: 69it [00:42,  1.67it/s]Extractor Predicting: 70it [00:42,  1.65it/s]Extractor Predicting: 71it [00:43,  1.64it/s]Extractor Predicting: 72it [00:44,  1.65it/s]Extractor Predicting: 73it [00:44,  1.61it/s]Extractor Predicting: 74it [00:45,  1.61it/s]Extractor Predicting: 75it [00:46,  1.62it/s]Extractor Predicting: 76it [00:46,  1.64it/s]Extractor Predicting: 77it [00:47,  1.60it/s]Extractor Predicting: 78it [00:47,  1.59it/s]Extractor Predicting: 79it [00:48,  1.61it/s]Extractor Predicting: 80it [00:49,  1.60it/s]Extractor Predicting: 81it [00:49,  1.59it/s]Extractor Predicting: 82it [00:50,  1.59it/s]Extractor Predicting: 83it [00:51,  1.57it/s]Extractor Predicting: 84it [00:51,  1.59it/s]Extractor Predicting: 85it [00:52,  1.60it/s]Extractor Predicting: 86it [00:52,  1.60it/s]Extractor Predicting: 87it [00:53,  1.59it/s]Extractor Predicting: 88it [00:54,  1.60it/s]Extractor Predicting: 89it [00:54,  1.57it/s]Extractor Predicting: 90it [00:55,  1.56it/s]Extractor Predicting: 91it [00:56,  1.57it/s]Extractor Predicting: 92it [00:56,  1.58it/s]Extractor Predicting: 93it [00:57,  1.57it/s]Extractor Predicting: 94it [00:58,  1.53it/s]Extractor Predicting: 95it [00:58,  1.53it/s]Extractor Predicting: 96it [00:59,  1.55it/s]Extractor Predicting: 97it [00:59,  1.56it/s]Extractor Predicting: 98it [01:00,  1.58it/s]Extractor Predicting: 99it [01:01,  1.56it/s]Extractor Predicting: 100it [01:01,  1.56it/s]Extractor Predicting: 101it [01:02,  1.59it/s]Extractor Predicting: 102it [01:03,  1.58it/s]Extractor Predicting: 103it [01:03,  1.57it/s]Extractor Predicting: 104it [01:04,  1.57it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:05,  1.57it/s]Extractor Predicting: 107it [01:06,  1.53it/s]Extractor Predicting: 108it [01:07,  1.54it/s]Extractor Predicting: 109it [01:07,  1.56it/s]Extractor Predicting: 110it [01:08,  1.53it/s]Extractor Predicting: 111it [01:08,  1.53it/s]Extractor Predicting: 112it [01:09,  1.55it/s]Extractor Predicting: 113it [01:10,  1.59it/s]Extractor Predicting: 114it [01:10,  1.61it/s]Extractor Predicting: 115it [01:11,  1.58it/s]Extractor Predicting: 116it [01:12,  1.60it/s]Extractor Predicting: 117it [01:12,  1.64it/s]Extractor Predicting: 118it [01:13,  1.62it/s]Extractor Predicting: 119it [01:13,  1.64it/s]Extractor Predicting: 120it [01:14,  1.64it/s]Extractor Predicting: 121it [01:15,  1.65it/s]Extractor Predicting: 122it [01:15,  1.69it/s]Extractor Predicting: 123it [01:16,  1.67it/s]Extractor Predicting: 124it [01:16,  1.67it/s]Extractor Predicting: 125it [01:17,  1.66it/s]Extractor Predicting: 126it [01:18,  1.63it/s]Extractor Predicting: 127it [01:18,  1.64it/s]Extractor Predicting: 128it [01:19,  1.67it/s]Extractor Predicting: 129it [01:19,  1.63it/s]Extractor Predicting: 130it [01:20,  1.63it/s]Extractor Predicting: 131it [01:21,  1.62it/s]Extractor Predicting: 132it [01:22,  1.44it/s]Extractor Predicting: 133it [01:22,  1.49it/s]Extractor Predicting: 134it [01:23,  1.53it/s]Extractor Predicting: 135it [01:23,  1.57it/s]Extractor Predicting: 136it [01:24,  1.59it/s]Extractor Predicting: 137it [01:25,  1.60it/s]Extractor Predicting: 138it [01:25,  1.60it/s]Extractor Predicting: 139it [01:26,  1.65it/s]Extractor Predicting: 140it [01:26,  1.64it/s]Extractor Predicting: 141it [01:27,  1.63it/s]Extractor Predicting: 142it [01:28,  1.65it/s]Extractor Predicting: 143it [01:28,  1.65it/s]Extractor Predicting: 144it [01:29,  1.62it/s]Extractor Predicting: 145it [01:29,  1.64it/s]Extractor Predicting: 146it [01:30,  1.63it/s]Extractor Predicting: 147it [01:31,  1.62it/s]Extractor Predicting: 148it [01:31,  1.60it/s]Extractor Predicting: 149it [01:32,  1.60it/s]Extractor Predicting: 150it [01:33,  1.62it/s]Extractor Predicting: 151it [01:33,  1.65it/s]Extractor Predicting: 152it [01:34,  1.66it/s]Extractor Predicting: 153it [01:34,  1.66it/s]Extractor Predicting: 154it [01:35,  1.62it/s]Extractor Predicting: 155it [01:36,  1.62it/s]Extractor Predicting: 156it [01:36,  1.61it/s]Extractor Predicting: 157it [01:37,  1.60it/s]Extractor Predicting: 158it [01:38,  1.58it/s]Extractor Predicting: 159it [01:38,  1.58it/s]Extractor Predicting: 160it [01:39,  1.59it/s]Extractor Predicting: 161it [01:39,  1.61it/s]Extractor Predicting: 162it [01:40,  1.62it/s]Extractor Predicting: 163it [01:41,  1.63it/s]Extractor Predicting: 164it [01:41,  1.62it/s]Extractor Predicting: 165it [01:42,  1.60it/s]Extractor Predicting: 166it [01:42,  1.61it/s]Extractor Predicting: 167it [01:43,  1.64it/s]Extractor Predicting: 168it [01:44,  1.63it/s]Extractor Predicting: 169it [01:44,  1.61it/s]Extractor Predicting: 170it [01:45,  1.60it/s]Extractor Predicting: 171it [01:46,  1.57it/s]Extractor Predicting: 172it [01:46,  1.56it/s]Extractor Predicting: 173it [01:47,  1.55it/s]Extractor Predicting: 174it [01:48,  1.51it/s]Extractor Predicting: 175it [01:48,  1.49it/s]Extractor Predicting: 176it [01:49,  1.51it/s]Extractor Predicting: 177it [01:50,  1.54it/s]Extractor Predicting: 178it [01:50,  1.55it/s]Extractor Predicting: 179it [01:51,  1.59it/s]Extractor Predicting: 180it [01:51,  1.57it/s]Extractor Predicting: 181it [01:52,  1.59it/s]Extractor Predicting: 182it [01:53,  1.60it/s]Extractor Predicting: 183it [01:53,  1.60it/s]Extractor Predicting: 184it [01:54,  1.62it/s]Extractor Predicting: 185it [01:54,  1.64it/s]Extractor Predicting: 186it [01:55,  1.66it/s]Extractor Predicting: 187it [01:56,  1.65it/s]Extractor Predicting: 188it [01:56,  1.66it/s]Extractor Predicting: 189it [01:57,  1.66it/s]Extractor Predicting: 190it [01:57,  1.66it/s]Extractor Predicting: 191it [01:58,  1.61it/s]Extractor Predicting: 192it [01:59,  1.59it/s]Extractor Predicting: 193it [01:59,  1.61it/s]Extractor Predicting: 194it [02:00,  1.64it/s]Extractor Predicting: 195it [02:01,  1.65it/s]Extractor Predicting: 196it [02:01,  1.65it/s]Extractor Predicting: 197it [02:02,  1.67it/s]Extractor Predicting: 198it [02:02,  1.66it/s]Extractor Predicting: 199it [02:03,  1.64it/s]Extractor Predicting: 200it [02:04,  1.63it/s]Extractor Predicting: 201it [02:04,  1.63it/s]Extractor Predicting: 202it [02:05,  1.65it/s]Extractor Predicting: 203it [02:05,  1.68it/s]Extractor Predicting: 204it [02:06,  1.68it/s]Extractor Predicting: 205it [02:07,  1.67it/s]Extractor Predicting: 206it [02:07,  1.66it/s]Extractor Predicting: 207it [02:08,  1.62it/s]Extractor Predicting: 208it [02:08,  1.62it/s]Extractor Predicting: 209it [02:09,  1.63it/s]Extractor Predicting: 210it [02:10,  1.62it/s]Extractor Predicting: 211it [02:10,  1.64it/s]Extractor Predicting: 212it [02:11,  1.62it/s]Extractor Predicting: 213it [02:12,  1.66it/s]Extractor Predicting: 214it [02:12,  1.64it/s]Extractor Predicting: 215it [02:13,  1.67it/s]Extractor Predicting: 216it [02:13,  1.67it/s]Extractor Predicting: 217it [02:14,  1.60it/s]Extractor Predicting: 218it [02:15,  1.62it/s]Extractor Predicting: 219it [02:15,  1.65it/s]Extractor Predicting: 220it [02:16,  1.67it/s]Extractor Predicting: 221it [02:16,  1.65it/s]Extractor Predicting: 222it [02:17,  1.66it/s]Extractor Predicting: 223it [02:18,  1.68it/s]Extractor Predicting: 224it [02:18,  1.68it/s]Extractor Predicting: 225it [02:19,  1.65it/s]Extractor Predicting: 226it [02:19,  1.61it/s]Extractor Predicting: 227it [02:20,  1.61it/s]Extractor Predicting: 228it [02:21,  1.57it/s]Extractor Predicting: 229it [02:21,  1.57it/s]Extractor Predicting: 230it [02:22,  1.60it/s]Extractor Predicting: 231it [02:23,  1.59it/s]Extractor Predicting: 232it [02:23,  1.59it/s]Extractor Predicting: 233it [02:24,  1.60it/s]Extractor Predicting: 234it [02:24,  1.60it/s]Extractor Predicting: 235it [02:25,  1.57it/s]Extractor Predicting: 236it [02:26,  1.42it/s]Extractor Predicting: 237it [02:27,  1.51it/s]Extractor Predicting: 238it [02:27,  1.51it/s]Extractor Predicting: 239it [02:28,  1.52it/s]Extractor Predicting: 240it [02:29,  1.52it/s]Extractor Predicting: 241it [02:29,  1.53it/s]Extractor Predicting: 242it [02:30,  1.56it/s]Extractor Predicting: 243it [02:30,  1.56it/s]Extractor Predicting: 244it [02:31,  1.54it/s]Extractor Predicting: 245it [02:32,  1.57it/s]Extractor Predicting: 246it [02:32,  1.59it/s]Extractor Predicting: 247it [02:33,  1.62it/s]Extractor Predicting: 248it [02:34,  1.54it/s]Extractor Predicting: 249it [02:34,  1.56it/s]Extractor Predicting: 250it [02:35,  1.57it/s]Extractor Predicting: 251it [02:36,  1.55it/s]Extractor Predicting: 252it [02:36,  1.55it/s]Extractor Predicting: 253it [02:37,  1.55it/s]Extractor Predicting: 254it [02:37,  1.59it/s]Extractor Predicting: 255it [02:38,  1.60it/s]Extractor Predicting: 256it [02:39,  1.64it/s]Extractor Predicting: 257it [02:39,  1.67it/s]Extractor Predicting: 258it [02:40,  1.63it/s]Extractor Predicting: 259it [02:40,  1.61it/s]Extractor Predicting: 260it [02:41,  1.59it/s]Extractor Predicting: 261it [02:42,  1.58it/s]Extractor Predicting: 262it [02:42,  1.59it/s]Extractor Predicting: 263it [02:43,  1.53it/s]Extractor Predicting: 264it [02:44,  1.55it/s]Extractor Predicting: 265it [02:44,  1.57it/s]Extractor Predicting: 266it [02:45,  1.59it/s]Extractor Predicting: 267it [02:46,  1.59it/s]Extractor Predicting: 268it [02:46,  1.61it/s]Extractor Predicting: 269it [02:47,  1.65it/s]Extractor Predicting: 270it [02:47,  1.67it/s]Extractor Predicting: 271it [02:48,  1.63it/s]Extractor Predicting: 272it [02:49,  1.65it/s]Extractor Predicting: 273it [02:49,  1.63it/s]Extractor Predicting: 274it [02:50,  1.63it/s]Extractor Predicting: 275it [02:50,  1.64it/s]Extractor Predicting: 276it [02:51,  1.64it/s]Extractor Predicting: 277it [02:52,  1.61it/s]Extractor Predicting: 278it [02:52,  1.65it/s]Extractor Predicting: 279it [02:53,  1.60it/s]Extractor Predicting: 280it [02:54,  1.60it/s]Extractor Predicting: 281it [02:54,  1.60it/s]Extractor Predicting: 282it [02:55,  1.61it/s]Extractor Predicting: 283it [02:55,  1.62it/s]Extractor Predicting: 284it [02:56,  1.59it/s]Extractor Predicting: 285it [02:57,  1.59it/s]Extractor Predicting: 286it [02:57,  1.61it/s]Extractor Predicting: 287it [02:58,  1.63it/s]Extractor Predicting: 288it [02:58,  1.62it/s]Extractor Predicting: 289it [02:59,  1.58it/s]Extractor Predicting: 290it [03:00,  1.64it/s]Extractor Predicting: 291it [03:00,  1.62it/s]Extractor Predicting: 292it [03:01,  1.61it/s]Extractor Predicting: 293it [03:02,  1.58it/s]Extractor Predicting: 294it [03:02,  1.54it/s]Extractor Predicting: 295it [03:03,  1.59it/s]Extractor Predicting: 296it [03:04,  1.57it/s]Extractor Predicting: 297it [03:04,  1.60it/s]Extractor Predicting: 298it [03:05,  1.55it/s]Extractor Predicting: 299it [03:05,  1.56it/s]Extractor Predicting: 300it [03:06,  1.57it/s]Extractor Predicting: 301it [03:07,  1.58it/s]Extractor Predicting: 302it [03:07,  1.57it/s]Extractor Predicting: 303it [03:08,  1.55it/s]Extractor Predicting: 304it [03:09,  1.52it/s]Extractor Predicting: 305it [03:09,  1.54it/s]Extractor Predicting: 306it [03:10,  1.52it/s]Extractor Predicting: 307it [03:11,  1.51it/s]Extractor Predicting: 308it [03:11,  1.54it/s]Extractor Predicting: 309it [03:12,  1.54it/s]Extractor Predicting: 310it [03:13,  1.52it/s]Extractor Predicting: 311it [03:13,  1.51it/s]Extractor Predicting: 312it [03:14,  1.52it/s]Extractor Predicting: 313it [03:15,  1.53it/s]Extractor Predicting: 314it [03:15,  1.50it/s]Extractor Predicting: 315it [03:16,  1.55it/s]Extractor Predicting: 316it [03:17,  1.54it/s]Extractor Predicting: 317it [03:17,  1.52it/s]Extractor Predicting: 318it [03:18,  1.53it/s]Extractor Predicting: 319it [03:19,  1.53it/s]Extractor Predicting: 320it [03:19,  1.62it/s]Extractor Predicting: 321it [03:20,  1.66it/s]Extractor Predicting: 322it [03:20,  1.75it/s]Extractor Predicting: 323it [03:21,  1.77it/s]Extractor Predicting: 324it [03:21,  1.78it/s]Extractor Predicting: 325it [03:22,  1.80it/s]Extractor Predicting: 326it [03:22,  1.80it/s]Extractor Predicting: 327it [03:23,  1.78it/s]Extractor Predicting: 328it [03:23,  1.81it/s]Extractor Predicting: 329it [03:24,  1.85it/s]Extractor Predicting: 330it [03:24,  1.84it/s]Extractor Predicting: 331it [03:25,  1.87it/s]Extractor Predicting: 332it [03:26,  1.84it/s]Extractor Predicting: 333it [03:26,  1.87it/s]Extractor Predicting: 334it [03:27,  1.87it/s]Extractor Predicting: 335it [03:27,  1.86it/s]Extractor Predicting: 336it [03:28,  1.83it/s]Extractor Predicting: 337it [03:28,  1.88it/s]Extractor Predicting: 338it [03:29,  1.82it/s]Extractor Predicting: 339it [03:29,  1.81it/s]Extractor Predicting: 340it [03:30,  1.87it/s]Extractor Predicting: 341it [03:30,  1.85it/s]Extractor Predicting: 342it [03:31,  1.87it/s]Extractor Predicting: 343it [03:32,  1.60it/s]Extractor Predicting: 344it [03:32,  1.64it/s]Extractor Predicting: 345it [03:33,  1.71it/s]Extractor Predicting: 346it [03:33,  1.74it/s]Extractor Predicting: 347it [03:34,  1.74it/s]Extractor Predicting: 348it [03:35,  1.67it/s]Extractor Predicting: 349it [03:35,  1.66it/s]Extractor Predicting: 350it [03:36,  1.63it/s]Extractor Predicting: 351it [03:37,  1.62it/s]Extractor Predicting: 352it [03:37,  1.63it/s]Extractor Predicting: 353it [03:38,  1.65it/s]Extractor Predicting: 354it [03:38,  1.63it/s]Extractor Predicting: 355it [03:39,  1.62it/s]Extractor Predicting: 356it [03:40,  1.59it/s]Extractor Predicting: 357it [03:40,  1.56it/s]Extractor Predicting: 358it [03:41,  1.59it/s]Extractor Predicting: 359it [03:42,  1.59it/s]Extractor Predicting: 360it [03:42,  1.62it/s]Extractor Predicting: 361it [03:43,  1.61it/s]Extractor Predicting: 362it [03:43,  1.64it/s]Extractor Predicting: 363it [03:44,  1.62it/s]Extractor Predicting: 364it [03:45,  1.60it/s]Extractor Predicting: 365it [03:45,  1.60it/s]Extractor Predicting: 366it [03:46,  1.60it/s]Extractor Predicting: 367it [03:47,  1.60it/s]Extractor Predicting: 368it [03:47,  1.62it/s]Extractor Predicting: 369it [03:48,  1.61it/s]Extractor Predicting: 370it [03:48,  1.60it/s]Extractor Predicting: 371it [03:49,  1.59it/s]Extractor Predicting: 372it [03:50,  1.60it/s]Extractor Predicting: 373it [03:50,  1.58it/s]Extractor Predicting: 374it [03:51,  1.60it/s]Extractor Predicting: 375it [03:51,  1.61it/s]Extractor Predicting: 376it [03:52,  1.59it/s]Extractor Predicting: 377it [03:53,  1.60it/s]Extractor Predicting: 378it [03:53,  1.57it/s]Extractor Predicting: 379it [03:54,  1.57it/s]Extractor Predicting: 380it [03:55,  1.56it/s]Extractor Predicting: 381it [03:55,  1.54it/s]Extractor Predicting: 382it [03:56,  1.53it/s]Extractor Predicting: 383it [03:57,  1.54it/s]Extractor Predicting: 384it [03:57,  1.53it/s]Extractor Predicting: 385it [03:58,  1.54it/s]Extractor Predicting: 386it [03:59,  1.56it/s]Extractor Predicting: 387it [03:59,  1.54it/s]Extractor Predicting: 388it [04:00,  1.53it/s]Extractor Predicting: 389it [04:01,  1.56it/s]Extractor Predicting: 390it [04:01,  1.57it/s]Extractor Predicting: 391it [04:02,  1.57it/s]Extractor Predicting: 392it [04:03,  1.53it/s]Extractor Predicting: 393it [04:03,  1.58it/s]Extractor Predicting: 394it [04:04,  1.56it/s]Extractor Predicting: 395it [04:04,  1.54it/s]Extractor Predicting: 396it [04:05,  1.54it/s]Extractor Predicting: 397it [04:06,  1.51it/s]Extractor Predicting: 398it [04:06,  1.52it/s]Extractor Predicting: 399it [04:07,  1.48it/s]Extractor Predicting: 400it [04:08,  1.49it/s]Extractor Predicting: 401it [04:08,  1.50it/s]Extractor Predicting: 402it [04:09,  1.52it/s]Extractor Predicting: 403it [04:10,  1.52it/s]Extractor Predicting: 404it [04:10,  1.57it/s]Extractor Predicting: 405it [04:11,  1.56it/s]Extractor Predicting: 406it [04:12,  1.60it/s]Extractor Predicting: 407it [04:12,  1.59it/s]Extractor Predicting: 408it [04:13,  1.57it/s]Extractor Predicting: 409it [04:13,  1.61it/s]Extractor Predicting: 410it [04:14,  1.60it/s]Extractor Predicting: 411it [04:15,  1.63it/s]Extractor Predicting: 412it [04:15,  1.60it/s]Extractor Predicting: 413it [04:16,  1.63it/s]Extractor Predicting: 414it [04:17,  1.63it/s]Extractor Predicting: 415it [04:17,  1.62it/s]Extractor Predicting: 416it [04:18,  1.59it/s]Extractor Predicting: 417it [04:18,  1.62it/s]Extractor Predicting: 418it [04:19,  1.61it/s]Extractor Predicting: 419it [04:20,  1.59it/s]Extractor Predicting: 420it [04:20,  1.60it/s]Extractor Predicting: 421it [04:21,  1.60it/s]Extractor Predicting: 422it [04:22,  1.57it/s]Extractor Predicting: 423it [04:22,  1.58it/s]Extractor Predicting: 424it [04:23,  1.55it/s]Extractor Predicting: 425it [04:23,  1.56it/s]Extractor Predicting: 426it [04:24,  1.55it/s]Extractor Predicting: 427it [04:25,  1.57it/s]Extractor Predicting: 428it [04:25,  1.56it/s]Extractor Predicting: 429it [04:26,  1.58it/s]Extractor Predicting: 430it [04:26,  1.97it/s]Extractor Predicting: 430it [04:26,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:15,513 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:15,517 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:15,517 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:15,517 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:15,517 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:47:16,144 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:47:16,144 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:47:16,719 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:47:17,778 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:47:17,778 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:20,589 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:20,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:20,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:20,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:47:20,595 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:47:21,224 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:47:21,225 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:47:21,783 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:47:21,959 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:47:21,960 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.285542788543989,
  "recall": 0.1616818799766945,
  "score": 0.20646041292082584,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:02,  2.03it/s]Extractor Predicting: 5it [00:02,  1.74it/s]
[INFO|configuration_utils.py:515] 2023-08-29 01:47:25,250 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:47:25,251 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 01:47:25,255 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:47:25,255 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 01:47:25,257 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 01:47:28,621 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 01:47:28,624 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 01:47:28,640 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 01:47:28,641 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_15_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 01:47:28,646 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:28,653 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:28,653 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:28,653 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:28,653 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:28,653 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 01:47:28,653 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3125,
  "recall": 0.07425742574257425,
  "score": 0.12,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 01:47:28,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:29,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:30,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:30,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:31,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:32,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:32,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:33,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:33,742 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:34,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:35,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:35,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:36,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:37,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:37,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:38,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:38,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:39,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:40,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:40,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:41,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:13<04:09, 13.13s/it][WARNING|generation_utils.py:914] 2023-08-29 01:47:42,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:42,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:43,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:43,668 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:44,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:44,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:45,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:45,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:46,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:46,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:47,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:48,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:48,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:49,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:49,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:50,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:50,977 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:51,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:52,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:53,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:53,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:54,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:26<03:54, 13.02s/it][WARNING|generation_utils.py:914] 2023-08-29 01:47:54,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:55,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:55,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:56,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:56,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:57,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:58,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:58,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:59,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:59,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:47:59,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:00,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:00,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:01,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:01,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:02,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:02,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:03,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:03,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:04,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:36<03:19, 11.76s/it][WARNING|generation_utils.py:914] 2023-08-29 01:48:05,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:05,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:06,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:07,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:07,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:08,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:08,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:09,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:09,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:10,333 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:10,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:11,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:12,056 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:12,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:13,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:13,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:14,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:14,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:15,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:15,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:16,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:48<03:09, 11.82s/it][WARNING|generation_utils.py:914] 2023-08-29 01:48:17,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:17,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:18,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:18,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:19,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:19,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:20,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:21,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:21,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:22,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:22,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:23,269 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:23,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:24,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:24,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:25,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:25,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:26,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:27,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:27,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [00:59<02:52, 11.51s/it][WARNING|generation_utils.py:914] 2023-08-29 01:48:28,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:28,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:29,170 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:29,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:30,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:31,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:31,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:32,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:32,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:33,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:33,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:34,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:34,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:35,370 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:35,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:36,598 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:37,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:37,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:38,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:39,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:39,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:40,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:12<02:47, 11.98s/it][WARNING|generation_utils.py:914] 2023-08-29 01:48:40,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:41,457 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:41,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:42,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:43,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:43,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:44,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:44,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:45,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:45,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:46,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:46,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:47,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:47,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:48,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:48,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:49,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:49,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:50,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:51,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:22<02:29, 11.54s/it][WARNING|generation_utils.py:914] 2023-08-29 01:48:51,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:52,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:52,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:53,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:53,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:54,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:55,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:55,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:56,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:56,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:57,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:57,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:58,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:59,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:48:59,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:00,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:00,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:01,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:02,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:03,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:03,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:35<02:22, 11.88s/it][WARNING|generation_utils.py:914] 2023-08-29 01:49:04,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:04,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:05,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:05,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:06,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:06,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:07,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:08,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:09,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:09,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:10,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:10,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:11,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:11,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:12,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:13,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:13,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:14,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:15,024 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:15,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:16,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:16,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:17,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:49<02:17, 12.53s/it][WARNING|generation_utils.py:914] 2023-08-29 01:49:18,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:18,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:19,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:19,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:20,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:20,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:21,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:22,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:22,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:23,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:23,774 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:24,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:24,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:25,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:25,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:26,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:26,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:27,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:28,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:28,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:29,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:29,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [02:01<02:03, 12.36s/it][WARNING|generation_utils.py:914] 2023-08-29 01:49:30,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:30,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:31,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:31,901 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:32,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:32,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:33,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:33,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:34,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:35,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:35,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:36,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:36,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:37,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:38,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:38,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:39,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:39,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:40,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:41,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:12<01:48, 12.08s/it][WARNING|generation_utils.py:914] 2023-08-29 01:49:41,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:42,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:42,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:43,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:43,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:44,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:44,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:45,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:45,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:46,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:46,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:47,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:47,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:48,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:48,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:49,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:50,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:50,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:51,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:51,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:52,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:23<01:34, 11.80s/it][WARNING|generation_utils.py:914] 2023-08-29 01:49:52,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:53,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:53,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:54,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:55,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:55,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:56,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:56,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:57,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:57,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:58,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:58,911 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:59,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:49:59,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:00,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:01,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:01,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:02,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:02,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:03,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:34<01:20, 11.53s/it][WARNING|generation_utils.py:914] 2023-08-29 01:50:03,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:04,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:04,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:05,273 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:05,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:06,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:07,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:07,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:08,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:08,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:09,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:09,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:10,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:11,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:11,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:12,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:13,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:13,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:14,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:15,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:15,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:47<01:11, 11.88s/it][WARNING|generation_utils.py:914] 2023-08-29 01:50:16,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:16,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:17,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:17,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:18,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:19,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:19,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:20,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:20,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:21,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:21,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:22,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:23,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:23,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:24,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:24,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:25,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:26,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:26,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:27,128 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:27,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:28,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [02:59<01:00, 12.05s/it][WARNING|generation_utils.py:914] 2023-08-29 01:50:28,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:29,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:30,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:30,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:31,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:31,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:32,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:33,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:33,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:34,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:34,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:35,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:36,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:36,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:37,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:38,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:38,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:39,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:39,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:40,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:40,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:12<00:49, 12.28s/it][WARNING|generation_utils.py:914] 2023-08-29 01:50:41,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:42,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:42,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:43,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:44,222 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:44,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:45,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:45,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:46,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:47,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:47,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:48,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:48,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:49,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:49,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:50,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:51,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:51,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:52,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:52,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:53,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:54,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:26<00:37, 12.60s/it][WARNING|generation_utils.py:914] 2023-08-29 01:50:54,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:55,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:56,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:56,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:57,073 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:57,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:58,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:58,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:59,406 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:50:59,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:00,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:01,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:01,706 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:02,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:02,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:03,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:04,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:04,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:05,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:05,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:06,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:38<00:24, 12.40s/it][WARNING|generation_utils.py:914] 2023-08-29 01:51:06,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:07,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:07,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:08,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:08,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:09,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:10,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:10,919 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:11,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:11,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:12,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:12,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:13,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:13,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:14,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:14,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:15,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:16,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:16,574 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:17,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:17,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:18,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:18,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:19,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:19,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:20,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:20,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:52<00:12, 12.99s/it][WARNING|generation_utils.py:914] 2023-08-29 01:51:21,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:21,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:22,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:22,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:23,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:24,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:24,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:25,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:25,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:26,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:27,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:27,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:28,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:28,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:29,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:29,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:30,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:31,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:31,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:32,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 01:51:32,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [04:04<00:00, 12.70s/it]Generating: 100%|██████████| 20/20 [04:04<00:00, 12.22s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:39,067 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:39,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:39,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:39,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:39,069 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:51:39,384 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:51:39,384 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:51:39,642 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:51:40,711 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:51:40,711 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:42,770 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:42,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:42,775 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:42,776 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:51:42,776 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:51:43,519 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:51:43,520 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:51:43,781 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:51:43,956 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:51:43,956 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 151, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 331, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 480, 'raw': 512}
{'target': 600, 'success': 511, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 599, 'raw': 640}
{'target': 600, 'success': 629, 'raw': 672}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9360119047619048, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 219, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 394, 'raw': 448}
{'target': 600, 'success': 420, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 525, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 611, 'raw': 704}
{'prompt': 'Relation : military rank .', 'success_rate': 0.8678977272727273, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 394, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 454, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 542, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 605, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9453125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 234, 'raw': 256}
{'target': 600, 'success': 264, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 325, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 585, 'raw': 640}
{'target': 600, 'success': 616, 'raw': 672}
{'prompt': 'Relation : record label .', 'success_rate': 0.9166666666666666, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 249, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 435, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : tributary .', 'success_rate': 0.9640625, 'errors': {''}}
['Relation : architect . Context : Later in 1837 , the architect of the new palace was Charles V , who at the time was mayor of Paris . Head Entity : Charles V , Tail Entity : Charles V , Paris .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 200, 'raw': 224}
{'target': 600, 'success': 230, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 284, 'raw': 320}
{'target': 600, 'success': 312, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 456, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 535, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 621, 'raw': 704}
{'prompt': 'Relation : architect .', 'success_rate': 0.8821022727272727, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 456, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 551, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : constellation .', 'success_rate': 0.9609375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 303, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 360, 'raw': 384}
{'target': 600, 'success': 388, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 535, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 595, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.9315476190476191, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 188, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 342, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 401, 'raw': 480}
{'target': 600, 'success': 430, 'raw': 512}
{'target': 600, 'success': 454, 'raw': 544}
{'target': 600, 'success': 481, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 535, 'raw': 640}
{'target': 600, 'success': 564, 'raw': 672}
{'target': 600, 'success': 590, 'raw': 704}
{'target': 600, 'success': 612, 'raw': 736}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8315217391304348, 'errors': {'', 'too many values to unpack (expected 2)', "('1891', 'country of origin', '', 'It was built in 1891 , and was renamed after him in 1892 .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 276, 'raw': 320}
{'target': 600, 'success': 306, 'raw': 352}
{'target': 600, 'success': 335, 'raw': 384}
{'target': 600, 'success': 360, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 442, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 498, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 557, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 614, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8721590909090909, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 574, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 171, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 410, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 499, 'raw': 544}
{'target': 600, 'success': 530, 'raw': 576}
{'target': 600, 'success': 557, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 177, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 332, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 389, 'raw': 416}
{'target': 600, 'success': 420, 'raw': 448}
{'target': 600, 'success': 448, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 541, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 601, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9390625, 'errors': {''}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 316, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 463, 'raw': 512}
{'target': 600, 'success': 492, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : member of .', 'success_rate': 0.9107142857142857, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 196, 'raw': 224}
{'target': 600, 'success': 224, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 278, 'raw': 320}
{'target': 600, 'success': 307, 'raw': 352}
{'target': 600, 'success': 336, 'raw': 384}
{'target': 600, 'success': 365, 'raw': 416}
{'target': 600, 'success': 396, 'raw': 448}
{'target': 600, 'success': 423, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 505, 'raw': 576}
{'target': 600, 'success': 534, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8849431818181818, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 287, 'raw': 320}
{'target': 600, 'success': 317, 'raw': 352}
{'target': 600, 'success': 346, 'raw': 384}
{'target': 600, 'success': 375, 'raw': 416}
{'target': 600, 'success': 403, 'raw': 448}
{'target': 600, 'success': 432, 'raw': 480}
{'target': 600, 'success': 460, 'raw': 512}
{'target': 600, 'success': 488, 'raw': 544}
{'target': 600, 'success': 519, 'raw': 576}
{'target': 600, 'success': 548, 'raw': 608}
{'target': 600, 'success': 577, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : notable work .', 'success_rate': 0.90625, 'errors': {''}}
['Relation : operator . Context : Later in 2008 , the operator of the New River railway announced that it would operate at a new facility , New River Station , for the first time . Head Entity : New River Station , Tail Entity : New River Railway .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 280, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 363, 'raw': 416}
{'target': 600, 'success': 388, 'raw': 448}
{'target': 600, 'success': 413, 'raw': 480}
{'target': 600, 'success': 440, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 548, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8650568181818182, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 237, 'raw': 256}
{'target': 600, 'success': 267, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 355, 'raw': 384}
{'target': 600, 'success': 386, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 447, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 505, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 566, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 625, 'raw': 672}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9300595238095238, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : position held . Context : On 31 March 2014 , the Brazilian national football team won their first match as the national football club , losing 2–0 to Real Sociedad . Head Entity : Real Sociedad , Tail Entity : forward .\n']
{'target': 600, 'success': 22, 'raw': 32}
{'target': 600, 'success': 49, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 94, 'raw': 128}
{'target': 600, 'success': 118, 'raw': 160}
{'target': 600, 'success': 143, 'raw': 192}
{'target': 600, 'success': 168, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 216, 'raw': 288}
{'target': 600, 'success': 239, 'raw': 320}
{'target': 600, 'success': 261, 'raw': 352}
{'target': 600, 'success': 281, 'raw': 384}
{'target': 600, 'success': 301, 'raw': 416}
{'target': 600, 'success': 322, 'raw': 448}
{'target': 600, 'success': 344, 'raw': 480}
{'target': 600, 'success': 368, 'raw': 512}
{'target': 600, 'success': 391, 'raw': 544}
{'target': 600, 'success': 414, 'raw': 576}
{'target': 600, 'success': 435, 'raw': 608}
{'target': 600, 'success': 455, 'raw': 640}
{'target': 600, 'success': 476, 'raw': 672}
{'target': 600, 'success': 503, 'raw': 704}
{'target': 600, 'success': 525, 'raw': 736}
{'target': 600, 'success': 547, 'raw': 768}
{'target': 600, 'success': 567, 'raw': 800}
{'target': 600, 'success': 591, 'raw': 832}
{'target': 600, 'success': 615, 'raw': 864}
{'prompt': 'Relation : position held .', 'success_rate': 0.7118055555555556, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 148, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 297, 'raw': 320}
{'target': 600, 'success': 326, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 413, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 472, 'raw': 512}
{'target': 600, 'success': 497, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 555, 'raw': 608}
{'target': 600, 'success': 586, 'raw': 640}
{'target': 600, 'success': 615, 'raw': 672}
{'prompt': 'Relation : residence .', 'success_rate': 0.9151785714285714, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 10367
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10467, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.48it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:01,  1.54it/s]Extractor Estimating: 4it [00:02,  1.53it/s]Extractor Estimating: 5it [00:03,  1.59it/s]Extractor Estimating: 6it [00:03,  1.59it/s]Extractor Estimating: 7it [00:04,  1.55it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.59it/s]Extractor Estimating: 10it [00:06,  1.61it/s]Extractor Estimating: 11it [00:06,  1.61it/s]Extractor Estimating: 12it [00:07,  1.51it/s]Extractor Estimating: 13it [00:08,  1.54it/s]Extractor Estimating: 14it [00:09,  1.47it/s]Extractor Estimating: 15it [00:09,  1.52it/s]Extractor Estimating: 16it [00:10,  1.54it/s]Extractor Estimating: 17it [00:10,  1.61it/s]Extractor Estimating: 18it [00:11,  1.64it/s]Extractor Estimating: 19it [00:12,  1.61it/s]Extractor Estimating: 20it [00:12,  1.62it/s]Extractor Estimating: 21it [00:13,  1.64it/s]Extractor Estimating: 22it [00:13,  1.62it/s]Extractor Estimating: 23it [00:14,  1.60it/s]Extractor Estimating: 24it [00:15,  1.58it/s]Extractor Estimating: 25it [00:15,  1.55it/s]Extractor Estimating: 26it [00:16,  1.63it/s]Extractor Estimating: 27it [00:16,  1.71it/s]Extractor Estimating: 28it [00:17,  1.74it/s]Extractor Estimating: 29it [00:18,  1.79it/s]Extractor Estimating: 30it [00:18,  1.80it/s]Extractor Estimating: 31it [00:19,  1.78it/s]Extractor Estimating: 32it [00:19,  1.75it/s]Extractor Estimating: 33it [00:20,  1.72it/s]Extractor Estimating: 34it [00:20,  1.73it/s]Extractor Estimating: 35it [00:21,  1.70it/s]Extractor Estimating: 36it [00:22,  1.72it/s]Extractor Estimating: 37it [00:22,  1.73it/s]Extractor Estimating: 38it [00:23,  1.73it/s]Extractor Estimating: 39it [00:23,  1.72it/s]Extractor Estimating: 40it [00:24,  1.74it/s]Extractor Estimating: 41it [00:24,  1.74it/s]Extractor Estimating: 42it [00:25,  1.72it/s]Extractor Estimating: 43it [00:26,  1.79it/s]Extractor Estimating: 44it [00:26,  1.79it/s]Extractor Estimating: 45it [00:27,  1.78it/s]Extractor Estimating: 46it [00:27,  1.77it/s]Extractor Estimating: 47it [00:28,  1.67it/s]Extractor Estimating: 48it [00:29,  1.68it/s]Extractor Estimating: 49it [00:29,  1.70it/s]Extractor Estimating: 50it [00:30,  1.69it/s]Extractor Estimating: 51it [00:30,  1.71it/s]Extractor Estimating: 52it [00:31,  1.73it/s]Extractor Estimating: 53it [00:31,  1.79it/s]Extractor Estimating: 54it [00:32,  1.82it/s]Extractor Estimating: 55it [00:32,  1.81it/s]Extractor Estimating: 56it [00:33,  1.87it/s]Extractor Estimating: 57it [00:34,  1.76it/s]Extractor Estimating: 58it [00:34,  1.80it/s]Extractor Estimating: 59it [00:35,  1.88it/s]Extractor Estimating: 60it [00:35,  1.90it/s]Extractor Estimating: 61it [00:36,  1.94it/s]Extractor Estimating: 62it [00:36,  2.02it/s]Extractor Estimating: 63it [00:37,  2.02it/s]Extractor Estimating: 64it [00:37,  2.00it/s]Extractor Estimating: 65it [00:38,  1.93it/s]Extractor Estimating: 66it [00:38,  1.90it/s]Extractor Estimating: 67it [00:39,  1.90it/s]Extractor Estimating: 68it [00:39,  1.90it/s]Extractor Estimating: 69it [00:40,  1.93it/s]Extractor Estimating: 70it [00:40,  1.93it/s]Extractor Estimating: 71it [00:41,  1.92it/s]Extractor Estimating: 72it [00:41,  2.01it/s]Extractor Estimating: 73it [00:42,  1.99it/s]Extractor Estimating: 74it [00:42,  1.98it/s]Extractor Estimating: 75it [00:43,  1.96it/s]Extractor Estimating: 76it [00:43,  1.80it/s]Extractor Estimating: 77it [00:44,  1.76it/s]Extractor Estimating: 78it [00:45,  1.72it/s]Extractor Estimating: 79it [00:45,  1.69it/s]Extractor Estimating: 80it [00:46,  1.66it/s]Extractor Estimating: 81it [00:46,  1.69it/s]Extractor Estimating: 82it [00:47,  1.64it/s]Extractor Estimating: 83it [00:48,  1.66it/s]Extractor Estimating: 84it [00:48,  1.69it/s]Extractor Estimating: 85it [00:49,  1.70it/s]Extractor Estimating: 86it [00:49,  1.71it/s]Extractor Estimating: 87it [00:50,  1.69it/s]Extractor Estimating: 88it [00:51,  1.68it/s]Extractor Estimating: 89it [00:51,  1.64it/s]Extractor Estimating: 90it [00:52,  1.68it/s]Extractor Estimating: 91it [00:52,  1.75it/s]Extractor Estimating: 92it [00:53,  1.70it/s]Extractor Estimating: 93it [00:54,  1.56it/s]Extractor Estimating: 94it [00:54,  1.67it/s]Extractor Estimating: 95it [00:55,  1.69it/s]Extractor Estimating: 96it [00:55,  1.67it/s]Extractor Estimating: 97it [00:56,  1.71it/s]Extractor Estimating: 98it [00:57,  1.66it/s]Extractor Estimating: 99it [00:57,  1.61it/s]Extractor Estimating: 100it [00:58,  1.61it/s]Extractor Estimating: 101it [00:58,  1.73it/s]Extractor Estimating: 102it [00:59,  1.84it/s]Extractor Estimating: 103it [00:59,  1.92it/s]Extractor Estimating: 104it [01:00,  1.97it/s]Extractor Estimating: 105it [01:00,  2.04it/s]Extractor Estimating: 106it [01:01,  2.08it/s]Extractor Estimating: 107it [01:01,  2.03it/s]Extractor Estimating: 108it [01:02,  2.07it/s]Extractor Estimating: 109it [01:02,  2.01it/s]Extractor Estimating: 110it [01:03,  2.01it/s]Extractor Estimating: 111it [01:03,  2.07it/s]Extractor Estimating: 112it [01:04,  2.06it/s]Extractor Estimating: 113it [01:04,  2.13it/s]Extractor Estimating: 114it [01:05,  2.14it/s]Extractor Estimating: 115it [01:05,  2.12it/s]Extractor Estimating: 116it [01:05,  2.15it/s]Extractor Estimating: 117it [01:06,  2.16it/s]Extractor Estimating: 118it [01:06,  2.13it/s]Extractor Estimating: 119it [01:07,  2.17it/s]Extractor Estimating: 120it [01:07,  2.18it/s]Extractor Estimating: 121it [01:08,  2.17it/s]Extractor Estimating: 122it [01:08,  2.17it/s]Extractor Estimating: 123it [01:09,  2.14it/s]Extractor Estimating: 124it [01:09,  2.15it/s]Extractor Estimating: 125it [01:10,  2.12it/s]Extractor Estimating: 126it [01:10,  2.03it/s]Extractor Estimating: 127it [01:11,  2.00it/s]Extractor Estimating: 128it [01:11,  1.96it/s]Extractor Estimating: 129it [01:12,  1.89it/s]Extractor Estimating: 130it [01:12,  1.90it/s]Extractor Estimating: 131it [01:13,  1.87it/s]Extractor Estimating: 132it [01:13,  1.89it/s]Extractor Estimating: 133it [01:14,  1.86it/s]Extractor Estimating: 134it [01:15,  1.84it/s]Extractor Estimating: 135it [01:15,  1.86it/s]Extractor Estimating: 136it [01:16,  1.84it/s]Extractor Estimating: 137it [01:16,  1.91it/s]Extractor Estimating: 138it [01:17,  1.91it/s]Extractor Estimating: 139it [01:17,  1.94it/s]Extractor Estimating: 140it [01:18,  1.97it/s]Extractor Estimating: 141it [01:18,  1.90it/s]Extractor Estimating: 142it [01:19,  1.77it/s]Extractor Estimating: 143it [01:19,  1.84it/s]Extractor Estimating: 144it [01:20,  1.89it/s]Extractor Estimating: 145it [01:20,  1.79it/s]Extractor Estimating: 146it [01:21,  1.76it/s]Extractor Estimating: 147it [01:22,  1.78it/s]Extractor Estimating: 148it [01:22,  1.76it/s]Extractor Estimating: 149it [01:23,  1.76it/s]Extractor Estimating: 150it [01:23,  1.82it/s]Extractor Estimating: 151it [01:24,  1.88it/s]Extractor Estimating: 152it [01:24,  1.89it/s]Extractor Estimating: 153it [01:25,  1.93it/s]Extractor Estimating: 154it [01:25,  1.99it/s]Extractor Estimating: 155it [01:26,  1.75it/s]Extractor Estimating: 156it [01:26,  1.83it/s]Extractor Estimating: 157it [01:27,  1.91it/s]Extractor Estimating: 158it [01:27,  1.93it/s]Extractor Estimating: 159it [01:28,  1.93it/s]Extractor Estimating: 160it [01:28,  1.93it/s]Extractor Estimating: 161it [01:29,  1.91it/s]Extractor Estimating: 162it [01:29,  1.90it/s]Extractor Estimating: 163it [01:30,  1.93it/s]Extractor Estimating: 164it [01:30,  2.02it/s]Extractor Estimating: 165it [01:31,  2.05it/s]Extractor Estimating: 166it [01:31,  2.02it/s]Extractor Estimating: 167it [01:32,  2.03it/s]Extractor Estimating: 168it [01:32,  1.96it/s]Extractor Estimating: 169it [01:33,  1.97it/s]Extractor Estimating: 170it [01:33,  1.97it/s]Extractor Estimating: 171it [01:34,  1.98it/s]Extractor Estimating: 172it [01:34,  1.96it/s]Extractor Estimating: 173it [01:35,  1.89it/s]Extractor Estimating: 174it [01:36,  1.84it/s]Extractor Estimating: 175it [01:36,  1.81it/s]Extractor Estimating: 176it [01:37,  1.87it/s]Extractor Estimating: 177it [01:37,  1.95it/s]Extractor Estimating: 178it [01:38,  1.89it/s]Extractor Estimating: 179it [01:38,  1.93it/s]Extractor Estimating: 180it [01:39,  1.93it/s]Extractor Estimating: 181it [01:39,  1.98it/s]Extractor Estimating: 182it [01:40,  2.01it/s]Extractor Estimating: 183it [01:40,  2.04it/s]Extractor Estimating: 184it [01:41,  2.07it/s]Extractor Estimating: 185it [01:41,  2.09it/s]Extractor Estimating: 186it [01:42,  2.05it/s]Extractor Estimating: 187it [01:42,  1.95it/s]Extractor Estimating: 188it [01:43,  1.89it/s]Extractor Estimating: 189it [01:43,  1.92it/s]Extractor Estimating: 190it [01:44,  1.97it/s]Extractor Estimating: 191it [01:44,  2.04it/s]Extractor Estimating: 192it [01:45,  2.02it/s]Extractor Estimating: 193it [01:45,  2.03it/s]Extractor Estimating: 194it [01:46,  1.93it/s]Extractor Estimating: 195it [01:46,  1.95it/s]Extractor Estimating: 196it [01:47,  1.98it/s]Extractor Estimating: 197it [01:47,  1.95it/s]Extractor Estimating: 198it [01:48,  2.01it/s]Extractor Estimating: 199it [01:48,  1.98it/s]Extractor Estimating: 200it [01:49,  1.84it/s]Extractor Estimating: 201it [01:49,  1.88it/s]Extractor Estimating: 202it [01:50,  1.92it/s]Extractor Estimating: 203it [01:50,  1.86it/s]Extractor Estimating: 204it [01:51,  1.73it/s]Extractor Estimating: 205it [01:52,  1.78it/s]Extractor Estimating: 206it [01:52,  1.80it/s]Extractor Estimating: 207it [01:53,  1.81it/s]Extractor Estimating: 208it [01:53,  1.86it/s]Extractor Estimating: 209it [01:54,  1.88it/s]Extractor Estimating: 210it [01:54,  1.80it/s]Extractor Estimating: 211it [01:55,  1.83it/s]Extractor Estimating: 212it [01:55,  1.81it/s]Extractor Estimating: 213it [01:56,  1.86it/s]Extractor Estimating: 214it [01:57,  1.88it/s]Extractor Estimating: 215it [01:57,  1.93it/s]Extractor Estimating: 216it [01:58,  1.85it/s]Extractor Estimating: 217it [01:58,  1.90it/s]Extractor Estimating: 218it [01:59,  1.85it/s]Extractor Estimating: 219it [01:59,  1.84it/s]Extractor Estimating: 220it [02:00,  1.83it/s]Extractor Estimating: 221it [02:00,  1.92it/s]Extractor Estimating: 222it [02:01,  1.92it/s]Extractor Estimating: 223it [02:01,  1.90it/s]Extractor Estimating: 224it [02:02,  1.84it/s]Extractor Estimating: 225it [02:02,  1.80it/s]Extractor Estimating: 226it [02:03,  1.70it/s]Extractor Estimating: 227it [02:04,  1.63it/s]Extractor Estimating: 228it [02:04,  1.67it/s]Extractor Estimating: 229it [02:05,  1.63it/s]Extractor Estimating: 230it [02:06,  1.67it/s]Extractor Estimating: 231it [02:06,  1.59it/s]Extractor Estimating: 232it [02:07,  1.52it/s]Extractor Estimating: 233it [02:08,  1.59it/s]Extractor Estimating: 234it [02:08,  1.59it/s]Extractor Estimating: 235it [02:09,  1.58it/s]Extractor Estimating: 236it [02:09,  1.62it/s]Extractor Estimating: 237it [02:10,  1.58it/s]Extractor Estimating: 238it [02:11,  1.60it/s]Extractor Estimating: 239it [02:11,  1.59it/s]Extractor Estimating: 240it [02:12,  1.66it/s]Extractor Estimating: 241it [02:12,  1.66it/s]Extractor Estimating: 242it [02:13,  1.61it/s]Extractor Estimating: 243it [02:14,  1.64it/s]Extractor Estimating: 244it [02:14,  1.67it/s]Extractor Estimating: 245it [02:15,  1.70it/s]Extractor Estimating: 246it [02:15,  1.68it/s]Extractor Estimating: 247it [02:16,  1.63it/s]Extractor Estimating: 248it [02:17,  1.72it/s]Extractor Estimating: 249it [02:17,  1.68it/s]Extractor Estimating: 250it [02:18,  1.67it/s]Extractor Estimating: 251it [02:18,  1.63it/s]Extractor Estimating: 252it [02:19,  1.55it/s]Extractor Estimating: 253it [02:20,  1.52it/s]Extractor Estimating: 254it [02:21,  1.55it/s]Extractor Estimating: 255it [02:21,  1.55it/s]Extractor Estimating: 256it [02:22,  1.58it/s]Extractor Estimating: 257it [02:22,  1.59it/s]Extractor Estimating: 258it [02:23,  1.59it/s]Extractor Estimating: 259it [02:24,  1.61it/s]Extractor Estimating: 260it [02:24,  1.60it/s]Extractor Estimating: 261it [02:25,  1.58it/s]Extractor Estimating: 262it [02:26,  1.58it/s]Extractor Estimating: 263it [02:26,  1.65it/s]Extractor Estimating: 264it [02:27,  1.58it/s]Extractor Estimating: 265it [02:27,  1.57it/s]Extractor Estimating: 266it [02:28,  1.58it/s]Extractor Estimating: 267it [02:29,  1.54it/s]Extractor Estimating: 268it [02:29,  1.56it/s]Extractor Estimating: 269it [02:30,  1.55it/s]Extractor Estimating: 270it [02:31,  1.53it/s]Extractor Estimating: 271it [02:31,  1.52it/s]Extractor Estimating: 272it [02:32,  1.54it/s]Extractor Estimating: 273it [02:33,  1.55it/s]Extractor Estimating: 274it [02:33,  1.53it/s]Extractor Estimating: 275it [02:34,  1.56it/s]Extractor Estimating: 276it [02:34,  1.59it/s]Extractor Estimating: 277it [02:35,  1.64it/s]Extractor Estimating: 278it [02:36,  1.69it/s]Extractor Estimating: 279it [02:36,  1.73it/s]Extractor Estimating: 280it [02:37,  1.73it/s]Extractor Estimating: 281it [02:37,  1.78it/s]Extractor Estimating: 282it [02:38,  1.81it/s]Extractor Estimating: 283it [02:38,  1.82it/s]Extractor Estimating: 284it [02:39,  1.78it/s]Extractor Estimating: 285it [02:39,  1.76it/s]Extractor Estimating: 286it [02:40,  1.87it/s]Extractor Estimating: 287it [02:41,  1.86it/s]Extractor Estimating: 288it [02:41,  1.84it/s]Extractor Estimating: 289it [02:42,  1.83it/s]Extractor Estimating: 290it [02:42,  1.81it/s]Extractor Estimating: 291it [02:43,  1.85it/s]Extractor Estimating: 292it [02:43,  1.79it/s]Extractor Estimating: 293it [02:44,  1.62it/s]Extractor Estimating: 294it [02:45,  1.67it/s]Extractor Estimating: 295it [02:45,  1.69it/s]Extractor Estimating: 296it [02:46,  1.72it/s]Extractor Estimating: 297it [02:46,  1.73it/s]Extractor Estimating: 298it [02:47,  1.71it/s]Extractor Estimating: 299it [02:48,  1.69it/s]Extractor Estimating: 300it [02:48,  1.72it/s]Extractor Estimating: 301it [02:49,  1.78it/s]Extractor Estimating: 302it [02:49,  1.89it/s]Extractor Estimating: 303it [02:50,  1.96it/s]Extractor Estimating: 304it [02:50,  1.93it/s]Extractor Estimating: 305it [02:51,  2.01it/s]Extractor Estimating: 306it [02:51,  2.11it/s]Extractor Estimating: 307it [02:51,  2.19it/s]Extractor Estimating: 308it [02:52,  2.15it/s]Extractor Estimating: 309it [02:52,  2.17it/s]Extractor Estimating: 310it [02:53,  2.19it/s]Extractor Estimating: 311it [02:53,  2.24it/s]Extractor Estimating: 312it [02:54,  2.22it/s]Extractor Estimating: 313it [02:54,  2.23it/s]Extractor Estimating: 314it [02:54,  2.22it/s]Extractor Estimating: 315it [02:55,  2.25it/s]Extractor Estimating: 316it [02:55,  2.28it/s]Extractor Estimating: 317it [02:56,  2.22it/s]Extractor Estimating: 318it [02:56,  2.25it/s]Extractor Estimating: 319it [02:57,  2.29it/s]Extractor Estimating: 320it [02:57,  2.25it/s]Extractor Estimating: 321it [02:58,  2.15it/s]Extractor Estimating: 322it [02:58,  2.14it/s]Extractor Estimating: 323it [02:59,  2.12it/s]Extractor Estimating: 324it [02:59,  2.17it/s]Extractor Estimating: 325it [03:00,  2.18it/s]Extractor Estimating: 326it [03:00,  2.06it/s]Extractor Estimating: 327it [03:01,  1.96it/s]Extractor Estimating: 328it [03:01,  1.81it/s]Extractor Estimating: 329it [03:02,  1.78it/s]Extractor Estimating: 330it [03:02,  1.72it/s]Extractor Estimating: 331it [03:03,  1.76it/s]Extractor Estimating: 332it [03:04,  1.73it/s]Extractor Estimating: 333it [03:04,  1.76it/s]Extractor Estimating: 334it [03:05,  1.73it/s]Extractor Estimating: 335it [03:05,  1.71it/s]Extractor Estimating: 336it [03:06,  1.73it/s]Extractor Estimating: 337it [03:07,  1.71it/s]Extractor Estimating: 338it [03:07,  1.67it/s]Extractor Estimating: 339it [03:08,  1.66it/s]Extractor Estimating: 340it [03:08,  1.70it/s]Extractor Estimating: 341it [03:09,  1.77it/s]Extractor Estimating: 342it [03:09,  1.79it/s]Extractor Estimating: 343it [03:10,  1.77it/s]Extractor Estimating: 344it [03:11,  1.77it/s]Extractor Estimating: 345it [03:11,  1.79it/s]Extractor Estimating: 346it [03:12,  1.74it/s]Extractor Estimating: 347it [03:12,  1.70it/s]Extractor Estimating: 348it [03:13,  1.66it/s]Extractor Estimating: 349it [03:13,  1.70it/s]Extractor Estimating: 350it [03:14,  1.72it/s]Extractor Estimating: 351it [03:15,  1.73it/s]Extractor Estimating: 352it [03:15,  1.78it/s]Extractor Estimating: 353it [03:16,  1.81it/s]Extractor Estimating: 354it [03:16,  1.77it/s]Extractor Estimating: 355it [03:17,  1.81it/s]Extractor Estimating: 356it [03:17,  1.85it/s]Extractor Estimating: 357it [03:18,  1.75it/s]Extractor Estimating: 358it [03:19,  1.76it/s]Extractor Estimating: 359it [03:19,  1.77it/s]Extractor Estimating: 360it [03:20,  1.79it/s]Extractor Estimating: 361it [03:20,  1.80it/s]Extractor Estimating: 362it [03:21,  1.82it/s]Extractor Estimating: 363it [03:21,  1.77it/s]Extractor Estimating: 364it [03:22,  1.78it/s]Extractor Estimating: 365it [03:22,  1.81it/s]Extractor Estimating: 366it [03:23,  1.81it/s]Extractor Estimating: 367it [03:23,  1.85it/s]Extractor Estimating: 368it [03:24,  1.86it/s]Extractor Estimating: 369it [03:25,  1.87it/s]Extractor Estimating: 370it [03:25,  1.91it/s]Extractor Estimating: 371it [03:26,  1.82it/s]Extractor Estimating: 372it [03:26,  1.81it/s]Extractor Estimating: 373it [03:27,  1.80it/s]Extractor Estimating: 374it [03:27,  1.78it/s]Extractor Estimating: 375it [03:28,  1.76it/s]Extractor Estimating: 376it [03:28,  1.75it/s]Extractor Estimating: 377it [03:29,  1.66it/s]Extractor Estimating: 378it [03:30,  1.56it/s]Extractor Estimating: 379it [03:30,  1.60it/s]Extractor Estimating: 380it [03:31,  1.60it/s]Extractor Estimating: 381it [03:32,  1.59it/s]Extractor Estimating: 382it [03:32,  1.56it/s]Extractor Estimating: 383it [03:33,  1.61it/s]Extractor Estimating: 384it [03:34,  1.60it/s]Extractor Estimating: 385it [03:34,  1.66it/s]Extractor Estimating: 386it [03:35,  1.64it/s]Extractor Estimating: 387it [03:35,  1.63it/s]Extractor Estimating: 388it [03:36,  1.43it/s]Extractor Estimating: 389it [03:37,  1.47it/s]Extractor Estimating: 390it [03:38,  1.54it/s]Extractor Estimating: 391it [03:38,  1.52it/s]Extractor Estimating: 392it [03:39,  1.52it/s]Extractor Estimating: 393it [03:40,  1.52it/s]Extractor Estimating: 394it [03:40,  1.52it/s]Extractor Estimating: 395it [03:41,  1.56it/s]Extractor Estimating: 396it [03:41,  1.58it/s]Extractor Estimating: 397it [03:42,  1.61it/s]Extractor Estimating: 398it [03:43,  1.61it/s]Extractor Estimating: 399it [03:43,  1.68it/s]Extractor Estimating: 400it [03:44,  1.64it/s]Extractor Estimating: 401it [03:44,  1.65it/s]Extractor Estimating: 402it [03:45,  1.65it/s]Extractor Estimating: 403it [03:46,  1.61it/s]Extractor Estimating: 404it [03:46,  1.64it/s]Extractor Estimating: 405it [03:47,  1.64it/s]Extractor Estimating: 406it [03:47,  1.69it/s]Extractor Estimating: 407it [03:48,  1.72it/s]Extractor Estimating: 408it [03:49,  1.73it/s]Extractor Estimating: 409it [03:49,  1.73it/s]Extractor Estimating: 410it [03:50,  1.69it/s]Extractor Estimating: 411it [03:50,  1.64it/s]Extractor Estimating: 412it [03:51,  1.70it/s]Extractor Estimating: 413it [03:52,  1.66it/s]Extractor Estimating: 414it [03:52,  1.69it/s]Extractor Estimating: 415it [03:53,  1.72it/s]Extractor Estimating: 416it [03:53,  1.71it/s]Extractor Estimating: 417it [03:54,  1.70it/s]Extractor Estimating: 418it [03:54,  1.68it/s]Extractor Estimating: 419it [03:55,  1.69it/s]Extractor Estimating: 420it [03:56,  1.69it/s]Extractor Estimating: 421it [03:56,  1.66it/s]Extractor Estimating: 422it [03:57,  1.72it/s]Extractor Estimating: 423it [03:57,  1.72it/s]Extractor Estimating: 424it [03:58,  1.67it/s]Extractor Estimating: 425it [03:59,  1.63it/s]Extractor Estimating: 426it [03:59,  1.65it/s]Extractor Estimating: 427it [04:00,  1.64it/s]Extractor Estimating: 428it [04:01,  1.62it/s]Extractor Estimating: 429it [04:01,  1.68it/s]Extractor Estimating: 430it [04:02,  1.71it/s]Extractor Estimating: 431it [04:02,  1.71it/s]Extractor Estimating: 432it [04:03,  1.74it/s]Extractor Estimating: 433it [04:03,  1.70it/s]Extractor Estimating: 434it [04:04,  1.66it/s]Extractor Estimating: 435it [04:05,  1.64it/s]Extractor Estimating: 436it [04:05,  1.66it/s]Extractor Estimating: 437it [04:06,  1.62it/s]Extractor Estimating: 438it [04:06,  1.64it/s]Extractor Estimating: 439it [04:07,  1.67it/s]Extractor Estimating: 440it [04:08,  1.67it/s]Extractor Estimating: 441it [04:08,  1.68it/s]Extractor Estimating: 442it [04:09,  1.66it/s]Extractor Estimating: 443it [04:09,  1.62it/s]Extractor Estimating: 444it [04:10,  1.60it/s]Extractor Estimating: 445it [04:11,  1.63it/s]Extractor Estimating: 446it [04:11,  1.65it/s]Extractor Estimating: 447it [04:12,  1.64it/s]Extractor Estimating: 448it [04:12,  1.68it/s]Extractor Estimating: 449it [04:13,  1.63it/s]Extractor Estimating: 450it [04:14,  1.71it/s]Extractor Estimating: 451it [04:14,  1.75it/s]Extractor Estimating: 452it [04:15,  1.78it/s]Extractor Estimating: 453it [04:15,  1.80it/s]Extractor Estimating: 454it [04:16,  1.83it/s]Extractor Estimating: 455it [04:16,  1.89it/s]Extractor Estimating: 456it [04:17,  1.85it/s]Extractor Estimating: 457it [04:17,  1.87it/s]Extractor Estimating: 458it [04:18,  1.91it/s]Extractor Estimating: 459it [04:18,  1.92it/s]Extractor Estimating: 460it [04:19,  1.90it/s]Extractor Estimating: 461it [04:19,  1.92it/s]Extractor Estimating: 462it [04:20,  1.87it/s]Extractor Estimating: 463it [04:20,  1.94it/s]Extractor Estimating: 464it [04:21,  1.95it/s]Extractor Estimating: 465it [04:22,  1.87it/s]Extractor Estimating: 466it [04:22,  1.81it/s]Extractor Estimating: 467it [04:23,  1.83it/s]Extractor Estimating: 468it [04:23,  1.86it/s]Extractor Estimating: 469it [04:24,  1.86it/s]Extractor Estimating: 470it [04:24,  1.88it/s]Extractor Estimating: 471it [04:25,  1.86it/s]Extractor Estimating: 472it [04:25,  1.92it/s]Extractor Estimating: 473it [04:26,  1.63it/s]Extractor Estimating: 474it [04:27,  1.69it/s]Extractor Estimating: 475it [04:27,  1.66it/s]Extractor Estimating: 476it [04:28,  1.69it/s]Extractor Estimating: 477it [04:28,  1.71it/s]Extractor Estimating: 478it [04:29,  1.74it/s]Extractor Estimating: 479it [04:30,  1.78it/s]Extractor Estimating: 480it [04:30,  1.84it/s]Extractor Estimating: 481it [04:31,  1.85it/s]Extractor Estimating: 482it [04:31,  1.85it/s]Extractor Estimating: 483it [04:32,  1.83it/s]Extractor Estimating: 484it [04:32,  1.82it/s]Extractor Estimating: 485it [04:33,  1.82it/s]Extractor Estimating: 486it [04:33,  1.82it/s]Extractor Estimating: 487it [04:34,  1.81it/s]Extractor Estimating: 488it [04:34,  1.81it/s]Extractor Estimating: 489it [04:35,  1.81it/s]Extractor Estimating: 490it [04:36,  1.80it/s]Extractor Estimating: 491it [04:36,  1.81it/s]Extractor Estimating: 492it [04:37,  1.84it/s]Extractor Estimating: 493it [04:37,  1.86it/s]Extractor Estimating: 494it [04:38,  1.84it/s]Extractor Estimating: 495it [04:38,  1.83it/s]Extractor Estimating: 496it [04:39,  1.85it/s]Extractor Estimating: 497it [04:39,  1.82it/s]Extractor Estimating: 498it [04:40,  1.80it/s]Extractor Estimating: 499it [04:40,  1.77it/s]Extractor Estimating: 500it [04:41,  1.95it/s]Extractor Estimating: 500it [04:41,  1.78it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,089 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,089 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,089 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:40,089 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 01:56:40,880 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 01:56:40,881 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:56:41,452 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 01:56:42,515 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:56:42,515 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,362 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 01:56:45,364 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 01:56:46,022 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 01:56:46,023 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 01:56:46,596 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 01:56:46,773 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 01:56:46,773 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 03:34:20,073 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 03:34:20,115 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 6000, 'num_train': 4000}
num of filtered data: 6006 mean pseudo reward: 0.9816563494722228
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 14755
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14855, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=14855, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.947, loss:252.0152
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.920, loss:237.3095
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 49, avg_time 0.948, loss:216.5477
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 149, avg_time 0.935, loss:219.0539
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 249, avg_time 0.935, loss:218.4792
>> valid entity prec:0.6083, rec:0.5334, f1:0.5684
>> valid relation prec:0.2265, rec:0.1295, f1:0.1648
>> valid relation with NER prec:0.2265, rec:0.1295, f1:0.1648
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 98, avg_time 2.107, loss:189.6163
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 198, avg_time 0.935, loss:206.8874
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 47, avg_time 0.927, loss:184.7585
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 147, avg_time 0.949, loss:204.5273
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 247, avg_time 0.928, loss:230.1957
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5628, rec:0.5408, f1:0.5515
>> valid relation prec:0.1889, rec:0.1202, f1:0.1469
>> valid relation with NER prec:0.1889, rec:0.1202, f1:0.1469
g_step 1100, step 96, avg_time 2.110, loss:178.7332
g_step 1200, step 196, avg_time 0.922, loss:205.8685
g_step 1300, step 45, avg_time 0.916, loss:179.8485
g_step 1400, step 145, avg_time 0.936, loss:187.2567
g_step 1500, step 245, avg_time 0.942, loss:175.0891
>> valid entity prec:0.5952, rec:0.5280, f1:0.5596
>> valid relation prec:0.2036, rec:0.1303, f1:0.1589
>> valid relation with NER prec:0.2036, rec:0.1303, f1:0.1589
g_step 1600, step 94, avg_time 2.102, loss:166.9184
g_step 1700, step 194, avg_time 0.949, loss:172.0026
g_step 1800, step 43, avg_time 0.926, loss:159.1554
g_step 1900, step 143, avg_time 0.930, loss:160.2977
g_step 2000, step 243, avg_time 0.939, loss:166.7850
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5651, rec:0.5192, f1:0.5412
>> valid relation prec:0.1996, rec:0.1158, f1:0.1466
>> valid relation with NER prec:0.1996, rec:0.1158, f1:0.1466
g_step 2100, step 92, avg_time 2.097, loss:134.9686
g_step 2200, step 192, avg_time 0.938, loss:149.6851
g_step 2300, step 41, avg_time 0.930, loss:142.4881
g_step 2400, step 141, avg_time 0.932, loss:129.5278
g_step 2500, step 241, avg_time 0.936, loss:135.3819
>> valid entity prec:0.5682, rec:0.5682, f1:0.5682
>> valid relation prec:0.1790, rec:0.1263, f1:0.1481
>> valid relation with NER prec:0.1790, rec:0.1263, f1:0.1481
g_step 2600, step 90, avg_time 2.089, loss:126.5583
g_step 2700, step 190, avg_time 0.944, loss:135.2258
g_step 2800, step 39, avg_time 0.926, loss:144.2714
g_step 2900, step 139, avg_time 0.924, loss:113.1842
g_step 3000, step 239, avg_time 0.940, loss:126.4118
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5537, rec:0.5824, f1:0.5677
>> valid relation prec:0.1817, rec:0.1306, f1:0.1520
>> valid relation with NER prec:0.1817, rec:0.1306, f1:0.1520
g_step 3100, step 88, avg_time 2.117, loss:119.2686
g_step 3200, step 188, avg_time 0.931, loss:112.3273
g_step 3300, step 37, avg_time 0.927, loss:109.1548
g_step 3400, step 137, avg_time 0.928, loss:113.6072
g_step 3500, step 237, avg_time 0.935, loss:126.9153
>> valid entity prec:0.5772, rec:0.5114, f1:0.5423
>> valid relation prec:0.1936, rec:0.1187, f1:0.1472
>> valid relation with NER prec:0.1936, rec:0.1187, f1:0.1472
g_step 3600, step 86, avg_time 2.088, loss:95.8546
g_step 3700, step 186, avg_time 0.934, loss:118.2492
g_step 3800, step 35, avg_time 0.935, loss:104.5455
g_step 3900, step 135, avg_time 0.933, loss:105.2725
g_step 4000, step 235, avg_time 0.928, loss:101.7013
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5733, rec:0.5570, f1:0.5651
>> valid relation prec:0.1788, rec:0.1283, f1:0.1494
>> valid relation with NER prec:0.1788, rec:0.1283, f1:0.1494
g_step 4100, step 84, avg_time 2.102, loss:99.3065
g_step 4200, step 184, avg_time 0.927, loss:104.2353
g_step 4300, step 33, avg_time 0.929, loss:90.3542
g_step 4400, step 133, avg_time 0.938, loss:92.7831
g_step 4500, step 233, avg_time 0.929, loss:98.7407
>> valid entity prec:0.5600, rec:0.5290, f1:0.5441
>> valid relation prec:0.1836, rec:0.1283, f1:0.1510
>> valid relation with NER prec:0.1836, rec:0.1283, f1:0.1510
g_step 4600, step 82, avg_time 2.101, loss:90.9568
g_step 4700, step 182, avg_time 0.938, loss:102.4115
g_step 4800, step 31, avg_time 0.915, loss:89.5033
g_step 4900, step 131, avg_time 0.932, loss:98.4778
g_step 5000, step 231, avg_time 0.923, loss:90.0331
learning rate was adjusted to 0.0008
>> valid entity prec:0.5725, rec:0.4755, f1:0.5195
>> valid relation prec:0.1984, rec:0.1210, f1:0.1504
>> valid relation with NER prec:0.1984, rec:0.1210, f1:0.1504
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 03:34:20 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 03:34:20 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_03-34-20_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 03:34:21 - WARNING - datasets.builder -   Using custom data configuration default-2c5c8fb14ce218ac
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2c5c8fb14ce218ac/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 03:34:21,366 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:34:21,367 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:34:21,368 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:34:21,368 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:34:21,385 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:34:21,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:34:21,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:34:21,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:34:21,388 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:34:21,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:34:21,388 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 03:34:21,542 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:34:24,508 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 03:34:24,513 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2c5c8fb14ce218ac/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:02,  2.68ba/s] 29%|██▊       | 2/7 [00:00<00:01,  3.76ba/s] 43%|████▎     | 3/7 [00:00<00:00,  4.34ba/s] 57%|█████▋    | 4/7 [00:00<00:00,  4.64ba/s] 71%|███████▏  | 5/7 [00:01<00:00,  4.88ba/s] 86%|████████▌ | 6/7 [00:01<00:00,  5.00ba/s]100%|██████████| 7/7 [00:01<00:00,  5.27ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.44ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.63ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.68ba/s]100%|██████████| 4/4 [00:00<00:00,  5.40ba/s]
  0%|          | 0/7 [00:00<?, ?ba/s] 14%|█▍        | 1/7 [00:00<00:00,  9.74ba/s] 43%|████▎     | 3/7 [00:00<00:00, 11.02ba/s] 71%|███████▏  | 5/7 [00:00<00:00, 11.28ba/s]100%|██████████| 7/7 [00:00<00:00, 12.98ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.44ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.53ba/s]100%|██████████| 4/4 [00:00<00:00, 11.79ba/s]
[INFO|trainer.py:414] 2023-08-29 03:34:27,815 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 03:34:27,832 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 03:34:27,832 >>   Num examples = 6009
[INFO|trainer.py:1149] 2023-08-29 03:34:27,832 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 03:34:27,832 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 03:34:27,832 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 03:34:27,832 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 03:34:27,832 >>   Total optimization steps = 470
  0%|          | 0/470 [00:00<?, ?it/s]  0%|          | 1/470 [00:00<02:21,  3.32it/s]  0%|          | 2/470 [00:00<02:16,  3.42it/s]  1%|          | 3/470 [00:00<02:14,  3.46it/s]  1%|          | 4/470 [00:01<02:14,  3.48it/s]  1%|          | 5/470 [00:01<02:13,  3.48it/s]  1%|▏         | 6/470 [00:01<02:13,  3.49it/s]  1%|▏         | 7/470 [00:02<02:12,  3.49it/s]  2%|▏         | 8/470 [00:02<02:12,  3.49it/s]  2%|▏         | 9/470 [00:02<02:11,  3.49it/s]  2%|▏         | 10/470 [00:02<02:11,  3.49it/s]  2%|▏         | 11/470 [00:03<02:11,  3.48it/s]  3%|▎         | 12/470 [00:03<02:11,  3.49it/s]  3%|▎         | 13/470 [00:03<02:10,  3.49it/s]  3%|▎         | 14/470 [00:04<02:10,  3.49it/s]  3%|▎         | 15/470 [00:04<02:10,  3.49it/s]  3%|▎         | 16/470 [00:04<02:09,  3.50it/s]  4%|▎         | 17/470 [00:04<02:09,  3.50it/s]  4%|▍         | 18/470 [00:05<02:09,  3.50it/s]  4%|▍         | 19/470 [00:05<02:08,  3.50it/s]  4%|▍         | 20/470 [00:05<02:08,  3.50it/s]  4%|▍         | 21/470 [00:06<02:08,  3.50it/s]  5%|▍         | 22/470 [00:06<02:08,  3.49it/s]  5%|▍         | 23/470 [00:06<02:08,  3.49it/s]  5%|▌         | 24/470 [00:06<02:07,  3.49it/s]  5%|▌         | 25/470 [00:07<02:07,  3.49it/s]  6%|▌         | 26/470 [00:07<02:07,  3.49it/s]  6%|▌         | 27/470 [00:07<02:06,  3.49it/s]  6%|▌         | 28/470 [00:08<02:06,  3.50it/s]  6%|▌         | 29/470 [00:08<02:06,  3.49it/s]  6%|▋         | 30/470 [00:08<02:05,  3.49it/s]  7%|▋         | 31/470 [00:08<02:05,  3.49it/s]  7%|▋         | 32/470 [00:09<02:05,  3.49it/s]  7%|▋         | 33/470 [00:09<02:05,  3.48it/s]  7%|▋         | 34/470 [00:09<02:05,  3.48it/s]  7%|▋         | 35/470 [00:10<02:04,  3.48it/s]  8%|▊         | 36/470 [00:10<02:04,  3.49it/s]  8%|▊         | 37/470 [00:10<02:04,  3.49it/s]  8%|▊         | 38/470 [00:10<02:03,  3.49it/s]  8%|▊         | 39/470 [00:11<02:03,  3.49it/s]  9%|▊         | 40/470 [00:11<02:03,  3.49it/s]  9%|▊         | 41/470 [00:11<02:02,  3.49it/s]  9%|▉         | 42/470 [00:12<02:02,  3.49it/s]  9%|▉         | 43/470 [00:12<02:02,  3.49it/s]  9%|▉         | 44/470 [00:12<02:02,  3.47it/s] 10%|▉         | 45/470 [00:12<02:02,  3.48it/s] 10%|▉         | 46/470 [00:13<02:01,  3.48it/s] 10%|█         | 47/470 [00:13<02:01,  3.48it/s] 10%|█         | 48/470 [00:13<02:00,  3.49it/s] 10%|█         | 49/470 [00:14<02:00,  3.49it/s] 11%|█         | 50/470 [00:14<02:00,  3.49it/s] 11%|█         | 51/470 [00:14<02:00,  3.49it/s] 11%|█         | 52/470 [00:14<01:59,  3.49it/s] 11%|█▏        | 53/470 [00:15<01:59,  3.48it/s] 11%|█▏        | 54/470 [00:15<01:59,  3.49it/s] 12%|█▏        | 55/470 [00:15<01:59,  3.48it/s] 12%|█▏        | 56/470 [00:16<01:58,  3.49it/s] 12%|█▏        | 57/470 [00:16<01:58,  3.48it/s] 12%|█▏        | 58/470 [00:16<01:58,  3.49it/s] 13%|█▎        | 59/470 [00:16<01:57,  3.49it/s] 13%|█▎        | 60/470 [00:17<01:57,  3.49it/s] 13%|█▎        | 61/470 [00:17<01:57,  3.49it/s] 13%|█▎        | 62/470 [00:17<01:56,  3.49it/s] 13%|█▎        | 63/470 [00:18<01:56,  3.49it/s] 14%|█▎        | 64/470 [00:18<01:56,  3.49it/s] 14%|█▍        | 65/470 [00:18<01:56,  3.49it/s] 14%|█▍        | 66/470 [00:18<01:55,  3.48it/s] 14%|█▍        | 67/470 [00:19<01:55,  3.48it/s] 14%|█▍        | 68/470 [00:19<01:55,  3.49it/s] 15%|█▍        | 69/470 [00:19<01:55,  3.48it/s] 15%|█▍        | 70/470 [00:20<01:54,  3.49it/s] 15%|█▌        | 71/470 [00:20<01:54,  3.49it/s] 15%|█▌        | 72/470 [00:20<01:54,  3.49it/s] 16%|█▌        | 73/470 [00:20<01:53,  3.49it/s] 16%|█▌        | 74/470 [00:21<01:53,  3.49it/s] 16%|█▌        | 75/470 [00:21<01:53,  3.49it/s] 16%|█▌        | 76/470 [00:21<01:52,  3.49it/s] 16%|█▋        | 77/470 [00:22<01:52,  3.48it/s] 17%|█▋        | 78/470 [00:22<01:52,  3.48it/s] 17%|█▋        | 79/470 [00:22<01:52,  3.49it/s] 17%|█▋        | 80/470 [00:22<01:51,  3.48it/s] 17%|█▋        | 81/470 [00:23<01:51,  3.49it/s] 17%|█▋        | 82/470 [00:23<01:51,  3.49it/s] 18%|█▊        | 83/470 [00:23<01:50,  3.49it/s] 18%|█▊        | 84/470 [00:24<01:50,  3.48it/s] 18%|█▊        | 85/470 [00:24<01:50,  3.48it/s] 18%|█▊        | 86/470 [00:24<01:50,  3.48it/s] 19%|█▊        | 87/470 [00:24<01:49,  3.49it/s] 19%|█▊        | 88/470 [00:25<01:49,  3.49it/s] 19%|█▉        | 89/470 [00:25<01:49,  3.49it/s] 19%|█▉        | 90/470 [00:25<01:48,  3.49it/s] 19%|█▉        | 91/470 [00:26<01:48,  3.49it/s] 20%|█▉        | 92/470 [00:26<01:48,  3.48it/s] 20%|█▉        | 93/470 [00:26<01:48,  3.48it/s] 20%|██        | 94/470 [00:26<01:44,  3.59it/s][INFO|trainer.py:2140] 2023-08-29 03:34:54,770 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:34:54,770 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 03:34:54,770 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.35it/s][A
  3%|▎         | 12/431 [00:00<00:08, 48.99it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.11it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.22it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.83it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.44it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.35it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.31it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.27it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.23it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.32it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.25it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.21it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.07it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.05it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 45.00it/s][A
 20%|██        | 87/431 [00:01<00:07, 45.09it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 45.04it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.21it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.25it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.26it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.10it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.02it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 44.99it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 45.03it/s][A
 31%|███       | 132/431 [00:02<00:06, 45.04it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 45.07it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.24it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.29it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.24it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.17it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.04it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.05it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 45.04it/s][A
 41%|████      | 177/431 [00:03<00:05, 45.06it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 45.18it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.29it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.24it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.17it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.12it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.01it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 44.96it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.99it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 45.06it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 45.13it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 45.25it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.35it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.30it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.17it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.06it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 44.84it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.99it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 45.01it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 45.11it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.20it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.33it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.26it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.10it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.10it/s][A
 70%|███████   | 302/431 [00:06<00:02, 44.96it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.98it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.97it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 45.05it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.17it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.28it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.28it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.17it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.06it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.02it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 45.00it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 45.05it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 45.07it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 45.13it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.28it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.18it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.16it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.02it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.06it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.01it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 45.01it/s][A
 94%|█████████▍| 407/431 [00:08<00:00, 45.06it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.13it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.20it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.18it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.15it/s][A
                                                 [A                                                
100%|██████████| 431/431 [00:09<00:00, 45.15it/s][A 20%|██        | 94/470 [00:36<01:44,  3.59it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:35:04,337 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-94
[INFO|configuration_utils.py:351] 2023-08-29 03:35:04,357 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-94/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:35:05,918 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-94/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:35:05,940 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-94/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:35:05,953 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-94/special_tokens_map.json
 20%|██        | 95/470 [00:41<29:01,  4.65s/it] 20%|██        | 96/470 [00:42<20:49,  3.34s/it] 21%|██        | 97/470 [00:42<15:04,  2.42s/it] 21%|██        | 98/470 [00:42<11:04,  1.79s/it] 21%|██        | 99/470 [00:42<08:15,  1.34s/it] 21%|██▏       | 100/470 [00:43<06:18,  1.02s/it] 21%|██▏       | 101/470 [00:43<04:56,  1.24it/s] 22%|██▏       | 102/470 [00:43<03:59,  1.54it/s] 22%|██▏       | 103/470 [00:44<03:19,  1.84it/s] 22%|██▏       | 104/470 [00:44<02:51,  2.14it/s] 22%|██▏       | 105/470 [00:44<02:31,  2.41it/s] 23%|██▎       | 106/470 [00:44<02:17,  2.64it/s] 23%|██▎       | 107/470 [00:45<02:08,  2.82it/s] 23%|██▎       | 108/470 [00:45<02:01,  2.98it/s] 23%|██▎       | 109/470 [00:45<01:56,  3.10it/s] 23%|██▎       | 110/470 [00:46<01:52,  3.19it/s] 24%|██▎       | 111/470 [00:46<01:50,  3.26it/s] 24%|██▍       | 112/470 [00:46<01:48,  3.31it/s] 24%|██▍       | 113/470 [00:47<01:46,  3.35it/s] 24%|██▍       | 114/470 [00:47<01:45,  3.37it/s] 24%|██▍       | 115/470 [00:47<01:44,  3.39it/s] 25%|██▍       | 116/470 [00:47<01:44,  3.40it/s] 25%|██▍       | 117/470 [00:48<01:43,  3.41it/s] 25%|██▌       | 118/470 [00:48<01:43,  3.41it/s] 25%|██▌       | 119/470 [00:48<01:42,  3.41it/s] 26%|██▌       | 120/470 [00:49<01:42,  3.42it/s] 26%|██▌       | 121/470 [00:49<01:42,  3.42it/s] 26%|██▌       | 122/470 [00:49<01:41,  3.43it/s] 26%|██▌       | 123/470 [00:49<01:41,  3.42it/s] 26%|██▋       | 124/470 [00:50<01:40,  3.43it/s] 27%|██▋       | 125/470 [00:50<01:40,  3.43it/s] 27%|██▋       | 126/470 [00:50<01:40,  3.43it/s] 27%|██▋       | 127/470 [00:51<01:40,  3.43it/s] 27%|██▋       | 128/470 [00:51<01:40,  3.42it/s] 27%|██▋       | 129/470 [00:51<01:39,  3.41it/s] 28%|██▊       | 130/470 [00:51<01:39,  3.42it/s] 28%|██▊       | 131/470 [00:52<01:41,  3.34it/s] 28%|██▊       | 132/470 [00:52<01:40,  3.37it/s] 28%|██▊       | 133/470 [00:52<01:39,  3.39it/s] 29%|██▊       | 134/470 [00:53<01:38,  3.40it/s] 29%|██▊       | 135/470 [00:53<01:38,  3.41it/s] 29%|██▉       | 136/470 [00:53<01:37,  3.41it/s] 29%|██▉       | 137/470 [00:54<01:37,  3.41it/s] 29%|██▉       | 138/470 [00:54<01:37,  3.42it/s] 30%|██▉       | 139/470 [00:54<01:36,  3.42it/s] 30%|██▉       | 140/470 [00:54<01:36,  3.43it/s] 30%|███       | 141/470 [00:55<01:36,  3.42it/s] 30%|███       | 142/470 [00:55<01:35,  3.43it/s] 30%|███       | 143/470 [00:55<01:35,  3.42it/s] 31%|███       | 144/470 [00:56<01:35,  3.43it/s] 31%|███       | 145/470 [00:56<01:34,  3.43it/s] 31%|███       | 146/470 [00:56<01:34,  3.42it/s] 31%|███▏      | 147/470 [00:56<01:34,  3.42it/s] 31%|███▏      | 148/470 [00:57<01:34,  3.40it/s] 32%|███▏      | 149/470 [00:57<01:34,  3.41it/s] 32%|███▏      | 150/470 [00:57<01:33,  3.42it/s] 32%|███▏      | 151/470 [00:58<01:33,  3.42it/s] 32%|███▏      | 152/470 [00:58<01:32,  3.42it/s] 33%|███▎      | 153/470 [00:58<01:32,  3.43it/s] 33%|███▎      | 154/470 [00:59<01:32,  3.42it/s] 33%|███▎      | 155/470 [00:59<01:31,  3.43it/s] 33%|███▎      | 156/470 [00:59<01:31,  3.43it/s] 33%|███▎      | 157/470 [00:59<01:31,  3.42it/s] 34%|███▎      | 158/470 [01:00<01:31,  3.42it/s] 34%|███▍      | 159/470 [01:00<01:31,  3.41it/s] 34%|███▍      | 160/470 [01:00<01:30,  3.41it/s] 34%|███▍      | 161/470 [01:01<01:30,  3.42it/s] 34%|███▍      | 162/470 [01:01<01:30,  3.42it/s] 35%|███▍      | 163/470 [01:01<01:29,  3.42it/s] 35%|███▍      | 164/470 [01:01<01:29,  3.43it/s] 35%|███▌      | 165/470 [01:02<01:29,  3.42it/s] 35%|███▌      | 166/470 [01:02<01:28,  3.43it/s] 36%|███▌      | 167/470 [01:02<01:28,  3.43it/s] 36%|███▌      | 168/470 [01:03<01:28,  3.43it/s] 36%|███▌      | 169/470 [01:03<01:27,  3.43it/s] 36%|███▌      | 170/470 [01:03<01:28,  3.41it/s] 36%|███▋      | 171/470 [01:03<01:27,  3.41it/s] 37%|███▋      | 172/470 [01:04<01:27,  3.42it/s] 37%|███▋      | 173/470 [01:04<01:26,  3.42it/s] 37%|███▋      | 174/470 [01:04<01:26,  3.42it/s] 37%|███▋      | 175/470 [01:05<01:26,  3.42it/s] 37%|███▋      | 176/470 [01:05<01:25,  3.42it/s] 38%|███▊      | 177/470 [01:05<01:25,  3.42it/s] 38%|███▊      | 178/470 [01:06<01:25,  3.43it/s] 38%|███▊      | 179/470 [01:06<01:24,  3.43it/s] 38%|███▊      | 180/470 [01:06<01:24,  3.42it/s] 39%|███▊      | 181/470 [01:06<01:24,  3.41it/s] 39%|███▊      | 182/470 [01:07<01:24,  3.42it/s] 39%|███▉      | 183/470 [01:07<01:23,  3.42it/s] 39%|███▉      | 184/470 [01:07<01:23,  3.43it/s] 39%|███▉      | 185/470 [01:08<01:22,  3.44it/s] 40%|███▉      | 186/470 [01:08<01:22,  3.45it/s] 40%|███▉      | 187/470 [01:08<01:21,  3.46it/s] 40%|████      | 188/470 [01:08<01:19,  3.57it/s][INFO|trainer.py:2140] 2023-08-29 03:35:36,748 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:35:36,748 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 03:35:36,748 >>   Batch size = 8
{'eval_loss': 1.0576757192611694, 'eval_runtime': 9.55, 'eval_samples_per_second': 360.839, 'eval_steps_per_second': 45.131, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.83it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.37it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.66it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.58it/s][A
  6%|▋         | 27/431 [00:00<00:08, 46.07it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.57it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.35it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.14it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.14it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.40it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.46it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.46it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.28it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.14it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 45.04it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.88it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 45.07it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.08it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.25it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.37it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.42it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.36it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.17it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.99it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.91it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.99it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.14it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.25it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.27it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.31it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.32it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.18it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.99it/s][A
 41%|████      | 177/431 [00:03<00:05, 45.05it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.98it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.00it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.10it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.13it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.21it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.37it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.30it/s][A
 50%|█████     | 217/431 [00:04<00:04, 45.24it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 45.15it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 45.07it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 45.06it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.11it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.21it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.24it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.33it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.29it/s][A
 61%|██████    | 262/431 [00:05<00:03, 45.27it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 45.22it/s][A
 63%|██████▎   | 272/431 [00:05<00:03, 45.17it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.08it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 44.97it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.17it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.26it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.31it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.28it/s][A
 71%|███████   | 307/431 [00:06<00:02, 45.26it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 45.25it/s][A
 74%|███████▎  | 317/431 [00:06<00:02, 45.21it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.12it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 44.99it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.10it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.27it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.30it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.32it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 45.21it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 45.29it/s][A
 84%|████████▍ | 362/431 [00:07<00:01, 45.26it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 45.07it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.05it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 44.90it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.22it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.31it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.31it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.34it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 45.22it/s][A
 94%|█████████▍| 407/431 [00:08<00:00, 45.25it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.08it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 44.95it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.03it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.08it/s][A                                                 
                                                 [A 40%|████      | 188/470 [01:18<01:19,  3.57it/s]
100%|██████████| 431/431 [00:09<00:00, 45.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:35:46,318 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-188
[INFO|configuration_utils.py:351] 2023-08-29 03:35:46,346 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-188/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:35:48,107 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-188/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:35:48,123 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-188/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:35:48,138 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-188/special_tokens_map.json
 40%|████      | 189/470 [01:24<22:16,  4.76s/it] 40%|████      | 190/470 [01:24<15:56,  3.42s/it] 41%|████      | 191/470 [01:24<11:31,  2.48s/it] 41%|████      | 192/470 [01:24<08:26,  1.82s/it] 41%|████      | 193/470 [01:25<06:17,  1.36s/it] 41%|████▏     | 194/470 [01:25<04:47,  1.04s/it] 41%|████▏     | 195/470 [01:25<03:44,  1.22it/s] 42%|████▏     | 196/470 [01:26<03:00,  1.52it/s] 42%|████▏     | 197/470 [01:26<02:29,  1.82it/s] 42%|████▏     | 198/470 [01:26<02:08,  2.12it/s] 42%|████▏     | 199/470 [01:27<01:53,  2.39it/s] 43%|████▎     | 200/470 [01:27<01:42,  2.63it/s] 43%|████▎     | 201/470 [01:27<01:35,  2.83it/s] 43%|████▎     | 202/470 [01:27<01:30,  2.97it/s] 43%|████▎     | 203/470 [01:28<01:26,  3.10it/s] 43%|████▎     | 204/470 [01:28<01:23,  3.19it/s] 44%|████▎     | 205/470 [01:28<01:21,  3.26it/s] 44%|████▍     | 206/470 [01:29<01:19,  3.31it/s] 44%|████▍     | 207/470 [01:29<01:18,  3.34it/s] 44%|████▍     | 208/470 [01:29<01:17,  3.37it/s] 44%|████▍     | 209/470 [01:29<01:17,  3.38it/s] 45%|████▍     | 210/470 [01:30<01:16,  3.40it/s] 45%|████▍     | 211/470 [01:30<01:15,  3.41it/s] 45%|████▌     | 212/470 [01:30<01:15,  3.42it/s] 45%|████▌     | 213/470 [01:31<01:15,  3.40it/s] 46%|████▌     | 214/470 [01:31<01:15,  3.41it/s] 46%|████▌     | 215/470 [01:31<01:14,  3.41it/s] 46%|████▌     | 216/470 [01:31<01:14,  3.42it/s] 46%|████▌     | 217/470 [01:32<01:13,  3.42it/s] 46%|████▋     | 218/470 [01:32<01:13,  3.42it/s] 47%|████▋     | 219/470 [01:32<01:13,  3.42it/s] 47%|████▋     | 220/470 [01:33<01:13,  3.42it/s] 47%|████▋     | 221/470 [01:33<01:12,  3.42it/s] 47%|████▋     | 222/470 [01:33<01:12,  3.42it/s] 47%|████▋     | 223/470 [01:34<01:12,  3.43it/s] 48%|████▊     | 224/470 [01:34<01:11,  3.42it/s] 48%|████▊     | 225/470 [01:34<01:11,  3.42it/s] 48%|████▊     | 226/470 [01:34<01:11,  3.43it/s] 48%|████▊     | 227/470 [01:35<01:10,  3.42it/s] 49%|████▊     | 228/470 [01:35<01:10,  3.42it/s] 49%|████▊     | 229/470 [01:35<01:10,  3.43it/s] 49%|████▉     | 230/470 [01:36<01:10,  3.43it/s] 49%|████▉     | 231/470 [01:36<01:09,  3.43it/s] 49%|████▉     | 232/470 [01:36<01:09,  3.42it/s] 50%|████▉     | 233/470 [01:36<01:09,  3.42it/s] 50%|████▉     | 234/470 [01:37<01:08,  3.42it/s] 50%|█████     | 235/470 [01:37<01:09,  3.40it/s] 50%|█████     | 236/470 [01:37<01:08,  3.41it/s] 50%|█████     | 237/470 [01:38<01:08,  3.42it/s] 51%|█████     | 238/470 [01:38<01:07,  3.42it/s] 51%|█████     | 239/470 [01:38<01:07,  3.42it/s] 51%|█████     | 240/470 [01:39<01:07,  3.42it/s] 51%|█████▏    | 241/470 [01:39<01:06,  3.42it/s] 51%|█████▏    | 242/470 [01:39<01:06,  3.42it/s] 52%|█████▏    | 243/470 [01:39<01:06,  3.42it/s] 52%|█████▏    | 244/470 [01:40<01:06,  3.42it/s] 52%|█████▏    | 245/470 [01:40<01:05,  3.43it/s] 52%|█████▏    | 246/470 [01:40<01:05,  3.42it/s] 53%|█████▎    | 247/470 [01:41<01:05,  3.42it/s] 53%|█████▎    | 248/470 [01:41<01:04,  3.42it/s] 53%|█████▎    | 249/470 [01:41<01:04,  3.43it/s] 53%|█████▎    | 250/470 [01:41<01:04,  3.43it/s] 53%|█████▎    | 251/470 [01:42<01:03,  3.43it/s] 54%|█████▎    | 252/470 [01:42<01:03,  3.43it/s] 54%|█████▍    | 253/470 [01:42<01:03,  3.42it/s] 54%|█████▍    | 254/470 [01:43<01:03,  3.43it/s] 54%|█████▍    | 255/470 [01:43<01:02,  3.43it/s] 54%|█████▍    | 256/470 [01:43<01:02,  3.42it/s] 55%|█████▍    | 257/470 [01:43<01:02,  3.42it/s] 55%|█████▍    | 258/470 [01:44<01:01,  3.42it/s] 55%|█████▌    | 259/470 [01:44<01:01,  3.42it/s] 55%|█████▌    | 260/470 [01:44<01:01,  3.42it/s] 56%|█████▌    | 261/470 [01:45<01:01,  3.42it/s] 56%|█████▌    | 262/470 [01:45<01:00,  3.42it/s] 56%|█████▌    | 263/470 [01:45<01:00,  3.42it/s] 56%|█████▌    | 264/470 [01:46<01:00,  3.43it/s] 56%|█████▋    | 265/470 [01:46<00:59,  3.43it/s] 57%|█████▋    | 266/470 [01:46<00:59,  3.42it/s] 57%|█████▋    | 267/470 [01:46<00:59,  3.42it/s] 57%|█████▋    | 268/470 [01:47<00:59,  3.41it/s] 57%|█████▋    | 269/470 [01:47<00:58,  3.41it/s] 57%|█████▋    | 270/470 [01:47<00:58,  3.42it/s] 58%|█████▊    | 271/470 [01:48<00:58,  3.42it/s] 58%|█████▊    | 272/470 [01:48<00:57,  3.42it/s] 58%|█████▊    | 273/470 [01:48<00:57,  3.42it/s] 58%|█████▊    | 274/470 [01:48<00:57,  3.42it/s] 59%|█████▊    | 275/470 [01:49<00:56,  3.42it/s] 59%|█████▊    | 276/470 [01:49<00:56,  3.42it/s] 59%|█████▉    | 277/470 [01:49<00:56,  3.43it/s] 59%|█████▉    | 278/470 [01:50<00:56,  3.43it/s] 59%|█████▉    | 279/470 [01:50<00:55,  3.42it/s] 60%|█████▉    | 280/470 [01:50<00:55,  3.42it/s] 60%|█████▉    | 281/470 [01:50<00:55,  3.42it/s] 60%|██████    | 282/470 [01:51<00:53,  3.52it/s][INFO|trainer.py:2140] 2023-08-29 03:36:19,094 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:36:19,095 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 03:36:19,095 >>   Batch size = 8
{'eval_loss': 1.069799542427063, 'eval_runtime': 9.5383, 'eval_samples_per_second': 361.281, 'eval_steps_per_second': 45.186, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.53it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.24it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.49it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.51it/s][A
  6%|▋         | 27/431 [00:00<00:08, 46.00it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.61it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.28it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.13it/s][A
 11%|█         | 47/431 [00:01<00:08, 43.79it/s][A
 12%|█▏        | 53/431 [00:01<00:08, 45.56it/s][A
 13%|█▎        | 58/431 [00:01<00:08, 45.54it/s][A
 15%|█▍        | 63/431 [00:01<00:08, 45.43it/s][A
 16%|█▌        | 68/431 [00:01<00:07, 45.48it/s][A
 17%|█▋        | 73/431 [00:01<00:07, 45.37it/s][A
 18%|█▊        | 78/431 [00:01<00:07, 45.24it/s][A
 19%|█▉        | 83/431 [00:01<00:07, 45.04it/s][A
 20%|██        | 88/431 [00:01<00:07, 44.92it/s][A
 22%|██▏       | 93/431 [00:02<00:07, 44.92it/s][A
 23%|██▎       | 98/431 [00:02<00:07, 45.02it/s][A
 24%|██▍       | 103/431 [00:02<00:07, 45.25it/s][A
 25%|██▌       | 108/431 [00:02<00:07, 45.37it/s][A
 26%|██▌       | 113/431 [00:02<00:07, 45.26it/s][A
 27%|██▋       | 118/431 [00:02<00:06, 45.18it/s][A
 29%|██▊       | 123/431 [00:02<00:06, 45.21it/s][A
 30%|██▉       | 128/431 [00:02<00:06, 44.94it/s][A
 31%|███       | 133/431 [00:02<00:06, 44.86it/s][A
 32%|███▏      | 138/431 [00:03<00:06, 44.95it/s][A
 33%|███▎      | 143/431 [00:03<00:06, 45.05it/s][A
 34%|███▍      | 148/431 [00:03<00:06, 45.18it/s][A
 35%|███▌      | 153/431 [00:03<00:06, 45.29it/s][A
 37%|███▋      | 158/431 [00:03<00:06, 45.36it/s][A
 38%|███▊      | 163/431 [00:03<00:05, 45.37it/s][A
 39%|███▉      | 168/431 [00:03<00:05, 45.31it/s][A
 40%|████      | 173/431 [00:03<00:05, 45.08it/s][A
 41%|████▏     | 178/431 [00:03<00:05, 44.96it/s][A
 42%|████▏     | 183/431 [00:04<00:05, 44.97it/s][A
 44%|████▎     | 188/431 [00:04<00:05, 44.79it/s][A
 45%|████▍     | 193/431 [00:04<00:05, 44.95it/s][A
 46%|████▌     | 198/431 [00:04<00:05, 45.16it/s][A
 47%|████▋     | 203/431 [00:04<00:05, 45.32it/s][A
 48%|████▊     | 208/431 [00:04<00:04, 45.31it/s][A
 49%|████▉     | 213/431 [00:04<00:04, 45.32it/s][A
 51%|█████     | 218/431 [00:04<00:04, 45.16it/s][A
 52%|█████▏    | 223/431 [00:04<00:04, 45.03it/s][A
 53%|█████▎    | 228/431 [00:05<00:04, 44.96it/s][A
 54%|█████▍    | 233/431 [00:05<00:04, 44.89it/s][A
 55%|█████▌    | 238/431 [00:05<00:04, 45.02it/s][A
 56%|█████▋    | 243/431 [00:05<00:04, 45.13it/s][A
 58%|█████▊    | 248/431 [00:05<00:04, 45.35it/s][A
 59%|█████▊    | 253/431 [00:05<00:03, 45.36it/s][A
 60%|█████▉    | 258/431 [00:05<00:03, 45.29it/s][A
 61%|██████    | 263/431 [00:05<00:03, 45.23it/s][A
 62%|██████▏   | 268/431 [00:05<00:03, 45.19it/s][A
 63%|██████▎   | 273/431 [00:06<00:03, 45.07it/s][A
 65%|██████▍   | 278/431 [00:06<00:03, 44.95it/s][A
 66%|██████▌   | 283/431 [00:06<00:03, 45.04it/s][A
 67%|██████▋   | 288/431 [00:06<00:03, 45.11it/s][A
 68%|██████▊   | 293/431 [00:06<00:03, 45.25it/s][A
 69%|██████▉   | 298/431 [00:06<00:02, 45.21it/s][A
 70%|███████   | 303/431 [00:06<00:02, 44.93it/s][A
 71%|███████▏  | 308/431 [00:06<00:02, 45.47it/s][A
 73%|███████▎  | 313/431 [00:06<00:02, 45.40it/s][A
 74%|███████▍  | 318/431 [00:07<00:02, 45.13it/s][A
 75%|███████▍  | 323/431 [00:07<00:02, 45.12it/s][A
 76%|███████▌  | 328/431 [00:07<00:02, 45.15it/s][A
 77%|███████▋  | 333/431 [00:07<00:02, 45.14it/s][A
 78%|███████▊  | 338/431 [00:07<00:02, 45.10it/s][A
 80%|███████▉  | 343/431 [00:07<00:01, 45.15it/s][A
 81%|████████  | 348/431 [00:07<00:01, 45.32it/s][A
 82%|████████▏ | 353/431 [00:07<00:01, 45.31it/s][A
 83%|████████▎ | 358/431 [00:07<00:01, 45.31it/s][A
 84%|████████▍ | 363/431 [00:08<00:01, 45.14it/s][A
 85%|████████▌ | 368/431 [00:08<00:01, 45.04it/s][A
 87%|████████▋ | 373/431 [00:08<00:01, 45.07it/s][A
 88%|████████▊ | 378/431 [00:08<00:01, 45.17it/s][A
 89%|████████▉ | 383/431 [00:08<00:01, 45.15it/s][A
 90%|█████████ | 388/431 [00:08<00:00, 45.23it/s][A
 91%|█████████ | 393/431 [00:08<00:00, 45.27it/s][A
 92%|█████████▏| 398/431 [00:08<00:00, 45.35it/s][A
 94%|█████████▎| 403/431 [00:08<00:00, 45.28it/s][A
 95%|█████████▍| 408/431 [00:09<00:00, 45.18it/s][A
 96%|█████████▌| 413/431 [00:09<00:00, 45.06it/s][A
 97%|█████████▋| 418/431 [00:09<00:00, 45.07it/s][A
 98%|█████████▊| 423/431 [00:09<00:00, 45.07it/s][A
 99%|█████████▉| 428/431 [00:09<00:00, 45.18it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 45.18it/s][A 60%|██████    | 282/470 [02:00<00:53,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:36:28,653 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-282
[INFO|configuration_utils.py:351] 2023-08-29 03:36:28,676 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-282/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:36:30,430 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-282/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:36:30,446 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-282/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:36:30,458 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-282/special_tokens_map.json
 60%|██████    | 283/470 [02:06<14:47,  4.74s/it] 60%|██████    | 284/470 [02:06<10:34,  3.41s/it] 61%|██████    | 285/470 [02:07<07:37,  2.47s/it] 61%|██████    | 286/470 [02:07<05:34,  1.82s/it] 61%|██████    | 287/470 [02:07<04:09,  1.36s/it] 61%|██████▏   | 288/470 [02:07<03:09,  1.04s/it] 61%|██████▏   | 289/470 [02:08<02:27,  1.23it/s] 62%|██████▏   | 290/470 [02:08<01:58,  1.52it/s] 62%|██████▏   | 291/470 [02:08<01:38,  1.82it/s] 62%|██████▏   | 292/470 [02:09<01:23,  2.12it/s] 62%|██████▏   | 293/470 [02:09<01:13,  2.39it/s] 63%|██████▎   | 294/470 [02:09<01:06,  2.63it/s] 63%|██████▎   | 295/470 [02:09<01:02,  2.82it/s] 63%|██████▎   | 296/470 [02:10<00:58,  2.98it/s] 63%|██████▎   | 297/470 [02:10<00:55,  3.10it/s] 63%|██████▎   | 298/470 [02:10<00:53,  3.19it/s] 64%|██████▎   | 299/470 [02:11<00:52,  3.26it/s] 64%|██████▍   | 300/470 [02:11<00:51,  3.31it/s] 64%|██████▍   | 301/470 [02:11<00:50,  3.34it/s] 64%|██████▍   | 302/470 [02:11<00:49,  3.37it/s] 64%|██████▍   | 303/470 [02:12<00:49,  3.39it/s] 65%|██████▍   | 304/470 [02:12<00:48,  3.40it/s] 65%|██████▍   | 305/470 [02:12<00:48,  3.41it/s] 65%|██████▌   | 306/470 [02:13<00:48,  3.41it/s] 65%|██████▌   | 307/470 [02:13<00:47,  3.41it/s] 66%|██████▌   | 308/470 [02:13<00:47,  3.42it/s] 66%|██████▌   | 309/470 [02:14<00:47,  3.42it/s] 66%|██████▌   | 310/470 [02:14<00:46,  3.42it/s] 66%|██████▌   | 311/470 [02:14<00:46,  3.43it/s] 66%|██████▋   | 312/470 [02:14<00:46,  3.42it/s] 67%|██████▋   | 313/470 [02:15<00:45,  3.43it/s] 67%|██████▋   | 314/470 [02:15<00:45,  3.43it/s] 67%|██████▋   | 315/470 [02:15<00:45,  3.43it/s] 67%|██████▋   | 316/470 [02:16<00:44,  3.43it/s] 67%|██████▋   | 317/470 [02:16<00:44,  3.40it/s] 68%|██████▊   | 318/470 [02:16<00:44,  3.41it/s] 68%|██████▊   | 319/470 [02:16<00:44,  3.42it/s] 68%|██████▊   | 320/470 [02:17<00:43,  3.42it/s] 68%|██████▊   | 321/470 [02:17<00:43,  3.42it/s] 69%|██████▊   | 322/470 [02:17<00:43,  3.43it/s] 69%|██████▊   | 323/470 [02:18<00:42,  3.43it/s] 69%|██████▉   | 324/470 [02:18<00:42,  3.43it/s] 69%|██████▉   | 325/470 [02:18<00:42,  3.43it/s] 69%|██████▉   | 326/470 [02:18<00:42,  3.43it/s] 70%|██████▉   | 327/470 [02:19<00:41,  3.43it/s] 70%|██████▉   | 328/470 [02:19<00:41,  3.41it/s] 70%|███████   | 329/470 [02:19<00:41,  3.41it/s] 70%|███████   | 330/470 [02:20<00:40,  3.42it/s] 70%|███████   | 331/470 [02:20<00:40,  3.42it/s] 71%|███████   | 332/470 [02:20<00:40,  3.42it/s] 71%|███████   | 333/470 [02:21<00:40,  3.42it/s] 71%|███████   | 334/470 [02:21<00:39,  3.44it/s] 71%|███████▏  | 335/470 [02:21<00:39,  3.45it/s] 71%|███████▏  | 336/470 [02:21<00:38,  3.46it/s] 72%|███████▏  | 337/470 [02:22<00:38,  3.47it/s] 72%|███████▏  | 338/470 [02:22<00:38,  3.47it/s] 72%|███████▏  | 339/470 [02:22<00:37,  3.46it/s] 72%|███████▏  | 340/470 [02:23<00:37,  3.47it/s] 73%|███████▎  | 341/470 [02:23<00:37,  3.47it/s] 73%|███████▎  | 342/470 [02:23<00:36,  3.48it/s] 73%|███████▎  | 343/470 [02:23<00:36,  3.48it/s] 73%|███████▎  | 344/470 [02:24<00:36,  3.48it/s] 73%|███████▎  | 345/470 [02:24<00:35,  3.48it/s] 74%|███████▎  | 346/470 [02:24<00:35,  3.48it/s] 74%|███████▍  | 347/470 [02:25<00:35,  3.48it/s] 74%|███████▍  | 348/470 [02:25<00:35,  3.48it/s] 74%|███████▍  | 349/470 [02:25<00:34,  3.48it/s] 74%|███████▍  | 350/470 [02:25<00:34,  3.47it/s] 75%|███████▍  | 351/470 [02:26<00:34,  3.47it/s] 75%|███████▍  | 352/470 [02:26<00:33,  3.48it/s] 75%|███████▌  | 353/470 [02:26<00:33,  3.48it/s] 75%|███████▌  | 354/470 [02:27<00:33,  3.48it/s] 76%|███████▌  | 355/470 [02:27<00:33,  3.48it/s] 76%|███████▌  | 356/470 [02:27<00:32,  3.48it/s] 76%|███████▌  | 357/470 [02:27<00:32,  3.48it/s] 76%|███████▌  | 358/470 [02:28<00:32,  3.48it/s] 76%|███████▋  | 359/470 [02:28<00:31,  3.48it/s] 77%|███████▋  | 360/470 [02:28<00:31,  3.48it/s] 77%|███████▋  | 361/470 [02:29<00:31,  3.48it/s] 77%|███████▋  | 362/470 [02:29<00:31,  3.47it/s] 77%|███████▋  | 363/470 [02:29<00:30,  3.48it/s] 77%|███████▋  | 364/470 [02:29<00:30,  3.48it/s] 78%|███████▊  | 365/470 [02:30<00:30,  3.48it/s] 78%|███████▊  | 366/470 [02:30<00:29,  3.48it/s] 78%|███████▊  | 367/470 [02:30<00:29,  3.48it/s] 78%|███████▊  | 368/470 [02:31<00:29,  3.48it/s] 79%|███████▊  | 369/470 [02:31<00:28,  3.48it/s] 79%|███████▊  | 370/470 [02:31<00:28,  3.48it/s] 79%|███████▉  | 371/470 [02:31<00:28,  3.48it/s] 79%|███████▉  | 372/470 [02:32<00:28,  3.48it/s] 79%|███████▉  | 373/470 [02:32<00:27,  3.47it/s] 80%|███████▉  | 374/470 [02:32<00:27,  3.47it/s] 80%|███████▉  | 375/470 [02:33<00:27,  3.47it/s] 80%|████████  | 376/470 [02:33<00:26,  3.58it/s][INFO|trainer.py:2140] 2023-08-29 03:37:01,188 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:37:01,188 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 03:37:01,188 >>   Batch size = 8
{'eval_loss': 1.090328574180603, 'eval_runtime': 9.5439, 'eval_samples_per_second': 361.069, 'eval_steps_per_second': 45.16, 'epoch': 3.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.71it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.13it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.20it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.40it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.95it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.57it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.28it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.30it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.22it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.37it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.42it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.32it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.28it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.14it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.02it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 45.00it/s][A
 20%|██        | 87/431 [00:01<00:07, 45.13it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 45.16it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.33it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.35it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.31it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.29it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.19it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.02it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 45.03it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.99it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 45.15it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.23it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.30it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.29it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.29it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.14it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.00it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 45.00it/s][A
 41%|████      | 177/431 [00:03<00:05, 45.05it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 45.18it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.21it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.28it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.32it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.28it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.19it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.07it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.91it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 45.13it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 45.23it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 45.08it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.33it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.13it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.27it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.15it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.05it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.99it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 45.07it/s][A
 63%|██████▎   | 272/431 [00:05<00:03, 45.13it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.21it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.24it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.34it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.33it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.18it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.11it/s][A
 71%|███████   | 307/431 [00:06<00:02, 45.09it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 45.06it/s][A
 74%|███████▎  | 317/431 [00:06<00:02, 45.06it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.19it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.27it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.28it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.33it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.24it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.17it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 45.04it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 45.17it/s][A
 84%|████████▍ | 362/431 [00:07<00:01, 45.23it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 45.23it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.18it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.19it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.26it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.26it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.17it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.14it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 45.13it/s][A
 94%|█████████▍| 407/431 [00:08<00:00, 45.16it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.19it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.13it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.12it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.07it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 45.07it/s][A 80%|████████  | 376/470 [02:42<00:26,  3.58it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:37:10,753 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-376
[INFO|configuration_utils.py:351] 2023-08-29 03:37:10,772 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-376/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:37:12,708 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-376/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:37:12,725 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-376/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:37:12,740 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-376/special_tokens_map.json
 80%|████████  | 377/470 [02:48<07:24,  4.77s/it] 80%|████████  | 378/470 [02:48<05:15,  3.43s/it] 81%|████████  | 379/470 [02:49<03:46,  2.49s/it] 81%|████████  | 380/470 [02:49<02:44,  1.83s/it] 81%|████████  | 381/470 [02:49<02:01,  1.37s/it] 81%|████████▏ | 382/470 [02:50<01:32,  1.05s/it] 81%|████████▏ | 383/470 [02:50<01:11,  1.22it/s] 82%|████████▏ | 384/470 [02:50<00:56,  1.51it/s] 82%|████████▏ | 385/470 [02:50<00:46,  1.82it/s] 82%|████████▏ | 386/470 [02:51<00:39,  2.11it/s] 82%|████████▏ | 387/470 [02:51<00:34,  2.38it/s] 83%|████████▎ | 388/470 [02:51<00:31,  2.62it/s] 83%|████████▎ | 389/470 [02:52<00:28,  2.81it/s] 83%|████████▎ | 390/470 [02:52<00:27,  2.89it/s] 83%|████████▎ | 391/470 [02:52<00:26,  3.04it/s] 83%|████████▎ | 392/470 [02:53<00:24,  3.15it/s] 84%|████████▎ | 393/470 [02:53<00:23,  3.22it/s] 84%|████████▍ | 394/470 [02:53<00:23,  3.28it/s] 84%|████████▍ | 395/470 [02:53<00:22,  3.32it/s] 84%|████████▍ | 396/470 [02:54<00:22,  3.35it/s] 84%|████████▍ | 397/470 [02:54<00:21,  3.37it/s] 85%|████████▍ | 398/470 [02:54<00:21,  3.39it/s] 85%|████████▍ | 399/470 [02:55<00:20,  3.40it/s] 85%|████████▌ | 400/470 [02:55<00:20,  3.40it/s] 85%|████████▌ | 401/470 [02:55<00:20,  3.40it/s] 86%|████████▌ | 402/470 [02:55<00:19,  3.41it/s] 86%|████████▌ | 403/470 [02:56<00:19,  3.41it/s] 86%|████████▌ | 404/470 [02:56<00:19,  3.41it/s] 86%|████████▌ | 405/470 [02:56<00:19,  3.42it/s] 86%|████████▋ | 406/470 [02:57<00:18,  3.42it/s] 87%|████████▋ | 407/470 [02:57<00:18,  3.42it/s] 87%|████████▋ | 408/470 [02:57<00:18,  3.42it/s] 87%|████████▋ | 409/470 [02:58<00:17,  3.42it/s] 87%|████████▋ | 410/470 [02:58<00:17,  3.42it/s] 87%|████████▋ | 411/470 [02:58<00:17,  3.42it/s] 88%|████████▊ | 412/470 [02:58<00:16,  3.42it/s] 88%|████████▊ | 413/470 [02:59<00:16,  3.42it/s] 88%|████████▊ | 414/470 [02:59<00:16,  3.42it/s] 88%|████████▊ | 415/470 [02:59<00:16,  3.42it/s] 89%|████████▊ | 416/470 [03:00<00:15,  3.40it/s] 89%|████████▊ | 417/470 [03:00<00:15,  3.41it/s] 89%|████████▉ | 418/470 [03:00<00:15,  3.41it/s] 89%|████████▉ | 419/470 [03:00<00:14,  3.42it/s] 89%|████████▉ | 420/470 [03:01<00:14,  3.42it/s] 90%|████████▉ | 421/470 [03:01<00:14,  3.42it/s] 90%|████████▉ | 422/470 [03:01<00:14,  3.42it/s] 90%|█████████ | 423/470 [03:02<00:13,  3.42it/s] 90%|█████████ | 424/470 [03:02<00:13,  3.42it/s] 90%|█████████ | 425/470 [03:02<00:13,  3.42it/s] 91%|█████████ | 426/470 [03:02<00:12,  3.42it/s] 91%|█████████ | 427/470 [03:03<00:12,  3.41it/s] 91%|█████████ | 428/470 [03:03<00:12,  3.41it/s] 91%|█████████▏| 429/470 [03:03<00:12,  3.41it/s] 91%|█████████▏| 430/470 [03:04<00:11,  3.42it/s] 92%|█████████▏| 431/470 [03:04<00:11,  3.42it/s] 92%|█████████▏| 432/470 [03:04<00:11,  3.42it/s] 92%|█████████▏| 433/470 [03:05<00:10,  3.42it/s] 92%|█████████▏| 434/470 [03:05<00:10,  3.42it/s] 93%|█████████▎| 435/470 [03:05<00:10,  3.42it/s] 93%|█████████▎| 436/470 [03:05<00:09,  3.42it/s] 93%|█████████▎| 437/470 [03:06<00:09,  3.41it/s] 93%|█████████▎| 438/470 [03:06<00:09,  3.41it/s] 93%|█████████▎| 439/470 [03:06<00:09,  3.41it/s] 94%|█████████▎| 440/470 [03:07<00:08,  3.41it/s] 94%|█████████▍| 441/470 [03:07<00:08,  3.42it/s] 94%|█████████▍| 442/470 [03:07<00:08,  3.41it/s] 94%|█████████▍| 443/470 [03:07<00:07,  3.41it/s] 94%|█████████▍| 444/470 [03:08<00:07,  3.42it/s] 95%|█████████▍| 445/470 [03:08<00:07,  3.42it/s] 95%|█████████▍| 446/470 [03:08<00:07,  3.42it/s] 95%|█████████▌| 447/470 [03:09<00:06,  3.42it/s] 95%|█████████▌| 448/470 [03:09<00:06,  3.41it/s] 96%|█████████▌| 449/470 [03:09<00:06,  3.41it/s] 96%|█████████▌| 450/470 [03:10<00:05,  3.42it/s] 96%|█████████▌| 451/470 [03:10<00:05,  3.41it/s] 96%|█████████▌| 452/470 [03:10<00:05,  3.42it/s] 96%|█████████▋| 453/470 [03:10<00:04,  3.40it/s] 97%|█████████▋| 454/470 [03:11<00:04,  3.40it/s] 97%|█████████▋| 455/470 [03:11<00:04,  3.41it/s] 97%|█████████▋| 456/470 [03:11<00:04,  3.42it/s] 97%|█████████▋| 457/470 [03:12<00:03,  3.42it/s] 97%|█████████▋| 458/470 [03:12<00:03,  3.42it/s] 98%|█████████▊| 459/470 [03:12<00:03,  3.42it/s] 98%|█████████▊| 460/470 [03:12<00:02,  3.41it/s] 98%|█████████▊| 461/470 [03:13<00:02,  3.42it/s] 98%|█████████▊| 462/470 [03:13<00:02,  3.42it/s] 99%|█████████▊| 463/470 [03:13<00:02,  3.42it/s] 99%|█████████▊| 464/470 [03:14<00:01,  3.42it/s] 99%|█████████▉| 465/470 [03:14<00:01,  3.42it/s] 99%|█████████▉| 466/470 [03:14<00:01,  3.42it/s] 99%|█████████▉| 467/470 [03:14<00:00,  3.42it/s]100%|█████████▉| 468/470 [03:15<00:00,  3.41it/s]100%|█████████▉| 469/470 [03:15<00:00,  3.42it/s]100%|██████████| 470/470 [03:15<00:00,  3.52it/s][INFO|trainer.py:2140] 2023-08-29 03:37:43,676 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:37:43,676 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 03:37:43,676 >>   Batch size = 8
{'eval_loss': 1.0967092514038086, 'eval_runtime': 9.5372, 'eval_samples_per_second': 361.324, 'eval_steps_per_second': 45.192, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.97it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.49it/s][A
  4%|▍         | 18/431 [00:00<00:08, 47.66it/s][A
  5%|▌         | 23/431 [00:00<00:08, 46.78it/s][A
  6%|▋         | 28/431 [00:00<00:08, 46.16it/s][A
  8%|▊         | 33/431 [00:00<00:08, 45.64it/s][A
  9%|▉         | 38/431 [00:00<00:08, 45.40it/s][A
 10%|▉         | 43/431 [00:00<00:08, 45.18it/s][A
 11%|█         | 48/431 [00:01<00:08, 45.10it/s][A
 12%|█▏        | 53/431 [00:01<00:08, 45.20it/s][A
 13%|█▎        | 58/431 [00:01<00:08, 45.31it/s][A
 15%|█▍        | 63/431 [00:01<00:08, 45.43it/s][A
 16%|█▌        | 68/431 [00:01<00:07, 45.45it/s][A
 17%|█▋        | 73/431 [00:01<00:07, 45.34it/s][A
 18%|█▊        | 78/431 [00:01<00:07, 45.27it/s][A
 19%|█▉        | 83/431 [00:01<00:07, 45.01it/s][A
 20%|██        | 88/431 [00:01<00:07, 45.01it/s][A
 22%|██▏       | 93/431 [00:02<00:07, 45.05it/s][A
 23%|██▎       | 98/431 [00:02<00:07, 45.03it/s][A
 24%|██▍       | 103/431 [00:02<00:07, 45.11it/s][A
 25%|██▌       | 108/431 [00:02<00:07, 45.28it/s][A
 26%|██▌       | 113/431 [00:02<00:07, 45.28it/s][A
 27%|██▋       | 118/431 [00:02<00:06, 45.39it/s][A
 29%|██▊       | 123/431 [00:02<00:06, 45.19it/s][A
 30%|██▉       | 128/431 [00:02<00:06, 45.18it/s][A
 31%|███       | 133/431 [00:02<00:06, 45.12it/s][A
 32%|███▏      | 138/431 [00:03<00:06, 45.15it/s][A
 33%|███▎      | 143/431 [00:03<00:06, 45.09it/s][A
 34%|███▍      | 148/431 [00:03<00:06, 45.05it/s][A
 35%|███▌      | 153/431 [00:03<00:06, 45.12it/s][A
 37%|███▋      | 158/431 [00:03<00:06, 45.29it/s][A
 38%|███▊      | 163/431 [00:03<00:05, 45.37it/s][A
 39%|███▉      | 168/431 [00:03<00:05, 45.37it/s][A
 40%|████      | 173/431 [00:03<00:05, 45.20it/s][A
 41%|████▏     | 178/431 [00:03<00:05, 45.12it/s][A
 42%|████▏     | 183/431 [00:04<00:05, 45.20it/s][A
 44%|████▎     | 188/431 [00:04<00:05, 45.11it/s][A
 45%|████▍     | 193/431 [00:04<00:05, 45.00it/s][A
 46%|████▌     | 198/431 [00:04<00:05, 45.06it/s][A
 47%|████▋     | 203/431 [00:04<00:05, 45.21it/s][A
 48%|████▊     | 208/431 [00:04<00:04, 45.23it/s][A
 49%|████▉     | 213/431 [00:04<00:04, 45.35it/s][A
 51%|█████     | 218/431 [00:04<00:04, 45.27it/s][A
 52%|█████▏    | 223/431 [00:04<00:04, 45.35it/s][A
 53%|█████▎    | 228/431 [00:05<00:04, 45.20it/s][A
 54%|█████▍    | 233/431 [00:05<00:04, 45.13it/s][A
 55%|█████▌    | 238/431 [00:05<00:04, 45.10it/s][A
 56%|█████▋    | 243/431 [00:05<00:04, 45.04it/s][A
 58%|█████▊    | 248/431 [00:05<00:04, 45.15it/s][A
 59%|█████▊    | 253/431 [00:05<00:03, 45.23it/s][A
 60%|█████▉    | 258/431 [00:05<00:03, 45.29it/s][A
 61%|██████    | 263/431 [00:05<00:03, 45.31it/s][A
 62%|██████▏   | 268/431 [00:05<00:03, 45.30it/s][A
 63%|██████▎   | 273/431 [00:06<00:03, 45.21it/s][A
 65%|██████▍   | 278/431 [00:06<00:03, 45.22it/s][A
 66%|██████▌   | 283/431 [00:06<00:03, 45.12it/s][A
 67%|██████▋   | 288/431 [00:06<00:03, 45.11it/s][A
 68%|██████▊   | 293/431 [00:06<00:03, 45.14it/s][A
 69%|██████▉   | 298/431 [00:06<00:02, 45.23it/s][A
 70%|███████   | 303/431 [00:06<00:02, 45.30it/s][A
 71%|███████▏  | 308/431 [00:06<00:02, 45.31it/s][A
 73%|███████▎  | 313/431 [00:06<00:02, 45.26it/s][A
 74%|███████▍  | 318/431 [00:07<00:02, 45.27it/s][A
 75%|███████▍  | 323/431 [00:07<00:02, 45.16it/s][A
 76%|███████▌  | 328/431 [00:07<00:02, 45.08it/s][A
 77%|███████▋  | 333/431 [00:07<00:02, 45.14it/s][A
 78%|███████▊  | 338/431 [00:07<00:02, 45.12it/s][A
 80%|███████▉  | 343/431 [00:07<00:01, 45.20it/s][A
 81%|████████  | 348/431 [00:07<00:01, 45.30it/s][A
 82%|████████▏ | 353/431 [00:07<00:01, 45.25it/s][A
 83%|████████▎ | 358/431 [00:07<00:01, 45.22it/s][A
 84%|████████▍ | 363/431 [00:08<00:01, 45.31it/s][A
 85%|████████▌ | 368/431 [00:08<00:01, 45.22it/s][A
 87%|████████▋ | 373/431 [00:08<00:01, 45.25it/s][A
 88%|████████▊ | 378/431 [00:08<00:01, 45.17it/s][A
 89%|████████▉ | 383/431 [00:08<00:01, 45.19it/s][A
 90%|█████████ | 388/431 [00:08<00:00, 45.17it/s][A
 91%|█████████ | 393/431 [00:08<00:00, 45.28it/s][A
 92%|█████████▏| 398/431 [00:08<00:00, 45.31it/s][A
 94%|█████████▎| 403/431 [00:08<00:00, 45.32it/s][A
 95%|█████████▍| 408/431 [00:08<00:00, 45.30it/s][A
 96%|█████████▌| 413/431 [00:09<00:00, 45.21it/s][A
 97%|█████████▋| 418/431 [00:09<00:00, 45.15it/s][A
 98%|█████████▊| 423/431 [00:09<00:00, 45.19it/s][A
 99%|█████████▉| 428/431 [00:09<00:00, 45.17it/s][A
                                                 [A                                                 
100%|██████████| 431/431 [00:09<00:00, 45.17it/s][A100%|██████████| 470/470 [03:25<00:00,  3.52it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-29 03:37:53,219 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-470
[INFO|configuration_utils.py:351] 2023-08-29 03:37:53,242 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-470/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:37:55,475 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-470/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:37:55,497 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-470/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:37:55,510 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-470/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 03:38:00,239 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 03:38:00,243 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-94 (score: 1.0576757192611694).
                                                 100%|██████████| 470/470 [03:35<00:00,  3.52it/s]100%|██████████| 470/470 [03:35<00:00,  2.19it/s]
[INFO|trainer.py:1894] 2023-08-29 03:38:02,881 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-29 03:38:02,919 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 03:38:05,194 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 03:38:05,222 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 03:38:05,256 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:38:05,456 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:05,457 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:05,457 >>   train_loss               =     0.3953
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:05,457 >>   train_runtime            = 0:03:35.03
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:05,457 >>   train_samples            =       6009
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:05,457 >>   train_samples_per_second =    139.722
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:05,457 >>   train_steps_per_second   =      2.186
{'eval_loss': 1.10331130027771, 'eval_runtime': 9.5324, 'eval_samples_per_second': 361.503, 'eval_steps_per_second': 45.214, 'epoch': 5.0}
{'train_runtime': 215.0335, 'train_samples_per_second': 139.722, 'train_steps_per_second': 2.186, 'train_loss': 0.39533740104512965, 'epoch': 5.0}
08/29/2023 03:38:05 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 03:38:05,498 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 03:38:05,498 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 03:38:05,499 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 57.04it/s]  3%|▎         | 12/431 [00:00<00:08, 49.76it/s]  4%|▍         | 18/431 [00:00<00:08, 47.59it/s]  5%|▌         | 23/431 [00:00<00:08, 47.13it/s]  6%|▋         | 28/431 [00:00<00:08, 46.59it/s]  8%|▊         | 33/431 [00:00<00:08, 46.28it/s]  9%|▉         | 38/431 [00:00<00:08, 46.14it/s] 10%|▉         | 43/431 [00:00<00:08, 45.70it/s] 11%|█         | 48/431 [00:01<00:08, 44.88it/s] 12%|█▏        | 53/431 [00:01<00:08, 45.01it/s] 13%|█▎        | 58/431 [00:01<00:08, 45.05it/s] 15%|█▍        | 63/431 [00:01<00:08, 45.13it/s] 16%|█▌        | 68/431 [00:01<00:08, 45.30it/s] 17%|█▋        | 73/431 [00:01<00:07, 45.48it/s] 18%|█▊        | 78/431 [00:01<00:07, 45.61it/s] 19%|█▉        | 83/431 [00:01<00:07, 45.63it/s] 20%|██        | 88/431 [00:01<00:07, 45.42it/s] 22%|██▏       | 93/431 [00:02<00:07, 45.07it/s] 23%|██▎       | 98/431 [00:02<00:07, 44.84it/s] 24%|██▍       | 103/431 [00:02<00:07, 44.92it/s] 25%|██▌       | 108/431 [00:02<00:07, 44.95it/s] 26%|██▌       | 113/431 [00:02<00:07, 45.18it/s] 27%|██▋       | 118/431 [00:02<00:06, 45.34it/s] 29%|██▊       | 123/431 [00:02<00:06, 45.52it/s] 30%|██▉       | 128/431 [00:02<00:06, 45.59it/s] 31%|███       | 133/431 [00:02<00:06, 45.46it/s] 32%|███▏      | 138/431 [00:03<00:06, 45.06it/s] 33%|███▎      | 143/431 [00:03<00:06, 44.95it/s] 34%|███▍      | 148/431 [00:03<00:06, 44.88it/s] 35%|███▌      | 153/431 [00:03<00:06, 44.94it/s] 37%|███▋      | 158/431 [00:03<00:06, 45.08it/s] 38%|███▊      | 163/431 [00:03<00:05, 45.26it/s] 39%|███▉      | 168/431 [00:03<00:05, 45.43it/s] 40%|████      | 173/431 [00:03<00:05, 45.51it/s] 41%|████▏     | 178/431 [00:03<00:05, 45.44it/s] 42%|████▏     | 183/431 [00:04<00:05, 45.26it/s] 44%|████▎     | 188/431 [00:04<00:05, 44.96it/s] 45%|████▍     | 193/431 [00:04<00:05, 44.88it/s] 46%|████▌     | 198/431 [00:04<00:05, 44.92it/s] 47%|████▋     | 203/431 [00:04<00:05, 45.08it/s] 48%|████▊     | 208/431 [00:04<00:04, 45.30it/s] 49%|████▉     | 213/431 [00:04<00:04, 45.46it/s] 51%|█████     | 218/431 [00:04<00:04, 45.55it/s] 52%|█████▏    | 223/431 [00:04<00:04, 45.37it/s] 53%|█████▎    | 228/431 [00:05<00:04, 45.28it/s] 54%|█████▍    | 233/431 [00:05<00:04, 45.03it/s] 55%|█████▌    | 238/431 [00:05<00:04, 44.82it/s] 56%|█████▋    | 243/431 [00:05<00:04, 44.83it/s] 58%|█████▊    | 248/431 [00:05<00:04, 44.93it/s] 59%|█████▊    | 253/431 [00:05<00:03, 45.13it/s] 60%|█████▉    | 258/431 [00:05<00:03, 45.26it/s] 61%|██████    | 263/431 [00:05<00:03, 45.46it/s] 62%|██████▏   | 268/431 [00:05<00:03, 45.49it/s] 63%|██████▎   | 273/431 [00:06<00:03, 45.35it/s] 65%|██████▍   | 278/431 [00:06<00:03, 45.10it/s] 66%|██████▌   | 283/431 [00:06<00:03, 44.95it/s] 67%|██████▋   | 288/431 [00:06<00:03, 44.91it/s] 68%|██████▊   | 293/431 [00:06<00:03, 44.96it/s] 69%|██████▉   | 298/431 [00:06<00:02, 44.98it/s] 70%|███████   | 303/431 [00:06<00:02, 45.22it/s] 71%|███████▏  | 308/431 [00:06<00:02, 45.39it/s] 73%|███████▎  | 313/431 [00:06<00:02, 45.41it/s] 74%|███████▍  | 318/431 [00:07<00:02, 45.36it/s] 75%|███████▍  | 323/431 [00:07<00:02, 45.18it/s] 76%|███████▌  | 328/431 [00:07<00:02, 44.98it/s] 77%|███████▋  | 333/431 [00:07<00:02, 44.96it/s] 78%|███████▊  | 338/431 [00:07<00:02, 44.96it/s] 80%|███████▉  | 343/431 [00:07<00:01, 45.05it/s] 81%|████████  | 348/431 [00:07<00:01, 45.21it/s] 82%|████████▏ | 353/431 [00:07<00:01, 45.36it/s] 83%|████████▎ | 358/431 [00:07<00:01, 45.48it/s] 84%|████████▍ | 363/431 [00:08<00:01, 45.36it/s] 85%|████████▌ | 368/431 [00:08<00:01, 45.21it/s] 87%|████████▋ | 373/431 [00:08<00:01, 45.09it/s] 88%|████████▊ | 378/431 [00:08<00:01, 44.93it/s] 89%|████████▉ | 383/431 [00:08<00:01, 44.96it/s] 90%|█████████ | 388/431 [00:08<00:00, 45.01it/s] 91%|█████████ | 393/431 [00:08<00:00, 45.18it/s] 92%|█████████▏| 398/431 [00:08<00:00, 45.34it/s] 94%|█████████▎| 403/431 [00:08<00:00, 45.22it/s] 95%|█████████▍| 408/431 [00:08<00:00, 45.37it/s] 96%|█████████▌| 413/431 [00:09<00:00, 45.25it/s] 97%|█████████▋| 418/431 [00:09<00:00, 45.11it/s] 98%|█████████▊| 423/431 [00:09<00:00, 44.92it/s] 99%|█████████▉| 428/431 [00:09<00:00, 44.99it/s]100%|██████████| 431/431 [00:09<00:00, 45.28it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 03:38:15,035 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:15,035 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:15,035 >>   eval_loss               =     1.0577
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:15,035 >>   eval_runtime            = 0:00:09.53
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:15,035 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:15,035 >>   eval_samples_per_second =    361.367
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:15,035 >>   eval_steps_per_second   =     45.197
[INFO|trainer_pt_utils.py:913] 2023-08-29 03:38:15,035 >>   perplexity              =     2.8797
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:20,402 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:20,407 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:20,407 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:20,407 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:20,407 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:38:21,150 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:38:21,150 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:38:21,404 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:38:22,440 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:38:22,440 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:24,519 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:24,524 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:24,524 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:24,524 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:38:24,524 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:38:24,834 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:38:24,835 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:38:25,500 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:38:25,653 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:38:25,653 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-376
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-188
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-282
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-470
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/checkpoint-94
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.24it/s]Extractor Predicting: 2it [00:01,  1.41it/s]Extractor Predicting: 3it [00:02,  1.55it/s]Extractor Predicting: 4it [00:02,  1.63it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.52it/s]Extractor Predicting: 14it [00:09,  1.53it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:11,  1.52it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.60it/s]Extractor Predicting: 20it [00:12,  1.56it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.57it/s]Extractor Predicting: 23it [00:14,  1.59it/s]Extractor Predicting: 24it [00:15,  1.58it/s]Extractor Predicting: 25it [00:16,  1.54it/s]Extractor Predicting: 26it [00:16,  1.51it/s]Extractor Predicting: 27it [00:17,  1.50it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:18,  1.53it/s]Extractor Predicting: 30it [00:19,  1.52it/s]Extractor Predicting: 31it [00:20,  1.51it/s]Extractor Predicting: 32it [00:20,  1.53it/s]Extractor Predicting: 33it [00:21,  1.56it/s]Extractor Predicting: 34it [00:21,  1.61it/s]Extractor Predicting: 35it [00:22,  1.63it/s]Extractor Predicting: 36it [00:23,  1.65it/s]Extractor Predicting: 37it [00:23,  1.66it/s]Extractor Predicting: 38it [00:24,  1.62it/s]Extractor Predicting: 39it [00:25,  1.64it/s]Extractor Predicting: 40it [00:25,  1.64it/s]Extractor Predicting: 41it [00:26,  1.62it/s]Extractor Predicting: 42it [00:26,  1.66it/s]Extractor Predicting: 43it [00:27,  1.63it/s]Extractor Predicting: 44it [00:28,  1.63it/s]Extractor Predicting: 45it [00:28,  1.64it/s]Extractor Predicting: 46it [00:29,  1.65it/s]Extractor Predicting: 47it [00:29,  1.69it/s]Extractor Predicting: 48it [00:30,  1.70it/s]Extractor Predicting: 49it [00:31,  1.70it/s]Extractor Predicting: 50it [00:31,  1.67it/s]Extractor Predicting: 51it [00:32,  1.65it/s]Extractor Predicting: 52it [00:32,  1.63it/s]Extractor Predicting: 53it [00:33,  1.60it/s]Extractor Predicting: 54it [00:34,  1.64it/s]Extractor Predicting: 55it [00:34,  1.64it/s]Extractor Predicting: 56it [00:35,  1.66it/s]Extractor Predicting: 57it [00:35,  1.70it/s]Extractor Predicting: 58it [00:36,  1.67it/s]Extractor Predicting: 59it [00:37,  1.72it/s]Extractor Predicting: 60it [00:37,  1.66it/s]Extractor Predicting: 61it [00:38,  1.68it/s]Extractor Predicting: 62it [00:38,  1.74it/s]Extractor Predicting: 63it [00:39,  1.74it/s]Extractor Predicting: 64it [00:39,  1.77it/s]Extractor Predicting: 65it [00:40,  1.77it/s]Extractor Predicting: 66it [00:40,  1.79it/s]Extractor Predicting: 67it [00:41,  1.77it/s]Extractor Predicting: 68it [00:42,  1.77it/s]Extractor Predicting: 69it [00:42,  1.81it/s]Extractor Predicting: 70it [00:43,  1.78it/s]Extractor Predicting: 71it [00:43,  1.74it/s]Extractor Predicting: 72it [00:44,  1.77it/s]Extractor Predicting: 73it [00:45,  1.71it/s]Extractor Predicting: 74it [00:45,  1.70it/s]Extractor Predicting: 75it [00:46,  1.75it/s]Extractor Predicting: 76it [00:46,  1.76it/s]Extractor Predicting: 77it [00:47,  1.77it/s]Extractor Predicting: 78it [00:47,  1.79it/s]Extractor Predicting: 79it [00:48,  1.79it/s]Extractor Predicting: 80it [00:48,  1.77it/s]Extractor Predicting: 81it [00:49,  1.72it/s]Extractor Predicting: 82it [00:50,  1.69it/s]Extractor Predicting: 83it [00:50,  1.72it/s]Extractor Predicting: 84it [00:51,  1.71it/s]Extractor Predicting: 85it [00:52,  1.51it/s]Extractor Predicting: 86it [00:52,  1.54it/s]Extractor Predicting: 87it [00:53,  1.57it/s]Extractor Predicting: 88it [00:54,  1.56it/s]Extractor Predicting: 89it [00:54,  1.56it/s]Extractor Predicting: 90it [00:55,  1.56it/s]Extractor Predicting: 91it [00:55,  1.58it/s]Extractor Predicting: 92it [00:56,  1.57it/s]Extractor Predicting: 93it [00:57,  1.56it/s]Extractor Predicting: 94it [00:57,  1.58it/s]Extractor Predicting: 95it [00:58,  1.57it/s]Extractor Predicting: 96it [00:59,  1.59it/s]Extractor Predicting: 97it [00:59,  1.58it/s]Extractor Predicting: 98it [01:00,  1.58it/s]Extractor Predicting: 99it [01:01,  1.60it/s]Extractor Predicting: 100it [01:01,  1.61it/s]Extractor Predicting: 101it [01:02,  1.60it/s]Extractor Predicting: 102it [01:02,  1.62it/s]Extractor Predicting: 103it [01:03,  1.62it/s]Extractor Predicting: 104it [01:04,  1.64it/s]Extractor Predicting: 105it [01:04,  1.66it/s]Extractor Predicting: 106it [01:05,  1.64it/s]Extractor Predicting: 107it [01:05,  1.63it/s]Extractor Predicting: 108it [01:06,  1.63it/s]Extractor Predicting: 109it [01:07,  1.62it/s]Extractor Predicting: 110it [01:07,  1.60it/s]Extractor Predicting: 111it [01:08,  1.59it/s]Extractor Predicting: 112it [01:09,  1.61it/s]Extractor Predicting: 113it [01:09,  1.63it/s]Extractor Predicting: 114it [01:10,  1.62it/s]Extractor Predicting: 115it [01:10,  1.61it/s]Extractor Predicting: 116it [01:11,  1.64it/s]Extractor Predicting: 117it [01:12,  1.64it/s]Extractor Predicting: 118it [01:12,  1.62it/s]Extractor Predicting: 119it [01:13,  1.63it/s]Extractor Predicting: 120it [01:13,  1.62it/s]Extractor Predicting: 121it [01:14,  1.61it/s]Extractor Predicting: 122it [01:15,  1.60it/s]Extractor Predicting: 123it [01:15,  1.60it/s]Extractor Predicting: 124it [01:16,  1.58it/s]Extractor Predicting: 125it [01:17,  1.60it/s]Extractor Predicting: 126it [01:17,  1.60it/s]Extractor Predicting: 127it [01:18,  1.59it/s]Extractor Predicting: 128it [01:19,  1.57it/s]Extractor Predicting: 129it [01:19,  1.59it/s]Extractor Predicting: 130it [01:20,  1.62it/s]Extractor Predicting: 131it [01:20,  1.60it/s]Extractor Predicting: 132it [01:21,  1.62it/s]Extractor Predicting: 133it [01:22,  1.64it/s]Extractor Predicting: 134it [01:22,  1.62it/s]Extractor Predicting: 135it [01:23,  1.60it/s]Extractor Predicting: 136it [01:23,  1.60it/s]Extractor Predicting: 137it [01:24,  1.63it/s]Extractor Predicting: 138it [01:25,  1.64it/s]Extractor Predicting: 139it [01:25,  1.64it/s]Extractor Predicting: 140it [01:26,  1.65it/s]Extractor Predicting: 141it [01:26,  1.70it/s]Extractor Predicting: 141it [01:26,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:00,412 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:00,414 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:00,415 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:00,415 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:00,415 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:40:01,009 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:40:01,016 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:40:01,574 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:40:02,595 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:40:02,595 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:05,476 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:05,481 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:05,481 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:05,481 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:40:05,481 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:40:06,107 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:40:06,108 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:40:06,696 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:40:06,866 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:40:06,867 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2739179954441913,
  "recall": 0.1395821242019733,
  "score": 0.18492887351018839,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.62it/s]Extractor Predicting: 2it [00:01,  1.67it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.61it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.69it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.64it/s]Extractor Predicting: 17it [00:10,  1.64it/s]Extractor Predicting: 18it [00:10,  1.63it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.65it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:13,  1.63it/s]Extractor Predicting: 24it [00:14,  1.62it/s]Extractor Predicting: 25it [00:15,  1.63it/s]Extractor Predicting: 26it [00:15,  1.64it/s]Extractor Predicting: 27it [00:16,  1.64it/s]Extractor Predicting: 28it [00:16,  1.66it/s]Extractor Predicting: 29it [00:17,  1.62it/s]Extractor Predicting: 30it [00:18,  1.57it/s]Extractor Predicting: 31it [00:18,  1.57it/s]Extractor Predicting: 32it [00:19,  1.56it/s]Extractor Predicting: 33it [00:20,  1.57it/s]Extractor Predicting: 34it [00:20,  1.58it/s]Extractor Predicting: 35it [00:21,  1.59it/s]Extractor Predicting: 36it [00:22,  1.61it/s]Extractor Predicting: 37it [00:22,  1.66it/s]Extractor Predicting: 38it [00:23,  1.65it/s]Extractor Predicting: 39it [00:23,  1.64it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:26,  1.62it/s]Extractor Predicting: 44it [00:26,  1.62it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:28,  1.63it/s]Extractor Predicting: 48it [00:29,  1.63it/s]Extractor Predicting: 49it [00:29,  1.62it/s]Extractor Predicting: 50it [00:30,  1.60it/s]Extractor Predicting: 51it [00:31,  1.61it/s]Extractor Predicting: 52it [00:31,  1.61it/s]Extractor Predicting: 53it [00:32,  1.62it/s]Extractor Predicting: 54it [00:33,  1.63it/s]Extractor Predicting: 55it [00:33,  1.59it/s]Extractor Predicting: 56it [00:34,  1.61it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:36,  1.66it/s]Extractor Predicting: 60it [00:36,  1.67it/s]Extractor Predicting: 61it [00:37,  1.66it/s]Extractor Predicting: 62it [00:37,  1.65it/s]Extractor Predicting: 63it [00:38,  1.62it/s]Extractor Predicting: 64it [00:39,  1.64it/s]Extractor Predicting: 65it [00:39,  1.62it/s]Extractor Predicting: 66it [00:40,  1.59it/s]Extractor Predicting: 67it [00:41,  1.64it/s]Extractor Predicting: 68it [00:41,  1.65it/s]Extractor Predicting: 69it [00:42,  1.67it/s]Extractor Predicting: 70it [00:42,  1.66it/s]Extractor Predicting: 71it [00:43,  1.64it/s]Extractor Predicting: 72it [00:44,  1.65it/s]Extractor Predicting: 73it [00:44,  1.61it/s]Extractor Predicting: 74it [00:45,  1.46it/s]Extractor Predicting: 75it [00:46,  1.51it/s]Extractor Predicting: 76it [00:46,  1.55it/s]Extractor Predicting: 77it [00:47,  1.53it/s]Extractor Predicting: 78it [00:48,  1.54it/s]Extractor Predicting: 79it [00:48,  1.57it/s]Extractor Predicting: 80it [00:49,  1.57it/s]Extractor Predicting: 81it [00:49,  1.58it/s]Extractor Predicting: 82it [00:50,  1.58it/s]Extractor Predicting: 83it [00:51,  1.56it/s]Extractor Predicting: 84it [00:51,  1.58it/s]Extractor Predicting: 85it [00:52,  1.59it/s]Extractor Predicting: 86it [00:53,  1.59it/s]Extractor Predicting: 87it [00:53,  1.58it/s]Extractor Predicting: 88it [00:54,  1.59it/s]Extractor Predicting: 89it [00:55,  1.54it/s]Extractor Predicting: 90it [00:55,  1.55it/s]Extractor Predicting: 91it [00:56,  1.57it/s]Extractor Predicting: 92it [00:56,  1.57it/s]Extractor Predicting: 93it [00:57,  1.57it/s]Extractor Predicting: 94it [00:58,  1.53it/s]Extractor Predicting: 95it [00:58,  1.53it/s]Extractor Predicting: 96it [00:59,  1.54it/s]Extractor Predicting: 97it [01:00,  1.55it/s]Extractor Predicting: 98it [01:00,  1.57it/s]Extractor Predicting: 99it [01:01,  1.56it/s]Extractor Predicting: 100it [01:02,  1.55it/s]Extractor Predicting: 101it [01:02,  1.58it/s]Extractor Predicting: 102it [01:03,  1.58it/s]Extractor Predicting: 103it [01:03,  1.57it/s]Extractor Predicting: 104it [01:04,  1.56it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:05,  1.57it/s]Extractor Predicting: 107it [01:06,  1.53it/s]Extractor Predicting: 108it [01:07,  1.54it/s]Extractor Predicting: 109it [01:07,  1.55it/s]Extractor Predicting: 110it [01:08,  1.53it/s]Extractor Predicting: 111it [01:09,  1.53it/s]Extractor Predicting: 112it [01:09,  1.54it/s]Extractor Predicting: 113it [01:10,  1.58it/s]Extractor Predicting: 114it [01:11,  1.61it/s]Extractor Predicting: 115it [01:11,  1.58it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:12,  1.63it/s]Extractor Predicting: 118it [01:13,  1.61it/s]Extractor Predicting: 119it [01:14,  1.63it/s]Extractor Predicting: 120it [01:14,  1.63it/s]Extractor Predicting: 121it [01:15,  1.64it/s]Extractor Predicting: 122it [01:15,  1.68it/s]Extractor Predicting: 123it [01:16,  1.67it/s]Extractor Predicting: 124it [01:17,  1.66it/s]Extractor Predicting: 125it [01:17,  1.64it/s]Extractor Predicting: 126it [01:18,  1.63it/s]Extractor Predicting: 127it [01:18,  1.64it/s]Extractor Predicting: 128it [01:19,  1.67it/s]Extractor Predicting: 129it [01:20,  1.62it/s]Extractor Predicting: 130it [01:20,  1.62it/s]Extractor Predicting: 131it [01:21,  1.62it/s]Extractor Predicting: 132it [01:22,  1.62it/s]Extractor Predicting: 133it [01:22,  1.62it/s]Extractor Predicting: 134it [01:23,  1.62it/s]Extractor Predicting: 135it [01:23,  1.64it/s]Extractor Predicting: 136it [01:24,  1.64it/s]Extractor Predicting: 137it [01:25,  1.64it/s]Extractor Predicting: 138it [01:25,  1.64it/s]Extractor Predicting: 139it [01:26,  1.67it/s]Extractor Predicting: 140it [01:26,  1.66it/s]Extractor Predicting: 141it [01:27,  1.64it/s]Extractor Predicting: 142it [01:28,  1.65it/s]Extractor Predicting: 143it [01:28,  1.65it/s]Extractor Predicting: 144it [01:29,  1.62it/s]Extractor Predicting: 145it [01:29,  1.64it/s]Extractor Predicting: 146it [01:30,  1.63it/s]Extractor Predicting: 147it [01:31,  1.61it/s]Extractor Predicting: 148it [01:31,  1.60it/s]Extractor Predicting: 149it [01:32,  1.60it/s]Extractor Predicting: 150it [01:33,  1.62it/s]Extractor Predicting: 151it [01:33,  1.65it/s]Extractor Predicting: 152it [01:34,  1.66it/s]Extractor Predicting: 153it [01:34,  1.67it/s]Extractor Predicting: 154it [01:35,  1.63it/s]Extractor Predicting: 155it [01:36,  1.63it/s]Extractor Predicting: 156it [01:36,  1.61it/s]Extractor Predicting: 157it [01:37,  1.60it/s]Extractor Predicting: 158it [01:38,  1.40it/s]Extractor Predicting: 159it [01:38,  1.45it/s]Extractor Predicting: 160it [01:39,  1.49it/s]Extractor Predicting: 161it [01:40,  1.54it/s]Extractor Predicting: 162it [01:40,  1.57it/s]Extractor Predicting: 163it [01:41,  1.59it/s]Extractor Predicting: 164it [01:41,  1.59it/s]Extractor Predicting: 165it [01:42,  1.58it/s]Extractor Predicting: 166it [01:43,  1.58it/s]Extractor Predicting: 167it [01:43,  1.61it/s]Extractor Predicting: 168it [01:44,  1.60it/s]Extractor Predicting: 169it [01:45,  1.60it/s]Extractor Predicting: 170it [01:45,  1.59it/s]Extractor Predicting: 171it [01:46,  1.56it/s]Extractor Predicting: 172it [01:47,  1.55it/s]Extractor Predicting: 173it [01:47,  1.54it/s]Extractor Predicting: 174it [01:48,  1.51it/s]Extractor Predicting: 175it [01:49,  1.48it/s]Extractor Predicting: 176it [01:49,  1.50it/s]Extractor Predicting: 177it [01:50,  1.53it/s]Extractor Predicting: 178it [01:51,  1.54it/s]Extractor Predicting: 179it [01:51,  1.57it/s]Extractor Predicting: 180it [01:52,  1.56it/s]Extractor Predicting: 181it [01:52,  1.59it/s]Extractor Predicting: 182it [01:53,  1.60it/s]Extractor Predicting: 183it [01:54,  1.60it/s]Extractor Predicting: 184it [01:54,  1.62it/s]Extractor Predicting: 185it [01:55,  1.64it/s]Extractor Predicting: 186it [01:55,  1.67it/s]Extractor Predicting: 187it [01:56,  1.65it/s]Extractor Predicting: 188it [01:57,  1.66it/s]Extractor Predicting: 189it [01:57,  1.66it/s]Extractor Predicting: 190it [01:58,  1.65it/s]Extractor Predicting: 191it [01:59,  1.60it/s]Extractor Predicting: 192it [01:59,  1.58it/s]Extractor Predicting: 193it [02:00,  1.60it/s]Extractor Predicting: 194it [02:00,  1.64it/s]Extractor Predicting: 195it [02:01,  1.65it/s]Extractor Predicting: 196it [02:02,  1.64it/s]Extractor Predicting: 197it [02:02,  1.66it/s]Extractor Predicting: 198it [02:03,  1.66it/s]Extractor Predicting: 199it [02:03,  1.64it/s]Extractor Predicting: 200it [02:04,  1.63it/s]Extractor Predicting: 201it [02:05,  1.63it/s]Extractor Predicting: 202it [02:05,  1.65it/s]Extractor Predicting: 203it [02:06,  1.68it/s]Extractor Predicting: 204it [02:06,  1.68it/s]Extractor Predicting: 205it [02:07,  1.67it/s]Extractor Predicting: 206it [02:08,  1.66it/s]Extractor Predicting: 207it [02:08,  1.62it/s]Extractor Predicting: 208it [02:09,  1.62it/s]Extractor Predicting: 209it [02:09,  1.63it/s]Extractor Predicting: 210it [02:10,  1.62it/s]Extractor Predicting: 211it [02:11,  1.64it/s]Extractor Predicting: 212it [02:11,  1.62it/s]Extractor Predicting: 213it [02:12,  1.66it/s]Extractor Predicting: 214it [02:13,  1.63it/s]Extractor Predicting: 215it [02:13,  1.67it/s]Extractor Predicting: 216it [02:14,  1.67it/s]Extractor Predicting: 217it [02:14,  1.59it/s]Extractor Predicting: 218it [02:15,  1.61it/s]Extractor Predicting: 219it [02:16,  1.64it/s]Extractor Predicting: 220it [02:16,  1.67it/s]Extractor Predicting: 221it [02:17,  1.65it/s]Extractor Predicting: 222it [02:17,  1.66it/s]Extractor Predicting: 223it [02:18,  1.67it/s]Extractor Predicting: 224it [02:19,  1.68it/s]Extractor Predicting: 225it [02:19,  1.66it/s]Extractor Predicting: 226it [02:20,  1.61it/s]Extractor Predicting: 227it [02:20,  1.61it/s]Extractor Predicting: 228it [02:21,  1.57it/s]Extractor Predicting: 229it [02:22,  1.57it/s]Extractor Predicting: 230it [02:22,  1.60it/s]Extractor Predicting: 231it [02:23,  1.58it/s]Extractor Predicting: 232it [02:24,  1.59it/s]Extractor Predicting: 233it [02:24,  1.60it/s]Extractor Predicting: 234it [02:25,  1.61it/s]Extractor Predicting: 235it [02:26,  1.57it/s]Extractor Predicting: 236it [02:26,  1.60it/s]Extractor Predicting: 237it [02:27,  1.65it/s]Extractor Predicting: 238it [02:27,  1.61it/s]Extractor Predicting: 239it [02:28,  1.59it/s]Extractor Predicting: 240it [02:29,  1.57it/s]Extractor Predicting: 241it [02:29,  1.58it/s]Extractor Predicting: 242it [02:30,  1.61it/s]Extractor Predicting: 243it [02:30,  1.62it/s]Extractor Predicting: 244it [02:31,  1.58it/s]Extractor Predicting: 245it [02:32,  1.60it/s]Extractor Predicting: 246it [02:32,  1.62it/s]Extractor Predicting: 247it [02:33,  1.64it/s]Extractor Predicting: 248it [02:34,  1.57it/s]Extractor Predicting: 249it [02:34,  1.59it/s]Extractor Predicting: 250it [02:35,  1.59it/s]Extractor Predicting: 251it [02:36,  1.56it/s]Extractor Predicting: 252it [02:36,  1.56it/s]Extractor Predicting: 253it [02:37,  1.57it/s]Extractor Predicting: 254it [02:37,  1.60it/s]Extractor Predicting: 255it [02:38,  1.60it/s]Extractor Predicting: 256it [02:39,  1.64it/s]Extractor Predicting: 257it [02:39,  1.67it/s]Extractor Predicting: 258it [02:40,  1.64it/s]Extractor Predicting: 259it [02:40,  1.63it/s]Extractor Predicting: 260it [02:41,  1.60it/s]Extractor Predicting: 261it [02:42,  1.60it/s]Extractor Predicting: 262it [02:42,  1.60it/s]Extractor Predicting: 263it [02:43,  1.54it/s]Extractor Predicting: 264it [02:44,  1.55it/s]Extractor Predicting: 265it [02:44,  1.57it/s]Extractor Predicting: 266it [02:45,  1.59it/s]Extractor Predicting: 267it [02:46,  1.59it/s]Extractor Predicting: 268it [02:46,  1.61it/s]Extractor Predicting: 269it [02:47,  1.65it/s]Extractor Predicting: 270it [02:47,  1.68it/s]Extractor Predicting: 271it [02:48,  1.64it/s]Extractor Predicting: 272it [02:49,  1.66it/s]Extractor Predicting: 273it [02:49,  1.64it/s]Extractor Predicting: 274it [02:50,  1.63it/s]Extractor Predicting: 275it [02:50,  1.65it/s]Extractor Predicting: 276it [02:51,  1.64it/s]Extractor Predicting: 277it [02:52,  1.42it/s]Extractor Predicting: 278it [02:52,  1.50it/s]Extractor Predicting: 279it [02:53,  1.50it/s]Extractor Predicting: 280it [02:54,  1.52it/s]Extractor Predicting: 281it [02:54,  1.55it/s]Extractor Predicting: 282it [02:55,  1.57it/s]Extractor Predicting: 283it [02:56,  1.59it/s]Extractor Predicting: 284it [02:56,  1.57it/s]Extractor Predicting: 285it [02:57,  1.56it/s]Extractor Predicting: 286it [02:58,  1.58it/s]Extractor Predicting: 287it [02:58,  1.61it/s]Extractor Predicting: 288it [02:59,  1.60it/s]Extractor Predicting: 289it [02:59,  1.56it/s]Extractor Predicting: 290it [03:00,  1.62it/s]Extractor Predicting: 291it [03:01,  1.61it/s]Extractor Predicting: 292it [03:01,  1.60it/s]Extractor Predicting: 293it [03:02,  1.57it/s]Extractor Predicting: 294it [03:03,  1.54it/s]Extractor Predicting: 295it [03:03,  1.58it/s]Extractor Predicting: 296it [03:04,  1.56it/s]Extractor Predicting: 297it [03:04,  1.59it/s]Extractor Predicting: 298it [03:05,  1.54it/s]Extractor Predicting: 299it [03:06,  1.56it/s]Extractor Predicting: 300it [03:06,  1.57it/s]Extractor Predicting: 301it [03:07,  1.57it/s]Extractor Predicting: 302it [03:08,  1.57it/s]Extractor Predicting: 303it [03:08,  1.55it/s]Extractor Predicting: 304it [03:09,  1.52it/s]Extractor Predicting: 305it [03:10,  1.54it/s]Extractor Predicting: 306it [03:10,  1.53it/s]Extractor Predicting: 307it [03:11,  1.52it/s]Extractor Predicting: 308it [03:12,  1.54it/s]Extractor Predicting: 309it [03:12,  1.54it/s]Extractor Predicting: 310it [03:13,  1.51it/s]Extractor Predicting: 311it [03:14,  1.51it/s]Extractor Predicting: 312it [03:14,  1.52it/s]Extractor Predicting: 313it [03:15,  1.53it/s]Extractor Predicting: 314it [03:16,  1.50it/s]Extractor Predicting: 315it [03:16,  1.55it/s]Extractor Predicting: 316it [03:17,  1.54it/s]Extractor Predicting: 317it [03:18,  1.52it/s]Extractor Predicting: 318it [03:18,  1.53it/s]Extractor Predicting: 319it [03:19,  1.54it/s]Extractor Predicting: 320it [03:19,  1.62it/s]Extractor Predicting: 321it [03:20,  1.66it/s]Extractor Predicting: 322it [03:20,  1.74it/s]Extractor Predicting: 323it [03:21,  1.77it/s]Extractor Predicting: 324it [03:22,  1.79it/s]Extractor Predicting: 325it [03:22,  1.81it/s]Extractor Predicting: 326it [03:23,  1.80it/s]Extractor Predicting: 327it [03:23,  1.79it/s]Extractor Predicting: 328it [03:24,  1.81it/s]Extractor Predicting: 329it [03:24,  1.85it/s]Extractor Predicting: 330it [03:25,  1.84it/s]Extractor Predicting: 331it [03:25,  1.89it/s]Extractor Predicting: 332it [03:26,  1.85it/s]Extractor Predicting: 333it [03:26,  1.88it/s]Extractor Predicting: 334it [03:27,  1.88it/s]Extractor Predicting: 335it [03:27,  1.86it/s]Extractor Predicting: 336it [03:28,  1.83it/s]Extractor Predicting: 337it [03:29,  1.87it/s]Extractor Predicting: 338it [03:29,  1.82it/s]Extractor Predicting: 339it [03:30,  1.81it/s]Extractor Predicting: 340it [03:30,  1.88it/s]Extractor Predicting: 341it [03:31,  1.84it/s]Extractor Predicting: 342it [03:31,  1.86it/s]Extractor Predicting: 343it [03:32,  1.86it/s]Extractor Predicting: 344it [03:32,  1.82it/s]Extractor Predicting: 345it [03:33,  1.85it/s]Extractor Predicting: 346it [03:33,  1.84it/s]Extractor Predicting: 347it [03:34,  1.81it/s]Extractor Predicting: 348it [03:35,  1.76it/s]Extractor Predicting: 349it [03:35,  1.71it/s]Extractor Predicting: 350it [03:36,  1.67it/s]Extractor Predicting: 351it [03:36,  1.65it/s]Extractor Predicting: 352it [03:37,  1.65it/s]Extractor Predicting: 353it [03:38,  1.67it/s]Extractor Predicting: 354it [03:38,  1.65it/s]Extractor Predicting: 355it [03:39,  1.63it/s]Extractor Predicting: 356it [03:40,  1.60it/s]Extractor Predicting: 357it [03:40,  1.58it/s]Extractor Predicting: 358it [03:41,  1.60it/s]Extractor Predicting: 359it [03:41,  1.60it/s]Extractor Predicting: 360it [03:42,  1.63it/s]Extractor Predicting: 361it [03:43,  1.61it/s]Extractor Predicting: 362it [03:43,  1.64it/s]Extractor Predicting: 363it [03:44,  1.62it/s]Extractor Predicting: 364it [03:45,  1.60it/s]Extractor Predicting: 365it [03:45,  1.60it/s]Extractor Predicting: 366it [03:46,  1.61it/s]Extractor Predicting: 367it [03:46,  1.60it/s]Extractor Predicting: 368it [03:47,  1.62it/s]Extractor Predicting: 369it [03:48,  1.61it/s]Extractor Predicting: 370it [03:48,  1.60it/s]Extractor Predicting: 371it [03:49,  1.60it/s]Extractor Predicting: 372it [03:50,  1.61it/s]Extractor Predicting: 373it [03:50,  1.59it/s]Extractor Predicting: 374it [03:51,  1.60it/s]Extractor Predicting: 375it [03:51,  1.62it/s]Extractor Predicting: 376it [03:52,  1.59it/s]Extractor Predicting: 377it [03:53,  1.60it/s]Extractor Predicting: 378it [03:53,  1.58it/s]Extractor Predicting: 379it [03:54,  1.57it/s]Extractor Predicting: 380it [03:55,  1.57it/s]Extractor Predicting: 381it [03:55,  1.55it/s]Extractor Predicting: 382it [03:56,  1.54it/s]Extractor Predicting: 383it [03:57,  1.55it/s]Extractor Predicting: 384it [03:57,  1.54it/s]Extractor Predicting: 385it [03:58,  1.54it/s]Extractor Predicting: 386it [03:58,  1.56it/s]Extractor Predicting: 387it [03:59,  1.37it/s]Extractor Predicting: 388it [04:00,  1.41it/s]Extractor Predicting: 389it [04:01,  1.47it/s]Extractor Predicting: 390it [04:01,  1.50it/s]Extractor Predicting: 391it [04:02,  1.52it/s]Extractor Predicting: 392it [04:03,  1.49it/s]Extractor Predicting: 393it [04:03,  1.55it/s]Extractor Predicting: 394it [04:04,  1.54it/s]Extractor Predicting: 395it [04:05,  1.52it/s]Extractor Predicting: 396it [04:05,  1.52it/s]Extractor Predicting: 397it [04:06,  1.50it/s]Extractor Predicting: 398it [04:07,  1.51it/s]Extractor Predicting: 399it [04:07,  1.48it/s]Extractor Predicting: 400it [04:08,  1.49it/s]Extractor Predicting: 401it [04:09,  1.49it/s]Extractor Predicting: 402it [04:09,  1.52it/s]Extractor Predicting: 403it [04:10,  1.52it/s]Extractor Predicting: 404it [04:10,  1.58it/s]Extractor Predicting: 405it [04:11,  1.56it/s]Extractor Predicting: 406it [04:12,  1.60it/s]Extractor Predicting: 407it [04:12,  1.59it/s]Extractor Predicting: 408it [04:13,  1.56it/s]Extractor Predicting: 409it [04:14,  1.60it/s]Extractor Predicting: 410it [04:14,  1.60it/s]Extractor Predicting: 411it [04:15,  1.62it/s]Extractor Predicting: 412it [04:15,  1.59it/s]Extractor Predicting: 413it [04:16,  1.63it/s]Extractor Predicting: 414it [04:17,  1.63it/s]Extractor Predicting: 415it [04:17,  1.62it/s]Extractor Predicting: 416it [04:18,  1.59it/s]Extractor Predicting: 417it [04:19,  1.62it/s]Extractor Predicting: 418it [04:19,  1.61it/s]Extractor Predicting: 419it [04:20,  1.59it/s]Extractor Predicting: 420it [04:20,  1.60it/s]Extractor Predicting: 421it [04:21,  1.60it/s]Extractor Predicting: 422it [04:22,  1.57it/s]Extractor Predicting: 423it [04:22,  1.58it/s]Extractor Predicting: 424it [04:23,  1.55it/s]Extractor Predicting: 425it [04:24,  1.57it/s]Extractor Predicting: 426it [04:24,  1.55it/s]Extractor Predicting: 427it [04:25,  1.58it/s]Extractor Predicting: 428it [04:26,  1.57it/s]Extractor Predicting: 429it [04:26,  1.59it/s]Extractor Predicting: 430it [04:26,  1.98it/s]Extractor Predicting: 430it [04:26,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:42,934 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:42,940 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:42,940 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:42,940 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:42,940 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:44:43,552 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:44:43,552 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:44:44,113 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:44:45,169 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:44:45,169 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:48,072 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:48,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:48,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:48,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:44:48,082 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:44:48,710 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:44:48,711 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:44:49,285 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:44:49,464 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:44:49,464 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3018796278716537,
  "recall": 0.15439891241017673,
  "score": 0.2043045293928686,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:02,  2.01it/s]Extractor Predicting: 5it [00:02,  1.73it/s]
[INFO|configuration_utils.py:515] 2023-08-29 03:44:52,771 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:44:52,772 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 03:44:52,779 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:44:52,780 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 03:44:52,782 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 03:44:55,778 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 03:44:55,780 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 03:44:55,794 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 03:44:55,794 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 03:44:55,801 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:44:55,806 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:44:55,806 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:44:55,806 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:44:55,806 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:44:55,806 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 03:44:55,806 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3488372093023256,
  "recall": 0.07425742574257425,
  "score": 0.12244897959183673,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 03:44:56,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:44:56,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:44:57,300 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:44:57,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:44:58,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:44:59,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:44:59,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:00,233 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:00,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:01,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:02,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:02,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:03,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:03,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:04,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:05,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:05,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:06,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:06,968 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:07,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<03:49, 12.07s/it][WARNING|generation_utils.py:914] 2023-08-29 03:45:08,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:08,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:09,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:09,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:10,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:11,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:11,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:12,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:12,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:13,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:14,212 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:15,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:15,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:16,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:17,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:17,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:18,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:18,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:19,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:19,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:24<03:39, 12.17s/it][WARNING|generation_utils.py:914] 2023-08-29 03:45:20,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:20,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:21,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:21,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:22,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:22,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:23,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:23,974 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:24,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:24,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:25,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:26,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:26,544 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:27,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:27,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:27,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:28,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:28,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:29,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:29,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:34<03:10, 11.21s/it][WARNING|generation_utils.py:914] 2023-08-29 03:45:30,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:31,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:31,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:32,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:33,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:33,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:34,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:34,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:35,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:35,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:36,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:37,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:37,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:38,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:38,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:39,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:39,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:40,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:40,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:41,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:46<03:02, 11.40s/it][WARNING|generation_utils.py:914] 2023-08-29 03:45:42,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:42,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:43,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:43,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:44,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:44,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:45,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:45,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:46,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:46,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:47,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:47,934 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:48,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:48,990 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:49,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:50,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:50,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:51,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:51,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:52,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [00:56<02:47, 11.16s/it][WARNING|generation_utils.py:914] 2023-08-29 03:45:52,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:53,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:54,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:55,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:55,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:56,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:56,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:57,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:58,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:58,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:59,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:45:59,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:00,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:01,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:01,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:02,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:03,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:03,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:04,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:05,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:05,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:10<02:47, 11.93s/it][WARNING|generation_utils.py:914] 2023-08-29 03:46:06,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:06,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:07,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:07,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:08,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:08,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:09,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:09,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:10,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:10,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:11,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:11,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:12,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:12,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:13,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:13,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:14,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:14,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:15,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:15,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:20<02:25, 11.23s/it][WARNING|generation_utils.py:914] 2023-08-29 03:46:16,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:16,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:17,157 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:17,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:18,465 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:19,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:19,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:20,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:20,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:21,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:21,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:22,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:22,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:23,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:23,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:24,453 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:24,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:25,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:26,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:26,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:31<02:14, 11.19s/it][WARNING|generation_utils.py:914] 2023-08-29 03:46:27,176 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:27,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:28,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:28,725 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:29,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:29,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:30,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:31,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:31,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:32,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:32,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:33,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:33,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:34,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:34,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:35,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:36,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:36,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:37,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:37,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:38,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:38,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:43<02:07, 11.57s/it][WARNING|generation_utils.py:914] 2023-08-29 03:46:39,586 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:40,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:40,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:41,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:41,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:42,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:42,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:43,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:44,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:44,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:45,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:45,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:46,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:46,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:47,143 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:47,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:48,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:48,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:49,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:49,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:50,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [01:54<01:55, 11.52s/it][WARNING|generation_utils.py:914] 2023-08-29 03:46:50,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:51,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:52,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:52,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:53,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:54,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:54,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:55,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:56,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:56,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:57,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:57,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:58,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:59,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:46:59,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:00,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:00,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:01,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:02,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:02,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:07<01:45, 11.77s/it][WARNING|generation_utils.py:914] 2023-08-29 03:47:03,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:03,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:04,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:04,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:05,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:05,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:06,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:07,032 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:07,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:08,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:08,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:09,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:09,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:10,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:10,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:11,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:11,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:12,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:12,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:13,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:13,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:18<01:32, 11.55s/it][WARNING|generation_utils.py:914] 2023-08-29 03:47:14,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:14,865 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:15,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:15,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:16,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:16,862 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:17,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:17,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:18,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:18,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:19,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:19,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:20,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:21,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:21,568 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:22,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:22,831 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:23,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:23,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:24,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:28<01:18, 11.22s/it][WARNING|generation_utils.py:914] 2023-08-29 03:47:24,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:25,386 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:26,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:26,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:27,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:27,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:28,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:28,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:29,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:29,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:30,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:30,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:31,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:32,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:32,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:33,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:33,874 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:34,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:34,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:35,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:40<01:07, 11.28s/it][WARNING|generation_utils.py:914] 2023-08-29 03:47:36,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:36,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:37,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:37,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:38,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:38,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:39,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:40,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:40,576 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:41,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:41,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:42,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:43,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:43,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:44,064 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:44,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:45,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:45,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:46,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:46,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:47,442 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:47,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [02:52<00:57, 11.56s/it][WARNING|generation_utils.py:914] 2023-08-29 03:47:48,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:48,981 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:49,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:50,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:50,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:51,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:52,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:52,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:53,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:53,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:54,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:55,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:55,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:56,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:57,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:57,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:58,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:59,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:47:59,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:00,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:04<00:47, 11.78s/it][WARNING|generation_utils.py:914] 2023-08-29 03:48:00,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:01,318 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:01,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:02,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:03,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:03,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:04,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:04,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:05,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:06,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:06,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:07,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:07,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:08,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:09,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:09,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:10,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:11,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:11,829 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:12,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:13,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:13,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:18<00:36, 12.29s/it][WARNING|generation_utils.py:914] 2023-08-29 03:48:14,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:14,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:15,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:16,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:16,658 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:17,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:17,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:18,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:18,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:19,507 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:20,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:20,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:21,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:22,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:22,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:23,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:24,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:24,540 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:25,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:25,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:30<00:24, 12.17s/it][WARNING|generation_utils.py:914] 2023-08-29 03:48:26,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:26,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:27,083 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:27,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:28,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:28,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:29,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:29,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:30,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:31,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:31,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:32,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:32,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:33,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:33,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:34,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:34,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:35,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:35,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:36,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:36,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:37,373 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:38,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:38,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:42<00:12, 12.40s/it][WARNING|generation_utils.py:914] 2023-08-29 03:48:39,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:39,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:40,139 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:40,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:41,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:41,844 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:42,335 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:42,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:43,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:43,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:44,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:45,427 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:46,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:46,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:47,104 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:47,645 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:48,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:48,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:49,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 03:48:49,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [03:54<00:00, 12.10s/it]Generating: 100%|██████████| 20/20 [03:54<00:00, 11.72s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:56,585 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:56,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:56,590 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:56,590 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:48:56,590 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:48:57,312 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:48:57,313 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:48:57,991 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:48:59,040 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:48:59,040 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:49:00,731 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:49:00,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:49:00,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:49:00,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:49:00,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:49:01,043 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:49:01,044 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:49:01,299 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:49:01,454 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:49:01,454 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 374, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 619, 'raw': 640}
{'prompt': 'Relation : followed by .', 'success_rate': 0.9671875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 300, 'raw': 320}
{'target': 600, 'success': 328, 'raw': 352}
{'target': 600, 'success': 359, 'raw': 384}
{'target': 600, 'success': 390, 'raw': 416}
{'target': 600, 'success': 421, 'raw': 448}
{'target': 600, 'success': 451, 'raw': 480}
{'target': 600, 'success': 483, 'raw': 512}
{'target': 600, 'success': 510, 'raw': 544}
{'target': 600, 'success': 539, 'raw': 576}
{'target': 600, 'success': 571, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : military rank .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 522, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9640625, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 180, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 242, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.965625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 158, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 281, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 342, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 436, 'raw': 448}
{'target': 600, 'success': 468, 'raw': 480}
{'target': 600, 'success': 499, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 561, 'raw': 576}
{'target': 600, 'success': 593, 'raw': 608}
{'target': 600, 'success': 625, 'raw': 640}
{'prompt': 'Relation : tributary .', 'success_rate': 0.9765625, 'errors': {''}}
['Relation : architect . Context : Later in 1837 , the architect of the new palace was Charles V . Head Entity : Charles V . Tail Entity : Charles V .\n']
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 209, 'raw': 224}
{'target': 600, 'success': 239, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 299, 'raw': 320}
{'target': 600, 'success': 330, 'raw': 352}
{'target': 600, 'success': 361, 'raw': 384}
{'target': 600, 'success': 392, 'raw': 416}
{'target': 600, 'success': 424, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 537, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 598, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : architect .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 343, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 407, 'raw': 416}
{'target': 600, 'success': 439, 'raw': 448}
{'target': 600, 'success': 471, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 534, 'raw': 544}
{'target': 600, 'success': 566, 'raw': 576}
{'target': 600, 'success': 598, 'raw': 608}
{'target': 600, 'success': 630, 'raw': 640}
{'prompt': 'Relation : constellation .', 'success_rate': 0.984375, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 212, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 337, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 517, 'raw': 544}
{'target': 600, 'success': 546, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 341, 'raw': 384}
{'target': 600, 'success': 371, 'raw': 416}
{'target': 600, 'success': 400, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 455, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 538, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 596, 'raw': 672}
{'target': 600, 'success': 624, 'raw': 704}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8863636363636364, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 320, 'raw': 352}
{'target': 600, 'success': 350, 'raw': 384}
{'target': 600, 'success': 378, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 522, 'raw': 576}
{'target': 600, 'success': 550, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 611, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9092261904761905, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.96875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 206, 'raw': 224}
{'target': 600, 'success': 235, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 295, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 351, 'raw': 384}
{'target': 600, 'success': 380, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 437, 'raw': 480}
{'target': 600, 'success': 469, 'raw': 512}
{'target': 600, 'success': 501, 'raw': 544}
{'target': 600, 'success': 533, 'raw': 576}
{'target': 600, 'success': 563, 'raw': 608}
{'target': 600, 'success': 594, 'raw': 640}
{'target': 600, 'success': 623, 'raw': 672}
{'prompt': 'Relation : league .', 'success_rate': 0.9270833333333334, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 280, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 402, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 528, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 589, 'raw': 608}
{'target': 600, 'success': 620, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.96875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 339, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : member of .', 'success_rate': 0.9515625, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 261, 'raw': 288}
{'target': 600, 'success': 288, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 342, 'raw': 384}
{'target': 600, 'success': 372, 'raw': 416}
{'target': 600, 'success': 399, 'raw': 448}
{'target': 600, 'success': 428, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 484, 'raw': 544}
{'target': 600, 'success': 509, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 562, 'raw': 640}
{'target': 600, 'success': 592, 'raw': 672}
{'target': 600, 'success': 620, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8806818181818182, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 310, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 490, 'raw': 512}
{'target': 600, 'success': 519, 'raw': 544}
{'target': 600, 'success': 549, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 609, 'raw': 640}
{'prompt': 'Relation : notable work .', 'success_rate': 0.9515625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 256, 'raw': 288}
{'target': 600, 'success': 282, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 416, 'raw': 480}
{'target': 600, 'success': 444, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 528, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 583, 'raw': 672}
{'target': 600, 'success': 608, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8636363636363636, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 155, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 521, 'raw': 544}
{'target': 600, 'success': 553, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 611, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.9546875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 206, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 292, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 366, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 445, 'raw': 544}
{'target': 600, 'success': 474, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 525, 'raw': 640}
{'target': 600, 'success': 548, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 623, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.8111979166666666, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 397, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 457, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 516, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 580, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : residence .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 9083
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9183, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.33it/s]Extractor Estimating: 2it [00:01,  1.33it/s]Extractor Estimating: 3it [00:02,  1.41it/s]Extractor Estimating: 4it [00:02,  1.42it/s]Extractor Estimating: 5it [00:03,  1.47it/s]Extractor Estimating: 6it [00:04,  1.51it/s]Extractor Estimating: 7it [00:04,  1.55it/s]Extractor Estimating: 8it [00:05,  1.59it/s]Extractor Estimating: 9it [00:05,  1.61it/s]Extractor Estimating: 10it [00:06,  1.55it/s]Extractor Estimating: 11it [00:07,  1.47it/s]Extractor Estimating: 12it [00:08,  1.47it/s]Extractor Estimating: 13it [00:08,  1.47it/s]Extractor Estimating: 14it [00:09,  1.48it/s]Extractor Estimating: 15it [00:10,  1.52it/s]Extractor Estimating: 16it [00:10,  1.50it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:12,  1.46it/s]Extractor Estimating: 19it [00:12,  1.51it/s]Extractor Estimating: 20it [00:13,  1.56it/s]Extractor Estimating: 21it [00:13,  1.55it/s]Extractor Estimating: 22it [00:14,  1.55it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:16,  1.47it/s]Extractor Estimating: 25it [00:16,  1.52it/s]Extractor Estimating: 26it [00:17,  1.61it/s]Extractor Estimating: 27it [00:17,  1.70it/s]Extractor Estimating: 28it [00:18,  1.66it/s]Extractor Estimating: 29it [00:18,  1.67it/s]Extractor Estimating: 30it [00:19,  1.71it/s]Extractor Estimating: 31it [00:20,  1.74it/s]Extractor Estimating: 32it [00:20,  1.76it/s]Extractor Estimating: 33it [00:21,  1.79it/s]Extractor Estimating: 34it [00:21,  1.78it/s]Extractor Estimating: 35it [00:22,  1.74it/s]Extractor Estimating: 36it [00:22,  1.80it/s]Extractor Estimating: 37it [00:23,  1.75it/s]Extractor Estimating: 38it [00:24,  1.72it/s]Extractor Estimating: 39it [00:24,  1.69it/s]Extractor Estimating: 40it [00:25,  1.75it/s]Extractor Estimating: 41it [00:25,  1.73it/s]Extractor Estimating: 42it [00:26,  1.69it/s]Extractor Estimating: 43it [00:26,  1.71it/s]Extractor Estimating: 44it [00:27,  1.71it/s]Extractor Estimating: 45it [00:28,  1.73it/s]Extractor Estimating: 46it [00:28,  1.77it/s]Extractor Estimating: 47it [00:29,  1.78it/s]Extractor Estimating: 48it [00:29,  1.73it/s]Extractor Estimating: 49it [00:30,  1.68it/s]Extractor Estimating: 50it [00:31,  1.67it/s]Extractor Estimating: 51it [00:31,  1.76it/s]Extractor Estimating: 52it [00:32,  1.81it/s]Extractor Estimating: 53it [00:32,  1.82it/s]Extractor Estimating: 54it [00:33,  1.84it/s]Extractor Estimating: 55it [00:33,  1.86it/s]Extractor Estimating: 56it [00:34,  1.88it/s]Extractor Estimating: 57it [00:34,  1.97it/s]Extractor Estimating: 58it [00:35,  1.95it/s]Extractor Estimating: 59it [00:35,  1.89it/s]Extractor Estimating: 60it [00:36,  1.89it/s]Extractor Estimating: 61it [00:36,  1.93it/s]Extractor Estimating: 62it [00:37,  1.96it/s]Extractor Estimating: 63it [00:37,  1.87it/s]Extractor Estimating: 64it [00:38,  1.86it/s]Extractor Estimating: 65it [00:38,  1.90it/s]Extractor Estimating: 66it [00:39,  1.88it/s]Extractor Estimating: 67it [00:39,  1.93it/s]Extractor Estimating: 68it [00:40,  1.96it/s]Extractor Estimating: 69it [00:40,  1.97it/s]Extractor Estimating: 70it [00:41,  1.95it/s]Extractor Estimating: 71it [00:41,  1.95it/s]Extractor Estimating: 72it [00:42,  1.94it/s]Extractor Estimating: 73it [00:42,  1.90it/s]Extractor Estimating: 74it [00:43,  1.94it/s]Extractor Estimating: 75it [00:44,  1.91it/s]Extractor Estimating: 76it [00:44,  1.82it/s]Extractor Estimating: 77it [00:45,  1.75it/s]Extractor Estimating: 78it [00:45,  1.72it/s]Extractor Estimating: 79it [00:46,  1.75it/s]Extractor Estimating: 80it [00:47,  1.70it/s]Extractor Estimating: 81it [00:47,  1.64it/s]Extractor Estimating: 82it [00:48,  1.70it/s]Extractor Estimating: 83it [00:48,  1.64it/s]Extractor Estimating: 84it [00:49,  1.45it/s]Extractor Estimating: 85it [00:50,  1.47it/s]Extractor Estimating: 86it [00:50,  1.56it/s]Extractor Estimating: 87it [00:51,  1.55it/s]Extractor Estimating: 88it [00:52,  1.61it/s]Extractor Estimating: 89it [00:52,  1.59it/s]Extractor Estimating: 90it [00:53,  1.54it/s]Extractor Estimating: 91it [00:54,  1.56it/s]Extractor Estimating: 92it [00:54,  1.59it/s]Extractor Estimating: 93it [00:55,  1.60it/s]Extractor Estimating: 94it [00:55,  1.64it/s]Extractor Estimating: 95it [00:56,  1.64it/s]Extractor Estimating: 96it [00:57,  1.69it/s]Extractor Estimating: 97it [00:57,  1.70it/s]Extractor Estimating: 98it [00:58,  1.64it/s]Extractor Estimating: 99it [00:59,  1.60it/s]Extractor Estimating: 100it [00:59,  1.62it/s]Extractor Estimating: 101it [01:00,  1.78it/s]Extractor Estimating: 102it [01:00,  1.81it/s]Extractor Estimating: 103it [01:01,  1.94it/s]Extractor Estimating: 104it [01:01,  2.03it/s]Extractor Estimating: 105it [01:01,  2.08it/s]Extractor Estimating: 106it [01:02,  2.20it/s]Extractor Estimating: 107it [01:02,  2.17it/s]Extractor Estimating: 108it [01:03,  2.24it/s]Extractor Estimating: 109it [01:03,  2.24it/s]Extractor Estimating: 110it [01:04,  2.27it/s]Extractor Estimating: 111it [01:04,  2.25it/s]Extractor Estimating: 112it [01:04,  2.35it/s]Extractor Estimating: 113it [01:05,  2.32it/s]Extractor Estimating: 114it [01:05,  2.30it/s]Extractor Estimating: 115it [01:06,  2.29it/s]Extractor Estimating: 116it [01:06,  2.32it/s]Extractor Estimating: 117it [01:07,  2.29it/s]Extractor Estimating: 118it [01:07,  2.22it/s]Extractor Estimating: 119it [01:08,  2.22it/s]Extractor Estimating: 120it [01:08,  2.19it/s]Extractor Estimating: 121it [01:08,  2.16it/s]Extractor Estimating: 122it [01:09,  2.11it/s]Extractor Estimating: 123it [01:09,  2.09it/s]Extractor Estimating: 124it [01:10,  2.10it/s]Extractor Estimating: 125it [01:10,  2.10it/s]Extractor Estimating: 126it [01:11,  1.94it/s]Extractor Estimating: 127it [01:12,  1.89it/s]Extractor Estimating: 128it [01:12,  1.88it/s]Extractor Estimating: 129it [01:13,  1.86it/s]Extractor Estimating: 130it [01:13,  1.88it/s]Extractor Estimating: 131it [01:14,  1.87it/s]Extractor Estimating: 132it [01:14,  1.85it/s]Extractor Estimating: 133it [01:15,  1.85it/s]Extractor Estimating: 134it [01:15,  1.81it/s]Extractor Estimating: 135it [01:16,  1.77it/s]Extractor Estimating: 136it [01:17,  1.79it/s]Extractor Estimating: 137it [01:17,  1.79it/s]Extractor Estimating: 138it [01:18,  1.80it/s]Extractor Estimating: 139it [01:18,  1.79it/s]Extractor Estimating: 140it [01:19,  1.84it/s]Extractor Estimating: 141it [01:19,  1.83it/s]Extractor Estimating: 142it [01:20,  1.82it/s]Extractor Estimating: 143it [01:20,  1.82it/s]Extractor Estimating: 144it [01:21,  1.84it/s]Extractor Estimating: 145it [01:21,  1.85it/s]Extractor Estimating: 146it [01:22,  1.85it/s]Extractor Estimating: 147it [01:23,  1.83it/s]Extractor Estimating: 148it [01:23,  1.76it/s]Extractor Estimating: 149it [01:24,  1.77it/s]Extractor Estimating: 150it [01:24,  1.79it/s]Extractor Estimating: 151it [01:25,  1.86it/s]Extractor Estimating: 152it [01:25,  1.91it/s]Extractor Estimating: 153it [01:26,  1.90it/s]Extractor Estimating: 154it [01:26,  1.95it/s]Extractor Estimating: 155it [01:27,  1.93it/s]Extractor Estimating: 156it [01:27,  1.94it/s]Extractor Estimating: 157it [01:28,  1.98it/s]Extractor Estimating: 158it [01:28,  1.99it/s]Extractor Estimating: 159it [01:29,  1.96it/s]Extractor Estimating: 160it [01:29,  1.99it/s]Extractor Estimating: 161it [01:30,  1.94it/s]Extractor Estimating: 162it [01:30,  1.99it/s]Extractor Estimating: 163it [01:31,  2.05it/s]Extractor Estimating: 164it [01:31,  1.91it/s]Extractor Estimating: 165it [01:32,  1.92it/s]Extractor Estimating: 166it [01:32,  1.92it/s]Extractor Estimating: 167it [01:33,  1.98it/s]Extractor Estimating: 168it [01:33,  1.92it/s]Extractor Estimating: 169it [01:34,  1.90it/s]Extractor Estimating: 170it [01:35,  1.88it/s]Extractor Estimating: 171it [01:35,  1.95it/s]Extractor Estimating: 172it [01:35,  1.96it/s]Extractor Estimating: 173it [01:36,  1.99it/s]Extractor Estimating: 174it [01:36,  1.99it/s]Extractor Estimating: 175it [01:37,  2.01it/s]Extractor Estimating: 176it [01:37,  1.97it/s]Extractor Estimating: 177it [01:38,  1.97it/s]Extractor Estimating: 178it [01:39,  1.90it/s]Extractor Estimating: 179it [01:39,  1.90it/s]Extractor Estimating: 180it [01:40,  1.97it/s]Extractor Estimating: 181it [01:40,  2.01it/s]Extractor Estimating: 182it [01:41,  1.85it/s]Extractor Estimating: 183it [01:41,  1.89it/s]Extractor Estimating: 184it [01:42,  1.87it/s]Extractor Estimating: 185it [01:42,  1.88it/s]Extractor Estimating: 186it [01:43,  1.88it/s]Extractor Estimating: 187it [01:43,  1.94it/s]Extractor Estimating: 188it [01:44,  1.94it/s]Extractor Estimating: 189it [01:44,  1.92it/s]Extractor Estimating: 190it [01:45,  2.00it/s]Extractor Estimating: 191it [01:45,  2.01it/s]Extractor Estimating: 192it [01:46,  2.09it/s]Extractor Estimating: 193it [01:46,  2.02it/s]Extractor Estimating: 194it [01:47,  2.02it/s]Extractor Estimating: 195it [01:47,  2.12it/s]Extractor Estimating: 196it [01:48,  2.06it/s]Extractor Estimating: 197it [01:48,  2.08it/s]Extractor Estimating: 198it [01:49,  2.06it/s]Extractor Estimating: 199it [01:49,  2.02it/s]Extractor Estimating: 200it [01:50,  2.03it/s]Extractor Estimating: 201it [01:50,  2.01it/s]Extractor Estimating: 202it [01:51,  1.92it/s]Extractor Estimating: 203it [01:51,  1.91it/s]Extractor Estimating: 204it [01:52,  1.90it/s]Extractor Estimating: 205it [01:52,  1.94it/s]Extractor Estimating: 206it [01:53,  1.86it/s]Extractor Estimating: 207it [01:53,  1.88it/s]Extractor Estimating: 208it [01:54,  1.90it/s]Extractor Estimating: 209it [01:54,  1.91it/s]Extractor Estimating: 210it [01:55,  1.91it/s]Extractor Estimating: 211it [01:55,  1.86it/s]Extractor Estimating: 212it [01:56,  1.90it/s]Extractor Estimating: 213it [01:57,  1.89it/s]Extractor Estimating: 214it [01:57,  1.93it/s]Extractor Estimating: 215it [01:58,  1.93it/s]Extractor Estimating: 216it [01:58,  1.94it/s]Extractor Estimating: 217it [01:59,  1.90it/s]Extractor Estimating: 218it [01:59,  1.87it/s]Extractor Estimating: 219it [02:00,  1.85it/s]Extractor Estimating: 220it [02:00,  1.84it/s]Extractor Estimating: 221it [02:01,  1.79it/s]Extractor Estimating: 222it [02:01,  1.85it/s]Extractor Estimating: 223it [02:02,  1.85it/s]Extractor Estimating: 224it [02:02,  1.80it/s]Extractor Estimating: 225it [02:03,  1.85it/s]Extractor Estimating: 226it [02:04,  1.72it/s]Extractor Estimating: 227it [02:04,  1.71it/s]Extractor Estimating: 228it [02:05,  1.64it/s]Extractor Estimating: 229it [02:06,  1.59it/s]Extractor Estimating: 230it [02:06,  1.64it/s]Extractor Estimating: 231it [02:07,  1.66it/s]Extractor Estimating: 232it [02:07,  1.70it/s]Extractor Estimating: 233it [02:08,  1.68it/s]Extractor Estimating: 234it [02:09,  1.67it/s]Extractor Estimating: 235it [02:09,  1.65it/s]Extractor Estimating: 236it [02:10,  1.68it/s]Extractor Estimating: 237it [02:10,  1.74it/s]Extractor Estimating: 238it [02:11,  1.72it/s]Extractor Estimating: 239it [02:11,  1.68it/s]Extractor Estimating: 240it [02:12,  1.69it/s]Extractor Estimating: 241it [02:13,  1.70it/s]Extractor Estimating: 242it [02:13,  1.67it/s]Extractor Estimating: 243it [02:14,  1.64it/s]Extractor Estimating: 244it [02:14,  1.65it/s]Extractor Estimating: 245it [02:15,  1.62it/s]Extractor Estimating: 246it [02:16,  1.56it/s]Extractor Estimating: 247it [02:16,  1.59it/s]Extractor Estimating: 248it [02:17,  1.64it/s]Extractor Estimating: 249it [02:18,  1.63it/s]Extractor Estimating: 250it [02:18,  1.58it/s]Extractor Estimating: 251it [02:19,  1.53it/s]Extractor Estimating: 252it [02:20,  1.55it/s]Extractor Estimating: 253it [02:20,  1.54it/s]Extractor Estimating: 254it [02:21,  1.50it/s]Extractor Estimating: 255it [02:22,  1.51it/s]Extractor Estimating: 256it [02:22,  1.54it/s]Extractor Estimating: 257it [02:23,  1.52it/s]Extractor Estimating: 258it [02:24,  1.39it/s]Extractor Estimating: 259it [02:24,  1.43it/s]Extractor Estimating: 260it [02:25,  1.46it/s]Extractor Estimating: 261it [02:26,  1.47it/s]Extractor Estimating: 262it [02:26,  1.49it/s]Extractor Estimating: 263it [02:27,  1.50it/s]Extractor Estimating: 264it [02:28,  1.53it/s]Extractor Estimating: 265it [02:28,  1.50it/s]Extractor Estimating: 266it [02:29,  1.55it/s]Extractor Estimating: 267it [02:30,  1.56it/s]Extractor Estimating: 268it [02:30,  1.42it/s]Extractor Estimating: 269it [02:31,  1.45it/s]Extractor Estimating: 270it [02:32,  1.53it/s]Extractor Estimating: 271it [02:32,  1.50it/s]Extractor Estimating: 272it [02:33,  1.51it/s]Extractor Estimating: 273it [02:34,  1.50it/s]Extractor Estimating: 274it [02:34,  1.45it/s]Extractor Estimating: 275it [02:35,  1.48it/s]Extractor Estimating: 276it [02:36,  1.56it/s]Extractor Estimating: 277it [02:36,  1.64it/s]Extractor Estimating: 278it [02:37,  1.64it/s]Extractor Estimating: 279it [02:37,  1.64it/s]Extractor Estimating: 280it [02:38,  1.70it/s]Extractor Estimating: 281it [02:39,  1.72it/s]Extractor Estimating: 282it [02:39,  1.73it/s]Extractor Estimating: 283it [02:40,  1.71it/s]Extractor Estimating: 284it [02:40,  1.77it/s]Extractor Estimating: 285it [02:41,  1.78it/s]Extractor Estimating: 286it [02:41,  1.75it/s]Extractor Estimating: 287it [02:42,  1.76it/s]Extractor Estimating: 288it [02:43,  1.73it/s]Extractor Estimating: 289it [02:43,  1.72it/s]Extractor Estimating: 290it [02:44,  1.72it/s]Extractor Estimating: 291it [02:44,  1.78it/s]Extractor Estimating: 292it [02:45,  1.77it/s]Extractor Estimating: 293it [02:45,  1.80it/s]Extractor Estimating: 294it [02:46,  1.82it/s]Extractor Estimating: 295it [02:46,  1.81it/s]Extractor Estimating: 296it [02:47,  1.82it/s]Extractor Estimating: 297it [02:47,  1.84it/s]Extractor Estimating: 298it [02:48,  1.76it/s]Extractor Estimating: 299it [02:49,  1.78it/s]Extractor Estimating: 300it [02:49,  1.81it/s]Extractor Estimating: 301it [02:50,  1.95it/s]Extractor Estimating: 302it [02:50,  2.09it/s]Extractor Estimating: 303it [02:50,  2.16it/s]Extractor Estimating: 304it [02:51,  2.15it/s]Extractor Estimating: 305it [02:51,  2.23it/s]Extractor Estimating: 306it [02:52,  2.25it/s]Extractor Estimating: 307it [02:52,  2.30it/s]Extractor Estimating: 308it [02:53,  2.35it/s]Extractor Estimating: 309it [02:53,  2.31it/s]Extractor Estimating: 310it [02:53,  2.38it/s]Extractor Estimating: 311it [02:54,  2.39it/s]Extractor Estimating: 312it [02:54,  2.33it/s]Extractor Estimating: 313it [02:55,  2.33it/s]Extractor Estimating: 314it [02:55,  2.35it/s]Extractor Estimating: 315it [02:56,  2.33it/s]Extractor Estimating: 316it [02:56,  2.27it/s]Extractor Estimating: 317it [02:56,  2.22it/s]Extractor Estimating: 318it [02:57,  2.17it/s]Extractor Estimating: 319it [02:57,  2.15it/s]Extractor Estimating: 320it [02:58,  2.21it/s]Extractor Estimating: 321it [02:58,  2.22it/s]Extractor Estimating: 322it [02:59,  2.26it/s]Extractor Estimating: 323it [02:59,  2.23it/s]Extractor Estimating: 324it [03:00,  2.15it/s]Extractor Estimating: 325it [03:00,  2.00it/s]Extractor Estimating: 326it [03:01,  1.83it/s]Extractor Estimating: 327it [03:02,  1.78it/s]Extractor Estimating: 328it [03:02,  1.76it/s]Extractor Estimating: 329it [03:03,  1.75it/s]Extractor Estimating: 330it [03:03,  1.77it/s]Extractor Estimating: 331it [03:04,  1.78it/s]Extractor Estimating: 332it [03:04,  1.77it/s]Extractor Estimating: 333it [03:05,  1.75it/s]Extractor Estimating: 334it [03:06,  1.79it/s]Extractor Estimating: 335it [03:06,  1.79it/s]Extractor Estimating: 336it [03:07,  1.70it/s]Extractor Estimating: 337it [03:07,  1.74it/s]Extractor Estimating: 338it [03:08,  1.76it/s]Extractor Estimating: 339it [03:08,  1.72it/s]Extractor Estimating: 340it [03:09,  1.64it/s]Extractor Estimating: 341it [03:10,  1.61it/s]Extractor Estimating: 342it [03:10,  1.63it/s]Extractor Estimating: 343it [03:11,  1.67it/s]Extractor Estimating: 344it [03:11,  1.68it/s]Extractor Estimating: 345it [03:12,  1.64it/s]Extractor Estimating: 346it [03:13,  1.68it/s]Extractor Estimating: 347it [03:13,  1.65it/s]Extractor Estimating: 348it [03:14,  1.71it/s]Extractor Estimating: 349it [03:15,  1.63it/s]Extractor Estimating: 350it [03:15,  1.71it/s]Extractor Estimating: 351it [03:16,  1.71it/s]Extractor Estimating: 352it [03:16,  1.78it/s]Extractor Estimating: 353it [03:17,  1.75it/s]Extractor Estimating: 354it [03:17,  1.77it/s]Extractor Estimating: 355it [03:18,  1.83it/s]Extractor Estimating: 356it [03:18,  1.88it/s]Extractor Estimating: 357it [03:19,  1.88it/s]Extractor Estimating: 358it [03:19,  1.83it/s]Extractor Estimating: 359it [03:20,  1.85it/s]Extractor Estimating: 360it [03:21,  1.76it/s]Extractor Estimating: 361it [03:21,  1.77it/s]Extractor Estimating: 362it [03:22,  1.69it/s]Extractor Estimating: 363it [03:22,  1.67it/s]Extractor Estimating: 364it [03:23,  1.72it/s]Extractor Estimating: 365it [03:24,  1.61it/s]Extractor Estimating: 366it [03:24,  1.67it/s]Extractor Estimating: 367it [03:25,  1.73it/s]Extractor Estimating: 368it [03:25,  1.75it/s]Extractor Estimating: 369it [03:26,  1.79it/s]Extractor Estimating: 370it [03:26,  1.80it/s]Extractor Estimating: 371it [03:27,  1.77it/s]Extractor Estimating: 372it [03:27,  1.84it/s]Extractor Estimating: 373it [03:28,  1.82it/s]Extractor Estimating: 374it [03:29,  1.83it/s]Extractor Estimating: 375it [03:29,  1.82it/s]Extractor Estimating: 376it [03:30,  1.84it/s]Extractor Estimating: 377it [03:30,  1.80it/s]Extractor Estimating: 378it [03:31,  1.63it/s]Extractor Estimating: 379it [03:32,  1.65it/s]Extractor Estimating: 380it [03:32,  1.66it/s]Extractor Estimating: 381it [03:33,  1.62it/s]Extractor Estimating: 382it [03:33,  1.57it/s]Extractor Estimating: 383it [03:34,  1.57it/s]Extractor Estimating: 384it [03:35,  1.60it/s]Extractor Estimating: 385it [03:35,  1.65it/s]Extractor Estimating: 386it [03:36,  1.66it/s]Extractor Estimating: 387it [03:36,  1.65it/s]Extractor Estimating: 388it [03:37,  1.63it/s]Extractor Estimating: 389it [03:38,  1.60it/s]Extractor Estimating: 390it [03:38,  1.59it/s]Extractor Estimating: 391it [03:39,  1.50it/s]Extractor Estimating: 392it [03:40,  1.58it/s]Extractor Estimating: 393it [03:40,  1.58it/s]Extractor Estimating: 394it [03:41,  1.58it/s]Extractor Estimating: 395it [03:42,  1.59it/s]Extractor Estimating: 396it [03:42,  1.60it/s]Extractor Estimating: 397it [03:43,  1.62it/s]Extractor Estimating: 398it [03:43,  1.60it/s]Extractor Estimating: 399it [03:44,  1.64it/s]Extractor Estimating: 400it [03:45,  1.64it/s]Extractor Estimating: 401it [03:45,  1.67it/s]Extractor Estimating: 402it [03:46,  1.68it/s]Extractor Estimating: 403it [03:46,  1.70it/s]Extractor Estimating: 404it [03:47,  1.72it/s]Extractor Estimating: 405it [03:48,  1.70it/s]Extractor Estimating: 406it [03:48,  1.66it/s]Extractor Estimating: 407it [03:49,  1.69it/s]Extractor Estimating: 408it [03:49,  1.73it/s]Extractor Estimating: 409it [03:50,  1.66it/s]Extractor Estimating: 410it [03:51,  1.69it/s]Extractor Estimating: 411it [03:51,  1.74it/s]Extractor Estimating: 412it [03:52,  1.76it/s]Extractor Estimating: 413it [03:52,  1.78it/s]Extractor Estimating: 414it [03:53,  1.75it/s]Extractor Estimating: 415it [03:53,  1.72it/s]Extractor Estimating: 416it [03:54,  1.71it/s]Extractor Estimating: 417it [03:55,  1.66it/s]Extractor Estimating: 418it [03:55,  1.64it/s]Extractor Estimating: 419it [03:56,  1.68it/s]Extractor Estimating: 420it [03:56,  1.65it/s]Extractor Estimating: 421it [03:57,  1.61it/s]Extractor Estimating: 422it [03:58,  1.64it/s]Extractor Estimating: 423it [03:58,  1.67it/s]Extractor Estimating: 424it [03:59,  1.74it/s]Extractor Estimating: 425it [03:59,  1.73it/s]Extractor Estimating: 426it [04:00,  1.64it/s]Extractor Estimating: 427it [04:01,  1.64it/s]Extractor Estimating: 428it [04:01,  1.65it/s]Extractor Estimating: 429it [04:02,  1.68it/s]Extractor Estimating: 430it [04:02,  1.66it/s]Extractor Estimating: 431it [04:03,  1.65it/s]Extractor Estimating: 432it [04:04,  1.63it/s]Extractor Estimating: 433it [04:04,  1.70it/s]Extractor Estimating: 434it [04:05,  1.71it/s]Extractor Estimating: 435it [04:05,  1.67it/s]Extractor Estimating: 436it [04:06,  1.71it/s]Extractor Estimating: 437it [04:07,  1.68it/s]Extractor Estimating: 438it [04:07,  1.59it/s]Extractor Estimating: 439it [04:08,  1.58it/s]Extractor Estimating: 440it [04:08,  1.63it/s]Extractor Estimating: 441it [04:09,  1.63it/s]Extractor Estimating: 442it [04:10,  1.62it/s]Extractor Estimating: 443it [04:10,  1.65it/s]Extractor Estimating: 444it [04:11,  1.67it/s]Extractor Estimating: 445it [04:11,  1.68it/s]Extractor Estimating: 446it [04:12,  1.70it/s]Extractor Estimating: 447it [04:13,  1.71it/s]Extractor Estimating: 448it [04:13,  1.77it/s]Extractor Estimating: 449it [04:14,  1.76it/s]Extractor Estimating: 450it [04:14,  1.76it/s]Extractor Estimating: 451it [04:15,  1.80it/s]Extractor Estimating: 452it [04:15,  1.85it/s]Extractor Estimating: 453it [04:16,  1.86it/s]Extractor Estimating: 454it [04:16,  1.87it/s]Extractor Estimating: 455it [04:17,  1.88it/s]Extractor Estimating: 456it [04:17,  1.90it/s]Extractor Estimating: 457it [04:18,  1.90it/s]Extractor Estimating: 458it [04:18,  1.89it/s]Extractor Estimating: 459it [04:19,  1.88it/s]Extractor Estimating: 460it [04:20,  1.90it/s]Extractor Estimating: 461it [04:20,  1.90it/s]Extractor Estimating: 462it [04:21,  1.93it/s]Extractor Estimating: 463it [04:21,  1.90it/s]Extractor Estimating: 464it [04:22,  1.89it/s]Extractor Estimating: 465it [04:22,  1.83it/s]Extractor Estimating: 466it [04:23,  1.86it/s]Extractor Estimating: 467it [04:23,  1.90it/s]Extractor Estimating: 468it [04:24,  1.90it/s]Extractor Estimating: 469it [04:24,  1.92it/s]Extractor Estimating: 470it [04:25,  1.81it/s]Extractor Estimating: 471it [04:25,  1.79it/s]Extractor Estimating: 472it [04:26,  1.87it/s]Extractor Estimating: 473it [04:27,  1.80it/s]Extractor Estimating: 474it [04:27,  1.85it/s]Extractor Estimating: 475it [04:28,  1.87it/s]Extractor Estimating: 476it [04:28,  1.83it/s]Extractor Estimating: 477it [04:29,  1.67it/s]Extractor Estimating: 478it [04:29,  1.70it/s]Extractor Estimating: 479it [04:30,  1.73it/s]Extractor Estimating: 480it [04:31,  1.77it/s]Extractor Estimating: 481it [04:31,  1.71it/s]Extractor Estimating: 482it [04:32,  1.83it/s]Extractor Estimating: 483it [04:32,  1.86it/s]Extractor Estimating: 484it [04:33,  1.83it/s]Extractor Estimating: 485it [04:33,  1.86it/s]Extractor Estimating: 486it [04:34,  1.86it/s]Extractor Estimating: 487it [04:34,  1.80it/s]Extractor Estimating: 488it [04:35,  1.78it/s]Extractor Estimating: 489it [04:36,  1.75it/s]Extractor Estimating: 490it [04:36,  1.81it/s]Extractor Estimating: 491it [04:37,  1.78it/s]Extractor Estimating: 492it [04:37,  1.80it/s]Extractor Estimating: 493it [04:38,  1.77it/s]Extractor Estimating: 494it [04:38,  1.77it/s]Extractor Estimating: 495it [04:39,  1.85it/s]Extractor Estimating: 496it [04:39,  1.87it/s]Extractor Estimating: 497it [04:40,  1.85it/s]Extractor Estimating: 498it [04:40,  1.86it/s]Extractor Estimating: 499it [04:41,  1.86it/s]Extractor Estimating: 500it [04:41,  2.10it/s]Extractor Estimating: 500it [04:41,  1.77it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:53:55,901 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:53:55,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:53:55,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:53:55,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:53:55,907 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 03:53:56,531 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 03:53:56,532 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:53:57,111 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 03:53:58,167 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:53:58,167 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:01,079 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:01,083 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:01,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:01,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 03:54:01,084 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 03:54:01,748 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 03:54:01,750 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 03:54:02,338 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 03:54:02,513 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 03:54:02,513 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 06:05:36,036 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 06:05:36,070 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 8000, 'num_train': 1999}
num of filtered data: 7985 mean pseudo reward: 0.9708700660913017
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 14932
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15032, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15032, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.007, loss:302.2727
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.943, loss:303.0702
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.948, loss:285.0944
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 67, avg_time 0.955, loss:286.4900
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 167, avg_time 0.949, loss:299.7235
>> valid entity prec:0.5925, rec:0.5536, f1:0.5724
>> valid relation prec:0.2249, rec:0.1431, f1:0.1749
>> valid relation with NER prec:0.2249, rec:0.1431, f1:0.1749
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 267, avg_time 2.126, loss:281.8316
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 34, avg_time 0.962, loss:258.0786
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 134, avg_time 0.954, loss:259.7961
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 234, avg_time 0.960, loss:275.6298
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1, avg_time 0.944, loss:306.0774
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5588, rec:0.6220, f1:0.5887
>> valid relation prec:0.1992, rec:0.1475, f1:0.1695
>> valid relation with NER prec:0.1992, rec:0.1475, f1:0.1695
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 101, avg_time 2.129, loss:266.3374
g_step 1200, step 201, avg_time 0.961, loss:254.4106
g_step 1300, step 301, avg_time 0.957, loss:271.0954
g_step 1400, step 68, avg_time 0.942, loss:252.8678
g_step 1500, step 168, avg_time 0.945, loss:236.1611
>> valid entity prec:0.5583, rec:0.5718, f1:0.5650
>> valid relation prec:0.1882, rec:0.1315, f1:0.1548
>> valid relation with NER prec:0.1882, rec:0.1315, f1:0.1548
g_step 1600, step 268, avg_time 2.137, loss:247.9665
g_step 1700, step 35, avg_time 0.951, loss:242.1337
g_step 1800, step 135, avg_time 0.952, loss:220.3787
g_step 1900, step 235, avg_time 0.966, loss:237.3002
g_step 2000, step 2, avg_time 0.954, loss:250.7518
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5768, rec:0.5148, f1:0.5440
>> valid relation prec:0.2101, rec:0.1280, f1:0.1591
>> valid relation with NER prec:0.2101, rec:0.1280, f1:0.1591
g_step 2100, step 102, avg_time 2.130, loss:190.7948
g_step 2200, step 202, avg_time 0.959, loss:227.8525
g_step 2300, step 302, avg_time 0.958, loss:226.1728
g_step 2400, step 69, avg_time 0.943, loss:209.2255
g_step 2500, step 169, avg_time 0.947, loss:211.5412
>> valid entity prec:0.5616, rec:0.5575, f1:0.5595
>> valid relation prec:0.2050, rec:0.1332, f1:0.1615
>> valid relation with NER prec:0.2050, rec:0.1332, f1:0.1615
g_step 2600, step 269, avg_time 2.138, loss:209.2933
g_step 2700, step 36, avg_time 0.948, loss:205.6934
g_step 2800, step 136, avg_time 0.957, loss:193.1794
g_step 2900, step 236, avg_time 0.952, loss:203.3483
g_step 3000, step 3, avg_time 0.953, loss:224.3618
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5823, rec:0.5500, f1:0.5657
>> valid relation prec:0.1953, rec:0.1266, f1:0.1536
>> valid relation with NER prec:0.1953, rec:0.1266, f1:0.1536
g_step 3100, step 103, avg_time 2.133, loss:182.3937
g_step 3200, step 203, avg_time 0.964, loss:179.7889
g_step 3300, step 303, avg_time 0.940, loss:197.8747
g_step 3400, step 70, avg_time 0.947, loss:174.3164
g_step 3500, step 170, avg_time 0.958, loss:180.0885
>> valid entity prec:0.5824, rec:0.5545, f1:0.5681
>> valid relation prec:0.1924, rec:0.1312, f1:0.1560
>> valid relation with NER prec:0.1924, rec:0.1312, f1:0.1560
g_step 3600, step 270, avg_time 2.129, loss:173.8054
g_step 3700, step 37, avg_time 0.948, loss:177.2964
g_step 3800, step 137, avg_time 0.959, loss:167.4426
g_step 3900, step 237, avg_time 0.957, loss:168.5722
g_step 4000, step 4, avg_time 0.962, loss:183.7676
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5707, rec:0.5410, f1:0.5555
>> valid relation prec:0.2166, rec:0.1399, f1:0.1700
>> valid relation with NER prec:0.2166, rec:0.1399, f1:0.1700
g_step 4100, step 104, avg_time 2.136, loss:157.0697
g_step 4200, step 204, avg_time 0.966, loss:162.6001
g_step 4300, step 304, avg_time 0.945, loss:169.8726
g_step 4400, step 71, avg_time 0.953, loss:152.8678
g_step 4500, step 171, avg_time 0.968, loss:155.0016
>> valid entity prec:0.5645, rec:0.5551, f1:0.5598
>> valid relation prec:0.1953, rec:0.1315, f1:0.1572
>> valid relation with NER prec:0.1953, rec:0.1315, f1:0.1572
g_step 4600, step 271, avg_time 2.120, loss:164.4207
g_step 4700, step 38, avg_time 0.955, loss:167.5587
g_step 4800, step 138, avg_time 0.955, loss:154.0916
g_step 4900, step 238, avg_time 0.957, loss:154.1980
g_step 5000, step 5, avg_time 0.942, loss:161.8571
learning rate was adjusted to 0.0008
>> valid entity prec:0.5692, rec:0.5386, f1:0.5534
>> valid relation prec:0.1869, rec:0.1321, f1:0.1548
>> valid relation with NER prec:0.1869, rec:0.1321, f1:0.1548
g_step 5100, step 105, avg_time 2.138, loss:132.2876
g_step 5200, step 205, avg_time 0.935, loss:141.5632
g_step 5300, step 305, avg_time 0.976, loss:158.7947
g_step 5400, step 72, avg_time 0.940, loss:139.7142
g_step 5500, step 172, avg_time 0.972, loss:135.2131
>> valid entity prec:0.5655, rec:0.5778, f1:0.5716
>> valid relation prec:0.1798, rec:0.1448, f1:0.1605
>> valid relation with NER prec:0.1798, rec:0.1448, f1:0.1605
g_step 5600, step 272, avg_time 2.127, loss:141.6698
g_step 5700, step 39, avg_time 0.972, loss:139.3812
g_step 5800, step 139, avg_time 0.944, loss:137.6355
g_step 5900, step 239, avg_time 0.956, loss:132.8713
g_step 6000, step 6, avg_time 0.949, loss:158.6792
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5782, rec:0.5591, f1:0.5685
>> valid relation prec:0.1907, rec:0.1312, f1:0.1555
>> valid relation with NER prec:0.1907, rec:0.1312, f1:0.1555
g_step 6100, step 106, avg_time 2.144, loss:133.0123
g_step 6200, step 206, avg_time 0.951, loss:138.2323
g_step 6300, step 306, avg_time 0.959, loss:137.0267
g_step 6400, step 73, avg_time 0.951, loss:120.5724
g_step 6500, step 173, avg_time 0.957, loss:134.8776
>> valid entity prec:0.5684, rec:0.5790, f1:0.5737
>> valid relation prec:0.2039, rec:0.1367, f1:0.1637
>> valid relation with NER prec:0.2039, rec:0.1367, f1:0.1637
g_step 6600, step 273, avg_time 2.138, loss:132.4913
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 06:05:36 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 06:05:36 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_06-05-36_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 06:05:37 - WARNING - datasets.builder -   Using custom data configuration default-209b14edc2d1c485
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-209b14edc2d1c485/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 06:05:37,305 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:05:37,306 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:05:37,307 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:05:37,308 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:05:37,318 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:05:37,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:05:37,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:05:37,321 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:05:37,321 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:05:37,322 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:05:37,322 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 06:05:37,463 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:05:40,417 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 06:05:40,421 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-209b14edc2d1c485/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:02,  2.41ba/s] 25%|██▌       | 2/8 [00:00<00:01,  3.53ba/s] 38%|███▊      | 3/8 [00:00<00:01,  4.14ba/s] 50%|█████     | 4/8 [00:00<00:00,  4.51ba/s] 62%|██████▎   | 5/8 [00:01<00:00,  4.78ba/s] 75%|███████▌  | 6/8 [00:01<00:00,  4.95ba/s] 88%|████████▊ | 7/8 [00:01<00:00,  5.04ba/s]100%|██████████| 8/8 [00:01<00:00,  5.11ba/s]100%|██████████| 8/8 [00:01<00:00,  4.58ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.46ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.62ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.72ba/s]100%|██████████| 4/4 [00:00<00:00,  5.43ba/s]
  0%|          | 0/8 [00:00<?, ?ba/s] 12%|█▎        | 1/8 [00:00<00:00,  9.22ba/s] 38%|███▊      | 3/8 [00:00<00:00, 10.89ba/s] 62%|██████▎   | 5/8 [00:00<00:00, 11.24ba/s] 88%|████████▊ | 7/8 [00:00<00:00, 11.17ba/s]100%|██████████| 8/8 [00:00<00:00, 11.11ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.66ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.76ba/s]100%|██████████| 4/4 [00:00<00:00, 12.37ba/s]
[INFO|trainer.py:414] 2023-08-29 06:05:44,359 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 06:05:44,374 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 06:05:44,374 >>   Num examples = 8000
[INFO|trainer.py:1149] 2023-08-29 06:05:44,374 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 06:05:44,374 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 06:05:44,374 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 06:05:44,374 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 06:05:44,374 >>   Total optimization steps = 625
  0%|          | 0/625 [00:00<?, ?it/s]  0%|          | 1/625 [00:00<03:07,  3.33it/s]  0%|          | 2/625 [00:00<03:01,  3.43it/s]  0%|          | 3/625 [00:00<02:59,  3.46it/s]  1%|          | 4/625 [00:01<02:58,  3.47it/s]  1%|          | 5/625 [00:01<02:58,  3.48it/s]  1%|          | 6/625 [00:01<02:57,  3.49it/s]  1%|          | 7/625 [00:02<02:56,  3.50it/s]  1%|▏         | 8/625 [00:02<02:56,  3.50it/s]  1%|▏         | 9/625 [00:02<02:56,  3.49it/s]  2%|▏         | 10/625 [00:02<02:56,  3.49it/s]  2%|▏         | 11/625 [00:03<02:55,  3.50it/s]  2%|▏         | 12/625 [00:03<02:55,  3.50it/s]  2%|▏         | 13/625 [00:03<02:55,  3.50it/s]  2%|▏         | 14/625 [00:04<02:54,  3.50it/s]  2%|▏         | 15/625 [00:04<02:54,  3.49it/s]  3%|▎         | 16/625 [00:04<02:54,  3.50it/s]  3%|▎         | 17/625 [00:04<02:53,  3.50it/s]  3%|▎         | 18/625 [00:05<02:53,  3.50it/s]  3%|▎         | 19/625 [00:05<02:53,  3.50it/s]  3%|▎         | 20/625 [00:05<02:53,  3.49it/s]  3%|▎         | 21/625 [00:06<02:52,  3.49it/s]  4%|▎         | 22/625 [00:06<02:52,  3.50it/s]  4%|▎         | 23/625 [00:06<02:52,  3.50it/s]  4%|▍         | 24/625 [00:06<02:51,  3.50it/s]  4%|▍         | 25/625 [00:07<02:51,  3.50it/s]  4%|▍         | 26/625 [00:07<02:51,  3.50it/s]  4%|▍         | 27/625 [00:07<02:51,  3.49it/s]  4%|▍         | 28/625 [00:08<02:51,  3.49it/s]  5%|▍         | 29/625 [00:08<02:50,  3.49it/s]  5%|▍         | 30/625 [00:08<02:50,  3.49it/s]  5%|▍         | 31/625 [00:08<02:50,  3.48it/s]  5%|▌         | 32/625 [00:09<02:49,  3.49it/s]  5%|▌         | 33/625 [00:09<02:49,  3.49it/s]  5%|▌         | 34/625 [00:09<02:49,  3.50it/s]  6%|▌         | 35/625 [00:10<02:48,  3.49it/s]  6%|▌         | 36/625 [00:10<02:48,  3.50it/s]  6%|▌         | 37/625 [00:10<02:48,  3.50it/s]  6%|▌         | 38/625 [00:10<02:47,  3.50it/s]  6%|▌         | 39/625 [00:11<02:47,  3.50it/s]  6%|▋         | 40/625 [00:11<02:47,  3.50it/s]  7%|▋         | 41/625 [00:11<02:47,  3.50it/s]  7%|▋         | 42/625 [00:12<02:46,  3.49it/s]  7%|▋         | 43/625 [00:12<02:46,  3.49it/s]  7%|▋         | 44/625 [00:12<02:46,  3.49it/s]  7%|▋         | 45/625 [00:12<02:45,  3.50it/s]  7%|▋         | 46/625 [00:13<02:45,  3.49it/s]  8%|▊         | 47/625 [00:13<02:45,  3.50it/s]  8%|▊         | 48/625 [00:13<02:45,  3.50it/s]  8%|▊         | 49/625 [00:14<02:44,  3.50it/s]  8%|▊         | 50/625 [00:14<02:44,  3.50it/s]  8%|▊         | 51/625 [00:14<02:44,  3.49it/s]  8%|▊         | 52/625 [00:14<02:43,  3.50it/s]  8%|▊         | 53/625 [00:15<02:43,  3.49it/s]  9%|▊         | 54/625 [00:15<02:43,  3.49it/s]  9%|▉         | 55/625 [00:15<02:43,  3.49it/s]  9%|▉         | 56/625 [00:16<02:42,  3.50it/s]  9%|▉         | 57/625 [00:16<02:42,  3.50it/s]  9%|▉         | 58/625 [00:16<02:42,  3.50it/s]  9%|▉         | 59/625 [00:16<02:41,  3.50it/s] 10%|▉         | 60/625 [00:17<02:41,  3.50it/s] 10%|▉         | 61/625 [00:17<02:41,  3.50it/s] 10%|▉         | 62/625 [00:17<02:40,  3.50it/s] 10%|█         | 63/625 [00:18<02:40,  3.50it/s] 10%|█         | 64/625 [00:18<02:40,  3.49it/s] 10%|█         | 65/625 [00:18<02:40,  3.49it/s] 11%|█         | 66/625 [00:18<02:40,  3.49it/s] 11%|█         | 67/625 [00:19<02:39,  3.49it/s] 11%|█         | 68/625 [00:19<02:39,  3.49it/s] 11%|█         | 69/625 [00:19<02:38,  3.50it/s] 11%|█         | 70/625 [00:20<02:38,  3.49it/s] 11%|█▏        | 71/625 [00:20<02:38,  3.50it/s] 12%|█▏        | 72/625 [00:20<02:38,  3.50it/s] 12%|█▏        | 73/625 [00:20<02:37,  3.50it/s] 12%|█▏        | 74/625 [00:21<02:37,  3.49it/s] 12%|█▏        | 75/625 [00:21<02:37,  3.49it/s] 12%|█▏        | 76/625 [00:21<02:37,  3.49it/s] 12%|█▏        | 77/625 [00:22<02:36,  3.50it/s] 12%|█▏        | 78/625 [00:22<02:36,  3.49it/s] 13%|█▎        | 79/625 [00:22<02:36,  3.49it/s] 13%|█▎        | 80/625 [00:22<02:36,  3.49it/s] 13%|█▎        | 81/625 [00:23<02:35,  3.50it/s] 13%|█▎        | 82/625 [00:23<02:35,  3.49it/s] 13%|█▎        | 83/625 [00:23<02:35,  3.49it/s] 13%|█▎        | 84/625 [00:24<02:34,  3.49it/s] 14%|█▎        | 85/625 [00:24<02:34,  3.50it/s] 14%|█▍        | 86/625 [00:24<02:34,  3.48it/s] 14%|█▍        | 87/625 [00:24<02:34,  3.49it/s] 14%|█▍        | 88/625 [00:25<02:33,  3.49it/s] 14%|█▍        | 89/625 [00:25<02:33,  3.49it/s] 14%|█▍        | 90/625 [00:25<02:33,  3.49it/s] 15%|█▍        | 91/625 [00:26<02:32,  3.49it/s] 15%|█▍        | 92/625 [00:26<02:32,  3.49it/s] 15%|█▍        | 93/625 [00:26<02:32,  3.50it/s] 15%|█▌        | 94/625 [00:26<02:32,  3.49it/s] 15%|█▌        | 95/625 [00:27<02:31,  3.49it/s] 15%|█▌        | 96/625 [00:27<02:31,  3.49it/s] 16%|█▌        | 97/625 [00:27<02:31,  3.49it/s] 16%|█▌        | 98/625 [00:28<02:30,  3.49it/s] 16%|█▌        | 99/625 [00:28<02:30,  3.49it/s] 16%|█▌        | 100/625 [00:28<02:30,  3.48it/s] 16%|█▌        | 101/625 [00:28<02:30,  3.48it/s] 16%|█▋        | 102/625 [00:29<02:29,  3.49it/s] 16%|█▋        | 103/625 [00:29<02:29,  3.49it/s] 17%|█▋        | 104/625 [00:29<02:29,  3.49it/s] 17%|█▋        | 105/625 [00:30<02:28,  3.49it/s] 17%|█▋        | 106/625 [00:30<02:28,  3.49it/s] 17%|█▋        | 107/625 [00:30<02:28,  3.49it/s] 17%|█▋        | 108/625 [00:30<02:27,  3.49it/s] 17%|█▋        | 109/625 [00:31<02:27,  3.49it/s] 18%|█▊        | 110/625 [00:31<02:27,  3.49it/s] 18%|█▊        | 111/625 [00:31<02:28,  3.47it/s] 18%|█▊        | 112/625 [00:32<02:27,  3.47it/s] 18%|█▊        | 113/625 [00:32<02:27,  3.48it/s] 18%|█▊        | 114/625 [00:32<02:26,  3.48it/s] 18%|█▊        | 115/625 [00:32<02:26,  3.48it/s] 19%|█▊        | 116/625 [00:33<02:25,  3.49it/s] 19%|█▊        | 117/625 [00:33<02:25,  3.49it/s] 19%|█▉        | 118/625 [00:33<02:25,  3.49it/s] 19%|█▉        | 119/625 [00:34<02:24,  3.49it/s] 19%|█▉        | 120/625 [00:34<02:24,  3.49it/s] 19%|█▉        | 121/625 [00:34<02:24,  3.49it/s] 20%|█▉        | 122/625 [00:34<02:25,  3.46it/s] 20%|█▉        | 123/625 [00:35<02:24,  3.47it/s] 20%|█▉        | 124/625 [00:35<02:24,  3.48it/s] 20%|██        | 125/625 [00:35<02:23,  3.48it/s][INFO|trainer.py:2140] 2023-08-29 06:06:20,189 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:06:20,189 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 06:06:20,189 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.85it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.07it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.06it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.08it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.73it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.39it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.35it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.25it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.19it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.25it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.11it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.00it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 44.98it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 44.95it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 45.00it/s][A
 20%|██        | 87/431 [00:01<00:07, 45.09it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 45.04it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.21it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.23it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.18it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.03it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 44.90it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 44.88it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.90it/s][A
 31%|███       | 132/431 [00:02<00:06, 45.00it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 45.15it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.01it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.17it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.16it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 44.92it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 44.84it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 44.83it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.93it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.99it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 45.14it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.24it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.27it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.23it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.01it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 44.76it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 44.82it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.92it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 45.04it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 45.10it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 45.23it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.29it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.24it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 44.98it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 44.82it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 44.78it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.87it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 45.01it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 45.10it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.24it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.22it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.22it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.01it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 44.88it/s][A
 70%|███████   | 302/431 [00:06<00:02, 44.81it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.86it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.98it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 45.16it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.25it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.21it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.14it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.01it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 44.93it/s][A
 81%|████████  | 347/431 [00:07<00:01, 44.85it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 44.91it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.89it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 45.08it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 45.23it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.24it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.18it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 44.86it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 44.96it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 44.92it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 44.89it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 44.96it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.97it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.22it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.23it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.18it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.06it/s][A                                                 
                                                 [A 20%|██        | 125/625 [00:45<02:23,  3.48it/s]
100%|██████████| 431/431 [00:09<00:00, 45.06it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:06:29,775 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-125
[INFO|configuration_utils.py:351] 2023-08-29 06:06:29,800 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-125/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:06:31,289 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-125/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:06:31,306 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-125/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:06:31,318 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-125/special_tokens_map.json
 20%|██        | 126/625 [00:51<39:34,  4.76s/it] 20%|██        | 127/625 [00:51<28:22,  3.42s/it] 20%|██        | 128/625 [00:51<20:32,  2.48s/it] 21%|██        | 129/625 [00:51<15:07,  1.83s/it] 21%|██        | 130/625 [00:52<11:17,  1.37s/it] 21%|██        | 131/625 [00:52<08:36,  1.05s/it] 21%|██        | 132/625 [00:52<06:44,  1.22it/s] 21%|██▏       | 133/625 [00:53<05:25,  1.51it/s] 21%|██▏       | 134/625 [00:53<04:30,  1.82it/s] 22%|██▏       | 135/625 [00:53<03:51,  2.11it/s] 22%|██▏       | 136/625 [00:53<03:24,  2.39it/s] 22%|██▏       | 137/625 [00:54<03:05,  2.63it/s] 22%|██▏       | 138/625 [00:54<02:52,  2.82it/s] 22%|██▏       | 139/625 [00:54<02:42,  2.98it/s] 22%|██▏       | 140/625 [00:55<02:36,  3.10it/s] 23%|██▎       | 141/625 [00:55<02:31,  3.19it/s] 23%|██▎       | 142/625 [00:55<02:28,  3.26it/s] 23%|██▎       | 143/625 [00:55<02:25,  3.31it/s] 23%|██▎       | 144/625 [00:56<02:23,  3.35it/s] 23%|██▎       | 145/625 [00:56<02:22,  3.37it/s] 23%|██▎       | 146/625 [00:56<02:21,  3.39it/s] 24%|██▎       | 147/625 [00:57<02:20,  3.40it/s] 24%|██▎       | 148/625 [00:57<02:19,  3.41it/s] 24%|██▍       | 149/625 [00:57<02:19,  3.41it/s] 24%|██▍       | 150/625 [00:58<02:19,  3.41it/s] 24%|██▍       | 151/625 [00:58<02:18,  3.42it/s] 24%|██▍       | 152/625 [00:58<02:18,  3.42it/s] 24%|██▍       | 153/625 [00:58<02:17,  3.43it/s] 25%|██▍       | 154/625 [00:59<02:17,  3.42it/s] 25%|██▍       | 155/625 [00:59<02:17,  3.43it/s] 25%|██▍       | 156/625 [00:59<02:16,  3.43it/s] 25%|██▌       | 157/625 [01:00<02:16,  3.43it/s] 25%|██▌       | 158/625 [01:00<02:16,  3.43it/s] 25%|██▌       | 159/625 [01:00<02:16,  3.43it/s] 26%|██▌       | 160/625 [01:00<02:16,  3.42it/s] 26%|██▌       | 161/625 [01:01<02:15,  3.42it/s] 26%|██▌       | 162/625 [01:01<02:15,  3.42it/s] 26%|██▌       | 163/625 [01:01<02:14,  3.43it/s] 26%|██▌       | 164/625 [01:02<02:14,  3.43it/s] 26%|██▋       | 165/625 [01:02<02:14,  3.43it/s] 27%|██▋       | 166/625 [01:02<02:13,  3.43it/s] 27%|██▋       | 167/625 [01:02<02:13,  3.43it/s] 27%|██▋       | 168/625 [01:03<02:12,  3.45it/s] 27%|██▋       | 169/625 [01:03<02:11,  3.46it/s] 27%|██▋       | 170/625 [01:03<02:11,  3.46it/s] 27%|██▋       | 171/625 [01:04<02:11,  3.46it/s] 28%|██▊       | 172/625 [01:04<02:10,  3.47it/s] 28%|██▊       | 173/625 [01:04<02:10,  3.47it/s] 28%|██▊       | 174/625 [01:04<02:09,  3.48it/s] 28%|██▊       | 175/625 [01:05<02:09,  3.48it/s] 28%|██▊       | 176/625 [01:05<02:09,  3.48it/s] 28%|██▊       | 177/625 [01:05<02:08,  3.48it/s] 28%|██▊       | 178/625 [01:06<02:08,  3.48it/s] 29%|██▊       | 179/625 [01:06<02:08,  3.48it/s] 29%|██▉       | 180/625 [01:06<02:07,  3.48it/s] 29%|██▉       | 181/625 [01:07<02:07,  3.48it/s] 29%|██▉       | 182/625 [01:07<02:08,  3.46it/s] 29%|██▉       | 183/625 [01:07<02:07,  3.46it/s] 29%|██▉       | 184/625 [01:07<02:07,  3.47it/s] 30%|██▉       | 185/625 [01:08<02:06,  3.47it/s] 30%|██▉       | 186/625 [01:08<02:06,  3.48it/s] 30%|██▉       | 187/625 [01:08<02:05,  3.48it/s] 30%|███       | 188/625 [01:09<02:05,  3.48it/s] 30%|███       | 189/625 [01:09<02:05,  3.48it/s] 30%|███       | 190/625 [01:09<02:04,  3.49it/s] 31%|███       | 191/625 [01:09<02:04,  3.48it/s] 31%|███       | 192/625 [01:10<02:04,  3.49it/s] 31%|███       | 193/625 [01:10<02:04,  3.46it/s] 31%|███       | 194/625 [01:10<02:04,  3.47it/s] 31%|███       | 195/625 [01:11<02:03,  3.47it/s] 31%|███▏      | 196/625 [01:11<02:03,  3.48it/s] 32%|███▏      | 197/625 [01:11<02:03,  3.48it/s] 32%|███▏      | 198/625 [01:11<02:02,  3.48it/s] 32%|███▏      | 199/625 [01:12<02:02,  3.48it/s] 32%|███▏      | 200/625 [01:12<02:02,  3.48it/s] 32%|███▏      | 201/625 [01:12<02:01,  3.48it/s] 32%|███▏      | 202/625 [01:13<02:01,  3.48it/s] 32%|███▏      | 203/625 [01:13<02:01,  3.48it/s] 33%|███▎      | 204/625 [01:13<02:01,  3.46it/s] 33%|███▎      | 205/625 [01:13<02:01,  3.47it/s] 33%|███▎      | 206/625 [01:14<02:00,  3.47it/s] 33%|███▎      | 207/625 [01:14<02:00,  3.47it/s] 33%|███▎      | 208/625 [01:14<02:00,  3.47it/s] 33%|███▎      | 209/625 [01:15<01:59,  3.47it/s] 34%|███▎      | 210/625 [01:15<01:59,  3.48it/s] 34%|███▍      | 211/625 [01:15<01:59,  3.48it/s] 34%|███▍      | 212/625 [01:15<01:58,  3.48it/s] 34%|███▍      | 213/625 [01:16<01:58,  3.48it/s] 34%|███▍      | 214/625 [01:16<01:58,  3.48it/s] 34%|███▍      | 215/625 [01:16<01:57,  3.48it/s] 35%|███▍      | 216/625 [01:17<01:57,  3.48it/s] 35%|███▍      | 217/625 [01:17<01:57,  3.48it/s] 35%|███▍      | 218/625 [01:17<01:56,  3.48it/s] 35%|███▌      | 219/625 [01:17<01:56,  3.48it/s] 35%|███▌      | 220/625 [01:18<01:56,  3.48it/s] 35%|███▌      | 221/625 [01:18<01:56,  3.48it/s] 36%|███▌      | 222/625 [01:18<01:55,  3.48it/s] 36%|███▌      | 223/625 [01:19<01:55,  3.48it/s] 36%|███▌      | 224/625 [01:19<01:55,  3.48it/s] 36%|███▌      | 225/625 [01:19<01:54,  3.48it/s] 36%|███▌      | 226/625 [01:19<01:55,  3.46it/s] 36%|███▋      | 227/625 [01:20<01:54,  3.46it/s] 36%|███▋      | 228/625 [01:20<01:54,  3.47it/s] 37%|███▋      | 229/625 [01:20<01:54,  3.47it/s] 37%|███▋      | 230/625 [01:21<01:53,  3.47it/s] 37%|███▋      | 231/625 [01:21<01:53,  3.48it/s] 37%|███▋      | 232/625 [01:21<01:53,  3.48it/s] 37%|███▋      | 233/625 [01:21<01:52,  3.48it/s] 37%|███▋      | 234/625 [01:22<01:52,  3.48it/s] 38%|███▊      | 235/625 [01:22<01:52,  3.47it/s] 38%|███▊      | 236/625 [01:22<01:51,  3.47it/s] 38%|███▊      | 237/625 [01:23<01:51,  3.47it/s] 38%|███▊      | 238/625 [01:23<01:51,  3.47it/s] 38%|███▊      | 239/625 [01:23<01:51,  3.47it/s] 38%|███▊      | 240/625 [01:23<01:50,  3.48it/s] 39%|███▊      | 241/625 [01:24<01:50,  3.48it/s] 39%|███▊      | 242/625 [01:24<01:50,  3.47it/s] 39%|███▉      | 243/625 [01:24<01:49,  3.48it/s] 39%|███▉      | 244/625 [01:25<01:49,  3.47it/s] 39%|███▉      | 245/625 [01:25<01:49,  3.47it/s] 39%|███▉      | 246/625 [01:25<01:49,  3.48it/s] 40%|███▉      | 247/625 [01:26<01:48,  3.47it/s] 40%|███▉      | 248/625 [01:26<01:49,  3.44it/s] 40%|███▉      | 249/625 [01:26<01:48,  3.45it/s] 40%|████      | 250/625 [01:26<01:48,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 06:07:11,251 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:07:11,251 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 06:07:11,251 >>   Batch size = 8
{'eval_loss': 1.0823748111724854, 'eval_runtime': 9.5672, 'eval_samples_per_second': 360.19, 'eval_steps_per_second': 45.05, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.40it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.27it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.66it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.90it/s][A
  6%|▋         | 27/431 [00:00<00:08, 46.15it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.38it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.07it/s][A
 10%|▉         | 42/431 [00:00<00:08, 44.82it/s][A
 11%|█         | 47/431 [00:01<00:08, 44.92it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.08it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.26it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.34it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.43it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.22it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 44.98it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.76it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.64it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.78it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 44.92it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.06it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.20it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.30it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.21it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.10it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.88it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.74it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.76it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 44.86it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.04it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.20it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.36it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.17it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.08it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.85it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.81it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.82it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 44.87it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.08it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.22it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.33it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.23it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 44.99it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.94it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.81it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.72it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 44.77it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 44.87it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.06it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.27it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.30it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.25it/s][A
 61%|██████    | 262/431 [00:05<00:03, 45.09it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.84it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.76it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 44.81it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 44.92it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.05it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.12it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.21it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.15it/s][A
 71%|███████   | 307/431 [00:06<00:02, 45.09it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.76it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.87it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 44.85it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 44.95it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.03it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.14it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.20it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.18it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 45.06it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.91it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.86it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.86it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 44.94it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.06it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.13it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.19it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.11it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.05it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 44.90it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.91it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 44.82it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 44.96it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.05it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.14it/s][A                                                 
                                                 [A 40%|████      | 250/625 [01:36<01:48,  3.46it/s]
100%|██████████| 431/431 [00:09<00:00, 45.14it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:07:20,838 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-250
[INFO|configuration_utils.py:351] 2023-08-29 06:07:20,861 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-250/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:07:22,605 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-250/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:07:22,617 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-250/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:07:22,627 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-250/special_tokens_map.json
 40%|████      | 251/625 [01:42<29:41,  4.76s/it] 40%|████      | 252/625 [01:42<21:16,  3.42s/it] 40%|████      | 253/625 [01:42<15:23,  2.48s/it] 41%|████      | 254/625 [01:42<11:17,  1.83s/it] 41%|████      | 255/625 [01:43<08:25,  1.36s/it] 41%|████      | 256/625 [01:43<06:24,  1.04s/it] 41%|████      | 257/625 [01:43<05:00,  1.22it/s] 41%|████▏     | 258/625 [01:44<04:01,  1.52it/s] 41%|████▏     | 259/625 [01:44<03:20,  1.82it/s] 42%|████▏     | 260/625 [01:44<02:52,  2.12it/s] 42%|████▏     | 261/625 [01:44<02:31,  2.40it/s] 42%|████▏     | 262/625 [01:45<02:17,  2.64it/s] 42%|████▏     | 263/625 [01:45<02:08,  2.82it/s] 42%|████▏     | 264/625 [01:45<02:00,  2.98it/s] 42%|████▏     | 265/625 [01:46<01:55,  3.12it/s] 43%|████▎     | 266/625 [01:46<01:51,  3.22it/s] 43%|████▎     | 267/625 [01:46<01:48,  3.30it/s] 43%|████▎     | 268/625 [01:47<01:46,  3.36it/s] 43%|████▎     | 269/625 [01:47<01:44,  3.39it/s] 43%|████▎     | 270/625 [01:47<01:43,  3.43it/s] 43%|████▎     | 271/625 [01:47<01:42,  3.44it/s] 44%|████▎     | 272/625 [01:48<01:42,  3.46it/s] 44%|████▎     | 273/625 [01:48<01:41,  3.47it/s] 44%|████▍     | 274/625 [01:48<01:41,  3.45it/s] 44%|████▍     | 275/625 [01:49<01:40,  3.47it/s] 44%|████▍     | 276/625 [01:49<01:40,  3.48it/s] 44%|████▍     | 277/625 [01:49<01:39,  3.48it/s] 44%|████▍     | 278/625 [01:49<01:39,  3.48it/s] 45%|████▍     | 279/625 [01:50<01:39,  3.49it/s] 45%|████▍     | 280/625 [01:50<01:38,  3.49it/s] 45%|████▍     | 281/625 [01:50<01:38,  3.49it/s] 45%|████▌     | 282/625 [01:51<01:38,  3.49it/s] 45%|████▌     | 283/625 [01:51<01:38,  3.49it/s] 45%|████▌     | 284/625 [01:51<01:37,  3.49it/s] 46%|████▌     | 285/625 [01:51<01:39,  3.41it/s] 46%|████▌     | 286/625 [01:52<01:38,  3.43it/s] 46%|████▌     | 287/625 [01:52<01:38,  3.45it/s] 46%|████▌     | 288/625 [01:52<01:37,  3.46it/s] 46%|████▌     | 289/625 [01:53<01:36,  3.47it/s] 46%|████▋     | 290/625 [01:53<01:36,  3.47it/s] 47%|████▋     | 291/625 [01:53<01:35,  3.48it/s] 47%|████▋     | 292/625 [01:53<01:35,  3.48it/s] 47%|████▋     | 293/625 [01:54<01:35,  3.49it/s] 47%|████▋     | 294/625 [01:54<01:34,  3.49it/s] 47%|████▋     | 295/625 [01:54<01:34,  3.49it/s] 47%|████▋     | 296/625 [01:55<01:34,  3.48it/s] 48%|████▊     | 297/625 [01:55<01:34,  3.49it/s] 48%|████▊     | 298/625 [01:55<01:33,  3.48it/s] 48%|████▊     | 299/625 [01:55<01:33,  3.49it/s] 48%|████▊     | 300/625 [01:56<01:33,  3.49it/s] 48%|████▊     | 301/625 [01:56<01:32,  3.49it/s] 48%|████▊     | 302/625 [01:56<01:32,  3.49it/s] 48%|████▊     | 303/625 [01:57<01:32,  3.49it/s] 49%|████▊     | 304/625 [01:57<01:31,  3.49it/s] 49%|████▉     | 305/625 [01:57<01:31,  3.49it/s] 49%|████▉     | 306/625 [01:57<01:31,  3.49it/s] 49%|████▉     | 307/625 [01:58<01:31,  3.47it/s] 49%|████▉     | 308/625 [01:58<01:31,  3.48it/s] 49%|████▉     | 309/625 [01:58<01:30,  3.48it/s] 50%|████▉     | 310/625 [01:59<01:30,  3.48it/s] 50%|████▉     | 311/625 [01:59<01:30,  3.49it/s] 50%|████▉     | 312/625 [01:59<01:29,  3.49it/s] 50%|█████     | 313/625 [01:59<01:29,  3.49it/s] 50%|█████     | 314/625 [02:00<01:29,  3.49it/s] 50%|█████     | 315/625 [02:00<01:28,  3.49it/s] 51%|█████     | 316/625 [02:00<01:28,  3.49it/s] 51%|█████     | 317/625 [02:01<01:28,  3.49it/s] 51%|█████     | 318/625 [02:01<01:28,  3.48it/s] 51%|█████     | 319/625 [02:01<01:27,  3.48it/s] 51%|█████     | 320/625 [02:01<01:27,  3.49it/s] 51%|█████▏    | 321/625 [02:02<01:27,  3.49it/s] 52%|█████▏    | 322/625 [02:02<01:26,  3.49it/s] 52%|█████▏    | 323/625 [02:02<01:26,  3.49it/s] 52%|█████▏    | 324/625 [02:03<01:26,  3.49it/s] 52%|█████▏    | 325/625 [02:03<01:25,  3.49it/s] 52%|█████▏    | 326/625 [02:03<01:25,  3.49it/s] 52%|█████▏    | 327/625 [02:03<01:25,  3.49it/s] 52%|█████▏    | 328/625 [02:04<01:25,  3.49it/s] 53%|█████▎    | 329/625 [02:04<01:24,  3.48it/s] 53%|█████▎    | 330/625 [02:04<01:24,  3.48it/s] 53%|█████▎    | 331/625 [02:05<01:24,  3.49it/s] 53%|█████▎    | 332/625 [02:05<01:24,  3.49it/s] 53%|█████▎    | 333/625 [02:05<01:23,  3.49it/s] 53%|█████▎    | 334/625 [02:05<01:23,  3.49it/s] 54%|█████▎    | 335/625 [02:06<01:23,  3.49it/s] 54%|█████▍    | 336/625 [02:06<01:22,  3.49it/s] 54%|█████▍    | 337/625 [02:06<01:22,  3.49it/s] 54%|█████▍    | 338/625 [02:07<01:22,  3.49it/s] 54%|█████▍    | 339/625 [02:07<01:22,  3.48it/s] 54%|█████▍    | 340/625 [02:07<01:22,  3.47it/s] 55%|█████▍    | 341/625 [02:07<01:21,  3.48it/s] 55%|█████▍    | 342/625 [02:08<01:21,  3.48it/s] 55%|█████▍    | 343/625 [02:08<01:20,  3.48it/s] 55%|█████▌    | 344/625 [02:08<01:20,  3.48it/s] 55%|█████▌    | 345/625 [02:09<01:20,  3.49it/s] 55%|█████▌    | 346/625 [02:09<01:20,  3.49it/s] 56%|█████▌    | 347/625 [02:09<01:19,  3.49it/s] 56%|█████▌    | 348/625 [02:09<01:19,  3.49it/s] 56%|█████▌    | 349/625 [02:10<01:19,  3.49it/s] 56%|█████▌    | 350/625 [02:10<01:18,  3.49it/s] 56%|█████▌    | 351/625 [02:10<01:18,  3.49it/s] 56%|█████▋    | 352/625 [02:11<01:18,  3.48it/s] 56%|█████▋    | 353/625 [02:11<01:18,  3.48it/s] 57%|█████▋    | 354/625 [02:11<01:17,  3.48it/s] 57%|█████▋    | 355/625 [02:11<01:17,  3.49it/s] 57%|█████▋    | 356/625 [02:12<01:17,  3.49it/s] 57%|█████▋    | 357/625 [02:12<01:16,  3.49it/s] 57%|█████▋    | 358/625 [02:12<01:16,  3.49it/s] 57%|█████▋    | 359/625 [02:13<01:16,  3.49it/s] 58%|█████▊    | 360/625 [02:13<01:15,  3.49it/s] 58%|█████▊    | 361/625 [02:13<01:15,  3.49it/s] 58%|█████▊    | 362/625 [02:13<01:15,  3.49it/s] 58%|█████▊    | 363/625 [02:14<01:15,  3.48it/s] 58%|█████▊    | 364/625 [02:14<01:14,  3.48it/s] 58%|█████▊    | 365/625 [02:14<01:14,  3.48it/s] 59%|█████▊    | 366/625 [02:15<01:14,  3.49it/s] 59%|█████▊    | 367/625 [02:15<01:13,  3.49it/s] 59%|█████▉    | 368/625 [02:15<01:13,  3.49it/s] 59%|█████▉    | 369/625 [02:15<01:13,  3.49it/s] 59%|█████▉    | 370/625 [02:16<01:13,  3.49it/s] 59%|█████▉    | 371/625 [02:16<01:12,  3.49it/s] 60%|█████▉    | 372/625 [02:16<01:12,  3.49it/s] 60%|█████▉    | 373/625 [02:17<01:12,  3.49it/s] 60%|█████▉    | 374/625 [02:17<01:12,  3.49it/s] 60%|██████    | 375/625 [02:17<01:11,  3.49it/s][INFO|trainer.py:2140] 2023-08-29 06:08:02,091 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:08:02,091 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 06:08:02,091 >>   Batch size = 8
{'eval_loss': 1.1036566495895386, 'eval_runtime': 9.568, 'eval_samples_per_second': 360.159, 'eval_steps_per_second': 45.046, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.06it/s][A
  3%|▎         | 12/431 [00:00<00:08, 48.93it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.06it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.20it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.76it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.46it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.35it/s][A
 10%|▉         | 42/431 [00:00<00:08, 45.15it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.15it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.39it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.30it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.19it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.07it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.15it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 45.07it/s][A
 20%|██        | 87/431 [00:01<00:07, 45.10it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 45.04it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 45.07it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.27it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.27it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.28it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.12it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.11it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 45.06it/s][A
 31%|███       | 132/431 [00:02<00:06, 45.11it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 45.03it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 45.11it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.16it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.24it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.22it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.08it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.11it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 45.00it/s][A
 41%|████      | 177/431 [00:03<00:05, 45.07it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 45.14it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 45.11it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.13it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.23it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.11it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.10it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.12it/s][A
 50%|█████     | 217/431 [00:04<00:04, 45.10it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 45.15it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 45.05it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 45.06it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.09it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.17it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.18it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.20it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.11it/s][A
 61%|██████    | 262/431 [00:05<00:03, 45.17it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 45.12it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.98it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.07it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.09it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.22it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.20it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.21it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.12it/s][A
 71%|███████   | 307/431 [00:06<00:02, 45.13it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 45.09it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 45.03it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.15it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.12it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.17it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.19it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.18it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.11it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 45.06it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 45.05it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.98it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 45.18it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.14it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.14it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.20it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.20it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.14it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.08it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 45.08it/s][A
 94%|█████████▍| 407/431 [00:08<00:00, 45.12it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.17it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.16it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.18it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.17it/s][A                                                 
                                                 [A 60%|██████    | 375/625 [02:27<01:11,  3.49it/s]
100%|██████████| 431/431 [00:09<00:00, 45.17it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:08:11,663 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-375
[INFO|configuration_utils.py:351] 2023-08-29 06:08:11,682 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-375/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:08:13,452 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-375/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:08:13,465 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-375/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:08:13,478 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-375/special_tokens_map.json
 60%|██████    | 376/625 [02:33<19:55,  4.80s/it] 60%|██████    | 377/625 [02:33<14:15,  3.45s/it] 60%|██████    | 378/625 [02:33<10:17,  2.50s/it] 61%|██████    | 379/625 [02:33<07:32,  1.84s/it] 61%|██████    | 380/625 [02:34<05:36,  1.37s/it] 61%|██████    | 381/625 [02:34<04:15,  1.05s/it] 61%|██████    | 382/625 [02:34<03:19,  1.22it/s] 61%|██████▏   | 383/625 [02:35<02:39,  1.52it/s] 61%|██████▏   | 384/625 [02:35<02:11,  1.83it/s] 62%|██████▏   | 385/625 [02:35<01:52,  2.13it/s] 62%|██████▏   | 386/625 [02:35<01:38,  2.41it/s] 62%|██████▏   | 387/625 [02:36<01:29,  2.66it/s] 62%|██████▏   | 388/625 [02:36<01:22,  2.86it/s] 62%|██████▏   | 389/625 [02:36<01:18,  3.02it/s] 62%|██████▏   | 390/625 [02:37<01:14,  3.15it/s] 63%|██████▎   | 391/625 [02:37<01:12,  3.24it/s] 63%|██████▎   | 392/625 [02:37<01:10,  3.31it/s] 63%|██████▎   | 393/625 [02:37<01:09,  3.35it/s] 63%|██████▎   | 394/625 [02:38<01:08,  3.39it/s] 63%|██████▎   | 395/625 [02:38<01:07,  3.42it/s] 63%|██████▎   | 396/625 [02:38<01:06,  3.44it/s] 64%|██████▎   | 397/625 [02:39<01:06,  3.45it/s] 64%|██████▎   | 398/625 [02:39<01:05,  3.46it/s] 64%|██████▍   | 399/625 [02:39<01:05,  3.46it/s] 64%|██████▍   | 400/625 [02:39<01:04,  3.47it/s] 64%|██████▍   | 401/625 [02:40<01:04,  3.48it/s] 64%|██████▍   | 402/625 [02:40<01:04,  3.48it/s] 64%|██████▍   | 403/625 [02:40<01:03,  3.48it/s] 65%|██████▍   | 404/625 [02:41<01:03,  3.49it/s] 65%|██████▍   | 405/625 [02:41<01:03,  3.49it/s] 65%|██████▍   | 406/625 [02:41<01:02,  3.48it/s] 65%|██████▌   | 407/625 [02:41<01:02,  3.48it/s] 65%|██████▌   | 408/625 [02:42<01:02,  3.49it/s] 65%|██████▌   | 409/625 [02:42<01:01,  3.49it/s] 66%|██████▌   | 410/625 [02:42<01:01,  3.50it/s] 66%|██████▌   | 411/625 [02:43<01:01,  3.50it/s] 66%|██████▌   | 412/625 [02:43<01:00,  3.50it/s] 66%|██████▌   | 413/625 [02:43<01:00,  3.50it/s] 66%|██████▌   | 414/625 [02:43<01:00,  3.50it/s] 66%|██████▋   | 415/625 [02:44<01:00,  3.50it/s] 67%|██████▋   | 416/625 [02:44<00:59,  3.50it/s] 67%|██████▋   | 417/625 [02:44<00:59,  3.48it/s] 67%|██████▋   | 418/625 [02:45<00:59,  3.48it/s] 67%|██████▋   | 419/625 [02:45<00:59,  3.49it/s] 67%|██████▋   | 420/625 [02:45<00:58,  3.49it/s] 67%|██████▋   | 421/625 [02:45<00:58,  3.49it/s] 68%|██████▊   | 422/625 [02:46<00:58,  3.49it/s] 68%|██████▊   | 423/625 [02:46<00:58,  3.48it/s] 68%|██████▊   | 424/625 [02:46<00:57,  3.47it/s] 68%|██████▊   | 425/625 [02:47<00:57,  3.48it/s] 68%|██████▊   | 426/625 [02:47<00:57,  3.48it/s] 68%|██████▊   | 427/625 [02:47<00:56,  3.49it/s] 68%|██████▊   | 428/625 [02:47<00:56,  3.48it/s] 69%|██████▊   | 429/625 [02:48<00:56,  3.49it/s] 69%|██████▉   | 430/625 [02:48<00:55,  3.49it/s] 69%|██████▉   | 431/625 [02:48<00:55,  3.49it/s] 69%|██████▉   | 432/625 [02:49<00:55,  3.50it/s] 69%|██████▉   | 433/625 [02:49<00:54,  3.50it/s] 69%|██████▉   | 434/625 [02:49<00:54,  3.50it/s] 70%|██████▉   | 435/625 [02:49<00:54,  3.50it/s] 70%|██████▉   | 436/625 [02:50<00:54,  3.50it/s] 70%|██████▉   | 437/625 [02:50<00:53,  3.49it/s] 70%|███████   | 438/625 [02:50<00:53,  3.49it/s] 70%|███████   | 439/625 [02:51<00:53,  3.47it/s] 70%|███████   | 440/625 [02:51<00:53,  3.47it/s] 71%|███████   | 441/625 [02:51<00:53,  3.47it/s] 71%|███████   | 442/625 [02:52<00:53,  3.40it/s] 71%|███████   | 443/625 [02:52<00:53,  3.41it/s] 71%|███████   | 444/625 [02:52<00:52,  3.42it/s] 71%|███████   | 445/625 [02:52<00:52,  3.43it/s] 71%|███████▏  | 446/625 [02:53<00:52,  3.44it/s] 72%|███████▏  | 447/625 [02:53<00:51,  3.45it/s] 72%|███████▏  | 448/625 [02:53<00:51,  3.46it/s] 72%|███████▏  | 449/625 [02:54<00:50,  3.46it/s] 72%|███████▏  | 450/625 [02:54<00:50,  3.44it/s] 72%|███████▏  | 451/625 [02:54<00:50,  3.43it/s] 72%|███████▏  | 452/625 [02:54<00:50,  3.43it/s] 72%|███████▏  | 453/625 [02:55<00:50,  3.42it/s] 73%|███████▎  | 454/625 [02:55<00:49,  3.42it/s] 73%|███████▎  | 455/625 [02:55<00:49,  3.42it/s] 73%|███████▎  | 456/625 [02:56<00:49,  3.42it/s] 73%|███████▎  | 457/625 [02:56<00:49,  3.42it/s] 73%|███████▎  | 458/625 [02:56<00:48,  3.42it/s] 73%|███████▎  | 459/625 [02:56<00:48,  3.42it/s] 74%|███████▎  | 460/625 [02:57<00:48,  3.42it/s] 74%|███████▍  | 461/625 [02:57<00:48,  3.40it/s] 74%|███████▍  | 462/625 [02:57<00:47,  3.40it/s] 74%|███████▍  | 463/625 [02:58<00:47,  3.41it/s] 74%|███████▍  | 464/625 [02:58<00:47,  3.41it/s] 74%|███████▍  | 465/625 [02:58<00:46,  3.42it/s] 75%|███████▍  | 466/625 [02:59<00:46,  3.42it/s] 75%|███████▍  | 467/625 [02:59<00:46,  3.42it/s] 75%|███████▍  | 468/625 [02:59<00:45,  3.42it/s] 75%|███████▌  | 469/625 [02:59<00:45,  3.42it/s] 75%|███████▌  | 470/625 [03:00<00:45,  3.41it/s] 75%|███████▌  | 471/625 [03:00<00:45,  3.42it/s] 76%|███████▌  | 472/625 [03:00<00:44,  3.41it/s] 76%|███████▌  | 473/625 [03:01<00:44,  3.41it/s] 76%|███████▌  | 474/625 [03:01<00:44,  3.41it/s] 76%|███████▌  | 475/625 [03:01<00:43,  3.42it/s] 76%|███████▌  | 476/625 [03:01<00:43,  3.42it/s] 76%|███████▋  | 477/625 [03:02<00:43,  3.42it/s] 76%|███████▋  | 478/625 [03:02<00:43,  3.42it/s] 77%|███████▋  | 479/625 [03:02<00:42,  3.42it/s] 77%|███████▋  | 480/625 [03:03<00:42,  3.42it/s] 77%|███████▋  | 481/625 [03:03<00:42,  3.42it/s] 77%|███████▋  | 482/625 [03:03<00:41,  3.42it/s] 77%|███████▋  | 483/625 [03:03<00:41,  3.41it/s] 77%|███████▋  | 484/625 [03:04<00:41,  3.41it/s] 78%|███████▊  | 485/625 [03:04<00:40,  3.41it/s] 78%|███████▊  | 486/625 [03:04<00:40,  3.42it/s] 78%|███████▊  | 487/625 [03:05<00:40,  3.42it/s] 78%|███████▊  | 488/625 [03:05<00:40,  3.42it/s] 78%|███████▊  | 489/625 [03:05<00:39,  3.42it/s] 78%|███████▊  | 490/625 [03:06<00:39,  3.43it/s] 79%|███████▊  | 491/625 [03:06<00:38,  3.44it/s] 79%|███████▊  | 492/625 [03:06<00:38,  3.45it/s] 79%|███████▉  | 493/625 [03:06<00:38,  3.46it/s] 79%|███████▉  | 494/625 [03:07<00:37,  3.45it/s] 79%|███████▉  | 495/625 [03:07<00:37,  3.45it/s] 79%|███████▉  | 496/625 [03:07<00:37,  3.46it/s] 80%|███████▉  | 497/625 [03:08<00:36,  3.46it/s] 80%|███████▉  | 498/625 [03:08<00:36,  3.46it/s] 80%|███████▉  | 499/625 [03:08<00:36,  3.47it/s] 80%|████████  | 500/625 [03:08<00:36,  3.47it/s]                                                  80%|████████  | 500/625 [03:08<00:36,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 06:08:53,306 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:08:53,306 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 06:08:53,306 >>   Batch size = 8
{'eval_loss': 1.1240782737731934, 'eval_runtime': 9.5484, 'eval_samples_per_second': 360.899, 'eval_steps_per_second': 45.139, 'epoch': 3.0}
{'loss': 0.3787, 'learning_rate': 7.5e-06, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.71it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.09it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.50it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.76it/s][A
  6%|▋         | 27/431 [00:00<00:08, 46.10it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.64it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.23it/s][A
 10%|▉         | 42/431 [00:00<00:08, 44.87it/s][A
 11%|█         | 47/431 [00:01<00:08, 45.01it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 44.99it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.22it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.22it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.35it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.28it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.05it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.86it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.72it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.69it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 44.93it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.02it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.21it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.35it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.18it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.14it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.74it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.74it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.69it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 44.84it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 44.95it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.18it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.27it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.31it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.14it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.78it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.74it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.66it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 44.82it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 44.93it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.08it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.20it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.27it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.19it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.92it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.71it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.75it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 44.86it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 44.95it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.12it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.24it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.27it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.09it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.98it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.83it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.79it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 44.88it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 44.88it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.05it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.20it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.25it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.17it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.87it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.82it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.90it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 44.82it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 44.85it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.00it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.18it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.24it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.18it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 44.98it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.86it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.79it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.81it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 44.94it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 44.96it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.22it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.14it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.21it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 44.96it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 44.89it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.87it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 44.76it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 44.90it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.04it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.25it/s][A                                                 
                                                 [A 80%|████████  | 500/625 [03:18<00:36,  3.47it/s]
100%|██████████| 431/431 [00:09<00:00, 45.25it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:09:02,893 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-500
[INFO|configuration_utils.py:351] 2023-08-29 06:09:02,923 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-500/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:09:04,577 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:09:04,589 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:09:04,600 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-500/special_tokens_map.json
 80%|████████  | 501/625 [03:24<09:47,  4.74s/it] 80%|████████  | 502/625 [03:24<06:59,  3.41s/it] 80%|████████  | 503/625 [03:24<05:01,  2.47s/it] 81%|████████  | 504/625 [03:24<03:39,  1.82s/it] 81%|████████  | 505/625 [03:25<02:43,  1.36s/it] 81%|████████  | 506/625 [03:25<02:03,  1.04s/it] 81%|████████  | 507/625 [03:25<01:36,  1.23it/s] 81%|████████▏ | 508/625 [03:26<01:16,  1.52it/s] 81%|████████▏ | 509/625 [03:26<01:03,  1.82it/s] 82%|████████▏ | 510/625 [03:26<00:54,  2.12it/s] 82%|████████▏ | 511/625 [03:26<00:47,  2.40it/s] 82%|████████▏ | 512/625 [03:27<00:42,  2.64it/s] 82%|████████▏ | 513/625 [03:27<00:39,  2.82it/s] 82%|████████▏ | 514/625 [03:27<00:37,  2.98it/s] 82%|████████▏ | 515/625 [03:28<00:35,  3.10it/s] 83%|████████▎ | 516/625 [03:28<00:34,  3.19it/s] 83%|████████▎ | 517/625 [03:28<00:33,  3.26it/s] 83%|████████▎ | 518/625 [03:29<00:32,  3.31it/s] 83%|████████▎ | 519/625 [03:29<00:31,  3.35it/s] 83%|████████▎ | 520/625 [03:29<00:31,  3.37it/s] 83%|████████▎ | 521/625 [03:29<00:30,  3.39it/s] 84%|████████▎ | 522/625 [03:30<00:30,  3.40it/s] 84%|████████▎ | 523/625 [03:30<00:29,  3.41it/s] 84%|████████▍ | 524/625 [03:30<00:29,  3.41it/s] 84%|████████▍ | 525/625 [03:31<00:29,  3.42it/s] 84%|████████▍ | 526/625 [03:31<00:28,  3.43it/s] 84%|████████▍ | 527/625 [03:31<00:28,  3.42it/s] 84%|████████▍ | 528/625 [03:31<00:28,  3.43it/s] 85%|████████▍ | 529/625 [03:32<00:27,  3.43it/s] 85%|████████▍ | 530/625 [03:32<00:27,  3.44it/s] 85%|████████▍ | 531/625 [03:32<00:27,  3.43it/s] 85%|████████▌ | 532/625 [03:33<00:27,  3.43it/s] 85%|████████▌ | 533/625 [03:33<00:26,  3.43it/s] 85%|████████▌ | 534/625 [03:33<00:26,  3.43it/s] 86%|████████▌ | 535/625 [03:33<00:26,  3.42it/s] 86%|████████▌ | 536/625 [03:34<00:25,  3.43it/s] 86%|████████▌ | 537/625 [03:34<00:25,  3.43it/s] 86%|████████▌ | 538/625 [03:34<00:25,  3.43it/s] 86%|████████▌ | 539/625 [03:35<00:25,  3.43it/s] 86%|████████▋ | 540/625 [03:35<00:24,  3.43it/s] 87%|████████▋ | 541/625 [03:35<00:24,  3.43it/s] 87%|████████▋ | 542/625 [03:36<00:24,  3.43it/s] 87%|████████▋ | 543/625 [03:36<00:23,  3.43it/s] 87%|████████▋ | 544/625 [03:36<00:23,  3.43it/s] 87%|████████▋ | 545/625 [03:36<00:23,  3.43it/s] 87%|████████▋ | 546/625 [03:37<00:23,  3.42it/s] 88%|████████▊ | 547/625 [03:37<00:22,  3.42it/s] 88%|████████▊ | 548/625 [03:37<00:22,  3.43it/s] 88%|████████▊ | 549/625 [03:38<00:22,  3.43it/s] 88%|████████▊ | 550/625 [03:38<00:21,  3.43it/s] 88%|████████▊ | 551/625 [03:38<00:21,  3.43it/s] 88%|████████▊ | 552/625 [03:38<00:21,  3.43it/s] 88%|████████▊ | 553/625 [03:39<00:20,  3.43it/s] 89%|████████▊ | 554/625 [03:39<00:20,  3.43it/s] 89%|████████▉ | 555/625 [03:39<00:20,  3.43it/s] 89%|████████▉ | 556/625 [03:40<00:20,  3.43it/s] 89%|████████▉ | 557/625 [03:40<00:19,  3.44it/s] 89%|████████▉ | 558/625 [03:40<00:19,  3.45it/s] 89%|████████▉ | 559/625 [03:40<00:19,  3.46it/s] 90%|████████▉ | 560/625 [03:41<00:18,  3.47it/s] 90%|████████▉ | 561/625 [03:41<00:18,  3.47it/s] 90%|████████▉ | 562/625 [03:41<00:18,  3.48it/s] 90%|█████████ | 563/625 [03:42<00:17,  3.48it/s] 90%|█████████ | 564/625 [03:42<00:17,  3.48it/s] 90%|█████████ | 565/625 [03:42<00:17,  3.48it/s] 91%|█████████ | 566/625 [03:42<00:16,  3.48it/s] 91%|█████████ | 567/625 [03:43<00:16,  3.48it/s] 91%|█████████ | 568/625 [03:43<00:16,  3.48it/s] 91%|█████████ | 569/625 [03:43<00:16,  3.48it/s] 91%|█████████ | 570/625 [03:44<00:15,  3.48it/s] 91%|█████████▏| 571/625 [03:44<00:15,  3.48it/s] 92%|█████████▏| 572/625 [03:44<00:15,  3.48it/s] 92%|█████████▏| 573/625 [03:44<00:14,  3.48it/s] 92%|█████████▏| 574/625 [03:45<00:14,  3.48it/s] 92%|█████████▏| 575/625 [03:45<00:14,  3.48it/s] 92%|█████████▏| 576/625 [03:45<00:14,  3.48it/s] 92%|█████████▏| 577/625 [03:46<00:13,  3.48it/s] 92%|█████████▏| 578/625 [03:46<00:13,  3.47it/s] 93%|█████████▎| 579/625 [03:46<00:13,  3.48it/s] 93%|█████████▎| 580/625 [03:46<00:12,  3.48it/s] 93%|█████████▎| 581/625 [03:47<00:12,  3.48it/s] 93%|█████████▎| 582/625 [03:47<00:12,  3.48it/s] 93%|█████████▎| 583/625 [03:47<00:12,  3.48it/s] 93%|█████████▎| 584/625 [03:48<00:11,  3.48it/s] 94%|█████████▎| 585/625 [03:48<00:11,  3.48it/s] 94%|█████████▍| 586/625 [03:48<00:11,  3.48it/s] 94%|█████████▍| 587/625 [03:49<00:10,  3.48it/s] 94%|█████████▍| 588/625 [03:49<00:10,  3.49it/s] 94%|█████████▍| 589/625 [03:49<00:10,  3.42it/s] 94%|█████████▍| 590/625 [03:49<00:10,  3.44it/s] 95%|█████████▍| 591/625 [03:50<00:09,  3.45it/s] 95%|█████████▍| 592/625 [03:50<00:09,  3.47it/s] 95%|█████████▍| 593/625 [03:50<00:09,  3.47it/s] 95%|█████████▌| 594/625 [03:51<00:08,  3.47it/s] 95%|█████████▌| 595/625 [03:51<00:08,  3.47it/s] 95%|█████████▌| 596/625 [03:51<00:08,  3.48it/s] 96%|█████████▌| 597/625 [03:51<00:08,  3.48it/s] 96%|█████████▌| 598/625 [03:52<00:07,  3.38it/s] 96%|█████████▌| 599/625 [03:52<00:07,  3.40it/s] 96%|█████████▌| 600/625 [03:52<00:07,  3.42it/s] 96%|█████████▌| 601/625 [03:53<00:06,  3.43it/s] 96%|█████████▋| 602/625 [03:53<00:06,  3.45it/s] 96%|█████████▋| 603/625 [03:53<00:06,  3.45it/s] 97%|█████████▋| 604/625 [03:53<00:06,  3.46it/s] 97%|█████████▋| 605/625 [03:54<00:05,  3.46it/s] 97%|█████████▋| 606/625 [03:54<00:05,  3.46it/s] 97%|█████████▋| 607/625 [03:54<00:05,  3.47it/s] 97%|█████████▋| 608/625 [03:55<00:04,  3.47it/s] 97%|█████████▋| 609/625 [03:55<00:04,  3.47it/s] 98%|█████████▊| 610/625 [03:55<00:04,  3.47it/s] 98%|█████████▊| 611/625 [03:55<00:04,  3.47it/s] 98%|█████████▊| 612/625 [03:56<00:03,  3.47it/s] 98%|█████████▊| 613/625 [03:56<00:03,  3.47it/s] 98%|█████████▊| 614/625 [03:56<00:03,  3.47it/s] 98%|█████████▊| 615/625 [03:57<00:02,  3.48it/s] 99%|█████████▊| 616/625 [03:57<00:02,  3.47it/s] 99%|█████████▊| 617/625 [03:57<00:02,  3.48it/s] 99%|█████████▉| 618/625 [03:57<00:02,  3.47it/s] 99%|█████████▉| 619/625 [03:58<00:01,  3.47it/s] 99%|█████████▉| 620/625 [03:58<00:01,  3.47it/s] 99%|█████████▉| 621/625 [03:58<00:01,  3.47it/s]100%|█████████▉| 622/625 [03:59<00:00,  3.46it/s]100%|█████████▉| 623/625 [03:59<00:00,  3.46it/s]100%|█████████▉| 624/625 [03:59<00:00,  3.47it/s]100%|██████████| 625/625 [03:59<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 06:09:44,369 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:09:44,369 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 06:09:44,369 >>   Batch size = 8
{'eval_loss': 1.129500389099121, 'eval_runtime': 9.5708, 'eval_samples_per_second': 360.053, 'eval_steps_per_second': 45.033, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.80it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.21it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.46it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.61it/s][A
  6%|▋         | 27/431 [00:00<00:08, 46.27it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.65it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.03it/s][A
 10%|▉         | 42/431 [00:00<00:08, 44.95it/s][A
 11%|█         | 47/431 [00:01<00:08, 44.99it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.09it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.20it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.12it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.29it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.31it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.06it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.92it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.77it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.68it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 44.87it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.06it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.13it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.24it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.22it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.06it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.90it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.84it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.72it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 44.91it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 44.97it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.09it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.22it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.18it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.15it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.92it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.81it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.74it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 44.98it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 44.91it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.01it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.07it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.23it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.22it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.97it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.93it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.71it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 44.98it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.02it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.07it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.09it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.25it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.13it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.96it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.92it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.83it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.00it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 44.92it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.05it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.22it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.20it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.15it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.94it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.85it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.80it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 44.93it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 44.70it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 44.99it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.03it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.27it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.24it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 44.90it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.98it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.88it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.82it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 44.83it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 44.92it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.03it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.10it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.23it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.08it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 45.05it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.73it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 44.86it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 44.90it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 44.88it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 44.96it/s][A                                                 
                                                 [A100%|██████████| 625/625 [04:09<00:00,  3.46it/s]
100%|██████████| 431/431 [00:09<00:00, 44.96it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 06:09:53,962 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-625
[INFO|configuration_utils.py:351] 2023-08-29 06:09:53,986 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-625/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:09:55,669 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-625/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:09:55,682 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-625/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:09:55,695 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-625/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 06:09:59,040 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 06:09:59,043 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-125 (score: 1.0823748111724854).
                                                 100%|██████████| 625/625 [04:16<00:00,  3.46it/s]100%|██████████| 625/625 [04:16<00:00,  2.44it/s]
[INFO|trainer.py:1894] 2023-08-29 06:10:00,886 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-29 06:10:00,911 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 06:10:02,612 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 06:10:02,640 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 06:10:02,654 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:10:02,854 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:02,855 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:02,855 >>   train_loss               =     0.3749
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:02,855 >>   train_runtime            = 0:04:16.50
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:02,855 >>   train_samples            =       8000
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:02,855 >>   train_samples_per_second =     155.94
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:02,855 >>   train_steps_per_second   =      2.437
{'eval_loss': 1.137716293334961, 'eval_runtime': 9.5716, 'eval_samples_per_second': 360.022, 'eval_steps_per_second': 45.029, 'epoch': 5.0}
{'train_runtime': 256.5086, 'train_samples_per_second': 155.94, 'train_steps_per_second': 2.437, 'train_loss': 0.3749237060546875, 'epoch': 5.0}
08/29/2023 06:10:02 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 06:10:02,889 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 06:10:02,889 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 06:10:02,890 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 55.91it/s]  3%|▎         | 12/431 [00:00<00:08, 49.05it/s]  4%|▍         | 17/431 [00:00<00:08, 47.62it/s]  5%|▌         | 22/431 [00:00<00:08, 47.13it/s]  6%|▋         | 27/431 [00:00<00:08, 46.71it/s]  7%|▋         | 32/431 [00:00<00:08, 46.41it/s]  9%|▊         | 37/431 [00:00<00:08, 46.14it/s] 10%|▉         | 42/431 [00:00<00:08, 45.65it/s] 11%|█         | 47/431 [00:01<00:08, 45.22it/s] 12%|█▏        | 52/431 [00:01<00:08, 44.78it/s] 13%|█▎        | 57/431 [00:01<00:08, 44.80it/s] 14%|█▍        | 62/431 [00:01<00:08, 44.97it/s] 16%|█▌        | 67/431 [00:01<00:08, 45.15it/s] 17%|█▋        | 72/431 [00:01<00:07, 45.24it/s] 18%|█▊        | 77/431 [00:01<00:07, 45.39it/s] 19%|█▉        | 82/431 [00:01<00:07, 45.45it/s] 20%|██        | 87/431 [00:01<00:07, 45.46it/s] 21%|██▏       | 92/431 [00:02<00:07, 45.22it/s] 23%|██▎       | 97/431 [00:02<00:07, 44.96it/s] 24%|██▎       | 102/431 [00:02<00:07, 44.62it/s] 25%|██▍       | 107/431 [00:02<00:07, 44.87it/s] 26%|██▌       | 112/431 [00:02<00:07, 45.04it/s] 27%|██▋       | 117/431 [00:02<00:06, 45.23it/s] 28%|██▊       | 122/431 [00:02<00:06, 45.42it/s] 29%|██▉       | 127/431 [00:02<00:06, 45.38it/s] 31%|███       | 132/431 [00:02<00:06, 45.41it/s] 32%|███▏      | 137/431 [00:03<00:06, 45.14it/s] 33%|███▎      | 142/431 [00:03<00:06, 44.98it/s] 34%|███▍      | 147/431 [00:03<00:06, 44.87it/s] 35%|███▌      | 152/431 [00:03<00:06, 44.96it/s] 36%|███▋      | 157/431 [00:03<00:06, 44.86it/s] 38%|███▊      | 162/431 [00:03<00:05, 45.20it/s] 39%|███▊      | 167/431 [00:03<00:05, 45.27it/s] 40%|███▉      | 172/431 [00:03<00:05, 45.38it/s] 41%|████      | 177/431 [00:03<00:05, 45.39it/s] 42%|████▏     | 182/431 [00:04<00:05, 45.09it/s] 43%|████▎     | 187/431 [00:04<00:05, 44.81it/s] 45%|████▍     | 192/431 [00:04<00:05, 44.88it/s] 46%|████▌     | 197/431 [00:04<00:05, 44.97it/s] 47%|████▋     | 202/431 [00:04<00:05, 45.01it/s] 48%|████▊     | 207/431 [00:04<00:04, 45.05it/s] 49%|████▉     | 212/431 [00:04<00:04, 45.08it/s] 50%|█████     | 217/431 [00:04<00:04, 45.32it/s] 52%|█████▏    | 222/431 [00:04<00:04, 45.44it/s] 53%|█████▎    | 227/431 [00:04<00:04, 45.28it/s] 54%|█████▍    | 232/431 [00:05<00:04, 45.14it/s] 55%|█████▍    | 237/431 [00:05<00:04, 44.99it/s] 56%|█████▌    | 242/431 [00:05<00:04, 44.90it/s] 57%|█████▋    | 247/431 [00:05<00:04, 45.00it/s] 58%|█████▊    | 252/431 [00:05<00:03, 45.16it/s] 60%|█████▉    | 257/431 [00:05<00:03, 45.18it/s] 61%|██████    | 262/431 [00:05<00:03, 45.29it/s] 62%|██████▏   | 267/431 [00:05<00:03, 45.31it/s] 63%|██████▎   | 272/431 [00:05<00:03, 45.15it/s] 64%|██████▍   | 277/431 [00:06<00:03, 45.10it/s] 65%|██████▌   | 282/431 [00:06<00:03, 44.97it/s] 67%|██████▋   | 287/431 [00:06<00:03, 45.02it/s] 68%|██████▊   | 292/431 [00:06<00:03, 45.12it/s] 69%|██████▉   | 297/431 [00:06<00:02, 45.25it/s] 70%|███████   | 302/431 [00:06<00:02, 45.17it/s] 71%|███████   | 307/431 [00:06<00:02, 45.32it/s] 72%|███████▏  | 312/431 [00:06<00:02, 45.21it/s] 74%|███████▎  | 317/431 [00:06<00:02, 45.23it/s] 75%|███████▍  | 322/431 [00:07<00:02, 45.11it/s] 76%|███████▌  | 327/431 [00:07<00:02, 44.98it/s] 77%|███████▋  | 332/431 [00:07<00:02, 44.91it/s] 78%|███████▊  | 337/431 [00:07<00:02, 45.09it/s] 79%|███████▉  | 342/431 [00:07<00:01, 45.15it/s] 81%|████████  | 347/431 [00:07<00:01, 45.20it/s] 82%|████████▏ | 352/431 [00:07<00:01, 45.27it/s] 83%|████████▎ | 357/431 [00:07<00:01, 45.33it/s] 84%|████████▍ | 362/431 [00:07<00:01, 45.07it/s] 85%|████████▌ | 367/431 [00:08<00:01, 44.95it/s] 86%|████████▋ | 372/431 [00:08<00:01, 44.98it/s] 87%|████████▋ | 377/431 [00:08<00:01, 45.02it/s] 89%|████████▊ | 382/431 [00:08<00:01, 44.69it/s] 90%|████████▉ | 387/431 [00:08<00:00, 45.04it/s] 91%|█████████ | 392/431 [00:08<00:00, 45.15it/s] 92%|█████████▏| 397/431 [00:08<00:00, 45.24it/s] 93%|█████████▎| 402/431 [00:08<00:00, 45.32it/s] 94%|█████████▍| 407/431 [00:08<00:00, 45.07it/s] 96%|█████████▌| 412/431 [00:09<00:00, 44.99it/s] 97%|█████████▋| 417/431 [00:09<00:00, 44.58it/s] 98%|█████████▊| 422/431 [00:09<00:00, 44.88it/s] 99%|█████████▉| 427/431 [00:09<00:00, 45.14it/s]100%|██████████| 431/431 [00:09<00:00, 45.22it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 06:10:12,439 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:12,439 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:12,439 >>   eval_loss               =     1.0824
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:12,439 >>   eval_runtime            = 0:00:09.54
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:12,439 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:12,439 >>   eval_samples_per_second =    360.869
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:12,439 >>   eval_steps_per_second   =     45.135
[INFO|trainer_pt_utils.py:913] 2023-08-29 06:10:12,439 >>   perplexity              =     2.9517
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:18,089 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:18,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:18,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:18,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:18,095 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:10:18,880 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:10:18,881 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:10:19,550 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:10:20,574 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:10:20,574 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:22,242 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:22,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:22,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:22,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:10:22,248 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:10:22,553 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:10:22,554 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:10:23,225 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:10:23,395 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:10:23,395 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-625
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-250
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-500
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-125
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/checkpoint-375
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.38it/s]Extractor Predicting: 2it [00:01,  1.48it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.61it/s]Extractor Predicting: 6it [00:03,  1.58it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.51it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.56it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.57it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.50it/s]Extractor Predicting: 27it [00:17,  1.48it/s]Extractor Predicting: 28it [00:18,  1.50it/s]Extractor Predicting: 29it [00:18,  1.50it/s]Extractor Predicting: 30it [00:19,  1.50it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:20,  1.51it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.59it/s]Extractor Predicting: 35it [00:22,  1.61it/s]Extractor Predicting: 36it [00:23,  1.63it/s]Extractor Predicting: 37it [00:23,  1.64it/s]Extractor Predicting: 38it [00:24,  1.60it/s]Extractor Predicting: 39it [00:25,  1.63it/s]Extractor Predicting: 40it [00:25,  1.63it/s]Extractor Predicting: 41it [00:26,  1.60it/s]Extractor Predicting: 42it [00:26,  1.65it/s]Extractor Predicting: 43it [00:27,  1.51it/s]Extractor Predicting: 44it [00:28,  1.54it/s]Extractor Predicting: 45it [00:29,  1.56it/s]Extractor Predicting: 46it [00:29,  1.59it/s]Extractor Predicting: 47it [00:30,  1.65it/s]Extractor Predicting: 48it [00:30,  1.66it/s]Extractor Predicting: 49it [00:31,  1.66it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:32,  1.62it/s]Extractor Predicting: 52it [00:33,  1.60it/s]Extractor Predicting: 53it [00:33,  1.58it/s]Extractor Predicting: 54it [00:34,  1.62it/s]Extractor Predicting: 55it [00:35,  1.62it/s]Extractor Predicting: 56it [00:35,  1.63it/s]Extractor Predicting: 57it [00:36,  1.67it/s]Extractor Predicting: 58it [00:36,  1.64it/s]Extractor Predicting: 59it [00:37,  1.69it/s]Extractor Predicting: 60it [00:38,  1.64it/s]Extractor Predicting: 61it [00:38,  1.65it/s]Extractor Predicting: 62it [00:39,  1.71it/s]Extractor Predicting: 63it [00:39,  1.71it/s]Extractor Predicting: 64it [00:40,  1.75it/s]Extractor Predicting: 65it [00:40,  1.76it/s]Extractor Predicting: 66it [00:41,  1.77it/s]Extractor Predicting: 67it [00:42,  1.75it/s]Extractor Predicting: 68it [00:42,  1.74it/s]Extractor Predicting: 69it [00:43,  1.78it/s]Extractor Predicting: 70it [00:43,  1.75it/s]Extractor Predicting: 71it [00:44,  1.72it/s]Extractor Predicting: 72it [00:44,  1.75it/s]Extractor Predicting: 73it [00:45,  1.68it/s]Extractor Predicting: 74it [00:46,  1.67it/s]Extractor Predicting: 75it [00:46,  1.72it/s]Extractor Predicting: 76it [00:47,  1.72it/s]Extractor Predicting: 77it [00:47,  1.74it/s]Extractor Predicting: 78it [00:48,  1.76it/s]Extractor Predicting: 79it [00:49,  1.75it/s]Extractor Predicting: 80it [00:49,  1.73it/s]Extractor Predicting: 81it [00:50,  1.69it/s]Extractor Predicting: 82it [00:50,  1.66it/s]Extractor Predicting: 83it [00:51,  1.68it/s]Extractor Predicting: 84it [00:52,  1.68it/s]Extractor Predicting: 85it [00:52,  1.62it/s]Extractor Predicting: 86it [00:53,  1.62it/s]Extractor Predicting: 87it [00:53,  1.63it/s]Extractor Predicting: 88it [00:54,  1.60it/s]Extractor Predicting: 89it [00:55,  1.59it/s]Extractor Predicting: 90it [00:55,  1.59it/s]Extractor Predicting: 91it [00:56,  1.60it/s]Extractor Predicting: 92it [00:57,  1.59it/s]Extractor Predicting: 93it [00:57,  1.57it/s]Extractor Predicting: 94it [00:58,  1.59it/s]Extractor Predicting: 95it [00:59,  1.57it/s]Extractor Predicting: 96it [00:59,  1.59it/s]Extractor Predicting: 97it [01:00,  1.58it/s]Extractor Predicting: 98it [01:00,  1.58it/s]Extractor Predicting: 99it [01:01,  1.60it/s]Extractor Predicting: 100it [01:02,  1.61it/s]Extractor Predicting: 101it [01:02,  1.61it/s]Extractor Predicting: 102it [01:03,  1.61it/s]Extractor Predicting: 103it [01:03,  1.62it/s]Extractor Predicting: 104it [01:04,  1.64it/s]Extractor Predicting: 105it [01:05,  1.66it/s]Extractor Predicting: 106it [01:05,  1.63it/s]Extractor Predicting: 107it [01:06,  1.62it/s]Extractor Predicting: 108it [01:07,  1.62it/s]Extractor Predicting: 109it [01:07,  1.61it/s]Extractor Predicting: 110it [01:08,  1.59it/s]Extractor Predicting: 111it [01:08,  1.58it/s]Extractor Predicting: 112it [01:09,  1.61it/s]Extractor Predicting: 113it [01:10,  1.62it/s]Extractor Predicting: 114it [01:10,  1.62it/s]Extractor Predicting: 115it [01:11,  1.61it/s]Extractor Predicting: 116it [01:11,  1.64it/s]Extractor Predicting: 117it [01:12,  1.64it/s]Extractor Predicting: 118it [01:13,  1.62it/s]Extractor Predicting: 119it [01:13,  1.63it/s]Extractor Predicting: 120it [01:14,  1.62it/s]Extractor Predicting: 121it [01:15,  1.61it/s]Extractor Predicting: 122it [01:15,  1.59it/s]Extractor Predicting: 123it [01:16,  1.59it/s]Extractor Predicting: 124it [01:17,  1.57it/s]Extractor Predicting: 125it [01:17,  1.59it/s]Extractor Predicting: 126it [01:18,  1.58it/s]Extractor Predicting: 127it [01:18,  1.57it/s]Extractor Predicting: 128it [01:19,  1.55it/s]Extractor Predicting: 129it [01:20,  1.58it/s]Extractor Predicting: 130it [01:20,  1.47it/s]Extractor Predicting: 131it [01:21,  1.48it/s]Extractor Predicting: 132it [01:22,  1.52it/s]Extractor Predicting: 133it [01:22,  1.56it/s]Extractor Predicting: 134it [01:23,  1.54it/s]Extractor Predicting: 135it [01:24,  1.54it/s]Extractor Predicting: 136it [01:24,  1.55it/s]Extractor Predicting: 137it [01:25,  1.59it/s]Extractor Predicting: 138it [01:26,  1.60it/s]Extractor Predicting: 139it [01:26,  1.61it/s]Extractor Predicting: 140it [01:27,  1.62it/s]Extractor Predicting: 141it [01:27,  1.66it/s]Extractor Predicting: 141it [01:27,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:59,955 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:59,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:59,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:59,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:11:59,958 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:12:00,585 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:12:00,586 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:12:01,159 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:12:02,204 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:12:02,204 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:05,038 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:05,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:05,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:05,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:12:05,042 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:12:05,670 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:12:05,671 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:12:06,222 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:12:06,383 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:12:06,383 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.2351938645078824,
  "recall": 0.16018572257690075,
  "score": 0.1905748316934231,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.61it/s]Extractor Predicting: 2it [00:01,  1.66it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.66it/s]Extractor Predicting: 9it [00:05,  1.67it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.67it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.64it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:09,  1.67it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:10,  1.62it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.64it/s]Extractor Predicting: 22it [00:13,  1.66it/s]Extractor Predicting: 23it [00:13,  1.61it/s]Extractor Predicting: 24it [00:14,  1.60it/s]Extractor Predicting: 25it [00:15,  1.62it/s]Extractor Predicting: 26it [00:15,  1.63it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.63it/s]Extractor Predicting: 29it [00:17,  1.60it/s]Extractor Predicting: 30it [00:18,  1.56it/s]Extractor Predicting: 31it [00:19,  1.55it/s]Extractor Predicting: 32it [00:19,  1.42it/s]Extractor Predicting: 33it [00:20,  1.47it/s]Extractor Predicting: 34it [00:21,  1.51it/s]Extractor Predicting: 35it [00:21,  1.53it/s]Extractor Predicting: 36it [00:22,  1.56it/s]Extractor Predicting: 37it [00:22,  1.63it/s]Extractor Predicting: 38it [00:23,  1.62it/s]Extractor Predicting: 39it [00:24,  1.61it/s]Extractor Predicting: 40it [00:24,  1.63it/s]Extractor Predicting: 41it [00:25,  1.62it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:27,  1.60it/s]Extractor Predicting: 45it [00:27,  1.58it/s]Extractor Predicting: 46it [00:28,  1.61it/s]Extractor Predicting: 47it [00:29,  1.61it/s]Extractor Predicting: 48it [00:29,  1.61it/s]Extractor Predicting: 49it [00:30,  1.60it/s]Extractor Predicting: 50it [00:31,  1.58it/s]Extractor Predicting: 51it [00:31,  1.59it/s]Extractor Predicting: 52it [00:32,  1.59it/s]Extractor Predicting: 53it [00:32,  1.60it/s]Extractor Predicting: 54it [00:33,  1.61it/s]Extractor Predicting: 55it [00:34,  1.57it/s]Extractor Predicting: 56it [00:34,  1.58it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:35,  1.64it/s]Extractor Predicting: 59it [00:36,  1.64it/s]Extractor Predicting: 60it [00:37,  1.65it/s]Extractor Predicting: 61it [00:37,  1.63it/s]Extractor Predicting: 62it [00:38,  1.63it/s]Extractor Predicting: 63it [00:39,  1.60it/s]Extractor Predicting: 64it [00:39,  1.63it/s]Extractor Predicting: 65it [00:40,  1.60it/s]Extractor Predicting: 66it [00:40,  1.57it/s]Extractor Predicting: 67it [00:41,  1.62it/s]Extractor Predicting: 68it [00:42,  1.64it/s]Extractor Predicting: 69it [00:42,  1.64it/s]Extractor Predicting: 70it [00:43,  1.63it/s]Extractor Predicting: 71it [00:43,  1.62it/s]Extractor Predicting: 72it [00:44,  1.63it/s]Extractor Predicting: 73it [00:45,  1.58it/s]Extractor Predicting: 74it [00:45,  1.60it/s]Extractor Predicting: 75it [00:46,  1.61it/s]Extractor Predicting: 76it [00:47,  1.62it/s]Extractor Predicting: 77it [00:47,  1.58it/s]Extractor Predicting: 78it [00:48,  1.58it/s]Extractor Predicting: 79it [00:49,  1.60it/s]Extractor Predicting: 80it [00:49,  1.59it/s]Extractor Predicting: 81it [00:50,  1.59it/s]Extractor Predicting: 82it [00:50,  1.59it/s]Extractor Predicting: 83it [00:51,  1.57it/s]Extractor Predicting: 84it [00:52,  1.59it/s]Extractor Predicting: 85it [00:52,  1.60it/s]Extractor Predicting: 86it [00:53,  1.59it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:54,  1.58it/s]Extractor Predicting: 89it [00:55,  1.57it/s]Extractor Predicting: 90it [00:55,  1.56it/s]Extractor Predicting: 91it [00:56,  1.58it/s]Extractor Predicting: 92it [00:57,  1.58it/s]Extractor Predicting: 93it [00:57,  1.57it/s]Extractor Predicting: 94it [00:58,  1.53it/s]Extractor Predicting: 95it [00:59,  1.52it/s]Extractor Predicting: 96it [00:59,  1.54it/s]Extractor Predicting: 97it [01:00,  1.55it/s]Extractor Predicting: 98it [01:01,  1.57it/s]Extractor Predicting: 99it [01:01,  1.55it/s]Extractor Predicting: 100it [01:02,  1.55it/s]Extractor Predicting: 101it [01:03,  1.58it/s]Extractor Predicting: 102it [01:03,  1.58it/s]Extractor Predicting: 103it [01:04,  1.57it/s]Extractor Predicting: 104it [01:04,  1.56it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:06,  1.57it/s]Extractor Predicting: 107it [01:06,  1.53it/s]Extractor Predicting: 108it [01:07,  1.54it/s]Extractor Predicting: 109it [01:08,  1.55it/s]Extractor Predicting: 110it [01:08,  1.53it/s]Extractor Predicting: 111it [01:09,  1.53it/s]Extractor Predicting: 112it [01:10,  1.55it/s]Extractor Predicting: 113it [01:10,  1.58it/s]Extractor Predicting: 114it [01:11,  1.61it/s]Extractor Predicting: 115it [01:12,  1.58it/s]Extractor Predicting: 116it [01:12,  1.60it/s]Extractor Predicting: 117it [01:13,  1.64it/s]Extractor Predicting: 118it [01:13,  1.61it/s]Extractor Predicting: 119it [01:14,  1.63it/s]Extractor Predicting: 120it [01:15,  1.63it/s]Extractor Predicting: 121it [01:15,  1.64it/s]Extractor Predicting: 122it [01:16,  1.68it/s]Extractor Predicting: 123it [01:16,  1.67it/s]Extractor Predicting: 124it [01:17,  1.66it/s]Extractor Predicting: 125it [01:18,  1.46it/s]Extractor Predicting: 126it [01:18,  1.49it/s]Extractor Predicting: 127it [01:19,  1.53it/s]Extractor Predicting: 128it [01:20,  1.58it/s]Extractor Predicting: 129it [01:20,  1.56it/s]Extractor Predicting: 130it [01:21,  1.57it/s]Extractor Predicting: 131it [01:22,  1.57it/s]Extractor Predicting: 132it [01:22,  1.59it/s]Extractor Predicting: 133it [01:23,  1.59it/s]Extractor Predicting: 134it [01:23,  1.59it/s]Extractor Predicting: 135it [01:24,  1.61it/s]Extractor Predicting: 136it [01:25,  1.62it/s]Extractor Predicting: 137it [01:25,  1.61it/s]Extractor Predicting: 138it [01:26,  1.61it/s]Extractor Predicting: 139it [01:26,  1.64it/s]Extractor Predicting: 140it [01:27,  1.62it/s]Extractor Predicting: 141it [01:28,  1.61it/s]Extractor Predicting: 142it [01:28,  1.63it/s]Extractor Predicting: 143it [01:29,  1.63it/s]Extractor Predicting: 144it [01:30,  1.60it/s]Extractor Predicting: 145it [01:30,  1.61it/s]Extractor Predicting: 146it [01:31,  1.61it/s]Extractor Predicting: 147it [01:31,  1.59it/s]Extractor Predicting: 148it [01:32,  1.58it/s]Extractor Predicting: 149it [01:33,  1.58it/s]Extractor Predicting: 150it [01:33,  1.59it/s]Extractor Predicting: 151it [01:34,  1.63it/s]Extractor Predicting: 152it [01:35,  1.63it/s]Extractor Predicting: 153it [01:35,  1.63it/s]Extractor Predicting: 154it [01:36,  1.60it/s]Extractor Predicting: 155it [01:36,  1.60it/s]Extractor Predicting: 156it [01:37,  1.58it/s]Extractor Predicting: 157it [01:38,  1.58it/s]Extractor Predicting: 158it [01:38,  1.57it/s]Extractor Predicting: 159it [01:39,  1.59it/s]Extractor Predicting: 160it [01:40,  1.60it/s]Extractor Predicting: 161it [01:40,  1.63it/s]Extractor Predicting: 162it [01:41,  1.65it/s]Extractor Predicting: 163it [01:41,  1.66it/s]Extractor Predicting: 164it [01:42,  1.66it/s]Extractor Predicting: 165it [01:43,  1.63it/s]Extractor Predicting: 166it [01:43,  1.62it/s]Extractor Predicting: 167it [01:44,  1.66it/s]Extractor Predicting: 168it [01:44,  1.65it/s]Extractor Predicting: 169it [01:45,  1.64it/s]Extractor Predicting: 170it [01:46,  1.62it/s]Extractor Predicting: 171it [01:46,  1.57it/s]Extractor Predicting: 172it [01:47,  1.55it/s]Extractor Predicting: 173it [01:48,  1.53it/s]Extractor Predicting: 174it [01:48,  1.49it/s]Extractor Predicting: 175it [01:49,  1.47it/s]Extractor Predicting: 176it [01:50,  1.50it/s]Extractor Predicting: 177it [01:50,  1.52it/s]Extractor Predicting: 178it [01:51,  1.54it/s]Extractor Predicting: 179it [01:52,  1.57it/s]Extractor Predicting: 180it [01:52,  1.56it/s]Extractor Predicting: 181it [01:53,  1.59it/s]Extractor Predicting: 182it [01:53,  1.60it/s]Extractor Predicting: 183it [01:54,  1.60it/s]Extractor Predicting: 184it [01:55,  1.62it/s]Extractor Predicting: 185it [01:55,  1.64it/s]Extractor Predicting: 186it [01:56,  1.67it/s]Extractor Predicting: 187it [01:57,  1.65it/s]Extractor Predicting: 188it [01:57,  1.66it/s]Extractor Predicting: 189it [01:58,  1.65it/s]Extractor Predicting: 190it [01:58,  1.65it/s]Extractor Predicting: 191it [01:59,  1.60it/s]Extractor Predicting: 192it [02:00,  1.58it/s]Extractor Predicting: 193it [02:00,  1.60it/s]Extractor Predicting: 194it [02:01,  1.64it/s]Extractor Predicting: 195it [02:01,  1.65it/s]Extractor Predicting: 196it [02:02,  1.65it/s]Extractor Predicting: 197it [02:03,  1.66it/s]Extractor Predicting: 198it [02:03,  1.65it/s]Extractor Predicting: 199it [02:04,  1.64it/s]Extractor Predicting: 200it [02:04,  1.63it/s]Extractor Predicting: 201it [02:05,  1.63it/s]Extractor Predicting: 202it [02:06,  1.65it/s]Extractor Predicting: 203it [02:06,  1.68it/s]Extractor Predicting: 204it [02:07,  1.68it/s]Extractor Predicting: 205it [02:07,  1.67it/s]Extractor Predicting: 206it [02:08,  1.46it/s]Extractor Predicting: 207it [02:09,  1.47it/s]Extractor Predicting: 208it [02:10,  1.51it/s]Extractor Predicting: 209it [02:10,  1.55it/s]Extractor Predicting: 210it [02:11,  1.56it/s]Extractor Predicting: 211it [02:11,  1.59it/s]Extractor Predicting: 212it [02:12,  1.57it/s]Extractor Predicting: 213it [02:13,  1.62it/s]Extractor Predicting: 214it [02:13,  1.60it/s]Extractor Predicting: 215it [02:14,  1.64it/s]Extractor Predicting: 216it [02:15,  1.64it/s]Extractor Predicting: 217it [02:15,  1.57it/s]Extractor Predicting: 218it [02:16,  1.59it/s]Extractor Predicting: 219it [02:16,  1.62it/s]Extractor Predicting: 220it [02:17,  1.64it/s]Extractor Predicting: 221it [02:18,  1.62it/s]Extractor Predicting: 222it [02:18,  1.63it/s]Extractor Predicting: 223it [02:19,  1.65it/s]Extractor Predicting: 224it [02:19,  1.64it/s]Extractor Predicting: 225it [02:20,  1.63it/s]Extractor Predicting: 226it [02:21,  1.58it/s]Extractor Predicting: 227it [02:21,  1.58it/s]Extractor Predicting: 228it [02:22,  1.54it/s]Extractor Predicting: 229it [02:23,  1.55it/s]Extractor Predicting: 230it [02:23,  1.58it/s]Extractor Predicting: 231it [02:24,  1.57it/s]Extractor Predicting: 232it [02:25,  1.57it/s]Extractor Predicting: 233it [02:25,  1.58it/s]Extractor Predicting: 234it [02:26,  1.59it/s]Extractor Predicting: 235it [02:27,  1.55it/s]Extractor Predicting: 236it [02:27,  1.58it/s]Extractor Predicting: 237it [02:28,  1.63it/s]Extractor Predicting: 238it [02:28,  1.59it/s]Extractor Predicting: 239it [02:29,  1.57it/s]Extractor Predicting: 240it [02:30,  1.54it/s]Extractor Predicting: 241it [02:30,  1.55it/s]Extractor Predicting: 242it [02:31,  1.59it/s]Extractor Predicting: 243it [02:32,  1.60it/s]Extractor Predicting: 244it [02:32,  1.56it/s]Extractor Predicting: 245it [02:33,  1.58it/s]Extractor Predicting: 246it [02:33,  1.60it/s]Extractor Predicting: 247it [02:34,  1.62it/s]Extractor Predicting: 248it [02:35,  1.54it/s]Extractor Predicting: 249it [02:35,  1.56it/s]Extractor Predicting: 250it [02:36,  1.57it/s]Extractor Predicting: 251it [02:37,  1.55it/s]Extractor Predicting: 252it [02:37,  1.54it/s]Extractor Predicting: 253it [02:38,  1.55it/s]Extractor Predicting: 254it [02:39,  1.59it/s]Extractor Predicting: 255it [02:39,  1.59it/s]Extractor Predicting: 256it [02:40,  1.63it/s]Extractor Predicting: 257it [02:40,  1.65it/s]Extractor Predicting: 258it [02:41,  1.63it/s]Extractor Predicting: 259it [02:42,  1.62it/s]Extractor Predicting: 260it [02:42,  1.59it/s]Extractor Predicting: 261it [02:43,  1.58it/s]Extractor Predicting: 262it [02:44,  1.59it/s]Extractor Predicting: 263it [02:44,  1.53it/s]Extractor Predicting: 264it [02:45,  1.55it/s]Extractor Predicting: 265it [02:46,  1.56it/s]Extractor Predicting: 266it [02:46,  1.58it/s]Extractor Predicting: 267it [02:47,  1.59it/s]Extractor Predicting: 268it [02:47,  1.61it/s]Extractor Predicting: 269it [02:48,  1.64it/s]Extractor Predicting: 270it [02:49,  1.67it/s]Extractor Predicting: 271it [02:49,  1.63it/s]Extractor Predicting: 272it [02:50,  1.65it/s]Extractor Predicting: 273it [02:50,  1.63it/s]Extractor Predicting: 274it [02:51,  1.63it/s]Extractor Predicting: 275it [02:52,  1.63it/s]Extractor Predicting: 276it [02:52,  1.63it/s]Extractor Predicting: 277it [02:53,  1.61it/s]Extractor Predicting: 278it [02:53,  1.64it/s]Extractor Predicting: 279it [02:54,  1.60it/s]Extractor Predicting: 280it [02:55,  1.60it/s]Extractor Predicting: 281it [02:55,  1.60it/s]Extractor Predicting: 282it [02:56,  1.62it/s]Extractor Predicting: 283it [02:57,  1.62it/s]Extractor Predicting: 284it [02:57,  1.59it/s]Extractor Predicting: 285it [02:58,  1.58it/s]Extractor Predicting: 286it [02:58,  1.60it/s]Extractor Predicting: 287it [02:59,  1.63it/s]Extractor Predicting: 288it [03:00,  1.62it/s]Extractor Predicting: 289it [03:00,  1.57it/s]Extractor Predicting: 290it [03:01,  1.63it/s]Extractor Predicting: 291it [03:02,  1.61it/s]Extractor Predicting: 292it [03:02,  1.61it/s]Extractor Predicting: 293it [03:03,  1.56it/s]Extractor Predicting: 294it [03:04,  1.51it/s]Extractor Predicting: 295it [03:04,  1.55it/s]Extractor Predicting: 296it [03:05,  1.55it/s]Extractor Predicting: 297it [03:05,  1.58it/s]Extractor Predicting: 298it [03:06,  1.53it/s]Extractor Predicting: 299it [03:07,  1.55it/s]Extractor Predicting: 300it [03:07,  1.56it/s]Extractor Predicting: 301it [03:08,  1.57it/s]Extractor Predicting: 302it [03:09,  1.56it/s]Extractor Predicting: 303it [03:09,  1.55it/s]Extractor Predicting: 304it [03:10,  1.52it/s]Extractor Predicting: 305it [03:11,  1.54it/s]Extractor Predicting: 306it [03:11,  1.52it/s]Extractor Predicting: 307it [03:12,  1.51it/s]Extractor Predicting: 308it [03:13,  1.54it/s]Extractor Predicting: 309it [03:13,  1.53it/s]Extractor Predicting: 310it [03:14,  1.51it/s]Extractor Predicting: 311it [03:15,  1.50it/s]Extractor Predicting: 312it [03:15,  1.52it/s]Extractor Predicting: 313it [03:16,  1.52it/s]Extractor Predicting: 314it [03:17,  1.49it/s]Extractor Predicting: 315it [03:17,  1.54it/s]Extractor Predicting: 316it [03:18,  1.53it/s]Extractor Predicting: 317it [03:19,  1.51it/s]Extractor Predicting: 318it [03:19,  1.53it/s]Extractor Predicting: 319it [03:20,  1.53it/s]Extractor Predicting: 320it [03:20,  1.62it/s]Extractor Predicting: 321it [03:21,  1.65it/s]Extractor Predicting: 322it [03:21,  1.73it/s]Extractor Predicting: 323it [03:22,  1.76it/s]Extractor Predicting: 324it [03:23,  1.78it/s]Extractor Predicting: 325it [03:23,  1.79it/s]Extractor Predicting: 326it [03:24,  1.80it/s]Extractor Predicting: 327it [03:24,  1.78it/s]Extractor Predicting: 328it [03:25,  1.80it/s]Extractor Predicting: 329it [03:25,  1.84it/s]Extractor Predicting: 330it [03:26,  1.83it/s]Extractor Predicting: 331it [03:26,  1.88it/s]Extractor Predicting: 332it [03:27,  1.84it/s]Extractor Predicting: 333it [03:27,  1.87it/s]Extractor Predicting: 334it [03:28,  1.86it/s]Extractor Predicting: 335it [03:29,  1.85it/s]Extractor Predicting: 336it [03:29,  1.83it/s]Extractor Predicting: 337it [03:30,  1.87it/s]Extractor Predicting: 338it [03:30,  1.82it/s]Extractor Predicting: 339it [03:31,  1.80it/s]Extractor Predicting: 340it [03:31,  1.87it/s]Extractor Predicting: 341it [03:32,  1.84it/s]Extractor Predicting: 342it [03:33,  1.61it/s]Extractor Predicting: 343it [03:33,  1.67it/s]Extractor Predicting: 344it [03:34,  1.69it/s]Extractor Predicting: 345it [03:34,  1.74it/s]Extractor Predicting: 346it [03:35,  1.75it/s]Extractor Predicting: 347it [03:35,  1.74it/s]Extractor Predicting: 348it [03:36,  1.71it/s]Extractor Predicting: 349it [03:37,  1.67it/s]Extractor Predicting: 350it [03:37,  1.63it/s]Extractor Predicting: 351it [03:38,  1.61it/s]Extractor Predicting: 352it [03:39,  1.61it/s]Extractor Predicting: 353it [03:39,  1.63it/s]Extractor Predicting: 354it [03:40,  1.61it/s]Extractor Predicting: 355it [03:40,  1.60it/s]Extractor Predicting: 356it [03:41,  1.57it/s]Extractor Predicting: 357it [03:42,  1.55it/s]Extractor Predicting: 358it [03:42,  1.58it/s]Extractor Predicting: 359it [03:43,  1.57it/s]Extractor Predicting: 360it [03:44,  1.60it/s]Extractor Predicting: 361it [03:44,  1.58it/s]Extractor Predicting: 362it [03:45,  1.61it/s]Extractor Predicting: 363it [03:46,  1.59it/s]Extractor Predicting: 364it [03:46,  1.58it/s]Extractor Predicting: 365it [03:47,  1.57it/s]Extractor Predicting: 366it [03:47,  1.57it/s]Extractor Predicting: 367it [03:48,  1.57it/s]Extractor Predicting: 368it [03:49,  1.59it/s]Extractor Predicting: 369it [03:49,  1.58it/s]Extractor Predicting: 370it [03:50,  1.57it/s]Extractor Predicting: 371it [03:51,  1.57it/s]Extractor Predicting: 372it [03:51,  1.58it/s]Extractor Predicting: 373it [03:52,  1.56it/s]Extractor Predicting: 374it [03:53,  1.58it/s]Extractor Predicting: 375it [03:53,  1.59it/s]Extractor Predicting: 376it [03:54,  1.57it/s]Extractor Predicting: 377it [03:54,  1.58it/s]Extractor Predicting: 378it [03:55,  1.56it/s]Extractor Predicting: 379it [03:56,  1.55it/s]Extractor Predicting: 380it [03:56,  1.55it/s]Extractor Predicting: 381it [03:57,  1.53it/s]Extractor Predicting: 382it [03:58,  1.52it/s]Extractor Predicting: 383it [03:58,  1.53it/s]Extractor Predicting: 384it [03:59,  1.51it/s]Extractor Predicting: 385it [04:00,  1.52it/s]Extractor Predicting: 386it [04:00,  1.54it/s]Extractor Predicting: 387it [04:01,  1.53it/s]Extractor Predicting: 388it [04:02,  1.52it/s]Extractor Predicting: 389it [04:02,  1.55it/s]Extractor Predicting: 390it [04:03,  1.56it/s]Extractor Predicting: 391it [04:04,  1.56it/s]Extractor Predicting: 392it [04:04,  1.52it/s]Extractor Predicting: 393it [04:05,  1.57it/s]Extractor Predicting: 394it [04:05,  1.55it/s]Extractor Predicting: 395it [04:06,  1.53it/s]Extractor Predicting: 396it [04:07,  1.52it/s]Extractor Predicting: 397it [04:08,  1.50it/s]Extractor Predicting: 398it [04:08,  1.51it/s]Extractor Predicting: 399it [04:09,  1.47it/s]Extractor Predicting: 400it [04:10,  1.48it/s]Extractor Predicting: 401it [04:10,  1.49it/s]Extractor Predicting: 402it [04:11,  1.52it/s]Extractor Predicting: 403it [04:11,  1.52it/s]Extractor Predicting: 404it [04:12,  1.56it/s]Extractor Predicting: 405it [04:13,  1.55it/s]Extractor Predicting: 406it [04:13,  1.59it/s]Extractor Predicting: 407it [04:14,  1.58it/s]Extractor Predicting: 408it [04:15,  1.55it/s]Extractor Predicting: 409it [04:15,  1.60it/s]Extractor Predicting: 410it [04:16,  1.59it/s]Extractor Predicting: 411it [04:16,  1.62it/s]Extractor Predicting: 412it [04:17,  1.59it/s]Extractor Predicting: 413it [04:18,  1.62it/s]Extractor Predicting: 414it [04:18,  1.62it/s]Extractor Predicting: 415it [04:19,  1.61it/s]Extractor Predicting: 416it [04:20,  1.58it/s]Extractor Predicting: 417it [04:20,  1.61it/s]Extractor Predicting: 418it [04:21,  1.60it/s]Extractor Predicting: 419it [04:21,  1.58it/s]Extractor Predicting: 420it [04:22,  1.58it/s]Extractor Predicting: 421it [04:23,  1.59it/s]Extractor Predicting: 422it [04:23,  1.57it/s]Extractor Predicting: 423it [04:24,  1.58it/s]Extractor Predicting: 424it [04:25,  1.54it/s]Extractor Predicting: 425it [04:25,  1.56it/s]Extractor Predicting: 426it [04:26,  1.55it/s]Extractor Predicting: 427it [04:27,  1.57it/s]Extractor Predicting: 428it [04:27,  1.56it/s]Extractor Predicting: 429it [04:28,  1.57it/s]Extractor Predicting: 430it [04:28,  1.95it/s]Extractor Predicting: 430it [04:28,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:43,949 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:43,956 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:43,957 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:43,957 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:43,957 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:16:44,676 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:16:44,676 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:16:45,045 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:16:46,113 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:16:46,113 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:48,645 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:48,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:48,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:48,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:16:48,651 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:16:48,980 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:16:48,980 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:16:49,295 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:16:49,464 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:16:49,464 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.26717557251908397,
  "recall": 0.16993590988541465,
  "score": 0.20773979107312443,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:02,  2.00it/s]Extractor Predicting: 5it [00:02,  1.72it/s]
[INFO|configuration_utils.py:515] 2023-08-29 06:16:52,783 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:16:52,784 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 06:16:52,791 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:16:52,792 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-29 06:16:52,795 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 06:16:55,887 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-29 06:16:55,890 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-29 06:16:55,900 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 06:16:55,901 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 06:16:55,908 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:16:55,910 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:16:55,910 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:16:55,910 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:16:55,910 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:16:55,910 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 06:16:55,910 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3968253968253968,
  "recall": 0.12376237623762376,
  "score": 0.18867924528301883,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/20 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-29 06:16:56,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:56,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:57,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:58,135 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:58,687 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:59,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:16:59,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:00,497 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:01,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:01,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:02,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:02,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:03,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:04,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:04,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:05,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:05,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:06,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:07,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:07,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:   5%|▌         | 1/20 [00:12<03:50, 12.12s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:08,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:08,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:09,363 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:09,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:10,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:10,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:11,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:11,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:12,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:13,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:13,549 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:14,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:14,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:15,228 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:15,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:16,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:16,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:17,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:17,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:18,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 2/20 [00:22<03:24, 11.34s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:19,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:19,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:20,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:20,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:21,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:21,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:22,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:22,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:22,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:23,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:23,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:24,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:25,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:25,530 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:25,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:26,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:26,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:27,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:27,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:28,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  15%|█▌        | 3/20 [00:32<02:59, 10.56s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:28,695 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:29,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:29,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:30,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:30,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:31,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:32,409 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:32,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:33,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:34,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:34,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:35,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:35,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:36,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:36,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:37,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:37,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:38,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:39,061 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:39,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 4/20 [00:44<02:54, 10.92s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:40,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:40,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:41,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:41,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:42,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:42,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:43,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:43,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:44,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:44,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:45,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:45,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:46,379 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:46,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:47,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:47,950 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:48,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:49,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:49,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:50,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  25%|██▌       | 5/20 [00:54<02:40, 10.73s/it][WARNING|generation_utils.py:914] 2023-08-29 06:17:50,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:51,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:52,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:52,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:53,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:54,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:54,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:55,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:56,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:57,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:57,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:58,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:59,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:17:59,793 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:00,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:00,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:01,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:01,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:02,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:03,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:03,680 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 6/20 [01:08<02:43, 11.71s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:04,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:04,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:05,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:05,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:06,306 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:06,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:07,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:07,892 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:08,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:08,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:09,394 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:09,890 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:10,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:10,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:11,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:11,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:12,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:12,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:13,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:13,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  35%|███▌      | 7/20 [01:18<02:25, 11.16s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:14,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:14,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:15,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:15,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:16,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:16,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:17,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:18,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:18,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:19,117 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:19,611 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:20,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:20,824 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:21,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:21,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:22,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:22,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:23,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:23,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:24,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 8/20 [01:29<02:14, 11.17s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:25,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:25,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:26,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:26,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:27,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:27,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:28,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:29,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:29,791 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:30,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:31,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:31,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:32,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:32,523 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:33,027 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:33,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:34,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:34,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:35,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:35,873 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:36,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:36,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  45%|████▌     | 9/20 [01:41<02:06, 11.47s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:37,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:38,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:38,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:39,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:39,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:40,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:40,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:41,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:41,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:42,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:42,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:43,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:43,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:44,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:44,989 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:45,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:46,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:46,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:47,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:47,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 10/20 [01:52<01:52, 11.23s/it][WARNING|generation_utils.py:914] 2023-08-29 06:18:48,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:48,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:49,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:50,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:50,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:51,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:51,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:52,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:53,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:53,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:54,325 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:54,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:55,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:56,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:56,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:57,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:57,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:58,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:59,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:18:59,673 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  55%|█████▌    | 11/20 [02:04<01:43, 11.48s/it][WARNING|generation_utils.py:914] 2023-08-29 06:19:00,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:00,768 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:01,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:01,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:02,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:02,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:03,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:03,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:04,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:05,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:05,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:05,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:06,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:07,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:07,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:08,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:08,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:09,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:09,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:10,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 12/20 [02:14<01:29, 11.15s/it][WARNING|generation_utils.py:914] 2023-08-29 06:19:10,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:11,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:11,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:12,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:12,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:13,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:13,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:14,421 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:14,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:15,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:15,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:16,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:16,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:17,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:17,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:18,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:19,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:19,572 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:20,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:20,573 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  65%|██████▌   | 13/20 [02:24<01:16, 10.93s/it][WARNING|generation_utils.py:914] 2023-08-29 06:19:21,084 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:21,652 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:22,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:22,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:23,433 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:23,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:24,594 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:25,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:25,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:26,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:27,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:27,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:28,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:28,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:29,559 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:30,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:30,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:31,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:31,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:32,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 14/20 [02:36<01:07, 11.27s/it][WARNING|generation_utils.py:914] 2023-08-29 06:19:33,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:33,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:34,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:34,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:35,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:35,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:36,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:37,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:37,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:38,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:38,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:39,069 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:39,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:40,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:40,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:41,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:41,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:42,292 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:42,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:43,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:44,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  75%|███████▌  | 15/20 [02:48<00:56, 11.31s/it][WARNING|generation_utils.py:914] 2023-08-29 06:19:44,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:45,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:45,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:46,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:46,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:47,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:48,280 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:48,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:49,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:50,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:50,646 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:51,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:51,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:52,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:53,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:53,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:54,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:54,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:55,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:56,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 16/20 [03:00<00:46, 11.65s/it][WARNING|generation_utils.py:914] 2023-08-29 06:19:56,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:57,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:58,324 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:58,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:19:59,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:00,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:00,788 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:01,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:01,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:02,466 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:03,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:03,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:04,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:04,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:05,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:06,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:06,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:07,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:07,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:08,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:08,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:09,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  85%|████████▌ | 17/20 [03:14<00:36, 12.11s/it][WARNING|generation_utils.py:914] 2023-08-29 06:20:10,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:10,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:11,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:12,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:12,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:13,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:13,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:14,463 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:15,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:15,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:16,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:16,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:17,374 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:17,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:18,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:19,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:19,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:20,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:20,761 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:21,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 18/20 [03:25<00:24, 12.04s/it][WARNING|generation_utils.py:914] 2023-08-29 06:20:22,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:22,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:23,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:23,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:24,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:24,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:25,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:25,730 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:26,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:26,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:27,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:27,676 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:28,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:28,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:29,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:29,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:30,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:30,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:31,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:31,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:32,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  95%|█████████▌| 19/20 [03:36<00:11, 11.60s/it][WARNING|generation_utils.py:914] 2023-08-29 06:20:32,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:33,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:33,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:34,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:35,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:35,464 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:36,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:37,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:37,686 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:38,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:38,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:39,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:39,812 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:40,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:41,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:41,971 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:42,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:43,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:43,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-29 06:20:44,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 20/20 [03:48<00:00, 11.79s/it]Generating: 100%|██████████| 20/20 [03:48<00:00, 11.44s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:51,816 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:51,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:51,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:51,820 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:51,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:20:52,602 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:20:52,603 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:20:53,196 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:20:54,269 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:20:54,269 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:57,145 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:57,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:57,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:57,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:20:57,149 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:20:57,785 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:20:57,786 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:20:58,383 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:20:58,559 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:20:58,560 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 96, 'raw': 96}
{'target': 600, 'success': 128, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 347, 'raw': 352}
{'target': 600, 'success': 379, 'raw': 384}
{'target': 600, 'success': 410, 'raw': 416}
{'target': 600, 'success': 442, 'raw': 448}
{'target': 600, 'success': 473, 'raw': 480}
{'target': 600, 'success': 505, 'raw': 512}
{'target': 600, 'success': 535, 'raw': 544}
{'target': 600, 'success': 565, 'raw': 576}
{'target': 600, 'success': 596, 'raw': 608}
{'target': 600, 'success': 628, 'raw': 640}
{'prompt': 'Relation : followed by .', 'success_rate': 0.98125, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 398, 'raw': 416}
{'target': 600, 'success': 428, 'raw': 448}
{'target': 600, 'success': 460, 'raw': 480}
{'target': 600, 'success': 491, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 585, 'raw': 608}
{'target': 600, 'success': 617, 'raw': 640}
{'prompt': 'Relation : military rank .', 'success_rate': 0.9640625, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 188, 'raw': 192}
{'target': 600, 'success': 220, 'raw': 224}
{'target': 600, 'success': 251, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 472, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 532, 'raw': 544}
{'target': 600, 'success': 563, 'raw': 576}
{'target': 600, 'success': 595, 'raw': 608}
{'target': 600, 'success': 627, 'raw': 640}
{'prompt': 'Relation : operating system .', 'success_rate': 0.9796875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 187, 'raw': 192}
{'target': 600, 'success': 218, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 377, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 440, 'raw': 448}
{'target': 600, 'success': 472, 'raw': 480}
{'target': 600, 'success': 504, 'raw': 512}
{'target': 600, 'success': 536, 'raw': 544}
{'target': 600, 'success': 565, 'raw': 576}
{'target': 600, 'success': 597, 'raw': 608}
{'target': 600, 'success': 629, 'raw': 640}
{'prompt': 'Relation : record label .', 'success_rate': 0.9828125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 182, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 245, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 432, 'raw': 448}
{'target': 600, 'success': 464, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 622, 'raw': 640}
{'prompt': 'Relation : tributary .', 'success_rate': 0.971875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 323, 'raw': 352}
{'target': 600, 'success': 354, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 446, 'raw': 480}
{'target': 600, 'success': 477, 'raw': 512}
{'target': 600, 'success': 506, 'raw': 544}
{'target': 600, 'success': 536, 'raw': 576}
{'target': 600, 'success': 567, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 628, 'raw': 672}
{'prompt': 'Relation : architect .', 'success_rate': 0.9345238095238095, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 127, 'raw': 128}
{'target': 600, 'success': 159, 'raw': 160}
{'target': 600, 'success': 190, 'raw': 192}
{'target': 600, 'success': 219, 'raw': 224}
{'target': 600, 'success': 250, 'raw': 256}
{'target': 600, 'success': 282, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 346, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 408, 'raw': 416}
{'target': 600, 'success': 439, 'raw': 448}
{'target': 600, 'success': 471, 'raw': 480}
{'target': 600, 'success': 503, 'raw': 512}
{'target': 600, 'success': 534, 'raw': 544}
{'target': 600, 'success': 566, 'raw': 576}
{'target': 600, 'success': 598, 'raw': 608}
{'target': 600, 'success': 627, 'raw': 640}
{'prompt': 'Relation : constellation .', 'success_rate': 0.9796875, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 95, 'raw': 96}
{'target': 600, 'success': 126, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 252, 'raw': 256}
{'target': 600, 'success': 284, 'raw': 288}
{'target': 600, 'success': 314, 'raw': 320}
{'target': 600, 'success': 344, 'raw': 352}
{'target': 600, 'success': 376, 'raw': 384}
{'target': 600, 'success': 406, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 564, 'raw': 576}
{'target': 600, 'success': 595, 'raw': 608}
{'target': 600, 'success': 626, 'raw': 640}
{'prompt': 'Relation : contains administrative territorial entity .', 'success_rate': 0.978125, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 310, 'raw': 352}
{'target': 600, 'success': 338, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 395, 'raw': 448}
{'target': 600, 'success': 424, 'raw': 480}
{'target': 600, 'success': 451, 'raw': 512}
{'target': 600, 'success': 481, 'raw': 544}
{'target': 600, 'success': 511, 'raw': 576}
{'target': 600, 'success': 536, 'raw': 608}
{'target': 600, 'success': 564, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 622, 'raw': 704}
{'prompt': 'Relation : country of origin .', 'success_rate': 0.8835227272727273, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 210, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 302, 'raw': 320}
{'target': 600, 'success': 334, 'raw': 352}
{'target': 600, 'success': 365, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 422, 'raw': 448}
{'target': 600, 'success': 452, 'raw': 480}
{'target': 600, 'success': 482, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 544, 'raw': 576}
{'target': 600, 'success': 575, 'raw': 608}
{'target': 600, 'success': 606, 'raw': 640}
{'prompt': 'Relation : developer .', 'success_rate': 0.946875, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 157, 'raw': 160}
{'target': 600, 'success': 189, 'raw': 192}
{'target': 600, 'success': 221, 'raw': 224}
{'target': 600, 'success': 253, 'raw': 256}
{'target': 600, 'success': 283, 'raw': 288}
{'target': 600, 'success': 315, 'raw': 320}
{'target': 600, 'success': 345, 'raw': 352}
{'target': 600, 'success': 375, 'raw': 384}
{'target': 600, 'success': 405, 'raw': 416}
{'target': 600, 'success': 437, 'raw': 448}
{'target': 600, 'success': 469, 'raw': 480}
{'target': 600, 'success': 501, 'raw': 512}
{'target': 600, 'success': 533, 'raw': 544}
{'target': 600, 'success': 565, 'raw': 576}
{'target': 600, 'success': 596, 'raw': 608}
{'target': 600, 'success': 627, 'raw': 640}
{'prompt': 'Relation : follows .', 'success_rate': 0.9796875, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 119, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 178, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 271, 'raw': 288}
{'target': 600, 'success': 301, 'raw': 320}
{'target': 600, 'success': 333, 'raw': 352}
{'target': 600, 'success': 363, 'raw': 384}
{'target': 600, 'success': 393, 'raw': 416}
{'target': 600, 'success': 423, 'raw': 448}
{'target': 600, 'success': 453, 'raw': 480}
{'target': 600, 'success': 485, 'raw': 512}
{'target': 600, 'success': 513, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 573, 'raw': 608}
{'target': 600, 'success': 603, 'raw': 640}
{'prompt': 'Relation : league .', 'success_rate': 0.9421875, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 305, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 364, 'raw': 384}
{'target': 600, 'success': 396, 'raw': 416}
{'target': 600, 'success': 427, 'raw': 448}
{'target': 600, 'success': 458, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 550, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 614, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.959375, 'errors': {''}}
['Relation : member of . Context : Later in 2008 , he joined the band , called " The Three Stooges at the end of 2010 " . Head Entity : The Three Stooges at the end of 2010 , Tail Entity : The Three Stooges at the end of 2008 .\n']
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 186, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 311, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 404, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 466, 'raw': 480}
{'target': 600, 'success': 497, 'raw': 512}
{'target': 600, 'success': 529, 'raw': 544}
{'target': 600, 'success': 558, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : member of .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 181, 'raw': 192}
{'target': 600, 'success': 211, 'raw': 224}
{'target': 600, 'success': 240, 'raw': 256}
{'target': 600, 'success': 270, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 385, 'raw': 416}
{'target': 600, 'success': 414, 'raw': 448}
{'target': 600, 'success': 444, 'raw': 480}
{'target': 600, 'success': 476, 'raw': 512}
{'target': 600, 'success': 503, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 587, 'raw': 640}
{'target': 600, 'success': 614, 'raw': 672}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.9136904761904762, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 277, 'raw': 288}
{'target': 600, 'success': 306, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 429, 'raw': 448}
{'target': 600, 'success': 459, 'raw': 480}
{'target': 600, 'success': 488, 'raw': 512}
{'target': 600, 'success': 520, 'raw': 544}
{'target': 600, 'success': 548, 'raw': 576}
{'target': 600, 'success': 579, 'raw': 608}
{'target': 600, 'success': 610, 'raw': 640}
{'prompt': 'Relation : notable work .', 'success_rate': 0.953125, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : operator . Context : On 31 March 2014 , the company announced that it would be replacing its fleet of diesel power plants with hydrogen fuel cells and run on electricity from the plant under the new model . Head Entity : hydrogen fuel cells , Tail Entity : Suez .\n']
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 173, 'raw': 192}
{'target': 600, 'success': 203, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 283, 'raw': 320}
{'target': 600, 'success': 311, 'raw': 352}
{'target': 600, 'success': 334, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 391, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 450, 'raw': 512}
{'target': 600, 'success': 480, 'raw': 544}
{'target': 600, 'success': 508, 'raw': 576}
{'target': 600, 'success': 537, 'raw': 608}
{'target': 600, 'success': 567, 'raw': 640}
{'target': 600, 'success': 594, 'raw': 672}
{'target': 600, 'success': 623, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8849431818181818, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 213, 'raw': 224}
{'target': 600, 'success': 244, 'raw': 256}
{'target': 600, 'success': 274, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 524, 'raw': 544}
{'target': 600, 'success': 555, 'raw': 576}
{'target': 600, 'success': 586, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : original broadcaster .', 'success_rate': 0.965625, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 258, 'raw': 288}
{'target': 600, 'success': 286, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 343, 'raw': 384}
{'target': 600, 'success': 369, 'raw': 416}
{'target': 600, 'success': 398, 'raw': 448}
{'target': 600, 'success': 427, 'raw': 480}
{'target': 600, 'success': 452, 'raw': 512}
{'target': 600, 'success': 482, 'raw': 544}
{'target': 600, 'success': 512, 'raw': 576}
{'target': 600, 'success': 544, 'raw': 608}
{'target': 600, 'success': 574, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : position held .', 'success_rate': 0.8943452380952381, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 64, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 156, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 248, 'raw': 256}
{'target': 600, 'success': 279, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 341, 'raw': 352}
{'target': 600, 'success': 373, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 433, 'raw': 448}
{'target': 600, 'success': 465, 'raw': 480}
{'target': 600, 'success': 496, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 556, 'raw': 576}
{'target': 600, 'success': 587, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : residence .', 'success_rate': 0.9609375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 8128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.09it/s]Extractor Estimating: 2it [00:01,  1.27it/s]Extractor Estimating: 3it [00:02,  1.40it/s]Extractor Estimating: 4it [00:02,  1.45it/s]Extractor Estimating: 5it [00:03,  1.45it/s]Extractor Estimating: 6it [00:04,  1.45it/s]Extractor Estimating: 7it [00:04,  1.47it/s]Extractor Estimating: 8it [00:05,  1.46it/s]Extractor Estimating: 9it [00:06,  1.47it/s]Extractor Estimating: 10it [00:07,  1.43it/s]Extractor Estimating: 11it [00:07,  1.46it/s]Extractor Estimating: 12it [00:08,  1.46it/s]Extractor Estimating: 13it [00:08,  1.52it/s]Extractor Estimating: 14it [00:09,  1.51it/s]Extractor Estimating: 15it [00:10,  1.50it/s]Extractor Estimating: 16it [00:10,  1.56it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:12,  1.47it/s]Extractor Estimating: 19it [00:12,  1.52it/s]Extractor Estimating: 20it [00:13,  1.50it/s]Extractor Estimating: 21it [00:14,  1.50it/s]Extractor Estimating: 22it [00:15,  1.41it/s]Extractor Estimating: 23it [00:15,  1.41it/s]Extractor Estimating: 24it [00:16,  1.43it/s]Extractor Estimating: 25it [00:17,  1.45it/s]Extractor Estimating: 26it [00:17,  1.54it/s]Extractor Estimating: 27it [00:18,  1.65it/s]Extractor Estimating: 28it [00:18,  1.69it/s]Extractor Estimating: 29it [00:19,  1.71it/s]Extractor Estimating: 30it [00:19,  1.76it/s]Extractor Estimating: 31it [00:20,  1.80it/s]Extractor Estimating: 32it [00:20,  1.80it/s]Extractor Estimating: 33it [00:21,  1.83it/s]Extractor Estimating: 34it [00:22,  1.81it/s]Extractor Estimating: 35it [00:22,  1.83it/s]Extractor Estimating: 36it [00:23,  1.84it/s]Extractor Estimating: 37it [00:23,  1.84it/s]Extractor Estimating: 38it [00:24,  1.76it/s]Extractor Estimating: 39it [00:24,  1.80it/s]Extractor Estimating: 40it [00:25,  1.83it/s]Extractor Estimating: 41it [00:25,  1.82it/s]Extractor Estimating: 42it [00:26,  1.74it/s]Extractor Estimating: 43it [00:27,  1.74it/s]Extractor Estimating: 44it [00:27,  1.80it/s]Extractor Estimating: 45it [00:28,  1.75it/s]Extractor Estimating: 46it [00:28,  1.79it/s]Extractor Estimating: 47it [00:29,  1.78it/s]Extractor Estimating: 48it [00:29,  1.83it/s]Extractor Estimating: 49it [00:30,  1.82it/s]Extractor Estimating: 50it [00:30,  1.81it/s]Extractor Estimating: 51it [00:31,  1.83it/s]Extractor Estimating: 52it [00:31,  1.88it/s]Extractor Estimating: 53it [00:32,  1.93it/s]Extractor Estimating: 54it [00:32,  2.03it/s]Extractor Estimating: 55it [00:33,  2.01it/s]Extractor Estimating: 56it [00:33,  1.98it/s]Extractor Estimating: 57it [00:34,  2.06it/s]Extractor Estimating: 58it [00:34,  2.05it/s]Extractor Estimating: 59it [00:35,  2.03it/s]Extractor Estimating: 60it [00:35,  2.03it/s]Extractor Estimating: 61it [00:36,  2.03it/s]Extractor Estimating: 62it [00:36,  2.07it/s]Extractor Estimating: 63it [00:37,  2.06it/s]Extractor Estimating: 64it [00:37,  2.12it/s]Extractor Estimating: 65it [00:38,  2.04it/s]Extractor Estimating: 66it [00:38,  2.03it/s]Extractor Estimating: 67it [00:39,  2.02it/s]Extractor Estimating: 68it [00:39,  2.04it/s]Extractor Estimating: 69it [00:40,  2.05it/s]Extractor Estimating: 70it [00:40,  2.03it/s]Extractor Estimating: 71it [00:41,  2.07it/s]Extractor Estimating: 72it [00:41,  2.01it/s]Extractor Estimating: 73it [00:42,  2.04it/s]Extractor Estimating: 74it [00:42,  2.04it/s]Extractor Estimating: 75it [00:43,  2.04it/s]Extractor Estimating: 76it [00:43,  1.92it/s]Extractor Estimating: 77it [00:44,  1.84it/s]Extractor Estimating: 78it [00:44,  1.78it/s]Extractor Estimating: 79it [00:45,  1.72it/s]Extractor Estimating: 80it [00:46,  1.71it/s]Extractor Estimating: 81it [00:46,  1.69it/s]Extractor Estimating: 82it [00:47,  1.54it/s]Extractor Estimating: 83it [00:48,  1.58it/s]Extractor Estimating: 84it [00:48,  1.56it/s]Extractor Estimating: 85it [00:49,  1.54it/s]Extractor Estimating: 86it [00:50,  1.60it/s]Extractor Estimating: 87it [00:50,  1.59it/s]Extractor Estimating: 88it [00:51,  1.65it/s]Extractor Estimating: 89it [00:51,  1.66it/s]Extractor Estimating: 90it [00:52,  1.61it/s]Extractor Estimating: 91it [00:53,  1.65it/s]Extractor Estimating: 92it [00:53,  1.69it/s]Extractor Estimating: 93it [00:54,  1.67it/s]Extractor Estimating: 94it [00:54,  1.65it/s]Extractor Estimating: 95it [00:55,  1.66it/s]Extractor Estimating: 96it [00:56,  1.68it/s]Extractor Estimating: 97it [00:56,  1.68it/s]Extractor Estimating: 98it [00:57,  1.64it/s]Extractor Estimating: 99it [00:57,  1.62it/s]Extractor Estimating: 100it [00:58,  1.57it/s]Extractor Estimating: 101it [00:59,  1.72it/s]Extractor Estimating: 102it [00:59,  1.90it/s]Extractor Estimating: 103it [00:59,  1.99it/s]Extractor Estimating: 104it [01:00,  2.04it/s]Extractor Estimating: 105it [01:00,  2.12it/s]Extractor Estimating: 106it [01:01,  2.28it/s]Extractor Estimating: 107it [01:01,  2.32it/s]Extractor Estimating: 108it [01:02,  2.27it/s]Extractor Estimating: 109it [01:02,  2.32it/s]Extractor Estimating: 110it [01:02,  2.34it/s]Extractor Estimating: 111it [01:03,  2.40it/s]Extractor Estimating: 112it [01:03,  2.30it/s]Extractor Estimating: 113it [01:04,  2.24it/s]Extractor Estimating: 114it [01:04,  2.31it/s]Extractor Estimating: 115it [01:04,  2.36it/s]Extractor Estimating: 116it [01:05,  2.27it/s]Extractor Estimating: 117it [01:05,  2.27it/s]Extractor Estimating: 118it [01:06,  2.21it/s]Extractor Estimating: 119it [01:06,  2.26it/s]Extractor Estimating: 120it [01:07,  2.26it/s]Extractor Estimating: 121it [01:07,  2.21it/s]Extractor Estimating: 122it [01:08,  2.29it/s]Extractor Estimating: 123it [01:08,  2.25it/s]Extractor Estimating: 124it [01:09,  2.26it/s]Extractor Estimating: 125it [01:09,  2.22it/s]Extractor Estimating: 126it [01:10,  2.13it/s]Extractor Estimating: 127it [01:10,  1.98it/s]Extractor Estimating: 128it [01:11,  1.99it/s]Extractor Estimating: 129it [01:11,  1.92it/s]Extractor Estimating: 130it [01:12,  1.81it/s]Extractor Estimating: 131it [01:12,  1.83it/s]Extractor Estimating: 132it [01:13,  1.82it/s]Extractor Estimating: 133it [01:13,  1.77it/s]Extractor Estimating: 134it [01:14,  1.79it/s]Extractor Estimating: 135it [01:15,  1.84it/s]Extractor Estimating: 136it [01:15,  1.81it/s]Extractor Estimating: 137it [01:16,  1.83it/s]Extractor Estimating: 138it [01:16,  1.83it/s]Extractor Estimating: 139it [01:17,  1.82it/s]Extractor Estimating: 140it [01:17,  1.83it/s]Extractor Estimating: 141it [01:18,  1.82it/s]Extractor Estimating: 142it [01:18,  1.82it/s]Extractor Estimating: 143it [01:19,  1.85it/s]Extractor Estimating: 144it [01:19,  1.87it/s]Extractor Estimating: 145it [01:20,  1.83it/s]Extractor Estimating: 146it [01:21,  1.87it/s]Extractor Estimating: 147it [01:21,  1.83it/s]Extractor Estimating: 148it [01:22,  1.79it/s]Extractor Estimating: 149it [01:22,  1.82it/s]Extractor Estimating: 150it [01:23,  1.81it/s]Extractor Estimating: 151it [01:23,  1.92it/s]Extractor Estimating: 152it [01:24,  1.91it/s]Extractor Estimating: 153it [01:24,  1.93it/s]Extractor Estimating: 154it [01:25,  1.99it/s]Extractor Estimating: 155it [01:25,  1.95it/s]Extractor Estimating: 156it [01:26,  1.90it/s]Extractor Estimating: 157it [01:26,  1.88it/s]Extractor Estimating: 158it [01:27,  1.89it/s]Extractor Estimating: 159it [01:27,  1.94it/s]Extractor Estimating: 160it [01:28,  1.97it/s]Extractor Estimating: 161it [01:28,  1.98it/s]Extractor Estimating: 162it [01:29,  1.97it/s]Extractor Estimating: 163it [01:29,  2.02it/s]Extractor Estimating: 164it [01:30,  2.03it/s]Extractor Estimating: 165it [01:30,  1.97it/s]Extractor Estimating: 166it [01:31,  2.01it/s]Extractor Estimating: 167it [01:31,  1.93it/s]Extractor Estimating: 168it [01:32,  1.99it/s]Extractor Estimating: 169it [01:32,  2.03it/s]Extractor Estimating: 170it [01:33,  2.05it/s]Extractor Estimating: 171it [01:33,  2.06it/s]Extractor Estimating: 172it [01:34,  2.05it/s]Extractor Estimating: 173it [01:34,  2.06it/s]Extractor Estimating: 174it [01:35,  2.08it/s]Extractor Estimating: 175it [01:35,  2.06it/s]Extractor Estimating: 176it [01:36,  2.07it/s]Extractor Estimating: 177it [01:36,  2.11it/s]Extractor Estimating: 178it [01:37,  2.13it/s]Extractor Estimating: 179it [01:37,  2.10it/s]Extractor Estimating: 180it [01:38,  2.03it/s]Extractor Estimating: 181it [01:38,  1.98it/s]Extractor Estimating: 182it [01:39,  1.98it/s]Extractor Estimating: 183it [01:39,  2.04it/s]Extractor Estimating: 184it [01:40,  2.07it/s]Extractor Estimating: 185it [01:40,  2.14it/s]Extractor Estimating: 186it [01:41,  1.87it/s]Extractor Estimating: 187it [01:41,  1.93it/s]Extractor Estimating: 188it [01:42,  1.97it/s]Extractor Estimating: 189it [01:42,  2.01it/s]Extractor Estimating: 190it [01:43,  2.01it/s]Extractor Estimating: 191it [01:43,  2.07it/s]Extractor Estimating: 192it [01:44,  2.13it/s]Extractor Estimating: 193it [01:44,  2.18it/s]Extractor Estimating: 194it [01:45,  2.08it/s]Extractor Estimating: 195it [01:45,  2.07it/s]Extractor Estimating: 196it [01:45,  2.13it/s]Extractor Estimating: 197it [01:46,  2.07it/s]Extractor Estimating: 198it [01:46,  2.10it/s]Extractor Estimating: 199it [01:47,  2.01it/s]Extractor Estimating: 200it [01:47,  2.10it/s]Extractor Estimating: 201it [01:48,  2.04it/s]Extractor Estimating: 202it [01:48,  2.00it/s]Extractor Estimating: 203it [01:49,  1.94it/s]Extractor Estimating: 204it [01:49,  2.01it/s]Extractor Estimating: 205it [01:50,  2.04it/s]Extractor Estimating: 206it [01:50,  2.07it/s]Extractor Estimating: 207it [01:51,  2.09it/s]Extractor Estimating: 208it [01:51,  1.98it/s]Extractor Estimating: 209it [01:52,  1.93it/s]Extractor Estimating: 210it [01:52,  1.96it/s]Extractor Estimating: 211it [01:53,  1.88it/s]Extractor Estimating: 212it [01:54,  1.94it/s]Extractor Estimating: 213it [01:54,  1.92it/s]Extractor Estimating: 214it [01:54,  1.99it/s]Extractor Estimating: 215it [01:55,  1.98it/s]Extractor Estimating: 216it [01:56,  1.99it/s]Extractor Estimating: 217it [01:56,  1.97it/s]Extractor Estimating: 218it [01:57,  1.83it/s]Extractor Estimating: 219it [01:57,  1.86it/s]Extractor Estimating: 220it [01:58,  1.88it/s]Extractor Estimating: 221it [01:58,  1.88it/s]Extractor Estimating: 222it [01:59,  1.93it/s]Extractor Estimating: 223it [01:59,  1.97it/s]Extractor Estimating: 224it [02:00,  1.98it/s]Extractor Estimating: 225it [02:00,  1.92it/s]Extractor Estimating: 226it [02:01,  1.78it/s]Extractor Estimating: 227it [02:01,  1.75it/s]Extractor Estimating: 228it [02:02,  1.71it/s]Extractor Estimating: 229it [02:03,  1.65it/s]Extractor Estimating: 230it [02:03,  1.65it/s]Extractor Estimating: 231it [02:04,  1.64it/s]Extractor Estimating: 232it [02:05,  1.59it/s]Extractor Estimating: 233it [02:05,  1.58it/s]Extractor Estimating: 234it [02:06,  1.59it/s]Extractor Estimating: 235it [02:07,  1.59it/s]Extractor Estimating: 236it [02:07,  1.64it/s]Extractor Estimating: 237it [02:08,  1.63it/s]Extractor Estimating: 238it [02:08,  1.71it/s]Extractor Estimating: 239it [02:09,  1.70it/s]Extractor Estimating: 240it [02:09,  1.67it/s]Extractor Estimating: 241it [02:10,  1.70it/s]Extractor Estimating: 242it [02:11,  1.70it/s]Extractor Estimating: 243it [02:11,  1.67it/s]Extractor Estimating: 244it [02:12,  1.65it/s]Extractor Estimating: 245it [02:13,  1.63it/s]Extractor Estimating: 246it [02:13,  1.62it/s]Extractor Estimating: 247it [02:14,  1.60it/s]Extractor Estimating: 248it [02:14,  1.61it/s]Extractor Estimating: 249it [02:15,  1.58it/s]Extractor Estimating: 250it [02:16,  1.57it/s]Extractor Estimating: 251it [02:16,  1.52it/s]Extractor Estimating: 252it [02:17,  1.50it/s]Extractor Estimating: 253it [02:18,  1.48it/s]Extractor Estimating: 254it [02:19,  1.45it/s]Extractor Estimating: 255it [02:19,  1.47it/s]Extractor Estimating: 256it [02:20,  1.47it/s]Extractor Estimating: 257it [02:21,  1.44it/s]Extractor Estimating: 258it [02:21,  1.45it/s]Extractor Estimating: 259it [02:22,  1.46it/s]Extractor Estimating: 260it [02:23,  1.43it/s]Extractor Estimating: 261it [02:23,  1.43it/s]Extractor Estimating: 262it [02:24,  1.45it/s]Extractor Estimating: 263it [02:25,  1.48it/s]Extractor Estimating: 264it [02:25,  1.44it/s]Extractor Estimating: 265it [02:26,  1.47it/s]Extractor Estimating: 266it [02:27,  1.48it/s]Extractor Estimating: 267it [02:27,  1.48it/s]Extractor Estimating: 268it [02:28,  1.52it/s]Extractor Estimating: 269it [02:29,  1.52it/s]Extractor Estimating: 270it [02:29,  1.54it/s]Extractor Estimating: 271it [02:30,  1.34it/s]Extractor Estimating: 272it [02:31,  1.37it/s]Extractor Estimating: 273it [02:32,  1.42it/s]Extractor Estimating: 274it [02:32,  1.46it/s]Extractor Estimating: 275it [02:33,  1.55it/s]Extractor Estimating: 276it [02:33,  1.59it/s]Extractor Estimating: 277it [02:34,  1.59it/s]Extractor Estimating: 278it [02:35,  1.63it/s]Extractor Estimating: 279it [02:35,  1.69it/s]Extractor Estimating: 280it [02:36,  1.68it/s]Extractor Estimating: 281it [02:36,  1.67it/s]Extractor Estimating: 282it [02:37,  1.73it/s]Extractor Estimating: 283it [02:37,  1.74it/s]Extractor Estimating: 284it [02:38,  1.75it/s]Extractor Estimating: 285it [02:39,  1.75it/s]Extractor Estimating: 286it [02:39,  1.69it/s]Extractor Estimating: 287it [02:40,  1.75it/s]Extractor Estimating: 288it [02:40,  1.77it/s]Extractor Estimating: 289it [02:41,  1.76it/s]Extractor Estimating: 290it [02:41,  1.80it/s]Extractor Estimating: 291it [02:42,  1.78it/s]Extractor Estimating: 292it [02:43,  1.76it/s]Extractor Estimating: 293it [02:43,  1.78it/s]Extractor Estimating: 294it [02:44,  1.79it/s]Extractor Estimating: 295it [02:44,  1.82it/s]Extractor Estimating: 296it [02:45,  1.79it/s]Extractor Estimating: 297it [02:45,  1.81it/s]Extractor Estimating: 298it [02:46,  1.79it/s]Extractor Estimating: 299it [02:46,  1.77it/s]Extractor Estimating: 300it [02:47,  1.90it/s]Extractor Estimating: 301it [02:47,  1.93it/s]Extractor Estimating: 302it [02:48,  2.06it/s]Extractor Estimating: 303it [02:48,  2.14it/s]Extractor Estimating: 304it [02:49,  2.17it/s]Extractor Estimating: 305it [02:49,  2.27it/s]Extractor Estimating: 306it [02:50,  2.24it/s]Extractor Estimating: 307it [02:50,  2.18it/s]Extractor Estimating: 308it [02:50,  2.21it/s]Extractor Estimating: 309it [02:51,  2.20it/s]Extractor Estimating: 310it [02:51,  2.25it/s]Extractor Estimating: 311it [02:52,  2.19it/s]Extractor Estimating: 312it [02:52,  2.20it/s]Extractor Estimating: 313it [02:53,  2.27it/s]Extractor Estimating: 314it [02:53,  2.32it/s]Extractor Estimating: 315it [02:54,  2.32it/s]Extractor Estimating: 316it [02:54,  2.28it/s]Extractor Estimating: 317it [02:54,  2.38it/s]Extractor Estimating: 318it [02:55,  2.28it/s]Extractor Estimating: 319it [02:55,  2.25it/s]Extractor Estimating: 320it [02:56,  2.26it/s]Extractor Estimating: 321it [02:56,  2.22it/s]Extractor Estimating: 322it [02:57,  2.30it/s]Extractor Estimating: 323it [02:57,  2.39it/s]Extractor Estimating: 324it [02:58,  2.20it/s]Extractor Estimating: 325it [02:58,  1.99it/s]Extractor Estimating: 326it [02:59,  1.84it/s]Extractor Estimating: 327it [02:59,  1.76it/s]Extractor Estimating: 328it [03:00,  1.69it/s]Extractor Estimating: 329it [03:01,  1.73it/s]Extractor Estimating: 330it [03:01,  1.71it/s]Extractor Estimating: 331it [03:02,  1.65it/s]Extractor Estimating: 332it [03:02,  1.63it/s]Extractor Estimating: 333it [03:03,  1.64it/s]Extractor Estimating: 334it [03:04,  1.60it/s]Extractor Estimating: 335it [03:04,  1.65it/s]Extractor Estimating: 336it [03:05,  1.58it/s]Extractor Estimating: 337it [03:06,  1.62it/s]Extractor Estimating: 338it [03:06,  1.60it/s]Extractor Estimating: 339it [03:07,  1.59it/s]Extractor Estimating: 340it [03:07,  1.61it/s]Extractor Estimating: 341it [03:08,  1.60it/s]Extractor Estimating: 342it [03:09,  1.59it/s]Extractor Estimating: 343it [03:09,  1.56it/s]Extractor Estimating: 344it [03:10,  1.56it/s]Extractor Estimating: 345it [03:11,  1.59it/s]Extractor Estimating: 346it [03:11,  1.63it/s]Extractor Estimating: 347it [03:12,  1.66it/s]Extractor Estimating: 348it [03:12,  1.68it/s]Extractor Estimating: 349it [03:13,  1.66it/s]Extractor Estimating: 350it [03:14,  1.73it/s]Extractor Estimating: 351it [03:14,  1.76it/s]Extractor Estimating: 352it [03:15,  1.84it/s]Extractor Estimating: 353it [03:15,  1.84it/s]Extractor Estimating: 354it [03:16,  1.82it/s]Extractor Estimating: 355it [03:16,  1.88it/s]Extractor Estimating: 356it [03:17,  1.88it/s]Extractor Estimating: 357it [03:17,  1.88it/s]Extractor Estimating: 358it [03:18,  1.89it/s]Extractor Estimating: 359it [03:18,  1.87it/s]Extractor Estimating: 360it [03:19,  1.87it/s]Extractor Estimating: 361it [03:19,  1.88it/s]Extractor Estimating: 362it [03:20,  1.89it/s]Extractor Estimating: 363it [03:21,  1.69it/s]Extractor Estimating: 364it [03:21,  1.72it/s]Extractor Estimating: 365it [03:22,  1.84it/s]Extractor Estimating: 366it [03:22,  1.83it/s]Extractor Estimating: 367it [03:23,  1.87it/s]Extractor Estimating: 368it [03:23,  1.86it/s]Extractor Estimating: 369it [03:24,  1.84it/s]Extractor Estimating: 370it [03:24,  1.82it/s]Extractor Estimating: 371it [03:25,  1.79it/s]Extractor Estimating: 372it [03:25,  1.78it/s]Extractor Estimating: 373it [03:26,  1.79it/s]Extractor Estimating: 374it [03:27,  1.81it/s]Extractor Estimating: 375it [03:27,  1.76it/s]Extractor Estimating: 376it [03:28,  1.71it/s]Extractor Estimating: 377it [03:28,  1.71it/s]Extractor Estimating: 378it [03:29,  1.69it/s]Extractor Estimating: 379it [03:30,  1.68it/s]Extractor Estimating: 380it [03:30,  1.62it/s]Extractor Estimating: 381it [03:31,  1.60it/s]Extractor Estimating: 382it [03:32,  1.60it/s]Extractor Estimating: 383it [03:32,  1.57it/s]Extractor Estimating: 384it [03:33,  1.58it/s]Extractor Estimating: 385it [03:33,  1.59it/s]Extractor Estimating: 386it [03:34,  1.63it/s]Extractor Estimating: 387it [03:35,  1.64it/s]Extractor Estimating: 388it [03:35,  1.66it/s]Extractor Estimating: 389it [03:36,  1.66it/s]Extractor Estimating: 390it [03:36,  1.67it/s]Extractor Estimating: 391it [03:37,  1.69it/s]Extractor Estimating: 392it [03:38,  1.62it/s]Extractor Estimating: 393it [03:38,  1.69it/s]Extractor Estimating: 394it [03:39,  1.71it/s]Extractor Estimating: 395it [03:39,  1.70it/s]Extractor Estimating: 396it [03:40,  1.69it/s]Extractor Estimating: 397it [03:40,  1.75it/s]Extractor Estimating: 398it [03:41,  1.76it/s]Extractor Estimating: 399it [03:42,  1.77it/s]Extractor Estimating: 400it [03:42,  1.74it/s]Extractor Estimating: 401it [03:43,  1.70it/s]Extractor Estimating: 402it [03:43,  1.65it/s]Extractor Estimating: 403it [03:44,  1.65it/s]Extractor Estimating: 404it [03:45,  1.69it/s]Extractor Estimating: 405it [03:45,  1.69it/s]Extractor Estimating: 406it [03:46,  1.67it/s]Extractor Estimating: 407it [03:46,  1.65it/s]Extractor Estimating: 408it [03:47,  1.71it/s]Extractor Estimating: 409it [03:48,  1.73it/s]Extractor Estimating: 410it [03:48,  1.69it/s]Extractor Estimating: 411it [03:49,  1.69it/s]Extractor Estimating: 412it [03:49,  1.69it/s]Extractor Estimating: 413it [03:50,  1.70it/s]Extractor Estimating: 414it [03:50,  1.75it/s]Extractor Estimating: 415it [03:51,  1.76it/s]Extractor Estimating: 416it [03:52,  1.74it/s]Extractor Estimating: 417it [03:52,  1.72it/s]Extractor Estimating: 418it [03:53,  1.70it/s]Extractor Estimating: 419it [03:53,  1.73it/s]Extractor Estimating: 420it [03:54,  1.72it/s]Extractor Estimating: 421it [03:55,  1.67it/s]Extractor Estimating: 422it [03:55,  1.73it/s]Extractor Estimating: 423it [03:56,  1.74it/s]Extractor Estimating: 424it [03:56,  1.72it/s]Extractor Estimating: 425it [03:57,  1.66it/s]Extractor Estimating: 426it [03:58,  1.67it/s]Extractor Estimating: 427it [03:58,  1.66it/s]Extractor Estimating: 428it [03:59,  1.65it/s]Extractor Estimating: 429it [03:59,  1.64it/s]Extractor Estimating: 430it [04:00,  1.62it/s]Extractor Estimating: 431it [04:01,  1.67it/s]Extractor Estimating: 432it [04:01,  1.70it/s]Extractor Estimating: 433it [04:02,  1.66it/s]Extractor Estimating: 434it [04:02,  1.70it/s]Extractor Estimating: 435it [04:03,  1.68it/s]Extractor Estimating: 436it [04:03,  1.68it/s]Extractor Estimating: 437it [04:04,  1.66it/s]Extractor Estimating: 438it [04:05,  1.65it/s]Extractor Estimating: 439it [04:05,  1.62it/s]Extractor Estimating: 440it [04:06,  1.65it/s]Extractor Estimating: 441it [04:07,  1.64it/s]Extractor Estimating: 442it [04:07,  1.64it/s]Extractor Estimating: 443it [04:08,  1.65it/s]Extractor Estimating: 444it [04:08,  1.68it/s]Extractor Estimating: 445it [04:09,  1.72it/s]Extractor Estimating: 446it [04:09,  1.72it/s]Extractor Estimating: 447it [04:10,  1.72it/s]Extractor Estimating: 448it [04:11,  1.63it/s]Extractor Estimating: 449it [04:11,  1.66it/s]Extractor Estimating: 450it [04:12,  1.73it/s]Extractor Estimating: 451it [04:12,  1.79it/s]Extractor Estimating: 452it [04:13,  1.82it/s]Extractor Estimating: 453it [04:13,  1.82it/s]Extractor Estimating: 454it [04:14,  1.90it/s]Extractor Estimating: 455it [04:15,  1.83it/s]Extractor Estimating: 456it [04:15,  1.89it/s]Extractor Estimating: 457it [04:16,  1.92it/s]Extractor Estimating: 458it [04:16,  1.96it/s]Extractor Estimating: 459it [04:17,  1.93it/s]Extractor Estimating: 460it [04:17,  1.96it/s]Extractor Estimating: 461it [04:18,  2.00it/s]Extractor Estimating: 462it [04:18,  1.98it/s]Extractor Estimating: 463it [04:19,  1.98it/s]Extractor Estimating: 464it [04:19,  1.96it/s]Extractor Estimating: 465it [04:20,  2.00it/s]Extractor Estimating: 466it [04:20,  2.06it/s]Extractor Estimating: 467it [04:20,  2.04it/s]Extractor Estimating: 468it [04:21,  2.00it/s]Extractor Estimating: 469it [04:21,  2.01it/s]Extractor Estimating: 470it [04:22,  1.98it/s]Extractor Estimating: 471it [04:23,  1.95it/s]Extractor Estimating: 472it [04:23,  1.93it/s]Extractor Estimating: 473it [04:24,  1.98it/s]Extractor Estimating: 474it [04:24,  1.94it/s]Extractor Estimating: 475it [04:25,  1.92it/s]Extractor Estimating: 476it [04:25,  1.85it/s]Extractor Estimating: 477it [04:26,  1.89it/s]Extractor Estimating: 478it [04:26,  1.85it/s]Extractor Estimating: 479it [04:27,  1.93it/s]Extractor Estimating: 480it [04:27,  1.76it/s]Extractor Estimating: 481it [04:28,  1.79it/s]Extractor Estimating: 482it [04:28,  1.81it/s]Extractor Estimating: 483it [04:29,  1.87it/s]Extractor Estimating: 484it [04:30,  1.85it/s]Extractor Estimating: 485it [04:30,  1.87it/s]Extractor Estimating: 486it [04:31,  1.91it/s]Extractor Estimating: 487it [04:31,  1.96it/s]Extractor Estimating: 488it [04:32,  1.97it/s]Extractor Estimating: 489it [04:32,  1.93it/s]Extractor Estimating: 490it [04:33,  1.86it/s]Extractor Estimating: 491it [04:33,  1.91it/s]Extractor Estimating: 492it [04:34,  1.86it/s]Extractor Estimating: 493it [04:34,  1.89it/s]Extractor Estimating: 494it [04:35,  1.89it/s]Extractor Estimating: 495it [04:35,  1.85it/s]Extractor Estimating: 496it [04:36,  1.87it/s]Extractor Estimating: 497it [04:36,  1.95it/s]Extractor Estimating: 498it [04:37,  1.91it/s]Extractor Estimating: 499it [04:37,  2.09it/s]Extractor Estimating: 499it [04:37,  1.80it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:48,890 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:48,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:48,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:48,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:48,892 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 06:25:49,491 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 06:25:49,491 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:25:50,041 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 06:25:51,102 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:25:51,104 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:54,046 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:54,048 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:54,048 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:54,048 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 06:25:54,048 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 06:25:54,696 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 06:25:54,698 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 06:25:55,282 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 06:25:55,468 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 06:25:55,468 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-29 09:08:42,826 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-29 09:08:42,830 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': False, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 10000, 'num_train': 0}
num of filtered data: 9967 mean pseudo reward: 0.9573533536480867
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl'}
train vocab size: 15029
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 15129, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=15129, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.956, loss:329.7903
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.957, loss:320.4485
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 0.941, loss:291.6316
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 0.937, loss:306.5334
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 84, avg_time 0.933, loss:282.7758
>> valid entity prec:0.5800, rec:0.5191, f1:0.5478
>> valid relation prec:0.2200, rec:0.1155, f1:0.1515
>> valid relation with NER prec:0.2200, rec:0.1155, f1:0.1515
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 184, avg_time 2.137, loss:274.4289
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 284, avg_time 0.954, loss:308.1453
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 384, avg_time 0.933, loss:294.1731
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 68, avg_time 0.940, loss:279.2098
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 168, avg_time 0.935, loss:307.3047
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5457, rec:0.5032, f1:0.5236
>> valid relation prec:0.1902, rec:0.1042, f1:0.1347
>> valid relation with NER prec:0.1902, rec:0.1042, f1:0.1347
g_step 1100, step 268, avg_time 2.113, loss:296.7543
g_step 1200, step 368, avg_time 0.930, loss:299.7303
g_step 1300, step 52, avg_time 0.943, loss:280.7874
g_step 1400, step 152, avg_time 0.953, loss:285.5545
g_step 1500, step 252, avg_time 0.946, loss:268.6096
>> valid entity prec:0.5626, rec:0.5193, f1:0.5401
>> valid relation prec:0.1978, rec:0.0996, f1:0.1325
>> valid relation with NER prec:0.1978, rec:0.0996, f1:0.1325
g_step 1600, step 352, avg_time 2.116, loss:282.2642
g_step 1700, step 36, avg_time 0.951, loss:274.4382
g_step 1800, step 136, avg_time 0.934, loss:252.9101
g_step 1900, step 236, avg_time 0.936, loss:257.7691
g_step 2000, step 336, avg_time 0.950, loss:264.5595
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5757, rec:0.5528, f1:0.5640
>> valid relation prec:0.2077, rec:0.1048, f1:0.1393
>> valid relation with NER prec:0.2077, rec:0.1048, f1:0.1393
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 20, avg_time 2.130, loss:252.6747
g_step 2200, step 120, avg_time 0.941, loss:246.6660
g_step 2300, step 220, avg_time 0.945, loss:237.0499
g_step 2400, step 320, avg_time 0.939, loss:260.2026
g_step 2500, step 4, avg_time 0.937, loss:240.4252
>> valid entity prec:0.5855, rec:0.5069, f1:0.5434
>> valid relation prec:0.2101, rec:0.1089, f1:0.1434
>> valid relation with NER prec:0.2101, rec:0.1089, f1:0.1434
g_step 2600, step 104, avg_time 2.126, loss:221.6036
g_step 2700, step 204, avg_time 0.929, loss:230.2925
g_step 2800, step 304, avg_time 0.942, loss:236.5577
g_step 2900, step 404, avg_time 0.951, loss:237.2028
g_step 3000, step 88, avg_time 0.927, loss:219.8097
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5552, rec:0.5306, f1:0.5426
>> valid relation prec:0.1799, rec:0.1045, f1:0.1322
>> valid relation with NER prec:0.1799, rec:0.1045, f1:0.1322
g_step 3100, step 188, avg_time 2.110, loss:220.4675
g_step 3200, step 288, avg_time 0.947, loss:227.8105
g_step 3300, step 388, avg_time 0.962, loss:211.9663
g_step 3400, step 72, avg_time 0.931, loss:200.6293
g_step 3500, step 172, avg_time 0.946, loss:220.0013
>> valid entity prec:0.5505, rec:0.5644, f1:0.5573
>> valid relation prec:0.1737, rec:0.1001, f1:0.1270
>> valid relation with NER prec:0.1737, rec:0.1001, f1:0.1270
g_step 3600, step 272, avg_time 2.128, loss:213.7187
g_step 3700, step 372, avg_time 0.941, loss:221.3733
g_step 3800, step 56, avg_time 0.943, loss:209.6327
g_step 3900, step 156, avg_time 0.949, loss:188.1714
g_step 4000, step 256, avg_time 0.948, loss:206.7606
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5587, rec:0.5454, f1:0.5520
>> valid relation prec:0.1803, rec:0.1054, f1:0.1330
>> valid relation with NER prec:0.1803, rec:0.1054, f1:0.1330
g_step 4100, step 356, avg_time 2.116, loss:211.5832
g_step 4200, step 40, avg_time 0.919, loss:200.6623
g_step 4300, step 140, avg_time 0.944, loss:177.5576
g_step 4400, step 240, avg_time 0.943, loss:190.3868
g_step 4500, step 340, avg_time 0.955, loss:209.5069
>> valid entity prec:0.5772, rec:0.5287, f1:0.5519
>> valid relation prec:0.1936, rec:0.1033, f1:0.1347
>> valid relation with NER prec:0.1936, rec:0.1033, f1:0.1347
g_step 4600, step 24, avg_time 2.140, loss:203.2302
g_step 4700, step 124, avg_time 0.950, loss:177.8706
g_step 4800, step 224, avg_time 0.950, loss:183.5984
g_step 4900, step 324, avg_time 0.955, loss:196.0958
g_step 5000, step 8, avg_time 0.945, loss:192.6780
learning rate was adjusted to 0.0008
>> valid entity prec:0.5467, rec:0.5622, f1:0.5543
>> valid relation prec:0.1530, rec:0.1060, f1:0.1252
>> valid relation with NER prec:0.1530, rec:0.1060, f1:0.1252
g_step 5100, step 108, avg_time 2.141, loss:179.4551
g_step 5200, step 208, avg_time 0.956, loss:178.0684
g_step 5300, step 308, avg_time 0.952, loss:181.3569
g_step 5400, step 408, avg_time 0.947, loss:194.2844
g_step 5500, step 92, avg_time 0.951, loss:154.1238
>> valid entity prec:0.5575, rec:0.5764, f1:0.5668
>> valid relation prec:0.1669, rec:0.1030, f1:0.1274
>> valid relation with NER prec:0.1669, rec:0.1030, f1:0.1274
new max entity f1 on valid!
g_step 5600, step 192, avg_time 2.124, loss:165.9373
g_step 5700, step 292, avg_time 0.950, loss:182.9846
g_step 5800, step 392, avg_time 0.960, loss:188.2695
g_step 5900, step 76, avg_time 0.965, loss:165.5016
g_step 6000, step 176, avg_time 0.954, loss:167.2451
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5504, rec:0.5166, f1:0.5329
>> valid relation prec:0.1663, rec:0.1028, f1:0.1270
>> valid relation with NER prec:0.1663, rec:0.1028, f1:0.1270
g_step 6100, step 276, avg_time 2.135, loss:176.6734
g_step 6200, step 376, avg_time 0.944, loss:163.5920
g_step 6300, step 60, avg_time 0.957, loss:169.4319
g_step 6400, step 160, avg_time 0.943, loss:159.9507
g_step 6500, step 260, avg_time 0.949, loss:158.2997
>> valid entity prec:0.5618, rec:0.5473, f1:0.5545
>> valid relation prec:0.1672, rec:0.1068, f1:0.1304
>> valid relation with NER prec:0.1672, rec:0.1068, f1:0.1304
g_step 6600, step 360, avg_time 2.136, loss:157.5047
g_step 6700, step 44, avg_time 0.950, loss:161.7035
g_step 6800, step 144, avg_time 0.942, loss:150.9021
g_step 6900, step 244, avg_time 0.961, loss:147.7450
g_step 7000, step 344, avg_time 0.946, loss:180.0224
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5528, rec:0.5336, f1:0.5430
>> valid relation prec:0.1704, rec:0.0940, f1:0.1212
>> valid relation with NER prec:0.1704, rec:0.0940, f1:0.1212
g_step 7100, step 28, avg_time 2.145, loss:155.7684
g_step 7200, step 128, avg_time 0.944, loss:150.6105
g_step 7300, step 228, avg_time 0.958, loss:142.1148
g_step 7400, step 328, avg_time 0.947, loss:150.1780
g_step 7500, step 12, avg_time 0.959, loss:159.0430
>> valid entity prec:0.5541, rec:0.5649, f1:0.5595
>> valid relation prec:0.1598, rec:0.1089, f1:0.1295
>> valid relation with NER prec:0.1598, rec:0.1089, f1:0.1295
g_step 7600, step 112, avg_time 2.129, loss:149.6196
g_step 7700, step 212, avg_time 0.947, loss:156.6025
g_step 7800, step 312, avg_time 0.953, loss:149.1121
g_step 7900, step 412, avg_time 0.946, loss:156.2787
g_step 8000, step 96, avg_time 0.940, loss:133.0199
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5558, rec:0.5201, f1:0.5373
>> valid relation prec:0.1591, rec:0.1042, f1:0.1259
>> valid relation with NER prec:0.1591, rec:0.1042, f1:0.1259
g_step 8100, step 196, avg_time 2.152, loss:145.8379
g_step 8200, step 296, avg_time 0.934, loss:152.9989
g_step 8300, step 396, avg_time 0.959, loss:140.3994
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/29/2023 09:08:42 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/29/2023 09:08:42 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug29_09-08-42_ctolab10.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/29/2023 09:08:43 - WARNING - datasets.builder -   Using custom data configuration default-8a8f47de3ffb8050
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8a8f47de3ffb8050/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-29 09:08:44,107 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:08:44,108 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-29 09:08:44,108 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-29 09:08:44,109 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-29 09:08:44,120 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:08:44,126 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:08:44,126 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:08:44,126 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:08:44,126 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:08:44,126 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-29 09:08:44,126 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-29 09:08:44,277 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-29 09:08:47,233 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-29 09:08:47,240 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8a8f47de3ffb8050/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:02,  3.42ba/s] 20%|██        | 2/10 [00:00<00:01,  4.30ba/s] 30%|███       | 3/10 [00:00<00:01,  3.66ba/s] 40%|████      | 4/10 [00:00<00:01,  4.15ba/s] 50%|█████     | 5/10 [00:01<00:01,  4.49ba/s] 60%|██████    | 6/10 [00:01<00:00,  4.73ba/s] 70%|███████   | 7/10 [00:01<00:00,  4.90ba/s] 80%|████████  | 8/10 [00:01<00:00,  5.01ba/s] 90%|█████████ | 9/10 [00:01<00:00,  5.09ba/s]100%|██████████| 10/10 [00:02<00:00,  5.16ba/s]100%|██████████| 10/10 [00:02<00:00,  4.69ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.41ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.60ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  4.67ba/s]100%|██████████| 4/4 [00:00<00:00,  5.37ba/s]
  0%|          | 0/10 [00:00<?, ?ba/s] 10%|█         | 1/10 [00:00<00:00,  9.74ba/s] 30%|███       | 3/10 [00:00<00:00, 10.91ba/s] 50%|█████     | 5/10 [00:00<00:00, 11.11ba/s] 70%|███████   | 7/10 [00:00<00:00, 11.24ba/s] 90%|█████████ | 9/10 [00:00<00:00, 11.19ba/s]100%|██████████| 10/10 [00:00<00:00, 11.16ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  9.81ba/s] 75%|███████▌  | 3/4 [00:00<00:00, 10.80ba/s]100%|██████████| 4/4 [00:00<00:00, 12.38ba/s]
[INFO|trainer.py:414] 2023-08-29 09:08:51,743 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-29 09:08:51,750 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-29 09:08:51,750 >>   Num examples = 10000
[INFO|trainer.py:1149] 2023-08-29 09:08:51,750 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-29 09:08:51,750 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-29 09:08:51,750 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-29 09:08:51,750 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-29 09:08:51,750 >>   Total optimization steps = 780
  0%|          | 0/780 [00:00<?, ?it/s]  0%|          | 1/780 [00:00<03:50,  3.38it/s]  0%|          | 2/780 [00:00<03:45,  3.44it/s]  0%|          | 3/780 [00:00<03:44,  3.46it/s]  1%|          | 4/780 [00:01<03:43,  3.48it/s]  1%|          | 5/780 [00:01<03:42,  3.48it/s]  1%|          | 6/780 [00:01<03:42,  3.48it/s]  1%|          | 7/780 [00:02<03:43,  3.46it/s]  1%|          | 8/780 [00:02<03:43,  3.45it/s]  1%|          | 9/780 [00:02<03:44,  3.44it/s]  1%|▏         | 10/780 [00:02<03:43,  3.44it/s]  1%|▏         | 11/780 [00:03<03:43,  3.43it/s]  2%|▏         | 12/780 [00:03<03:43,  3.43it/s]  2%|▏         | 13/780 [00:03<03:43,  3.43it/s]  2%|▏         | 14/780 [00:04<03:43,  3.42it/s]  2%|▏         | 15/780 [00:04<03:43,  3.42it/s]  2%|▏         | 16/780 [00:04<03:43,  3.42it/s]  2%|▏         | 17/780 [00:04<03:43,  3.42it/s]  2%|▏         | 18/780 [00:05<03:47,  3.35it/s]  2%|▏         | 19/780 [00:05<03:46,  3.37it/s]  3%|▎         | 20/780 [00:05<03:44,  3.38it/s]  3%|▎         | 21/780 [00:06<03:43,  3.40it/s]  3%|▎         | 22/780 [00:06<03:42,  3.41it/s]  3%|▎         | 23/780 [00:06<03:41,  3.41it/s]  3%|▎         | 24/780 [00:07<03:40,  3.43it/s]  3%|▎         | 25/780 [00:07<03:39,  3.44it/s]  3%|▎         | 26/780 [00:07<03:38,  3.45it/s]  3%|▎         | 27/780 [00:07<03:37,  3.46it/s]  4%|▎         | 28/780 [00:08<03:36,  3.47it/s]  4%|▎         | 29/780 [00:08<03:36,  3.46it/s]  4%|▍         | 30/780 [00:08<03:36,  3.47it/s]  4%|▍         | 31/780 [00:09<03:35,  3.47it/s]  4%|▍         | 32/780 [00:09<03:35,  3.47it/s]  4%|▍         | 33/780 [00:09<03:34,  3.48it/s]  4%|▍         | 34/780 [00:09<03:34,  3.48it/s]  4%|▍         | 35/780 [00:10<03:34,  3.48it/s]  5%|▍         | 36/780 [00:10<03:33,  3.48it/s]  5%|▍         | 37/780 [00:10<03:33,  3.48it/s]  5%|▍         | 38/780 [00:11<03:33,  3.47it/s]  5%|▌         | 39/780 [00:11<03:33,  3.48it/s]  5%|▌         | 40/780 [00:11<03:33,  3.47it/s]  5%|▌         | 41/780 [00:11<03:32,  3.47it/s]  5%|▌         | 42/780 [00:12<03:32,  3.47it/s]  6%|▌         | 43/780 [00:12<03:32,  3.48it/s]  6%|▌         | 44/780 [00:12<03:31,  3.48it/s]  6%|▌         | 45/780 [00:13<03:31,  3.48it/s]  6%|▌         | 46/780 [00:13<03:30,  3.48it/s]  6%|▌         | 47/780 [00:13<03:30,  3.48it/s]  6%|▌         | 48/780 [00:13<03:30,  3.48it/s]  6%|▋         | 49/780 [00:14<03:30,  3.48it/s]  6%|▋         | 50/780 [00:14<03:29,  3.48it/s]  7%|▋         | 51/780 [00:14<03:30,  3.47it/s]  7%|▋         | 52/780 [00:15<03:30,  3.46it/s]  7%|▋         | 53/780 [00:15<03:29,  3.47it/s]  7%|▋         | 54/780 [00:15<03:29,  3.47it/s]  7%|▋         | 55/780 [00:15<03:28,  3.47it/s]  7%|▋         | 56/780 [00:16<03:28,  3.47it/s]  7%|▋         | 57/780 [00:16<03:28,  3.48it/s]  7%|▋         | 58/780 [00:16<03:27,  3.48it/s]  8%|▊         | 59/780 [00:17<03:27,  3.48it/s]  8%|▊         | 60/780 [00:17<03:27,  3.47it/s]  8%|▊         | 61/780 [00:17<03:27,  3.47it/s]  8%|▊         | 62/780 [00:17<03:27,  3.47it/s]  8%|▊         | 63/780 [00:18<03:26,  3.47it/s]  8%|▊         | 64/780 [00:18<03:26,  3.47it/s]  8%|▊         | 65/780 [00:18<03:25,  3.47it/s]  8%|▊         | 66/780 [00:19<03:25,  3.48it/s]  9%|▊         | 67/780 [00:19<03:25,  3.47it/s]  9%|▊         | 68/780 [00:19<03:25,  3.47it/s]  9%|▉         | 69/780 [00:19<03:24,  3.47it/s]  9%|▉         | 70/780 [00:20<03:24,  3.47it/s]  9%|▉         | 71/780 [00:20<03:23,  3.48it/s]  9%|▉         | 72/780 [00:20<03:23,  3.48it/s]  9%|▉         | 73/780 [00:21<03:23,  3.47it/s]  9%|▉         | 74/780 [00:21<03:23,  3.47it/s] 10%|▉         | 75/780 [00:21<03:23,  3.47it/s] 10%|▉         | 76/780 [00:21<03:22,  3.48it/s] 10%|▉         | 77/780 [00:22<03:22,  3.47it/s] 10%|█         | 78/780 [00:22<03:21,  3.48it/s] 10%|█         | 79/780 [00:22<03:22,  3.46it/s] 10%|█         | 80/780 [00:23<03:21,  3.47it/s] 10%|█         | 81/780 [00:23<03:21,  3.47it/s] 11%|█         | 82/780 [00:23<03:20,  3.48it/s] 11%|█         | 83/780 [00:23<03:20,  3.47it/s] 11%|█         | 84/780 [00:24<03:20,  3.48it/s] 11%|█         | 85/780 [00:24<03:19,  3.48it/s] 11%|█         | 86/780 [00:24<03:19,  3.48it/s] 11%|█         | 87/780 [00:25<03:19,  3.48it/s] 11%|█▏        | 88/780 [00:25<03:18,  3.48it/s] 11%|█▏        | 89/780 [00:25<03:18,  3.48it/s] 12%|█▏        | 90/780 [00:26<03:19,  3.46it/s] 12%|█▏        | 91/780 [00:26<03:18,  3.47it/s] 12%|█▏        | 92/780 [00:26<03:18,  3.47it/s] 12%|█▏        | 93/780 [00:26<03:18,  3.46it/s] 12%|█▏        | 94/780 [00:27<03:18,  3.46it/s] 12%|█▏        | 95/780 [00:27<03:17,  3.46it/s] 12%|█▏        | 96/780 [00:27<03:17,  3.46it/s] 12%|█▏        | 97/780 [00:28<03:17,  3.46it/s] 13%|█▎        | 98/780 [00:28<03:16,  3.46it/s] 13%|█▎        | 99/780 [00:28<03:16,  3.47it/s] 13%|█▎        | 100/780 [00:28<03:15,  3.47it/s] 13%|█▎        | 101/780 [00:29<03:16,  3.46it/s] 13%|█▎        | 102/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 103/780 [00:29<03:15,  3.46it/s] 13%|█▎        | 104/780 [00:30<03:15,  3.47it/s] 13%|█▎        | 105/780 [00:30<03:14,  3.46it/s] 14%|█▎        | 106/780 [00:30<03:14,  3.47it/s] 14%|█▎        | 107/780 [00:30<03:13,  3.47it/s] 14%|█▍        | 108/780 [00:31<03:13,  3.47it/s] 14%|█▍        | 109/780 [00:31<03:13,  3.47it/s] 14%|█▍        | 110/780 [00:31<03:12,  3.47it/s] 14%|█▍        | 111/780 [00:32<03:12,  3.47it/s] 14%|█▍        | 112/780 [00:32<03:12,  3.46it/s] 14%|█▍        | 113/780 [00:32<03:12,  3.46it/s] 15%|█▍        | 114/780 [00:32<03:11,  3.47it/s] 15%|█▍        | 115/780 [00:33<03:11,  3.47it/s] 15%|█▍        | 116/780 [00:33<03:11,  3.47it/s] 15%|█▌        | 117/780 [00:33<03:10,  3.48it/s] 15%|█▌        | 118/780 [00:34<03:10,  3.48it/s] 15%|█▌        | 119/780 [00:34<03:10,  3.48it/s] 15%|█▌        | 120/780 [00:34<03:09,  3.48it/s] 16%|█▌        | 121/780 [00:34<03:09,  3.48it/s] 16%|█▌        | 122/780 [00:35<03:09,  3.48it/s] 16%|█▌        | 123/780 [00:35<03:09,  3.47it/s] 16%|█▌        | 124/780 [00:35<03:08,  3.47it/s] 16%|█▌        | 125/780 [00:36<03:08,  3.47it/s] 16%|█▌        | 126/780 [00:36<03:08,  3.47it/s] 16%|█▋        | 127/780 [00:36<03:08,  3.47it/s] 16%|█▋        | 128/780 [00:36<03:07,  3.47it/s] 17%|█▋        | 129/780 [00:37<03:07,  3.47it/s] 17%|█▋        | 130/780 [00:37<03:07,  3.47it/s] 17%|█▋        | 131/780 [00:37<03:06,  3.47it/s] 17%|█▋        | 132/780 [00:38<03:06,  3.48it/s] 17%|█▋        | 133/780 [00:38<03:06,  3.47it/s] 17%|█▋        | 134/780 [00:38<03:06,  3.46it/s] 17%|█▋        | 135/780 [00:38<03:06,  3.46it/s] 17%|█▋        | 136/780 [00:39<03:05,  3.47it/s] 18%|█▊        | 137/780 [00:39<03:05,  3.47it/s] 18%|█▊        | 138/780 [00:39<03:04,  3.48it/s] 18%|█▊        | 139/780 [00:40<03:04,  3.48it/s] 18%|█▊        | 140/780 [00:40<03:04,  3.48it/s] 18%|█▊        | 141/780 [00:40<03:03,  3.48it/s] 18%|█▊        | 142/780 [00:40<03:03,  3.47it/s] 18%|█▊        | 143/780 [00:41<03:03,  3.48it/s] 18%|█▊        | 144/780 [00:41<03:02,  3.48it/s] 19%|█▊        | 145/780 [00:41<03:03,  3.46it/s] 19%|█▊        | 146/780 [00:42<03:03,  3.46it/s] 19%|█▉        | 147/780 [00:42<03:02,  3.46it/s] 19%|█▉        | 148/780 [00:42<03:02,  3.47it/s] 19%|█▉        | 149/780 [00:43<03:01,  3.47it/s] 19%|█▉        | 150/780 [00:43<03:01,  3.47it/s] 19%|█▉        | 151/780 [00:43<03:00,  3.48it/s] 19%|█▉        | 152/780 [00:43<03:00,  3.47it/s] 20%|█▉        | 153/780 [00:44<03:00,  3.48it/s] 20%|█▉        | 154/780 [00:44<03:00,  3.47it/s] 20%|█▉        | 155/780 [00:44<03:00,  3.47it/s] 20%|██        | 156/780 [00:45<03:00,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 09:09:36,822 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:09:36,822 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 09:09:36,822 >>   Batch size = 8

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.57it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.26it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.08it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.32it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.68it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.25it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.01it/s][A
 10%|▉         | 42/431 [00:00<00:08, 44.94it/s][A
 11%|█         | 47/431 [00:01<00:08, 44.90it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.03it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.27it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.29it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.31it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.10it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 44.95it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.79it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.69it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.78it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 44.90it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.05it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.14it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.23it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.13it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 44.98it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.85it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.78it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.89it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 44.95it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.05it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.05it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.26it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.18it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 44.97it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.86it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.84it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.86it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 44.93it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 44.93it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.01it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.03it/s][A
 48%|████▊     | 207/431 [00:04<00:05, 44.75it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 44.76it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.70it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.77it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.75it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 44.91it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 44.93it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.05it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 44.95it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 44.93it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 44.98it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.92it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.92it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.94it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.11it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.07it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.14it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.09it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 44.98it/s][A
 70%|███████   | 302/431 [00:06<00:02, 44.89it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.77it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.87it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.90it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 44.96it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.06it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.17it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.13it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.09it/s][A
 81%|████████  | 347/431 [00:07<00:01, 44.96it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 44.93it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.88it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.94it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.95it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.02it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.10it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.06it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.07it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.04it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 44.96it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 44.91it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.81it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.03it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.03it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.11it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.08it/s][A                                                 
                                                 [A 20%|██        | 156/780 [00:54<03:00,  3.45it/s]
100%|██████████| 431/431 [00:09<00:00, 45.08it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:09:46,482 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-29 09:09:46,508 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:09:48,448 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:09:48,462 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:09:48,474 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156/special_tokens_map.json
 20%|██        | 157/780 [01:00<50:20,  4.85s/it] 20%|██        | 158/780 [01:00<36:05,  3.48s/it] 20%|██        | 159/780 [01:01<26:07,  2.52s/it] 21%|██        | 160/780 [01:01<19:10,  1.86s/it] 21%|██        | 161/780 [01:01<14:18,  1.39s/it] 21%|██        | 162/780 [01:01<10:53,  1.06s/it] 21%|██        | 163/780 [01:02<08:31,  1.21it/s] 21%|██        | 164/780 [01:02<06:51,  1.50it/s] 21%|██        | 165/780 [01:02<05:41,  1.80it/s] 21%|██▏       | 166/780 [01:03<04:52,  2.10it/s] 21%|██▏       | 167/780 [01:03<04:17,  2.38it/s] 22%|██▏       | 168/780 [01:03<03:53,  2.62it/s] 22%|██▏       | 169/780 [01:04<03:37,  2.81it/s] 22%|██▏       | 170/780 [01:04<03:25,  2.97it/s] 22%|██▏       | 171/780 [01:04<03:16,  3.10it/s] 22%|██▏       | 172/780 [01:04<03:09,  3.20it/s] 22%|██▏       | 173/780 [01:05<03:05,  3.28it/s] 22%|██▏       | 174/780 [01:05<03:01,  3.34it/s] 22%|██▏       | 175/780 [01:05<02:59,  3.38it/s] 23%|██▎       | 176/780 [01:06<02:57,  3.40it/s] 23%|██▎       | 177/780 [01:06<02:56,  3.42it/s] 23%|██▎       | 178/780 [01:06<02:54,  3.44it/s] 23%|██▎       | 179/780 [01:06<02:54,  3.45it/s] 23%|██▎       | 180/780 [01:07<02:53,  3.45it/s] 23%|██▎       | 181/780 [01:07<02:53,  3.46it/s] 23%|██▎       | 182/780 [01:07<02:52,  3.47it/s] 23%|██▎       | 183/780 [01:08<02:51,  3.47it/s] 24%|██▎       | 184/780 [01:08<02:51,  3.47it/s] 24%|██▎       | 185/780 [01:08<02:51,  3.47it/s] 24%|██▍       | 186/780 [01:08<02:50,  3.47it/s] 24%|██▍       | 187/780 [01:09<02:50,  3.47it/s] 24%|██▍       | 188/780 [01:09<02:50,  3.48it/s] 24%|██▍       | 189/780 [01:09<02:49,  3.48it/s] 24%|██▍       | 190/780 [01:10<02:49,  3.48it/s] 24%|██▍       | 191/780 [01:10<02:50,  3.46it/s] 25%|██▍       | 192/780 [01:10<02:49,  3.47it/s] 25%|██▍       | 193/780 [01:10<02:49,  3.47it/s] 25%|██▍       | 194/780 [01:11<02:48,  3.47it/s] 25%|██▌       | 195/780 [01:11<02:48,  3.48it/s] 25%|██▌       | 196/780 [01:11<02:48,  3.47it/s] 25%|██▌       | 197/780 [01:12<02:48,  3.47it/s] 25%|██▌       | 198/780 [01:12<02:48,  3.46it/s] 26%|██▌       | 199/780 [01:12<02:47,  3.46it/s] 26%|██▌       | 200/780 [01:12<02:47,  3.46it/s] 26%|██▌       | 201/780 [01:13<02:47,  3.46it/s] 26%|██▌       | 202/780 [01:13<02:50,  3.40it/s] 26%|██▌       | 203/780 [01:13<02:48,  3.42it/s] 26%|██▌       | 204/780 [01:14<02:47,  3.43it/s] 26%|██▋       | 205/780 [01:14<02:46,  3.45it/s] 26%|██▋       | 206/780 [01:14<02:46,  3.46it/s] 27%|██▋       | 207/780 [01:14<02:45,  3.46it/s] 27%|██▋       | 208/780 [01:15<02:45,  3.47it/s] 27%|██▋       | 209/780 [01:15<02:44,  3.47it/s] 27%|██▋       | 210/780 [01:15<02:44,  3.47it/s] 27%|██▋       | 211/780 [01:16<02:43,  3.47it/s] 27%|██▋       | 212/780 [01:16<02:43,  3.48it/s] 27%|██▋       | 213/780 [01:16<02:43,  3.46it/s] 27%|██▋       | 214/780 [01:17<02:43,  3.46it/s] 28%|██▊       | 215/780 [01:17<02:42,  3.47it/s] 28%|██▊       | 216/780 [01:17<02:42,  3.47it/s] 28%|██▊       | 217/780 [01:17<02:42,  3.47it/s] 28%|██▊       | 218/780 [01:18<02:41,  3.47it/s] 28%|██▊       | 219/780 [01:18<02:41,  3.47it/s] 28%|██▊       | 220/780 [01:18<02:41,  3.48it/s] 28%|██▊       | 221/780 [01:19<02:40,  3.47it/s] 28%|██▊       | 222/780 [01:19<02:40,  3.47it/s] 29%|██▊       | 223/780 [01:19<02:40,  3.47it/s] 29%|██▊       | 224/780 [01:19<02:41,  3.45it/s] 29%|██▉       | 225/780 [01:20<02:40,  3.45it/s] 29%|██▉       | 226/780 [01:20<02:40,  3.45it/s] 29%|██▉       | 227/780 [01:20<02:40,  3.45it/s] 29%|██▉       | 228/780 [01:21<02:39,  3.46it/s] 29%|██▉       | 229/780 [01:21<02:39,  3.46it/s] 29%|██▉       | 230/780 [01:21<02:38,  3.47it/s] 30%|██▉       | 231/780 [01:21<02:38,  3.47it/s] 30%|██▉       | 232/780 [01:22<02:37,  3.47it/s] 30%|██▉       | 233/780 [01:22<02:37,  3.47it/s] 30%|███       | 234/780 [01:22<02:37,  3.47it/s] 30%|███       | 235/780 [01:23<02:37,  3.45it/s] 30%|███       | 236/780 [01:23<02:37,  3.46it/s] 30%|███       | 237/780 [01:23<02:36,  3.47it/s] 31%|███       | 238/780 [01:23<02:36,  3.47it/s] 31%|███       | 239/780 [01:24<02:35,  3.47it/s] 31%|███       | 240/780 [01:24<02:35,  3.47it/s] 31%|███       | 241/780 [01:24<02:35,  3.47it/s] 31%|███       | 242/780 [01:25<02:35,  3.47it/s] 31%|███       | 243/780 [01:25<02:34,  3.48it/s] 31%|███▏      | 244/780 [01:25<02:34,  3.48it/s] 31%|███▏      | 245/780 [01:25<02:33,  3.48it/s] 32%|███▏      | 246/780 [01:26<02:33,  3.48it/s] 32%|███▏      | 247/780 [01:26<02:33,  3.48it/s] 32%|███▏      | 248/780 [01:26<02:33,  3.47it/s] 32%|███▏      | 249/780 [01:27<02:32,  3.47it/s] 32%|███▏      | 250/780 [01:27<02:32,  3.47it/s] 32%|███▏      | 251/780 [01:27<02:32,  3.47it/s] 32%|███▏      | 252/780 [01:27<02:32,  3.47it/s] 32%|███▏      | 253/780 [01:28<02:31,  3.48it/s] 33%|███▎      | 254/780 [01:28<02:31,  3.48it/s] 33%|███▎      | 255/780 [01:28<02:30,  3.48it/s] 33%|███▎      | 256/780 [01:29<02:30,  3.47it/s] 33%|███▎      | 257/780 [01:29<02:30,  3.47it/s] 33%|███▎      | 258/780 [01:29<02:30,  3.47it/s] 33%|███▎      | 259/780 [01:29<02:29,  3.48it/s] 33%|███▎      | 260/780 [01:30<02:29,  3.47it/s] 33%|███▎      | 261/780 [01:30<02:30,  3.46it/s] 34%|███▎      | 262/780 [01:30<02:29,  3.46it/s] 34%|███▎      | 263/780 [01:31<02:29,  3.47it/s] 34%|███▍      | 264/780 [01:31<02:28,  3.47it/s] 34%|███▍      | 265/780 [01:31<02:28,  3.47it/s] 34%|███▍      | 266/780 [01:32<02:27,  3.47it/s] 34%|███▍      | 267/780 [01:32<02:27,  3.48it/s] 34%|███▍      | 268/780 [01:32<02:27,  3.47it/s] 34%|███▍      | 269/780 [01:32<02:27,  3.47it/s] 35%|███▍      | 270/780 [01:33<02:26,  3.48it/s] 35%|███▍      | 271/780 [01:33<02:26,  3.48it/s] 35%|███▍      | 272/780 [01:33<02:26,  3.46it/s] 35%|███▌      | 273/780 [01:34<02:26,  3.47it/s] 35%|███▌      | 274/780 [01:34<02:25,  3.47it/s] 35%|███▌      | 275/780 [01:34<02:25,  3.47it/s] 35%|███▌      | 276/780 [01:34<02:25,  3.46it/s] 36%|███▌      | 277/780 [01:35<02:25,  3.46it/s] 36%|███▌      | 278/780 [01:35<02:25,  3.46it/s] 36%|███▌      | 279/780 [01:35<02:25,  3.45it/s] 36%|███▌      | 280/780 [01:36<02:25,  3.44it/s] 36%|███▌      | 281/780 [01:36<02:24,  3.44it/s] 36%|███▌      | 282/780 [01:36<02:30,  3.31it/s] 36%|███▋      | 283/780 [01:36<02:28,  3.34it/s] 36%|███▋      | 284/780 [01:37<02:27,  3.36it/s] 37%|███▋      | 285/780 [01:37<02:26,  3.38it/s] 37%|███▋      | 286/780 [01:37<02:25,  3.40it/s] 37%|███▋      | 287/780 [01:38<02:24,  3.41it/s] 37%|███▋      | 288/780 [01:38<02:23,  3.42it/s] 37%|███▋      | 289/780 [01:38<02:23,  3.43it/s] 37%|███▋      | 290/780 [01:39<02:22,  3.43it/s] 37%|███▋      | 291/780 [01:39<02:22,  3.44it/s] 37%|███▋      | 292/780 [01:39<02:21,  3.44it/s] 38%|███▊      | 293/780 [01:39<02:21,  3.45it/s] 38%|███▊      | 294/780 [01:40<02:21,  3.44it/s] 38%|███▊      | 295/780 [01:40<02:20,  3.44it/s] 38%|███▊      | 296/780 [01:40<02:20,  3.45it/s] 38%|███▊      | 297/780 [01:41<02:19,  3.45it/s] 38%|███▊      | 298/780 [01:41<02:19,  3.46it/s] 38%|███▊      | 299/780 [01:41<02:18,  3.46it/s] 38%|███▊      | 300/780 [01:41<02:18,  3.46it/s] 39%|███▊      | 301/780 [01:42<02:18,  3.47it/s] 39%|███▊      | 302/780 [01:42<02:17,  3.47it/s] 39%|███▉      | 303/780 [01:42<02:17,  3.47it/s] 39%|███▉      | 304/780 [01:43<02:17,  3.47it/s] 39%|███▉      | 305/780 [01:43<02:17,  3.45it/s] 39%|███▉      | 306/780 [01:43<02:17,  3.46it/s] 39%|███▉      | 307/780 [01:43<02:16,  3.45it/s] 39%|███▉      | 308/780 [01:44<02:16,  3.46it/s] 40%|███▉      | 309/780 [01:44<02:15,  3.47it/s] 40%|███▉      | 310/780 [01:44<02:15,  3.46it/s] 40%|███▉      | 311/780 [01:45<02:15,  3.46it/s] 40%|████      | 312/780 [01:45<02:15,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 09:10:37,147 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:10:37,147 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 09:10:37,148 >>   Batch size = 8
{'eval_loss': 1.1127866506576538, 'eval_runtime': 9.614, 'eval_samples_per_second': 358.435, 'eval_steps_per_second': 44.83, 'epoch': 1.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.30it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.43it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.36it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.35it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.77it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.43it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.28it/s][A
 10%|▉         | 42/431 [00:00<00:08, 44.98it/s][A
 11%|█         | 47/431 [00:01<00:08, 44.82it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.35it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.34it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.15it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.10it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.00it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.95it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.83it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.78it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 44.99it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.10it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.25it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.19it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.15it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.04it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.90it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.88it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.93it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 44.95it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 45.09it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.20it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.12it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.09it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 44.99it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.89it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.88it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.91it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 44.99it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.10it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 45.12it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.08it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.09it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 44.98it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.90it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.89it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.97it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 44.98it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.10it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.18it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.12it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 44.95it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 44.95it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.87it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.82it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.86it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 44.86it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 44.92it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.16it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.08it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.14it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.02it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.90it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.87it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.92it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 45.03it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 44.97it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.08it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.06it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.19it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.06it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 45.00it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.92it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.88it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.95it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 44.87it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.11it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.14it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.01it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.03it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 44.93it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 44.90it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.90it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 45.05it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 44.80it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.02it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.09it/s][A                                                 
                                                 [A 40%|████      | 312/780 [01:55<02:15,  3.46it/s]
100%|██████████| 431/431 [00:09<00:00, 45.09it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:10:46,774 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-29 09:10:46,803 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:10:48,790 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:10:48,806 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:10:48,816 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312/special_tokens_map.json
 40%|████      | 313/780 [02:00<37:47,  4.85s/it] 40%|████      | 314/780 [02:01<27:04,  3.49s/it] 40%|████      | 315/780 [02:01<19:35,  2.53s/it] 41%|████      | 316/780 [02:01<14:21,  1.86s/it] 41%|████      | 317/780 [02:02<10:42,  1.39s/it] 41%|████      | 318/780 [02:02<08:09,  1.06s/it] 41%|████      | 319/780 [02:02<06:22,  1.20it/s] 41%|████      | 320/780 [02:02<05:07,  1.49it/s] 41%|████      | 321/780 [02:03<04:15,  1.80it/s] 41%|████▏     | 322/780 [02:03<03:38,  2.09it/s] 41%|████▏     | 323/780 [02:03<03:12,  2.37it/s] 42%|████▏     | 324/780 [02:04<02:54,  2.61it/s] 42%|████▏     | 325/780 [02:04<02:42,  2.80it/s] 42%|████▏     | 326/780 [02:04<02:33,  2.97it/s] 42%|████▏     | 327/780 [02:04<02:26,  3.09it/s] 42%|████▏     | 328/780 [02:05<02:22,  3.18it/s] 42%|████▏     | 329/780 [02:05<02:18,  3.25it/s] 42%|████▏     | 330/780 [02:05<02:16,  3.30it/s] 42%|████▏     | 331/780 [02:06<02:14,  3.33it/s] 43%|████▎     | 332/780 [02:06<02:13,  3.36it/s] 43%|████▎     | 333/780 [02:06<02:12,  3.37it/s] 43%|████▎     | 334/780 [02:07<02:11,  3.39it/s] 43%|████▎     | 335/780 [02:07<02:10,  3.40it/s] 43%|████▎     | 336/780 [02:07<02:11,  3.39it/s] 43%|████▎     | 337/780 [02:07<02:10,  3.40it/s] 43%|████▎     | 338/780 [02:08<02:09,  3.41it/s] 43%|████▎     | 339/780 [02:08<02:09,  3.41it/s] 44%|████▎     | 340/780 [02:08<02:08,  3.41it/s] 44%|████▎     | 341/780 [02:09<02:08,  3.41it/s] 44%|████▍     | 342/780 [02:09<02:08,  3.42it/s] 44%|████▍     | 343/780 [02:09<02:07,  3.42it/s] 44%|████▍     | 344/780 [02:09<02:07,  3.42it/s] 44%|████▍     | 345/780 [02:10<02:07,  3.42it/s] 44%|████▍     | 346/780 [02:10<02:06,  3.42it/s] 44%|████▍     | 347/780 [02:10<02:07,  3.40it/s] 45%|████▍     | 348/780 [02:11<02:07,  3.40it/s] 45%|████▍     | 349/780 [02:11<02:06,  3.41it/s] 45%|████▍     | 350/780 [02:11<02:06,  3.41it/s] 45%|████▌     | 351/780 [02:12<02:05,  3.41it/s] 45%|████▌     | 352/780 [02:12<02:05,  3.42it/s] 45%|████▌     | 353/780 [02:12<02:04,  3.42it/s] 45%|████▌     | 354/780 [02:12<02:04,  3.42it/s] 46%|████▌     | 355/780 [02:13<02:04,  3.42it/s] 46%|████▌     | 356/780 [02:13<02:04,  3.42it/s] 46%|████▌     | 357/780 [02:13<02:03,  3.42it/s] 46%|████▌     | 358/780 [02:14<02:03,  3.41it/s] 46%|████▌     | 359/780 [02:14<02:03,  3.41it/s] 46%|████▌     | 360/780 [02:14<02:02,  3.42it/s] 46%|████▋     | 361/780 [02:14<02:03,  3.40it/s] 46%|████▋     | 362/780 [02:15<02:02,  3.41it/s] 47%|████▋     | 363/780 [02:15<02:02,  3.41it/s] 47%|████▋     | 364/780 [02:15<02:01,  3.41it/s] 47%|████▋     | 365/780 [02:16<02:01,  3.41it/s] 47%|████▋     | 366/780 [02:16<02:01,  3.41it/s] 47%|████▋     | 367/780 [02:16<02:00,  3.42it/s] 47%|████▋     | 368/780 [02:16<02:00,  3.42it/s] 47%|████▋     | 369/780 [02:17<02:00,  3.41it/s] 47%|████▋     | 370/780 [02:17<02:00,  3.41it/s] 48%|████▊     | 371/780 [02:17<01:59,  3.42it/s] 48%|████▊     | 372/780 [02:18<01:59,  3.42it/s] 48%|████▊     | 373/780 [02:18<01:59,  3.42it/s] 48%|████▊     | 374/780 [02:18<01:58,  3.42it/s] 48%|████▊     | 375/780 [02:19<01:58,  3.42it/s] 48%|████▊     | 376/780 [02:19<01:58,  3.42it/s] 48%|████▊     | 377/780 [02:19<01:57,  3.42it/s] 48%|████▊     | 378/780 [02:19<01:57,  3.42it/s] 49%|████▊     | 379/780 [02:20<01:57,  3.42it/s] 49%|████▊     | 380/780 [02:20<01:57,  3.41it/s] 49%|████▉     | 381/780 [02:20<01:56,  3.42it/s] 49%|████▉     | 382/780 [02:21<01:56,  3.42it/s] 49%|████▉     | 383/780 [02:21<01:56,  3.42it/s] 49%|████▉     | 384/780 [02:21<01:55,  3.42it/s] 49%|████▉     | 385/780 [02:21<01:55,  3.42it/s] 49%|████▉     | 386/780 [02:22<01:55,  3.42it/s] 50%|████▉     | 387/780 [02:22<01:54,  3.42it/s] 50%|████▉     | 388/780 [02:22<01:54,  3.42it/s] 50%|████▉     | 389/780 [02:23<01:54,  3.42it/s] 50%|█████     | 390/780 [02:23<01:54,  3.42it/s] 50%|█████     | 391/780 [02:23<01:54,  3.41it/s] 50%|█████     | 392/780 [02:24<01:53,  3.41it/s] 50%|█████     | 393/780 [02:24<01:53,  3.42it/s] 51%|█████     | 394/780 [02:24<01:53,  3.41it/s] 51%|█████     | 395/780 [02:24<01:52,  3.41it/s] 51%|█████     | 396/780 [02:25<01:52,  3.42it/s] 51%|█████     | 397/780 [02:25<01:52,  3.42it/s] 51%|█████     | 398/780 [02:25<01:51,  3.41it/s] 51%|█████     | 399/780 [02:26<01:51,  3.42it/s] 51%|█████▏    | 400/780 [02:26<01:51,  3.41it/s] 51%|█████▏    | 401/780 [02:26<01:51,  3.41it/s] 52%|█████▏    | 402/780 [02:26<01:50,  3.42it/s] 52%|█████▏    | 403/780 [02:27<01:50,  3.41it/s] 52%|█████▏    | 404/780 [02:27<01:50,  3.42it/s] 52%|█████▏    | 405/780 [02:27<01:49,  3.42it/s] 52%|█████▏    | 406/780 [02:28<01:49,  3.42it/s] 52%|█████▏    | 407/780 [02:28<01:48,  3.42it/s] 52%|█████▏    | 408/780 [02:28<01:48,  3.44it/s] 52%|█████▏    | 409/780 [02:28<01:47,  3.44it/s] 53%|█████▎    | 410/780 [02:29<01:47,  3.45it/s] 53%|█████▎    | 411/780 [02:29<01:46,  3.45it/s] 53%|█████▎    | 412/780 [02:29<01:46,  3.46it/s] 53%|█████▎    | 413/780 [02:30<01:46,  3.46it/s] 53%|█████▎    | 414/780 [02:30<01:45,  3.47it/s] 53%|█████▎    | 415/780 [02:30<01:45,  3.46it/s] 53%|█████▎    | 416/780 [02:30<01:44,  3.47it/s] 53%|█████▎    | 417/780 [02:31<01:44,  3.47it/s] 54%|█████▎    | 418/780 [02:31<01:44,  3.47it/s] 54%|█████▎    | 419/780 [02:31<01:44,  3.47it/s] 54%|█████▍    | 420/780 [02:32<01:43,  3.46it/s] 54%|█████▍    | 421/780 [02:32<01:43,  3.46it/s] 54%|█████▍    | 422/780 [02:32<01:43,  3.46it/s] 54%|█████▍    | 423/780 [02:33<01:43,  3.46it/s] 54%|█████▍    | 424/780 [02:33<01:42,  3.47it/s] 54%|█████▍    | 425/780 [02:33<01:42,  3.47it/s] 55%|█████▍    | 426/780 [02:33<01:41,  3.47it/s] 55%|█████▍    | 427/780 [02:34<01:41,  3.47it/s] 55%|█████▍    | 428/780 [02:34<01:41,  3.47it/s] 55%|█████▌    | 429/780 [02:34<01:41,  3.47it/s] 55%|█████▌    | 430/780 [02:35<01:40,  3.48it/s] 55%|█████▌    | 431/780 [02:35<01:40,  3.46it/s] 55%|█████▌    | 432/780 [02:35<01:40,  3.46it/s] 56%|█████▌    | 433/780 [02:35<01:40,  3.47it/s] 56%|█████▌    | 434/780 [02:36<01:40,  3.45it/s] 56%|█████▌    | 435/780 [02:36<01:39,  3.46it/s] 56%|█████▌    | 436/780 [02:36<01:41,  3.39it/s] 56%|█████▌    | 437/780 [02:37<01:40,  3.41it/s] 56%|█████▌    | 438/780 [02:37<01:39,  3.43it/s] 56%|█████▋    | 439/780 [02:37<01:39,  3.44it/s] 56%|█████▋    | 440/780 [02:37<01:38,  3.44it/s] 57%|█████▋    | 441/780 [02:38<01:38,  3.45it/s] 57%|█████▋    | 442/780 [02:38<01:38,  3.43it/s] 57%|█████▋    | 443/780 [02:38<01:37,  3.44it/s] 57%|█████▋    | 444/780 [02:39<01:37,  3.44it/s] 57%|█████▋    | 445/780 [02:39<01:37,  3.45it/s] 57%|█████▋    | 446/780 [02:39<01:36,  3.45it/s] 57%|█████▋    | 447/780 [02:39<01:36,  3.45it/s] 57%|█████▋    | 448/780 [02:40<01:36,  3.45it/s] 58%|█████▊    | 449/780 [02:40<01:35,  3.45it/s] 58%|█████▊    | 450/780 [02:40<01:35,  3.45it/s] 58%|█████▊    | 451/780 [02:41<01:35,  3.45it/s] 58%|█████▊    | 452/780 [02:41<01:35,  3.45it/s] 58%|█████▊    | 453/780 [02:41<01:34,  3.45it/s] 58%|█████▊    | 454/780 [02:41<01:34,  3.45it/s] 58%|█████▊    | 455/780 [02:42<01:34,  3.45it/s] 58%|█████▊    | 456/780 [02:42<01:33,  3.45it/s] 59%|█████▊    | 457/780 [02:42<01:33,  3.46it/s] 59%|█████▊    | 458/780 [02:43<01:33,  3.45it/s] 59%|█████▉    | 459/780 [02:43<01:32,  3.46it/s] 59%|█████▉    | 460/780 [02:43<01:32,  3.46it/s] 59%|█████▉    | 461/780 [02:44<01:32,  3.45it/s] 59%|█████▉    | 462/780 [02:44<01:32,  3.45it/s] 59%|█████▉    | 463/780 [02:44<01:31,  3.45it/s] 59%|█████▉    | 464/780 [02:44<01:31,  3.44it/s] 60%|█████▉    | 465/780 [02:45<01:31,  3.44it/s] 60%|█████▉    | 466/780 [02:45<01:31,  3.44it/s] 60%|█████▉    | 467/780 [02:45<01:30,  3.45it/s] 60%|██████    | 468/780 [02:46<01:30,  3.45it/s][INFO|trainer.py:2140] 2023-08-29 09:11:37,845 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:11:37,845 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 09:11:37,845 >>   Batch size = 8
{'eval_loss': 1.1302827596664429, 'eval_runtime': 9.6061, 'eval_samples_per_second': 358.73, 'eval_steps_per_second': 44.867, 'epoch': 2.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.61it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.22it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.51it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.84it/s][A
  6%|▋         | 27/431 [00:00<00:08, 46.02it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.50it/s][A
  9%|▊         | 37/431 [00:00<00:08, 44.96it/s][A
 10%|▉         | 42/431 [00:00<00:08, 44.62it/s][A
 11%|█         | 47/431 [00:01<00:08, 44.71it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 44.89it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.10it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 44.96it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.29it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.19it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.06it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.63it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.54it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.60it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 44.74it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 44.90it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.04it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.16it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.25it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 45.13it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.87it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.68it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.57it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 44.65it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 44.79it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 44.92it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.07it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.19it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.19it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.96it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.88it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.70it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 44.66it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 44.62it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 44.74it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 44.93it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.17it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.06it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.97it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.84it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.76it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 44.77it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 44.81it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 44.74it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.03it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.17it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.25it/s][A
 61%|██████    | 262/431 [00:05<00:03, 45.04it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.85it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.75it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 44.79it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 44.78it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 44.88it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 44.96it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 44.97it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.13it/s][A
 71%|███████   | 307/431 [00:06<00:02, 45.06it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.92it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.84it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 44.67it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 44.83it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 44.83it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.01it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 44.95it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.09it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 45.00it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.81it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.87it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.81it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 44.79it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 44.87it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 44.89it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 44.97it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.07it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.01it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 44.90it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.87it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 44.69it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.02it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 44.88it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.00it/s][A                                                 
                                                 [A 60%|██████    | 468/780 [02:55<01:30,  3.45it/s]
100%|██████████| 431/431 [00:09<00:00, 45.00it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:11:47,506 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468
[INFO|configuration_utils.py:351] 2023-08-29 09:11:47,550 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:11:49,574 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:11:49,611 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:11:49,621 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468/special_tokens_map.json
 60%|██████    | 469/780 [03:01<25:28,  4.92s/it] 60%|██████    | 470/780 [03:02<18:14,  3.53s/it] 60%|██████    | 471/780 [03:02<13:10,  2.56s/it] 61%|██████    | 472/780 [03:02<09:38,  1.88s/it] 61%|██████    | 473/780 [03:02<07:10,  1.40s/it] 61%|██████    | 474/780 [03:03<05:27,  1.07s/it] 61%|██████    | 475/780 [03:03<04:15,  1.20it/s] 61%|██████    | 476/780 [03:03<03:24,  1.49it/s] 61%|██████    | 477/780 [03:04<02:49,  1.79it/s] 61%|██████▏   | 478/780 [03:04<02:24,  2.09it/s] 61%|██████▏   | 479/780 [03:04<02:07,  2.36it/s] 62%|██████▏   | 480/780 [03:04<01:55,  2.61it/s] 62%|██████▏   | 481/780 [03:05<01:46,  2.80it/s] 62%|██████▏   | 482/780 [03:05<01:40,  2.96it/s] 62%|██████▏   | 483/780 [03:05<01:36,  3.09it/s] 62%|██████▏   | 484/780 [03:06<01:33,  3.18it/s] 62%|██████▏   | 485/780 [03:06<01:30,  3.24it/s] 62%|██████▏   | 486/780 [03:06<01:29,  3.30it/s] 62%|██████▏   | 487/780 [03:07<01:27,  3.34it/s] 63%|██████▎   | 488/780 [03:07<01:26,  3.36it/s] 63%|██████▎   | 489/780 [03:07<01:26,  3.38it/s] 63%|██████▎   | 490/780 [03:07<01:25,  3.39it/s] 63%|██████▎   | 491/780 [03:08<01:24,  3.40it/s] 63%|██████▎   | 492/780 [03:08<01:24,  3.40it/s] 63%|██████▎   | 493/780 [03:08<01:24,  3.40it/s] 63%|██████▎   | 494/780 [03:09<01:23,  3.41it/s] 63%|██████▎   | 495/780 [03:09<01:23,  3.41it/s] 64%|██████▎   | 496/780 [03:09<01:23,  3.41it/s] 64%|██████▎   | 497/780 [03:09<01:22,  3.41it/s] 64%|██████▍   | 498/780 [03:10<01:22,  3.41it/s] 64%|██████▍   | 499/780 [03:10<01:22,  3.42it/s] 64%|██████▍   | 500/780 [03:10<01:21,  3.42it/s]                                                  64%|██████▍   | 500/780 [03:10<01:21,  3.42it/s] 64%|██████▍   | 501/780 [03:11<01:21,  3.42it/s] 64%|██████▍   | 502/780 [03:11<01:21,  3.42it/s] 64%|██████▍   | 503/780 [03:11<01:21,  3.42it/s] 65%|██████▍   | 504/780 [03:12<01:20,  3.41it/s] 65%|██████▍   | 505/780 [03:12<01:20,  3.42it/s] 65%|██████▍   | 506/780 [03:12<01:20,  3.42it/s] 65%|██████▌   | 507/780 [03:12<01:19,  3.43it/s] 65%|██████▌   | 508/780 [03:13<01:19,  3.42it/s] 65%|██████▌   | 509/780 [03:13<01:19,  3.42it/s] 65%|██████▌   | 510/780 [03:13<01:18,  3.42it/s] 66%|██████▌   | 511/780 [03:14<01:18,  3.42it/s] 66%|██████▌   | 512/780 [03:14<01:18,  3.42it/s] 66%|██████▌   | 513/780 [03:14<01:18,  3.42it/s] 66%|██████▌   | 514/780 [03:14<01:18,  3.41it/s] 66%|██████▌   | 515/780 [03:15<01:17,  3.42it/s] 66%|██████▌   | 516/780 [03:15<01:17,  3.42it/s] 66%|██████▋   | 517/780 [03:15<01:16,  3.42it/s] 66%|██████▋   | 518/780 [03:16<01:16,  3.42it/s] 67%|██████▋   | 519/780 [03:16<01:16,  3.42it/s] 67%|██████▋   | 520/780 [03:16<01:15,  3.43it/s] 67%|██████▋   | 521/780 [03:16<01:15,  3.42it/s] 67%|██████▋   | 522/780 [03:17<01:15,  3.43it/s] 67%|██████▋   | 523/780 [03:17<01:14,  3.43it/s] 67%|██████▋   | 524/780 [03:17<01:14,  3.42it/s] 67%|██████▋   | 525/780 [03:18<01:14,  3.42it/s] 67%|██████▋   | 526/780 [03:18<01:14,  3.42it/s] 68%|██████▊   | 527/780 [03:18<01:13,  3.42it/s] 68%|██████▊   | 528/780 [03:19<01:13,  3.42it/s] 68%|██████▊   | 529/780 [03:19<01:13,  3.42it/s] 68%|██████▊   | 530/780 [03:19<01:12,  3.43it/s] 68%|██████▊   | 531/780 [03:19<01:12,  3.42it/s] 68%|██████▊   | 532/780 [03:20<01:12,  3.42it/s] 68%|██████▊   | 533/780 [03:20<01:12,  3.42it/s] 68%|██████▊   | 534/780 [03:20<01:11,  3.42it/s] 69%|██████▊   | 535/780 [03:21<01:11,  3.42it/s] 69%|██████▊   | 536/780 [03:21<01:11,  3.42it/s] 69%|██████▉   | 537/780 [03:21<01:11,  3.42it/s] 69%|██████▉   | 538/780 [03:21<01:10,  3.42it/s] 69%|██████▉   | 539/780 [03:22<01:10,  3.42it/s] 69%|██████▉   | 540/780 [03:22<01:10,  3.42it/s] 69%|██████▉   | 541/780 [03:22<01:09,  3.42it/s] 69%|██████▉   | 542/780 [03:23<01:09,  3.42it/s] 70%|██████▉   | 543/780 [03:23<01:09,  3.43it/s] 70%|██████▉   | 544/780 [03:23<01:08,  3.42it/s] 70%|██████▉   | 545/780 [03:23<01:08,  3.42it/s] 70%|███████   | 546/780 [03:24<01:08,  3.43it/s] 70%|███████   | 547/780 [03:24<01:08,  3.42it/s] 70%|███████   | 548/780 [03:24<01:07,  3.42it/s] 70%|███████   | 549/780 [03:25<01:07,  3.43it/s] 71%|███████   | 550/780 [03:25<01:07,  3.42it/s] 71%|███████   | 551/780 [03:25<01:06,  3.43it/s] 71%|███████   | 552/780 [03:26<01:06,  3.43it/s] 71%|███████   | 553/780 [03:26<01:06,  3.43it/s] 71%|███████   | 554/780 [03:26<01:06,  3.42it/s] 71%|███████   | 555/780 [03:26<01:05,  3.42it/s] 71%|███████▏  | 556/780 [03:27<01:05,  3.43it/s] 71%|███████▏  | 557/780 [03:27<01:05,  3.43it/s] 72%|███████▏  | 558/780 [03:27<01:04,  3.43it/s] 72%|███████▏  | 559/780 [03:28<01:04,  3.43it/s] 72%|███████▏  | 560/780 [03:28<01:04,  3.43it/s] 72%|███████▏  | 561/780 [03:28<01:03,  3.42it/s] 72%|███████▏  | 562/780 [03:28<01:03,  3.42it/s] 72%|███████▏  | 563/780 [03:29<01:03,  3.42it/s] 72%|███████▏  | 564/780 [03:29<01:03,  3.42it/s] 72%|███████▏  | 565/780 [03:29<01:02,  3.41it/s] 73%|███████▎  | 566/780 [03:30<01:02,  3.41it/s] 73%|███████▎  | 567/780 [03:30<01:02,  3.41it/s] 73%|███████▎  | 568/780 [03:30<01:02,  3.41it/s] 73%|███████▎  | 569/780 [03:30<01:01,  3.42it/s] 73%|███████▎  | 570/780 [03:31<01:01,  3.42it/s] 73%|███████▎  | 571/780 [03:31<01:01,  3.42it/s] 73%|███████▎  | 572/780 [03:31<01:00,  3.43it/s] 73%|███████▎  | 573/780 [03:32<01:00,  3.42it/s] 74%|███████▎  | 574/780 [03:32<01:00,  3.43it/s] 74%|███████▎  | 575/780 [03:32<00:59,  3.43it/s] 74%|███████▍  | 576/780 [03:33<00:59,  3.43it/s] 74%|███████▍  | 577/780 [03:33<00:59,  3.43it/s] 74%|███████▍  | 578/780 [03:33<00:59,  3.42it/s] 74%|███████▍  | 579/780 [03:33<00:58,  3.42it/s] 74%|███████▍  | 580/780 [03:34<00:58,  3.42it/s] 74%|███████▍  | 581/780 [03:34<00:58,  3.43it/s] 75%|███████▍  | 582/780 [03:34<00:57,  3.43it/s] 75%|███████▍  | 583/780 [03:35<00:57,  3.44it/s] 75%|███████▍  | 584/780 [03:35<00:56,  3.45it/s] 75%|███████▌  | 585/780 [03:35<00:56,  3.47it/s] 75%|███████▌  | 586/780 [03:35<00:55,  3.47it/s] 75%|███████▌  | 587/780 [03:36<00:55,  3.47it/s] 75%|███████▌  | 588/780 [03:36<00:55,  3.47it/s] 76%|███████▌  | 589/780 [03:36<00:55,  3.43it/s] 76%|███████▌  | 590/780 [03:37<00:55,  3.44it/s] 76%|███████▌  | 591/780 [03:37<00:54,  3.45it/s] 76%|███████▌  | 592/780 [03:37<00:54,  3.46it/s] 76%|███████▌  | 593/780 [03:37<00:53,  3.47it/s] 76%|███████▌  | 594/780 [03:38<00:53,  3.47it/s] 76%|███████▋  | 595/780 [03:38<00:53,  3.47it/s] 76%|███████▋  | 596/780 [03:38<00:52,  3.48it/s] 77%|███████▋  | 597/780 [03:39<00:52,  3.48it/s] 77%|███████▋  | 598/780 [03:39<00:52,  3.48it/s] 77%|███████▋  | 599/780 [03:39<00:52,  3.48it/s] 77%|███████▋  | 600/780 [03:39<00:52,  3.46it/s] 77%|███████▋  | 601/780 [03:40<00:51,  3.47it/s] 77%|███████▋  | 602/780 [03:40<00:51,  3.48it/s] 77%|███████▋  | 603/780 [03:40<00:50,  3.47it/s] 77%|███████▋  | 604/780 [03:41<00:50,  3.48it/s] 78%|███████▊  | 605/780 [03:41<00:50,  3.48it/s] 78%|███████▊  | 606/780 [03:41<00:50,  3.47it/s] 78%|███████▊  | 607/780 [03:41<00:49,  3.47it/s] 78%|███████▊  | 608/780 [03:42<00:49,  3.47it/s] 78%|███████▊  | 609/780 [03:42<00:49,  3.47it/s] 78%|███████▊  | 610/780 [03:42<00:48,  3.47it/s] 78%|███████▊  | 611/780 [03:43<00:48,  3.46it/s] 78%|███████▊  | 612/780 [03:43<00:48,  3.47it/s] 79%|███████▊  | 613/780 [03:43<00:48,  3.47it/s] 79%|███████▊  | 614/780 [03:44<00:47,  3.47it/s] 79%|███████▉  | 615/780 [03:44<00:47,  3.47it/s] 79%|███████▉  | 616/780 [03:44<00:47,  3.47it/s] 79%|███████▉  | 617/780 [03:44<00:46,  3.47it/s] 79%|███████▉  | 618/780 [03:45<00:46,  3.47it/s] 79%|███████▉  | 619/780 [03:45<00:46,  3.48it/s] 79%|███████▉  | 620/780 [03:45<00:46,  3.47it/s] 80%|███████▉  | 621/780 [03:46<00:45,  3.48it/s] 80%|███████▉  | 622/780 [03:46<00:45,  3.47it/s] 80%|███████▉  | 623/780 [03:46<00:45,  3.47it/s] 80%|████████  | 624/780 [03:46<00:44,  3.47it/s][INFO|trainer.py:2140] 2023-08-29 09:12:38,681 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:12:38,681 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 09:12:38,681 >>   Batch size = 8
{'eval_loss': 1.1395262479782104, 'eval_runtime': 9.6245, 'eval_samples_per_second': 358.045, 'eval_steps_per_second': 44.782, 'epoch': 3.0}
{'loss': 0.3622, 'learning_rate': 1.3461538461538462e-05, 'epoch': 3.2}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.54it/s][A
  3%|▎         | 12/431 [00:00<00:08, 48.97it/s][A
  4%|▍         | 17/431 [00:00<00:08, 46.94it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.09it/s][A
  6%|▋         | 27/431 [00:00<00:08, 45.60it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.37it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.13it/s][A
 10%|▉         | 42/431 [00:00<00:08, 44.74it/s][A
 11%|█         | 47/431 [00:01<00:08, 44.92it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.09it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.30it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.27it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.21it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 44.99it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 44.96it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.76it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.64it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.86it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 44.89it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.19it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.27it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.19it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.18it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 44.95it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.80it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.73it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.78it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 44.92it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 44.93it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.04it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.16it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.25it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.18it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.89it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.78it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.82it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 44.91it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.00it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 44.89it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.07it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.21it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.18it/s][A
 50%|█████     | 217/431 [00:04<00:04, 45.03it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.73it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.87it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 44.98it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.01it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 44.99it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 44.96it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.04it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.11it/s][A
 61%|██████    | 262/431 [00:05<00:03, 45.05it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.87it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.82it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 44.85it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 44.96it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.01it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.05it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.10it/s][A
 70%|███████   | 302/431 [00:06<00:02, 45.02it/s][A
 71%|███████   | 307/431 [00:06<00:02, 45.10it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.92it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.92it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 44.84it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 44.70it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.04it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.15it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.03it/s][A
 81%|████████  | 347/431 [00:07<00:01, 45.12it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 45.10it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.89it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.85it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.88it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 44.93it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.02it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 44.98it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.01it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.08it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 45.12it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 45.01it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.87it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 44.89it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 44.92it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 44.98it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.11it/s][A                                                 
                                                 [A 80%|████████  | 624/780 [03:56<00:44,  3.47it/s]
100%|██████████| 431/431 [00:09<00:00, 45.11it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:12:48,315 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624
[INFO|configuration_utils.py:351] 2023-08-29 09:12:48,333 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:12:50,716 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:12:50,738 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:12:50,767 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624/special_tokens_map.json
 80%|████████  | 625/780 [04:03<13:38,  5.28s/it] 80%|████████  | 626/780 [04:04<09:43,  3.79s/it] 80%|████████  | 627/780 [04:04<06:59,  2.74s/it] 81%|████████  | 628/780 [04:04<05:05,  2.01s/it] 81%|████████  | 629/780 [04:05<03:45,  1.49s/it] 81%|████████  | 630/780 [04:05<02:50,  1.13s/it] 81%|████████  | 631/780 [04:05<02:11,  1.13it/s] 81%|████████  | 632/780 [04:05<01:44,  1.42it/s] 81%|████████  | 633/780 [04:06<01:25,  1.72it/s] 81%|████████▏ | 634/780 [04:06<01:12,  2.02it/s] 81%|████████▏ | 635/780 [04:06<01:02,  2.31it/s] 82%|████████▏ | 636/780 [04:07<00:56,  2.56it/s] 82%|████████▏ | 637/780 [04:07<00:51,  2.76it/s] 82%|████████▏ | 638/780 [04:07<00:48,  2.93it/s] 82%|████████▏ | 639/780 [04:07<00:46,  3.06it/s] 82%|████████▏ | 640/780 [04:08<00:44,  3.16it/s] 82%|████████▏ | 641/780 [04:08<00:42,  3.24it/s] 82%|████████▏ | 642/780 [04:08<00:41,  3.29it/s] 82%|████████▏ | 643/780 [04:09<00:41,  3.33it/s] 83%|████████▎ | 644/780 [04:09<00:40,  3.35it/s] 83%|████████▎ | 645/780 [04:09<00:39,  3.38it/s] 83%|████████▎ | 646/780 [04:09<00:39,  3.39it/s] 83%|████████▎ | 647/780 [04:10<00:39,  3.40it/s] 83%|████████▎ | 648/780 [04:10<00:38,  3.39it/s] 83%|████████▎ | 649/780 [04:10<00:38,  3.40it/s] 83%|████████▎ | 650/780 [04:11<00:38,  3.41it/s] 83%|████████▎ | 651/780 [04:11<00:37,  3.41it/s] 84%|████████▎ | 652/780 [04:11<00:37,  3.42it/s] 84%|████████▎ | 653/780 [04:12<00:37,  3.42it/s] 84%|████████▍ | 654/780 [04:12<00:36,  3.43it/s] 84%|████████▍ | 655/780 [04:12<00:36,  3.44it/s] 84%|████████▍ | 656/780 [04:12<00:35,  3.45it/s] 84%|████████▍ | 657/780 [04:13<00:35,  3.46it/s] 84%|████████▍ | 658/780 [04:13<00:35,  3.47it/s] 84%|████████▍ | 659/780 [04:13<00:34,  3.46it/s] 85%|████████▍ | 660/780 [04:14<00:34,  3.47it/s] 85%|████████▍ | 661/780 [04:14<00:34,  3.47it/s] 85%|████████▍ | 662/780 [04:14<00:33,  3.47it/s] 85%|████████▌ | 663/780 [04:14<00:33,  3.47it/s] 85%|████████▌ | 664/780 [04:15<00:33,  3.47it/s] 85%|████████▌ | 665/780 [04:15<00:33,  3.48it/s] 85%|████████▌ | 666/780 [04:15<00:32,  3.48it/s] 86%|████████▌ | 667/780 [04:16<00:32,  3.48it/s] 86%|████████▌ | 668/780 [04:16<00:32,  3.48it/s] 86%|████████▌ | 669/780 [04:16<00:31,  3.48it/s] 86%|████████▌ | 670/780 [04:16<00:31,  3.47it/s] 86%|████████▌ | 671/780 [04:17<00:31,  3.47it/s] 86%|████████▌ | 672/780 [04:17<00:31,  3.47it/s] 86%|████████▋ | 673/780 [04:17<00:30,  3.48it/s] 86%|████████▋ | 674/780 [04:18<00:30,  3.47it/s] 87%|████████▋ | 675/780 [04:18<00:30,  3.48it/s] 87%|████████▋ | 676/780 [04:18<00:29,  3.47it/s] 87%|████████▋ | 677/780 [04:18<00:29,  3.48it/s] 87%|████████▋ | 678/780 [04:19<00:29,  3.48it/s] 87%|████████▋ | 679/780 [04:19<00:29,  3.48it/s] 87%|████████▋ | 680/780 [04:19<00:28,  3.48it/s] 87%|████████▋ | 681/780 [04:20<00:28,  3.47it/s] 87%|████████▋ | 682/780 [04:20<00:28,  3.47it/s] 88%|████████▊ | 683/780 [04:20<00:27,  3.47it/s] 88%|████████▊ | 684/780 [04:20<00:27,  3.47it/s] 88%|████████▊ | 685/780 [04:21<00:27,  3.48it/s] 88%|████████▊ | 686/780 [04:21<00:27,  3.47it/s] 88%|████████▊ | 687/780 [04:21<00:26,  3.47it/s] 88%|████████▊ | 688/780 [04:22<00:26,  3.47it/s] 88%|████████▊ | 689/780 [04:22<00:26,  3.48it/s] 88%|████████▊ | 690/780 [04:22<00:25,  3.47it/s] 89%|████████▊ | 691/780 [04:22<00:25,  3.47it/s] 89%|████████▊ | 692/780 [04:23<00:25,  3.46it/s] 89%|████████▉ | 693/780 [04:23<00:25,  3.47it/s] 89%|████████▉ | 694/780 [04:23<00:24,  3.47it/s] 89%|████████▉ | 695/780 [04:24<00:24,  3.47it/s] 89%|████████▉ | 696/780 [04:24<00:24,  3.47it/s] 89%|████████▉ | 697/780 [04:24<00:23,  3.48it/s] 89%|████████▉ | 698/780 [04:24<00:23,  3.47it/s] 90%|████████▉ | 699/780 [04:25<00:23,  3.47it/s] 90%|████████▉ | 700/780 [04:25<00:23,  3.48it/s] 90%|████████▉ | 701/780 [04:25<00:22,  3.48it/s] 90%|█████████ | 702/780 [04:26<00:22,  3.48it/s] 90%|█████████ | 703/780 [04:26<00:22,  3.46it/s] 90%|█████████ | 704/780 [04:26<00:21,  3.47it/s] 90%|█████████ | 705/780 [04:27<00:21,  3.47it/s] 91%|█████████ | 706/780 [04:27<00:21,  3.47it/s] 91%|█████████ | 707/780 [04:27<00:21,  3.47it/s] 91%|█████████ | 708/780 [04:27<00:20,  3.48it/s] 91%|█████████ | 709/780 [04:28<00:20,  3.48it/s] 91%|█████████ | 710/780 [04:28<00:20,  3.48it/s] 91%|█████████ | 711/780 [04:28<00:19,  3.48it/s] 91%|█████████▏| 712/780 [04:29<00:19,  3.48it/s] 91%|█████████▏| 713/780 [04:29<00:19,  3.48it/s] 92%|█████████▏| 714/780 [04:29<00:18,  3.48it/s] 92%|█████████▏| 715/780 [04:29<00:18,  3.47it/s] 92%|█████████▏| 716/780 [04:30<00:18,  3.47it/s] 92%|█████████▏| 717/780 [04:30<00:18,  3.47it/s] 92%|█████████▏| 718/780 [04:30<00:17,  3.47it/s] 92%|█████████▏| 719/780 [04:31<00:17,  3.47it/s] 92%|█████████▏| 720/780 [04:31<00:17,  3.47it/s] 92%|█████████▏| 721/780 [04:31<00:17,  3.46it/s] 93%|█████████▎| 722/780 [04:31<00:16,  3.46it/s] 93%|█████████▎| 723/780 [04:32<00:16,  3.47it/s] 93%|█████████▎| 724/780 [04:32<00:16,  3.47it/s] 93%|█████████▎| 725/780 [04:32<00:15,  3.47it/s] 93%|█████████▎| 726/780 [04:33<00:15,  3.47it/s] 93%|█████████▎| 727/780 [04:33<00:15,  3.47it/s] 93%|█████████▎| 728/780 [04:33<00:14,  3.47it/s] 93%|█████████▎| 729/780 [04:33<00:14,  3.47it/s] 94%|█████████▎| 730/780 [04:34<00:14,  3.47it/s] 94%|█████████▎| 731/780 [04:34<00:14,  3.47it/s] 94%|█████████▍| 732/780 [04:34<00:13,  3.46it/s] 94%|█████████▍| 733/780 [04:35<00:13,  3.47it/s] 94%|█████████▍| 734/780 [04:35<00:13,  3.47it/s] 94%|█████████▍| 735/780 [04:35<00:12,  3.47it/s] 94%|█████████▍| 736/780 [04:35<00:12,  3.48it/s] 94%|█████████▍| 737/780 [04:36<00:12,  3.47it/s] 95%|█████████▍| 738/780 [04:36<00:12,  3.48it/s] 95%|█████████▍| 739/780 [04:36<00:12,  3.39it/s] 95%|█████████▍| 740/780 [04:37<00:11,  3.42it/s] 95%|█████████▌| 741/780 [04:37<00:11,  3.43it/s] 95%|█████████▌| 742/780 [04:37<00:11,  3.44it/s] 95%|█████████▌| 743/780 [04:37<00:10,  3.43it/s] 95%|█████████▌| 744/780 [04:38<00:10,  3.45it/s] 96%|█████████▌| 745/780 [04:38<00:10,  3.45it/s] 96%|█████████▌| 746/780 [04:38<00:09,  3.46it/s] 96%|█████████▌| 747/780 [04:39<00:09,  3.46it/s] 96%|█████████▌| 748/780 [04:39<00:09,  3.46it/s] 96%|█████████▌| 749/780 [04:39<00:08,  3.47it/s] 96%|█████████▌| 750/780 [04:40<00:08,  3.47it/s] 96%|█████████▋| 751/780 [04:40<00:08,  3.46it/s] 96%|█████████▋| 752/780 [04:40<00:08,  3.46it/s] 97%|█████████▋| 753/780 [04:40<00:07,  3.47it/s] 97%|█████████▋| 754/780 [04:41<00:07,  3.45it/s] 97%|█████████▋| 755/780 [04:41<00:07,  3.46it/s] 97%|█████████▋| 756/780 [04:41<00:06,  3.46it/s] 97%|█████████▋| 757/780 [04:42<00:06,  3.47it/s] 97%|█████████▋| 758/780 [04:42<00:06,  3.47it/s] 97%|█████████▋| 759/780 [04:42<00:06,  3.47it/s] 97%|█████████▋| 760/780 [04:42<00:05,  3.47it/s] 98%|█████████▊| 761/780 [04:43<00:05,  3.48it/s] 98%|█████████▊| 762/780 [04:43<00:05,  3.47it/s] 98%|█████████▊| 763/780 [04:43<00:04,  3.48it/s] 98%|█████████▊| 764/780 [04:44<00:04,  3.47it/s] 98%|█████████▊| 765/780 [04:44<00:04,  3.47it/s] 98%|█████████▊| 766/780 [04:44<00:04,  3.46it/s] 98%|█████████▊| 767/780 [04:44<00:03,  3.47it/s] 98%|█████████▊| 768/780 [04:45<00:03,  3.46it/s] 99%|█████████▊| 769/780 [04:45<00:03,  3.46it/s] 99%|█████████▊| 770/780 [04:45<00:02,  3.46it/s] 99%|█████████▉| 771/780 [04:46<00:02,  3.47it/s] 99%|█████████▉| 772/780 [04:46<00:02,  3.46it/s] 99%|█████████▉| 773/780 [04:46<00:02,  3.47it/s] 99%|█████████▉| 774/780 [04:46<00:01,  3.47it/s] 99%|█████████▉| 775/780 [04:47<00:01,  3.47it/s] 99%|█████████▉| 776/780 [04:47<00:01,  3.46it/s]100%|█████████▉| 777/780 [04:47<00:00,  3.46it/s]100%|█████████▉| 778/780 [04:48<00:00,  3.46it/s]100%|█████████▉| 779/780 [04:48<00:00,  3.46it/s]100%|██████████| 780/780 [04:48<00:00,  3.46it/s][INFO|trainer.py:2140] 2023-08-29 09:13:40,414 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:13:40,414 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 09:13:40,414 >>   Batch size = 8
{'eval_loss': 1.15248703956604, 'eval_runtime': 9.6126, 'eval_samples_per_second': 358.487, 'eval_steps_per_second': 44.837, 'epoch': 4.0}

  0%|          | 0/431 [00:00<?, ?it/s][A
  1%|▏         | 6/431 [00:00<00:07, 56.45it/s][A
  3%|▎         | 12/431 [00:00<00:08, 49.08it/s][A
  4%|▍         | 17/431 [00:00<00:08, 47.50it/s][A
  5%|▌         | 22/431 [00:00<00:08, 46.74it/s][A
  6%|▋         | 27/431 [00:00<00:08, 46.31it/s][A
  7%|▋         | 32/431 [00:00<00:08, 45.73it/s][A
  9%|▊         | 37/431 [00:00<00:08, 45.14it/s][A
 10%|▉         | 42/431 [00:00<00:08, 44.97it/s][A
 11%|█         | 47/431 [00:01<00:08, 44.96it/s][A
 12%|█▏        | 52/431 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 57/431 [00:01<00:08, 45.20it/s][A
 14%|█▍        | 62/431 [00:01<00:08, 45.25it/s][A
 16%|█▌        | 67/431 [00:01<00:08, 45.13it/s][A
 17%|█▋        | 72/431 [00:01<00:07, 45.21it/s][A
 18%|█▊        | 77/431 [00:01<00:07, 45.13it/s][A
 19%|█▉        | 82/431 [00:01<00:07, 44.86it/s][A
 20%|██        | 87/431 [00:01<00:07, 44.87it/s][A
 21%|██▏       | 92/431 [00:02<00:07, 44.83it/s][A
 23%|██▎       | 97/431 [00:02<00:07, 44.94it/s][A
 24%|██▎       | 102/431 [00:02<00:07, 45.02it/s][A
 25%|██▍       | 107/431 [00:02<00:07, 45.20it/s][A
 26%|██▌       | 112/431 [00:02<00:07, 45.22it/s][A
 27%|██▋       | 117/431 [00:02<00:06, 45.24it/s][A
 28%|██▊       | 122/431 [00:02<00:06, 44.95it/s][A
 29%|██▉       | 127/431 [00:02<00:06, 44.96it/s][A
 31%|███       | 132/431 [00:02<00:06, 44.87it/s][A
 32%|███▏      | 137/431 [00:03<00:06, 44.77it/s][A
 33%|███▎      | 142/431 [00:03<00:06, 44.95it/s][A
 34%|███▍      | 147/431 [00:03<00:06, 44.94it/s][A
 35%|███▌      | 152/431 [00:03<00:06, 45.18it/s][A
 36%|███▋      | 157/431 [00:03<00:06, 45.25it/s][A
 38%|███▊      | 162/431 [00:03<00:05, 45.19it/s][A
 39%|███▊      | 167/431 [00:03<00:05, 45.08it/s][A
 40%|███▉      | 172/431 [00:03<00:05, 44.78it/s][A
 41%|████      | 177/431 [00:03<00:05, 44.77it/s][A
 42%|████▏     | 182/431 [00:04<00:05, 44.86it/s][A
 43%|████▎     | 187/431 [00:04<00:05, 44.99it/s][A
 45%|████▍     | 192/431 [00:04<00:05, 45.05it/s][A
 46%|████▌     | 197/431 [00:04<00:05, 44.97it/s][A
 47%|████▋     | 202/431 [00:04<00:05, 45.20it/s][A
 48%|████▊     | 207/431 [00:04<00:04, 45.24it/s][A
 49%|████▉     | 212/431 [00:04<00:04, 45.11it/s][A
 50%|█████     | 217/431 [00:04<00:04, 44.90it/s][A
 52%|█████▏    | 222/431 [00:04<00:04, 44.64it/s][A
 53%|█████▎    | 227/431 [00:05<00:04, 44.82it/s][A
 54%|█████▍    | 232/431 [00:05<00:04, 44.98it/s][A
 55%|█████▍    | 237/431 [00:05<00:04, 45.02it/s][A
 56%|█████▌    | 242/431 [00:05<00:04, 45.11it/s][A
 57%|█████▋    | 247/431 [00:05<00:04, 45.11it/s][A
 58%|█████▊    | 252/431 [00:05<00:03, 45.15it/s][A
 60%|█████▉    | 257/431 [00:05<00:03, 45.14it/s][A
 61%|██████    | 262/431 [00:05<00:03, 44.94it/s][A
 62%|██████▏   | 267/431 [00:05<00:03, 44.90it/s][A
 63%|██████▎   | 272/431 [00:06<00:03, 44.95it/s][A
 64%|██████▍   | 277/431 [00:06<00:03, 45.00it/s][A
 65%|██████▌   | 282/431 [00:06<00:03, 45.12it/s][A
 67%|██████▋   | 287/431 [00:06<00:03, 45.16it/s][A
 68%|██████▊   | 292/431 [00:06<00:03, 45.20it/s][A
 69%|██████▉   | 297/431 [00:06<00:02, 45.10it/s][A
 70%|███████   | 302/431 [00:06<00:02, 44.96it/s][A
 71%|███████   | 307/431 [00:06<00:02, 44.94it/s][A
 72%|███████▏  | 312/431 [00:06<00:02, 44.87it/s][A
 74%|███████▎  | 317/431 [00:07<00:02, 44.87it/s][A
 75%|███████▍  | 322/431 [00:07<00:02, 44.92it/s][A
 76%|███████▌  | 327/431 [00:07<00:02, 45.06it/s][A
 77%|███████▋  | 332/431 [00:07<00:02, 45.08it/s][A
 78%|███████▊  | 337/431 [00:07<00:02, 45.16it/s][A
 79%|███████▉  | 342/431 [00:07<00:01, 45.12it/s][A
 81%|████████  | 347/431 [00:07<00:01, 44.98it/s][A
 82%|████████▏ | 352/431 [00:07<00:01, 44.94it/s][A
 83%|████████▎ | 357/431 [00:07<00:01, 44.88it/s][A
 84%|████████▍ | 362/431 [00:08<00:01, 44.83it/s][A
 85%|████████▌ | 367/431 [00:08<00:01, 44.91it/s][A
 86%|████████▋ | 372/431 [00:08<00:01, 45.06it/s][A
 87%|████████▋ | 377/431 [00:08<00:01, 45.16it/s][A
 89%|████████▊ | 382/431 [00:08<00:01, 45.13it/s][A
 90%|████████▉ | 387/431 [00:08<00:00, 45.10it/s][A
 91%|█████████ | 392/431 [00:08<00:00, 45.06it/s][A
 92%|█████████▏| 397/431 [00:08<00:00, 44.85it/s][A
 93%|█████████▎| 402/431 [00:08<00:00, 44.97it/s][A
 94%|█████████▍| 407/431 [00:09<00:00, 44.83it/s][A
 96%|█████████▌| 412/431 [00:09<00:00, 44.89it/s][A
 97%|█████████▋| 417/431 [00:09<00:00, 45.02it/s][A
 98%|█████████▊| 422/431 [00:09<00:00, 45.14it/s][A
 99%|█████████▉| 427/431 [00:09<00:00, 45.21it/s][A                                                 
                                                 [A100%|██████████| 780/780 [04:58<00:00,  3.46it/s]
100%|██████████| 431/431 [00:09<00:00, 45.21it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-29 09:13:50,007 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780
[INFO|configuration_utils.py:351] 2023-08-29 09:13:50,026 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:13:52,316 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:13:52,331 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:13:52,345 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-29 09:13:55,772 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-29 09:13:55,775 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156 (score: 1.1127866506576538).
                                                 100%|██████████| 780/780 [05:05<00:00,  3.46it/s]100%|██████████| 780/780 [05:05<00:00,  2.55it/s]
[INFO|trainer.py:1894] 2023-08-29 09:13:57,580 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-29 09:13:57,603 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-29 09:13:59,572 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-29 09:13:59,589 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-29 09:13:59,603 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:13:59,802 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:13:59,802 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:13:59,802 >>   train_loss               =     0.3556
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:13:59,802 >>   train_runtime            = 0:05:05.82
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:13:59,802 >>   train_samples            =      10000
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:13:59,802 >>   train_samples_per_second =    163.492
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:13:59,802 >>   train_steps_per_second   =       2.55
{'eval_loss': 1.1606968641281128, 'eval_runtime': 9.5685, 'eval_samples_per_second': 360.141, 'eval_steps_per_second': 45.044, 'epoch': 5.0}
{'train_runtime': 305.825, 'train_samples_per_second': 163.492, 'train_steps_per_second': 2.55, 'train_loss': 0.35555717272636217, 'epoch': 5.0}
08/29/2023 09:13:59 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-29 09:13:59,841 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-29 09:13:59,841 >>   Num examples = 3446
[INFO|trainer.py:2145] 2023-08-29 09:13:59,841 >>   Batch size = 8
  0%|          | 0/431 [00:00<?, ?it/s]  1%|▏         | 6/431 [00:00<00:07, 56.84it/s]  3%|▎         | 12/431 [00:00<00:08, 49.46it/s]  4%|▍         | 18/431 [00:00<00:08, 47.44it/s]  5%|▌         | 23/431 [00:00<00:08, 46.92it/s]  6%|▋         | 28/431 [00:00<00:08, 46.56it/s]  8%|▊         | 33/431 [00:00<00:08, 46.20it/s]  9%|▉         | 38/431 [00:00<00:08, 45.99it/s] 10%|▉         | 43/431 [00:00<00:08, 45.51it/s] 11%|█         | 48/431 [00:01<00:08, 44.79it/s] 12%|█▏        | 53/431 [00:01<00:08, 44.54it/s] 13%|█▎        | 58/431 [00:01<00:08, 44.80it/s] 15%|█▍        | 63/431 [00:01<00:08, 45.02it/s] 16%|█▌        | 68/431 [00:01<00:08, 45.19it/s] 17%|█▋        | 73/431 [00:01<00:07, 45.30it/s] 18%|█▊        | 78/431 [00:01<00:07, 45.35it/s] 19%|█▉        | 83/431 [00:01<00:07, 45.51it/s] 20%|██        | 88/431 [00:01<00:07, 45.31it/s] 22%|██▏       | 93/431 [00:02<00:07, 45.14it/s] 23%|██▎       | 98/431 [00:02<00:07, 44.88it/s] 24%|██▍       | 103/431 [00:02<00:07, 44.90it/s] 25%|██▌       | 108/431 [00:02<00:07, 44.98it/s] 26%|██▌       | 113/431 [00:02<00:07, 45.03it/s] 27%|██▋       | 118/431 [00:02<00:06, 45.17it/s] 29%|██▊       | 123/431 [00:02<00:06, 45.25it/s] 30%|██▉       | 128/431 [00:02<00:06, 45.41it/s] 31%|███       | 133/431 [00:02<00:06, 45.33it/s] 32%|███▏      | 138/431 [00:03<00:06, 45.06it/s] 33%|███▎      | 143/431 [00:03<00:06, 44.77it/s] 34%|███▍      | 148/431 [00:03<00:06, 44.58it/s] 35%|███▌      | 153/431 [00:03<00:06, 44.58it/s] 37%|███▋      | 158/431 [00:03<00:06, 44.87it/s] 38%|███▊      | 163/431 [00:03<00:05, 45.04it/s] 39%|███▉      | 168/431 [00:03<00:05, 44.99it/s] 40%|████      | 173/431 [00:03<00:05, 45.24it/s] 41%|████▏     | 178/431 [00:03<00:05, 45.10it/s] 42%|████▏     | 183/431 [00:04<00:05, 45.07it/s] 44%|████▎     | 188/431 [00:04<00:05, 44.89it/s] 45%|████▍     | 193/431 [00:04<00:05, 44.68it/s] 46%|████▌     | 198/431 [00:04<00:05, 44.83it/s] 47%|████▋     | 203/431 [00:04<00:05, 44.84it/s] 48%|████▊     | 208/431 [00:04<00:04, 45.14it/s] 49%|████▉     | 213/431 [00:04<00:04, 45.19it/s] 51%|█████     | 218/431 [00:04<00:04, 45.30it/s] 52%|█████▏    | 223/431 [00:04<00:04, 45.19it/s] 53%|█████▎    | 228/431 [00:05<00:04, 45.06it/s] 54%|█████▍    | 233/431 [00:05<00:04, 44.87it/s] 55%|█████▌    | 238/431 [00:05<00:04, 44.85it/s] 56%|█████▋    | 243/431 [00:05<00:04, 44.95it/s] 58%|█████▊    | 248/431 [00:05<00:04, 45.01it/s] 59%|█████▊    | 253/431 [00:05<00:03, 45.15it/s] 60%|█████▉    | 258/431 [00:05<00:03, 45.28it/s] 61%|██████    | 263/431 [00:05<00:03, 45.43it/s] 62%|██████▏   | 268/431 [00:05<00:03, 45.48it/s] 63%|██████▎   | 273/431 [00:06<00:03, 45.31it/s] 65%|██████▍   | 278/431 [00:06<00:03, 45.05it/s] 66%|██████▌   | 283/431 [00:06<00:03, 44.93it/s] 67%|██████▋   | 288/431 [00:06<00:03, 44.79it/s] 68%|██████▊   | 293/431 [00:06<00:03, 44.94it/s] 69%|██████▉   | 298/431 [00:06<00:02, 45.11it/s] 70%|███████   | 303/431 [00:06<00:02, 45.19it/s] 71%|███████▏  | 308/431 [00:06<00:02, 45.36it/s] 73%|███████▎  | 313/431 [00:06<00:02, 45.38it/s] 74%|███████▍  | 318/431 [00:07<00:02, 45.28it/s] 75%|███████▍  | 323/431 [00:07<00:02, 45.09it/s] 76%|███████▌  | 328/431 [00:07<00:02, 44.97it/s] 77%|███████▋  | 333/431 [00:07<00:02, 44.94it/s] 78%|███████▊  | 338/431 [00:07<00:02, 44.77it/s] 80%|███████▉  | 343/431 [00:07<00:01, 45.02it/s] 81%|████████  | 348/431 [00:07<00:01, 45.16it/s] 82%|████████▏ | 353/431 [00:07<00:01, 45.35it/s] 83%|████████▎ | 358/431 [00:07<00:01, 45.43it/s] 84%|████████▍ | 363/431 [00:08<00:01, 45.27it/s] 85%|████████▌ | 368/431 [00:08<00:01, 45.10it/s] 87%|████████▋ | 373/431 [00:08<00:01, 44.91it/s] 88%|████████▊ | 378/431 [00:08<00:01, 44.90it/s] 89%|████████▉ | 383/431 [00:08<00:01, 44.95it/s] 90%|█████████ | 388/431 [00:08<00:00, 45.04it/s] 91%|█████████ | 393/431 [00:08<00:00, 45.11it/s] 92%|█████████▏| 398/431 [00:08<00:00, 45.21it/s] 94%|█████████▎| 403/431 [00:08<00:00, 45.31it/s] 95%|█████████▍| 408/431 [00:09<00:00, 45.29it/s] 96%|█████████▌| 413/431 [00:09<00:00, 45.20it/s] 97%|█████████▋| 418/431 [00:09<00:00, 45.09it/s] 98%|█████████▊| 423/431 [00:09<00:00, 44.90it/s] 99%|█████████▉| 428/431 [00:09<00:00, 44.94it/s]100%|██████████| 431/431 [00:09<00:00, 45.18it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-29 09:14:09,398 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:14:09,398 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:14:09,398 >>   eval_loss               =     1.1128
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:14:09,398 >>   eval_runtime            = 0:00:09.55
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:14:09,398 >>   eval_samples            =       3446
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:14:09,398 >>   eval_samples_per_second =    360.568
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:14:09,398 >>   eval_steps_per_second   =     45.097
[INFO|trainer_pt_utils.py:913] 2023-08-29 09:14:09,398 >>   perplexity              =     3.0428
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:14,817 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:14,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:14,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:14,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:14,821 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:14:15,113 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:14:15,114 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:14:15,458 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:14:16,501 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:14:16,501 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:19,037 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:19,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:19,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:19,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:14:19,044 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:14:19,697 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:14:19,698 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:14:20,249 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:14:20,414 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:14:20,414 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-624
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-780
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-468
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/generator/iter5/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/dev.jsonl', 'labels': ['followed by', 'military rank', 'operating system', 'record label', 'tributary'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 11128
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 11228, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.44it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.59it/s]Extractor Predicting: 7it [00:04,  1.57it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.52it/s]Extractor Predicting: 10it [00:06,  1.53it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.51it/s]Extractor Predicting: 14it [00:09,  1.51it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.58it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.55it/s]Extractor Predicting: 23it [00:14,  1.57it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.50it/s]Extractor Predicting: 27it [00:17,  1.49it/s]Extractor Predicting: 28it [00:18,  1.51it/s]Extractor Predicting: 29it [00:18,  1.52it/s]Extractor Predicting: 30it [00:19,  1.51it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:22,  1.60it/s]Extractor Predicting: 35it [00:22,  1.62it/s]Extractor Predicting: 36it [00:23,  1.64it/s]Extractor Predicting: 37it [00:23,  1.65it/s]Extractor Predicting: 38it [00:24,  1.61it/s]Extractor Predicting: 39it [00:25,  1.64it/s]Extractor Predicting: 40it [00:25,  1.64it/s]Extractor Predicting: 41it [00:26,  1.61it/s]Extractor Predicting: 42it [00:26,  1.66it/s]Extractor Predicting: 43it [00:27,  1.63it/s]Extractor Predicting: 44it [00:28,  1.63it/s]Extractor Predicting: 45it [00:28,  1.63it/s]Extractor Predicting: 46it [00:29,  1.64it/s]Extractor Predicting: 47it [00:29,  1.69it/s]Extractor Predicting: 48it [00:30,  1.68it/s]Extractor Predicting: 49it [00:31,  1.69it/s]Extractor Predicting: 50it [00:31,  1.67it/s]Extractor Predicting: 51it [00:32,  1.65it/s]Extractor Predicting: 52it [00:32,  1.63it/s]Extractor Predicting: 53it [00:33,  1.61it/s]Extractor Predicting: 54it [00:34,  1.64it/s]Extractor Predicting: 55it [00:34,  1.63it/s]Extractor Predicting: 56it [00:35,  1.66it/s]Extractor Predicting: 57it [00:35,  1.69it/s]Extractor Predicting: 58it [00:36,  1.67it/s]Extractor Predicting: 59it [00:37,  1.71it/s]Extractor Predicting: 60it [00:37,  1.65it/s]Extractor Predicting: 61it [00:38,  1.67it/s]Extractor Predicting: 62it [00:38,  1.72it/s]Extractor Predicting: 63it [00:39,  1.72it/s]Extractor Predicting: 64it [00:39,  1.76it/s]Extractor Predicting: 65it [00:40,  1.77it/s]Extractor Predicting: 66it [00:41,  1.79it/s]Extractor Predicting: 67it [00:41,  1.77it/s]Extractor Predicting: 68it [00:42,  1.76it/s]Extractor Predicting: 69it [00:42,  1.79it/s]Extractor Predicting: 70it [00:43,  1.77it/s]Extractor Predicting: 71it [00:43,  1.74it/s]Extractor Predicting: 72it [00:44,  1.77it/s]Extractor Predicting: 73it [00:45,  1.69it/s]Extractor Predicting: 74it [00:45,  1.68it/s]Extractor Predicting: 75it [00:46,  1.74it/s]Extractor Predicting: 76it [00:46,  1.74it/s]Extractor Predicting: 77it [00:47,  1.75it/s]Extractor Predicting: 78it [00:47,  1.75it/s]Extractor Predicting: 79it [00:48,  1.74it/s]Extractor Predicting: 80it [00:49,  1.73it/s]Extractor Predicting: 81it [00:50,  1.52it/s]Extractor Predicting: 82it [00:50,  1.51it/s]Extractor Predicting: 83it [00:51,  1.55it/s]Extractor Predicting: 84it [00:51,  1.57it/s]Extractor Predicting: 85it [00:52,  1.55it/s]Extractor Predicting: 86it [00:53,  1.56it/s]Extractor Predicting: 87it [00:53,  1.59it/s]Extractor Predicting: 88it [00:54,  1.57it/s]Extractor Predicting: 89it [00:55,  1.56it/s]Extractor Predicting: 90it [00:55,  1.56it/s]Extractor Predicting: 91it [00:56,  1.58it/s]Extractor Predicting: 92it [00:57,  1.57it/s]Extractor Predicting: 93it [00:57,  1.56it/s]Extractor Predicting: 94it [00:58,  1.57it/s]Extractor Predicting: 95it [00:58,  1.56it/s]Extractor Predicting: 96it [00:59,  1.58it/s]Extractor Predicting: 97it [01:00,  1.56it/s]Extractor Predicting: 98it [01:00,  1.55it/s]Extractor Predicting: 99it [01:01,  1.57it/s]Extractor Predicting: 100it [01:02,  1.58it/s]Extractor Predicting: 101it [01:02,  1.58it/s]Extractor Predicting: 102it [01:03,  1.59it/s]Extractor Predicting: 103it [01:03,  1.60it/s]Extractor Predicting: 104it [01:04,  1.63it/s]Extractor Predicting: 105it [01:05,  1.64it/s]Extractor Predicting: 106it [01:05,  1.62it/s]Extractor Predicting: 107it [01:06,  1.61it/s]Extractor Predicting: 108it [01:07,  1.61it/s]Extractor Predicting: 109it [01:07,  1.59it/s]Extractor Predicting: 110it [01:08,  1.58it/s]Extractor Predicting: 111it [01:08,  1.56it/s]Extractor Predicting: 112it [01:09,  1.59it/s]Extractor Predicting: 113it [01:10,  1.61it/s]Extractor Predicting: 114it [01:10,  1.60it/s]Extractor Predicting: 115it [01:11,  1.60it/s]Extractor Predicting: 116it [01:12,  1.62it/s]Extractor Predicting: 117it [01:12,  1.63it/s]Extractor Predicting: 118it [01:13,  1.61it/s]Extractor Predicting: 119it [01:13,  1.62it/s]Extractor Predicting: 120it [01:14,  1.60it/s]Extractor Predicting: 121it [01:15,  1.59it/s]Extractor Predicting: 122it [01:15,  1.58it/s]Extractor Predicting: 123it [01:16,  1.58it/s]Extractor Predicting: 124it [01:17,  1.55it/s]Extractor Predicting: 125it [01:17,  1.58it/s]Extractor Predicting: 126it [01:18,  1.58it/s]Extractor Predicting: 127it [01:19,  1.57it/s]Extractor Predicting: 128it [01:19,  1.55it/s]Extractor Predicting: 129it [01:20,  1.57it/s]Extractor Predicting: 130it [01:20,  1.60it/s]Extractor Predicting: 131it [01:21,  1.57it/s]Extractor Predicting: 132it [01:22,  1.60it/s]Extractor Predicting: 133it [01:22,  1.62it/s]Extractor Predicting: 134it [01:23,  1.60it/s]Extractor Predicting: 135it [01:24,  1.58it/s]Extractor Predicting: 136it [01:24,  1.59it/s]Extractor Predicting: 137it [01:25,  1.62it/s]Extractor Predicting: 138it [01:25,  1.63it/s]Extractor Predicting: 139it [01:26,  1.64it/s]Extractor Predicting: 140it [01:27,  1.65it/s]Extractor Predicting: 141it [01:27,  1.69it/s]Extractor Predicting: 141it [01:27,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:56,914 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:56,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:56,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:56,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:15:56,918 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:15:57,572 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:15:57,573 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:15:58,161 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:15:59,191 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:15:59,192 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:00,803 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:00,808 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:00,808 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:00,808 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:16:00,808 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:16:01,140 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:16:01,141 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:16:01,404 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:16:01,536 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:16:01,536 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.25078963992419456,
  "recall": 0.11520603598374927,
  "score": 0.15788427122688406,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27658
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27758, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.68it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.69it/s]Extractor Predicting: 6it [00:03,  1.63it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.67it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.72it/s]Extractor Predicting: 11it [00:06,  1.69it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.68it/s]Extractor Predicting: 15it [00:08,  1.67it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:10,  1.62it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:13,  1.60it/s]Extractor Predicting: 24it [00:14,  1.57it/s]Extractor Predicting: 25it [00:15,  1.59it/s]Extractor Predicting: 26it [00:15,  1.60it/s]Extractor Predicting: 27it [00:16,  1.58it/s]Extractor Predicting: 28it [00:17,  1.59it/s]Extractor Predicting: 29it [00:17,  1.55it/s]Extractor Predicting: 30it [00:18,  1.51it/s]Extractor Predicting: 31it [00:19,  1.52it/s]Extractor Predicting: 32it [00:19,  1.52it/s]Extractor Predicting: 33it [00:20,  1.53it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:21,  1.57it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.64it/s]Extractor Predicting: 39it [00:24,  1.63it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.63it/s]Extractor Predicting: 42it [00:25,  1.62it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:27,  1.62it/s]Extractor Predicting: 45it [00:27,  1.59it/s]Extractor Predicting: 46it [00:28,  1.63it/s]Extractor Predicting: 47it [00:29,  1.62it/s]Extractor Predicting: 48it [00:29,  1.62it/s]Extractor Predicting: 49it [00:30,  1.61it/s]Extractor Predicting: 50it [00:30,  1.59it/s]Extractor Predicting: 51it [00:31,  1.45it/s]Extractor Predicting: 52it [00:32,  1.49it/s]Extractor Predicting: 53it [00:32,  1.52it/s]Extractor Predicting: 54it [00:33,  1.55it/s]Extractor Predicting: 55it [00:34,  1.53it/s]Extractor Predicting: 56it [00:34,  1.56it/s]Extractor Predicting: 57it [00:35,  1.60it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:36,  1.63it/s]Extractor Predicting: 60it [00:37,  1.64it/s]Extractor Predicting: 61it [00:37,  1.63it/s]Extractor Predicting: 62it [00:38,  1.63it/s]Extractor Predicting: 63it [00:39,  1.59it/s]Extractor Predicting: 64it [00:39,  1.62it/s]Extractor Predicting: 65it [00:40,  1.59it/s]Extractor Predicting: 66it [00:41,  1.57it/s]Extractor Predicting: 67it [00:41,  1.61it/s]Extractor Predicting: 68it [00:42,  1.63it/s]Extractor Predicting: 69it [00:42,  1.65it/s]Extractor Predicting: 70it [00:43,  1.63it/s]Extractor Predicting: 71it [00:44,  1.60it/s]Extractor Predicting: 72it [00:44,  1.61it/s]Extractor Predicting: 73it [00:45,  1.57it/s]Extractor Predicting: 74it [00:46,  1.58it/s]Extractor Predicting: 75it [00:46,  1.59it/s]Extractor Predicting: 76it [00:47,  1.61it/s]Extractor Predicting: 77it [00:47,  1.57it/s]Extractor Predicting: 78it [00:48,  1.57it/s]Extractor Predicting: 79it [00:49,  1.58it/s]Extractor Predicting: 80it [00:49,  1.57it/s]Extractor Predicting: 81it [00:50,  1.58it/s]Extractor Predicting: 82it [00:51,  1.58it/s]Extractor Predicting: 83it [00:51,  1.56it/s]Extractor Predicting: 84it [00:52,  1.58it/s]Extractor Predicting: 85it [00:53,  1.59it/s]Extractor Predicting: 86it [00:53,  1.59it/s]Extractor Predicting: 87it [00:54,  1.58it/s]Extractor Predicting: 88it [00:54,  1.59it/s]Extractor Predicting: 89it [00:55,  1.56it/s]Extractor Predicting: 90it [00:56,  1.56it/s]Extractor Predicting: 91it [00:56,  1.57it/s]Extractor Predicting: 92it [00:57,  1.57it/s]Extractor Predicting: 93it [00:58,  1.56it/s]Extractor Predicting: 94it [00:58,  1.52it/s]Extractor Predicting: 95it [00:59,  1.52it/s]Extractor Predicting: 96it [01:00,  1.54it/s]Extractor Predicting: 97it [01:00,  1.55it/s]Extractor Predicting: 98it [01:01,  1.57it/s]Extractor Predicting: 99it [01:02,  1.55it/s]Extractor Predicting: 100it [01:02,  1.55it/s]Extractor Predicting: 101it [01:03,  1.57it/s]Extractor Predicting: 102it [01:03,  1.57it/s]Extractor Predicting: 103it [01:04,  1.56it/s]Extractor Predicting: 104it [01:05,  1.55it/s]Extractor Predicting: 105it [01:05,  1.56it/s]Extractor Predicting: 106it [01:06,  1.57it/s]Extractor Predicting: 107it [01:07,  1.52it/s]Extractor Predicting: 108it [01:07,  1.53it/s]Extractor Predicting: 109it [01:08,  1.54it/s]Extractor Predicting: 110it [01:09,  1.53it/s]Extractor Predicting: 111it [01:09,  1.53it/s]Extractor Predicting: 112it [01:10,  1.54it/s]Extractor Predicting: 113it [01:11,  1.59it/s]Extractor Predicting: 114it [01:11,  1.61it/s]Extractor Predicting: 115it [01:12,  1.58it/s]Extractor Predicting: 116it [01:12,  1.59it/s]Extractor Predicting: 117it [01:13,  1.64it/s]Extractor Predicting: 118it [01:14,  1.62it/s]Extractor Predicting: 119it [01:14,  1.63it/s]Extractor Predicting: 120it [01:15,  1.63it/s]Extractor Predicting: 121it [01:15,  1.64it/s]Extractor Predicting: 122it [01:16,  1.68it/s]Extractor Predicting: 123it [01:17,  1.67it/s]Extractor Predicting: 124it [01:17,  1.66it/s]Extractor Predicting: 125it [01:18,  1.64it/s]Extractor Predicting: 126it [01:18,  1.62it/s]Extractor Predicting: 127it [01:19,  1.64it/s]Extractor Predicting: 128it [01:20,  1.67it/s]Extractor Predicting: 129it [01:20,  1.62it/s]Extractor Predicting: 130it [01:21,  1.62it/s]Extractor Predicting: 131it [01:22,  1.60it/s]Extractor Predicting: 132it [01:22,  1.60it/s]Extractor Predicting: 133it [01:23,  1.60it/s]Extractor Predicting: 134it [01:23,  1.60it/s]Extractor Predicting: 135it [01:24,  1.61it/s]Extractor Predicting: 136it [01:25,  1.60it/s]Extractor Predicting: 137it [01:25,  1.60it/s]Extractor Predicting: 138it [01:26,  1.59it/s]Extractor Predicting: 139it [01:27,  1.62it/s]Extractor Predicting: 140it [01:27,  1.41it/s]Extractor Predicting: 141it [01:28,  1.45it/s]Extractor Predicting: 142it [01:29,  1.50it/s]Extractor Predicting: 143it [01:29,  1.51it/s]Extractor Predicting: 144it [01:30,  1.51it/s]Extractor Predicting: 145it [01:31,  1.56it/s]Extractor Predicting: 146it [01:31,  1.56it/s]Extractor Predicting: 147it [01:32,  1.56it/s]Extractor Predicting: 148it [01:33,  1.56it/s]Extractor Predicting: 149it [01:33,  1.56it/s]Extractor Predicting: 150it [01:34,  1.58it/s]Extractor Predicting: 151it [01:34,  1.61it/s]Extractor Predicting: 152it [01:35,  1.63it/s]Extractor Predicting: 153it [01:36,  1.63it/s]Extractor Predicting: 154it [01:36,  1.60it/s]Extractor Predicting: 155it [01:37,  1.60it/s]Extractor Predicting: 156it [01:38,  1.58it/s]Extractor Predicting: 157it [01:38,  1.57it/s]Extractor Predicting: 158it [01:39,  1.55it/s]Extractor Predicting: 159it [01:39,  1.56it/s]Extractor Predicting: 160it [01:40,  1.57it/s]Extractor Predicting: 161it [01:41,  1.59it/s]Extractor Predicting: 162it [01:41,  1.61it/s]Extractor Predicting: 163it [01:42,  1.61it/s]Extractor Predicting: 164it [01:43,  1.61it/s]Extractor Predicting: 165it [01:43,  1.59it/s]Extractor Predicting: 166it [01:44,  1.60it/s]Extractor Predicting: 167it [01:44,  1.63it/s]Extractor Predicting: 168it [01:45,  1.61it/s]Extractor Predicting: 169it [01:46,  1.60it/s]Extractor Predicting: 170it [01:46,  1.59it/s]Extractor Predicting: 171it [01:47,  1.55it/s]Extractor Predicting: 172it [01:48,  1.54it/s]Extractor Predicting: 173it [01:48,  1.53it/s]Extractor Predicting: 174it [01:49,  1.50it/s]Extractor Predicting: 175it [01:50,  1.47it/s]Extractor Predicting: 176it [01:50,  1.50it/s]Extractor Predicting: 177it [01:51,  1.53it/s]Extractor Predicting: 178it [01:52,  1.54it/s]Extractor Predicting: 179it [01:52,  1.58it/s]Extractor Predicting: 180it [01:53,  1.56it/s]Extractor Predicting: 181it [01:53,  1.60it/s]Extractor Predicting: 182it [01:54,  1.60it/s]Extractor Predicting: 183it [01:55,  1.60it/s]Extractor Predicting: 184it [01:55,  1.62it/s]Extractor Predicting: 185it [01:56,  1.64it/s]Extractor Predicting: 186it [01:56,  1.66it/s]Extractor Predicting: 187it [01:57,  1.64it/s]Extractor Predicting: 188it [01:58,  1.65it/s]Extractor Predicting: 189it [01:58,  1.66it/s]Extractor Predicting: 190it [01:59,  1.65it/s]Extractor Predicting: 191it [02:00,  1.61it/s]Extractor Predicting: 192it [02:00,  1.59it/s]Extractor Predicting: 193it [02:01,  1.59it/s]Extractor Predicting: 194it [02:01,  1.63it/s]Extractor Predicting: 195it [02:02,  1.63it/s]Extractor Predicting: 196it [02:03,  1.64it/s]Extractor Predicting: 197it [02:03,  1.65it/s]Extractor Predicting: 198it [02:04,  1.65it/s]Extractor Predicting: 199it [02:04,  1.64it/s]Extractor Predicting: 200it [02:05,  1.63it/s]Extractor Predicting: 201it [02:06,  1.62it/s]Extractor Predicting: 202it [02:06,  1.64it/s]Extractor Predicting: 203it [02:07,  1.68it/s]Extractor Predicting: 204it [02:07,  1.68it/s]Extractor Predicting: 205it [02:08,  1.67it/s]Extractor Predicting: 206it [02:09,  1.65it/s]Extractor Predicting: 207it [02:09,  1.61it/s]Extractor Predicting: 208it [02:10,  1.61it/s]Extractor Predicting: 209it [02:11,  1.62it/s]Extractor Predicting: 210it [02:11,  1.61it/s]Extractor Predicting: 211it [02:12,  1.64it/s]Extractor Predicting: 212it [02:12,  1.61it/s]Extractor Predicting: 213it [02:13,  1.65it/s]Extractor Predicting: 214it [02:14,  1.63it/s]Extractor Predicting: 215it [02:14,  1.66it/s]Extractor Predicting: 216it [02:15,  1.66it/s]Extractor Predicting: 217it [02:16,  1.59it/s]Extractor Predicting: 218it [02:16,  1.61it/s]Extractor Predicting: 219it [02:17,  1.64it/s]Extractor Predicting: 220it [02:17,  1.66it/s]Extractor Predicting: 221it [02:18,  1.62it/s]Extractor Predicting: 222it [02:19,  1.64it/s]Extractor Predicting: 223it [02:19,  1.66it/s]Extractor Predicting: 224it [02:20,  1.67it/s]Extractor Predicting: 225it [02:20,  1.64it/s]Extractor Predicting: 226it [02:21,  1.60it/s]Extractor Predicting: 227it [02:22,  1.60it/s]Extractor Predicting: 228it [02:22,  1.56it/s]Extractor Predicting: 229it [02:23,  1.56it/s]Extractor Predicting: 230it [02:24,  1.59it/s]Extractor Predicting: 231it [02:24,  1.57it/s]Extractor Predicting: 232it [02:25,  1.58it/s]Extractor Predicting: 233it [02:25,  1.59it/s]Extractor Predicting: 234it [02:26,  1.60it/s]Extractor Predicting: 235it [02:27,  1.57it/s]Extractor Predicting: 236it [02:27,  1.59it/s]Extractor Predicting: 237it [02:28,  1.64it/s]Extractor Predicting: 238it [02:29,  1.61it/s]Extractor Predicting: 239it [02:29,  1.58it/s]Extractor Predicting: 240it [02:30,  1.57it/s]Extractor Predicting: 241it [02:30,  1.58it/s]Extractor Predicting: 242it [02:31,  1.61it/s]Extractor Predicting: 243it [02:32,  1.61it/s]Extractor Predicting: 244it [02:33,  1.39it/s]Extractor Predicting: 245it [02:33,  1.45it/s]Extractor Predicting: 246it [02:34,  1.50it/s]Extractor Predicting: 247it [02:34,  1.54it/s]Extractor Predicting: 248it [02:35,  1.50it/s]Extractor Predicting: 249it [02:36,  1.53it/s]Extractor Predicting: 250it [02:36,  1.54it/s]Extractor Predicting: 251it [02:37,  1.52it/s]Extractor Predicting: 252it [02:38,  1.52it/s]Extractor Predicting: 253it [02:38,  1.53it/s]Extractor Predicting: 254it [02:39,  1.57it/s]Extractor Predicting: 255it [02:40,  1.57it/s]Extractor Predicting: 256it [02:40,  1.61it/s]Extractor Predicting: 257it [02:41,  1.64it/s]Extractor Predicting: 258it [02:41,  1.61it/s]Extractor Predicting: 259it [02:42,  1.61it/s]Extractor Predicting: 260it [02:43,  1.58it/s]Extractor Predicting: 261it [02:43,  1.57it/s]Extractor Predicting: 262it [02:44,  1.58it/s]Extractor Predicting: 263it [02:45,  1.52it/s]Extractor Predicting: 264it [02:45,  1.53it/s]Extractor Predicting: 265it [02:46,  1.55it/s]Extractor Predicting: 266it [02:47,  1.57it/s]Extractor Predicting: 267it [02:47,  1.57it/s]Extractor Predicting: 268it [02:48,  1.58it/s]Extractor Predicting: 269it [02:48,  1.62it/s]Extractor Predicting: 270it [02:49,  1.64it/s]Extractor Predicting: 271it [02:50,  1.60it/s]Extractor Predicting: 272it [02:50,  1.62it/s]Extractor Predicting: 273it [02:51,  1.61it/s]Extractor Predicting: 274it [02:52,  1.60it/s]Extractor Predicting: 275it [02:52,  1.61it/s]Extractor Predicting: 276it [02:53,  1.61it/s]Extractor Predicting: 277it [02:53,  1.59it/s]Extractor Predicting: 278it [02:54,  1.62it/s]Extractor Predicting: 279it [02:55,  1.58it/s]Extractor Predicting: 280it [02:55,  1.58it/s]Extractor Predicting: 281it [02:56,  1.59it/s]Extractor Predicting: 282it [02:57,  1.61it/s]Extractor Predicting: 283it [02:57,  1.61it/s]Extractor Predicting: 284it [02:58,  1.58it/s]Extractor Predicting: 285it [02:58,  1.58it/s]Extractor Predicting: 286it [02:59,  1.60it/s]Extractor Predicting: 287it [03:00,  1.62it/s]Extractor Predicting: 288it [03:00,  1.61it/s]Extractor Predicting: 289it [03:01,  1.57it/s]Extractor Predicting: 290it [03:02,  1.63it/s]Extractor Predicting: 291it [03:02,  1.61it/s]Extractor Predicting: 292it [03:03,  1.60it/s]Extractor Predicting: 293it [03:03,  1.56it/s]Extractor Predicting: 294it [03:04,  1.53it/s]Extractor Predicting: 295it [03:05,  1.58it/s]Extractor Predicting: 296it [03:05,  1.56it/s]Extractor Predicting: 297it [03:06,  1.59it/s]Extractor Predicting: 298it [03:07,  1.54it/s]Extractor Predicting: 299it [03:07,  1.55it/s]Extractor Predicting: 300it [03:08,  1.57it/s]Extractor Predicting: 301it [03:09,  1.57it/s]Extractor Predicting: 302it [03:09,  1.56it/s]Extractor Predicting: 303it [03:10,  1.55it/s]Extractor Predicting: 304it [03:11,  1.52it/s]Extractor Predicting: 305it [03:11,  1.54it/s]Extractor Predicting: 306it [03:12,  1.52it/s]Extractor Predicting: 307it [03:13,  1.51it/s]Extractor Predicting: 308it [03:13,  1.54it/s]Extractor Predicting: 309it [03:14,  1.53it/s]Extractor Predicting: 310it [03:15,  1.51it/s]Extractor Predicting: 311it [03:15,  1.50it/s]Extractor Predicting: 312it [03:16,  1.52it/s]Extractor Predicting: 313it [03:16,  1.53it/s]Extractor Predicting: 314it [03:17,  1.49it/s]Extractor Predicting: 315it [03:18,  1.54it/s]Extractor Predicting: 316it [03:18,  1.52it/s]Extractor Predicting: 317it [03:19,  1.51it/s]Extractor Predicting: 318it [03:20,  1.53it/s]Extractor Predicting: 319it [03:20,  1.53it/s]Extractor Predicting: 320it [03:21,  1.62it/s]Extractor Predicting: 321it [03:22,  1.66it/s]Extractor Predicting: 322it [03:22,  1.74it/s]Extractor Predicting: 323it [03:23,  1.77it/s]Extractor Predicting: 324it [03:23,  1.79it/s]Extractor Predicting: 325it [03:24,  1.80it/s]Extractor Predicting: 326it [03:24,  1.80it/s]Extractor Predicting: 327it [03:25,  1.79it/s]Extractor Predicting: 328it [03:25,  1.80it/s]Extractor Predicting: 329it [03:26,  1.84it/s]Extractor Predicting: 330it [03:26,  1.83it/s]Extractor Predicting: 331it [03:27,  1.88it/s]Extractor Predicting: 332it [03:27,  1.85it/s]Extractor Predicting: 333it [03:28,  1.88it/s]Extractor Predicting: 334it [03:29,  1.87it/s]Extractor Predicting: 335it [03:29,  1.87it/s]Extractor Predicting: 336it [03:30,  1.84it/s]Extractor Predicting: 337it [03:30,  1.87it/s]Extractor Predicting: 338it [03:31,  1.81it/s]Extractor Predicting: 339it [03:31,  1.79it/s]Extractor Predicting: 340it [03:32,  1.85it/s]Extractor Predicting: 341it [03:32,  1.83it/s]Extractor Predicting: 342it [03:33,  1.85it/s]Extractor Predicting: 343it [03:33,  1.84it/s]Extractor Predicting: 344it [03:34,  1.81it/s]Extractor Predicting: 345it [03:35,  1.83it/s]Extractor Predicting: 346it [03:35,  1.83it/s]Extractor Predicting: 347it [03:36,  1.80it/s]Extractor Predicting: 348it [03:36,  1.76it/s]Extractor Predicting: 349it [03:37,  1.71it/s]Extractor Predicting: 350it [03:38,  1.67it/s]Extractor Predicting: 351it [03:38,  1.64it/s]Extractor Predicting: 352it [03:39,  1.64it/s]Extractor Predicting: 353it [03:39,  1.66it/s]Extractor Predicting: 354it [03:40,  1.64it/s]Extractor Predicting: 355it [03:41,  1.43it/s]Extractor Predicting: 356it [03:42,  1.45it/s]Extractor Predicting: 357it [03:42,  1.46it/s]Extractor Predicting: 358it [03:43,  1.51it/s]Extractor Predicting: 359it [03:43,  1.53it/s]Extractor Predicting: 360it [03:44,  1.56it/s]Extractor Predicting: 361it [03:45,  1.56it/s]Extractor Predicting: 362it [03:45,  1.59it/s]Extractor Predicting: 363it [03:46,  1.58it/s]Extractor Predicting: 364it [03:47,  1.56it/s]Extractor Predicting: 365it [03:47,  1.57it/s]Extractor Predicting: 366it [03:48,  1.56it/s]Extractor Predicting: 367it [03:49,  1.56it/s]Extractor Predicting: 368it [03:49,  1.57it/s]Extractor Predicting: 369it [03:50,  1.57it/s]Extractor Predicting: 370it [03:50,  1.56it/s]Extractor Predicting: 371it [03:51,  1.57it/s]Extractor Predicting: 372it [03:52,  1.57it/s]Extractor Predicting: 373it [03:52,  1.55it/s]Extractor Predicting: 374it [03:53,  1.57it/s]Extractor Predicting: 375it [03:54,  1.58it/s]Extractor Predicting: 376it [03:54,  1.56it/s]Extractor Predicting: 377it [03:55,  1.57it/s]Extractor Predicting: 378it [03:56,  1.54it/s]Extractor Predicting: 379it [03:56,  1.54it/s]Extractor Predicting: 380it [03:57,  1.54it/s]Extractor Predicting: 381it [03:58,  1.52it/s]Extractor Predicting: 382it [03:58,  1.52it/s]Extractor Predicting: 383it [03:59,  1.53it/s]Extractor Predicting: 384it [04:00,  1.52it/s]Extractor Predicting: 385it [04:00,  1.52it/s]Extractor Predicting: 386it [04:01,  1.54it/s]Extractor Predicting: 387it [04:02,  1.52it/s]Extractor Predicting: 388it [04:02,  1.51it/s]Extractor Predicting: 389it [04:03,  1.54it/s]Extractor Predicting: 390it [04:03,  1.55it/s]Extractor Predicting: 391it [04:04,  1.55it/s]Extractor Predicting: 392it [04:05,  1.51it/s]Extractor Predicting: 393it [04:05,  1.56it/s]Extractor Predicting: 394it [04:06,  1.55it/s]Extractor Predicting: 395it [04:07,  1.53it/s]Extractor Predicting: 396it [04:07,  1.53it/s]Extractor Predicting: 397it [04:08,  1.50it/s]Extractor Predicting: 398it [04:09,  1.50it/s]Extractor Predicting: 399it [04:09,  1.45it/s]Extractor Predicting: 400it [04:10,  1.47it/s]Extractor Predicting: 401it [04:11,  1.48it/s]Extractor Predicting: 402it [04:11,  1.51it/s]Extractor Predicting: 403it [04:12,  1.51it/s]Extractor Predicting: 404it [04:13,  1.56it/s]Extractor Predicting: 405it [04:13,  1.55it/s]Extractor Predicting: 406it [04:14,  1.59it/s]Extractor Predicting: 407it [04:15,  1.58it/s]Extractor Predicting: 408it [04:15,  1.56it/s]Extractor Predicting: 409it [04:16,  1.60it/s]Extractor Predicting: 410it [04:16,  1.59it/s]Extractor Predicting: 411it [04:17,  1.62it/s]Extractor Predicting: 412it [04:18,  1.59it/s]Extractor Predicting: 413it [04:18,  1.62it/s]Extractor Predicting: 414it [04:19,  1.61it/s]Extractor Predicting: 415it [04:20,  1.61it/s]Extractor Predicting: 416it [04:20,  1.58it/s]Extractor Predicting: 417it [04:21,  1.60it/s]Extractor Predicting: 418it [04:21,  1.60it/s]Extractor Predicting: 419it [04:22,  1.58it/s]Extractor Predicting: 420it [04:23,  1.59it/s]Extractor Predicting: 421it [04:23,  1.59it/s]Extractor Predicting: 422it [04:24,  1.57it/s]Extractor Predicting: 423it [04:25,  1.58it/s]Extractor Predicting: 424it [04:25,  1.54it/s]Extractor Predicting: 425it [04:26,  1.56it/s]Extractor Predicting: 426it [04:27,  1.55it/s]Extractor Predicting: 427it [04:27,  1.57it/s]Extractor Predicting: 428it [04:28,  1.56it/s]Extractor Predicting: 429it [04:28,  1.57it/s]Extractor Predicting: 430it [04:29,  1.96it/s]Extractor Predicting: 430it [04:29,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:38,619 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:38,624 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:38,624 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:38,624 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:38,624 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-29 09:20:38,896 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-29 09:20:38,897 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:20:39,154 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-29 09:20:40,194 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:20:40,194 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:41,528 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:41,530 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:41,530 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:41,530 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-29 09:20:41,530 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-29 09:20:41,842 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-29 09:20:41,847 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-29 09:20:42,100 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-29 09:20:42,240 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-29 09:20:42,240 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.26130434782608697,
  "recall": 0.1167216935327248,
  "score": 0.1613639414686535,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 1134
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 1234, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:02,  2.00it/s]Extractor Predicting: 5it [00:02,  1.72it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.3333333333333333,
  "recall": 0.06435643564356436,
  "score": 0.1078838174273859,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_15_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/', 'labels': ['architect', 'constellation', 'contains administrative territorial entity', 'country of origin', 'developer', 'follows', 'league', 'located in or next to body of water', 'member of', 'member of political party', 'notable work', 'operator', 'original broadcaster', 'position held', 'residence'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_withTrainFalse_synthetic_large/unseen_15_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
