/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_15_seed_0', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 64330
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 64430, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/model', pretrained_wv='outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=64430, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.333, loss:49294.6825
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.058, loss:2542.5695
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.062, loss:2149.1682
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.049, loss:2173.5803
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.056, loss:2003.3187
>> valid entity prec:0.3264, rec:0.3687, f1:0.3463
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 2.526, loss:1926.0282
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 1.049, loss:1811.1571
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 1.030, loss:1658.3978
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 1.037, loss:1641.3321
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 1.030, loss:1470.1337
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6019, rec:0.5130, f1:0.5539
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 1100, avg_time 2.449, loss:1412.0783
g_step 1200, step 1200, avg_time 1.035, loss:1368.2089
g_step 1300, step 1300, avg_time 1.026, loss:1299.9654
g_step 1400, step 1400, avg_time 1.029, loss:1237.4415
g_step 1500, step 1500, avg_time 1.035, loss:1278.8848
>> valid entity prec:0.5000, rec:0.6077, f1:0.5486
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 1600, avg_time 2.459, loss:1231.2675
g_step 1700, step 10, avg_time 1.038, loss:1189.1927
g_step 1800, step 110, avg_time 1.030, loss:1171.1102
g_step 1900, step 210, avg_time 1.032, loss:1135.9262
g_step 2000, step 310, avg_time 1.032, loss:1110.8924
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6783, rec:0.6024, f1:0.6381
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 410, avg_time 2.452, loss:1129.8902
g_step 2200, step 510, avg_time 1.030, loss:1113.2259
g_step 2300, step 610, avg_time 1.027, loss:1062.1713
g_step 2400, step 710, avg_time 1.027, loss:1053.4689
g_step 2500, step 810, avg_time 1.033, loss:1070.5472
>> valid entity prec:0.5645, rec:0.6453, f1:0.6022
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 910, avg_time 2.458, loss:1039.3809
g_step 2700, step 1010, avg_time 1.031, loss:1047.1887
g_step 2800, step 1110, avg_time 1.033, loss:1024.7447
g_step 2900, step 1210, avg_time 1.030, loss:995.7320
g_step 3000, step 1310, avg_time 1.026, loss:1016.8378
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6378, rec:0.6337, f1:0.6357
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 1410, avg_time 2.453, loss:1002.5853
g_step 3200, step 1510, avg_time 1.037, loss:1007.0771
g_step 3300, step 1610, avg_time 1.029, loss:965.1525
g_step 3400, step 20, avg_time 1.026, loss:984.3817
g_step 3500, step 120, avg_time 1.032, loss:939.8344
>> valid entity prec:0.6000, rec:0.6277, f1:0.6135
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 220, avg_time 2.458, loss:921.4160
g_step 3700, step 320, avg_time 1.030, loss:947.9841
g_step 3800, step 420, avg_time 1.027, loss:947.3262
g_step 3900, step 520, avg_time 1.021, loss:957.1880
g_step 4000, step 620, avg_time 1.024, loss:948.0895
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6425, rec:0.6029, f1:0.6220
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 720, avg_time 2.459, loss:894.8341
g_step 4200, step 820, avg_time 1.022, loss:902.9382
g_step 4300, step 920, avg_time 1.028, loss:937.9763
g_step 4400, step 1020, avg_time 1.035, loss:907.5896
g_step 4500, step 1120, avg_time 1.033, loss:890.1836
>> valid entity prec:0.6480, rec:0.6371, f1:0.6425
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4600, step 1220, avg_time 2.457, loss:915.6989
g_step 4700, step 1320, avg_time 1.037, loss:897.4418
g_step 4800, step 1420, avg_time 1.034, loss:937.1365
g_step 4900, step 1520, avg_time 1.034, loss:896.1697
g_step 5000, step 1620, avg_time 1.033, loss:890.6741
learning rate was adjusted to 0.0008
>> valid entity prec:0.6576, rec:0.6148, f1:0.6354
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 30, avg_time 2.449, loss:869.5853
g_step 5200, step 130, avg_time 1.036, loss:878.6695
g_step 5300, step 230, avg_time 1.028, loss:843.8725
g_step 5400, step 330, avg_time 1.036, loss:838.5731
g_step 5500, step 430, avg_time 1.030, loss:820.5728
>> valid entity prec:0.6275, rec:0.6624, f1:0.6444
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5600, step 530, avg_time 2.459, loss:860.3501
g_step 5700, step 630, avg_time 1.033, loss:863.7385
g_step 5800, step 730, avg_time 1.026, loss:834.9791
g_step 5900, step 830, avg_time 1.032, loss:821.8094
g_step 6000, step 930, avg_time 1.036, loss:858.1394
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6295, rec:0.5983, f1:0.6135
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6100, step 1030, avg_time 2.456, loss:855.2357
g_step 6200, step 1130, avg_time 1.026, loss:826.8448
g_step 6300, step 1230, avg_time 1.033, loss:847.1823
g_step 6400, step 1330, avg_time 1.029, loss:854.7304
g_step 6500, step 1430, avg_time 1.033, loss:856.0583
>> valid entity prec:0.6321, rec:0.5827, f1:0.6064
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 1530, avg_time 2.447, loss:853.7759
g_step 6700, step 1630, avg_time 1.035, loss:881.3127
g_step 6800, step 40, avg_time 1.023, loss:809.9295
g_step 6900, step 140, avg_time 1.035, loss:800.0575
g_step 7000, step 240, avg_time 1.035, loss:836.9735
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6264, rec:0.6092, f1:0.6177
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 340, avg_time 2.453, loss:787.4718
g_step 7200, step 440, avg_time 1.033, loss:788.6541
g_step 7300, step 540, avg_time 1.031, loss:803.9503
g_step 7400, step 640, avg_time 1.031, loss:806.9937
g_step 7500, step 740, avg_time 1.026, loss:781.8989
>> valid entity prec:0.6864, rec:0.5808, f1:0.6292
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 840, avg_time 2.446, loss:780.3523
g_step 7700, step 940, avg_time 1.025, loss:803.5949
g_step 7800, step 1040, avg_time 1.024, loss:803.3773
g_step 7900, step 1140, avg_time 1.028, loss:796.7389
g_step 8000, step 1240, avg_time 1.037, loss:823.9943
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6394, rec:0.6601, f1:0.6495
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 8100, step 1340, avg_time 2.456, loss:823.8094
g_step 8200, step 1440, avg_time 1.038, loss:818.5852
g_step 8300, step 1540, avg_time 1.022, loss:781.5652
g_step 8400, step 1640, avg_time 1.031, loss:797.9570
g_step 8500, step 50, avg_time 1.030, loss:751.6384
>> valid entity prec:0.6557, rec:0.5837, f1:0.6176
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 150, avg_time 2.452, loss:777.5662
g_step 8700, step 250, avg_time 1.024, loss:767.0369
g_step 8800, step 350, avg_time 1.031, loss:783.9997
g_step 8900, step 450, avg_time 1.031, loss:769.5509
g_step 9000, step 550, avg_time 1.030, loss:800.0881
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.6542, rec:0.6466, f1:0.6504
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 9100, step 650, avg_time 2.484, loss:776.9642
g_step 9200, step 750, avg_time 1.026, loss:733.9862
g_step 9300, step 850, avg_time 1.030, loss:733.9045
g_step 9400, step 950, avg_time 1.025, loss:742.0142
g_step 9500, step 1050, avg_time 1.029, loss:794.2580
>> valid entity prec:0.6667, rec:0.6377, f1:0.6519
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 9600, step 1150, avg_time 2.457, loss:760.6245
g_step 9700, step 1250, avg_time 1.036, loss:758.3509
g_step 9800, step 1350, avg_time 1.031, loss:741.1438
g_step 9900, step 1450, avg_time 1.030, loss:768.8007
g_step 10000, step 1550, avg_time 1.034, loss:766.0593
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.6094, rec:0.6938, f1:0.6489
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.07s/it]Extractor Predicting: 2it [00:07,  3.26s/it]Extractor Predicting: 3it [00:08,  2.07s/it]Extractor Predicting: 4it [00:08,  1.50s/it]Extractor Predicting: 5it [00:09,  1.19s/it]Extractor Predicting: 6it [00:10,  1.04s/it]Extractor Predicting: 7it [00:11,  1.04s/it]Extractor Predicting: 8it [00:12,  1.09it/s]Extractor Predicting: 9it [00:12,  1.18it/s]Extractor Predicting: 10it [00:13,  1.26it/s]Extractor Predicting: 11it [00:14,  1.32it/s]Extractor Predicting: 12it [00:14,  1.38it/s]Extractor Predicting: 13it [00:15,  1.44it/s]Extractor Predicting: 14it [00:16,  1.46it/s]Extractor Predicting: 15it [00:16,  1.50it/s]Extractor Predicting: 16it [00:17,  1.49it/s]Extractor Predicting: 17it [00:17,  1.51it/s]Extractor Predicting: 18it [00:18,  1.52it/s]Extractor Predicting: 19it [00:19,  1.51it/s]Extractor Predicting: 20it [00:19,  1.48it/s]Extractor Predicting: 21it [00:20,  1.49it/s]Extractor Predicting: 22it [00:21,  1.47it/s]Extractor Predicting: 23it [00:21,  1.49it/s]Extractor Predicting: 24it [00:22,  1.54it/s]Extractor Predicting: 25it [00:23,  1.52it/s]Extractor Predicting: 26it [00:23,  1.57it/s]Extractor Predicting: 27it [00:24,  1.59it/s]Extractor Predicting: 28it [00:25,  1.59it/s]Extractor Predicting: 29it [00:25,  1.58it/s]Extractor Predicting: 30it [00:26,  1.59it/s]Extractor Predicting: 31it [00:26,  1.58it/s]Extractor Predicting: 32it [00:27,  1.55it/s]Extractor Predicting: 33it [00:28,  1.57it/s]Extractor Predicting: 34it [00:28,  1.53it/s]Extractor Predicting: 35it [00:29,  1.55it/s]Extractor Predicting: 36it [00:30,  1.53it/s]Extractor Predicting: 37it [00:30,  1.52it/s]Extractor Predicting: 38it [00:31,  1.49it/s]Extractor Predicting: 39it [00:32,  1.52it/s]Extractor Predicting: 40it [00:32,  1.51it/s]Extractor Predicting: 41it [00:33,  1.48it/s]Extractor Predicting: 42it [00:34,  1.48it/s]Extractor Predicting: 43it [00:34,  1.52it/s]Extractor Predicting: 44it [00:35,  1.50it/s]Extractor Predicting: 45it [00:36,  1.51it/s]Extractor Predicting: 46it [00:36,  1.54it/s]Extractor Predicting: 47it [00:37,  1.53it/s]Extractor Predicting: 48it [00:38,  1.50it/s]Extractor Predicting: 49it [00:38,  1.48it/s]Extractor Predicting: 50it [00:39,  1.46it/s]Extractor Predicting: 51it [00:40,  1.45it/s]Extractor Predicting: 52it [00:41,  1.44it/s]Extractor Predicting: 53it [00:41,  1.46it/s]Extractor Predicting: 54it [00:42,  1.46it/s]Extractor Predicting: 55it [00:43,  1.47it/s]Extractor Predicting: 56it [00:43,  1.48it/s]Extractor Predicting: 57it [00:44,  1.48it/s]Extractor Predicting: 58it [00:45,  1.47it/s]Extractor Predicting: 59it [00:45,  1.49it/s]Extractor Predicting: 60it [00:46,  1.45it/s]Extractor Predicting: 61it [00:47,  1.44it/s]Extractor Predicting: 62it [00:47,  1.42it/s]Extractor Predicting: 63it [00:48,  1.41it/s]Extractor Predicting: 64it [00:49,  1.42it/s]Extractor Predicting: 65it [00:50,  1.40it/s]Extractor Predicting: 66it [00:50,  1.39it/s]Extractor Predicting: 67it [00:51,  1.38it/s]Extractor Predicting: 68it [00:52,  1.38it/s]Extractor Predicting: 69it [00:53,  1.38it/s]Extractor Predicting: 70it [00:53,  1.38it/s]Extractor Predicting: 71it [00:54,  1.37it/s]Extractor Predicting: 72it [00:55,  1.37it/s]Extractor Predicting: 73it [00:55,  1.35it/s]Extractor Predicting: 74it [00:56,  1.36it/s]Extractor Predicting: 75it [00:57,  1.35it/s]Extractor Predicting: 76it [00:58,  1.38it/s]Extractor Predicting: 77it [00:58,  1.38it/s]Extractor Predicting: 78it [00:59,  1.37it/s]Extractor Predicting: 79it [01:00,  1.39it/s]Extractor Predicting: 80it [01:01,  1.37it/s]Extractor Predicting: 81it [01:01,  1.35it/s]Extractor Predicting: 82it [01:02,  1.35it/s]Extractor Predicting: 83it [01:03,  1.40it/s]Extractor Predicting: 84it [01:03,  1.40it/s]Extractor Predicting: 85it [01:04,  1.41it/s]Extractor Predicting: 86it [01:05,  1.40it/s]Extractor Predicting: 87it [01:06,  1.37it/s]Extractor Predicting: 88it [01:06,  1.28it/s]Extractor Predicting: 89it [01:07,  1.33it/s]Extractor Predicting: 90it [01:08,  1.36it/s]Extractor Predicting: 91it [01:09,  1.39it/s]Extractor Predicting: 92it [01:09,  1.38it/s]Extractor Predicting: 93it [01:10,  1.42it/s]Extractor Predicting: 94it [01:11,  1.43it/s]Extractor Predicting: 95it [01:11,  1.43it/s]Extractor Predicting: 96it [01:12,  1.42it/s]Extractor Predicting: 97it [01:13,  1.43it/s]Extractor Predicting: 98it [01:13,  1.42it/s]Extractor Predicting: 99it [01:14,  1.43it/s]Extractor Predicting: 100it [01:15,  1.44it/s]Extractor Predicting: 101it [01:16,  1.45it/s]Extractor Predicting: 102it [01:16,  1.42it/s]Extractor Predicting: 103it [01:17,  1.39it/s]Extractor Predicting: 104it [01:18,  1.40it/s]Extractor Predicting: 105it [01:18,  1.41it/s]Extractor Predicting: 106it [01:19,  1.41it/s]Extractor Predicting: 107it [01:20,  1.39it/s]Extractor Predicting: 108it [01:21,  1.40it/s]Extractor Predicting: 109it [01:21,  1.40it/s]Extractor Predicting: 110it [01:22,  1.38it/s]Extractor Predicting: 111it [01:23,  1.39it/s]Extractor Predicting: 112it [01:23,  1.39it/s]Extractor Predicting: 113it [01:24,  1.40it/s]Extractor Predicting: 114it [01:25,  1.40it/s]Extractor Predicting: 115it [01:26,  1.41it/s]Extractor Predicting: 116it [01:26,  1.40it/s]Extractor Predicting: 117it [01:27,  1.44it/s]Extractor Predicting: 118it [01:28,  1.46it/s]Extractor Predicting: 119it [01:28,  1.51it/s]Extractor Predicting: 120it [01:29,  1.51it/s]Extractor Predicting: 121it [01:30,  1.50it/s]Extractor Predicting: 122it [01:30,  1.54it/s]Extractor Predicting: 123it [01:31,  1.56it/s]Extractor Predicting: 124it [01:31,  1.54it/s]Extractor Predicting: 125it [01:32,  1.55it/s]Extractor Predicting: 126it [01:33,  1.57it/s]Extractor Predicting: 127it [01:33,  1.60it/s]Extractor Predicting: 128it [01:34,  1.58it/s]Extractor Predicting: 129it [01:35,  1.61it/s]Extractor Predicting: 130it [01:35,  1.55it/s]Extractor Predicting: 131it [01:36,  1.56it/s]Extractor Predicting: 132it [01:36,  1.61it/s]Extractor Predicting: 133it [01:37,  1.62it/s]Extractor Predicting: 134it [01:38,  1.55it/s]Extractor Predicting: 135it [01:38,  1.57it/s]Extractor Predicting: 136it [01:39,  1.60it/s]Extractor Predicting: 137it [01:40,  1.61it/s]Extractor Predicting: 138it [01:40,  1.60it/s]Extractor Predicting: 139it [01:41,  1.60it/s]Extractor Predicting: 140it [01:41,  1.64it/s]Extractor Predicting: 141it [01:42,  1.65it/s]Extractor Predicting: 142it [01:43,  1.68it/s]Extractor Predicting: 143it [01:43,  1.65it/s]Extractor Predicting: 144it [01:44,  1.62it/s]Extractor Predicting: 145it [01:44,  1.80it/s]Extractor Predicting: 145it [01:44,  1.38it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.50it/s]Extractor Predicting: 4it [00:02,  1.48it/s]Extractor Predicting: 5it [00:03,  1.46it/s]Extractor Predicting: 6it [00:04,  1.46it/s]Extractor Predicting: 7it [00:04,  1.47it/s]Extractor Predicting: 8it [00:05,  1.46it/s]Extractor Predicting: 9it [00:06,  1.45it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.50it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.50it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.45it/s]Extractor Predicting: 20it [00:13,  1.45it/s]Extractor Predicting: 21it [00:14,  1.47it/s]Extractor Predicting: 22it [00:14,  1.43it/s]Extractor Predicting: 23it [00:15,  1.45it/s]Extractor Predicting: 24it [00:16,  1.48it/s]Extractor Predicting: 25it [00:16,  1.49it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:19,  1.43it/s]Extractor Predicting: 30it [00:20,  1.40it/s]Extractor Predicting: 31it [00:21,  1.41it/s]Extractor Predicting: 32it [00:21,  1.41it/s]Extractor Predicting: 33it [00:22,  1.44it/s]Extractor Predicting: 34it [00:23,  1.45it/s]Extractor Predicting: 35it [00:23,  1.45it/s]Extractor Predicting: 36it [00:24,  1.47it/s]Extractor Predicting: 37it [00:25,  1.52it/s]Extractor Predicting: 38it [00:25,  1.51it/s]Extractor Predicting: 39it [00:26,  1.50it/s]Extractor Predicting: 40it [00:27,  1.51it/s]Extractor Predicting: 41it [00:27,  1.50it/s]Extractor Predicting: 42it [00:28,  1.49it/s]Extractor Predicting: 43it [00:29,  1.49it/s]Extractor Predicting: 44it [00:29,  1.49it/s]Extractor Predicting: 45it [00:30,  1.46it/s]Extractor Predicting: 46it [00:31,  1.50it/s]Extractor Predicting: 47it [00:31,  1.49it/s]Extractor Predicting: 48it [00:32,  1.49it/s]Extractor Predicting: 49it [00:33,  1.48it/s]Extractor Predicting: 50it [00:34,  1.47it/s]Extractor Predicting: 51it [00:34,  1.47it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:36,  1.48it/s]Extractor Predicting: 54it [00:36,  1.49it/s]Extractor Predicting: 55it [00:37,  1.46it/s]Extractor Predicting: 56it [00:38,  1.47it/s]Extractor Predicting: 57it [00:38,  1.50it/s]Extractor Predicting: 58it [00:39,  1.52it/s]Extractor Predicting: 59it [00:40,  1.52it/s]Extractor Predicting: 60it [00:40,  1.52it/s]Extractor Predicting: 61it [00:41,  1.50it/s]Extractor Predicting: 62it [00:42,  1.51it/s]Extractor Predicting: 63it [00:42,  1.49it/s]Extractor Predicting: 64it [00:43,  1.49it/s]Extractor Predicting: 65it [00:44,  1.47it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:45,  1.48it/s]Extractor Predicting: 68it [00:46,  1.51it/s]Extractor Predicting: 69it [00:46,  1.53it/s]Extractor Predicting: 70it [00:47,  1.52it/s]Extractor Predicting: 71it [00:48,  1.50it/s]Extractor Predicting: 72it [00:48,  1.52it/s]Extractor Predicting: 73it [00:49,  1.47it/s]Extractor Predicting: 74it [00:50,  1.48it/s]Extractor Predicting: 75it [00:50,  1.49it/s]Extractor Predicting: 76it [00:51,  1.50it/s]Extractor Predicting: 77it [00:52,  1.46it/s]Extractor Predicting: 78it [00:52,  1.46it/s]Extractor Predicting: 79it [00:53,  1.47it/s]Extractor Predicting: 80it [00:54,  1.46it/s]Extractor Predicting: 81it [00:54,  1.47it/s]Extractor Predicting: 82it [00:55,  1.46it/s]Extractor Predicting: 83it [00:56,  1.46it/s]Extractor Predicting: 84it [00:57,  1.33it/s]Extractor Predicting: 85it [00:57,  1.38it/s]Extractor Predicting: 86it [00:58,  1.40it/s]Extractor Predicting: 87it [00:59,  1.41it/s]Extractor Predicting: 88it [00:59,  1.43it/s]Extractor Predicting: 89it [01:00,  1.44it/s]Extractor Predicting: 90it [01:01,  1.47it/s]Extractor Predicting: 91it [01:01,  1.46it/s]Extractor Predicting: 92it [01:02,  1.45it/s]Extractor Predicting: 93it [01:03,  1.43it/s]Extractor Predicting: 94it [01:03,  1.46it/s]Extractor Predicting: 95it [01:04,  1.45it/s]Extractor Predicting: 96it [01:05,  1.44it/s]Extractor Predicting: 97it [01:06,  1.46it/s]Extractor Predicting: 98it [01:06,  1.46it/s]Extractor Predicting: 99it [01:07,  1.47it/s]Extractor Predicting: 100it [01:08,  1.48it/s]Extractor Predicting: 101it [01:08,  1.50it/s]Extractor Predicting: 102it [01:09,  1.50it/s]Extractor Predicting: 103it [01:10,  1.48it/s]Extractor Predicting: 104it [01:10,  1.45it/s]Extractor Predicting: 105it [01:11,  1.44it/s]Extractor Predicting: 106it [01:12,  1.45it/s]Extractor Predicting: 107it [01:12,  1.44it/s]Extractor Predicting: 108it [01:13,  1.46it/s]Extractor Predicting: 109it [01:14,  1.44it/s]Extractor Predicting: 110it [01:14,  1.46it/s]Extractor Predicting: 111it [01:15,  1.46it/s]Extractor Predicting: 112it [01:16,  1.43it/s]Extractor Predicting: 113it [01:17,  1.43it/s]Extractor Predicting: 114it [01:17,  1.42it/s]Extractor Predicting: 115it [01:18,  1.40it/s]Extractor Predicting: 116it [01:19,  1.39it/s]Extractor Predicting: 117it [01:19,  1.41it/s]Extractor Predicting: 118it [01:20,  1.42it/s]Extractor Predicting: 119it [01:21,  1.42it/s]Extractor Predicting: 120it [01:21,  1.44it/s]Extractor Predicting: 121it [01:22,  1.46it/s]Extractor Predicting: 122it [01:23,  1.48it/s]Extractor Predicting: 123it [01:23,  1.48it/s]Extractor Predicting: 124it [01:24,  1.48it/s]Extractor Predicting: 125it [01:25,  1.47it/s]Extractor Predicting: 126it [01:26,  1.44it/s]Extractor Predicting: 127it [01:26,  1.44it/s]Extractor Predicting: 128it [01:27,  1.45it/s]Extractor Predicting: 129it [01:28,  1.47it/s]Extractor Predicting: 130it [01:28,  1.47it/s]Extractor Predicting: 131it [01:29,  1.48it/s]Extractor Predicting: 132it [01:30,  1.50it/s]Extractor Predicting: 133it [01:30,  1.47it/s]Extractor Predicting: 134it [01:31,  1.46it/s]Extractor Predicting: 135it [01:32,  1.44it/s]Extractor Predicting: 136it [01:32,  1.44it/s]Extractor Predicting: 137it [01:33,  1.43it/s]Extractor Predicting: 138it [01:34,  1.48it/s]Extractor Predicting: 139it [01:34,  1.46it/s]Extractor Predicting: 140it [01:35,  1.44it/s]Extractor Predicting: 141it [01:36,  1.45it/s]Extractor Predicting: 142it [01:37,  1.45it/s]Extractor Predicting: 143it [01:37,  1.45it/s]Extractor Predicting: 144it [01:38,  1.43it/s]Extractor Predicting: 145it [01:39,  1.46it/s]Extractor Predicting: 146it [01:39,  1.46it/s]Extractor Predicting: 147it [01:40,  1.49it/s]Extractor Predicting: 148it [01:41,  1.47it/s]Extractor Predicting: 149it [01:41,  1.48it/s]Extractor Predicting: 150it [01:42,  1.49it/s]Extractor Predicting: 151it [01:43,  1.51it/s]Extractor Predicting: 152it [01:43,  1.53it/s]Extractor Predicting: 153it [01:44,  1.52it/s]Extractor Predicting: 154it [01:44,  1.54it/s]Extractor Predicting: 155it [01:45,  1.53it/s]Extractor Predicting: 156it [01:46,  1.53it/s]Extractor Predicting: 157it [01:47,  1.50it/s]Extractor Predicting: 158it [01:47,  1.46it/s]Extractor Predicting: 159it [01:48,  1.48it/s]Extractor Predicting: 160it [01:49,  1.52it/s]Extractor Predicting: 161it [01:49,  1.51it/s]Extractor Predicting: 162it [01:50,  1.52it/s]Extractor Predicting: 163it [01:50,  1.53it/s]Extractor Predicting: 164it [01:51,  1.53it/s]Extractor Predicting: 165it [01:52,  1.50it/s]Extractor Predicting: 166it [01:52,  1.50it/s]Extractor Predicting: 167it [01:53,  1.50it/s]Extractor Predicting: 168it [01:54,  1.50it/s]Extractor Predicting: 169it [01:54,  1.54it/s]Extractor Predicting: 170it [01:55,  1.54it/s]Extractor Predicting: 171it [01:56,  1.51it/s]Extractor Predicting: 172it [01:56,  1.49it/s]Extractor Predicting: 173it [01:57,  1.49it/s]Extractor Predicting: 174it [01:58,  1.51it/s]Extractor Predicting: 175it [01:58,  1.54it/s]Extractor Predicting: 176it [01:59,  1.54it/s]Extractor Predicting: 177it [02:00,  1.49it/s]Extractor Predicting: 178it [02:01,  1.37it/s]Extractor Predicting: 179it [02:01,  1.40it/s]Extractor Predicting: 180it [02:02,  1.43it/s]Extractor Predicting: 181it [02:03,  1.46it/s]Extractor Predicting: 182it [02:03,  1.45it/s]Extractor Predicting: 183it [02:04,  1.47it/s]Extractor Predicting: 184it [02:05,  1.49it/s]Extractor Predicting: 185it [02:05,  1.51it/s]Extractor Predicting: 186it [02:06,  1.53it/s]Extractor Predicting: 187it [02:06,  1.58it/s]Extractor Predicting: 188it [02:07,  1.55it/s]Extractor Predicting: 189it [02:08,  1.50it/s]Extractor Predicting: 190it [02:09,  1.50it/s]Extractor Predicting: 191it [02:09,  1.48it/s]Extractor Predicting: 192it [02:10,  1.50it/s]Extractor Predicting: 193it [02:11,  1.50it/s]Extractor Predicting: 194it [02:11,  1.52it/s]Extractor Predicting: 195it [02:12,  1.53it/s]Extractor Predicting: 196it [02:12,  1.54it/s]Extractor Predicting: 197it [02:13,  1.49it/s]Extractor Predicting: 198it [02:14,  1.50it/s]Extractor Predicting: 199it [02:15,  1.48it/s]Extractor Predicting: 200it [02:15,  1.49it/s]Extractor Predicting: 201it [02:16,  1.49it/s]Extractor Predicting: 202it [02:17,  1.49it/s]Extractor Predicting: 203it [02:17,  1.48it/s]Extractor Predicting: 204it [02:18,  1.45it/s]Extractor Predicting: 205it [02:19,  1.42it/s]Extractor Predicting: 206it [02:19,  1.46it/s]Extractor Predicting: 207it [02:20,  1.46it/s]Extractor Predicting: 208it [02:21,  1.49it/s]Extractor Predicting: 209it [02:21,  1.51it/s]Extractor Predicting: 210it [02:22,  1.49it/s]Extractor Predicting: 211it [02:23,  1.48it/s]Extractor Predicting: 212it [02:23,  1.47it/s]Extractor Predicting: 213it [02:24,  1.46it/s]Extractor Predicting: 214it [02:25,  1.45it/s]Extractor Predicting: 215it [02:25,  1.45it/s]Extractor Predicting: 216it [02:26,  1.45it/s]Extractor Predicting: 217it [02:27,  1.44it/s]Extractor Predicting: 218it [02:28,  1.45it/s]Extractor Predicting: 219it [02:28,  1.44it/s]Extractor Predicting: 220it [02:29,  1.47it/s]Extractor Predicting: 221it [02:30,  1.50it/s]Extractor Predicting: 222it [02:30,  1.45it/s]Extractor Predicting: 223it [02:31,  1.43it/s]Extractor Predicting: 224it [02:32,  1.47it/s]Extractor Predicting: 225it [02:32,  1.48it/s]Extractor Predicting: 226it [02:33,  1.46it/s]Extractor Predicting: 227it [02:34,  1.47it/s]Extractor Predicting: 228it [02:34,  1.48it/s]Extractor Predicting: 229it [02:35,  1.51it/s]Extractor Predicting: 230it [02:36,  1.46it/s]Extractor Predicting: 231it [02:36,  1.47it/s]Extractor Predicting: 232it [02:37,  1.49it/s]Extractor Predicting: 233it [02:38,  1.47it/s]Extractor Predicting: 234it [02:38,  1.45it/s]Extractor Predicting: 235it [02:39,  1.46it/s]Extractor Predicting: 236it [02:40,  1.46it/s]Extractor Predicting: 237it [02:40,  1.44it/s]Extractor Predicting: 238it [02:41,  1.45it/s]Extractor Predicting: 239it [02:42,  1.45it/s]Extractor Predicting: 240it [02:43,  1.44it/s]Extractor Predicting: 241it [02:43,  1.45it/s]Extractor Predicting: 242it [02:44,  1.47it/s]Extractor Predicting: 243it [02:45,  1.45it/s]Extractor Predicting: 244it [02:45,  1.47it/s]Extractor Predicting: 245it [02:46,  1.45it/s]Extractor Predicting: 246it [02:47,  1.46it/s]Extractor Predicting: 247it [02:47,  1.46it/s]Extractor Predicting: 248it [02:48,  1.46it/s]Extractor Predicting: 249it [02:49,  1.47it/s]Extractor Predicting: 250it [02:49,  1.46it/s]Extractor Predicting: 251it [02:50,  1.47it/s]Extractor Predicting: 252it [02:51,  1.45it/s]Extractor Predicting: 253it [02:51,  1.46it/s]Extractor Predicting: 254it [02:52,  1.47it/s]Extractor Predicting: 255it [02:53,  1.45it/s]Extractor Predicting: 256it [02:54,  1.45it/s]Extractor Predicting: 257it [02:54,  1.48it/s]Extractor Predicting: 258it [02:55,  1.48it/s]Extractor Predicting: 259it [02:55,  1.50it/s]Extractor Predicting: 260it [02:56,  1.48it/s]Extractor Predicting: 261it [02:57,  1.48it/s]Extractor Predicting: 262it [02:58,  1.48it/s]Extractor Predicting: 263it [02:58,  1.48it/s]Extractor Predicting: 264it [02:59,  1.45it/s]Extractor Predicting: 265it [03:00,  1.46it/s]Extractor Predicting: 266it [03:00,  1.46it/s]Extractor Predicting: 267it [03:01,  1.48it/s]Extractor Predicting: 268it [03:02,  1.49it/s]Extractor Predicting: 269it [03:02,  1.50it/s]Extractor Predicting: 270it [03:03,  1.49it/s]Extractor Predicting: 271it [03:04,  1.50it/s]Extractor Predicting: 272it [03:04,  1.54it/s]Extractor Predicting: 273it [03:05,  1.54it/s]Extractor Predicting: 274it [03:05,  1.55it/s]Extractor Predicting: 275it [03:06,  1.52it/s]Extractor Predicting: 276it [03:07,  1.50it/s]Extractor Predicting: 277it [03:07,  1.52it/s]Extractor Predicting: 278it [03:08,  1.49it/s]Extractor Predicting: 279it [03:09,  1.51it/s]Extractor Predicting: 280it [03:10,  1.48it/s]Extractor Predicting: 281it [03:10,  1.49it/s]Extractor Predicting: 282it [03:11,  1.48it/s]Extractor Predicting: 283it [03:12,  1.48it/s]Extractor Predicting: 284it [03:12,  1.48it/s]Extractor Predicting: 285it [03:13,  1.47it/s]Extractor Predicting: 286it [03:14,  1.45it/s]Extractor Predicting: 287it [03:14,  1.46it/s]Extractor Predicting: 288it [03:15,  1.47it/s]Extractor Predicting: 289it [03:16,  1.48it/s]Extractor Predicting: 290it [03:16,  1.48it/s]Extractor Predicting: 291it [03:17,  1.47it/s]Extractor Predicting: 292it [03:18,  1.49it/s]Extractor Predicting: 293it [03:18,  1.48it/s]Extractor Predicting: 294it [03:19,  1.46it/s]Extractor Predicting: 295it [03:20,  1.49it/s]Extractor Predicting: 296it [03:21,  1.33it/s]Extractor Predicting: 297it [03:21,  1.34it/s]Extractor Predicting: 298it [03:22,  1.40it/s]Extractor Predicting: 299it [03:23,  1.41it/s]Extractor Predicting: 300it [03:23,  1.41it/s]Extractor Predicting: 301it [03:24,  1.47it/s]Extractor Predicting: 302it [03:25,  1.47it/s]Extractor Predicting: 303it [03:25,  1.51it/s]Extractor Predicting: 304it [03:26,  1.49it/s]Extractor Predicting: 305it [03:27,  1.47it/s]Extractor Predicting: 306it [03:27,  1.48it/s]Extractor Predicting: 307it [03:28,  1.46it/s]Extractor Predicting: 308it [03:29,  1.45it/s]Extractor Predicting: 309it [03:29,  1.46it/s]Extractor Predicting: 310it [03:30,  1.45it/s]Extractor Predicting: 311it [03:31,  1.44it/s]Extractor Predicting: 312it [03:32,  1.41it/s]Extractor Predicting: 313it [03:32,  1.42it/s]Extractor Predicting: 314it [03:33,  1.43it/s]Extractor Predicting: 315it [03:34,  1.46it/s]Extractor Predicting: 316it [03:34,  1.48it/s]Extractor Predicting: 317it [03:35,  1.46it/s]Extractor Predicting: 318it [03:36,  1.48it/s]Extractor Predicting: 319it [03:36,  1.47it/s]Extractor Predicting: 320it [03:37,  1.46it/s]Extractor Predicting: 321it [03:38,  1.46it/s]Extractor Predicting: 322it [03:38,  1.45it/s]Extractor Predicting: 323it [03:39,  1.49it/s]Extractor Predicting: 324it [03:40,  1.47it/s]Extractor Predicting: 325it [03:40,  1.48it/s]Extractor Predicting: 326it [03:41,  1.48it/s]Extractor Predicting: 327it [03:42,  1.50it/s]Extractor Predicting: 328it [03:42,  1.52it/s]Extractor Predicting: 329it [03:43,  1.48it/s]Extractor Predicting: 330it [03:44,  1.46it/s]Extractor Predicting: 331it [03:45,  1.46it/s]Extractor Predicting: 332it [03:45,  1.45it/s]Extractor Predicting: 333it [03:46,  1.47it/s]Extractor Predicting: 334it [03:47,  1.46it/s]Extractor Predicting: 335it [03:47,  1.46it/s]Extractor Predicting: 336it [03:48,  1.49it/s]Extractor Predicting: 337it [03:49,  1.51it/s]Extractor Predicting: 338it [03:49,  1.50it/s]Extractor Predicting: 339it [03:50,  1.48it/s]Extractor Predicting: 340it [03:51,  1.49it/s]Extractor Predicting: 341it [03:51,  1.51it/s]Extractor Predicting: 342it [03:52,  1.48it/s]Extractor Predicting: 343it [03:53,  1.49it/s]Extractor Predicting: 344it [03:53,  1.49it/s]Extractor Predicting: 345it [03:54,  1.51it/s]Extractor Predicting: 346it [03:55,  1.50it/s]Extractor Predicting: 347it [03:55,  1.49it/s]Extractor Predicting: 348it [03:56,  1.50it/s]Extractor Predicting: 349it [03:57,  1.47it/s]Extractor Predicting: 350it [03:57,  1.47it/s]Extractor Predicting: 351it [03:58,  1.46it/s]Extractor Predicting: 352it [03:59,  1.43it/s]Extractor Predicting: 353it [03:59,  1.44it/s]Extractor Predicting: 354it [04:00,  1.44it/s]Extractor Predicting: 355it [04:01,  1.46it/s]Extractor Predicting: 356it [04:01,  1.47it/s]Extractor Predicting: 357it [04:02,  1.50it/s]Extractor Predicting: 358it [04:03,  1.50it/s]Extractor Predicting: 359it [04:03,  1.49it/s]Extractor Predicting: 360it [04:04,  1.48it/s]Extractor Predicting: 361it [04:05,  1.49it/s]Extractor Predicting: 362it [04:05,  1.49it/s]Extractor Predicting: 363it [04:06,  1.51it/s]Extractor Predicting: 364it [04:07,  1.51it/s]Extractor Predicting: 365it [04:07,  1.50it/s]Extractor Predicting: 366it [04:08,  1.49it/s]Extractor Predicting: 367it [04:09,  1.48it/s]Extractor Predicting: 368it [04:09,  1.50it/s]Extractor Predicting: 369it [04:10,  1.50it/s]Extractor Predicting: 370it [04:11,  1.53it/s]Extractor Predicting: 371it [04:11,  1.52it/s]Extractor Predicting: 372it [04:12,  1.52it/s]Extractor Predicting: 373it [04:13,  1.51it/s]Extractor Predicting: 374it [04:13,  1.50it/s]Extractor Predicting: 375it [04:14,  1.51it/s]Extractor Predicting: 376it [04:15,  1.51it/s]Extractor Predicting: 377it [04:15,  1.52it/s]Extractor Predicting: 378it [04:16,  1.53it/s]Extractor Predicting: 379it [04:17,  1.48it/s]Extractor Predicting: 380it [04:17,  1.48it/s]Extractor Predicting: 381it [04:18,  1.52it/s]Extractor Predicting: 382it [04:19,  1.52it/s]Extractor Predicting: 383it [04:19,  1.47it/s]Extractor Predicting: 384it [04:20,  1.48it/s]Extractor Predicting: 385it [04:21,  1.49it/s]Extractor Predicting: 386it [04:21,  1.53it/s]Extractor Predicting: 387it [04:22,  1.54it/s]Extractor Predicting: 388it [04:23,  1.54it/s]Extractor Predicting: 389it [04:23,  1.50it/s]Extractor Predicting: 390it [04:24,  1.52it/s]Extractor Predicting: 391it [04:25,  1.52it/s]Extractor Predicting: 392it [04:25,  1.53it/s]Extractor Predicting: 393it [04:26,  1.54it/s]Extractor Predicting: 394it [04:27,  1.57it/s]Extractor Predicting: 395it [04:27,  1.53it/s]Extractor Predicting: 396it [04:28,  1.52it/s]Extractor Predicting: 397it [04:29,  1.53it/s]Extractor Predicting: 398it [04:29,  1.53it/s]Extractor Predicting: 399it [04:30,  1.55it/s]Extractor Predicting: 400it [04:30,  1.59it/s]Extractor Predicting: 401it [04:31,  1.56it/s]Extractor Predicting: 402it [04:32,  1.56it/s]Extractor Predicting: 403it [04:32,  1.54it/s]Extractor Predicting: 404it [04:33,  1.53it/s]Extractor Predicting: 405it [04:34,  1.53it/s]Extractor Predicting: 406it [04:34,  1.52it/s]Extractor Predicting: 407it [04:35,  1.36it/s]Extractor Predicting: 408it [04:36,  1.41it/s]Extractor Predicting: 409it [04:37,  1.45it/s]Extractor Predicting: 409it [04:37,  1.48it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]> [0;32m/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/joint_models.py[0m(514)[0;36mpredict_step[0;34m()[0m
[0;32m    513 [0;31m        [0;31m#     import ipdb[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m--> 514 [0;31m        [0;31m#     ipdb.set_trace()[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m[0;32m    515 [0;31m        [0;31m#     relation_preds = self._postprocess_relations(re_tag_logits, entity_preds, labels)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m
ipdb> Extractor Predicting: 0it [00:06, ?it/s]

Traceback (most recent call last):
  File "wrapper.py", line 821, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 381, in main
    path_test=path_test, labels=labels_test, mode='multi', is_eval=False, model_size=model_size)
  File "wrapper.py", line 762, in run_eval
    model.predict(path_in, path_out, labels)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/extractor.py", line 233, in predict
    data = trainer.predict(model, dataloader, labels)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/data/joint_data.py", line 172, in predict
    outputs = model.predict_step(inputs, labels)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/base.py", line 98, in hooked_predict_step
    rets = self._predict_step(inputs, labels)
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/joint_models.py", line 514, in predict_step
    #     ipdb.set_trace()
  File "/cto_studio/xuting/OpenIE/DualOpenIE_v3/two-are-better-than-one/models/joint_models.py", line 514, in predict_step
    #     ipdb.set_trace()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

If you suspect this is an IPython 7.33.0 bug, please report it at:
    https://github.com/ipython/ipython/issues
or send an email to the mailing list at ipython-dev@python.org

You can print a more detailed traceback right now with "%tb", or use "%debug"
to interactively debug it.

Extra-detailed tracebacks for bug-reporting purposes can be enabled via:
    %config Application.verbose_crash=True

/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_15_seed_0', 'type': 'train', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl'}
train vocab size: 83376
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 83476, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/model', pretrained_wv='outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=83476, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.337, loss:53192.9094
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.042, loss:2783.7257
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.033, loss:2468.0093
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.016, loss:2408.5134
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.029, loss:2356.9745
>> valid entity prec:0.4043, rec:0.5616, f1:0.4702
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 600, avg_time 2.615, loss:2303.1863
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 700, avg_time 1.026, loss:2221.5935
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 800, avg_time 1.024, loss:1989.6289
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 900, avg_time 1.042, loss:1903.0673
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 1000, avg_time 1.039, loss:1756.8797
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4718, rec:0.5058, f1:0.4882
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 1100, avg_time 2.556, loss:1768.8357
g_step 1200, step 1200, avg_time 1.044, loss:1658.7248
g_step 1300, step 1300, avg_time 1.033, loss:1614.6912
g_step 1400, step 1400, avg_time 1.031, loss:1595.0075
g_step 1500, step 1500, avg_time 1.031, loss:1484.9625
>> valid entity prec:0.4989, rec:0.6155, f1:0.5511
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 1600, avg_time 2.548, loss:1516.1333
g_step 1700, step 1700, avg_time 1.029, loss:1508.1260
g_step 1800, step 1800, avg_time 1.025, loss:1483.3517
g_step 1900, step 1900, avg_time 1.038, loss:1465.8834
g_step 2000, step 2000, avg_time 1.043, loss:1460.6968
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4967, rec:0.4817, f1:0.4891
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 2100, avg_time 2.542, loss:1345.8616
g_step 2200, step 2200, avg_time 1.027, loss:1408.2897
g_step 2300, step 2300, avg_time 1.027, loss:1385.9234
g_step 2400, step 2400, avg_time 1.033, loss:1311.9371
g_step 2500, step 2500, avg_time 1.030, loss:1331.8187
>> valid entity prec:0.5171, rec:0.3640, f1:0.4272
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 2600, avg_time 2.527, loss:1285.6864
g_step 2700, step 2700, avg_time 1.029, loss:1324.3096
g_step 2800, step 2800, avg_time 1.047, loss:1282.8468
g_step 2900, step 2900, avg_time 1.023, loss:1277.4057
g_step 3000, step 3000, avg_time 1.039, loss:1246.7530
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5155, rec:0.5733, f1:0.5429
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 3100, avg_time 2.547, loss:1249.5064
g_step 3200, step 87, avg_time 1.037, loss:1261.4351
g_step 3300, step 187, avg_time 1.040, loss:1193.6335
g_step 3400, step 287, avg_time 1.032, loss:1256.9875
g_step 3500, step 387, avg_time 1.029, loss:1216.1646
>> valid entity prec:0.5072, rec:0.5149, f1:0.5110
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 487, avg_time 2.537, loss:1237.2078
g_step 3700, step 587, avg_time 1.034, loss:1237.5353
g_step 3800, step 687, avg_time 1.041, loss:1244.8803
g_step 3900, step 787, avg_time 1.023, loss:1184.3914
g_step 4000, step 887, avg_time 1.028, loss:1184.0000
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.4973, rec:0.6330, f1:0.5570
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 987, avg_time 2.552, loss:1173.8216
g_step 4200, step 1087, avg_time 1.026, loss:1164.8401
g_step 4300, step 1187, avg_time 1.038, loss:1145.9391
g_step 4400, step 1287, avg_time 1.022, loss:1177.2981
g_step 4500, step 1387, avg_time 1.040, loss:1164.0741
>> valid entity prec:0.4918, rec:0.6410, f1:0.5566
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4600, step 1487, avg_time 2.556, loss:1209.9571
g_step 4700, step 1587, avg_time 1.046, loss:1145.0533
g_step 4800, step 1687, avg_time 1.024, loss:1141.7004
g_step 4900, step 1787, avg_time 1.038, loss:1154.8711
g_step 5000, step 1887, avg_time 1.040, loss:1195.9227
learning rate was adjusted to 0.0008
>> valid entity prec:0.5141, rec:0.6013, f1:0.5543
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5100, step 1987, avg_time 2.546, loss:1156.0413
g_step 5200, step 2087, avg_time 1.038, loss:1113.6083
g_step 5300, step 2187, avg_time 1.030, loss:1147.7987
g_step 5400, step 2287, avg_time 1.038, loss:1148.5706
g_step 5500, step 2387, avg_time 1.023, loss:1107.3222
>> valid entity prec:0.4669, rec:0.6537, f1:0.5447
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 5600, step 2487, avg_time 2.542, loss:1137.8832
g_step 5700, step 2587, avg_time 1.033, loss:1138.5788
g_step 5800, step 2687, avg_time 1.023, loss:1114.8314
g_step 5900, step 2787, avg_time 1.038, loss:1167.8924
g_step 6000, step 2887, avg_time 1.046, loss:1086.7771
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5089, rec:0.6185, f1:0.5584
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6100, step 2987, avg_time 2.548, loss:1093.2404
g_step 6200, step 3087, avg_time 1.035, loss:1091.8536
g_step 6300, step 74, avg_time 1.035, loss:1098.0930
g_step 6400, step 174, avg_time 1.042, loss:1056.9455
g_step 6500, step 274, avg_time 1.030, loss:1054.7873
>> valid entity prec:0.4830, rec:0.6344, f1:0.5484
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 6600, step 374, avg_time 2.553, loss:1041.9700
g_step 6700, step 474, avg_time 1.024, loss:1100.4313
g_step 6800, step 574, avg_time 1.041, loss:1055.3276
g_step 6900, step 674, avg_time 1.040, loss:1075.6800
g_step 7000, step 774, avg_time 1.028, loss:1093.8174
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.4820, rec:0.6008, f1:0.5349
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7100, step 874, avg_time 2.546, loss:1059.8354
g_step 7200, step 974, avg_time 1.030, loss:1052.3400
g_step 7300, step 1074, avg_time 1.031, loss:1082.1673
g_step 7400, step 1174, avg_time 1.044, loss:1096.0321
g_step 7500, step 1274, avg_time 1.044, loss:1095.5314
>> valid entity prec:0.4999, rec:0.6105, f1:0.5497
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 7600, step 1374, avg_time 2.557, loss:1019.8072
g_step 7700, step 1474, avg_time 1.049, loss:1032.8823
g_step 7800, step 1574, avg_time 1.038, loss:1076.3362
g_step 7900, step 1674, avg_time 1.038, loss:1064.6648
g_step 8000, step 1774, avg_time 1.029, loss:1049.5634
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.4968, rec:0.5656, f1:0.5289
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8100, step 1874, avg_time 2.549, loss:1056.5254
g_step 8200, step 1974, avg_time 1.029, loss:1105.2437
g_step 8300, step 2074, avg_time 1.028, loss:1059.6942
g_step 8400, step 2174, avg_time 1.035, loss:1034.7869
g_step 8500, step 2274, avg_time 1.026, loss:1048.2954
>> valid entity prec:0.4897, rec:0.5699, f1:0.5268
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 8600, step 2374, avg_time 2.534, loss:1062.8663
g_step 8700, step 2474, avg_time 1.028, loss:1119.8121
g_step 8800, step 2574, avg_time 1.039, loss:1031.4899
g_step 8900, step 2674, avg_time 1.016, loss:1001.2255
g_step 9000, step 2774, avg_time 1.038, loss:1087.8527
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.4905, rec:0.6267, f1:0.5503
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9100, step 2874, avg_time 2.538, loss:1044.7755
g_step 9200, step 2974, avg_time 1.034, loss:1005.2863
g_step 9300, step 3074, avg_time 1.028, loss:1026.5625
g_step 9400, step 61, avg_time 1.035, loss:1011.2192
g_step 9500, step 161, avg_time 1.034, loss:1000.4257
>> valid entity prec:0.5189, rec:0.5864, f1:0.5506
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 9600, step 261, avg_time 2.539, loss:979.2799
g_step 9700, step 361, avg_time 1.036, loss:994.3984
g_step 9800, step 461, avg_time 1.034, loss:975.0732
g_step 9900, step 561, avg_time 1.030, loss:971.1155
g_step 10000, step 661, avg_time 1.045, loss:1048.0261
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.5220, rec:0.5549, f1:0.5379
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
reach max_steps, stop training
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'lowest point', 'manufacturer', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12876
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12976, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:05,  5.25s/it]Extractor Predicting: 2it [00:05,  2.51s/it]Extractor Predicting: 3it [00:06,  1.84s/it]Extractor Predicting: 4it [00:07,  1.51s/it]Extractor Predicting: 5it [00:08,  1.35s/it]Extractor Predicting: 6it [00:10,  1.41s/it]Extractor Predicting: 7it [00:11,  1.14s/it]Extractor Predicting: 8it [00:11,  1.02it/s]Extractor Predicting: 9it [00:12,  1.18it/s]Extractor Predicting: 10it [00:12,  1.30it/s]Extractor Predicting: 11it [00:13,  1.40it/s]Extractor Predicting: 12it [00:14,  1.47it/s]Extractor Predicting: 13it [00:14,  1.40it/s]Extractor Predicting: 14it [00:15,  1.43it/s]Extractor Predicting: 15it [00:16,  1.42it/s]Extractor Predicting: 16it [00:16,  1.46it/s]Extractor Predicting: 17it [00:17,  1.46it/s]Extractor Predicting: 18it [00:18,  1.45it/s]Extractor Predicting: 19it [00:18,  1.46it/s]Extractor Predicting: 20it [00:19,  1.50it/s]Extractor Predicting: 21it [00:20,  1.48it/s]Extractor Predicting: 22it [00:20,  1.47it/s]Extractor Predicting: 23it [00:21,  1.47it/s]Extractor Predicting: 24it [00:22,  1.44it/s]Extractor Predicting: 25it [00:22,  1.47it/s]Extractor Predicting: 26it [00:23,  1.49it/s]Extractor Predicting: 27it [00:24,  1.51it/s]Extractor Predicting: 28it [00:24,  1.53it/s]Extractor Predicting: 29it [00:25,  1.51it/s]Extractor Predicting: 30it [00:26,  1.51it/s]Extractor Predicting: 31it [00:26,  1.48it/s]Extractor Predicting: 32it [00:27,  1.50it/s]Extractor Predicting: 33it [00:28,  1.49it/s]Extractor Predicting: 34it [00:30,  1.20s/it]Extractor Predicting: 35it [00:31,  1.03s/it]Extractor Predicting: 36it [00:31,  1.10it/s]Extractor Predicting: 37it [00:32,  1.16it/s]Extractor Predicting: 38it [00:33,  1.24it/s]Extractor Predicting: 39it [00:34,  1.30it/s]Extractor Predicting: 40it [00:34,  1.35it/s]Extractor Predicting: 41it [00:35,  1.43it/s]Extractor Predicting: 42it [00:36,  1.42it/s]Extractor Predicting: 43it [00:36,  1.42it/s]Extractor Predicting: 44it [00:37,  1.42it/s]Extractor Predicting: 45it [00:38,  1.45it/s]Extractor Predicting: 46it [00:38,  1.48it/s]Extractor Predicting: 47it [00:39,  1.47it/s]Extractor Predicting: 48it [00:40,  1.49it/s]Extractor Predicting: 49it [00:40,  1.50it/s]Extractor Predicting: 50it [00:41,  1.53it/s]Extractor Predicting: 51it [00:42,  1.55it/s]Extractor Predicting: 52it [00:42,  1.53it/s]Extractor Predicting: 53it [00:43,  1.55it/s]Extractor Predicting: 54it [00:44,  1.51it/s]Extractor Predicting: 55it [00:44,  1.53it/s]Extractor Predicting: 56it [00:45,  1.50it/s]Extractor Predicting: 57it [00:46,  1.49it/s]Extractor Predicting: 58it [00:46,  1.48it/s]Extractor Predicting: 59it [00:47,  1.50it/s]Extractor Predicting: 60it [00:48,  1.48it/s]Extractor Predicting: 61it [00:48,  1.52it/s]Extractor Predicting: 62it [00:49,  1.52it/s]Extractor Predicting: 63it [00:49,  1.53it/s]Extractor Predicting: 64it [00:50,  1.53it/s]Extractor Predicting: 65it [00:51,  1.53it/s]Extractor Predicting: 66it [00:51,  1.52it/s]Extractor Predicting: 67it [00:52,  1.52it/s]Extractor Predicting: 68it [00:53,  1.49it/s]Extractor Predicting: 69it [00:53,  1.50it/s]Extractor Predicting: 70it [00:54,  1.52it/s]Extractor Predicting: 71it [00:55,  1.52it/s]Extractor Predicting: 72it [00:55,  1.50it/s]Extractor Predicting: 73it [00:56,  1.48it/s]Extractor Predicting: 74it [00:57,  1.46it/s]Extractor Predicting: 75it [00:57,  1.50it/s]Extractor Predicting: 76it [00:58,  1.44it/s]Extractor Predicting: 77it [00:59,  1.43it/s]Extractor Predicting: 78it [01:00,  1.42it/s]Extractor Predicting: 79it [01:00,  1.43it/s]Extractor Predicting: 80it [01:01,  1.43it/s]Extractor Predicting: 81it [01:02,  1.40it/s]Extractor Predicting: 82it [01:03,  1.41it/s]Extractor Predicting: 83it [01:03,  1.47it/s]Extractor Predicting: 84it [01:04,  1.47it/s]Extractor Predicting: 85it [01:04,  1.48it/s]Extractor Predicting: 86it [01:05,  1.49it/s]Extractor Predicting: 87it [01:06,  1.48it/s]Extractor Predicting: 88it [01:06,  1.50it/s]Extractor Predicting: 89it [01:07,  1.49it/s]Extractor Predicting: 90it [01:08,  1.48it/s]Extractor Predicting: 91it [01:08,  1.50it/s]Extractor Predicting: 92it [01:09,  1.52it/s]Extractor Predicting: 93it [01:10,  1.48it/s]Extractor Predicting: 94it [01:11,  1.46it/s]Extractor Predicting: 95it [01:11,  1.44it/s]Extractor Predicting: 96it [01:12,  1.47it/s]Extractor Predicting: 97it [01:13,  1.49it/s]Extractor Predicting: 98it [01:13,  1.50it/s]Extractor Predicting: 99it [01:14,  1.48it/s]Extractor Predicting: 100it [01:15,  1.37it/s]Extractor Predicting: 101it [01:15,  1.40it/s]Extractor Predicting: 102it [01:16,  1.41it/s]Extractor Predicting: 103it [01:17,  1.42it/s]Extractor Predicting: 104it [01:17,  1.45it/s]Extractor Predicting: 105it [01:18,  1.45it/s]Extractor Predicting: 106it [01:19,  1.48it/s]Extractor Predicting: 107it [01:19,  1.50it/s]Extractor Predicting: 108it [01:20,  1.47it/s]Extractor Predicting: 109it [01:21,  1.48it/s]Extractor Predicting: 110it [01:22,  1.47it/s]Extractor Predicting: 111it [01:22,  1.49it/s]Extractor Predicting: 112it [01:23,  1.46it/s]Extractor Predicting: 113it [01:24,  1.49it/s]Extractor Predicting: 114it [01:24,  1.47it/s]Extractor Predicting: 115it [01:25,  1.50it/s]Extractor Predicting: 116it [01:26,  1.50it/s]Extractor Predicting: 117it [01:26,  1.53it/s]Extractor Predicting: 118it [01:27,  1.48it/s]Extractor Predicting: 119it [01:28,  1.45it/s]Extractor Predicting: 120it [01:28,  1.49it/s]Extractor Predicting: 121it [01:29,  1.48it/s]Extractor Predicting: 122it [01:30,  1.46it/s]Extractor Predicting: 123it [01:30,  1.44it/s]Extractor Predicting: 124it [01:31,  1.44it/s]Extractor Predicting: 125it [01:32,  1.43it/s]Extractor Predicting: 126it [01:32,  1.44it/s]Extractor Predicting: 127it [01:33,  1.44it/s]Extractor Predicting: 128it [01:34,  1.44it/s]Extractor Predicting: 129it [01:35,  1.43it/s]Extractor Predicting: 130it [01:35,  1.45it/s]Extractor Predicting: 131it [01:36,  1.42it/s]Extractor Predicting: 132it [01:37,  1.44it/s]Extractor Predicting: 133it [01:37,  1.44it/s]Extractor Predicting: 134it [01:38,  1.46it/s]Extractor Predicting: 135it [01:39,  1.47it/s]Extractor Predicting: 136it [01:39,  1.47it/s]Extractor Predicting: 137it [01:40,  1.45it/s]Extractor Predicting: 138it [01:41,  1.45it/s]Extractor Predicting: 139it [01:41,  1.45it/s]Extractor Predicting: 140it [01:42,  1.46it/s]Extractor Predicting: 141it [01:43,  1.48it/s]Extractor Predicting: 142it [01:43,  1.45it/s]Extractor Predicting: 143it [01:44,  1.47it/s]Extractor Predicting: 144it [01:45,  1.45it/s]Extractor Predicting: 145it [01:45,  1.46it/s]Extractor Predicting: 146it [01:46,  1.43it/s]Extractor Predicting: 147it [01:47,  1.44it/s]Extractor Predicting: 148it [01:48,  1.45it/s]Extractor Predicting: 149it [01:48,  1.45it/s]Extractor Predicting: 150it [01:49,  1.46it/s]Extractor Predicting: 151it [01:50,  1.46it/s]Extractor Predicting: 152it [01:50,  1.42it/s]Extractor Predicting: 153it [01:51,  1.43it/s]Extractor Predicting: 154it [01:52,  1.55it/s]Extractor Predicting: 154it [01:52,  1.37it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27443
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27543, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.31it/s]Extractor Predicting: 2it [00:01,  1.45it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.53it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.59it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:08,  1.59it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:11,  1.53it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.57it/s]Extractor Predicting: 20it [00:12,  1.55it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.55it/s]Extractor Predicting: 28it [00:18,  1.57it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.55it/s]Extractor Predicting: 31it [00:20,  1.56it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.55it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.56it/s]Extractor Predicting: 36it [00:23,  1.55it/s]Extractor Predicting: 37it [00:23,  1.54it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:25,  1.57it/s]Extractor Predicting: 40it [00:25,  1.56it/s]Extractor Predicting: 41it [00:26,  1.51it/s]Extractor Predicting: 42it [00:27,  1.45it/s]Extractor Predicting: 43it [00:27,  1.45it/s]Extractor Predicting: 44it [00:28,  1.44it/s]Extractor Predicting: 45it [00:29,  1.45it/s]Extractor Predicting: 46it [00:30,  1.42it/s]Extractor Predicting: 47it [00:30,  1.41it/s]Extractor Predicting: 48it [00:31,  1.39it/s]Extractor Predicting: 49it [00:32,  1.41it/s]Extractor Predicting: 50it [00:32,  1.41it/s]Extractor Predicting: 51it [00:33,  1.41it/s]Extractor Predicting: 52it [00:34,  1.41it/s]Extractor Predicting: 53it [00:35,  1.41it/s]Extractor Predicting: 54it [00:35,  1.41it/s]Extractor Predicting: 55it [00:36,  1.37it/s]Extractor Predicting: 56it [00:37,  1.41it/s]Extractor Predicting: 57it [00:37,  1.45it/s]Extractor Predicting: 58it [00:38,  1.44it/s]Extractor Predicting: 59it [00:39,  1.42it/s]Extractor Predicting: 60it [00:40,  1.40it/s]Extractor Predicting: 61it [00:40,  1.40it/s]Extractor Predicting: 62it [00:41,  1.43it/s]Extractor Predicting: 63it [00:42,  1.43it/s]Extractor Predicting: 64it [00:42,  1.39it/s]Extractor Predicting: 65it [00:43,  1.40it/s]Extractor Predicting: 66it [00:44,  1.40it/s]Extractor Predicting: 67it [00:45,  1.38it/s]Extractor Predicting: 68it [00:45,  1.40it/s]Extractor Predicting: 69it [00:46,  1.38it/s]Extractor Predicting: 70it [00:47,  1.38it/s]Extractor Predicting: 71it [00:47,  1.39it/s]Extractor Predicting: 72it [00:48,  1.40it/s]Extractor Predicting: 73it [00:49,  1.39it/s]Extractor Predicting: 74it [00:50,  1.38it/s]Extractor Predicting: 75it [00:50,  1.38it/s]Extractor Predicting: 76it [00:51,  1.37it/s]Extractor Predicting: 77it [00:52,  1.42it/s]Extractor Predicting: 78it [00:52,  1.39it/s]Extractor Predicting: 79it [00:53,  1.41it/s]Extractor Predicting: 80it [00:54,  1.40it/s]Extractor Predicting: 81it [00:55,  1.36it/s]Extractor Predicting: 82it [00:55,  1.38it/s]Extractor Predicting: 83it [00:56,  1.42it/s]Extractor Predicting: 84it [00:57,  1.45it/s]Extractor Predicting: 85it [00:57,  1.41it/s]Extractor Predicting: 86it [00:58,  1.39it/s]Extractor Predicting: 87it [00:59,  1.44it/s]Extractor Predicting: 88it [01:00,  1.42it/s]Extractor Predicting: 89it [01:00,  1.40it/s]Extractor Predicting: 90it [01:01,  1.38it/s]Extractor Predicting: 91it [01:02,  1.40it/s]Extractor Predicting: 92it [01:02,  1.43it/s]Extractor Predicting: 93it [01:03,  1.45it/s]Extractor Predicting: 94it [01:04,  1.46it/s]Extractor Predicting: 95it [01:04,  1.48it/s]Extractor Predicting: 96it [01:05,  1.47it/s]Extractor Predicting: 97it [01:06,  1.39it/s]Extractor Predicting: 98it [01:07,  1.37it/s]Extractor Predicting: 99it [01:07,  1.40it/s]Extractor Predicting: 100it [01:08,  1.40it/s]Extractor Predicting: 101it [01:09,  1.43it/s]Extractor Predicting: 102it [01:09,  1.43it/s]Extractor Predicting: 103it [01:10,  1.44it/s]Extractor Predicting: 104it [01:11,  1.45it/s]Extractor Predicting: 105it [01:11,  1.44it/s]Extractor Predicting: 106it [01:12,  1.43it/s]Extractor Predicting: 107it [01:13,  1.46it/s]Extractor Predicting: 108it [01:14,  1.46it/s]Extractor Predicting: 109it [01:15,  1.26it/s]Extractor Predicting: 110it [01:17,  1.19s/it]Extractor Predicting: 111it [01:17,  1.04s/it]Extractor Predicting: 112it [01:18,  1.07it/s]Extractor Predicting: 113it [01:19,  1.18it/s]Extractor Predicting: 114it [01:19,  1.26it/s]Extractor Predicting: 115it [01:20,  1.29it/s]Extractor Predicting: 116it [01:21,  1.33it/s]Extractor Predicting: 117it [01:21,  1.40it/s]Extractor Predicting: 118it [01:24,  1.27s/it]Extractor Predicting: 119it [01:25,  1.09s/it]Extractor Predicting: 120it [01:25,  1.03it/s]Extractor Predicting: 121it [01:26,  1.11it/s]Extractor Predicting: 122it [01:27,  1.20it/s]Extractor Predicting: 123it [01:27,  1.27it/s]Extractor Predicting: 124it [01:28,  1.33it/s]Extractor Predicting: 125it [01:29,  1.36it/s]Extractor Predicting: 126it [01:29,  1.40it/s]Extractor Predicting: 127it [01:30,  1.42it/s]Extractor Predicting: 128it [01:31,  1.45it/s]Extractor Predicting: 129it [01:31,  1.48it/s]Extractor Predicting: 130it [01:32,  1.44it/s]Extractor Predicting: 131it [01:33,  1.49it/s]Extractor Predicting: 132it [01:33,  1.50it/s]Extractor Predicting: 133it [01:34,  1.50it/s]Extractor Predicting: 134it [01:35,  1.47it/s]Extractor Predicting: 135it [01:36,  1.46it/s]Extractor Predicting: 136it [01:36,  1.45it/s]Extractor Predicting: 137it [01:37,  1.45it/s]Extractor Predicting: 138it [01:38,  1.45it/s]Extractor Predicting: 139it [01:38,  1.45it/s]Extractor Predicting: 140it [01:39,  1.44it/s]Extractor Predicting: 141it [01:40,  1.48it/s]Extractor Predicting: 142it [01:40,  1.43it/s]Extractor Predicting: 143it [01:41,  1.38it/s]Extractor Predicting: 144it [01:42,  1.41it/s]Extractor Predicting: 145it [01:43,  1.43it/s]Extractor Predicting: 146it [01:43,  1.38it/s]Extractor Predicting: 147it [01:44,  1.40it/s]Extractor Predicting: 148it [01:45,  1.42it/s]Extractor Predicting: 149it [01:45,  1.51it/s]Extractor Predicting: 150it [01:46,  1.59it/s]Extractor Predicting: 151it [01:46,  1.57it/s]Extractor Predicting: 152it [01:47,  1.64it/s]Extractor Predicting: 153it [01:48,  1.70it/s]Extractor Predicting: 154it [01:48,  1.73it/s]Extractor Predicting: 155it [01:49,  1.74it/s]Extractor Predicting: 156it [01:49,  1.70it/s]Extractor Predicting: 157it [01:50,  1.73it/s]Extractor Predicting: 158it [01:50,  1.75it/s]Extractor Predicting: 159it [01:51,  1.78it/s]Extractor Predicting: 160it [01:52,  1.76it/s]Extractor Predicting: 161it [01:52,  1.72it/s]Extractor Predicting: 162it [01:53,  1.73it/s]Extractor Predicting: 163it [01:53,  1.69it/s]Extractor Predicting: 164it [01:54,  1.68it/s]Extractor Predicting: 165it [01:54,  1.72it/s]Extractor Predicting: 166it [01:55,  1.73it/s]Extractor Predicting: 167it [01:56,  1.71it/s]Extractor Predicting: 168it [01:56,  1.65it/s]Extractor Predicting: 169it [01:57,  1.59it/s]Extractor Predicting: 170it [01:58,  1.51it/s]Extractor Predicting: 171it [01:58,  1.50it/s]Extractor Predicting: 172it [01:59,  1.47it/s]Extractor Predicting: 173it [02:00,  1.44it/s]Extractor Predicting: 174it [02:01,  1.43it/s]Extractor Predicting: 175it [02:01,  1.47it/s]Extractor Predicting: 176it [02:02,  1.47it/s]Extractor Predicting: 177it [02:03,  1.49it/s]Extractor Predicting: 178it [02:03,  1.47it/s]Extractor Predicting: 179it [02:04,  1.51it/s]Extractor Predicting: 180it [02:04,  1.52it/s]Extractor Predicting: 181it [02:05,  1.53it/s]Extractor Predicting: 182it [02:06,  1.51it/s]Extractor Predicting: 183it [02:07,  1.48it/s]Extractor Predicting: 184it [02:07,  1.45it/s]Extractor Predicting: 185it [02:08,  1.48it/s]Extractor Predicting: 186it [02:09,  1.46it/s]Extractor Predicting: 187it [02:09,  1.46it/s]Extractor Predicting: 188it [02:10,  1.51it/s]Extractor Predicting: 189it [02:11,  1.50it/s]Extractor Predicting: 190it [02:11,  1.49it/s]Extractor Predicting: 191it [02:12,  1.51it/s]Extractor Predicting: 192it [02:13,  1.52it/s]Extractor Predicting: 193it [02:13,  1.50it/s]Extractor Predicting: 194it [02:14,  1.54it/s]Extractor Predicting: 195it [02:15,  1.51it/s]Extractor Predicting: 196it [02:15,  1.51it/s]Extractor Predicting: 197it [02:16,  1.52it/s]Extractor Predicting: 198it [02:17,  1.51it/s]Extractor Predicting: 199it [02:17,  1.49it/s]Extractor Predicting: 200it [02:18,  1.46it/s]Extractor Predicting: 201it [02:19,  1.46it/s]Extractor Predicting: 202it [02:19,  1.43it/s]Extractor Predicting: 203it [02:20,  1.40it/s]Extractor Predicting: 204it [02:21,  1.40it/s]Extractor Predicting: 205it [02:22,  1.41it/s]Extractor Predicting: 206it [02:22,  1.42it/s]Extractor Predicting: 207it [02:23,  1.44it/s]Extractor Predicting: 208it [02:24,  1.45it/s]Extractor Predicting: 209it [02:24,  1.46it/s]Extractor Predicting: 210it [02:25,  1.47it/s]Extractor Predicting: 211it [02:26,  1.48it/s]Extractor Predicting: 212it [02:26,  1.52it/s]Extractor Predicting: 213it [02:27,  1.48it/s]Extractor Predicting: 214it [02:28,  1.49it/s]Extractor Predicting: 215it [02:28,  1.51it/s]Extractor Predicting: 216it [02:29,  1.51it/s]Extractor Predicting: 217it [02:30,  1.50it/s]Extractor Predicting: 218it [02:30,  1.52it/s]Extractor Predicting: 219it [02:31,  1.43it/s]Extractor Predicting: 220it [02:32,  1.49it/s]Extractor Predicting: 221it [02:32,  1.47it/s]Extractor Predicting: 222it [02:33,  1.30it/s]Extractor Predicting: 223it [02:34,  1.38it/s]Extractor Predicting: 224it [02:35,  1.41it/s]Extractor Predicting: 225it [02:35,  1.48it/s]Extractor Predicting: 226it [02:36,  1.51it/s]Extractor Predicting: 227it [02:36,  1.54it/s]Extractor Predicting: 228it [02:37,  1.51it/s]Extractor Predicting: 229it [02:38,  1.52it/s]Extractor Predicting: 230it [02:38,  1.50it/s]Extractor Predicting: 231it [02:39,  1.49it/s]Extractor Predicting: 232it [02:40,  1.54it/s]Extractor Predicting: 233it [02:40,  1.53it/s]Extractor Predicting: 234it [02:41,  1.50it/s]Extractor Predicting: 235it [02:42,  1.50it/s]Extractor Predicting: 236it [02:42,  1.50it/s]Extractor Predicting: 237it [02:43,  1.58it/s]Extractor Predicting: 238it [02:44,  1.55it/s]Extractor Predicting: 239it [02:44,  1.54it/s]Extractor Predicting: 240it [02:45,  1.51it/s]Extractor Predicting: 241it [02:46,  1.50it/s]Extractor Predicting: 242it [02:46,  1.49it/s]Extractor Predicting: 243it [02:47,  1.52it/s]Extractor Predicting: 244it [02:48,  1.51it/s]Extractor Predicting: 245it [02:48,  1.48it/s]Extractor Predicting: 246it [02:49,  1.46it/s]Extractor Predicting: 247it [02:50,  1.44it/s]Extractor Predicting: 248it [02:50,  1.42it/s]Extractor Predicting: 249it [02:51,  1.43it/s]Extractor Predicting: 250it [02:52,  1.45it/s]Extractor Predicting: 251it [02:52,  1.48it/s]Extractor Predicting: 252it [02:53,  1.50it/s]Extractor Predicting: 253it [02:54,  1.46it/s]Extractor Predicting: 254it [02:55,  1.48it/s]Extractor Predicting: 255it [02:55,  1.48it/s]Extractor Predicting: 256it [02:56,  1.48it/s]Extractor Predicting: 257it [02:57,  1.47it/s]Extractor Predicting: 258it [02:57,  1.46it/s]Extractor Predicting: 259it [02:58,  1.47it/s]Extractor Predicting: 260it [02:59,  1.48it/s]Extractor Predicting: 261it [02:59,  1.48it/s]Extractor Predicting: 262it [03:00,  1.49it/s]Extractor Predicting: 263it [03:01,  1.48it/s]Extractor Predicting: 264it [03:01,  1.48it/s]Extractor Predicting: 265it [03:02,  1.48it/s]Extractor Predicting: 266it [03:03,  1.50it/s]Extractor Predicting: 267it [03:03,  1.49it/s]Extractor Predicting: 268it [03:04,  1.47it/s]Extractor Predicting: 269it [03:05,  1.47it/s]Extractor Predicting: 270it [03:05,  1.48it/s]Extractor Predicting: 271it [03:06,  1.49it/s]Extractor Predicting: 272it [03:07,  1.46it/s]Extractor Predicting: 273it [03:07,  1.43it/s]Extractor Predicting: 274it [03:08,  1.48it/s]Extractor Predicting: 275it [03:09,  1.48it/s]Extractor Predicting: 276it [03:09,  1.49it/s]Extractor Predicting: 277it [03:10,  1.46it/s]Extractor Predicting: 278it [03:11,  1.47it/s]Extractor Predicting: 279it [03:11,  1.45it/s]Extractor Predicting: 280it [03:12,  1.47it/s]Extractor Predicting: 281it [03:13,  1.41it/s]Extractor Predicting: 282it [03:14,  1.43it/s]Extractor Predicting: 283it [03:14,  1.42it/s]Extractor Predicting: 284it [03:15,  1.41it/s]Extractor Predicting: 285it [03:16,  1.38it/s]Extractor Predicting: 286it [03:17,  1.34it/s]Extractor Predicting: 287it [03:17,  1.36it/s]Extractor Predicting: 288it [03:18,  1.39it/s]Extractor Predicting: 289it [03:19,  1.39it/s]Extractor Predicting: 290it [03:19,  1.41it/s]Extractor Predicting: 291it [03:20,  1.41it/s]Extractor Predicting: 292it [03:21,  1.42it/s]Extractor Predicting: 293it [03:21,  1.43it/s]Extractor Predicting: 294it [03:22,  1.43it/s]Extractor Predicting: 295it [03:23,  1.46it/s]Extractor Predicting: 296it [03:23,  1.47it/s]Extractor Predicting: 297it [03:24,  1.48it/s]Extractor Predicting: 298it [03:25,  1.52it/s]Extractor Predicting: 299it [03:25,  1.51it/s]Extractor Predicting: 300it [03:26,  1.54it/s]Extractor Predicting: 301it [03:27,  1.53it/s]Extractor Predicting: 302it [03:27,  1.49it/s]Extractor Predicting: 303it [03:28,  1.46it/s]Extractor Predicting: 304it [03:29,  1.48it/s]Extractor Predicting: 305it [03:29,  1.49it/s]Extractor Predicting: 306it [03:30,  1.49it/s]Extractor Predicting: 307it [03:31,  1.49it/s]Extractor Predicting: 308it [03:31,  1.49it/s]Extractor Predicting: 309it [03:32,  1.47it/s]Extractor Predicting: 310it [03:33,  1.47it/s]Extractor Predicting: 311it [03:34,  1.47it/s]Extractor Predicting: 312it [03:34,  1.48it/s]Extractor Predicting: 313it [03:35,  1.46it/s]Extractor Predicting: 314it [03:36,  1.46it/s]Extractor Predicting: 315it [03:36,  1.50it/s]Extractor Predicting: 316it [03:37,  1.48it/s]Extractor Predicting: 317it [03:38,  1.46it/s]Extractor Predicting: 318it [03:38,  1.49it/s]Extractor Predicting: 319it [03:39,  1.44it/s]Extractor Predicting: 320it [03:40,  1.46it/s]Extractor Predicting: 321it [03:40,  1.47it/s]Extractor Predicting: 322it [03:41,  1.48it/s]Extractor Predicting: 323it [03:42,  1.48it/s]Extractor Predicting: 324it [03:42,  1.50it/s]Extractor Predicting: 325it [03:43,  1.53it/s]Extractor Predicting: 326it [03:44,  1.52it/s]Extractor Predicting: 327it [03:44,  1.53it/s]Extractor Predicting: 328it [03:45,  1.50it/s]Extractor Predicting: 329it [03:46,  1.51it/s]Extractor Predicting: 330it [03:46,  1.52it/s]Extractor Predicting: 331it [03:47,  1.50it/s]Extractor Predicting: 332it [03:48,  1.48it/s]Extractor Predicting: 333it [03:48,  1.46it/s]Extractor Predicting: 334it [03:49,  1.44it/s]Extractor Predicting: 335it [03:50,  1.45it/s]Extractor Predicting: 336it [03:50,  1.47it/s]Extractor Predicting: 337it [03:51,  1.27it/s]Extractor Predicting: 338it [03:52,  1.31it/s]Extractor Predicting: 339it [03:53,  1.35it/s]Extractor Predicting: 340it [03:53,  1.42it/s]Extractor Predicting: 341it [03:54,  1.41it/s]Extractor Predicting: 342it [03:55,  1.45it/s]Extractor Predicting: 343it [03:55,  1.49it/s]Extractor Predicting: 344it [03:56,  1.53it/s]Extractor Predicting: 345it [03:57,  1.56it/s]Extractor Predicting: 346it [03:57,  1.60it/s]Extractor Predicting: 347it [03:58,  1.63it/s]Extractor Predicting: 348it [03:58,  1.61it/s]Extractor Predicting: 349it [03:59,  1.62it/s]Extractor Predicting: 350it [04:00,  1.58it/s]Extractor Predicting: 351it [04:00,  1.61it/s]Extractor Predicting: 352it [04:01,  1.56it/s]Extractor Predicting: 353it [04:02,  1.52it/s]Extractor Predicting: 354it [04:03,  1.45it/s]Extractor Predicting: 355it [04:03,  1.49it/s]Extractor Predicting: 356it [04:04,  1.48it/s]Extractor Predicting: 357it [04:05,  1.48it/s]Extractor Predicting: 358it [04:05,  1.47it/s]Extractor Predicting: 359it [04:06,  1.45it/s]Extractor Predicting: 360it [04:07,  1.45it/s]Extractor Predicting: 361it [04:07,  1.45it/s]Extractor Predicting: 362it [04:08,  1.46it/s]Extractor Predicting: 363it [04:09,  1.48it/s]Extractor Predicting: 364it [04:09,  1.50it/s]Extractor Predicting: 365it [04:10,  1.47it/s]Extractor Predicting: 366it [04:11,  1.46it/s]Extractor Predicting: 367it [04:11,  1.46it/s]Extractor Predicting: 368it [04:12,  1.46it/s]Extractor Predicting: 369it [04:13,  1.44it/s]Extractor Predicting: 370it [04:13,  1.46it/s]Extractor Predicting: 371it [04:14,  1.46it/s]Extractor Predicting: 372it [04:15,  1.44it/s]Extractor Predicting: 373it [04:16,  1.44it/s]Extractor Predicting: 374it [04:16,  1.47it/s]Extractor Predicting: 375it [04:17,  1.49it/s]Extractor Predicting: 376it [04:17,  1.52it/s]Extractor Predicting: 377it [04:18,  1.57it/s]Extractor Predicting: 378it [04:19,  1.55it/s]Extractor Predicting: 379it [04:19,  1.56it/s]Extractor Predicting: 380it [04:20,  1.57it/s]Extractor Predicting: 381it [04:21,  1.57it/s]Extractor Predicting: 382it [04:21,  1.56it/s]Extractor Predicting: 383it [04:22,  1.54it/s]Extractor Predicting: 384it [04:23,  1.54it/s]Extractor Predicting: 385it [04:23,  1.56it/s]Extractor Predicting: 386it [04:24,  1.58it/s]Extractor Predicting: 387it [04:24,  1.59it/s]Extractor Predicting: 388it [04:25,  1.58it/s]Extractor Predicting: 389it [04:26,  1.59it/s]Extractor Predicting: 390it [04:26,  1.58it/s]Extractor Predicting: 391it [04:27,  1.56it/s]Extractor Predicting: 392it [04:28,  1.61it/s]Extractor Predicting: 393it [04:28,  1.59it/s]Extractor Predicting: 394it [04:29,  1.61it/s]Extractor Predicting: 395it [04:29,  1.59it/s]Extractor Predicting: 396it [04:30,  1.57it/s]Extractor Predicting: 397it [04:31,  1.54it/s]Extractor Predicting: 398it [04:31,  1.51it/s]Extractor Predicting: 399it [04:32,  1.49it/s]Extractor Predicting: 400it [04:33,  1.47it/s]Extractor Predicting: 401it [04:34,  1.47it/s]Extractor Predicting: 402it [04:34,  1.48it/s]Extractor Predicting: 403it [04:35,  1.51it/s]Extractor Predicting: 404it [04:36,  1.46it/s]Extractor Predicting: 405it [04:36,  1.45it/s]Extractor Predicting: 406it [04:37,  1.46it/s]Extractor Predicting: 407it [04:38,  1.46it/s]Extractor Predicting: 408it [04:38,  1.46it/s]Extractor Predicting: 409it [04:39,  1.43it/s]Extractor Predicting: 410it [04:40,  1.43it/s]Extractor Predicting: 411it [04:40,  1.45it/s]Extractor Predicting: 412it [04:41,  1.47it/s]Extractor Predicting: 413it [04:42,  1.45it/s]Extractor Predicting: 414it [04:43,  1.44it/s]Extractor Predicting: 415it [04:43,  1.45it/s]Extractor Predicting: 416it [04:44,  1.45it/s]Extractor Predicting: 417it [04:45,  1.46it/s]Extractor Predicting: 418it [04:45,  1.49it/s]Extractor Predicting: 419it [04:46,  1.49it/s]Extractor Predicting: 420it [04:47,  1.48it/s]Extractor Predicting: 421it [04:47,  1.48it/s]Extractor Predicting: 422it [04:48,  1.46it/s]Extractor Predicting: 423it [04:49,  1.44it/s]Extractor Predicting: 424it [04:49,  1.47it/s]Extractor Predicting: 425it [04:50,  1.46it/s]Extractor Predicting: 426it [04:51,  1.51it/s]Extractor Predicting: 427it [04:51,  1.55it/s]Extractor Predicting: 428it [04:52,  1.55it/s]Extractor Predicting: 429it [04:52,  1.57it/s]Extractor Predicting: 430it [04:53,  1.58it/s]Extractor Predicting: 431it [04:54,  1.62it/s]Extractor Predicting: 432it [04:54,  1.58it/s]Extractor Predicting: 433it [04:55,  1.55it/s]Extractor Predicting: 434it [04:56,  1.57it/s]Extractor Predicting: 435it [04:56,  1.55it/s]Extractor Predicting: 436it [04:57,  1.53it/s]Extractor Predicting: 437it [04:58,  1.57it/s]Extractor Predicting: 438it [04:58,  1.58it/s]Extractor Predicting: 439it [04:59,  1.58it/s]Extractor Predicting: 440it [04:59,  1.55it/s]Extractor Predicting: 441it [05:00,  1.56it/s]Extractor Predicting: 442it [05:01,  1.58it/s]Extractor Predicting: 443it [05:01,  1.53it/s]Extractor Predicting: 444it [05:02,  1.56it/s]Extractor Predicting: 445it [05:03,  1.57it/s]Extractor Predicting: 446it [05:03,  1.59it/s]Extractor Predicting: 447it [05:04,  1.61it/s]Extractor Predicting: 448it [05:04,  1.62it/s]Extractor Predicting: 449it [05:05,  1.62it/s]Extractor Predicting: 450it [05:06,  1.60it/s]Extractor Predicting: 451it [05:06,  1.60it/s]Extractor Predicting: 452it [05:07,  1.56it/s]Extractor Predicting: 453it [05:08,  1.54it/s]Extractor Predicting: 454it [05:08,  1.49it/s]Extractor Predicting: 455it [05:09,  1.50it/s]Extractor Predicting: 456it [05:10,  1.53it/s]Extractor Predicting: 457it [05:10,  1.53it/s]Extractor Predicting: 458it [05:11,  1.54it/s]Extractor Predicting: 459it [05:12,  1.50it/s]Extractor Predicting: 460it [05:12,  1.46it/s]Extractor Predicting: 461it [05:13,  1.45it/s]Extractor Predicting: 462it [05:14,  1.42it/s]Extractor Predicting: 463it [05:15,  1.42it/s]Extractor Predicting: 464it [05:15,  1.45it/s]Extractor Predicting: 465it [05:16,  1.46it/s]Extractor Predicting: 466it [05:17,  1.47it/s]Extractor Predicting: 467it [05:17,  1.43it/s]Extractor Predicting: 468it [05:18,  1.44it/s]Extractor Predicting: 469it [05:19,  1.47it/s]Extractor Predicting: 470it [05:19,  1.45it/s]Extractor Predicting: 471it [05:20,  1.42it/s]Extractor Predicting: 472it [05:21,  1.42it/s]Extractor Predicting: 473it [05:21,  1.44it/s]Extractor Predicting: 474it [05:22,  1.29it/s]Extractor Predicting: 475it [05:23,  1.34it/s]Extractor Predicting: 476it [05:24,  1.37it/s]Extractor Predicting: 477it [05:25,  1.36it/s]Extractor Predicting: 478it [05:25,  1.36it/s]Extractor Predicting: 479it [05:26,  1.37it/s]Extractor Predicting: 480it [05:27,  1.38it/s]Extractor Predicting: 481it [05:27,  1.41it/s]Extractor Predicting: 482it [05:28,  1.40it/s]Extractor Predicting: 483it [05:29,  1.42it/s]Extractor Predicting: 484it [05:30,  1.42it/s]Extractor Predicting: 485it [05:30,  1.47it/s]Extractor Predicting: 486it [05:31,  1.50it/s]Extractor Predicting: 487it [05:31,  1.47it/s]Extractor Predicting: 488it [05:32,  1.45it/s]Extractor Predicting: 489it [05:33,  1.45it/s]Extractor Predicting: 490it [05:34,  1.44it/s]Extractor Predicting: 491it [05:34,  1.44it/s]Extractor Predicting: 492it [05:35,  1.42it/s]Extractor Predicting: 493it [05:36,  1.43it/s]Extractor Predicting: 494it [05:36,  1.44it/s]Extractor Predicting: 495it [05:37,  1.49it/s]Extractor Predicting: 496it [05:38,  1.49it/s]Extractor Predicting: 496it [05:38,  1.47it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 10529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.43it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.46it/s]Extractor Predicting: 5it [00:03,  1.42it/s]Extractor Predicting: 6it [00:04,  1.38it/s]Extractor Predicting: 7it [00:04,  1.38it/s]Extractor Predicting: 8it [00:05,  1.37it/s]Extractor Predicting: 9it [00:06,  1.39it/s]Extractor Predicting: 10it [00:07,  1.38it/s]Extractor Predicting: 11it [00:07,  1.38it/s]Extractor Predicting: 12it [00:08,  1.37it/s]Extractor Predicting: 13it [00:09,  1.39it/s]Extractor Predicting: 14it [00:10,  1.40it/s]Extractor Predicting: 15it [00:10,  1.37it/s]Extractor Predicting: 16it [00:11,  1.38it/s]Extractor Predicting: 17it [00:12,  1.36it/s]Extractor Predicting: 18it [00:12,  1.35it/s]Extractor Predicting: 19it [00:13,  1.32it/s]Extractor Predicting: 20it [00:14,  1.27it/s]Extractor Predicting: 21it [00:15,  1.28it/s]Extractor Predicting: 22it [00:16,  1.26it/s]Extractor Predicting: 23it [00:17,  1.27it/s]Extractor Predicting: 24it [00:17,  1.30it/s]Extractor Predicting: 25it [00:18,  1.35it/s]Extractor Predicting: 26it [00:19,  1.36it/s]Extractor Predicting: 27it [00:19,  1.35it/s]Extractor Predicting: 28it [00:20,  1.34it/s]Extractor Predicting: 29it [00:21,  1.33it/s]Extractor Predicting: 30it [00:22,  1.34it/s]Extractor Predicting: 31it [00:22,  1.34it/s]Extractor Predicting: 32it [00:23,  1.36it/s]Extractor Predicting: 33it [00:24,  1.40it/s]Extractor Predicting: 34it [00:24,  1.42it/s]Extractor Predicting: 35it [00:25,  1.40it/s]Extractor Predicting: 36it [00:26,  1.41it/s]Extractor Predicting: 37it [00:27,  1.41it/s]Extractor Predicting: 38it [00:27,  1.41it/s]Extractor Predicting: 39it [00:28,  1.43it/s]Extractor Predicting: 40it [00:29,  1.44it/s]Extractor Predicting: 41it [00:29,  1.43it/s]Extractor Predicting: 42it [00:30,  1.45it/s]Extractor Predicting: 43it [00:31,  1.45it/s]Extractor Predicting: 44it [00:31,  1.46it/s]Extractor Predicting: 45it [00:32,  1.47it/s]Extractor Predicting: 46it [00:33,  1.41it/s]Extractor Predicting: 47it [00:34,  1.42it/s]Extractor Predicting: 48it [00:34,  1.44it/s]Extractor Predicting: 49it [00:35,  1.46it/s]Extractor Predicting: 50it [00:36,  1.45it/s]Extractor Predicting: 51it [00:36,  1.46it/s]Extractor Predicting: 52it [00:37,  1.45it/s]Extractor Predicting: 53it [00:38,  1.46it/s]Extractor Predicting: 54it [00:38,  1.43it/s]Extractor Predicting: 55it [00:39,  1.45it/s]Extractor Predicting: 56it [00:40,  1.46it/s]Extractor Predicting: 57it [00:40,  1.49it/s]Extractor Predicting: 58it [00:41,  1.48it/s]Extractor Predicting: 59it [00:42,  1.47it/s]Extractor Predicting: 60it [00:43,  1.39it/s]Extractor Predicting: 61it [00:43,  1.42it/s]Extractor Predicting: 62it [00:44,  1.41it/s]Extractor Predicting: 63it [00:45,  1.39it/s]Extractor Predicting: 64it [00:45,  1.41it/s]Extractor Predicting: 65it [00:46,  1.39it/s]Extractor Predicting: 66it [00:47,  1.45it/s]Extractor Predicting: 67it [00:47,  1.51it/s]Extractor Predicting: 68it [00:48,  1.56it/s]Extractor Predicting: 69it [00:48,  1.64it/s]Extractor Predicting: 70it [00:49,  1.69it/s]Extractor Predicting: 71it [00:50,  1.72it/s]Extractor Predicting: 72it [00:50,  1.71it/s]Extractor Predicting: 73it [00:51,  1.73it/s]Extractor Predicting: 74it [00:51,  1.72it/s]Extractor Predicting: 75it [00:52,  1.70it/s]Extractor Predicting: 76it [00:52,  1.70it/s]Extractor Predicting: 77it [00:53,  1.69it/s]Extractor Predicting: 78it [00:54,  1.71it/s]Extractor Predicting: 79it [00:54,  1.74it/s]Extractor Predicting: 80it [00:55,  1.71it/s]Extractor Predicting: 81it [00:55,  1.70it/s]Extractor Predicting: 82it [00:56,  1.71it/s]Extractor Predicting: 83it [00:57,  1.73it/s]Extractor Predicting: 84it [00:57,  1.74it/s]Extractor Predicting: 85it [00:58,  1.74it/s]Extractor Predicting: 86it [00:58,  1.72it/s]Extractor Predicting: 87it [00:59,  1.77it/s]Extractor Predicting: 88it [00:59,  1.72it/s]Extractor Predicting: 89it [01:00,  1.70it/s]Extractor Predicting: 90it [01:01,  1.69it/s]Extractor Predicting: 91it [01:01,  1.69it/s]Extractor Predicting: 92it [01:02,  1.70it/s]Extractor Predicting: 93it [01:02,  1.72it/s]Extractor Predicting: 94it [01:03,  1.73it/s]Extractor Predicting: 95it [01:04,  1.73it/s]Extractor Predicting: 96it [01:04,  1.62it/s]Extractor Predicting: 97it [01:05,  1.57it/s]Extractor Predicting: 98it [01:06,  1.50it/s]Extractor Predicting: 99it [01:06,  1.44it/s]Extractor Predicting: 100it [01:07,  1.39it/s]Extractor Predicting: 101it [01:08,  1.27it/s]Extractor Predicting: 102it [01:09,  1.29it/s]Extractor Predicting: 103it [01:10,  1.32it/s]Extractor Predicting: 104it [01:10,  1.33it/s]Extractor Predicting: 105it [01:11,  1.33it/s]Extractor Predicting: 106it [01:12,  1.34it/s]Extractor Predicting: 107it [01:13,  1.33it/s]Extractor Predicting: 108it [01:13,  1.31it/s]Extractor Predicting: 109it [01:14,  1.32it/s]Extractor Predicting: 110it [01:15,  1.32it/s]Extractor Predicting: 111it [01:16,  1.32it/s]Extractor Predicting: 112it [01:16,  1.35it/s]Extractor Predicting: 113it [01:17,  1.38it/s]Extractor Predicting: 114it [01:18,  1.44it/s]Extractor Predicting: 115it [01:18,  1.45it/s]Extractor Predicting: 116it [01:19,  1.47it/s]Extractor Predicting: 117it [01:20,  1.48it/s]Extractor Predicting: 118it [01:20,  1.49it/s]Extractor Predicting: 119it [01:21,  1.47it/s]Extractor Predicting: 120it [01:22,  1.51it/s]Extractor Predicting: 121it [01:22,  1.51it/s]Extractor Predicting: 122it [01:23,  1.53it/s]Extractor Predicting: 123it [01:24,  1.51it/s]Extractor Predicting: 124it [01:24,  1.47it/s]Extractor Predicting: 125it [01:25,  1.48it/s]Extractor Predicting: 126it [01:26,  1.48it/s]Extractor Predicting: 127it [01:26,  1.46it/s]Extractor Predicting: 128it [01:27,  1.43it/s]Extractor Predicting: 129it [01:28,  1.44it/s]Extractor Predicting: 130it [01:28,  1.44it/s]Extractor Predicting: 131it [01:29,  1.42it/s]Extractor Predicting: 132it [01:30,  1.39it/s]Extractor Predicting: 133it [01:31,  1.45it/s]Extractor Predicting: 133it [01:31,  1.46it/s]
{
  "path_pred": "outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_train_large/unseen_15_seed_0/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_15_seed_0', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/fewrel/unseen_15_seed_0/generator/synthetic.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 24337
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24437, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/model', pretrained_wv='outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24437, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.558, loss:58618.0749
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.161, loss:2380.7548
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.161, loss:1977.0188
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.184, loss:1892.6893
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.184, loss:1725.5503
>> valid entity prec:0.5268, rec:0.3923, f1:0.4497
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.184, loss:1676.9034
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 1.171, loss:1611.3853
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 300, avg_time 1.170, loss:1470.8132
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 400, avg_time 1.150, loss:1473.5827
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 500, avg_time 1.174, loss:1402.7102
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5651, rec:0.6459, f1:0.6028
>> valid relation prec:1.0000, rec:0.0003, f1:0.0006
>> valid relation with NER prec:1.0000, rec:0.0003, f1:0.0006
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 100, avg_time 1.184, loss:1379.0860
g_step 1200, step 200, avg_time 1.137, loss:1345.4378
g_step 1300, step 300, avg_time 1.150, loss:1328.9487
g_step 1400, step 400, avg_time 1.195, loss:1282.1478
g_step 1500, step 500, avg_time 1.178, loss:1281.5939
>> valid entity prec:0.4843, rec:0.7049, f1:0.5742
>> valid relation prec:0.3603, rec:0.0255, f1:0.0475
>> valid relation with NER prec:0.3603, rec:0.0255, f1:0.0475
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 100, avg_time 1.158, loss:1254.8243
g_step 1700, step 200, avg_time 1.167, loss:1235.5458
g_step 1800, step 300, avg_time 1.166, loss:1233.7702
g_step 1900, step 400, avg_time 1.173, loss:1224.2287
g_step 2000, step 500, avg_time 1.171, loss:1221.8487
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6732, rec:0.6013, f1:0.6352
>> valid relation prec:0.3357, rec:0.0137, f1:0.0264
>> valid relation with NER prec:0.3357, rec:0.0137, f1:0.0264
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 100, avg_time 1.170, loss:1195.9194
g_step 2200, step 200, avg_time 1.165, loss:1139.7705
g_step 2300, step 300, avg_time 1.181, loss:1165.8191
g_step 2400, step 400, avg_time 1.164, loss:1219.1205
g_step 2500, step 500, avg_time 1.149, loss:1135.5165
>> valid entity prec:0.6525, rec:0.6017, f1:0.6261
>> valid relation prec:0.3821, rec:0.1187, f1:0.1811
>> valid relation with NER prec:0.3821, rec:0.1187, f1:0.1811
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2600, step 100, avg_time 1.189, loss:1137.7120
g_step 2700, step 200, avg_time 1.160, loss:1145.5032
g_step 2800, step 300, avg_time 1.167, loss:1122.4479
g_step 2900, step 400, avg_time 1.171, loss:1122.5675
g_step 3000, step 500, avg_time 1.165, loss:1122.0265
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6436, rec:0.6419, f1:0.6427
>> valid relation prec:0.6356, rec:0.0918, f1:0.1604
>> valid relation with NER prec:0.6356, rec:0.0918, f1:0.1604
new max entity f1 on valid!
g_step 3100, step 100, avg_time 1.163, loss:1125.1829
g_step 3200, step 200, avg_time 1.157, loss:1093.1692
g_step 3300, step 300, avg_time 1.187, loss:1067.8703
g_step 3400, step 400, avg_time 1.169, loss:1080.2369
g_step 3500, step 500, avg_time 1.165, loss:1090.0677
>> valid entity prec:0.6069, rec:0.5503, f1:0.5772
>> valid relation prec:0.6462, rec:0.1055, f1:0.1814
>> valid relation with NER prec:0.6462, rec:0.1055, f1:0.1814
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 100, avg_time 1.180, loss:1070.0725
g_step 3700, step 200, avg_time 1.176, loss:1066.4457
g_step 3800, step 300, avg_time 1.156, loss:1050.5606
g_step 3900, step 400, avg_time 1.170, loss:1060.1346
g_step 4000, step 500, avg_time 1.158, loss:1057.8386
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6599, rec:0.7283, f1:0.6924
>> valid relation prec:0.6132, rec:0.1333, f1:0.2189
>> valid relation with NER prec:0.6132, rec:0.1333, f1:0.2189
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 100, avg_time 1.173, loss:1009.4603
g_step 4200, step 200, avg_time 1.166, loss:1026.4299
g_step 4300, step 300, avg_time 1.201, loss:1062.9299
g_step 4400, step 400, avg_time 1.162, loss:1034.1084
g_step 4500, step 500, avg_time 1.164, loss:1039.9782
>> valid entity prec:0.6672, rec:0.5897, f1:0.6260
>> valid relation prec:0.6028, rec:0.1250, f1:0.2070
>> valid relation with NER prec:0.6028, rec:0.1250, f1:0.2070
g_step 4600, step 100, avg_time 1.176, loss:1024.4030
g_step 4700, step 200, avg_time 1.184, loss:994.2962
g_step 4800, step 300, avg_time 1.158, loss:1005.6496
g_step 4900, step 400, avg_time 1.168, loss:1030.7624
g_step 5000, step 500, avg_time 1.165, loss:1007.7229
learning rate was adjusted to 0.0008
>> valid entity prec:0.6822, rec:0.6523, f1:0.6669
>> valid relation prec:0.6005, rec:0.1427, f1:0.2306
>> valid relation with NER prec:0.6005, rec:0.1427, f1:0.2306
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5100, step 100, avg_time 1.163, loss:944.7659
g_step 5200, step 200, avg_time 1.178, loss:986.4498
g_step 5300, step 300, avg_time 1.173, loss:976.3384
g_step 5400, step 400, avg_time 1.191, loss:1011.4818
g_step 5500, step 500, avg_time 1.153, loss:1007.8543
>> valid entity prec:0.6294, rec:0.7303, f1:0.6761
>> valid relation prec:0.4522, rec:0.1773, f1:0.2547
>> valid relation with NER prec:0.4522, rec:0.1773, f1:0.2547
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 5600, step 100, avg_time 1.180, loss:955.8782
g_step 5700, step 200, avg_time 1.197, loss:936.9207
g_step 5800, step 300, avg_time 1.153, loss:969.7375
g_step 5900, step 400, avg_time 1.164, loss:951.4667
g_step 6000, step 500, avg_time 1.188, loss:975.6813
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.6551, rec:0.6456, f1:0.6503
>> valid relation prec:0.5842, rec:0.1518, f1:0.2410
>> valid relation with NER prec:0.5842, rec:0.1518, f1:0.2410
g_step 6100, step 100, avg_time 1.183, loss:912.1632
g_step 6200, step 200, avg_time 1.175, loss:942.4265
g_step 6300, step 300, avg_time 1.180, loss:927.3307
g_step 6400, step 400, avg_time 1.171, loss:983.4609
g_step 6500, step 500, avg_time 1.159, loss:919.0117
>> valid entity prec:0.7057, rec:0.5844, f1:0.6393
>> valid relation prec:0.5672, rec:0.1424, f1:0.2277
>> valid relation with NER prec:0.5672, rec:0.1424, f1:0.2277
g_step 6600, step 100, avg_time 1.175, loss:881.2078
g_step 6700, step 200, avg_time 1.187, loss:880.3014
g_step 6800, step 300, avg_time 1.175, loss:947.5178
g_step 6900, step 400, avg_time 1.170, loss:924.1997
g_step 7000, step 500, avg_time 1.169, loss:948.6836
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.6232, rec:0.6394, f1:0.6312
>> valid relation prec:0.5414, rec:0.1870, f1:0.2780
>> valid relation with NER prec:0.5414, rec:0.1870, f1:0.2780
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 7100, step 100, avg_time 1.159, loss:883.5937
g_step 7200, step 200, avg_time 1.162, loss:865.6893
g_step 7300, step 300, avg_time 1.178, loss:868.0202
g_step 7400, step 400, avg_time 1.179, loss:908.3146
g_step 7500, step 500, avg_time 1.193, loss:925.3044
>> valid entity prec:0.6118, rec:0.6578, f1:0.6340
>> valid relation prec:0.4737, rec:0.1441, f1:0.2210
>> valid relation with NER prec:0.4737, rec:0.1441, f1:0.2210
g_step 7600, step 100, avg_time 1.174, loss:856.9069
g_step 7700, step 200, avg_time 1.166, loss:856.3214
g_step 7800, step 300, avg_time 1.159, loss:882.4464
g_step 7900, step 400, avg_time 1.178, loss:859.1681
g_step 8000, step 500, avg_time 1.184, loss:883.3071
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.6355, rec:0.6849, f1:0.6593
>> valid relation prec:0.4679, rec:0.2102, f1:0.2901
>> valid relation with NER prec:0.4679, rec:0.2102, f1:0.2901
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 8100, step 100, avg_time 1.164, loss:813.2570
g_step 8200, step 200, avg_time 1.201, loss:862.4556
g_step 8300, step 300, avg_time 1.164, loss:847.7677
g_step 8400, step 400, avg_time 1.164, loss:859.5465
g_step 8500, step 500, avg_time 1.172, loss:832.0996
>> valid entity prec:0.6532, rec:0.6502, f1:0.6517
>> valid relation prec:0.6104, rec:0.1904, f1:0.2903
>> valid relation with NER prec:0.6104, rec:0.1904, f1:0.2903
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 8600, step 100, avg_time 1.162, loss:801.5914
g_step 8700, step 200, avg_time 1.172, loss:824.9897
g_step 8800, step 300, avg_time 1.184, loss:816.6480
g_step 8900, step 400, avg_time 1.156, loss:818.7194
g_step 9000, step 500, avg_time 1.194, loss:845.9808
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.5867, rec:0.6575, f1:0.6201
>> valid relation prec:0.4697, rec:0.2128, f1:0.2929
>> valid relation with NER prec:0.4697, rec:0.2128, f1:0.2929
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 9100, step 100, avg_time 1.188, loss:789.4667
g_step 9200, step 200, avg_time 1.181, loss:797.3664
g_step 9300, step 300, avg_time 1.171, loss:797.6300
g_step 9400, step 400, avg_time 1.170, loss:816.2316
g_step 9500, step 500, avg_time 1.160, loss:802.8410
>> valid entity prec:0.6364, rec:0.6132, f1:0.6246
>> valid relation prec:0.4602, rec:0.1421, f1:0.2172
>> valid relation with NER prec:0.4602, rec:0.1421, f1:0.2172
g_step 9600, step 100, avg_time 1.170, loss:769.0185
g_step 9700, step 200, avg_time 1.200, loss:768.4993
g_step 9800, step 300, avg_time 1.165, loss:774.1416
g_step 9900, step 400, avg_time 1.178, loss:804.8201
g_step 10000, step 500, avg_time 1.159, loss:783.3503
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.5980, rec:0.6457, f1:0.6209
>> valid relation prec:0.5504, rec:0.1390, f1:0.2219
>> valid relation with NER prec:0.5504, rec:0.1390, f1:0.2219
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.22s/it]Extractor Predicting: 2it [00:07,  3.31s/it]Extractor Predicting: 3it [00:08,  2.10s/it]Extractor Predicting: 4it [00:09,  1.52s/it]Extractor Predicting: 5it [00:09,  1.20s/it]Extractor Predicting: 6it [00:10,  1.04s/it]Extractor Predicting: 7it [00:11,  1.10it/s]Extractor Predicting: 8it [00:11,  1.21it/s]Extractor Predicting: 9it [00:12,  1.28it/s]Extractor Predicting: 10it [00:13,  1.34it/s]Extractor Predicting: 11it [00:13,  1.38it/s]Extractor Predicting: 12it [00:14,  1.43it/s]Extractor Predicting: 13it [00:15,  1.48it/s]Extractor Predicting: 14it [00:15,  1.49it/s]Extractor Predicting: 15it [00:16,  1.52it/s]Extractor Predicting: 16it [00:16,  1.52it/s]Extractor Predicting: 17it [00:17,  1.53it/s]Extractor Predicting: 18it [00:18,  1.54it/s]Extractor Predicting: 19it [00:18,  1.53it/s]Extractor Predicting: 20it [00:19,  1.50it/s]Extractor Predicting: 21it [00:20,  1.50it/s]Extractor Predicting: 22it [00:20,  1.48it/s]Extractor Predicting: 23it [00:21,  1.51it/s]Extractor Predicting: 24it [00:22,  1.56it/s]Extractor Predicting: 25it [00:22,  1.54it/s]Extractor Predicting: 26it [00:23,  1.58it/s]Extractor Predicting: 27it [00:24,  1.60it/s]Extractor Predicting: 28it [00:24,  1.61it/s]Extractor Predicting: 29it [00:25,  1.59it/s]Extractor Predicting: 30it [00:25,  1.61it/s]Extractor Predicting: 31it [00:26,  1.59it/s]Extractor Predicting: 32it [00:27,  1.57it/s]Extractor Predicting: 33it [00:27,  1.58it/s]Extractor Predicting: 34it [00:28,  1.55it/s]Extractor Predicting: 35it [00:29,  1.57it/s]Extractor Predicting: 36it [00:29,  1.55it/s]Extractor Predicting: 37it [00:30,  1.54it/s]Extractor Predicting: 38it [00:31,  1.51it/s]Extractor Predicting: 39it [00:31,  1.54it/s]Extractor Predicting: 40it [00:32,  1.53it/s]Extractor Predicting: 41it [00:33,  1.50it/s]Extractor Predicting: 42it [00:33,  1.50it/s]Extractor Predicting: 43it [00:34,  1.53it/s]Extractor Predicting: 44it [00:35,  1.52it/s]Extractor Predicting: 45it [00:35,  1.52it/s]Extractor Predicting: 46it [00:36,  1.56it/s]Extractor Predicting: 47it [00:37,  1.54it/s]Extractor Predicting: 48it [00:37,  1.52it/s]Extractor Predicting: 49it [00:38,  1.50it/s]Extractor Predicting: 50it [00:39,  1.47it/s]Extractor Predicting: 51it [00:39,  1.46it/s]Extractor Predicting: 52it [00:40,  1.45it/s]Extractor Predicting: 53it [00:41,  1.47it/s]Extractor Predicting: 54it [00:41,  1.47it/s]Extractor Predicting: 55it [00:42,  1.48it/s]Extractor Predicting: 56it [00:43,  1.50it/s]Extractor Predicting: 57it [00:43,  1.50it/s]Extractor Predicting: 58it [00:44,  1.49it/s]Extractor Predicting: 59it [00:45,  1.51it/s]Extractor Predicting: 60it [00:45,  1.47it/s]Extractor Predicting: 61it [00:46,  1.45it/s]Extractor Predicting: 62it [00:47,  1.43it/s]Extractor Predicting: 63it [00:47,  1.43it/s]Extractor Predicting: 64it [00:48,  1.43it/s]Extractor Predicting: 65it [00:49,  1.32it/s]Extractor Predicting: 66it [00:50,  1.33it/s]Extractor Predicting: 67it [00:51,  1.34it/s]Extractor Predicting: 68it [00:51,  1.35it/s]Extractor Predicting: 69it [00:52,  1.36it/s]Extractor Predicting: 70it [00:53,  1.36it/s]Extractor Predicting: 71it [00:53,  1.36it/s]Extractor Predicting: 72it [00:54,  1.36it/s]Extractor Predicting: 73it [00:55,  1.35it/s]Extractor Predicting: 74it [00:56,  1.36it/s]Extractor Predicting: 75it [00:56,  1.34it/s]Extractor Predicting: 76it [00:57,  1.37it/s]Extractor Predicting: 77it [00:58,  1.37it/s]Extractor Predicting: 78it [00:59,  1.37it/s]Extractor Predicting: 79it [00:59,  1.39it/s]Extractor Predicting: 80it [01:00,  1.37it/s]Extractor Predicting: 81it [01:01,  1.35it/s]Extractor Predicting: 82it [01:02,  1.35it/s]Extractor Predicting: 83it [01:02,  1.39it/s]Extractor Predicting: 84it [01:03,  1.40it/s]Extractor Predicting: 85it [01:04,  1.41it/s]Extractor Predicting: 86it [01:04,  1.41it/s]Extractor Predicting: 87it [01:05,  1.38it/s]Extractor Predicting: 88it [01:06,  1.39it/s]Extractor Predicting: 89it [01:06,  1.42it/s]Extractor Predicting: 90it [01:07,  1.43it/s]Extractor Predicting: 91it [01:08,  1.44it/s]Extractor Predicting: 92it [01:09,  1.41it/s]Extractor Predicting: 93it [01:09,  1.45it/s]Extractor Predicting: 94it [01:10,  1.46it/s]Extractor Predicting: 95it [01:11,  1.44it/s]Extractor Predicting: 96it [01:11,  1.43it/s]Extractor Predicting: 97it [01:12,  1.44it/s]Extractor Predicting: 98it [01:13,  1.43it/s]Extractor Predicting: 99it [01:13,  1.45it/s]Extractor Predicting: 100it [01:14,  1.47it/s]Extractor Predicting: 101it [01:15,  1.47it/s]Extractor Predicting: 102it [01:15,  1.44it/s]Extractor Predicting: 103it [01:16,  1.41it/s]Extractor Predicting: 104it [01:17,  1.43it/s]Extractor Predicting: 105it [01:18,  1.44it/s]Extractor Predicting: 106it [01:18,  1.44it/s]Extractor Predicting: 107it [01:19,  1.42it/s]Extractor Predicting: 108it [01:20,  1.43it/s]Extractor Predicting: 109it [01:20,  1.42it/s]Extractor Predicting: 110it [01:21,  1.41it/s]Extractor Predicting: 111it [01:22,  1.41it/s]Extractor Predicting: 112it [01:23,  1.41it/s]Extractor Predicting: 113it [01:23,  1.42it/s]Extractor Predicting: 114it [01:24,  1.42it/s]Extractor Predicting: 115it [01:25,  1.43it/s]Extractor Predicting: 116it [01:25,  1.42it/s]Extractor Predicting: 117it [01:26,  1.45it/s]Extractor Predicting: 118it [01:27,  1.48it/s]Extractor Predicting: 119it [01:27,  1.52it/s]Extractor Predicting: 120it [01:28,  1.53it/s]Extractor Predicting: 121it [01:29,  1.52it/s]Extractor Predicting: 122it [01:29,  1.56it/s]Extractor Predicting: 123it [01:30,  1.58it/s]Extractor Predicting: 124it [01:30,  1.56it/s]Extractor Predicting: 125it [01:31,  1.57it/s]Extractor Predicting: 126it [01:32,  1.59it/s]Extractor Predicting: 127it [01:32,  1.62it/s]Extractor Predicting: 128it [01:33,  1.61it/s]Extractor Predicting: 129it [01:34,  1.63it/s]Extractor Predicting: 130it [01:34,  1.57it/s]Extractor Predicting: 131it [01:35,  1.57it/s]Extractor Predicting: 132it [01:35,  1.63it/s]Extractor Predicting: 133it [01:36,  1.64it/s]Extractor Predicting: 134it [01:37,  1.56it/s]Extractor Predicting: 135it [01:37,  1.58it/s]Extractor Predicting: 136it [01:38,  1.62it/s]Extractor Predicting: 137it [01:39,  1.63it/s]Extractor Predicting: 138it [01:39,  1.61it/s]Extractor Predicting: 139it [01:40,  1.62it/s]Extractor Predicting: 140it [01:40,  1.66it/s]Extractor Predicting: 141it [01:41,  1.67it/s]Extractor Predicting: 142it [01:41,  1.70it/s]Extractor Predicting: 143it [01:42,  1.67it/s]Extractor Predicting: 144it [01:43,  1.64it/s]Extractor Predicting: 145it [01:43,  1.82it/s]Extractor Predicting: 145it [01:43,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6100234925606891,
  "recall": 0.2227623677437804,
  "score": 0.32635106828655214,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.47it/s]Extractor Predicting: 3it [00:02,  1.48it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.46it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.44it/s]Extractor Predicting: 10it [00:06,  1.45it/s]Extractor Predicting: 11it [00:07,  1.46it/s]Extractor Predicting: 12it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.47it/s]Extractor Predicting: 14it [00:09,  1.48it/s]Extractor Predicting: 15it [00:10,  1.50it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:12,  1.48it/s]Extractor Predicting: 19it [00:12,  1.46it/s]Extractor Predicting: 20it [00:13,  1.46it/s]Extractor Predicting: 21it [00:14,  1.45it/s]Extractor Predicting: 22it [00:15,  1.41it/s]Extractor Predicting: 23it [00:15,  1.44it/s]Extractor Predicting: 24it [00:16,  1.47it/s]Extractor Predicting: 25it [00:17,  1.49it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:18,  1.45it/s]Extractor Predicting: 28it [00:19,  1.45it/s]Extractor Predicting: 29it [00:19,  1.44it/s]Extractor Predicting: 30it [00:20,  1.41it/s]Extractor Predicting: 31it [00:21,  1.42it/s]Extractor Predicting: 32it [00:21,  1.42it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:23,  1.46it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.48it/s]Extractor Predicting: 37it [00:25,  1.53it/s]Extractor Predicting: 38it [00:25,  1.52it/s]Extractor Predicting: 39it [00:26,  1.51it/s]Extractor Predicting: 40it [00:27,  1.52it/s]Extractor Predicting: 41it [00:27,  1.51it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:29,  1.50it/s]Extractor Predicting: 44it [00:29,  1.50it/s]Extractor Predicting: 45it [00:30,  1.48it/s]Extractor Predicting: 46it [00:31,  1.51it/s]Extractor Predicting: 47it [00:31,  1.50it/s]Extractor Predicting: 48it [00:32,  1.50it/s]Extractor Predicting: 49it [00:33,  1.50it/s]Extractor Predicting: 50it [00:33,  1.48it/s]Extractor Predicting: 51it [00:34,  1.48it/s]Extractor Predicting: 52it [00:35,  1.49it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.50it/s]Extractor Predicting: 55it [00:37,  1.46it/s]Extractor Predicting: 56it [00:37,  1.48it/s]Extractor Predicting: 57it [00:38,  1.51it/s]Extractor Predicting: 58it [00:39,  1.53it/s]Extractor Predicting: 59it [00:39,  1.53it/s]Extractor Predicting: 60it [00:40,  1.53it/s]Extractor Predicting: 61it [00:41,  1.52it/s]Extractor Predicting: 62it [00:41,  1.52it/s]Extractor Predicting: 63it [00:42,  1.51it/s]Extractor Predicting: 64it [00:43,  1.50it/s]Extractor Predicting: 65it [00:43,  1.49it/s]Extractor Predicting: 66it [00:44,  1.47it/s]Extractor Predicting: 67it [00:45,  1.49it/s]Extractor Predicting: 68it [00:45,  1.53it/s]Extractor Predicting: 69it [00:46,  1.54it/s]Extractor Predicting: 70it [00:47,  1.53it/s]Extractor Predicting: 71it [00:47,  1.51it/s]Extractor Predicting: 72it [00:48,  1.52it/s]Extractor Predicting: 73it [00:49,  1.47it/s]Extractor Predicting: 74it [00:49,  1.48it/s]Extractor Predicting: 75it [00:50,  1.49it/s]Extractor Predicting: 76it [00:51,  1.51it/s]Extractor Predicting: 77it [00:51,  1.47it/s]Extractor Predicting: 78it [00:52,  1.47it/s]Extractor Predicting: 79it [00:53,  1.47it/s]Extractor Predicting: 80it [00:54,  1.33it/s]Extractor Predicting: 81it [00:54,  1.38it/s]Extractor Predicting: 82it [00:55,  1.39it/s]Extractor Predicting: 83it [00:56,  1.41it/s]Extractor Predicting: 84it [00:56,  1.42it/s]Extractor Predicting: 85it [00:57,  1.46it/s]Extractor Predicting: 86it [00:58,  1.45it/s]Extractor Predicting: 87it [00:59,  1.45it/s]Extractor Predicting: 88it [00:59,  1.46it/s]Extractor Predicting: 89it [01:00,  1.46it/s]Extractor Predicting: 90it [01:01,  1.48it/s]Extractor Predicting: 91it [01:01,  1.48it/s]Extractor Predicting: 92it [01:02,  1.46it/s]Extractor Predicting: 93it [01:03,  1.44it/s]Extractor Predicting: 94it [01:03,  1.47it/s]Extractor Predicting: 95it [01:04,  1.46it/s]Extractor Predicting: 96it [01:05,  1.45it/s]Extractor Predicting: 97it [01:05,  1.47it/s]Extractor Predicting: 98it [01:06,  1.48it/s]Extractor Predicting: 99it [01:07,  1.49it/s]Extractor Predicting: 100it [01:07,  1.50it/s]Extractor Predicting: 101it [01:08,  1.52it/s]Extractor Predicting: 102it [01:09,  1.52it/s]Extractor Predicting: 103it [01:09,  1.50it/s]Extractor Predicting: 104it [01:10,  1.46it/s]Extractor Predicting: 105it [01:11,  1.45it/s]Extractor Predicting: 106it [01:11,  1.46it/s]Extractor Predicting: 107it [01:12,  1.45it/s]Extractor Predicting: 108it [01:13,  1.47it/s]Extractor Predicting: 109it [01:13,  1.45it/s]Extractor Predicting: 110it [01:14,  1.47it/s]Extractor Predicting: 111it [01:15,  1.47it/s]Extractor Predicting: 112it [01:16,  1.44it/s]Extractor Predicting: 113it [01:16,  1.44it/s]Extractor Predicting: 114it [01:17,  1.43it/s]Extractor Predicting: 115it [01:18,  1.41it/s]Extractor Predicting: 116it [01:18,  1.40it/s]Extractor Predicting: 117it [01:19,  1.42it/s]Extractor Predicting: 118it [01:20,  1.42it/s]Extractor Predicting: 119it [01:20,  1.43it/s]Extractor Predicting: 120it [01:21,  1.45it/s]Extractor Predicting: 121it [01:22,  1.47it/s]Extractor Predicting: 122it [01:22,  1.49it/s]Extractor Predicting: 123it [01:23,  1.49it/s]Extractor Predicting: 124it [01:24,  1.49it/s]Extractor Predicting: 125it [01:24,  1.47it/s]Extractor Predicting: 126it [01:25,  1.45it/s]Extractor Predicting: 127it [01:26,  1.45it/s]Extractor Predicting: 128it [01:27,  1.45it/s]Extractor Predicting: 129it [01:27,  1.47it/s]Extractor Predicting: 130it [01:28,  1.47it/s]Extractor Predicting: 131it [01:29,  1.48it/s]Extractor Predicting: 132it [01:29,  1.50it/s]Extractor Predicting: 133it [01:30,  1.47it/s]Extractor Predicting: 134it [01:31,  1.46it/s]Extractor Predicting: 135it [01:31,  1.45it/s]Extractor Predicting: 136it [01:32,  1.45it/s]Extractor Predicting: 137it [01:33,  1.44it/s]Extractor Predicting: 138it [01:33,  1.49it/s]Extractor Predicting: 139it [01:34,  1.47it/s]Extractor Predicting: 140it [01:35,  1.45it/s]Extractor Predicting: 141it [01:35,  1.46it/s]Extractor Predicting: 142it [01:36,  1.47it/s]Extractor Predicting: 143it [01:37,  1.47it/s]Extractor Predicting: 144it [01:37,  1.45it/s]Extractor Predicting: 145it [01:38,  1.47it/s]Extractor Predicting: 146it [01:39,  1.47it/s]Extractor Predicting: 147it [01:39,  1.51it/s]Extractor Predicting: 148it [01:40,  1.48it/s]Extractor Predicting: 149it [01:41,  1.50it/s]Extractor Predicting: 150it [01:41,  1.50it/s]Extractor Predicting: 151it [01:42,  1.53it/s]Extractor Predicting: 152it [01:43,  1.55it/s]Extractor Predicting: 153it [01:43,  1.54it/s]Extractor Predicting: 154it [01:44,  1.55it/s]Extractor Predicting: 155it [01:45,  1.53it/s]Extractor Predicting: 156it [01:45,  1.54it/s]Extractor Predicting: 157it [01:46,  1.51it/s]Extractor Predicting: 158it [01:47,  1.47it/s]Extractor Predicting: 159it [01:47,  1.48it/s]Extractor Predicting: 160it [01:48,  1.52it/s]Extractor Predicting: 161it [01:49,  1.51it/s]Extractor Predicting: 162it [01:49,  1.52it/s]Extractor Predicting: 163it [01:50,  1.54it/s]Extractor Predicting: 164it [01:51,  1.53it/s]Extractor Predicting: 165it [01:51,  1.52it/s]Extractor Predicting: 166it [01:52,  1.51it/s]Extractor Predicting: 167it [01:53,  1.51it/s]Extractor Predicting: 168it [01:53,  1.51it/s]Extractor Predicting: 169it [01:54,  1.54it/s]Extractor Predicting: 170it [01:55,  1.55it/s]Extractor Predicting: 171it [01:55,  1.52it/s]Extractor Predicting: 172it [01:56,  1.50it/s]Extractor Predicting: 173it [01:57,  1.50it/s]Extractor Predicting: 174it [01:57,  1.53it/s]Extractor Predicting: 175it [01:58,  1.55it/s]Extractor Predicting: 176it [01:58,  1.55it/s]Extractor Predicting: 177it [01:59,  1.50it/s]Extractor Predicting: 178it [02:00,  1.53it/s]Extractor Predicting: 179it [02:01,  1.51it/s]Extractor Predicting: 180it [02:01,  1.51it/s]Extractor Predicting: 181it [02:02,  1.51it/s]Extractor Predicting: 182it [02:03,  1.49it/s]Extractor Predicting: 183it [02:03,  1.50it/s]Extractor Predicting: 184it [02:04,  1.51it/s]Extractor Predicting: 185it [02:04,  1.53it/s]Extractor Predicting: 186it [02:05,  1.55it/s]Extractor Predicting: 187it [02:06,  1.59it/s]Extractor Predicting: 188it [02:06,  1.56it/s]Extractor Predicting: 189it [02:07,  1.51it/s]Extractor Predicting: 190it [02:08,  1.35it/s]Extractor Predicting: 191it [02:09,  1.37it/s]Extractor Predicting: 192it [02:09,  1.42it/s]Extractor Predicting: 193it [02:10,  1.44it/s]Extractor Predicting: 194it [02:11,  1.47it/s]Extractor Predicting: 195it [02:11,  1.49it/s]Extractor Predicting: 196it [02:12,  1.50it/s]Extractor Predicting: 197it [02:13,  1.47it/s]Extractor Predicting: 198it [02:13,  1.48it/s]Extractor Predicting: 199it [02:14,  1.47it/s]Extractor Predicting: 200it [02:15,  1.47it/s]Extractor Predicting: 201it [02:15,  1.48it/s]Extractor Predicting: 202it [02:16,  1.48it/s]Extractor Predicting: 203it [02:17,  1.47it/s]Extractor Predicting: 204it [02:17,  1.45it/s]Extractor Predicting: 205it [02:18,  1.42it/s]Extractor Predicting: 206it [02:19,  1.46it/s]Extractor Predicting: 207it [02:20,  1.46it/s]Extractor Predicting: 208it [02:20,  1.49it/s]Extractor Predicting: 209it [02:21,  1.51it/s]Extractor Predicting: 210it [02:21,  1.49it/s]Extractor Predicting: 211it [02:22,  1.48it/s]Extractor Predicting: 212it [02:23,  1.47it/s]Extractor Predicting: 213it [02:24,  1.46it/s]Extractor Predicting: 214it [02:24,  1.46it/s]Extractor Predicting: 215it [02:25,  1.45it/s]Extractor Predicting: 216it [02:26,  1.46it/s]Extractor Predicting: 217it [02:26,  1.45it/s]Extractor Predicting: 218it [02:27,  1.45it/s]Extractor Predicting: 219it [02:28,  1.45it/s]Extractor Predicting: 220it [02:28,  1.48it/s]Extractor Predicting: 221it [02:29,  1.51it/s]Extractor Predicting: 222it [02:30,  1.46it/s]Extractor Predicting: 223it [02:30,  1.43it/s]Extractor Predicting: 224it [02:31,  1.49it/s]Extractor Predicting: 225it [02:32,  1.49it/s]Extractor Predicting: 226it [02:32,  1.47it/s]Extractor Predicting: 227it [02:33,  1.47it/s]Extractor Predicting: 228it [02:34,  1.48it/s]Extractor Predicting: 229it [02:34,  1.51it/s]Extractor Predicting: 230it [02:35,  1.46it/s]Extractor Predicting: 231it [02:36,  1.48it/s]Extractor Predicting: 232it [02:36,  1.50it/s]Extractor Predicting: 233it [02:37,  1.48it/s]Extractor Predicting: 234it [02:38,  1.46it/s]Extractor Predicting: 235it [02:39,  1.47it/s]Extractor Predicting: 236it [02:39,  1.47it/s]Extractor Predicting: 237it [02:40,  1.45it/s]Extractor Predicting: 238it [02:41,  1.46it/s]Extractor Predicting: 239it [02:41,  1.46it/s]Extractor Predicting: 240it [02:42,  1.45it/s]Extractor Predicting: 241it [02:43,  1.47it/s]Extractor Predicting: 242it [02:43,  1.49it/s]Extractor Predicting: 243it [02:44,  1.46it/s]Extractor Predicting: 244it [02:45,  1.48it/s]Extractor Predicting: 245it [02:45,  1.47it/s]Extractor Predicting: 246it [02:46,  1.49it/s]Extractor Predicting: 247it [02:47,  1.48it/s]Extractor Predicting: 248it [02:47,  1.48it/s]Extractor Predicting: 249it [02:48,  1.49it/s]Extractor Predicting: 250it [02:49,  1.47it/s]Extractor Predicting: 251it [02:49,  1.49it/s]Extractor Predicting: 252it [02:50,  1.47it/s]Extractor Predicting: 253it [02:51,  1.48it/s]Extractor Predicting: 254it [02:51,  1.48it/s]Extractor Predicting: 255it [02:52,  1.48it/s]Extractor Predicting: 256it [02:53,  1.48it/s]Extractor Predicting: 257it [02:53,  1.51it/s]Extractor Predicting: 258it [02:54,  1.50it/s]Extractor Predicting: 259it [02:55,  1.52it/s]Extractor Predicting: 260it [02:55,  1.51it/s]Extractor Predicting: 261it [02:56,  1.51it/s]Extractor Predicting: 262it [02:57,  1.50it/s]Extractor Predicting: 263it [02:57,  1.51it/s]Extractor Predicting: 264it [02:58,  1.47it/s]Extractor Predicting: 265it [02:59,  1.47it/s]Extractor Predicting: 266it [02:59,  1.47it/s]Extractor Predicting: 267it [03:00,  1.49it/s]Extractor Predicting: 268it [03:01,  1.51it/s]Extractor Predicting: 269it [03:01,  1.50it/s]Extractor Predicting: 270it [03:02,  1.50it/s]Extractor Predicting: 271it [03:03,  1.50it/s]Extractor Predicting: 272it [03:03,  1.55it/s]Extractor Predicting: 273it [03:04,  1.55it/s]Extractor Predicting: 274it [03:05,  1.56it/s]Extractor Predicting: 275it [03:05,  1.53it/s]Extractor Predicting: 276it [03:06,  1.50it/s]Extractor Predicting: 277it [03:07,  1.52it/s]Extractor Predicting: 278it [03:07,  1.50it/s]Extractor Predicting: 279it [03:08,  1.51it/s]Extractor Predicting: 280it [03:09,  1.49it/s]Extractor Predicting: 281it [03:09,  1.49it/s]Extractor Predicting: 282it [03:10,  1.48it/s]Extractor Predicting: 283it [03:11,  1.47it/s]Extractor Predicting: 284it [03:11,  1.49it/s]Extractor Predicting: 285it [03:12,  1.48it/s]Extractor Predicting: 286it [03:13,  1.46it/s]Extractor Predicting: 287it [03:13,  1.47it/s]Extractor Predicting: 288it [03:14,  1.49it/s]Extractor Predicting: 289it [03:15,  1.49it/s]Extractor Predicting: 290it [03:15,  1.49it/s]Extractor Predicting: 291it [03:16,  1.48it/s]Extractor Predicting: 292it [03:17,  1.50it/s]Extractor Predicting: 293it [03:17,  1.49it/s]Extractor Predicting: 294it [03:18,  1.47it/s]Extractor Predicting: 295it [03:19,  1.50it/s]Extractor Predicting: 296it [03:19,  1.51it/s]Extractor Predicting: 297it [03:20,  1.47it/s]Extractor Predicting: 298it [03:21,  1.51it/s]Extractor Predicting: 299it [03:21,  1.50it/s]Extractor Predicting: 300it [03:22,  1.49it/s]Extractor Predicting: 301it [03:23,  1.54it/s]Extractor Predicting: 302it [03:23,  1.52it/s]Extractor Predicting: 303it [03:24,  1.39it/s]Extractor Predicting: 304it [03:25,  1.41it/s]Extractor Predicting: 305it [03:26,  1.42it/s]Extractor Predicting: 306it [03:26,  1.44it/s]Extractor Predicting: 307it [03:27,  1.44it/s]Extractor Predicting: 308it [03:28,  1.44it/s]Extractor Predicting: 309it [03:28,  1.46it/s]Extractor Predicting: 310it [03:29,  1.45it/s]Extractor Predicting: 311it [03:30,  1.44it/s]Extractor Predicting: 312it [03:31,  1.41it/s]Extractor Predicting: 313it [03:31,  1.43it/s]Extractor Predicting: 314it [03:32,  1.44it/s]Extractor Predicting: 315it [03:33,  1.48it/s]Extractor Predicting: 316it [03:33,  1.49it/s]Extractor Predicting: 317it [03:34,  1.48it/s]Extractor Predicting: 318it [03:35,  1.49it/s]Extractor Predicting: 319it [03:35,  1.48it/s]Extractor Predicting: 320it [03:36,  1.47it/s]Extractor Predicting: 321it [03:37,  1.48it/s]Extractor Predicting: 322it [03:37,  1.46it/s]Extractor Predicting: 323it [03:38,  1.50it/s]Extractor Predicting: 324it [03:39,  1.48it/s]Extractor Predicting: 325it [03:39,  1.50it/s]Extractor Predicting: 326it [03:40,  1.49it/s]Extractor Predicting: 327it [03:41,  1.51it/s]Extractor Predicting: 328it [03:41,  1.52it/s]Extractor Predicting: 329it [03:42,  1.48it/s]Extractor Predicting: 330it [03:43,  1.47it/s]Extractor Predicting: 331it [03:43,  1.46it/s]Extractor Predicting: 332it [03:44,  1.45it/s]Extractor Predicting: 333it [03:45,  1.47it/s]Extractor Predicting: 334it [03:45,  1.46it/s]Extractor Predicting: 335it [03:46,  1.47it/s]Extractor Predicting: 336it [03:47,  1.50it/s]Extractor Predicting: 337it [03:47,  1.52it/s]Extractor Predicting: 338it [03:48,  1.50it/s]Extractor Predicting: 339it [03:49,  1.48it/s]Extractor Predicting: 340it [03:49,  1.49it/s]Extractor Predicting: 341it [03:50,  1.52it/s]Extractor Predicting: 342it [03:51,  1.48it/s]Extractor Predicting: 343it [03:51,  1.49it/s]Extractor Predicting: 344it [03:52,  1.49it/s]Extractor Predicting: 345it [03:53,  1.51it/s]Extractor Predicting: 346it [03:53,  1.51it/s]Extractor Predicting: 347it [03:54,  1.49it/s]Extractor Predicting: 348it [03:55,  1.50it/s]Extractor Predicting: 349it [03:55,  1.48it/s]Extractor Predicting: 350it [03:56,  1.48it/s]Extractor Predicting: 351it [03:57,  1.46it/s]Extractor Predicting: 352it [03:57,  1.44it/s]Extractor Predicting: 353it [03:58,  1.45it/s]Extractor Predicting: 354it [03:59,  1.44it/s]Extractor Predicting: 355it [04:00,  1.46it/s]Extractor Predicting: 356it [04:00,  1.47it/s]Extractor Predicting: 357it [04:01,  1.50it/s]Extractor Predicting: 358it [04:01,  1.51it/s]Extractor Predicting: 359it [04:02,  1.50it/s]Extractor Predicting: 360it [04:03,  1.48it/s]Extractor Predicting: 361it [04:04,  1.49it/s]Extractor Predicting: 362it [04:04,  1.49it/s]Extractor Predicting: 363it [04:05,  1.51it/s]Extractor Predicting: 364it [04:05,  1.51it/s]Extractor Predicting: 365it [04:06,  1.51it/s]Extractor Predicting: 366it [04:07,  1.50it/s]Extractor Predicting: 367it [04:08,  1.48it/s]Extractor Predicting: 368it [04:08,  1.51it/s]Extractor Predicting: 369it [04:09,  1.51it/s]Extractor Predicting: 370it [04:09,  1.54it/s]Extractor Predicting: 371it [04:10,  1.52it/s]Extractor Predicting: 372it [04:11,  1.52it/s]Extractor Predicting: 373it [04:11,  1.51it/s]Extractor Predicting: 374it [04:12,  1.51it/s]Extractor Predicting: 375it [04:13,  1.51it/s]Extractor Predicting: 376it [04:13,  1.51it/s]Extractor Predicting: 377it [04:14,  1.51it/s]Extractor Predicting: 378it [04:15,  1.53it/s]Extractor Predicting: 379it [04:15,  1.48it/s]Extractor Predicting: 380it [04:16,  1.49it/s]Extractor Predicting: 381it [04:17,  1.53it/s]Extractor Predicting: 382it [04:17,  1.53it/s]Extractor Predicting: 383it [04:18,  1.48it/s]Extractor Predicting: 384it [04:19,  1.49it/s]Extractor Predicting: 385it [04:19,  1.50it/s]Extractor Predicting: 386it [04:20,  1.54it/s]Extractor Predicting: 387it [04:21,  1.55it/s]Extractor Predicting: 388it [04:21,  1.55it/s]Extractor Predicting: 389it [04:22,  1.52it/s]Extractor Predicting: 390it [04:23,  1.53it/s]Extractor Predicting: 391it [04:23,  1.53it/s]Extractor Predicting: 392it [04:24,  1.54it/s]Extractor Predicting: 393it [04:25,  1.55it/s]Extractor Predicting: 394it [04:25,  1.58it/s]Extractor Predicting: 395it [04:26,  1.53it/s]Extractor Predicting: 396it [04:27,  1.53it/s]Extractor Predicting: 397it [04:27,  1.54it/s]Extractor Predicting: 398it [04:28,  1.54it/s]Extractor Predicting: 399it [04:28,  1.56it/s]Extractor Predicting: 400it [04:29,  1.60it/s]Extractor Predicting: 401it [04:30,  1.56it/s]Extractor Predicting: 402it [04:31,  1.39it/s]Extractor Predicting: 403it [04:31,  1.42it/s]Extractor Predicting: 404it [04:32,  1.44it/s]Extractor Predicting: 405it [04:33,  1.47it/s]Extractor Predicting: 406it [04:33,  1.47it/s]Extractor Predicting: 407it [04:34,  1.48it/s]Extractor Predicting: 408it [04:35,  1.49it/s]Extractor Predicting: 409it [04:35,  1.52it/s]Extractor Predicting: 409it [04:35,  1.48it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.38507405270244277,
  "recall": 0.20401508203403648,
  "score": 0.26671995736743936,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.34it/s]Extractor Predicting: 2it [00:01,  1.33it/s]Extractor Predicting: 3it [00:02,  1.35it/s]Extractor Predicting: 4it [00:02,  1.39it/s]Extractor Predicting: 5it [00:03,  1.39it/s]Extractor Predicting: 6it [00:04,  1.40it/s]Extractor Predicting: 7it [00:05,  1.40it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.46it/s]Extractor Predicting: 10it [00:07,  1.44it/s]Extractor Predicting: 11it [00:07,  1.47it/s]Extractor Predicting: 12it [00:08,  1.44it/s]Extractor Predicting: 13it [00:09,  1.46it/s]Extractor Predicting: 13it [00:09,  1.43it/s]
{
  "path_pred": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.17708333333333334,
  "recall": 0.024745269286754003,
  "score": 0.04342273307790549,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_synthetic_large/unseen_15_seed_0/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_15_seed_0', 'type': 'synthetic', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
fit {'path_train': 'outputs/wrapper/wiki/unseen_15_seed_0/generator/synthetic.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl'}
train vocab size: 24847
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24947, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/model', pretrained_wv='outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24947, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.436, loss:57768.2628
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.132, loss:2061.5130
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 300, avg_time 1.091, loss:1750.2330
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 400, avg_time 1.137, loss:1644.1391
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 500, avg_time 1.164, loss:1635.7928
>> valid entity prec:0.3841, rec:0.5310, f1:0.4458
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 100, avg_time 1.111, loss:1538.7114
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 200, avg_time 1.102, loss:1503.7687
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 300, avg_time 1.103, loss:1482.6105
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 400, avg_time 1.155, loss:1395.7115
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 500, avg_time 1.100, loss:1393.3670
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.4822, rec:0.2640, f1:0.3412
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 100, avg_time 1.124, loss:1336.6110
g_step 1200, step 200, avg_time 1.099, loss:1311.3575
g_step 1300, step 300, avg_time 1.131, loss:1331.1020
g_step 1400, step 400, avg_time 1.115, loss:1306.0986
g_step 1500, step 500, avg_time 1.098, loss:1266.4004
>> valid entity prec:0.5136, rec:0.4377, f1:0.4726
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 100, avg_time 1.124, loss:1272.2428
g_step 1700, step 200, avg_time 1.107, loss:1251.5374
g_step 1800, step 300, avg_time 1.109, loss:1260.6676
g_step 1900, step 400, avg_time 1.121, loss:1203.8200
g_step 2000, step 500, avg_time 1.122, loss:1236.8539
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.4909, rec:0.6126, f1:0.5450
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 100, avg_time 1.120, loss:1200.2519
g_step 2200, step 200, avg_time 1.099, loss:1214.4284
g_step 2300, step 300, avg_time 1.110, loss:1193.4726
g_step 2400, step 400, avg_time 1.120, loss:1196.1044
g_step 2500, step 500, avg_time 1.107, loss:1203.7760
>> valid entity prec:0.4534, rec:0.6724, f1:0.5416
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 100, avg_time 1.119, loss:1160.7425
g_step 2700, step 200, avg_time 1.129, loss:1145.8271
g_step 2800, step 300, avg_time 1.126, loss:1197.0803
g_step 2900, step 400, avg_time 1.092, loss:1196.9335
g_step 3000, step 500, avg_time 1.110, loss:1174.9718
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5729, rec:0.4249, f1:0.4879
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 100, avg_time 1.115, loss:1126.7576
g_step 3200, step 200, avg_time 1.105, loss:1132.3037
g_step 3300, step 300, avg_time 1.105, loss:1151.9763
g_step 3400, step 400, avg_time 1.102, loss:1163.9572
g_step 3500, step 500, avg_time 1.136, loss:1153.6315
>> valid entity prec:0.5250, rec:0.5176, f1:0.5213
>> valid relation prec:0.2500, rec:0.0002, f1:0.0005
>> valid relation with NER prec:0.2500, rec:0.0002, f1:0.0005
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 3600, step 100, avg_time 1.118, loss:1147.8726
g_step 3700, step 200, avg_time 1.123, loss:1140.2612
g_step 3800, step 300, avg_time 1.119, loss:1100.4568
g_step 3900, step 400, avg_time 1.118, loss:1115.9307
g_step 4000, step 500, avg_time 1.094, loss:1109.1608
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5257, rec:0.5472, f1:0.5363
>> valid relation prec:0.0377, rec:0.0005, f1:0.0009
>> valid relation with NER prec:0.0377, rec:0.0005, f1:0.0009
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4100, step 100, avg_time 1.107, loss:1073.8486
g_step 4200, step 200, avg_time 1.109, loss:1113.9632
g_step 4300, step 300, avg_time 1.115, loss:1133.3028
g_step 4400, step 400, avg_time 1.131, loss:1085.9022
g_step 4500, step 500, avg_time 1.110, loss:1096.0610
>> valid entity prec:0.5734, rec:0.4716, f1:0.5175
>> valid relation prec:0.1562, rec:0.0012, f1:0.0024
>> valid relation with NER prec:0.1562, rec:0.0012, f1:0.0024
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 4600, step 100, avg_time 1.126, loss:1081.7983
g_step 4700, step 200, avg_time 1.125, loss:1091.3075
g_step 4800, step 300, avg_time 1.116, loss:1061.2082
g_step 4900, step 400, avg_time 1.100, loss:1138.0983
g_step 5000, step 500, avg_time 1.102, loss:1052.6445
learning rate was adjusted to 0.0008
>> valid entity prec:0.5659, rec:0.4825, f1:0.5209
>> valid relation prec:0.2000, rec:0.0010, f1:0.0019
>> valid relation with NER prec:0.2000, rec:0.0010, f1:0.0019
g_step 5100, step 100, avg_time 1.131, loss:1094.3843
g_step 5200, step 200, avg_time 1.114, loss:1058.1739
g_step 5300, step 300, avg_time 1.133, loss:1085.8962
g_step 5400, step 400, avg_time 1.105, loss:1046.3630
g_step 5500, step 500, avg_time 1.096, loss:1058.1589
>> valid entity prec:0.5724, rec:0.3759, f1:0.4538
>> valid relation prec:0.4494, rec:0.0096, f1:0.0188
>> valid relation with NER prec:0.4494, rec:0.0096, f1:0.0188
new max relation f1 on valid!
new max relation f1 with NER on valid!
g_step 5600, step 100, avg_time 1.114, loss:1028.6610
g_step 5700, step 200, avg_time 1.118, loss:1057.6663
g_step 5800, step 300, avg_time 1.111, loss:1076.8412
g_step 5900, step 400, avg_time 1.115, loss:1064.7323
g_step 6000, step 500, avg_time 1.119, loss:1044.2450
learning rate was adjusted to 0.0007692307692307692
>> valid entity prec:0.5569, rec:0.5541, f1:0.5555
>> valid relation prec:0.1030, rec:0.0041, f1:0.0079
>> valid relation with NER prec:0.1030, rec:0.0041, f1:0.0079
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 6100, step 100, avg_time 1.137, loss:1014.8060
g_step 6200, step 200, avg_time 1.125, loss:1033.1587
g_step 6300, step 300, avg_time 1.088, loss:1042.8973
g_step 6400, step 400, avg_time 1.112, loss:1028.7943
g_step 6500, step 500, avg_time 1.116, loss:1044.4011
>> valid entity prec:0.5468, rec:0.4662, f1:0.5033
>> valid relation prec:0.2703, rec:0.0072, f1:0.0141
>> valid relation with NER prec:0.2703, rec:0.0072, f1:0.0141
g_step 6600, step 100, avg_time 1.110, loss:1011.0832
g_step 6700, step 200, avg_time 1.100, loss:1052.7429
g_step 6800, step 300, avg_time 1.118, loss:994.5568
g_step 6900, step 400, avg_time 1.124, loss:1007.3688
g_step 7000, step 500, avg_time 1.118, loss:1020.2999
learning rate was adjusted to 0.0007407407407407407
>> valid entity prec:0.5317, rec:0.5888, f1:0.5588
>> valid relation prec:0.2510, rec:0.0154, f1:0.0290
>> valid relation with NER prec:0.2510, rec:0.0154, f1:0.0290
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 7100, step 100, avg_time 1.108, loss:981.5425
g_step 7200, step 200, avg_time 1.109, loss:992.6818
g_step 7300, step 300, avg_time 1.121, loss:1018.4622
g_step 7400, step 400, avg_time 1.117, loss:1018.4642
g_step 7500, step 500, avg_time 1.111, loss:987.3817
>> valid entity prec:0.5721, rec:0.3990, f1:0.4701
>> valid relation prec:0.2532, rec:0.0144, f1:0.0273
>> valid relation with NER prec:0.2532, rec:0.0144, f1:0.0273
g_step 7600, step 100, avg_time 1.123, loss:980.3121
g_step 7700, step 200, avg_time 1.132, loss:969.4641
g_step 7800, step 300, avg_time 1.109, loss:974.1776
g_step 7900, step 400, avg_time 1.103, loss:968.9379
g_step 8000, step 500, avg_time 1.112, loss:1016.7246
learning rate was adjusted to 0.0007142857142857144
>> valid entity prec:0.5680, rec:0.4847, f1:0.5230
>> valid relation prec:0.1527, rec:0.0048, f1:0.0093
>> valid relation with NER prec:0.1527, rec:0.0048, f1:0.0093
g_step 8100, step 100, avg_time 1.108, loss:942.5945
g_step 8200, step 200, avg_time 1.117, loss:949.9378
g_step 8300, step 300, avg_time 1.109, loss:964.0775
g_step 8400, step 400, avg_time 1.128, loss:984.5924
g_step 8500, step 500, avg_time 1.108, loss:975.2771
>> valid entity prec:0.5536, rec:0.4919, f1:0.5209
>> valid relation prec:0.0988, rec:0.0019, f1:0.0038
>> valid relation with NER prec:0.0988, rec:0.0019, f1:0.0038
g_step 8600, step 100, avg_time 1.121, loss:943.1143
g_step 8700, step 200, avg_time 1.110, loss:948.3187
g_step 8800, step 300, avg_time 1.094, loss:951.0636
g_step 8900, step 400, avg_time 1.124, loss:945.1223
g_step 9000, step 500, avg_time 1.130, loss:963.0230
learning rate was adjusted to 0.0006896551724137932
>> valid entity prec:0.5380, rec:0.4823, f1:0.5086
>> valid relation prec:0.2745, rec:0.0067, f1:0.0131
>> valid relation with NER prec:0.2745, rec:0.0067, f1:0.0131
g_step 9100, step 100, avg_time 1.111, loss:924.1795
g_step 9200, step 200, avg_time 1.110, loss:945.7489
g_step 9300, step 300, avg_time 1.105, loss:919.4788
g_step 9400, step 400, avg_time 1.118, loss:931.0432
g_step 9500, step 500, avg_time 1.128, loss:931.5380
>> valid entity prec:0.5659, rec:0.4189, f1:0.4814
>> valid relation prec:0.3103, rec:0.0151, f1:0.0289
>> valid relation with NER prec:0.3103, rec:0.0151, f1:0.0289
g_step 9600, step 100, avg_time 1.107, loss:878.4632
g_step 9700, step 200, avg_time 1.110, loss:927.9343
g_step 9800, step 300, avg_time 1.128, loss:900.6345
g_step 9900, step 400, avg_time 1.123, loss:917.0054
g_step 10000, step 500, avg_time 1.097, loss:916.4919
learning rate was adjusted to 0.0006666666666666666
>> valid entity prec:0.5563, rec:0.4296, f1:0.4848
>> valid relation prec:0.2575, rec:0.0166, f1:0.0312
>> valid relation with NER prec:0.2575, rec:0.0166, f1:0.0312
new max relation f1 on valid!
new max relation f1 with NER on valid!
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'lowest point', 'manufacturer', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12876
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12976, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:05,  5.08s/it]Extractor Predicting: 2it [00:05,  2.44s/it]Extractor Predicting: 3it [00:06,  1.80s/it]Extractor Predicting: 4it [00:07,  1.48s/it]Extractor Predicting: 5it [00:08,  1.32s/it]Extractor Predicting: 6it [00:10,  1.38s/it]Extractor Predicting: 7it [00:10,  1.12s/it]Extractor Predicting: 8it [00:11,  1.04it/s]Extractor Predicting: 9it [00:11,  1.20it/s]Extractor Predicting: 10it [00:12,  1.32it/s]Extractor Predicting: 11it [00:13,  1.42it/s]Extractor Predicting: 12it [00:13,  1.49it/s]Extractor Predicting: 13it [00:14,  1.49it/s]Extractor Predicting: 14it [00:15,  1.50it/s]Extractor Predicting: 15it [00:15,  1.47it/s]Extractor Predicting: 16it [00:16,  1.50it/s]Extractor Predicting: 17it [00:17,  1.49it/s]Extractor Predicting: 18it [00:17,  1.47it/s]Extractor Predicting: 19it [00:18,  1.48it/s]Extractor Predicting: 20it [00:19,  1.52it/s]Extractor Predicting: 21it [00:19,  1.49it/s]Extractor Predicting: 22it [00:20,  1.48it/s]Extractor Predicting: 23it [00:21,  1.47it/s]Extractor Predicting: 24it [00:21,  1.45it/s]Extractor Predicting: 25it [00:22,  1.48it/s]Extractor Predicting: 26it [00:23,  1.50it/s]Extractor Predicting: 27it [00:23,  1.52it/s]Extractor Predicting: 28it [00:24,  1.54it/s]Extractor Predicting: 29it [00:25,  1.52it/s]Extractor Predicting: 30it [00:25,  1.53it/s]Extractor Predicting: 31it [00:26,  1.50it/s]Extractor Predicting: 32it [00:27,  1.52it/s]Extractor Predicting: 33it [00:27,  1.51it/s]Extractor Predicting: 34it [00:30,  1.18s/it]Extractor Predicting: 35it [00:30,  1.01s/it]Extractor Predicting: 36it [00:31,  1.12it/s]Extractor Predicting: 37it [00:32,  1.18it/s]Extractor Predicting: 38it [00:32,  1.25it/s]Extractor Predicting: 39it [00:33,  1.32it/s]Extractor Predicting: 40it [00:34,  1.37it/s]Extractor Predicting: 41it [00:34,  1.45it/s]Extractor Predicting: 42it [00:35,  1.48it/s]Extractor Predicting: 43it [00:36,  1.46it/s]Extractor Predicting: 44it [00:36,  1.45it/s]Extractor Predicting: 45it [00:37,  1.48it/s]Extractor Predicting: 46it [00:38,  1.51it/s]Extractor Predicting: 47it [00:38,  1.49it/s]Extractor Predicting: 48it [00:39,  1.51it/s]Extractor Predicting: 49it [00:40,  1.52it/s]Extractor Predicting: 50it [00:40,  1.55it/s]Extractor Predicting: 51it [00:41,  1.57it/s]Extractor Predicting: 52it [00:41,  1.55it/s]Extractor Predicting: 53it [00:42,  1.57it/s]Extractor Predicting: 54it [00:43,  1.53it/s]Extractor Predicting: 55it [00:43,  1.55it/s]Extractor Predicting: 56it [00:44,  1.52it/s]Extractor Predicting: 57it [00:45,  1.52it/s]Extractor Predicting: 58it [00:45,  1.51it/s]Extractor Predicting: 59it [00:46,  1.52it/s]Extractor Predicting: 60it [00:47,  1.51it/s]Extractor Predicting: 61it [00:47,  1.55it/s]Extractor Predicting: 62it [00:48,  1.54it/s]Extractor Predicting: 63it [00:49,  1.55it/s]Extractor Predicting: 64it [00:49,  1.55it/s]Extractor Predicting: 65it [00:50,  1.55it/s]Extractor Predicting: 66it [00:51,  1.53it/s]Extractor Predicting: 67it [00:51,  1.54it/s]Extractor Predicting: 68it [00:52,  1.51it/s]Extractor Predicting: 69it [00:53,  1.51it/s]Extractor Predicting: 70it [00:53,  1.54it/s]Extractor Predicting: 71it [00:54,  1.54it/s]Extractor Predicting: 72it [00:55,  1.51it/s]Extractor Predicting: 73it [00:55,  1.50it/s]Extractor Predicting: 74it [00:56,  1.48it/s]Extractor Predicting: 75it [00:57,  1.52it/s]Extractor Predicting: 76it [00:57,  1.45it/s]Extractor Predicting: 77it [00:58,  1.44it/s]Extractor Predicting: 78it [00:59,  1.44it/s]Extractor Predicting: 79it [00:59,  1.45it/s]Extractor Predicting: 80it [01:00,  1.34it/s]Extractor Predicting: 81it [01:01,  1.34it/s]Extractor Predicting: 82it [01:02,  1.39it/s]Extractor Predicting: 83it [01:02,  1.45it/s]Extractor Predicting: 84it [01:03,  1.46it/s]Extractor Predicting: 85it [01:04,  1.47it/s]Extractor Predicting: 86it [01:04,  1.49it/s]Extractor Predicting: 87it [01:05,  1.48it/s]Extractor Predicting: 88it [01:06,  1.51it/s]Extractor Predicting: 89it [01:06,  1.49it/s]Extractor Predicting: 90it [01:07,  1.48it/s]Extractor Predicting: 91it [01:08,  1.51it/s]Extractor Predicting: 92it [01:08,  1.52it/s]Extractor Predicting: 93it [01:09,  1.49it/s]Extractor Predicting: 94it [01:10,  1.46it/s]Extractor Predicting: 95it [01:10,  1.45it/s]Extractor Predicting: 96it [01:11,  1.48it/s]Extractor Predicting: 97it [01:12,  1.50it/s]Extractor Predicting: 98it [01:12,  1.51it/s]Extractor Predicting: 99it [01:13,  1.49it/s]Extractor Predicting: 100it [01:14,  1.51it/s]Extractor Predicting: 101it [01:14,  1.50it/s]Extractor Predicting: 102it [01:15,  1.48it/s]Extractor Predicting: 103it [01:16,  1.48it/s]Extractor Predicting: 104it [01:16,  1.49it/s]Extractor Predicting: 105it [01:17,  1.48it/s]Extractor Predicting: 106it [01:18,  1.50it/s]Extractor Predicting: 107it [01:18,  1.51it/s]Extractor Predicting: 108it [01:19,  1.48it/s]Extractor Predicting: 109it [01:20,  1.49it/s]Extractor Predicting: 110it [01:20,  1.49it/s]Extractor Predicting: 111it [01:21,  1.50it/s]Extractor Predicting: 112it [01:22,  1.48it/s]Extractor Predicting: 113it [01:22,  1.50it/s]Extractor Predicting: 114it [01:23,  1.49it/s]Extractor Predicting: 115it [01:24,  1.51it/s]Extractor Predicting: 116it [01:24,  1.52it/s]Extractor Predicting: 117it [01:25,  1.54it/s]Extractor Predicting: 118it [01:26,  1.51it/s]Extractor Predicting: 119it [01:26,  1.48it/s]Extractor Predicting: 120it [01:27,  1.52it/s]Extractor Predicting: 121it [01:28,  1.51it/s]Extractor Predicting: 122it [01:28,  1.49it/s]Extractor Predicting: 123it [01:29,  1.47it/s]Extractor Predicting: 124it [01:30,  1.46it/s]Extractor Predicting: 125it [01:30,  1.45it/s]Extractor Predicting: 126it [01:31,  1.46it/s]Extractor Predicting: 127it [01:32,  1.45it/s]Extractor Predicting: 128it [01:33,  1.44it/s]Extractor Predicting: 129it [01:33,  1.44it/s]Extractor Predicting: 130it [01:34,  1.46it/s]Extractor Predicting: 131it [01:35,  1.43it/s]Extractor Predicting: 132it [01:35,  1.45it/s]Extractor Predicting: 133it [01:36,  1.47it/s]Extractor Predicting: 134it [01:37,  1.48it/s]Extractor Predicting: 135it [01:37,  1.49it/s]Extractor Predicting: 136it [01:38,  1.49it/s]Extractor Predicting: 137it [01:39,  1.47it/s]Extractor Predicting: 138it [01:39,  1.46it/s]Extractor Predicting: 139it [01:40,  1.47it/s]Extractor Predicting: 140it [01:41,  1.48it/s]Extractor Predicting: 141it [01:41,  1.50it/s]Extractor Predicting: 142it [01:42,  1.47it/s]Extractor Predicting: 143it [01:43,  1.49it/s]Extractor Predicting: 144it [01:43,  1.47it/s]Extractor Predicting: 145it [01:44,  1.36it/s]Extractor Predicting: 146it [01:45,  1.37it/s]Extractor Predicting: 147it [01:46,  1.39it/s]Extractor Predicting: 148it [01:46,  1.41it/s]Extractor Predicting: 149it [01:47,  1.42it/s]Extractor Predicting: 150it [01:48,  1.44it/s]Extractor Predicting: 151it [01:48,  1.45it/s]Extractor Predicting: 152it [01:49,  1.41it/s]Extractor Predicting: 153it [01:50,  1.42it/s]Extractor Predicting: 154it [01:50,  1.55it/s]Extractor Predicting: 154it [01:50,  1.39it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4175824175824176,
  "recall": 0.018247298919567827,
  "score": 0.034966643662295835,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27443
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27543, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.63it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.57it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.54it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.56it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.48it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.51it/s]Extractor Predicting: 16it [00:10,  1.52it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.51it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.54it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.53it/s]Extractor Predicting: 23it [00:14,  1.52it/s]Extractor Predicting: 24it [00:15,  1.52it/s]Extractor Predicting: 25it [00:16,  1.52it/s]Extractor Predicting: 26it [00:16,  1.53it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.58it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.54it/s]Extractor Predicting: 31it [00:20,  1.55it/s]Extractor Predicting: 32it [00:20,  1.56it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:22,  1.55it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.53it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.56it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:27,  1.46it/s]Extractor Predicting: 43it [00:28,  1.47it/s]Extractor Predicting: 44it [00:28,  1.46it/s]Extractor Predicting: 45it [00:29,  1.48it/s]Extractor Predicting: 46it [00:30,  1.46it/s]Extractor Predicting: 47it [00:30,  1.44it/s]Extractor Predicting: 48it [00:31,  1.42it/s]Extractor Predicting: 49it [00:32,  1.45it/s]Extractor Predicting: 50it [00:32,  1.44it/s]Extractor Predicting: 51it [00:33,  1.45it/s]Extractor Predicting: 52it [00:34,  1.45it/s]Extractor Predicting: 53it [00:34,  1.45it/s]Extractor Predicting: 54it [00:35,  1.44it/s]Extractor Predicting: 55it [00:36,  1.40it/s]Extractor Predicting: 56it [00:37,  1.45it/s]Extractor Predicting: 57it [00:37,  1.48it/s]Extractor Predicting: 58it [00:38,  1.48it/s]Extractor Predicting: 59it [00:39,  1.46it/s]Extractor Predicting: 60it [00:39,  1.43it/s]Extractor Predicting: 61it [00:40,  1.43it/s]Extractor Predicting: 62it [00:41,  1.46it/s]Extractor Predicting: 63it [00:41,  1.46it/s]Extractor Predicting: 64it [00:42,  1.42it/s]Extractor Predicting: 65it [00:43,  1.43it/s]Extractor Predicting: 66it [00:43,  1.44it/s]Extractor Predicting: 67it [00:44,  1.42it/s]Extractor Predicting: 68it [00:45,  1.43it/s]Extractor Predicting: 69it [00:46,  1.42it/s]Extractor Predicting: 70it [00:46,  1.42it/s]Extractor Predicting: 71it [00:47,  1.43it/s]Extractor Predicting: 72it [00:48,  1.43it/s]Extractor Predicting: 73it [00:48,  1.41it/s]Extractor Predicting: 74it [00:49,  1.40it/s]Extractor Predicting: 75it [00:50,  1.41it/s]Extractor Predicting: 76it [00:51,  1.39it/s]Extractor Predicting: 77it [00:51,  1.44it/s]Extractor Predicting: 78it [00:52,  1.41it/s]Extractor Predicting: 79it [00:53,  1.42it/s]Extractor Predicting: 80it [00:53,  1.42it/s]Extractor Predicting: 81it [00:54,  1.38it/s]Extractor Predicting: 82it [00:55,  1.40it/s]Extractor Predicting: 83it [00:56,  1.33it/s]Extractor Predicting: 84it [00:56,  1.38it/s]Extractor Predicting: 85it [00:57,  1.37it/s]Extractor Predicting: 86it [00:58,  1.38it/s]Extractor Predicting: 87it [00:58,  1.43it/s]Extractor Predicting: 88it [00:59,  1.41it/s]Extractor Predicting: 89it [01:00,  1.40it/s]Extractor Predicting: 90it [01:01,  1.39it/s]Extractor Predicting: 91it [01:01,  1.41it/s]Extractor Predicting: 92it [01:02,  1.44it/s]Extractor Predicting: 93it [01:03,  1.47it/s]Extractor Predicting: 94it [01:03,  1.48it/s]Extractor Predicting: 95it [01:04,  1.49it/s]Extractor Predicting: 96it [01:05,  1.49it/s]Extractor Predicting: 97it [01:05,  1.41it/s]Extractor Predicting: 98it [01:06,  1.41it/s]Extractor Predicting: 99it [01:07,  1.44it/s]Extractor Predicting: 100it [01:07,  1.43it/s]Extractor Predicting: 101it [01:08,  1.46it/s]Extractor Predicting: 102it [01:09,  1.46it/s]Extractor Predicting: 103it [01:10,  1.47it/s]Extractor Predicting: 104it [01:10,  1.48it/s]Extractor Predicting: 105it [01:11,  1.46it/s]Extractor Predicting: 106it [01:12,  1.46it/s]Extractor Predicting: 107it [01:12,  1.48it/s]Extractor Predicting: 108it [01:13,  1.47it/s]Extractor Predicting: 109it [01:14,  1.45it/s]Extractor Predicting: 110it [01:16,  1.11s/it]Extractor Predicting: 111it [01:16,  1.03it/s]Extractor Predicting: 112it [01:17,  1.13it/s]Extractor Predicting: 113it [01:18,  1.23it/s]Extractor Predicting: 114it [01:18,  1.31it/s]Extractor Predicting: 115it [01:19,  1.32it/s]Extractor Predicting: 116it [01:20,  1.36it/s]Extractor Predicting: 117it [01:21,  1.27it/s]Extractor Predicting: 118it [01:23,  1.29s/it]Extractor Predicting: 119it [01:24,  1.11s/it]Extractor Predicting: 120it [01:25,  1.02it/s]Extractor Predicting: 121it [01:25,  1.11it/s]Extractor Predicting: 122it [01:26,  1.20it/s]Extractor Predicting: 123it [01:27,  1.27it/s]Extractor Predicting: 124it [01:27,  1.34it/s]Extractor Predicting: 125it [01:28,  1.38it/s]Extractor Predicting: 126it [01:29,  1.42it/s]Extractor Predicting: 127it [01:29,  1.44it/s]Extractor Predicting: 128it [01:30,  1.48it/s]Extractor Predicting: 129it [01:30,  1.51it/s]Extractor Predicting: 130it [01:31,  1.46it/s]Extractor Predicting: 131it [01:32,  1.51it/s]Extractor Predicting: 132it [01:32,  1.52it/s]Extractor Predicting: 133it [01:33,  1.53it/s]Extractor Predicting: 134it [01:34,  1.49it/s]Extractor Predicting: 135it [01:35,  1.47it/s]Extractor Predicting: 136it [01:35,  1.47it/s]Extractor Predicting: 137it [01:36,  1.46it/s]Extractor Predicting: 138it [01:37,  1.47it/s]Extractor Predicting: 139it [01:37,  1.47it/s]Extractor Predicting: 140it [01:38,  1.46it/s]Extractor Predicting: 141it [01:39,  1.51it/s]Extractor Predicting: 142it [01:39,  1.46it/s]Extractor Predicting: 143it [01:40,  1.41it/s]Extractor Predicting: 144it [01:41,  1.44it/s]Extractor Predicting: 145it [01:41,  1.46it/s]Extractor Predicting: 146it [01:42,  1.40it/s]Extractor Predicting: 147it [01:43,  1.41it/s]Extractor Predicting: 148it [01:44,  1.43it/s]Extractor Predicting: 149it [01:44,  1.53it/s]Extractor Predicting: 150it [01:45,  1.65it/s]Extractor Predicting: 151it [01:45,  1.64it/s]Extractor Predicting: 152it [01:46,  1.76it/s]Extractor Predicting: 153it [01:46,  1.84it/s]Extractor Predicting: 154it [01:47,  1.89it/s]Extractor Predicting: 155it [01:47,  1.86it/s]Extractor Predicting: 156it [01:48,  1.83it/s]Extractor Predicting: 157it [01:48,  1.82it/s]Extractor Predicting: 158it [01:49,  1.87it/s]Extractor Predicting: 159it [01:49,  1.93it/s]Extractor Predicting: 160it [01:50,  1.90it/s]Extractor Predicting: 161it [01:50,  1.87it/s]Extractor Predicting: 162it [01:51,  1.84it/s]Extractor Predicting: 163it [01:52,  1.81it/s]Extractor Predicting: 164it [01:52,  1.81it/s]Extractor Predicting: 165it [01:53,  1.81it/s]Extractor Predicting: 166it [01:53,  1.86it/s]Extractor Predicting: 167it [01:54,  1.80it/s]Extractor Predicting: 168it [01:54,  1.71it/s]Extractor Predicting: 169it [01:55,  1.65it/s]Extractor Predicting: 170it [01:56,  1.55it/s]Extractor Predicting: 171it [01:56,  1.55it/s]Extractor Predicting: 172it [01:57,  1.50it/s]Extractor Predicting: 173it [01:58,  1.46it/s]Extractor Predicting: 174it [01:59,  1.46it/s]Extractor Predicting: 175it [01:59,  1.50it/s]Extractor Predicting: 176it [02:00,  1.50it/s]Extractor Predicting: 177it [02:01,  1.51it/s]Extractor Predicting: 178it [02:01,  1.50it/s]Extractor Predicting: 179it [02:02,  1.54it/s]Extractor Predicting: 180it [02:02,  1.55it/s]Extractor Predicting: 181it [02:03,  1.56it/s]Extractor Predicting: 182it [02:04,  1.54it/s]Extractor Predicting: 183it [02:04,  1.50it/s]Extractor Predicting: 184it [02:05,  1.48it/s]Extractor Predicting: 185it [02:06,  1.51it/s]Extractor Predicting: 186it [02:06,  1.49it/s]Extractor Predicting: 187it [02:07,  1.48it/s]Extractor Predicting: 188it [02:08,  1.53it/s]Extractor Predicting: 189it [02:08,  1.51it/s]Extractor Predicting: 190it [02:09,  1.52it/s]Extractor Predicting: 191it [02:10,  1.53it/s]Extractor Predicting: 192it [02:10,  1.54it/s]Extractor Predicting: 193it [02:11,  1.52it/s]Extractor Predicting: 194it [02:12,  1.57it/s]Extractor Predicting: 195it [02:12,  1.54it/s]Extractor Predicting: 196it [02:13,  1.53it/s]Extractor Predicting: 197it [02:14,  1.54it/s]Extractor Predicting: 198it [02:14,  1.53it/s]Extractor Predicting: 199it [02:15,  1.52it/s]Extractor Predicting: 200it [02:16,  1.49it/s]Extractor Predicting: 201it [02:16,  1.49it/s]Extractor Predicting: 202it [02:17,  1.46it/s]Extractor Predicting: 203it [02:18,  1.42it/s]Extractor Predicting: 204it [02:18,  1.43it/s]Extractor Predicting: 205it [02:19,  1.44it/s]Extractor Predicting: 206it [02:20,  1.45it/s]Extractor Predicting: 207it [02:21,  1.46it/s]Extractor Predicting: 208it [02:21,  1.46it/s]Extractor Predicting: 209it [02:22,  1.48it/s]Extractor Predicting: 210it [02:23,  1.48it/s]Extractor Predicting: 211it [02:23,  1.49it/s]Extractor Predicting: 212it [02:24,  1.52it/s]Extractor Predicting: 213it [02:25,  1.47it/s]Extractor Predicting: 214it [02:25,  1.49it/s]Extractor Predicting: 215it [02:26,  1.50it/s]Extractor Predicting: 216it [02:27,  1.50it/s]Extractor Predicting: 217it [02:27,  1.50it/s]Extractor Predicting: 218it [02:28,  1.52it/s]Extractor Predicting: 219it [02:29,  1.43it/s]Extractor Predicting: 220it [02:29,  1.48it/s]Extractor Predicting: 221it [02:30,  1.46it/s]Extractor Predicting: 222it [02:31,  1.49it/s]Extractor Predicting: 223it [02:31,  1.53it/s]Extractor Predicting: 224it [02:32,  1.52it/s]Extractor Predicting: 225it [02:32,  1.55it/s]Extractor Predicting: 226it [02:33,  1.57it/s]Extractor Predicting: 227it [02:34,  1.59it/s]Extractor Predicting: 228it [02:34,  1.54it/s]Extractor Predicting: 229it [02:35,  1.54it/s]Extractor Predicting: 230it [02:36,  1.52it/s]Extractor Predicting: 231it [02:36,  1.51it/s]Extractor Predicting: 232it [02:37,  1.55it/s]Extractor Predicting: 233it [02:38,  1.54it/s]Extractor Predicting: 234it [02:38,  1.50it/s]Extractor Predicting: 235it [02:39,  1.52it/s]Extractor Predicting: 236it [02:40,  1.51it/s]Extractor Predicting: 237it [02:40,  1.59it/s]Extractor Predicting: 238it [02:41,  1.55it/s]Extractor Predicting: 239it [02:42,  1.55it/s]Extractor Predicting: 240it [02:42,  1.51it/s]Extractor Predicting: 241it [02:43,  1.50it/s]Extractor Predicting: 242it [02:44,  1.49it/s]Extractor Predicting: 243it [02:44,  1.53it/s]Extractor Predicting: 244it [02:45,  1.52it/s]Extractor Predicting: 245it [02:46,  1.49it/s]Extractor Predicting: 246it [02:47,  1.31it/s]Extractor Predicting: 247it [02:47,  1.34it/s]Extractor Predicting: 248it [02:48,  1.35it/s]Extractor Predicting: 249it [02:49,  1.38it/s]Extractor Predicting: 250it [02:49,  1.42it/s]Extractor Predicting: 251it [02:50,  1.46it/s]Extractor Predicting: 252it [02:51,  1.48it/s]Extractor Predicting: 253it [02:51,  1.46it/s]Extractor Predicting: 254it [02:52,  1.48it/s]Extractor Predicting: 255it [02:53,  1.48it/s]Extractor Predicting: 256it [02:53,  1.48it/s]Extractor Predicting: 257it [02:54,  1.48it/s]Extractor Predicting: 258it [02:55,  1.46it/s]Extractor Predicting: 259it [02:55,  1.48it/s]Extractor Predicting: 260it [02:56,  1.49it/s]Extractor Predicting: 261it [02:57,  1.49it/s]Extractor Predicting: 262it [02:57,  1.50it/s]Extractor Predicting: 263it [02:58,  1.49it/s]Extractor Predicting: 264it [02:59,  1.50it/s]Extractor Predicting: 265it [02:59,  1.49it/s]Extractor Predicting: 266it [03:00,  1.51it/s]Extractor Predicting: 267it [03:01,  1.51it/s]Extractor Predicting: 268it [03:01,  1.49it/s]Extractor Predicting: 269it [03:02,  1.48it/s]Extractor Predicting: 270it [03:03,  1.49it/s]Extractor Predicting: 271it [03:03,  1.49it/s]Extractor Predicting: 272it [03:04,  1.47it/s]Extractor Predicting: 273it [03:05,  1.44it/s]Extractor Predicting: 274it [03:05,  1.49it/s]Extractor Predicting: 275it [03:06,  1.50it/s]Extractor Predicting: 276it [03:07,  1.51it/s]Extractor Predicting: 277it [03:07,  1.48it/s]Extractor Predicting: 278it [03:08,  1.49it/s]Extractor Predicting: 279it [03:09,  1.48it/s]Extractor Predicting: 280it [03:10,  1.49it/s]Extractor Predicting: 281it [03:10,  1.43it/s]Extractor Predicting: 282it [03:11,  1.45it/s]Extractor Predicting: 283it [03:12,  1.43it/s]Extractor Predicting: 284it [03:12,  1.44it/s]Extractor Predicting: 285it [03:13,  1.41it/s]Extractor Predicting: 286it [03:14,  1.37it/s]Extractor Predicting: 287it [03:15,  1.39it/s]Extractor Predicting: 288it [03:15,  1.42it/s]Extractor Predicting: 289it [03:16,  1.41it/s]Extractor Predicting: 290it [03:17,  1.43it/s]Extractor Predicting: 291it [03:17,  1.43it/s]Extractor Predicting: 292it [03:18,  1.45it/s]Extractor Predicting: 293it [03:19,  1.45it/s]Extractor Predicting: 294it [03:19,  1.46it/s]Extractor Predicting: 295it [03:20,  1.48it/s]Extractor Predicting: 296it [03:21,  1.49it/s]Extractor Predicting: 297it [03:21,  1.51it/s]Extractor Predicting: 298it [03:22,  1.57it/s]Extractor Predicting: 299it [03:23,  1.55it/s]Extractor Predicting: 300it [03:23,  1.59it/s]Extractor Predicting: 301it [03:24,  1.57it/s]Extractor Predicting: 302it [03:24,  1.52it/s]Extractor Predicting: 303it [03:25,  1.49it/s]Extractor Predicting: 304it [03:26,  1.51it/s]Extractor Predicting: 305it [03:27,  1.51it/s]Extractor Predicting: 306it [03:27,  1.51it/s]Extractor Predicting: 307it [03:28,  1.51it/s]Extractor Predicting: 308it [03:28,  1.50it/s]Extractor Predicting: 309it [03:29,  1.49it/s]Extractor Predicting: 310it [03:30,  1.49it/s]Extractor Predicting: 311it [03:31,  1.48it/s]Extractor Predicting: 312it [03:31,  1.49it/s]Extractor Predicting: 313it [03:32,  1.47it/s]Extractor Predicting: 314it [03:33,  1.47it/s]Extractor Predicting: 315it [03:33,  1.51it/s]Extractor Predicting: 316it [03:34,  1.49it/s]Extractor Predicting: 317it [03:35,  1.47it/s]Extractor Predicting: 318it [03:35,  1.49it/s]Extractor Predicting: 319it [03:36,  1.45it/s]Extractor Predicting: 320it [03:37,  1.47it/s]Extractor Predicting: 321it [03:37,  1.47it/s]Extractor Predicting: 322it [03:38,  1.49it/s]Extractor Predicting: 323it [03:39,  1.49it/s]Extractor Predicting: 324it [03:39,  1.50it/s]Extractor Predicting: 325it [03:40,  1.53it/s]Extractor Predicting: 326it [03:41,  1.52it/s]Extractor Predicting: 327it [03:41,  1.54it/s]Extractor Predicting: 328it [03:42,  1.51it/s]Extractor Predicting: 329it [03:43,  1.52it/s]Extractor Predicting: 330it [03:43,  1.53it/s]Extractor Predicting: 331it [03:44,  1.51it/s]Extractor Predicting: 332it [03:45,  1.49it/s]Extractor Predicting: 333it [03:45,  1.47it/s]Extractor Predicting: 334it [03:46,  1.43it/s]Extractor Predicting: 335it [03:47,  1.45it/s]Extractor Predicting: 336it [03:47,  1.48it/s]Extractor Predicting: 337it [03:48,  1.46it/s]Extractor Predicting: 338it [03:49,  1.46it/s]Extractor Predicting: 339it [03:49,  1.47it/s]Extractor Predicting: 340it [03:50,  1.53it/s]Extractor Predicting: 341it [03:51,  1.50it/s]Extractor Predicting: 342it [03:51,  1.52it/s]Extractor Predicting: 343it [03:52,  1.54it/s]Extractor Predicting: 344it [03:53,  1.58it/s]Extractor Predicting: 345it [03:53,  1.59it/s]Extractor Predicting: 346it [03:54,  1.63it/s]Extractor Predicting: 347it [03:55,  1.50it/s]Extractor Predicting: 348it [03:55,  1.53it/s]Extractor Predicting: 349it [03:56,  1.56it/s]Extractor Predicting: 350it [03:56,  1.54it/s]Extractor Predicting: 351it [03:57,  1.58it/s]Extractor Predicting: 352it [03:58,  1.56it/s]Extractor Predicting: 353it [03:59,  1.37it/s]Extractor Predicting: 354it [03:59,  1.37it/s]Extractor Predicting: 355it [04:00,  1.43it/s]Extractor Predicting: 356it [04:01,  1.44it/s]Extractor Predicting: 357it [04:01,  1.45it/s]Extractor Predicting: 358it [04:02,  1.45it/s]Extractor Predicting: 359it [04:03,  1.45it/s]Extractor Predicting: 360it [04:03,  1.46it/s]Extractor Predicting: 361it [04:04,  1.46it/s]Extractor Predicting: 362it [04:05,  1.47it/s]Extractor Predicting: 363it [04:05,  1.49it/s]Extractor Predicting: 364it [04:06,  1.50it/s]Extractor Predicting: 365it [04:07,  1.48it/s]Extractor Predicting: 366it [04:07,  1.46it/s]Extractor Predicting: 367it [04:08,  1.47it/s]Extractor Predicting: 368it [04:09,  1.47it/s]Extractor Predicting: 369it [04:10,  1.46it/s]Extractor Predicting: 370it [04:10,  1.48it/s]Extractor Predicting: 371it [04:11,  1.47it/s]Extractor Predicting: 372it [04:12,  1.45it/s]Extractor Predicting: 373it [04:12,  1.45it/s]Extractor Predicting: 374it [04:13,  1.48it/s]Extractor Predicting: 375it [04:14,  1.50it/s]Extractor Predicting: 376it [04:14,  1.53it/s]Extractor Predicting: 377it [04:15,  1.59it/s]Extractor Predicting: 378it [04:15,  1.57it/s]Extractor Predicting: 379it [04:16,  1.58it/s]Extractor Predicting: 380it [04:17,  1.58it/s]Extractor Predicting: 381it [04:17,  1.59it/s]Extractor Predicting: 382it [04:18,  1.58it/s]Extractor Predicting: 383it [04:19,  1.57it/s]Extractor Predicting: 384it [04:19,  1.56it/s]Extractor Predicting: 385it [04:20,  1.58it/s]Extractor Predicting: 386it [04:20,  1.60it/s]Extractor Predicting: 387it [04:21,  1.61it/s]Extractor Predicting: 388it [04:22,  1.60it/s]Extractor Predicting: 389it [04:22,  1.61it/s]Extractor Predicting: 390it [04:23,  1.61it/s]Extractor Predicting: 391it [04:24,  1.60it/s]Extractor Predicting: 392it [04:24,  1.64it/s]Extractor Predicting: 393it [04:25,  1.62it/s]Extractor Predicting: 394it [04:25,  1.63it/s]Extractor Predicting: 395it [04:26,  1.60it/s]Extractor Predicting: 396it [04:27,  1.59it/s]Extractor Predicting: 397it [04:27,  1.56it/s]Extractor Predicting: 398it [04:28,  1.53it/s]Extractor Predicting: 399it [04:29,  1.51it/s]Extractor Predicting: 400it [04:29,  1.50it/s]Extractor Predicting: 401it [04:30,  1.50it/s]Extractor Predicting: 402it [04:31,  1.51it/s]Extractor Predicting: 403it [04:31,  1.54it/s]Extractor Predicting: 404it [04:32,  1.49it/s]Extractor Predicting: 405it [04:33,  1.49it/s]Extractor Predicting: 406it [04:33,  1.49it/s]Extractor Predicting: 407it [04:34,  1.49it/s]Extractor Predicting: 408it [04:35,  1.49it/s]Extractor Predicting: 409it [04:35,  1.46it/s]Extractor Predicting: 410it [04:36,  1.46it/s]Extractor Predicting: 411it [04:37,  1.48it/s]Extractor Predicting: 412it [04:37,  1.51it/s]Extractor Predicting: 413it [04:38,  1.49it/s]Extractor Predicting: 414it [04:39,  1.48it/s]Extractor Predicting: 415it [04:39,  1.48it/s]Extractor Predicting: 416it [04:40,  1.48it/s]Extractor Predicting: 417it [04:41,  1.49it/s]Extractor Predicting: 418it [04:41,  1.52it/s]Extractor Predicting: 419it [04:42,  1.51it/s]Extractor Predicting: 420it [04:43,  1.51it/s]Extractor Predicting: 421it [04:43,  1.51it/s]Extractor Predicting: 422it [04:44,  1.49it/s]Extractor Predicting: 423it [04:45,  1.47it/s]Extractor Predicting: 424it [04:45,  1.50it/s]Extractor Predicting: 425it [04:46,  1.48it/s]Extractor Predicting: 426it [04:47,  1.53it/s]Extractor Predicting: 427it [04:47,  1.58it/s]Extractor Predicting: 428it [04:48,  1.58it/s]Extractor Predicting: 429it [04:49,  1.59it/s]Extractor Predicting: 430it [04:49,  1.60it/s]Extractor Predicting: 431it [04:50,  1.64it/s]Extractor Predicting: 432it [04:50,  1.60it/s]Extractor Predicting: 433it [04:51,  1.58it/s]Extractor Predicting: 434it [04:52,  1.60it/s]Extractor Predicting: 435it [04:52,  1.58it/s]Extractor Predicting: 436it [04:53,  1.57it/s]Extractor Predicting: 437it [04:54,  1.60it/s]Extractor Predicting: 438it [04:54,  1.61it/s]Extractor Predicting: 439it [04:55,  1.61it/s]Extractor Predicting: 440it [04:55,  1.57it/s]Extractor Predicting: 441it [04:56,  1.57it/s]Extractor Predicting: 442it [04:57,  1.60it/s]Extractor Predicting: 443it [04:57,  1.56it/s]Extractor Predicting: 444it [04:58,  1.58it/s]Extractor Predicting: 445it [04:59,  1.59it/s]Extractor Predicting: 446it [04:59,  1.61it/s]Extractor Predicting: 447it [05:00,  1.62it/s]Extractor Predicting: 448it [05:00,  1.62it/s]Extractor Predicting: 449it [05:01,  1.63it/s]Extractor Predicting: 450it [05:02,  1.61it/s]Extractor Predicting: 451it [05:02,  1.61it/s]Extractor Predicting: 452it [05:03,  1.57it/s]Extractor Predicting: 453it [05:04,  1.56it/s]Extractor Predicting: 454it [05:04,  1.51it/s]Extractor Predicting: 455it [05:05,  1.54it/s]Extractor Predicting: 456it [05:06,  1.58it/s]Extractor Predicting: 457it [05:06,  1.59it/s]Extractor Predicting: 458it [05:07,  1.60it/s]Extractor Predicting: 459it [05:07,  1.58it/s]Extractor Predicting: 460it [05:08,  1.52it/s]Extractor Predicting: 461it [05:09,  1.50it/s]Extractor Predicting: 462it [05:10,  1.46it/s]Extractor Predicting: 463it [05:10,  1.46it/s]Extractor Predicting: 464it [05:11,  1.49it/s]Extractor Predicting: 465it [05:12,  1.49it/s]Extractor Predicting: 466it [05:12,  1.49it/s]Extractor Predicting: 467it [05:13,  1.46it/s]Extractor Predicting: 468it [05:14,  1.47it/s]Extractor Predicting: 469it [05:14,  1.49it/s]Extractor Predicting: 470it [05:15,  1.46it/s]Extractor Predicting: 471it [05:16,  1.43it/s]Extractor Predicting: 472it [05:16,  1.43it/s]Extractor Predicting: 473it [05:17,  1.46it/s]Extractor Predicting: 474it [05:18,  1.50it/s]Extractor Predicting: 475it [05:18,  1.50it/s]Extractor Predicting: 476it [05:19,  1.49it/s]Extractor Predicting: 477it [05:20,  1.45it/s]Extractor Predicting: 478it [05:20,  1.43it/s]Extractor Predicting: 479it [05:21,  1.42it/s]Extractor Predicting: 480it [05:22,  1.42it/s]Extractor Predicting: 481it [05:23,  1.45it/s]Extractor Predicting: 482it [05:23,  1.43it/s]Extractor Predicting: 483it [05:24,  1.46it/s]Extractor Predicting: 484it [05:25,  1.45it/s]Extractor Predicting: 485it [05:26,  1.32it/s]Extractor Predicting: 486it [05:26,  1.39it/s]Extractor Predicting: 487it [05:27,  1.41it/s]Extractor Predicting: 488it [05:28,  1.41it/s]Extractor Predicting: 489it [05:28,  1.42it/s]Extractor Predicting: 490it [05:29,  1.43it/s]Extractor Predicting: 491it [05:30,  1.43it/s]Extractor Predicting: 492it [05:30,  1.42it/s]Extractor Predicting: 493it [05:31,  1.44it/s]Extractor Predicting: 494it [05:32,  1.45it/s]Extractor Predicting: 495it [05:32,  1.50it/s]Extractor Predicting: 496it [05:33,  1.50it/s]Extractor Predicting: 496it [05:33,  1.49it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.3867973182052604,
  "recall": 0.06300403225806452,
  "score": 0.10835801488116738,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 10529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 2it [00:01,  1.44it/s]Extractor Predicting: 3it [00:02,  1.47it/s]Extractor Predicting: 4it [00:02,  1.47it/s]Extractor Predicting: 5it [00:03,  1.44it/s]Extractor Predicting: 6it [00:04,  1.42it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.42it/s]Extractor Predicting: 9it [00:06,  1.43it/s]Extractor Predicting: 10it [00:06,  1.42it/s]Extractor Predicting: 11it [00:07,  1.42it/s]Extractor Predicting: 12it [00:08,  1.42it/s]Extractor Predicting: 13it [00:09,  1.43it/s]Extractor Predicting: 14it [00:09,  1.44it/s]Extractor Predicting: 15it [00:10,  1.41it/s]Extractor Predicting: 16it [00:11,  1.42it/s]Extractor Predicting: 17it [00:11,  1.39it/s]Extractor Predicting: 18it [00:12,  1.38it/s]Extractor Predicting: 19it [00:13,  1.34it/s]Extractor Predicting: 20it [00:14,  1.29it/s]Extractor Predicting: 21it [00:15,  1.31it/s]Extractor Predicting: 22it [00:15,  1.28it/s]Extractor Predicting: 23it [00:16,  1.28it/s]Extractor Predicting: 24it [00:17,  1.32it/s]Extractor Predicting: 25it [00:18,  1.37it/s]Extractor Predicting: 26it [00:18,  1.38it/s]Extractor Predicting: 27it [00:19,  1.37it/s]Extractor Predicting: 28it [00:20,  1.37it/s]Extractor Predicting: 29it [00:20,  1.36it/s]Extractor Predicting: 30it [00:21,  1.37it/s]Extractor Predicting: 31it [00:22,  1.37it/s]Extractor Predicting: 32it [00:23,  1.38it/s]Extractor Predicting: 33it [00:23,  1.42it/s]Extractor Predicting: 34it [00:24,  1.45it/s]Extractor Predicting: 35it [00:25,  1.42it/s]Extractor Predicting: 36it [00:25,  1.42it/s]Extractor Predicting: 37it [00:26,  1.43it/s]Extractor Predicting: 38it [00:27,  1.43it/s]Extractor Predicting: 39it [00:27,  1.45it/s]Extractor Predicting: 40it [00:28,  1.47it/s]Extractor Predicting: 41it [00:29,  1.45it/s]Extractor Predicting: 42it [00:29,  1.47it/s]Extractor Predicting: 43it [00:30,  1.46it/s]Extractor Predicting: 44it [00:31,  1.47it/s]Extractor Predicting: 45it [00:31,  1.48it/s]Extractor Predicting: 46it [00:32,  1.43it/s]Extractor Predicting: 47it [00:33,  1.43it/s]Extractor Predicting: 48it [00:34,  1.45it/s]Extractor Predicting: 49it [00:34,  1.47it/s]Extractor Predicting: 50it [00:35,  1.46it/s]Extractor Predicting: 51it [00:36,  1.46it/s]Extractor Predicting: 52it [00:36,  1.45it/s]Extractor Predicting: 53it [00:37,  1.46it/s]Extractor Predicting: 54it [00:38,  1.44it/s]Extractor Predicting: 55it [00:38,  1.46it/s]Extractor Predicting: 56it [00:39,  1.48it/s]Extractor Predicting: 57it [00:40,  1.50it/s]Extractor Predicting: 58it [00:40,  1.50it/s]Extractor Predicting: 59it [00:41,  1.49it/s]Extractor Predicting: 60it [00:42,  1.42it/s]Extractor Predicting: 61it [00:42,  1.45it/s]Extractor Predicting: 62it [00:43,  1.47it/s]Extractor Predicting: 63it [00:44,  1.43it/s]Extractor Predicting: 64it [00:45,  1.43it/s]Extractor Predicting: 65it [00:45,  1.40it/s]Extractor Predicting: 66it [00:46,  1.47it/s]Extractor Predicting: 67it [00:47,  1.54it/s]Extractor Predicting: 68it [00:47,  1.60it/s]Extractor Predicting: 69it [00:48,  1.68it/s]Extractor Predicting: 70it [00:48,  1.57it/s]Extractor Predicting: 71it [00:49,  1.65it/s]Extractor Predicting: 72it [00:49,  1.68it/s]Extractor Predicting: 73it [00:50,  1.72it/s]Extractor Predicting: 74it [00:51,  1.73it/s]Extractor Predicting: 75it [00:51,  1.73it/s]Extractor Predicting: 76it [00:52,  1.74it/s]Extractor Predicting: 77it [00:52,  1.74it/s]Extractor Predicting: 78it [00:53,  1.76it/s]Extractor Predicting: 79it [00:53,  1.80it/s]Extractor Predicting: 80it [00:54,  1.76it/s]Extractor Predicting: 81it [00:55,  1.76it/s]Extractor Predicting: 82it [00:55,  1.78it/s]Extractor Predicting: 83it [00:56,  1.82it/s]Extractor Predicting: 84it [00:56,  1.83it/s]Extractor Predicting: 85it [00:57,  1.83it/s]Extractor Predicting: 86it [00:57,  1.81it/s]Extractor Predicting: 87it [00:58,  1.86it/s]Extractor Predicting: 88it [00:58,  1.80it/s]Extractor Predicting: 89it [00:59,  1.78it/s]Extractor Predicting: 90it [00:59,  1.77it/s]Extractor Predicting: 91it [01:00,  1.77it/s]Extractor Predicting: 92it [01:01,  1.79it/s]Extractor Predicting: 93it [01:01,  1.80it/s]Extractor Predicting: 94it [01:02,  1.80it/s]Extractor Predicting: 95it [01:02,  1.80it/s]Extractor Predicting: 96it [01:03,  1.68it/s]Extractor Predicting: 97it [01:04,  1.61it/s]Extractor Predicting: 98it [01:04,  1.55it/s]Extractor Predicting: 99it [01:05,  1.49it/s]Extractor Predicting: 100it [01:06,  1.44it/s]Extractor Predicting: 101it [01:07,  1.43it/s]Extractor Predicting: 102it [01:07,  1.42it/s]Extractor Predicting: 103it [01:08,  1.42it/s]Extractor Predicting: 104it [01:09,  1.42it/s]Extractor Predicting: 105it [01:09,  1.41it/s]Extractor Predicting: 106it [01:10,  1.41it/s]Extractor Predicting: 107it [01:11,  1.40it/s]Extractor Predicting: 108it [01:12,  1.39it/s]Extractor Predicting: 109it [01:12,  1.40it/s]Extractor Predicting: 110it [01:13,  1.39it/s]Extractor Predicting: 111it [01:14,  1.39it/s]Extractor Predicting: 112it [01:14,  1.42it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:16,  1.50it/s]Extractor Predicting: 115it [01:16,  1.51it/s]Extractor Predicting: 116it [01:17,  1.54it/s]Extractor Predicting: 117it [01:18,  1.54it/s]Extractor Predicting: 118it [01:18,  1.54it/s]Extractor Predicting: 119it [01:19,  1.52it/s]Extractor Predicting: 120it [01:19,  1.57it/s]Extractor Predicting: 121it [01:20,  1.57it/s]Extractor Predicting: 122it [01:21,  1.59it/s]Extractor Predicting: 123it [01:21,  1.57it/s]Extractor Predicting: 124it [01:22,  1.53it/s]Extractor Predicting: 125it [01:23,  1.53it/s]Extractor Predicting: 126it [01:23,  1.54it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:25,  1.48it/s]Extractor Predicting: 129it [01:25,  1.48it/s]Extractor Predicting: 130it [01:26,  1.47it/s]Extractor Predicting: 131it [01:27,  1.45it/s]Extractor Predicting: 132it [01:28,  1.41it/s]Extractor Predicting: 133it [01:28,  1.48it/s]Extractor Predicting: 133it [01:28,  1.50it/s]
{
  "path_pred": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6527514231499051,
  "recall": 0.09118621603711068,
  "score": 0.16001860681474592,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_synthetic_large/unseen_15_seed_0/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'data_name': 'fewrel', 'split': 'unseen_15_seed_0', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/model', data_dir='outputs/wrapper/fewrel/unseen_15_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/fewrel/unseen_15_seed_0/generator/synthetic.jsonl', 'path_train': 'zero_rte/fewrel/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000, 'num_pseudo_per_label': 50, 'num_train_per_label': 66}
num of filtered data: 4788 mean pseudo reward: 1.0
fit {'path_train': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/filtered.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl'}
train vocab size: 23073
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23173, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/model', pretrained_wv='outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23173, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.417, loss:53570.0767
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.070, loss:2542.7277
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 100, avg_time 1.054, loss:2273.2123
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 200, avg_time 1.065, loss:2213.1142
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 100, avg_time 1.054, loss:2106.5878
>> valid entity prec:0.4440, rec:0.4652, f1:0.4544
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 200, avg_time 2.519, loss:2102.7258
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 100, avg_time 1.067, loss:1949.3722
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 200, avg_time 1.061, loss:1834.5528
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 100, avg_time 1.060, loss:1760.2768
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 200, avg_time 1.057, loss:1641.7030
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5460, rec:0.6929, f1:0.6108
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 100, avg_time 1.055, loss:1606.4669
g_step 1200, step 200, avg_time 1.061, loss:1485.8283
g_step 1300, step 100, avg_time 1.069, loss:1464.8399
g_step 1400, step 200, avg_time 1.050, loss:1388.2966
g_step 1500, step 100, avg_time 1.056, loss:1370.9232
>> valid entity prec:0.5282, rec:0.7379, f1:0.6157
>> valid relation prec:0.0060, rec:0.0006, f1:0.0010
>> valid relation with NER prec:0.0060, rec:0.0006, f1:0.0010
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 200, avg_time 2.475, loss:1323.5814
g_step 1700, step 100, avg_time 1.066, loss:1293.9914
g_step 1800, step 200, avg_time 1.049, loss:1267.9836
g_step 1900, step 100, avg_time 1.068, loss:1244.9753
g_step 2000, step 200, avg_time 1.045, loss:1212.9743
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5859, rec:0.6460, f1:0.6145
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 100, avg_time 1.063, loss:1183.3210
g_step 2200, step 200, avg_time 1.059, loss:1163.5993
g_step 2300, step 100, avg_time 1.065, loss:1129.7896
g_step 2400, step 200, avg_time 1.049, loss:1135.6376
g_step 2500, step 100, avg_time 1.058, loss:1086.4640
>> valid entity prec:0.6442, rec:0.5647, f1:0.6018
>> valid relation prec:0.0022, rec:0.0003, f1:0.0005
>> valid relation with NER prec:0.0022, rec:0.0003, f1:0.0005
g_step 2600, step 200, avg_time 2.450, loss:1098.7884
g_step 2700, step 100, avg_time 1.067, loss:1064.7093
g_step 2800, step 200, avg_time 1.045, loss:1040.3801
g_step 2900, step 100, avg_time 1.056, loss:995.6478
g_step 3000, step 200, avg_time 1.052, loss:1045.1824
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6825, rec:0.5171, f1:0.5884
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 100, avg_time 1.058, loss:974.1493
g_step 3200, step 200, avg_time 1.055, loss:968.7432
g_step 3300, step 100, avg_time 1.054, loss:917.0363
g_step 3400, step 200, avg_time 1.061, loss:956.7538
g_step 3500, step 100, avg_time 1.045, loss:880.7931
>> valid entity prec:0.6359, rec:0.6042, f1:0.6196
>> valid relation prec:0.0358, rec:0.0077, f1:0.0127
>> valid relation with NER prec:0.0358, rec:0.0077, f1:0.0127
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 3600, step 200, avg_time 2.458, loss:933.7121
g_step 3700, step 100, avg_time 1.059, loss:863.6154
g_step 3800, step 200, avg_time 1.050, loss:882.6720
g_step 3900, step 100, avg_time 1.068, loss:835.2826
g_step 4000, step 200, avg_time 1.049, loss:854.5664
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5880, rec:0.6357, f1:0.6109
>> valid relation prec:0.0241, rec:0.0100, f1:0.0141
>> valid relation with NER prec:0.0241, rec:0.0100, f1:0.0141
new max relation f1 on valid!
new max relation f1 with NER on valid!
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/dev.jsonl', 'labels': ['composer', 'military branch', 'place served by transport hub', 'screenwriter', 'voice type'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12366
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12466, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:07,  7.01s/it]Extractor Predicting: 2it [00:07,  3.22s/it]Extractor Predicting: 3it [00:08,  2.04s/it]Extractor Predicting: 4it [00:08,  1.48s/it]Extractor Predicting: 5it [00:09,  1.17s/it]Extractor Predicting: 6it [00:10,  1.01s/it]Extractor Predicting: 7it [00:10,  1.13it/s]Extractor Predicting: 8it [00:11,  1.25it/s]Extractor Predicting: 9it [00:12,  1.31it/s]Extractor Predicting: 10it [00:12,  1.38it/s]Extractor Predicting: 11it [00:13,  1.42it/s]Extractor Predicting: 12it [00:13,  1.47it/s]Extractor Predicting: 13it [00:14,  1.52it/s]Extractor Predicting: 14it [00:15,  1.53it/s]Extractor Predicting: 15it [00:15,  1.56it/s]Extractor Predicting: 16it [00:16,  1.55it/s]Extractor Predicting: 17it [00:17,  1.57it/s]Extractor Predicting: 18it [00:17,  1.58it/s]Extractor Predicting: 19it [00:18,  1.56it/s]Extractor Predicting: 20it [00:19,  1.54it/s]Extractor Predicting: 21it [00:19,  1.54it/s]Extractor Predicting: 22it [00:20,  1.51it/s]Extractor Predicting: 23it [00:21,  1.54it/s]Extractor Predicting: 24it [00:21,  1.59it/s]Extractor Predicting: 25it [00:22,  1.57it/s]Extractor Predicting: 26it [00:22,  1.62it/s]Extractor Predicting: 27it [00:23,  1.64it/s]Extractor Predicting: 28it [00:24,  1.65it/s]Extractor Predicting: 29it [00:24,  1.63it/s]Extractor Predicting: 30it [00:25,  1.64it/s]Extractor Predicting: 31it [00:25,  1.62it/s]Extractor Predicting: 32it [00:26,  1.59it/s]Extractor Predicting: 33it [00:27,  1.61it/s]Extractor Predicting: 34it [00:27,  1.58it/s]Extractor Predicting: 35it [00:28,  1.59it/s]Extractor Predicting: 36it [00:29,  1.57it/s]Extractor Predicting: 37it [00:29,  1.56it/s]Extractor Predicting: 38it [00:30,  1.53it/s]Extractor Predicting: 39it [00:31,  1.55it/s]Extractor Predicting: 40it [00:31,  1.55it/s]Extractor Predicting: 41it [00:32,  1.51it/s]Extractor Predicting: 42it [00:33,  1.52it/s]Extractor Predicting: 43it [00:33,  1.55it/s]Extractor Predicting: 44it [00:34,  1.54it/s]Extractor Predicting: 45it [00:34,  1.55it/s]Extractor Predicting: 46it [00:35,  1.58it/s]Extractor Predicting: 47it [00:36,  1.56it/s]Extractor Predicting: 48it [00:36,  1.54it/s]Extractor Predicting: 49it [00:37,  1.42it/s]Extractor Predicting: 50it [00:38,  1.43it/s]Extractor Predicting: 51it [00:39,  1.45it/s]Extractor Predicting: 52it [00:39,  1.44it/s]Extractor Predicting: 53it [00:40,  1.47it/s]Extractor Predicting: 54it [00:41,  1.48it/s]Extractor Predicting: 55it [00:41,  1.49it/s]Extractor Predicting: 56it [00:42,  1.51it/s]Extractor Predicting: 57it [00:43,  1.51it/s]Extractor Predicting: 58it [00:43,  1.50it/s]Extractor Predicting: 59it [00:44,  1.52it/s]Extractor Predicting: 60it [00:45,  1.48it/s]Extractor Predicting: 61it [00:45,  1.47it/s]Extractor Predicting: 62it [00:46,  1.45it/s]Extractor Predicting: 63it [00:47,  1.44it/s]Extractor Predicting: 64it [00:47,  1.44it/s]Extractor Predicting: 65it [00:48,  1.43it/s]Extractor Predicting: 66it [00:49,  1.42it/s]Extractor Predicting: 67it [00:50,  1.40it/s]Extractor Predicting: 68it [00:50,  1.40it/s]Extractor Predicting: 69it [00:51,  1.40it/s]Extractor Predicting: 70it [00:52,  1.40it/s]Extractor Predicting: 71it [00:52,  1.39it/s]Extractor Predicting: 72it [00:53,  1.39it/s]Extractor Predicting: 73it [00:54,  1.38it/s]Extractor Predicting: 74it [00:55,  1.39it/s]Extractor Predicting: 75it [00:55,  1.38it/s]Extractor Predicting: 76it [00:56,  1.40it/s]Extractor Predicting: 77it [00:57,  1.40it/s]Extractor Predicting: 78it [00:57,  1.39it/s]Extractor Predicting: 79it [00:58,  1.42it/s]Extractor Predicting: 80it [00:59,  1.39it/s]Extractor Predicting: 81it [01:00,  1.37it/s]Extractor Predicting: 82it [01:00,  1.38it/s]Extractor Predicting: 83it [01:01,  1.42it/s]Extractor Predicting: 84it [01:02,  1.42it/s]Extractor Predicting: 85it [01:02,  1.43it/s]Extractor Predicting: 86it [01:03,  1.42it/s]Extractor Predicting: 87it [01:04,  1.39it/s]Extractor Predicting: 88it [01:05,  1.41it/s]Extractor Predicting: 89it [01:05,  1.44it/s]Extractor Predicting: 90it [01:06,  1.46it/s]Extractor Predicting: 91it [01:07,  1.47it/s]Extractor Predicting: 92it [01:07,  1.44it/s]Extractor Predicting: 93it [01:08,  1.47it/s]Extractor Predicting: 94it [01:09,  1.48it/s]Extractor Predicting: 95it [01:09,  1.47it/s]Extractor Predicting: 96it [01:10,  1.47it/s]Extractor Predicting: 97it [01:11,  1.48it/s]Extractor Predicting: 98it [01:11,  1.46it/s]Extractor Predicting: 99it [01:12,  1.47it/s]Extractor Predicting: 100it [01:13,  1.49it/s]Extractor Predicting: 101it [01:13,  1.49it/s]Extractor Predicting: 102it [01:14,  1.46it/s]Extractor Predicting: 103it [01:15,  1.42it/s]Extractor Predicting: 104it [01:15,  1.45it/s]Extractor Predicting: 105it [01:16,  1.46it/s]Extractor Predicting: 106it [01:17,  1.46it/s]Extractor Predicting: 107it [01:18,  1.43it/s]Extractor Predicting: 108it [01:18,  1.45it/s]Extractor Predicting: 109it [01:19,  1.44it/s]Extractor Predicting: 110it [01:20,  1.43it/s]Extractor Predicting: 111it [01:20,  1.43it/s]Extractor Predicting: 112it [01:21,  1.43it/s]Extractor Predicting: 113it [01:22,  1.44it/s]Extractor Predicting: 114it [01:22,  1.44it/s]Extractor Predicting: 115it [01:23,  1.44it/s]Extractor Predicting: 116it [01:24,  1.43it/s]Extractor Predicting: 117it [01:24,  1.47it/s]Extractor Predicting: 118it [01:25,  1.49it/s]Extractor Predicting: 119it [01:26,  1.53it/s]Extractor Predicting: 120it [01:26,  1.54it/s]Extractor Predicting: 121it [01:27,  1.52it/s]Extractor Predicting: 122it [01:28,  1.56it/s]Extractor Predicting: 123it [01:28,  1.58it/s]Extractor Predicting: 124it [01:29,  1.56it/s]Extractor Predicting: 125it [01:30,  1.57it/s]Extractor Predicting: 126it [01:30,  1.59it/s]Extractor Predicting: 127it [01:31,  1.62it/s]Extractor Predicting: 128it [01:31,  1.61it/s]Extractor Predicting: 129it [01:32,  1.63it/s]Extractor Predicting: 130it [01:33,  1.58it/s]Extractor Predicting: 131it [01:33,  1.58it/s]Extractor Predicting: 132it [01:34,  1.64it/s]Extractor Predicting: 133it [01:34,  1.65it/s]Extractor Predicting: 134it [01:35,  1.58it/s]Extractor Predicting: 135it [01:36,  1.60it/s]Extractor Predicting: 136it [01:36,  1.63it/s]Extractor Predicting: 137it [01:37,  1.64it/s]Extractor Predicting: 138it [01:38,  1.50it/s]Extractor Predicting: 139it [01:38,  1.54it/s]Extractor Predicting: 140it [01:39,  1.60it/s]Extractor Predicting: 141it [01:39,  1.64it/s]Extractor Predicting: 142it [01:40,  1.67it/s]Extractor Predicting: 143it [01:41,  1.65it/s]Extractor Predicting: 144it [01:41,  1.63it/s]Extractor Predicting: 145it [01:42,  1.82it/s]Extractor Predicting: 145it [01:42,  1.42it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.5550239234449761,
  "recall": 0.03317128967686588,
  "score": 0.06260118726389638,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 26049
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 26149, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.52it/s]Extractor Predicting: 4it [00:02,  1.49it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.47it/s]Extractor Predicting: 7it [00:04,  1.49it/s]Extractor Predicting: 8it [00:05,  1.49it/s]Extractor Predicting: 9it [00:06,  1.47it/s]Extractor Predicting: 10it [00:06,  1.47it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:08,  1.52it/s]Extractor Predicting: 13it [00:08,  1.50it/s]Extractor Predicting: 14it [00:09,  1.50it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.54it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:12,  1.50it/s]Extractor Predicting: 19it [00:12,  1.48it/s]Extractor Predicting: 20it [00:13,  1.47it/s]Extractor Predicting: 21it [00:14,  1.50it/s]Extractor Predicting: 22it [00:14,  1.45it/s]Extractor Predicting: 23it [00:15,  1.47it/s]Extractor Predicting: 24it [00:16,  1.49it/s]Extractor Predicting: 25it [00:16,  1.51it/s]Extractor Predicting: 26it [00:17,  1.49it/s]Extractor Predicting: 27it [00:18,  1.47it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:19,  1.45it/s]Extractor Predicting: 30it [00:20,  1.42it/s]Extractor Predicting: 31it [00:20,  1.43it/s]Extractor Predicting: 32it [00:21,  1.43it/s]Extractor Predicting: 33it [00:22,  1.45it/s]Extractor Predicting: 34it [00:22,  1.47it/s]Extractor Predicting: 35it [00:23,  1.47it/s]Extractor Predicting: 36it [00:24,  1.49it/s]Extractor Predicting: 37it [00:24,  1.54it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:26,  1.52it/s]Extractor Predicting: 40it [00:26,  1.53it/s]Extractor Predicting: 41it [00:27,  1.52it/s]Extractor Predicting: 42it [00:28,  1.51it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:30,  1.49it/s]Extractor Predicting: 46it [00:30,  1.52it/s]Extractor Predicting: 47it [00:31,  1.52it/s]Extractor Predicting: 48it [00:32,  1.51it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.49it/s]Extractor Predicting: 51it [00:34,  1.49it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:35,  1.50it/s]Extractor Predicting: 54it [00:36,  1.51it/s]Extractor Predicting: 55it [00:36,  1.48it/s]Extractor Predicting: 56it [00:37,  1.49it/s]Extractor Predicting: 57it [00:38,  1.52it/s]Extractor Predicting: 58it [00:38,  1.54it/s]Extractor Predicting: 59it [00:39,  1.54it/s]Extractor Predicting: 60it [00:40,  1.54it/s]Extractor Predicting: 61it [00:40,  1.53it/s]Extractor Predicting: 62it [00:41,  1.53it/s]Extractor Predicting: 63it [00:42,  1.52it/s]Extractor Predicting: 64it [00:42,  1.52it/s]Extractor Predicting: 65it [00:43,  1.50it/s]Extractor Predicting: 66it [00:44,  1.35it/s]Extractor Predicting: 67it [00:44,  1.41it/s]Extractor Predicting: 68it [00:45,  1.47it/s]Extractor Predicting: 69it [00:46,  1.51it/s]Extractor Predicting: 70it [00:46,  1.51it/s]Extractor Predicting: 71it [00:47,  1.50it/s]Extractor Predicting: 72it [00:48,  1.52it/s]Extractor Predicting: 73it [00:48,  1.48it/s]Extractor Predicting: 74it [00:49,  1.49it/s]Extractor Predicting: 75it [00:50,  1.50it/s]Extractor Predicting: 76it [00:50,  1.52it/s]Extractor Predicting: 77it [00:51,  1.48it/s]Extractor Predicting: 78it [00:52,  1.48it/s]Extractor Predicting: 79it [00:52,  1.49it/s]Extractor Predicting: 80it [00:53,  1.48it/s]Extractor Predicting: 81it [00:54,  1.50it/s]Extractor Predicting: 82it [00:54,  1.48it/s]Extractor Predicting: 83it [00:55,  1.48it/s]Extractor Predicting: 84it [00:56,  1.48it/s]Extractor Predicting: 85it [00:56,  1.51it/s]Extractor Predicting: 86it [00:57,  1.49it/s]Extractor Predicting: 87it [00:58,  1.49it/s]Extractor Predicting: 88it [00:58,  1.49it/s]Extractor Predicting: 89it [00:59,  1.49it/s]Extractor Predicting: 90it [01:00,  1.51it/s]Extractor Predicting: 91it [01:00,  1.50it/s]Extractor Predicting: 92it [01:01,  1.48it/s]Extractor Predicting: 93it [01:02,  1.46it/s]Extractor Predicting: 94it [01:03,  1.49it/s]Extractor Predicting: 95it [01:03,  1.48it/s]Extractor Predicting: 96it [01:04,  1.47it/s]Extractor Predicting: 97it [01:05,  1.49it/s]Extractor Predicting: 98it [01:05,  1.49it/s]Extractor Predicting: 99it [01:06,  1.50it/s]Extractor Predicting: 100it [01:07,  1.51it/s]Extractor Predicting: 101it [01:07,  1.53it/s]Extractor Predicting: 102it [01:08,  1.53it/s]Extractor Predicting: 103it [01:08,  1.51it/s]Extractor Predicting: 104it [01:09,  1.48it/s]Extractor Predicting: 105it [01:10,  1.46it/s]Extractor Predicting: 106it [01:11,  1.47it/s]Extractor Predicting: 107it [01:11,  1.46it/s]Extractor Predicting: 108it [01:12,  1.48it/s]Extractor Predicting: 109it [01:13,  1.46it/s]Extractor Predicting: 110it [01:13,  1.48it/s]Extractor Predicting: 111it [01:14,  1.48it/s]Extractor Predicting: 112it [01:15,  1.45it/s]Extractor Predicting: 113it [01:15,  1.45it/s]Extractor Predicting: 114it [01:16,  1.44it/s]Extractor Predicting: 115it [01:17,  1.41it/s]Extractor Predicting: 116it [01:18,  1.41it/s]Extractor Predicting: 117it [01:18,  1.42it/s]Extractor Predicting: 118it [01:19,  1.43it/s]Extractor Predicting: 119it [01:20,  1.44it/s]Extractor Predicting: 120it [01:20,  1.46it/s]Extractor Predicting: 121it [01:21,  1.48it/s]Extractor Predicting: 122it [01:22,  1.50it/s]Extractor Predicting: 123it [01:22,  1.50it/s]Extractor Predicting: 124it [01:23,  1.50it/s]Extractor Predicting: 125it [01:24,  1.49it/s]Extractor Predicting: 126it [01:24,  1.46it/s]Extractor Predicting: 127it [01:25,  1.46it/s]Extractor Predicting: 128it [01:26,  1.46it/s]Extractor Predicting: 129it [01:26,  1.48it/s]Extractor Predicting: 130it [01:27,  1.48it/s]Extractor Predicting: 131it [01:28,  1.49it/s]Extractor Predicting: 132it [01:28,  1.51it/s]Extractor Predicting: 133it [01:29,  1.48it/s]Extractor Predicting: 134it [01:30,  1.47it/s]Extractor Predicting: 135it [01:30,  1.46it/s]Extractor Predicting: 136it [01:31,  1.46it/s]Extractor Predicting: 137it [01:32,  1.45it/s]Extractor Predicting: 138it [01:32,  1.49it/s]Extractor Predicting: 139it [01:33,  1.48it/s]Extractor Predicting: 140it [01:34,  1.45it/s]Extractor Predicting: 141it [01:34,  1.47it/s]Extractor Predicting: 142it [01:35,  1.48it/s]Extractor Predicting: 143it [01:36,  1.48it/s]Extractor Predicting: 144it [01:36,  1.46it/s]Extractor Predicting: 145it [01:37,  1.49it/s]Extractor Predicting: 146it [01:38,  1.48it/s]Extractor Predicting: 147it [01:38,  1.51it/s]Extractor Predicting: 148it [01:39,  1.49it/s]Extractor Predicting: 149it [01:40,  1.50it/s]Extractor Predicting: 150it [01:40,  1.51it/s]Extractor Predicting: 151it [01:41,  1.39it/s]Extractor Predicting: 152it [01:42,  1.45it/s]Extractor Predicting: 153it [01:43,  1.47it/s]Extractor Predicting: 154it [01:43,  1.51it/s]Extractor Predicting: 155it [01:44,  1.51it/s]Extractor Predicting: 156it [01:45,  1.52it/s]Extractor Predicting: 157it [01:45,  1.51it/s]Extractor Predicting: 158it [01:46,  1.47it/s]Extractor Predicting: 159it [01:47,  1.49it/s]Extractor Predicting: 160it [01:47,  1.53it/s]Extractor Predicting: 161it [01:48,  1.53it/s]Extractor Predicting: 162it [01:48,  1.54it/s]Extractor Predicting: 163it [01:49,  1.55it/s]Extractor Predicting: 164it [01:50,  1.55it/s]Extractor Predicting: 165it [01:50,  1.53it/s]Extractor Predicting: 166it [01:51,  1.53it/s]Extractor Predicting: 167it [01:52,  1.52it/s]Extractor Predicting: 168it [01:52,  1.52it/s]Extractor Predicting: 169it [01:53,  1.55it/s]Extractor Predicting: 170it [01:54,  1.56it/s]Extractor Predicting: 171it [01:54,  1.53it/s]Extractor Predicting: 172it [01:55,  1.51it/s]Extractor Predicting: 173it [01:56,  1.51it/s]Extractor Predicting: 174it [01:56,  1.53it/s]Extractor Predicting: 175it [01:57,  1.56it/s]Extractor Predicting: 176it [01:58,  1.57it/s]Extractor Predicting: 177it [01:58,  1.51it/s]Extractor Predicting: 178it [01:59,  1.54it/s]Extractor Predicting: 179it [02:00,  1.52it/s]Extractor Predicting: 180it [02:00,  1.52it/s]Extractor Predicting: 181it [02:01,  1.53it/s]Extractor Predicting: 182it [02:02,  1.51it/s]Extractor Predicting: 183it [02:02,  1.52it/s]Extractor Predicting: 184it [02:03,  1.53it/s]Extractor Predicting: 185it [02:03,  1.55it/s]Extractor Predicting: 186it [02:04,  1.57it/s]Extractor Predicting: 187it [02:05,  1.61it/s]Extractor Predicting: 188it [02:05,  1.58it/s]Extractor Predicting: 189it [02:06,  1.53it/s]Extractor Predicting: 190it [02:07,  1.52it/s]Extractor Predicting: 191it [02:07,  1.50it/s]Extractor Predicting: 192it [02:08,  1.52it/s]Extractor Predicting: 193it [02:09,  1.53it/s]Extractor Predicting: 194it [02:09,  1.54it/s]Extractor Predicting: 195it [02:10,  1.56it/s]Extractor Predicting: 196it [02:11,  1.56it/s]Extractor Predicting: 197it [02:11,  1.51it/s]Extractor Predicting: 198it [02:12,  1.52it/s]Extractor Predicting: 199it [02:13,  1.51it/s]Extractor Predicting: 200it [02:13,  1.51it/s]Extractor Predicting: 201it [02:14,  1.51it/s]Extractor Predicting: 202it [02:15,  1.51it/s]Extractor Predicting: 203it [02:15,  1.50it/s]Extractor Predicting: 204it [02:16,  1.48it/s]Extractor Predicting: 205it [02:17,  1.45it/s]Extractor Predicting: 206it [02:17,  1.49it/s]Extractor Predicting: 207it [02:18,  1.49it/s]Extractor Predicting: 208it [02:19,  1.52it/s]Extractor Predicting: 209it [02:19,  1.54it/s]Extractor Predicting: 210it [02:20,  1.52it/s]Extractor Predicting: 211it [02:21,  1.51it/s]Extractor Predicting: 212it [02:21,  1.50it/s]Extractor Predicting: 213it [02:22,  1.48it/s]Extractor Predicting: 214it [02:23,  1.48it/s]Extractor Predicting: 215it [02:23,  1.47it/s]Extractor Predicting: 216it [02:24,  1.48it/s]Extractor Predicting: 217it [02:25,  1.46it/s]Extractor Predicting: 218it [02:25,  1.46it/s]Extractor Predicting: 219it [02:26,  1.47it/s]Extractor Predicting: 220it [02:27,  1.49it/s]Extractor Predicting: 221it [02:27,  1.52it/s]Extractor Predicting: 222it [02:28,  1.46it/s]Extractor Predicting: 223it [02:29,  1.44it/s]Extractor Predicting: 224it [02:29,  1.49it/s]Extractor Predicting: 225it [02:30,  1.49it/s]Extractor Predicting: 226it [02:31,  1.47it/s]Extractor Predicting: 227it [02:31,  1.48it/s]Extractor Predicting: 228it [02:32,  1.49it/s]Extractor Predicting: 229it [02:33,  1.51it/s]Extractor Predicting: 230it [02:33,  1.47it/s]Extractor Predicting: 231it [02:34,  1.48it/s]Extractor Predicting: 232it [02:35,  1.51it/s]Extractor Predicting: 233it [02:35,  1.48it/s]Extractor Predicting: 234it [02:36,  1.46it/s]Extractor Predicting: 235it [02:37,  1.47it/s]Extractor Predicting: 236it [02:38,  1.48it/s]Extractor Predicting: 237it [02:38,  1.45it/s]Extractor Predicting: 238it [02:39,  1.46it/s]Extractor Predicting: 239it [02:40,  1.47it/s]Extractor Predicting: 240it [02:40,  1.45it/s]Extractor Predicting: 241it [02:41,  1.47it/s]Extractor Predicting: 242it [02:42,  1.48it/s]Extractor Predicting: 243it [02:42,  1.45it/s]Extractor Predicting: 244it [02:43,  1.47it/s]Extractor Predicting: 245it [02:44,  1.46it/s]Extractor Predicting: 246it [02:44,  1.47it/s]Extractor Predicting: 247it [02:45,  1.47it/s]Extractor Predicting: 248it [02:46,  1.47it/s]Extractor Predicting: 249it [02:47,  1.34it/s]Extractor Predicting: 250it [02:47,  1.37it/s]Extractor Predicting: 251it [02:48,  1.41it/s]Extractor Predicting: 252it [02:49,  1.41it/s]Extractor Predicting: 253it [02:49,  1.43it/s]Extractor Predicting: 254it [02:50,  1.45it/s]Extractor Predicting: 255it [02:51,  1.47it/s]Extractor Predicting: 256it [02:51,  1.47it/s]Extractor Predicting: 257it [02:52,  1.50it/s]Extractor Predicting: 258it [02:53,  1.51it/s]Extractor Predicting: 259it [02:53,  1.53it/s]Extractor Predicting: 260it [02:54,  1.51it/s]Extractor Predicting: 261it [02:55,  1.52it/s]Extractor Predicting: 262it [02:55,  1.51it/s]Extractor Predicting: 263it [02:56,  1.51it/s]Extractor Predicting: 264it [02:57,  1.48it/s]Extractor Predicting: 265it [02:57,  1.47it/s]Extractor Predicting: 266it [02:58,  1.48it/s]Extractor Predicting: 267it [02:59,  1.49it/s]Extractor Predicting: 268it [02:59,  1.51it/s]Extractor Predicting: 269it [03:00,  1.51it/s]Extractor Predicting: 270it [03:01,  1.51it/s]Extractor Predicting: 271it [03:01,  1.52it/s]Extractor Predicting: 272it [03:02,  1.56it/s]Extractor Predicting: 273it [03:03,  1.56it/s]Extractor Predicting: 274it [03:03,  1.57it/s]Extractor Predicting: 275it [03:04,  1.54it/s]Extractor Predicting: 276it [03:05,  1.52it/s]Extractor Predicting: 277it [03:05,  1.54it/s]Extractor Predicting: 278it [03:06,  1.51it/s]Extractor Predicting: 279it [03:07,  1.52it/s]Extractor Predicting: 280it [03:07,  1.50it/s]Extractor Predicting: 281it [03:08,  1.51it/s]Extractor Predicting: 282it [03:09,  1.49it/s]Extractor Predicting: 283it [03:09,  1.49it/s]Extractor Predicting: 284it [03:10,  1.50it/s]Extractor Predicting: 285it [03:11,  1.49it/s]Extractor Predicting: 286it [03:11,  1.48it/s]Extractor Predicting: 287it [03:12,  1.49it/s]Extractor Predicting: 288it [03:13,  1.50it/s]Extractor Predicting: 289it [03:13,  1.50it/s]Extractor Predicting: 290it [03:14,  1.50it/s]Extractor Predicting: 291it [03:15,  1.49it/s]Extractor Predicting: 292it [03:15,  1.51it/s]Extractor Predicting: 293it [03:16,  1.50it/s]Extractor Predicting: 294it [03:17,  1.49it/s]Extractor Predicting: 295it [03:17,  1.52it/s]Extractor Predicting: 296it [03:18,  1.52it/s]Extractor Predicting: 297it [03:19,  1.49it/s]Extractor Predicting: 298it [03:19,  1.52it/s]Extractor Predicting: 299it [03:20,  1.51it/s]Extractor Predicting: 300it [03:21,  1.50it/s]Extractor Predicting: 301it [03:21,  1.56it/s]Extractor Predicting: 302it [03:22,  1.54it/s]Extractor Predicting: 303it [03:22,  1.57it/s]Extractor Predicting: 304it [03:23,  1.55it/s]Extractor Predicting: 305it [03:24,  1.53it/s]Extractor Predicting: 306it [03:24,  1.52it/s]Extractor Predicting: 307it [03:25,  1.51it/s]Extractor Predicting: 308it [03:26,  1.49it/s]Extractor Predicting: 309it [03:26,  1.50it/s]Extractor Predicting: 310it [03:27,  1.48it/s]Extractor Predicting: 311it [03:28,  1.47it/s]Extractor Predicting: 312it [03:29,  1.44it/s]Extractor Predicting: 313it [03:29,  1.45it/s]Extractor Predicting: 314it [03:30,  1.46it/s]Extractor Predicting: 315it [03:31,  1.49it/s]Extractor Predicting: 316it [03:31,  1.50it/s]Extractor Predicting: 317it [03:32,  1.49it/s]Extractor Predicting: 318it [03:32,  1.50it/s]Extractor Predicting: 319it [03:33,  1.48it/s]Extractor Predicting: 320it [03:34,  1.48it/s]Extractor Predicting: 321it [03:35,  1.48it/s]Extractor Predicting: 322it [03:35,  1.47it/s]Extractor Predicting: 323it [03:36,  1.52it/s]Extractor Predicting: 324it [03:37,  1.49it/s]Extractor Predicting: 325it [03:37,  1.50it/s]Extractor Predicting: 326it [03:38,  1.50it/s]Extractor Predicting: 327it [03:39,  1.52it/s]Extractor Predicting: 328it [03:39,  1.53it/s]Extractor Predicting: 329it [03:40,  1.50it/s]Extractor Predicting: 330it [03:41,  1.48it/s]Extractor Predicting: 331it [03:41,  1.47it/s]Extractor Predicting: 332it [03:42,  1.46it/s]Extractor Predicting: 333it [03:43,  1.48it/s]Extractor Predicting: 334it [03:43,  1.48it/s]Extractor Predicting: 335it [03:44,  1.48it/s]Extractor Predicting: 336it [03:45,  1.51it/s]Extractor Predicting: 337it [03:45,  1.52it/s]Extractor Predicting: 338it [03:46,  1.51it/s]Extractor Predicting: 339it [03:47,  1.49it/s]Extractor Predicting: 340it [03:47,  1.50it/s]Extractor Predicting: 341it [03:48,  1.52it/s]Extractor Predicting: 342it [03:49,  1.48it/s]Extractor Predicting: 343it [03:49,  1.50it/s]Extractor Predicting: 344it [03:50,  1.50it/s]Extractor Predicting: 345it [03:51,  1.52it/s]Extractor Predicting: 346it [03:51,  1.51it/s]Extractor Predicting: 347it [03:52,  1.35it/s]Extractor Predicting: 348it [03:53,  1.40it/s]Extractor Predicting: 349it [03:53,  1.41it/s]Extractor Predicting: 350it [03:54,  1.43it/s]Extractor Predicting: 351it [03:55,  1.43it/s]Extractor Predicting: 352it [03:56,  1.42it/s]Extractor Predicting: 353it [03:56,  1.44it/s]Extractor Predicting: 354it [03:57,  1.44it/s]Extractor Predicting: 355it [03:58,  1.47it/s]Extractor Predicting: 356it [03:58,  1.48it/s]Extractor Predicting: 357it [03:59,  1.50it/s]Extractor Predicting: 358it [04:00,  1.52it/s]Extractor Predicting: 359it [04:00,  1.51it/s]Extractor Predicting: 360it [04:01,  1.49it/s]Extractor Predicting: 361it [04:02,  1.50it/s]Extractor Predicting: 362it [04:02,  1.51it/s]Extractor Predicting: 363it [04:03,  1.53it/s]Extractor Predicting: 364it [04:04,  1.53it/s]Extractor Predicting: 365it [04:04,  1.52it/s]Extractor Predicting: 366it [04:05,  1.51it/s]Extractor Predicting: 367it [04:06,  1.50it/s]Extractor Predicting: 368it [04:06,  1.52it/s]Extractor Predicting: 369it [04:07,  1.52it/s]Extractor Predicting: 370it [04:07,  1.55it/s]Extractor Predicting: 371it [04:08,  1.53it/s]Extractor Predicting: 372it [04:09,  1.53it/s]Extractor Predicting: 373it [04:09,  1.52it/s]Extractor Predicting: 374it [04:10,  1.51it/s]Extractor Predicting: 375it [04:11,  1.52it/s]Extractor Predicting: 376it [04:11,  1.51it/s]Extractor Predicting: 377it [04:12,  1.52it/s]Extractor Predicting: 378it [04:13,  1.54it/s]Extractor Predicting: 379it [04:13,  1.49it/s]Extractor Predicting: 380it [04:14,  1.50it/s]Extractor Predicting: 381it [04:15,  1.54it/s]Extractor Predicting: 382it [04:15,  1.53it/s]Extractor Predicting: 383it [04:16,  1.48it/s]Extractor Predicting: 384it [04:17,  1.48it/s]Extractor Predicting: 385it [04:17,  1.49it/s]Extractor Predicting: 386it [04:18,  1.54it/s]Extractor Predicting: 387it [04:19,  1.55it/s]Extractor Predicting: 388it [04:19,  1.54it/s]Extractor Predicting: 389it [04:20,  1.51it/s]Extractor Predicting: 390it [04:21,  1.52it/s]Extractor Predicting: 391it [04:21,  1.53it/s]Extractor Predicting: 392it [04:22,  1.53it/s]Extractor Predicting: 393it [04:23,  1.54it/s]Extractor Predicting: 394it [04:23,  1.57it/s]Extractor Predicting: 395it [04:24,  1.53it/s]Extractor Predicting: 396it [04:25,  1.52it/s]Extractor Predicting: 397it [04:25,  1.53it/s]Extractor Predicting: 398it [04:26,  1.53it/s]Extractor Predicting: 399it [04:26,  1.55it/s]Extractor Predicting: 400it [04:27,  1.59it/s]Extractor Predicting: 401it [04:28,  1.56it/s]Extractor Predicting: 402it [04:28,  1.56it/s]Extractor Predicting: 403it [04:29,  1.54it/s]Extractor Predicting: 404it [04:30,  1.53it/s]Extractor Predicting: 405it [04:30,  1.53it/s]Extractor Predicting: 406it [04:31,  1.52it/s]Extractor Predicting: 407it [04:32,  1.51it/s]Extractor Predicting: 408it [04:32,  1.51it/s]Extractor Predicting: 409it [04:33,  1.54it/s]Extractor Predicting: 409it [04:33,  1.50it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.decoder.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0.5536028119507909,
  "recall": 0.03210027514521553,
  "score": 0.060681949528029286,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/fewrel/unseen_15_seed_0/test.jsonl', 'labels': ['competition class', 'field of work', 'language of work or name', 'location', 'manufacturer', 'member of political party', 'nominated for', 'operating system', 'original broadcaster', 'owned by', 'position held', 'position played on team / speciality', 'record label', 'religion', 'said to be the same as'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 2333
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 2433, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.38it/s]Extractor Predicting: 3it [00:02,  1.39it/s]Extractor Predicting: 4it [00:02,  1.43it/s]Extractor Predicting: 5it [00:03,  1.43it/s]Extractor Predicting: 6it [00:04,  1.44it/s]Extractor Predicting: 7it [00:04,  1.43it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.50it/s]Extractor Predicting: 10it [00:06,  1.48it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:08,  1.48it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 13it [00:08,  1.46it/s]
{
  "path_pred": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.5,
  "recall": 0.004366812227074236,
  "score": 0.008658008658008658,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_filtered_large/unseen_15_seed_0/extractor/results_multi_is_eval_False.json"
}
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main': {'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'data_name': 'wiki', 'split': 'unseen_15_seed_0', 'type': 'filtered', 'model_size': 'large', 'num_gen_per_label': 250, 'g_encoder_name': 'generate'}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/wiki/unseen_15_seed_0/generator/model', data_dir='outputs/wrapper/wiki/unseen_15_seed_0/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'filter_data': {'path_pseudo': 'outputs/wrapper/wiki/unseen_15_seed_0/generator/synthetic.jsonl', 'path_train': 'zero_rte/wiki/unseen_15_seed_0/train.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/filtered.jsonl', 'total_pseudo_per_label': 250, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': True, 'version': 'single', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000, 'num_pseudo_per_label': 50, 'num_train_per_label': 43}
num of filtered data: 4547 mean pseudo reward: 1.0
fit {'path_train': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/filtered.jsonl', 'path_dev': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl'}
train vocab size: 22490
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22590, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='', model_write_ckpt='outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/model', pretrained_wv='outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22590, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.393, loss:52695.7413
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 10, avg_time 1.047, loss:2318.4563
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 110, avg_time 1.046, loss:2185.0074
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 20, avg_time 1.037, loss:2064.6020
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 120, avg_time 1.061, loss:2101.2478
>> valid entity prec:0.4075, rec:0.5099, f1:0.4530
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 30, avg_time 2.591, loss:2035.5461
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 130, avg_time 1.046, loss:1984.3722
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 40, avg_time 1.049, loss:1922.2139
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 140, avg_time 1.042, loss:1834.6873
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 50, avg_time 1.040, loss:1819.1959
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.3364, rec:0.2720, f1:0.3008
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 150, avg_time 2.524, loss:1700.8687
g_step 1200, step 60, avg_time 1.064, loss:1560.1274
g_step 1300, step 160, avg_time 1.050, loss:1598.8172
g_step 1400, step 70, avg_time 1.031, loss:1501.5341
g_step 1500, step 170, avg_time 1.057, loss:1487.8254
>> valid entity prec:0.4756, rec:0.4147, f1:0.4431
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 80, avg_time 2.519, loss:1433.7656
g_step 1700, step 180, avg_time 1.060, loss:1394.4094
g_step 1800, step 90, avg_time 1.046, loss:1344.9405
g_step 1900, step 190, avg_time 1.044, loss:1348.1725
g_step 2000, step 100, avg_time 1.045, loss:1289.7577
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5119, rec:0.5212, f1:0.5165
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 2100, step 10, avg_time 2.535, loss:1279.2244
g_step 2200, step 110, avg_time 1.051, loss:1241.7357
g_step 2300, step 20, avg_time 1.037, loss:1226.8609
g_step 2400, step 120, avg_time 1.051, loss:1183.9300
g_step 2500, step 30, avg_time 1.045, loss:1165.7578
>> valid entity prec:0.5476, rec:0.3907, f1:0.4560
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 130, avg_time 2.515, loss:1156.9991
g_step 2700, step 40, avg_time 1.051, loss:1118.6051
g_step 2800, step 140, avg_time 1.044, loss:1110.0145
g_step 2900, step 50, avg_time 1.056, loss:1044.7163
g_step 3000, step 150, avg_time 1.043, loss:1102.0743
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.4419, rec:0.5188, f1:0.4773
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 60, avg_time 2.520, loss:1004.8891
g_step 3200, step 160, avg_time 1.054, loss:1015.0354
g_step 3300, step 70, avg_time 1.046, loss:982.6256
g_step 3400, step 170, avg_time 1.043, loss:950.2727
g_step 3500, step 80, avg_time 1.041, loss:929.3083
>> valid entity prec:0.4872, rec:0.4543, f1:0.4702
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 180, avg_time 2.525, loss:947.3020
g_step 3700, step 90, avg_time 1.049, loss:887.0602
g_step 3800, step 190, avg_time 1.044, loss:924.2051
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/dev.jsonl', 'labels': ['characters', 'from narrative universe', 'lowest point', 'manufacturer', 'work location'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 12876
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12976, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:06,  6.07s/it]Extractor Predicting: 2it [00:06,  2.84s/it]Extractor Predicting: 3it [00:07,  2.01s/it]Extractor Predicting: 4it [00:08,  1.60s/it]Extractor Predicting: 5it [00:09,  1.39s/it]Extractor Predicting: 6it [00:11,  1.42s/it]Extractor Predicting: 7it [00:11,  1.15s/it]Extractor Predicting: 8it [00:12,  1.03it/s]Extractor Predicting: 9it [00:12,  1.19it/s]Extractor Predicting: 10it [00:13,  1.31it/s]Extractor Predicting: 11it [00:14,  1.42it/s]Extractor Predicting: 12it [00:14,  1.50it/s]Extractor Predicting: 13it [00:15,  1.50it/s]Extractor Predicting: 14it [00:15,  1.51it/s]Extractor Predicting: 15it [00:16,  1.49it/s]Extractor Predicting: 16it [00:17,  1.52it/s]Extractor Predicting: 17it [00:17,  1.52it/s]Extractor Predicting: 18it [00:18,  1.50it/s]Extractor Predicting: 19it [00:19,  1.51it/s]Extractor Predicting: 20it [00:19,  1.55it/s]Extractor Predicting: 21it [00:20,  1.53it/s]Extractor Predicting: 22it [00:21,  1.52it/s]Extractor Predicting: 23it [00:21,  1.51it/s]Extractor Predicting: 24it [00:22,  1.49it/s]Extractor Predicting: 25it [00:23,  1.51it/s]Extractor Predicting: 26it [00:23,  1.53it/s]Extractor Predicting: 27it [00:24,  1.54it/s]Extractor Predicting: 28it [00:25,  1.57it/s]Extractor Predicting: 29it [00:25,  1.55it/s]Extractor Predicting: 30it [00:26,  1.55it/s]Extractor Predicting: 31it [00:27,  1.43it/s]Extractor Predicting: 32it [00:27,  1.48it/s]Extractor Predicting: 33it [00:28,  1.49it/s]Extractor Predicting: 34it [00:30,  1.16s/it]Extractor Predicting: 35it [00:31,  1.00it/s]Extractor Predicting: 36it [00:32,  1.13it/s]Extractor Predicting: 37it [00:32,  1.20it/s]Extractor Predicting: 38it [00:33,  1.27it/s]Extractor Predicting: 39it [00:34,  1.34it/s]Extractor Predicting: 40it [00:34,  1.39it/s]Extractor Predicting: 41it [00:35,  1.47it/s]Extractor Predicting: 42it [00:35,  1.50it/s]Extractor Predicting: 43it [00:36,  1.49it/s]Extractor Predicting: 44it [00:37,  1.48it/s]Extractor Predicting: 45it [00:37,  1.50it/s]Extractor Predicting: 46it [00:38,  1.53it/s]Extractor Predicting: 47it [00:39,  1.51it/s]Extractor Predicting: 48it [00:39,  1.54it/s]Extractor Predicting: 49it [00:40,  1.54it/s]Extractor Predicting: 50it [00:41,  1.58it/s]Extractor Predicting: 51it [00:41,  1.58it/s]Extractor Predicting: 52it [00:42,  1.55it/s]Extractor Predicting: 53it [00:43,  1.58it/s]Extractor Predicting: 54it [00:43,  1.54it/s]Extractor Predicting: 55it [00:44,  1.57it/s]Extractor Predicting: 56it [00:45,  1.54it/s]Extractor Predicting: 57it [00:45,  1.53it/s]Extractor Predicting: 58it [00:46,  1.52it/s]Extractor Predicting: 59it [00:47,  1.54it/s]Extractor Predicting: 60it [00:47,  1.53it/s]Extractor Predicting: 61it [00:48,  1.57it/s]Extractor Predicting: 62it [00:48,  1.56it/s]Extractor Predicting: 63it [00:49,  1.57it/s]Extractor Predicting: 64it [00:50,  1.57it/s]Extractor Predicting: 65it [00:50,  1.57it/s]Extractor Predicting: 66it [00:51,  1.55it/s]Extractor Predicting: 67it [00:52,  1.56it/s]Extractor Predicting: 68it [00:52,  1.53it/s]Extractor Predicting: 69it [00:53,  1.54it/s]Extractor Predicting: 70it [00:54,  1.56it/s]Extractor Predicting: 71it [00:54,  1.56it/s]Extractor Predicting: 72it [00:55,  1.54it/s]Extractor Predicting: 73it [00:56,  1.52it/s]Extractor Predicting: 74it [00:56,  1.51it/s]Extractor Predicting: 75it [00:57,  1.55it/s]Extractor Predicting: 76it [00:58,  1.47it/s]Extractor Predicting: 77it [00:58,  1.47it/s]Extractor Predicting: 78it [00:59,  1.46it/s]Extractor Predicting: 79it [01:00,  1.47it/s]Extractor Predicting: 80it [01:00,  1.47it/s]Extractor Predicting: 81it [01:01,  1.43it/s]Extractor Predicting: 82it [01:02,  1.46it/s]Extractor Predicting: 83it [01:02,  1.51it/s]Extractor Predicting: 84it [01:03,  1.52it/s]Extractor Predicting: 85it [01:04,  1.52it/s]Extractor Predicting: 86it [01:04,  1.53it/s]Extractor Predicting: 87it [01:05,  1.52it/s]Extractor Predicting: 88it [01:06,  1.54it/s]Extractor Predicting: 89it [01:06,  1.52it/s]Extractor Predicting: 90it [01:07,  1.51it/s]Extractor Predicting: 91it [01:08,  1.54it/s]Extractor Predicting: 92it [01:08,  1.55it/s]Extractor Predicting: 93it [01:09,  1.52it/s]Extractor Predicting: 94it [01:10,  1.50it/s]Extractor Predicting: 95it [01:10,  1.47it/s]Extractor Predicting: 96it [01:11,  1.51it/s]Extractor Predicting: 97it [01:12,  1.53it/s]Extractor Predicting: 98it [01:12,  1.54it/s]Extractor Predicting: 99it [01:13,  1.52it/s]Extractor Predicting: 100it [01:13,  1.53it/s]Extractor Predicting: 101it [01:14,  1.52it/s]Extractor Predicting: 102it [01:15,  1.50it/s]Extractor Predicting: 103it [01:16,  1.49it/s]Extractor Predicting: 104it [01:16,  1.51it/s]Extractor Predicting: 105it [01:17,  1.50it/s]Extractor Predicting: 106it [01:17,  1.51it/s]Extractor Predicting: 107it [01:18,  1.53it/s]Extractor Predicting: 108it [01:19,  1.49it/s]Extractor Predicting: 109it [01:19,  1.51it/s]Extractor Predicting: 110it [01:20,  1.50it/s]Extractor Predicting: 111it [01:21,  1.51it/s]Extractor Predicting: 112it [01:21,  1.49it/s]Extractor Predicting: 113it [01:22,  1.51it/s]Extractor Predicting: 114it [01:23,  1.50it/s]Extractor Predicting: 115it [01:23,  1.52it/s]Extractor Predicting: 116it [01:24,  1.53it/s]Extractor Predicting: 117it [01:25,  1.56it/s]Extractor Predicting: 118it [01:25,  1.53it/s]Extractor Predicting: 119it [01:26,  1.49it/s]Extractor Predicting: 120it [01:27,  1.53it/s]Extractor Predicting: 121it [01:27,  1.52it/s]Extractor Predicting: 122it [01:28,  1.50it/s]Extractor Predicting: 123it [01:29,  1.48it/s]Extractor Predicting: 124it [01:29,  1.48it/s]Extractor Predicting: 125it [01:30,  1.37it/s]Extractor Predicting: 126it [01:31,  1.40it/s]Extractor Predicting: 127it [01:32,  1.41it/s]Extractor Predicting: 128it [01:32,  1.43it/s]Extractor Predicting: 129it [01:33,  1.44it/s]Extractor Predicting: 130it [01:34,  1.46it/s]Extractor Predicting: 131it [01:34,  1.45it/s]Extractor Predicting: 132it [01:35,  1.35it/s]Extractor Predicting: 133it [01:36,  1.40it/s]Extractor Predicting: 134it [01:37,  1.44it/s]Extractor Predicting: 135it [01:37,  1.46it/s]Extractor Predicting: 136it [01:38,  1.48it/s]Extractor Predicting: 137it [01:39,  1.47it/s]Extractor Predicting: 138it [01:39,  1.47it/s]Extractor Predicting: 139it [01:40,  1.47it/s]Extractor Predicting: 140it [01:41,  1.49it/s]Extractor Predicting: 141it [01:41,  1.51it/s]Extractor Predicting: 142it [01:42,  1.49it/s]Extractor Predicting: 143it [01:43,  1.52it/s]Extractor Predicting: 144it [01:43,  1.50it/s]Extractor Predicting: 145it [01:44,  1.50it/s]Extractor Predicting: 146it [01:45,  1.48it/s]Extractor Predicting: 147it [01:45,  1.47it/s]Extractor Predicting: 148it [01:46,  1.48it/s]Extractor Predicting: 149it [01:47,  1.48it/s]Extractor Predicting: 150it [01:47,  1.50it/s]Extractor Predicting: 151it [01:48,  1.50it/s]Extractor Predicting: 152it [01:49,  1.45it/s]Extractor Predicting: 153it [01:49,  1.46it/s]Extractor Predicting: 154it [01:50,  1.58it/s]Extractor Predicting: 154it [01:50,  1.40it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 27443
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 27543, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.67it/s]Extractor Predicting: 2it [00:01,  1.62it/s]Extractor Predicting: 3it [00:01,  1.61it/s]Extractor Predicting: 4it [00:02,  1.58it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.58it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.64it/s]Extractor Predicting: 13it [00:08,  1.62it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:10,  1.59it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.57it/s]Extractor Predicting: 19it [00:11,  1.61it/s]Extractor Predicting: 20it [00:12,  1.59it/s]Extractor Predicting: 21it [00:13,  1.60it/s]Extractor Predicting: 22it [00:13,  1.57it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.56it/s]Extractor Predicting: 25it [00:15,  1.56it/s]Extractor Predicting: 26it [00:16,  1.55it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:17,  1.60it/s]Extractor Predicting: 29it [00:18,  1.58it/s]Extractor Predicting: 30it [00:18,  1.58it/s]Extractor Predicting: 31it [00:19,  1.59it/s]Extractor Predicting: 32it [00:20,  1.59it/s]Extractor Predicting: 33it [00:20,  1.58it/s]Extractor Predicting: 34it [00:21,  1.57it/s]Extractor Predicting: 35it [00:22,  1.59it/s]Extractor Predicting: 36it [00:22,  1.58it/s]Extractor Predicting: 37it [00:23,  1.44it/s]Extractor Predicting: 38it [00:24,  1.47it/s]Extractor Predicting: 39it [00:24,  1.54it/s]Extractor Predicting: 40it [00:25,  1.55it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:26,  1.46it/s]Extractor Predicting: 43it [00:27,  1.48it/s]Extractor Predicting: 44it [00:28,  1.46it/s]Extractor Predicting: 45it [00:28,  1.48it/s]Extractor Predicting: 46it [00:29,  1.46it/s]Extractor Predicting: 47it [00:30,  1.45it/s]Extractor Predicting: 48it [00:30,  1.43it/s]Extractor Predicting: 49it [00:31,  1.46it/s]Extractor Predicting: 50it [00:32,  1.46it/s]Extractor Predicting: 51it [00:33,  1.46it/s]Extractor Predicting: 52it [00:33,  1.46it/s]Extractor Predicting: 53it [00:34,  1.46it/s]Extractor Predicting: 54it [00:35,  1.45it/s]Extractor Predicting: 55it [00:35,  1.40it/s]Extractor Predicting: 56it [00:36,  1.44it/s]Extractor Predicting: 57it [00:37,  1.48it/s]Extractor Predicting: 58it [00:37,  1.46it/s]Extractor Predicting: 59it [00:38,  1.44it/s]Extractor Predicting: 60it [00:39,  1.41it/s]Extractor Predicting: 61it [00:40,  1.41it/s]Extractor Predicting: 62it [00:40,  1.45it/s]Extractor Predicting: 63it [00:41,  1.45it/s]Extractor Predicting: 64it [00:42,  1.41it/s]Extractor Predicting: 65it [00:42,  1.42it/s]Extractor Predicting: 66it [00:43,  1.43it/s]Extractor Predicting: 67it [00:44,  1.41it/s]Extractor Predicting: 68it [00:44,  1.42it/s]Extractor Predicting: 69it [00:45,  1.41it/s]Extractor Predicting: 70it [00:46,  1.42it/s]Extractor Predicting: 71it [00:46,  1.43it/s]Extractor Predicting: 72it [00:47,  1.43it/s]Extractor Predicting: 73it [00:48,  1.40it/s]Extractor Predicting: 74it [00:49,  1.40it/s]Extractor Predicting: 75it [00:49,  1.40it/s]Extractor Predicting: 76it [00:50,  1.38it/s]Extractor Predicting: 77it [00:51,  1.43it/s]Extractor Predicting: 78it [00:51,  1.41it/s]Extractor Predicting: 79it [00:52,  1.42it/s]Extractor Predicting: 80it [00:53,  1.42it/s]Extractor Predicting: 81it [00:54,  1.37it/s]Extractor Predicting: 82it [00:54,  1.39it/s]Extractor Predicting: 83it [00:55,  1.43it/s]Extractor Predicting: 84it [00:56,  1.46it/s]Extractor Predicting: 85it [00:56,  1.41it/s]Extractor Predicting: 86it [00:57,  1.39it/s]Extractor Predicting: 87it [00:58,  1.44it/s]Extractor Predicting: 88it [00:59,  1.42it/s]Extractor Predicting: 89it [00:59,  1.40it/s]Extractor Predicting: 90it [01:00,  1.39it/s]Extractor Predicting: 91it [01:01,  1.41it/s]Extractor Predicting: 92it [01:01,  1.44it/s]Extractor Predicting: 93it [01:02,  1.48it/s]Extractor Predicting: 94it [01:03,  1.48it/s]Extractor Predicting: 95it [01:03,  1.49it/s]Extractor Predicting: 96it [01:04,  1.48it/s]Extractor Predicting: 97it [01:05,  1.41it/s]Extractor Predicting: 98it [01:06,  1.41it/s]Extractor Predicting: 99it [01:06,  1.44it/s]Extractor Predicting: 100it [01:07,  1.44it/s]Extractor Predicting: 101it [01:08,  1.46it/s]Extractor Predicting: 102it [01:08,  1.47it/s]Extractor Predicting: 103it [01:09,  1.48it/s]Extractor Predicting: 104it [01:10,  1.49it/s]Extractor Predicting: 105it [01:10,  1.48it/s]Extractor Predicting: 106it [01:11,  1.46it/s]Extractor Predicting: 107it [01:12,  1.48it/s]Extractor Predicting: 108it [01:12,  1.48it/s]Extractor Predicting: 109it [01:13,  1.46it/s]Extractor Predicting: 110it [01:15,  1.10s/it]Extractor Predicting: 111it [01:16,  1.03it/s]Extractor Predicting: 112it [01:16,  1.13it/s]Extractor Predicting: 113it [01:17,  1.24it/s]Extractor Predicting: 114it [01:18,  1.32it/s]Extractor Predicting: 115it [01:18,  1.34it/s]Extractor Predicting: 116it [01:19,  1.37it/s]Extractor Predicting: 117it [01:20,  1.44it/s]Extractor Predicting: 118it [01:22,  1.24s/it]Extractor Predicting: 119it [01:23,  1.07s/it]Extractor Predicting: 120it [01:24,  1.05it/s]Extractor Predicting: 121it [01:24,  1.14it/s]Extractor Predicting: 122it [01:25,  1.23it/s]Extractor Predicting: 123it [01:26,  1.29it/s]Extractor Predicting: 124it [01:26,  1.36it/s]Extractor Predicting: 125it [01:27,  1.39it/s]Extractor Predicting: 126it [01:28,  1.43it/s]Extractor Predicting: 127it [01:28,  1.45it/s]Extractor Predicting: 128it [01:29,  1.32it/s]Extractor Predicting: 129it [01:30,  1.39it/s]Extractor Predicting: 130it [01:31,  1.37it/s]Extractor Predicting: 131it [01:31,  1.45it/s]Extractor Predicting: 132it [01:32,  1.48it/s]Extractor Predicting: 133it [01:32,  1.50it/s]Extractor Predicting: 134it [01:33,  1.47it/s]Extractor Predicting: 135it [01:34,  1.46it/s]Extractor Predicting: 136it [01:35,  1.46it/s]Extractor Predicting: 137it [01:35,  1.45it/s]Extractor Predicting: 138it [01:36,  1.47it/s]Extractor Predicting: 139it [01:37,  1.47it/s]Extractor Predicting: 140it [01:37,  1.44it/s]Extractor Predicting: 141it [01:38,  1.50it/s]Extractor Predicting: 142it [01:39,  1.44it/s]Extractor Predicting: 143it [01:39,  1.39it/s]Extractor Predicting: 144it [01:40,  1.43it/s]Extractor Predicting: 145it [01:41,  1.45it/s]Extractor Predicting: 146it [01:42,  1.39it/s]Extractor Predicting: 147it [01:42,  1.40it/s]Extractor Predicting: 148it [01:43,  1.42it/s]Extractor Predicting: 149it [01:43,  1.52it/s]Extractor Predicting: 150it [01:44,  1.64it/s]Extractor Predicting: 151it [01:45,  1.63it/s]Extractor Predicting: 152it [01:45,  1.70it/s]Extractor Predicting: 153it [01:46,  1.79it/s]Extractor Predicting: 154it [01:46,  1.85it/s]Extractor Predicting: 155it [01:47,  1.88it/s]Extractor Predicting: 156it [01:47,  1.84it/s]Extractor Predicting: 157it [01:48,  1.88it/s]Extractor Predicting: 158it [01:48,  1.92it/s]Extractor Predicting: 159it [01:49,  1.96it/s]Extractor Predicting: 160it [01:49,  1.93it/s]Extractor Predicting: 161it [01:50,  1.89it/s]Extractor Predicting: 162it [01:50,  1.91it/s]Extractor Predicting: 163it [01:51,  1.83it/s]Extractor Predicting: 164it [01:51,  1.83it/s]Extractor Predicting: 165it [01:52,  1.88it/s]Extractor Predicting: 166it [01:52,  1.90it/s]Extractor Predicting: 167it [01:53,  1.88it/s]Extractor Predicting: 168it [01:54,  1.80it/s]Extractor Predicting: 169it [01:54,  1.69it/s]Extractor Predicting: 170it [01:55,  1.57it/s]Extractor Predicting: 171it [01:56,  1.56it/s]Extractor Predicting: 172it [01:56,  1.51it/s]Extractor Predicting: 173it [01:57,  1.47it/s]Extractor Predicting: 174it [01:58,  1.47it/s]Extractor Predicting: 175it [01:58,  1.51it/s]Extractor Predicting: 176it [01:59,  1.50it/s]Extractor Predicting: 177it [02:00,  1.52it/s]Extractor Predicting: 178it [02:00,  1.50it/s]Extractor Predicting: 179it [02:01,  1.55it/s]Extractor Predicting: 180it [02:02,  1.56it/s]Extractor Predicting: 181it [02:02,  1.57it/s]Extractor Predicting: 182it [02:03,  1.55it/s]Extractor Predicting: 183it [02:04,  1.50it/s]Extractor Predicting: 184it [02:04,  1.48it/s]Extractor Predicting: 185it [02:05,  1.51it/s]Extractor Predicting: 186it [02:06,  1.49it/s]Extractor Predicting: 187it [02:06,  1.48it/s]Extractor Predicting: 188it [02:07,  1.55it/s]Extractor Predicting: 189it [02:08,  1.51it/s]Extractor Predicting: 190it [02:08,  1.52it/s]Extractor Predicting: 191it [02:09,  1.53it/s]Extractor Predicting: 192it [02:10,  1.55it/s]Extractor Predicting: 193it [02:10,  1.53it/s]Extractor Predicting: 194it [02:11,  1.57it/s]Extractor Predicting: 195it [02:11,  1.54it/s]Extractor Predicting: 196it [02:12,  1.54it/s]Extractor Predicting: 197it [02:13,  1.55it/s]Extractor Predicting: 198it [02:13,  1.54it/s]Extractor Predicting: 199it [02:14,  1.53it/s]Extractor Predicting: 200it [02:15,  1.50it/s]Extractor Predicting: 201it [02:15,  1.51it/s]Extractor Predicting: 202it [02:16,  1.47it/s]Extractor Predicting: 203it [02:17,  1.43it/s]Extractor Predicting: 204it [02:18,  1.44it/s]Extractor Predicting: 205it [02:18,  1.45it/s]Extractor Predicting: 206it [02:19,  1.45it/s]Extractor Predicting: 207it [02:20,  1.46it/s]Extractor Predicting: 208it [02:20,  1.46it/s]Extractor Predicting: 209it [02:21,  1.47it/s]Extractor Predicting: 210it [02:22,  1.48it/s]Extractor Predicting: 211it [02:22,  1.49it/s]Extractor Predicting: 212it [02:23,  1.53it/s]Extractor Predicting: 213it [02:24,  1.48it/s]Extractor Predicting: 214it [02:24,  1.49it/s]Extractor Predicting: 215it [02:25,  1.50it/s]Extractor Predicting: 216it [02:26,  1.49it/s]Extractor Predicting: 217it [02:26,  1.49it/s]Extractor Predicting: 218it [02:27,  1.52it/s]Extractor Predicting: 219it [02:28,  1.42it/s]Extractor Predicting: 220it [02:28,  1.48it/s]Extractor Predicting: 221it [02:29,  1.46it/s]Extractor Predicting: 222it [02:30,  1.49it/s]Extractor Predicting: 223it [02:30,  1.54it/s]Extractor Predicting: 224it [02:31,  1.52it/s]Extractor Predicting: 225it [02:32,  1.56it/s]Extractor Predicting: 226it [02:32,  1.57it/s]Extractor Predicting: 227it [02:33,  1.59it/s]Extractor Predicting: 228it [02:34,  1.55it/s]Extractor Predicting: 229it [02:34,  1.54it/s]Extractor Predicting: 230it [02:35,  1.51it/s]Extractor Predicting: 231it [02:36,  1.51it/s]Extractor Predicting: 232it [02:36,  1.55it/s]Extractor Predicting: 233it [02:37,  1.53it/s]Extractor Predicting: 234it [02:37,  1.50it/s]Extractor Predicting: 235it [02:38,  1.51it/s]Extractor Predicting: 236it [02:39,  1.51it/s]Extractor Predicting: 237it [02:39,  1.59it/s]Extractor Predicting: 238it [02:40,  1.55it/s]Extractor Predicting: 239it [02:41,  1.55it/s]Extractor Predicting: 240it [02:41,  1.51it/s]Extractor Predicting: 241it [02:42,  1.51it/s]Extractor Predicting: 242it [02:43,  1.50it/s]Extractor Predicting: 243it [02:43,  1.54it/s]Extractor Predicting: 244it [02:44,  1.53it/s]Extractor Predicting: 245it [02:45,  1.51it/s]Extractor Predicting: 246it [02:45,  1.49it/s]Extractor Predicting: 247it [02:46,  1.30it/s]Extractor Predicting: 248it [02:47,  1.32it/s]Extractor Predicting: 249it [02:48,  1.36it/s]Extractor Predicting: 250it [02:48,  1.40it/s]Extractor Predicting: 251it [02:49,  1.46it/s]Extractor Predicting: 252it [02:50,  1.49it/s]Extractor Predicting: 253it [02:50,  1.47it/s]Extractor Predicting: 254it [02:51,  1.50it/s]Extractor Predicting: 255it [02:52,  1.49it/s]Extractor Predicting: 256it [02:52,  1.50it/s]Extractor Predicting: 257it [02:53,  1.50it/s]Extractor Predicting: 258it [02:54,  1.48it/s]Extractor Predicting: 259it [02:54,  1.50it/s]Extractor Predicting: 260it [02:55,  1.50it/s]Extractor Predicting: 261it [02:56,  1.51it/s]Extractor Predicting: 262it [02:56,  1.52it/s]Extractor Predicting: 263it [02:57,  1.50it/s]Extractor Predicting: 264it [02:58,  1.51it/s]Extractor Predicting: 265it [02:58,  1.50it/s]Extractor Predicting: 266it [02:59,  1.53it/s]Extractor Predicting: 267it [03:00,  1.52it/s]Extractor Predicting: 268it [03:00,  1.50it/s]Extractor Predicting: 269it [03:01,  1.49it/s]Extractor Predicting: 270it [03:02,  1.50it/s]Extractor Predicting: 271it [03:02,  1.51it/s]Extractor Predicting: 272it [03:03,  1.48it/s]Extractor Predicting: 273it [03:04,  1.46it/s]Extractor Predicting: 274it [03:04,  1.51it/s]Extractor Predicting: 275it [03:05,  1.52it/s]Extractor Predicting: 276it [03:06,  1.52it/s]Extractor Predicting: 277it [03:06,  1.49it/s]Extractor Predicting: 278it [03:07,  1.50it/s]Extractor Predicting: 279it [03:08,  1.48it/s]Extractor Predicting: 280it [03:08,  1.50it/s]Extractor Predicting: 281it [03:09,  1.44it/s]Extractor Predicting: 282it [03:10,  1.46it/s]Extractor Predicting: 283it [03:10,  1.45it/s]Extractor Predicting: 284it [03:11,  1.45it/s]Extractor Predicting: 285it [03:12,  1.42it/s]Extractor Predicting: 286it [03:13,  1.36it/s]Extractor Predicting: 287it [03:13,  1.38it/s]Extractor Predicting: 288it [03:14,  1.41it/s]Extractor Predicting: 289it [03:15,  1.41it/s]Extractor Predicting: 290it [03:15,  1.42it/s]Extractor Predicting: 291it [03:16,  1.42it/s]Extractor Predicting: 292it [03:17,  1.44it/s]Extractor Predicting: 293it [03:18,  1.44it/s]Extractor Predicting: 294it [03:18,  1.46it/s]Extractor Predicting: 295it [03:19,  1.48it/s]Extractor Predicting: 296it [03:20,  1.50it/s]Extractor Predicting: 297it [03:20,  1.51it/s]Extractor Predicting: 298it [03:21,  1.52it/s]Extractor Predicting: 299it [03:21,  1.52it/s]Extractor Predicting: 300it [03:22,  1.56it/s]Extractor Predicting: 301it [03:23,  1.56it/s]Extractor Predicting: 302it [03:23,  1.52it/s]Extractor Predicting: 303it [03:24,  1.49it/s]Extractor Predicting: 304it [03:25,  1.51it/s]Extractor Predicting: 305it [03:25,  1.52it/s]Extractor Predicting: 306it [03:26,  1.51it/s]Extractor Predicting: 307it [03:27,  1.52it/s]Extractor Predicting: 308it [03:27,  1.51it/s]Extractor Predicting: 309it [03:28,  1.50it/s]Extractor Predicting: 310it [03:29,  1.50it/s]Extractor Predicting: 311it [03:29,  1.49it/s]Extractor Predicting: 312it [03:30,  1.50it/s]Extractor Predicting: 313it [03:31,  1.47it/s]Extractor Predicting: 314it [03:31,  1.48it/s]Extractor Predicting: 315it [03:32,  1.51it/s]Extractor Predicting: 316it [03:33,  1.49it/s]Extractor Predicting: 317it [03:33,  1.47it/s]Extractor Predicting: 318it [03:34,  1.50it/s]Extractor Predicting: 319it [03:35,  1.45it/s]Extractor Predicting: 320it [03:36,  1.47it/s]Extractor Predicting: 321it [03:36,  1.48it/s]Extractor Predicting: 322it [03:37,  1.50it/s]Extractor Predicting: 323it [03:37,  1.50it/s]Extractor Predicting: 324it [03:38,  1.51it/s]Extractor Predicting: 325it [03:39,  1.54it/s]Extractor Predicting: 326it [03:39,  1.52it/s]Extractor Predicting: 327it [03:40,  1.54it/s]Extractor Predicting: 328it [03:41,  1.51it/s]Extractor Predicting: 329it [03:41,  1.52it/s]Extractor Predicting: 330it [03:42,  1.54it/s]Extractor Predicting: 331it [03:43,  1.51it/s]Extractor Predicting: 332it [03:43,  1.50it/s]Extractor Predicting: 333it [03:44,  1.47it/s]Extractor Predicting: 334it [03:45,  1.44it/s]Extractor Predicting: 335it [03:46,  1.46it/s]Extractor Predicting: 336it [03:46,  1.48it/s]Extractor Predicting: 337it [03:47,  1.47it/s]Extractor Predicting: 338it [03:48,  1.46it/s]Extractor Predicting: 339it [03:48,  1.48it/s]Extractor Predicting: 340it [03:49,  1.53it/s]Extractor Predicting: 341it [03:49,  1.50it/s]Extractor Predicting: 342it [03:50,  1.52it/s]Extractor Predicting: 343it [03:51,  1.54it/s]Extractor Predicting: 344it [03:51,  1.58it/s]Extractor Predicting: 345it [03:52,  1.60it/s]Extractor Predicting: 346it [03:53,  1.65it/s]Extractor Predicting: 347it [03:53,  1.68it/s]Extractor Predicting: 348it [03:54,  1.66it/s]Extractor Predicting: 349it [03:54,  1.66it/s]Extractor Predicting: 350it [03:55,  1.62it/s]Extractor Predicting: 351it [03:56,  1.65it/s]Extractor Predicting: 352it [03:56,  1.61it/s]Extractor Predicting: 353it [03:57,  1.59it/s]Extractor Predicting: 354it [03:58,  1.52it/s]Extractor Predicting: 355it [03:58,  1.55it/s]Extractor Predicting: 356it [03:59,  1.53it/s]Extractor Predicting: 357it [04:00,  1.53it/s]Extractor Predicting: 358it [04:00,  1.51it/s]Extractor Predicting: 359it [04:01,  1.49it/s]Extractor Predicting: 360it [04:02,  1.49it/s]Extractor Predicting: 361it [04:02,  1.49it/s]Extractor Predicting: 362it [04:03,  1.50it/s]Extractor Predicting: 363it [04:04,  1.34it/s]Extractor Predicting: 364it [04:04,  1.41it/s]Extractor Predicting: 365it [04:05,  1.42it/s]Extractor Predicting: 366it [04:06,  1.43it/s]Extractor Predicting: 367it [04:07,  1.45it/s]Extractor Predicting: 368it [04:07,  1.49it/s]Extractor Predicting: 369it [04:08,  1.48it/s]Extractor Predicting: 370it [04:08,  1.51it/s]Extractor Predicting: 371it [04:09,  1.50it/s]Extractor Predicting: 372it [04:10,  1.48it/s]Extractor Predicting: 373it [04:10,  1.48it/s]Extractor Predicting: 374it [04:11,  1.51it/s]Extractor Predicting: 375it [04:12,  1.53it/s]Extractor Predicting: 376it [04:12,  1.57it/s]Extractor Predicting: 377it [04:13,  1.62it/s]Extractor Predicting: 378it [04:14,  1.60it/s]Extractor Predicting: 379it [04:14,  1.62it/s]Extractor Predicting: 380it [04:15,  1.62it/s]Extractor Predicting: 381it [04:15,  1.62it/s]Extractor Predicting: 382it [04:16,  1.61it/s]Extractor Predicting: 383it [04:17,  1.60it/s]Extractor Predicting: 384it [04:17,  1.59it/s]Extractor Predicting: 385it [04:18,  1.61it/s]Extractor Predicting: 386it [04:19,  1.63it/s]Extractor Predicting: 387it [04:19,  1.63it/s]Extractor Predicting: 388it [04:20,  1.63it/s]Extractor Predicting: 389it [04:20,  1.64it/s]Extractor Predicting: 390it [04:21,  1.63it/s]Extractor Predicting: 391it [04:22,  1.61it/s]Extractor Predicting: 392it [04:22,  1.66it/s]Extractor Predicting: 393it [04:23,  1.64it/s]Extractor Predicting: 394it [04:23,  1.66it/s]Extractor Predicting: 395it [04:24,  1.63it/s]Extractor Predicting: 396it [04:25,  1.61it/s]Extractor Predicting: 397it [04:25,  1.59it/s]Extractor Predicting: 398it [04:26,  1.56it/s]Extractor Predicting: 399it [04:27,  1.53it/s]Extractor Predicting: 400it [04:27,  1.51it/s]Extractor Predicting: 401it [04:28,  1.51it/s]Extractor Predicting: 402it [04:29,  1.52it/s]Extractor Predicting: 403it [04:29,  1.55it/s]Extractor Predicting: 404it [04:30,  1.49it/s]Extractor Predicting: 405it [04:31,  1.49it/s]Extractor Predicting: 406it [04:31,  1.50it/s]Extractor Predicting: 407it [04:32,  1.50it/s]Extractor Predicting: 408it [04:33,  1.51it/s]Extractor Predicting: 409it [04:33,  1.48it/s]Extractor Predicting: 410it [04:34,  1.48it/s]Extractor Predicting: 411it [04:35,  1.49it/s]Extractor Predicting: 412it [04:35,  1.52it/s]Extractor Predicting: 413it [04:36,  1.50it/s]Extractor Predicting: 414it [04:37,  1.49it/s]Extractor Predicting: 415it [04:37,  1.50it/s]Extractor Predicting: 416it [04:38,  1.50it/s]Extractor Predicting: 417it [04:39,  1.51it/s]Extractor Predicting: 418it [04:39,  1.53it/s]Extractor Predicting: 419it [04:40,  1.53it/s]Extractor Predicting: 420it [04:41,  1.52it/s]Extractor Predicting: 421it [04:41,  1.53it/s]Extractor Predicting: 422it [04:42,  1.50it/s]Extractor Predicting: 423it [04:43,  1.49it/s]Extractor Predicting: 424it [04:43,  1.51it/s]Extractor Predicting: 425it [04:44,  1.50it/s]Extractor Predicting: 426it [04:45,  1.55it/s]Extractor Predicting: 427it [04:45,  1.59it/s]Extractor Predicting: 428it [04:46,  1.59it/s]Extractor Predicting: 429it [04:46,  1.62it/s]Extractor Predicting: 430it [04:47,  1.62it/s]Extractor Predicting: 431it [04:48,  1.67it/s]Extractor Predicting: 432it [04:48,  1.63it/s]Extractor Predicting: 433it [04:49,  1.60it/s]Extractor Predicting: 434it [04:49,  1.62it/s]Extractor Predicting: 435it [04:50,  1.60it/s]Extractor Predicting: 436it [04:51,  1.58it/s]Extractor Predicting: 437it [04:51,  1.63it/s]Extractor Predicting: 438it [04:52,  1.63it/s]Extractor Predicting: 439it [04:53,  1.63it/s]Extractor Predicting: 440it [04:53,  1.59it/s]Extractor Predicting: 441it [04:54,  1.60it/s]Extractor Predicting: 442it [04:54,  1.62it/s]Extractor Predicting: 443it [04:55,  1.59it/s]Extractor Predicting: 444it [04:56,  1.61it/s]Extractor Predicting: 445it [04:56,  1.62it/s]Extractor Predicting: 446it [04:57,  1.65it/s]Extractor Predicting: 447it [04:57,  1.65it/s]Extractor Predicting: 448it [04:58,  1.66it/s]Extractor Predicting: 449it [04:59,  1.66it/s]Extractor Predicting: 450it [04:59,  1.64it/s]Extractor Predicting: 451it [05:00,  1.64it/s]Extractor Predicting: 452it [05:01,  1.60it/s]Extractor Predicting: 453it [05:01,  1.58it/s]Extractor Predicting: 454it [05:02,  1.52it/s]Extractor Predicting: 455it [05:03,  1.55it/s]Extractor Predicting: 456it [05:03,  1.56it/s]Extractor Predicting: 457it [05:04,  1.57it/s]Extractor Predicting: 458it [05:04,  1.58it/s]Extractor Predicting: 459it [05:05,  1.56it/s]Extractor Predicting: 460it [05:06,  1.51it/s]Extractor Predicting: 461it [05:06,  1.51it/s]Extractor Predicting: 462it [05:07,  1.47it/s]Extractor Predicting: 463it [05:08,  1.46it/s]Extractor Predicting: 464it [05:08,  1.50it/s]Extractor Predicting: 465it [05:09,  1.51it/s]Extractor Predicting: 466it [05:10,  1.51it/s]Extractor Predicting: 467it [05:10,  1.48it/s]Extractor Predicting: 468it [05:11,  1.49it/s]Extractor Predicting: 469it [05:12,  1.51it/s]Extractor Predicting: 470it [05:12,  1.48it/s]Extractor Predicting: 471it [05:13,  1.43it/s]Extractor Predicting: 472it [05:14,  1.43it/s]Extractor Predicting: 473it [05:15,  1.47it/s]Extractor Predicting: 474it [05:15,  1.51it/s]Extractor Predicting: 475it [05:16,  1.52it/s]Extractor Predicting: 476it [05:17,  1.51it/s]Extractor Predicting: 477it [05:17,  1.47it/s]Extractor Predicting: 478it [05:18,  1.45it/s]Extractor Predicting: 479it [05:19,  1.44it/s]Extractor Predicting: 480it [05:19,  1.44it/s]Extractor Predicting: 481it [05:20,  1.46it/s]Extractor Predicting: 482it [05:21,  1.45it/s]Extractor Predicting: 483it [05:21,  1.48it/s]Extractor Predicting: 484it [05:22,  1.47it/s]Extractor Predicting: 485it [05:23,  1.52it/s]Extractor Predicting: 486it [05:23,  1.54it/s]Extractor Predicting: 487it [05:24,  1.52it/s]Extractor Predicting: 488it [05:25,  1.49it/s]Extractor Predicting: 489it [05:25,  1.48it/s]Extractor Predicting: 490it [05:26,  1.48it/s]Extractor Predicting: 491it [05:27,  1.31it/s]Extractor Predicting: 492it [05:28,  1.34it/s]Extractor Predicting: 493it [05:28,  1.38it/s]Extractor Predicting: 494it [05:29,  1.42it/s]Extractor Predicting: 495it [05:30,  1.49it/s]Extractor Predicting: 496it [05:30,  1.50it/s]Extractor Predicting: 496it [05:30,  1.50it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{
  "path_pred": "outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor', 'path_test': 'zero_rte/wiki/unseen_15_seed_0/test.jsonl', 'labels': ['cast member', 'follows', 'has quality', 'instrument', 'league', 'located in or next to body of water', 'located on astronomical body', 'member of', 'member of political party', 'mother', 'opposite of', 'residence', 'shares border with', 'subsidiary', 'twinned administrative body'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 10529
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10629, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.59it/s]Extractor Predicting: 2it [00:01,  1.50it/s]Extractor Predicting: 3it [00:01,  1.53it/s]Extractor Predicting: 4it [00:02,  1.52it/s]Extractor Predicting: 5it [00:03,  1.48it/s]Extractor Predicting: 6it [00:04,  1.45it/s]Extractor Predicting: 7it [00:04,  1.45it/s]Extractor Predicting: 8it [00:05,  1.45it/s]Extractor Predicting: 9it [00:06,  1.45it/s]Extractor Predicting: 10it [00:06,  1.44it/s]Extractor Predicting: 11it [00:07,  1.44it/s]Extractor Predicting: 12it [00:08,  1.43it/s]Extractor Predicting: 13it [00:08,  1.45it/s]Extractor Predicting: 14it [00:09,  1.46it/s]Extractor Predicting: 15it [00:10,  1.43it/s]Extractor Predicting: 16it [00:10,  1.44it/s]Extractor Predicting: 17it [00:11,  1.41it/s]Extractor Predicting: 18it [00:12,  1.39it/s]Extractor Predicting: 19it [00:13,  1.36it/s]Extractor Predicting: 20it [00:14,  1.29it/s]Extractor Predicting: 21it [00:14,  1.31it/s]Extractor Predicting: 22it [00:15,  1.27it/s]Extractor Predicting: 23it [00:16,  1.28it/s]Extractor Predicting: 24it [00:17,  1.32it/s]Extractor Predicting: 25it [00:17,  1.38it/s]Extractor Predicting: 26it [00:18,  1.38it/s]Extractor Predicting: 27it [00:19,  1.38it/s]Extractor Predicting: 28it [00:19,  1.38it/s]Extractor Predicting: 29it [00:20,  1.37it/s]Extractor Predicting: 30it [00:21,  1.38it/s]Extractor Predicting: 31it [00:22,  1.38it/s]Extractor Predicting: 32it [00:22,  1.40it/s]Extractor Predicting: 33it [00:23,  1.44it/s]Extractor Predicting: 34it [00:24,  1.47it/s]Extractor Predicting: 35it [00:24,  1.45it/s]Extractor Predicting: 36it [00:25,  1.45it/s]Extractor Predicting: 37it [00:26,  1.46it/s]Extractor Predicting: 38it [00:26,  1.45it/s]Extractor Predicting: 39it [00:27,  1.47it/s]Extractor Predicting: 40it [00:28,  1.49it/s]Extractor Predicting: 41it [00:28,  1.48it/s]Extractor Predicting: 42it [00:29,  1.49it/s]Extractor Predicting: 43it [00:30,  1.49it/s]Extractor Predicting: 44it [00:30,  1.50it/s]Extractor Predicting: 45it [00:31,  1.50it/s]Extractor Predicting: 46it [00:32,  1.45it/s]Extractor Predicting: 47it [00:33,  1.45it/s]Extractor Predicting: 48it [00:33,  1.46it/s]Extractor Predicting: 49it [00:34,  1.48it/s]Extractor Predicting: 50it [00:35,  1.48it/s]Extractor Predicting: 51it [00:35,  1.47it/s]Extractor Predicting: 52it [00:36,  1.46it/s]Extractor Predicting: 53it [00:37,  1.48it/s]Extractor Predicting: 54it [00:37,  1.46it/s]Extractor Predicting: 55it [00:38,  1.49it/s]Extractor Predicting: 56it [00:39,  1.50it/s]Extractor Predicting: 57it [00:39,  1.53it/s]Extractor Predicting: 58it [00:40,  1.53it/s]Extractor Predicting: 59it [00:41,  1.52it/s]Extractor Predicting: 60it [00:41,  1.44it/s]Extractor Predicting: 61it [00:42,  1.47it/s]Extractor Predicting: 62it [00:43,  1.47it/s]Extractor Predicting: 63it [00:43,  1.44it/s]Extractor Predicting: 64it [00:44,  1.44it/s]Extractor Predicting: 65it [00:45,  1.42it/s]Extractor Predicting: 66it [00:45,  1.48it/s]Extractor Predicting: 67it [00:46,  1.56it/s]Extractor Predicting: 68it [00:46,  1.63it/s]Extractor Predicting: 69it [00:47,  1.71it/s]Extractor Predicting: 70it [00:48,  1.77it/s]Extractor Predicting: 71it [00:48,  1.81it/s]Extractor Predicting: 72it [00:49,  1.81it/s]Extractor Predicting: 73it [00:49,  1.82it/s]Extractor Predicting: 74it [00:50,  1.81it/s]Extractor Predicting: 75it [00:50,  1.79it/s]Extractor Predicting: 76it [00:51,  1.79it/s]Extractor Predicting: 77it [00:51,  1.78it/s]Extractor Predicting: 78it [00:52,  1.80it/s]Extractor Predicting: 79it [00:52,  1.84it/s]Extractor Predicting: 80it [00:53,  1.80it/s]Extractor Predicting: 81it [00:54,  1.80it/s]Extractor Predicting: 82it [00:54,  1.82it/s]Extractor Predicting: 83it [00:55,  1.85it/s]Extractor Predicting: 84it [00:55,  1.87it/s]Extractor Predicting: 85it [00:56,  1.86it/s]Extractor Predicting: 86it [00:56,  1.84it/s]Extractor Predicting: 87it [00:57,  1.89it/s]Extractor Predicting: 88it [00:57,  1.83it/s]Extractor Predicting: 89it [00:58,  1.82it/s]Extractor Predicting: 90it [00:58,  1.80it/s]Extractor Predicting: 91it [00:59,  1.63it/s]Extractor Predicting: 92it [01:00,  1.70it/s]Extractor Predicting: 93it [01:00,  1.75it/s]Extractor Predicting: 94it [01:01,  1.78it/s]Extractor Predicting: 95it [01:01,  1.80it/s]Extractor Predicting: 96it [01:02,  1.68it/s]Extractor Predicting: 97it [01:03,  1.62it/s]Extractor Predicting: 98it [01:03,  1.57it/s]Extractor Predicting: 99it [01:04,  1.50it/s]Extractor Predicting: 100it [01:05,  1.46it/s]Extractor Predicting: 101it [01:06,  1.44it/s]Extractor Predicting: 102it [01:06,  1.42it/s]Extractor Predicting: 103it [01:07,  1.42it/s]Extractor Predicting: 104it [01:08,  1.42it/s]Extractor Predicting: 105it [01:08,  1.41it/s]Extractor Predicting: 106it [01:09,  1.41it/s]Extractor Predicting: 107it [01:10,  1.40it/s]Extractor Predicting: 108it [01:11,  1.39it/s]Extractor Predicting: 109it [01:11,  1.40it/s]Extractor Predicting: 110it [01:12,  1.39it/s]Extractor Predicting: 111it [01:13,  1.39it/s]Extractor Predicting: 112it [01:13,  1.41it/s]Extractor Predicting: 113it [01:14,  1.45it/s]Extractor Predicting: 114it [01:15,  1.50it/s]Extractor Predicting: 115it [01:15,  1.51it/s]Extractor Predicting: 116it [01:16,  1.54it/s]Extractor Predicting: 117it [01:17,  1.54it/s]Extractor Predicting: 118it [01:17,  1.55it/s]Extractor Predicting: 119it [01:18,  1.53it/s]Extractor Predicting: 120it [01:19,  1.57it/s]Extractor Predicting: 121it [01:19,  1.57it/s]Extractor Predicting: 122it [01:20,  1.58it/s]Extractor Predicting: 123it [01:20,  1.57it/s]Extractor Predicting: 124it [01:21,  1.51it/s]Extractor Predicting: 125it [01:22,  1.52it/s]Extractor Predicting: 126it [01:22,  1.52it/s]Extractor Predicting: 127it [01:23,  1.50it/s]Extractor Predicting: 128it [01:24,  1.47it/s]Extractor Predicting: 129it [01:25,  1.48it/s]Extractor Predicting: 130it [01:25,  1.48it/s]Extractor Predicting: 131it [01:26,  1.47it/s]Extractor Predicting: 132it [01:27,  1.43it/s]Extractor Predicting: 133it [01:27,  1.49it/s]Extractor Predicting: 133it [01:27,  1.52it/s]
{
  "path_pred": "outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/wiki_filtered_large/unseen_15_seed_0/extractor/results_multi_is_eval_False.json"
}
