Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_4', 'type': 'synthetic', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_5_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:57<08:34, 57.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [01:12<04:22, 32.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:32<03:05, 26.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:48<02:16, 22.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [02:06<01:43, 20.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:21<01:15, 18.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:35<00:52, 17.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:52<00:34, 17.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [03:08<00:16, 16.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:30<00:00, 18.37s/it]Generating: 100%|██████████| 10/10 [03:30<00:00, 21.06s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : director .', 'success_rate': 0.8138020833333334, 'errors': {'', "('\\n', 'director', 'Scott Zandt', 'It was written and directed by Scott Zandt ( a.k.a .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8505434782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 359, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 499, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7439903846153846, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : part of .', 'success_rate': 0.8020833333333334, 'errors': {'', '(\'1958 contest\', \'part of\', \'\', \'He was succeeded as Dutch representative at its 1958 contest by Johannes Eichhorn with " Allende en seine " .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a deal to turn " Ingress " into a mobile game . Head Entity : Ingress , Tail Entity : Ingress Studio .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.7981770833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8764204545454546, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n']
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n"]
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n", "Relation : member of political party . Context : This was the first coalition government which was elected in 1998 , and led by then - Prime Minister Naguib Sawiris of the People 's Alliance . Head Entity : Naguib Sawiris , Tail Entity : People ' Alliance .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8059895833333334, 'errors': {'', '(\'Tom Blomkamp\', \'member of political party\', \'\', \'" My Life ( " ; ) is a 2015 English language English language documentary film directed by Tom Blomkamp and starring Emma Thompson and Tom Hardy .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 58, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 96, 'raw': 160}
{'target': 600, 'success': 114, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 154, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 223, 'raw': 384}
{'target': 600, 'success': 245, 'raw': 416}
{'target': 600, 'success': 263, 'raw': 448}
{'target': 600, 'success': 276, 'raw': 480}
{'target': 600, 'success': 297, 'raw': 512}
{'target': 600, 'success': 318, 'raw': 544}
{'target': 600, 'success': 335, 'raw': 576}
{'target': 600, 'success': 359, 'raw': 608}
{'target': 600, 'success': 381, 'raw': 640}
{'target': 600, 'success': 398, 'raw': 672}
{'target': 600, 'success': 410, 'raw': 704}
{'target': 600, 'success': 431, 'raw': 736}
{'target': 600, 'success': 451, 'raw': 768}
{'target': 600, 'success': 470, 'raw': 800}
{'target': 600, 'success': 494, 'raw': 832}
{'target': 600, 'success': 516, 'raw': 864}
{'target': 600, 'success': 533, 'raw': 896}
{'target': 600, 'success': 552, 'raw': 928}
{'target': 600, 'success': 566, 'raw': 960}
{'target': 600, 'success': 584, 'raw': 992}
{'target': 600, 'success': 599, 'raw': 1024}
{'target': 600, 'success': 621, 'raw': 1056}
{'prompt': 'Relation : position held .', 'success_rate': 0.5880681818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 12256
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12356, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_synthetic_large/unseen_5_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:29, 29.85s/it]Extractor Estimating: 2it [00:31, 13.21s/it]Extractor Estimating: 3it [00:32,  7.49s/it]Extractor Estimating: 4it [00:32,  4.81s/it]Extractor Estimating: 5it [00:33,  3.30s/it]Extractor Estimating: 6it [00:34,  2.41s/it]Extractor Estimating: 7it [00:34,  1.86s/it]Extractor Estimating: 8it [00:36,  1.66s/it]Extractor Estimating: 9it [00:36,  1.35s/it]Extractor Estimating: 10it [00:37,  1.17s/it]Extractor Estimating: 11it [00:38,  1.00it/s]Extractor Estimating: 12it [00:38,  1.12it/s]Extractor Estimating: 13it [00:39,  1.22it/s]Extractor Estimating: 14it [00:40,  1.25it/s]Extractor Estimating: 15it [00:40,  1.28it/s]Extractor Estimating: 16it [00:41,  1.33it/s]Extractor Estimating: 17it [00:42,  1.37it/s]Extractor Estimating: 18it [00:42,  1.43it/s]Extractor Estimating: 19it [00:44,  1.16it/s]Extractor Estimating: 20it [00:45,  1.15it/s]Extractor Estimating: 21it [00:45,  1.22it/s]Extractor Estimating: 22it [00:46,  1.31it/s]Extractor Estimating: 23it [00:47,  1.36it/s]Extractor Estimating: 24it [00:47,  1.36it/s]Extractor Estimating: 25it [00:48,  1.30it/s]Extractor Estimating: 26it [00:49,  1.38it/s]Extractor Estimating: 27it [00:49,  1.44it/s]Extractor Estimating: 28it [00:50,  1.53it/s]Extractor Estimating: 29it [00:51,  1.54it/s]Extractor Estimating: 30it [00:51,  1.50it/s]Extractor Estimating: 31it [00:52,  1.55it/s]Extractor Estimating: 32it [00:52,  1.57it/s]Extractor Estimating: 33it [00:53,  1.56it/s]Extractor Estimating: 34it [00:54,  1.63it/s]Extractor Estimating: 35it [00:54,  1.68it/s]Extractor Estimating: 36it [00:55,  1.66it/s]Extractor Estimating: 37it [00:56,  1.54it/s]Extractor Estimating: 38it [00:56,  1.56it/s]Extractor Estimating: 39it [00:57,  1.54it/s]Extractor Estimating: 40it [00:57,  1.61it/s]Extractor Estimating: 41it [00:58,  1.58it/s]Extractor Estimating: 42it [00:59,  1.53it/s]Extractor Estimating: 43it [00:59,  1.58it/s]Extractor Estimating: 44it [01:00,  1.60it/s]Extractor Estimating: 45it [01:01,  1.61it/s]Extractor Estimating: 46it [01:01,  1.66it/s]Extractor Estimating: 47it [01:02,  1.54it/s]Extractor Estimating: 48it [01:03,  1.51it/s]Extractor Estimating: 49it [01:03,  1.58it/s]Extractor Estimating: 50it [01:04,  1.67it/s]Extractor Estimating: 51it [01:04,  1.60it/s]Extractor Estimating: 52it [01:05,  1.53it/s]Extractor Estimating: 53it [01:06,  1.58it/s]Extractor Estimating: 54it [01:06,  1.62it/s]Extractor Estimating: 55it [01:07,  1.60it/s]Extractor Estimating: 56it [01:08,  1.59it/s]Extractor Estimating: 57it [01:08,  1.54it/s]Extractor Estimating: 58it [01:09,  1.61it/s]Extractor Estimating: 59it [01:09,  1.58it/s]Extractor Estimating: 60it [01:10,  1.62it/s]Extractor Estimating: 61it [01:12,  1.15s/it]Extractor Estimating: 62it [01:13,  1.02s/it]Extractor Estimating: 63it [01:14,  1.12it/s]Extractor Estimating: 64it [01:14,  1.26it/s]Extractor Estimating: 65it [01:15,  1.29it/s]Extractor Estimating: 66it [01:16,  1.34it/s]Extractor Estimating: 67it [01:16,  1.37it/s]Extractor Estimating: 68it [01:17,  1.42it/s]Extractor Estimating: 69it [01:18,  1.46it/s]Extractor Estimating: 70it [01:18,  1.52it/s]Extractor Estimating: 71it [01:19,  1.54it/s]Extractor Estimating: 72it [01:20,  1.48it/s]Extractor Estimating: 73it [01:20,  1.48it/s]Extractor Estimating: 74it [01:21,  1.54it/s]Extractor Estimating: 75it [01:22,  1.45it/s]Extractor Estimating: 76it [01:22,  1.45it/s]Extractor Estimating: 77it [01:23,  1.45it/s]Extractor Estimating: 78it [01:24,  1.47it/s]Extractor Estimating: 79it [01:24,  1.49it/s]Extractor Estimating: 80it [01:25,  1.55it/s]Extractor Estimating: 81it [01:26,  1.52it/s]Extractor Estimating: 82it [01:26,  1.48it/s]Extractor Estimating: 83it [01:27,  1.47it/s]Extractor Estimating: 84it [01:28,  1.49it/s]Extractor Estimating: 85it [01:28,  1.47it/s]Extractor Estimating: 86it [01:29,  1.51it/s]Extractor Estimating: 87it [01:30,  1.50it/s]Extractor Estimating: 88it [01:31,  1.45it/s]Extractor Estimating: 89it [01:31,  1.47it/s]Extractor Estimating: 90it [01:32,  1.49it/s]Extractor Estimating: 91it [01:32,  1.49it/s]Extractor Estimating: 92it [01:33,  1.42it/s]Extractor Estimating: 93it [01:34,  1.46it/s]Extractor Estimating: 94it [01:35,  1.47it/s]Extractor Estimating: 95it [01:35,  1.55it/s]Extractor Estimating: 96it [01:36,  1.51it/s]Extractor Estimating: 97it [01:37,  1.44it/s]Extractor Estimating: 98it [01:37,  1.46it/s]Extractor Estimating: 99it [01:38,  1.50it/s]Extractor Estimating: 100it [01:39,  1.52it/s]Extractor Estimating: 101it [01:39,  1.55it/s]Extractor Estimating: 102it [01:40,  1.51it/s]Extractor Estimating: 103it [01:41,  1.50it/s]Extractor Estimating: 104it [01:41,  1.51it/s]Extractor Estimating: 105it [01:42,  1.53it/s]Extractor Estimating: 106it [01:42,  1.54it/s]Extractor Estimating: 107it [01:43,  1.46it/s]Extractor Estimating: 108it [01:44,  1.46it/s]Extractor Estimating: 109it [01:45,  1.49it/s]Extractor Estimating: 110it [01:45,  1.53it/s]Extractor Estimating: 111it [01:46,  1.54it/s]Extractor Estimating: 112it [01:47,  1.46it/s]Extractor Estimating: 113it [01:47,  1.49it/s]Extractor Estimating: 114it [01:48,  1.50it/s]Extractor Estimating: 115it [01:48,  1.55it/s]Extractor Estimating: 116it [01:49,  1.54it/s]Extractor Estimating: 117it [01:50,  1.48it/s]Extractor Estimating: 118it [01:51,  1.45it/s]Extractor Estimating: 119it [01:51,  1.51it/s]Extractor Estimating: 120it [01:52,  1.55it/s]Extractor Estimating: 121it [01:53,  1.43it/s]Extractor Estimating: 122it [01:53,  1.42it/s]Extractor Estimating: 123it [01:54,  1.47it/s]Extractor Estimating: 124it [01:55,  1.49it/s]Extractor Estimating: 125it [01:55,  1.55it/s]Extractor Estimating: 126it [01:56,  1.52it/s]Extractor Estimating: 127it [01:57,  1.50it/s]Extractor Estimating: 128it [01:57,  1.45it/s]Extractor Estimating: 129it [01:58,  1.52it/s]Extractor Estimating: 130it [01:58,  1.59it/s]Extractor Estimating: 131it [01:59,  1.63it/s]Extractor Estimating: 132it [02:00,  1.57it/s]Extractor Estimating: 133it [02:00,  1.53it/s]Extractor Estimating: 134it [02:01,  1.50it/s]Extractor Estimating: 135it [02:02,  1.54it/s]Extractor Estimating: 136it [02:02,  1.55it/s]Extractor Estimating: 137it [02:03,  1.59it/s]Extractor Estimating: 138it [02:04,  1.52it/s]Extractor Estimating: 139it [02:04,  1.55it/s]Extractor Estimating: 140it [02:05,  1.52it/s]Extractor Estimating: 141it [02:06,  1.58it/s]Extractor Estimating: 142it [02:06,  1.55it/s]Extractor Estimating: 143it [02:07,  1.49it/s]Extractor Estimating: 144it [02:08,  1.54it/s]Extractor Estimating: 145it [02:08,  1.57it/s]Extractor Estimating: 146it [02:09,  1.55it/s]Extractor Estimating: 147it [02:09,  1.58it/s]Extractor Estimating: 148it [02:10,  1.50it/s]Extractor Estimating: 149it [02:11,  1.54it/s]Extractor Estimating: 150it [02:11,  1.56it/s]Extractor Estimating: 151it [02:12,  1.62it/s]Extractor Estimating: 152it [02:12,  1.67it/s]Extractor Estimating: 153it [02:13,  1.62it/s]Extractor Estimating: 154it [02:14,  1.69it/s]Extractor Estimating: 155it [02:14,  1.73it/s]Extractor Estimating: 156it [02:15,  1.77it/s]Extractor Estimating: 157it [02:15,  1.78it/s]Extractor Estimating: 158it [02:16,  1.76it/s]Extractor Estimating: 159it [02:17,  1.72it/s]Extractor Estimating: 160it [02:17,  1.75it/s]Extractor Estimating: 161it [02:18,  1.83it/s]Extractor Estimating: 162it [02:18,  1.89it/s]Extractor Estimating: 163it [02:19,  1.84it/s]Extractor Estimating: 164it [02:19,  1.85it/s]Extractor Estimating: 165it [02:20,  1.77it/s]Extractor Estimating: 166it [02:20,  1.76it/s]Extractor Estimating: 167it [02:21,  1.85it/s]Extractor Estimating: 168it [02:21,  1.82it/s]Extractor Estimating: 169it [02:22,  1.84it/s]Extractor Estimating: 170it [02:23,  1.79it/s]Extractor Estimating: 171it [02:23,  1.70it/s]Extractor Estimating: 172it [02:24,  1.75it/s]Extractor Estimating: 173it [02:24,  1.79it/s]Extractor Estimating: 174it [02:25,  1.81it/s]Extractor Estimating: 175it [02:25,  1.83it/s]Extractor Estimating: 176it [02:26,  1.70it/s]Extractor Estimating: 177it [02:27,  1.67it/s]Extractor Estimating: 178it [02:27,  1.69it/s]Extractor Estimating: 179it [02:28,  1.57it/s]Extractor Estimating: 180it [02:29,  1.62it/s]Extractor Estimating: 181it [02:29,  1.64it/s]Extractor Estimating: 182it [02:30,  1.67it/s]Extractor Estimating: 183it [02:30,  1.60it/s]Extractor Estimating: 184it [02:31,  1.55it/s]Extractor Estimating: 185it [02:32,  1.57it/s]Extractor Estimating: 186it [02:32,  1.51it/s]Extractor Estimating: 187it [02:33,  1.55it/s]Extractor Estimating: 188it [02:34,  1.59it/s]Extractor Estimating: 189it [02:34,  1.52it/s]Extractor Estimating: 190it [02:35,  1.54it/s]Extractor Estimating: 191it [02:36,  1.54it/s]Extractor Estimating: 192it [02:36,  1.54it/s]Extractor Estimating: 193it [02:37,  1.59it/s]Extractor Estimating: 194it [02:38,  1.54it/s]Extractor Estimating: 195it [02:38,  1.59it/s]Extractor Estimating: 196it [02:39,  1.50it/s]Extractor Estimating: 197it [02:39,  1.57it/s]Extractor Estimating: 198it [02:40,  1.53it/s]Extractor Estimating: 199it [02:41,  1.51it/s]Extractor Estimating: 200it [02:41,  1.55it/s]Extractor Estimating: 201it [02:42,  1.57it/s]Extractor Estimating: 202it [02:43,  1.58it/s]Extractor Estimating: 203it [02:43,  1.56it/s]Extractor Estimating: 204it [02:44,  1.52it/s]Extractor Estimating: 205it [02:45,  1.52it/s]Extractor Estimating: 206it [02:45,  1.53it/s]Extractor Estimating: 207it [02:46,  1.54it/s]Extractor Estimating: 208it [02:47,  1.52it/s]Extractor Estimating: 209it [02:47,  1.49it/s]Extractor Estimating: 210it [02:48,  1.58it/s]Extractor Estimating: 211it [02:49,  1.54it/s]Extractor Estimating: 212it [02:49,  1.56it/s]Extractor Estimating: 213it [02:50,  1.57it/s]Extractor Estimating: 214it [02:51,  1.49it/s]Extractor Estimating: 215it [02:51,  1.55it/s]Extractor Estimating: 216it [02:52,  1.55it/s]Extractor Estimating: 217it [02:52,  1.58it/s]Extractor Estimating: 218it [02:53,  1.57it/s]Extractor Estimating: 219it [02:54,  1.46it/s]Extractor Estimating: 220it [02:55,  1.47it/s]Extractor Estimating: 221it [02:56,  1.24it/s]Extractor Estimating: 222it [02:56,  1.33it/s]Extractor Estimating: 223it [02:57,  1.42it/s]Extractor Estimating: 224it [02:57,  1.52it/s]Extractor Estimating: 225it [02:58,  1.48it/s]Extractor Estimating: 226it [02:59,  1.46it/s]Extractor Estimating: 227it [03:00,  1.41it/s]Extractor Estimating: 228it [03:00,  1.42it/s]Extractor Estimating: 229it [03:01,  1.47it/s]Extractor Estimating: 230it [03:01,  1.56it/s]Extractor Estimating: 231it [03:02,  1.47it/s]Extractor Estimating: 232it [03:03,  1.51it/s]Extractor Estimating: 233it [03:03,  1.52it/s]Extractor Estimating: 234it [03:04,  1.56it/s]Extractor Estimating: 235it [03:05,  1.60it/s]Extractor Estimating: 236it [03:05,  1.56it/s]Extractor Estimating: 237it [03:06,  1.57it/s]Extractor Estimating: 238it [03:07,  1.64it/s]Extractor Estimating: 239it [03:07,  1.65it/s]Extractor Estimating: 240it [03:08,  1.66it/s]Extractor Estimating: 241it [03:08,  1.54it/s]Extractor Estimating: 242it [03:09,  1.63it/s]Extractor Estimating: 243it [03:10,  1.60it/s]Extractor Estimating: 244it [03:10,  1.63it/s]Extractor Estimating: 245it [03:11,  1.61it/s]Extractor Estimating: 246it [03:12,  1.46it/s]Extractor Estimating: 247it [03:12,  1.51it/s]Extractor Estimating: 248it [03:13,  1.58it/s]Extractor Estimating: 249it [03:13,  1.61it/s]Extractor Estimating: 250it [03:14,  1.51it/s]Extractor Estimating: 250it [03:14,  1.28it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5030 mean pseudo reward: 0.9860212645242101
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 24628
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24728, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_synthetic_large/unseen_5_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24728, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.329, loss:3229.7888
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.005, loss:2122.6994
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 90, avg_time 0.992, loss:1743.1478
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 190, avg_time 0.992, loss:1641.9643
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 80, avg_time 0.998, loss:1499.5246
>> valid entity prec:0.5870, rec:0.5928, f1:0.5899
>> valid relation prec:0.7507, rec:0.0775, f1:0.1405
>> valid relation with NER prec:0.7507, rec:0.0775, f1:0.1405
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 180, avg_time 2.302, loss:1451.7680
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 70, avg_time 0.995, loss:1300.1066
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 170, avg_time 0.994, loss:1233.2558
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 60, avg_time 0.988, loss:1174.9854
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 160, avg_time 0.986, loss:1125.0811
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5912, rec:0.5982, f1:0.5947
>> valid relation prec:0.7460, rec:0.0663, f1:0.1218
>> valid relation with NER prec:0.7460, rec:0.0663, f1:0.1218
new max entity f1 on valid!
g_step 1100, step 50, avg_time 2.257, loss:1045.1498
g_step 1200, step 150, avg_time 0.990, loss:1060.6527
g_step 1300, step 40, avg_time 0.986, loss:956.6364
g_step 1400, step 140, avg_time 0.995, loss:926.4772
g_step 1500, step 30, avg_time 0.993, loss:953.8952
>> valid entity prec:0.5936, rec:0.5965, f1:0.5950
>> valid relation prec:0.5947, rec:0.1095, f1:0.1850
>> valid relation with NER prec:0.5947, rec:0.1095, f1:0.1850
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 130, avg_time 2.306, loss:855.9027
g_step 1700, step 20, avg_time 0.987, loss:893.2914
g_step 1800, step 120, avg_time 0.992, loss:815.1513
g_step 1900, step 10, avg_time 0.982, loss:834.1217
g_step 2000, step 110, avg_time 1.127, loss:755.0265
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6143, rec:0.5665, f1:0.5894
>> valid relation prec:0.4526, rec:0.1161, f1:0.1848
>> valid relation with NER prec:0.4526, rec:0.1161, f1:0.1848
g_step 2100, step 210, avg_time 2.255, loss:798.7935
g_step 2200, step 100, avg_time 1.003, loss:741.5858
g_step 2300, step 200, avg_time 0.972, loss:749.2383
g_step 2400, step 90, avg_time 0.983, loss:681.0548
g_step 2500, step 190, avg_time 1.000, loss:699.3338
>> valid entity prec:0.5836, rec:0.6054, f1:0.5943
>> valid relation prec:0.3775, rec:0.1044, f1:0.1635
>> valid relation with NER prec:0.3775, rec:0.1044, f1:0.1635
g_step 2600, step 80, avg_time 2.238, loss:654.6970
g_step 2700, step 180, avg_time 0.999, loss:695.8698
g_step 2800, step 70, avg_time 0.986, loss:620.8217
g_step 2900, step 170, avg_time 0.986, loss:652.8848
g_step 3000, step 60, avg_time 0.994, loss:620.0239
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5945, rec:0.5966, f1:0.5956
>> valid relation prec:0.2282, rec:0.0578, f1:0.0922
>> valid relation with NER prec:0.2282, rec:0.0578, f1:0.0922
new max entity f1 on valid!
g_step 3100, step 160, avg_time 2.248, loss:604.5995
g_step 3200, step 50, avg_time 0.983, loss:574.7733
g_step 3300, step 150, avg_time 1.000, loss:576.0079
g_step 3400, step 40, avg_time 0.976, loss:556.5920
g_step 3500, step 140, avg_time 0.987, loss:563.2444
>> valid entity prec:0.6225, rec:0.5197, f1:0.5664
>> valid relation prec:0.3831, rec:0.0918, f1:0.1481
>> valid relation with NER prec:0.3831, rec:0.0918, f1:0.1481
g_step 3600, step 30, avg_time 2.251, loss:540.9985
g_step 3700, step 130, avg_time 0.993, loss:525.0880
g_step 3800, step 20, avg_time 0.998, loss:526.2762
g_step 3900, step 120, avg_time 0.992, loss:519.4842
g_step 4000, step 10, avg_time 0.972, loss:502.3809
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6160, rec:0.5085, f1:0.5571
>> valid relation prec:0.3286, rec:0.0921, f1:0.1438
>> valid relation with NER prec:0.3286, rec:0.0921, f1:0.1438
g_step 4100, step 110, avg_time 2.250, loss:465.9634
g_step 4200, step 210, avg_time 0.986, loss:496.6527
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/27/2023 22:47:03 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/27/2023 22:47:04 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug27_22-47-02_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/27/2023 22:47:11 - WARNING - datasets.builder -   Using custom data configuration default-c948e2a4d485f8ce
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-c948e2a4d485f8ce/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:08,  8.73s/ tables]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-27 22:47:30,090 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:47:30,091 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-27 22:47:30,091 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-27 22:47:30,092 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-27 22:47:30,387 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:30,523 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:30,524 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:30,524 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:30,524 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:30,524 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-27 22:47:30,524 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-27 22:47:32,089 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-27 22:47:35,522 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-27 22:47:35,522 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-c948e2a4d485f8ce/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/27/2023 22:47:35 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x145d12bb5c20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:03<00:15,  3.08s/ba] 33%|███▎      | 2/6 [00:03<00:05,  1.41s/ba] 50%|█████     | 3/6 [00:03<00:02,  1.16ba/s] 67%|██████▋   | 4/6 [00:03<00:01,  1.64ba/s] 83%|████████▎ | 5/6 [00:03<00:00,  2.14ba/s]100%|██████████| 6/6 [00:03<00:00,  1.51ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.17ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.12ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.61ba/s]100%|██████████| 4/4 [00:01<00:00,  4.71ba/s]100%|██████████| 4/4 [00:01<00:00,  3.91ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  1.69ba/s] 50%|█████     | 3/6 [00:00<00:00,  4.45ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  6.32ba/s]100%|██████████| 6/6 [00:00<00:00,  6.11ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.75ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.29ba/s]100%|██████████| 4/4 [00:00<00:00,  6.35ba/s]100%|██████████| 4/4 [00:00<00:00,  4.83ba/s]
[INFO|trainer.py:414] 2023-08-27 22:47:48,750 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-27 22:47:49,359 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-27 22:47:49,359 >>   Num examples = 5038
[INFO|trainer.py:1149] 2023-08-27 22:47:49,359 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-27 22:47:49,359 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-27 22:47:49,359 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-27 22:47:49,359 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-27 22:47:49,359 >>   Total optimization steps = 395
  0%|          | 0/395 [00:00<?, ?it/s]  0%|          | 1/395 [00:04<32:43,  4.98s/it]  1%|          | 2/395 [00:05<14:50,  2.27s/it]  1%|          | 3/395 [00:05<08:55,  1.37s/it]  1%|          | 4/395 [00:05<06:08,  1.06it/s]  1%|▏         | 5/395 [00:06<04:36,  1.41it/s]  2%|▏         | 6/395 [00:06<03:40,  1.77it/s]  2%|▏         | 7/395 [00:06<03:04,  2.10it/s]  2%|▏         | 8/395 [00:07<02:41,  2.39it/s]  2%|▏         | 9/395 [00:07<02:26,  2.64it/s]  3%|▎         | 10/395 [00:07<02:15,  2.84it/s]  3%|▎         | 11/395 [00:07<02:08,  2.99it/s]  3%|▎         | 12/395 [00:08<02:14,  2.84it/s]  3%|▎         | 13/395 [00:08<02:07,  2.99it/s]  4%|▎         | 14/395 [00:08<02:02,  3.11it/s]  4%|▍         | 15/395 [00:09<01:58,  3.20it/s]  4%|▍         | 16/395 [00:09<01:56,  3.27it/s]  4%|▍         | 17/395 [00:09<02:05,  3.02it/s]  5%|▍         | 18/395 [00:10<02:00,  3.13it/s]  5%|▍         | 19/395 [00:10<01:57,  3.21it/s]  5%|▌         | 20/395 [00:10<01:54,  3.27it/s]  5%|▌         | 21/395 [00:11<01:52,  3.32it/s]  6%|▌         | 22/395 [00:11<01:57,  3.18it/s]  6%|▌         | 23/395 [00:11<01:54,  3.25it/s]  6%|▌         | 24/395 [00:12<01:52,  3.30it/s]  6%|▋         | 25/395 [00:12<01:50,  3.34it/s]  7%|▋         | 26/395 [00:12<01:49,  3.36it/s]  7%|▋         | 27/395 [00:12<01:48,  3.39it/s]  7%|▋         | 28/395 [00:13<01:47,  3.41it/s]  7%|▋         | 29/395 [00:13<01:46,  3.43it/s]  8%|▊         | 30/395 [00:14<02:23,  2.54it/s]  8%|▊         | 31/395 [00:14<02:58,  2.04it/s]  8%|▊         | 32/395 [00:15<02:35,  2.33it/s]  8%|▊         | 33/395 [00:15<02:20,  2.58it/s]  9%|▊         | 34/395 [00:15<02:09,  2.80it/s]  9%|▉         | 35/395 [00:15<02:01,  2.97it/s]  9%|▉         | 36/395 [00:16<01:55,  3.11it/s]  9%|▉         | 37/395 [00:16<01:51,  3.20it/s] 10%|▉         | 38/395 [00:16<01:48,  3.28it/s] 10%|▉         | 39/395 [00:17<01:46,  3.34it/s] 10%|█         | 40/395 [00:17<01:45,  3.37it/s] 10%|█         | 41/395 [00:17<01:44,  3.40it/s] 11%|█         | 42/395 [00:17<01:43,  3.42it/s] 11%|█         | 43/395 [00:18<01:42,  3.43it/s] 11%|█         | 44/395 [00:18<01:41,  3.44it/s] 11%|█▏        | 45/395 [00:18<01:41,  3.45it/s] 12%|█▏        | 46/395 [00:19<01:41,  3.45it/s] 12%|█▏        | 47/395 [00:19<01:40,  3.46it/s] 12%|█▏        | 48/395 [00:19<01:40,  3.46it/s] 12%|█▏        | 49/395 [00:20<01:39,  3.47it/s] 13%|█▎        | 50/395 [00:20<01:48,  3.18it/s] 13%|█▎        | 51/395 [00:20<01:45,  3.26it/s] 13%|█▎        | 52/395 [00:20<01:43,  3.33it/s] 13%|█▎        | 53/395 [00:21<01:41,  3.36it/s] 14%|█▎        | 54/395 [00:21<01:40,  3.39it/s] 14%|█▍        | 55/395 [00:21<01:39,  3.42it/s] 14%|█▍        | 56/395 [00:22<01:38,  3.43it/s] 14%|█▍        | 57/395 [00:22<01:38,  3.44it/s] 15%|█▍        | 58/395 [00:22<01:37,  3.45it/s] 15%|█▍        | 59/395 [00:22<01:37,  3.45it/s] 15%|█▌        | 60/395 [00:23<01:36,  3.46it/s] 15%|█▌        | 61/395 [00:23<01:45,  3.17it/s] 16%|█▌        | 62/395 [00:23<01:42,  3.25it/s] 16%|█▌        | 63/395 [00:24<01:40,  3.32it/s] 16%|█▌        | 64/395 [00:24<01:38,  3.36it/s] 16%|█▋        | 65/395 [00:24<01:37,  3.40it/s] 17%|█▋        | 66/395 [00:25<01:36,  3.42it/s] 17%|█▋        | 67/395 [00:25<01:35,  3.43it/s] 17%|█▋        | 68/395 [00:25<01:34,  3.45it/s] 17%|█▋        | 69/395 [00:25<01:34,  3.45it/s] 18%|█▊        | 70/395 [00:26<01:34,  3.46it/s] 18%|█▊        | 71/395 [00:26<01:33,  3.46it/s] 18%|█▊        | 72/395 [00:26<01:41,  3.20it/s] 18%|█▊        | 73/395 [00:27<01:38,  3.27it/s] 19%|█▊        | 74/395 [00:27<01:36,  3.33it/s] 19%|█▉        | 75/395 [00:27<01:34,  3.37it/s] 19%|█▉        | 76/395 [00:28<01:33,  3.40it/s] 19%|█▉        | 77/395 [00:28<01:33,  3.42it/s] 20%|█▉        | 78/395 [00:28<01:32,  3.43it/s] 20%|██        | 79/395 [00:28<01:35,  3.30it/s][INFO|trainer.py:2140] 2023-08-27 22:48:18,325 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:48:18,325 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-27 22:48:18,325 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.69it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.35it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.47it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.92it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.39it/s][A
  7%|▋         | 32/438 [00:00<00:10, 37.46it/s][A
  8%|▊         | 37/438 [00:00<00:10, 39.66it/s][A
 10%|▉         | 42/438 [00:00<00:09, 41.39it/s][A
 11%|█         | 47/438 [00:01<00:09, 42.52it/s][A
 12%|█▏        | 52/438 [00:01<00:10, 38.05it/s][A
 13%|█▎        | 57/438 [00:01<00:09, 40.17it/s][A
 14%|█▍        | 62/438 [00:01<00:09, 41.66it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 42.70it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 43.47it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.03it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.52it/s][A
 20%|█▉        | 87/438 [00:02<00:07, 44.81it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.58it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.39it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.67it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.88it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.03it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.03it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 45.12it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 45.14it/s][A
 30%|███       | 132/438 [00:03<00:06, 45.09it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.84it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.65it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.83it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.03it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.21it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 41.23it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 42.44it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 43.38it/s][A
 40%|████      | 177/438 [00:04<00:06, 38.78it/s][A
 42%|████▏     | 182/438 [00:04<00:06, 40.36it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 42.07it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 42.93it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 43.62it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.09it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.47it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.62it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.61it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.53it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.71it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.90it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 45.01it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 45.03it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 45.09it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 45.20it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 45.14it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.94it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.86it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.76it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 45.01it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 45.23it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 45.15it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.15it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 41.48it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 40.91it/s][A
 70%|███████   | 307/438 [00:07<00:03, 42.25it/s][A
 71%|███████   | 312/438 [00:07<00:02, 43.19it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 43.82it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.29it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.61it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.73it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.49it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.26it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.62it/s][A
 80%|████████  | 352/438 [00:08<00:01, 44.70it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.01it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.09it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.28it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 45.34it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 45.18it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.88it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.78it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.78it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.84it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 45.01it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 45.15it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 45.30it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 45.24it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 45.23it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.97it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 42.16it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 43.15it/s][A                                                
                                                 [A 20%|██        | 79/395 [00:38<01:35,  3.30it/s]
100%|██████████| 438/438 [00:09<00:00, 43.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:48:28,863 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79
[INFO|configuration_utils.py:351] 2023-08-27 22:48:29,164 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:48:35,453 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:48:35,811 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:48:35,982 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79/special_tokens_map.json
 20%|██        | 80/395 [01:17<1:17:00, 14.67s/it] 21%|██        | 81/395 [01:17<54:16, 10.37s/it]   21%|██        | 82/395 [01:17<38:19,  7.35s/it] 21%|██        | 83/395 [01:18<27:11,  5.23s/it] 21%|██▏       | 84/395 [01:18<19:25,  3.75s/it] 22%|██▏       | 85/395 [01:18<14:12,  2.75s/it] 22%|██▏       | 86/395 [01:19<10:22,  2.01s/it] 22%|██▏       | 87/395 [01:19<07:40,  1.50s/it] 22%|██▏       | 88/395 [01:19<05:48,  1.13s/it] 23%|██▎       | 89/395 [01:19<04:29,  1.14it/s] 23%|██▎       | 90/395 [01:20<03:34,  1.42it/s] 23%|██▎       | 91/395 [01:20<02:55,  1.73it/s] 23%|██▎       | 92/395 [01:20<02:28,  2.04it/s] 24%|██▎       | 93/395 [01:21<02:09,  2.33it/s] 24%|██▍       | 94/395 [01:21<01:56,  2.58it/s] 24%|██▍       | 95/395 [01:21<01:54,  2.61it/s] 24%|██▍       | 96/395 [01:22<01:45,  2.82it/s] 25%|██▍       | 97/395 [01:22<01:39,  2.99it/s] 25%|██▍       | 98/395 [01:22<01:35,  3.12it/s] 25%|██▌       | 99/395 [01:22<01:31,  3.22it/s] 25%|██▌       | 100/395 [01:23<01:29,  3.29it/s] 26%|██▌       | 101/395 [01:23<01:27,  3.34it/s] 26%|██▌       | 102/395 [01:23<01:26,  3.38it/s] 26%|██▌       | 103/395 [01:24<01:25,  3.41it/s] 26%|██▋       | 104/395 [01:24<01:24,  3.43it/s] 27%|██▋       | 105/395 [01:24<01:24,  3.45it/s] 27%|██▋       | 106/395 [01:24<01:29,  3.23it/s] 27%|██▋       | 107/395 [01:25<01:27,  3.30it/s] 27%|██▋       | 108/395 [01:25<01:25,  3.35it/s] 28%|██▊       | 109/395 [01:25<01:24,  3.39it/s] 28%|██▊       | 110/395 [01:26<01:23,  3.41it/s] 28%|██▊       | 111/395 [01:26<01:22,  3.43it/s] 28%|██▊       | 112/395 [01:26<01:22,  3.45it/s] 29%|██▊       | 113/395 [01:27<01:21,  3.45it/s] 29%|██▉       | 114/395 [01:27<01:21,  3.46it/s] 29%|██▉       | 115/395 [01:27<01:20,  3.46it/s] 29%|██▉       | 116/395 [01:27<01:20,  3.46it/s] 30%|██▉       | 117/395 [01:28<01:26,  3.23it/s] 30%|██▉       | 118/395 [01:28<01:23,  3.30it/s] 30%|███       | 119/395 [01:28<01:22,  3.35it/s] 30%|███       | 120/395 [01:29<01:21,  3.38it/s] 31%|███       | 121/395 [01:29<01:20,  3.41it/s] 31%|███       | 122/395 [01:29<01:19,  3.43it/s] 31%|███       | 123/395 [01:29<01:18,  3.44it/s] 31%|███▏      | 124/395 [01:30<01:18,  3.45it/s] 32%|███▏      | 125/395 [01:30<01:18,  3.46it/s] 32%|███▏      | 126/395 [01:30<01:17,  3.46it/s] 32%|███▏      | 127/395 [01:31<01:17,  3.46it/s] 32%|███▏      | 128/395 [01:31<01:21,  3.26it/s] 33%|███▎      | 129/395 [01:31<01:20,  3.32it/s] 33%|███▎      | 130/395 [01:32<01:18,  3.36it/s] 33%|███▎      | 131/395 [01:32<01:17,  3.40it/s] 33%|███▎      | 132/395 [01:32<01:16,  3.42it/s] 34%|███▎      | 133/395 [01:32<01:16,  3.44it/s] 34%|███▍      | 134/395 [01:33<01:15,  3.44it/s] 34%|███▍      | 135/395 [01:33<01:15,  3.45it/s] 34%|███▍      | 136/395 [01:33<01:14,  3.45it/s] 35%|███▍      | 137/395 [01:34<01:14,  3.46it/s] 35%|███▍      | 138/395 [01:34<01:14,  3.46it/s] 35%|███▌      | 139/395 [01:34<01:20,  3.19it/s] 35%|███▌      | 140/395 [01:35<01:17,  3.27it/s] 36%|███▌      | 141/395 [01:35<01:16,  3.33it/s] 36%|███▌      | 142/395 [01:35<01:15,  3.36it/s] 36%|███▌      | 143/395 [01:35<01:14,  3.39it/s] 36%|███▋      | 144/395 [01:36<01:13,  3.42it/s] 37%|███▋      | 145/395 [01:36<01:12,  3.43it/s] 37%|███▋      | 146/395 [01:36<01:12,  3.44it/s] 37%|███▋      | 147/395 [01:37<01:11,  3.45it/s] 37%|███▋      | 148/395 [01:37<01:11,  3.45it/s] 38%|███▊      | 149/395 [01:37<01:11,  3.45it/s] 38%|███▊      | 150/395 [01:38<01:23,  2.94it/s] 38%|███▊      | 151/395 [01:38<01:19,  3.07it/s] 38%|███▊      | 152/395 [01:38<01:16,  3.18it/s] 39%|███▊      | 153/395 [01:38<01:14,  3.26it/s] 39%|███▉      | 154/395 [01:39<01:12,  3.32it/s] 39%|███▉      | 155/395 [01:39<01:11,  3.36it/s] 39%|███▉      | 156/395 [01:39<01:10,  3.39it/s] 40%|███▉      | 157/395 [01:40<01:09,  3.41it/s] 40%|████      | 158/395 [01:40<01:04,  3.68it/s][INFO|trainer.py:2140] 2023-08-27 22:49:29,671 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:49:29,672 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-27 22:49:29,672 >>   Batch size = 8
{'eval_loss': 1.0175187587738037, 'eval_runtime': 9.9696, 'eval_samples_per_second': 350.766, 'eval_steps_per_second': 43.934, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.20it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.10it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.55it/s][A
  5%|▌         | 22/438 [00:00<00:10, 38.32it/s][A
  6%|▌         | 27/438 [00:00<00:10, 40.57it/s][A
  7%|▋         | 32/438 [00:00<00:09, 42.10it/s][A
  8%|▊         | 37/438 [00:00<00:09, 43.14it/s][A
 10%|▉         | 42/438 [00:00<00:09, 43.80it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.42it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.66it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.91it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.46it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.28it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.35it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.68it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.96it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 45.10it/s][A
 21%|██        | 92/438 [00:02<00:07, 45.26it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 45.26it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 45.23it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.94it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.56it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.62it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.75it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.89it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.99it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 45.27it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 45.37it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 45.29it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.96it/s][A
 36%|███▌      | 157/438 [00:03<00:07, 35.71it/s][A
 37%|███▋      | 162/438 [00:03<00:07, 38.11it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 40.01it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 41.49it/s][A
 40%|████      | 177/438 [00:04<00:06, 42.61it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 43.50it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.11it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.41it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.27it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.04it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.11it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.32it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.57it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.88it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 45.08it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 45.26it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 45.12it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 45.02it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.67it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.66it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.65it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.79it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.98it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 45.21it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 45.24it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 45.22it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.88it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 37.73it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 39.70it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 41.27it/s][A
 70%|███████   | 307/438 [00:07<00:03, 42.49it/s][A
 71%|███████   | 312/438 [00:07<00:02, 43.39it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.07it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.43it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.59it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.35it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.21it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.26it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.48it/s][A
 80%|████████  | 352/438 [00:08<00:01, 44.79it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.12it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.20it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.34it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 45.00it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.77it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.53it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.53it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 43.82it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.27it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.67it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.76it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.88it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 45.11it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.98it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.85it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.68it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.65it/s][A                                                 
                                                 [A 40%|████      | 158/395 [01:50<01:04,  3.68it/s]
100%|██████████| 438/438 [00:09<00:00, 44.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:49:40,059 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-27 22:49:40,502 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:50:04,583 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:50:05,399 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:50:05,527 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158/special_tokens_map.json
 40%|████      | 159/395 [03:01<1:36:47, 24.61s/it] 41%|████      | 160/395 [03:02<1:07:54, 17.34s/it] 41%|████      | 161/395 [03:02<47:40, 12.22s/it]   41%|████      | 162/395 [03:02<33:33,  8.64s/it] 41%|████▏     | 163/395 [03:02<23:43,  6.14s/it] 42%|████▏     | 164/395 [03:03<16:52,  4.38s/it] 42%|████▏     | 165/395 [03:03<12:05,  3.15s/it] 42%|████▏     | 166/395 [03:03<08:45,  2.29s/it] 42%|████▏     | 167/395 [03:04<06:25,  1.69s/it] 43%|████▎     | 168/395 [03:04<04:48,  1.27s/it] 43%|████▎     | 169/395 [03:04<03:40,  1.02it/s] 43%|████▎     | 170/395 [03:04<02:53,  1.30it/s] 43%|████▎     | 171/395 [03:05<02:24,  1.56it/s] 44%|████▎     | 172/395 [03:05<01:59,  1.86it/s] 44%|████▍     | 173/395 [03:05<01:42,  2.16it/s] 44%|████▍     | 174/395 [03:06<01:30,  2.44it/s] 44%|████▍     | 175/395 [03:06<01:22,  2.68it/s] 45%|████▍     | 176/395 [03:06<01:16,  2.87it/s] 45%|████▍     | 177/395 [03:07<01:11,  3.03it/s] 45%|████▌     | 178/395 [03:07<01:08,  3.15it/s] 45%|████▌     | 179/395 [03:07<01:06,  3.24it/s] 46%|████▌     | 180/395 [03:07<01:05,  3.30it/s] 46%|████▌     | 181/395 [03:08<01:03,  3.36it/s] 46%|████▌     | 182/395 [03:08<01:07,  3.16it/s] 46%|████▋     | 183/395 [03:08<01:05,  3.25it/s] 47%|████▋     | 184/395 [03:09<01:03,  3.32it/s] 47%|████▋     | 185/395 [03:09<01:02,  3.36it/s] 47%|████▋     | 186/395 [03:09<01:01,  3.40it/s] 47%|████▋     | 187/395 [03:09<01:00,  3.42it/s] 48%|████▊     | 188/395 [03:10<01:00,  3.44it/s] 48%|████▊     | 189/395 [03:10<00:59,  3.45it/s] 48%|████▊     | 190/395 [03:10<00:59,  3.46it/s] 48%|████▊     | 191/395 [03:11<00:58,  3.47it/s] 49%|████▊     | 192/395 [03:11<00:58,  3.47it/s] 49%|████▉     | 193/395 [03:11<01:04,  3.14it/s] 49%|████▉     | 194/395 [03:12<01:02,  3.23it/s] 49%|████▉     | 195/395 [03:12<01:00,  3.30it/s] 50%|████▉     | 196/395 [03:12<00:59,  3.36it/s] 50%|████▉     | 197/395 [03:12<00:58,  3.39it/s] 50%|█████     | 198/395 [03:13<00:57,  3.42it/s] 50%|█████     | 199/395 [03:13<00:57,  3.43it/s] 51%|█████     | 200/395 [03:13<00:56,  3.45it/s] 51%|█████     | 201/395 [03:14<00:56,  3.46it/s] 51%|█████     | 202/395 [03:14<00:55,  3.46it/s] 51%|█████▏    | 203/395 [03:14<00:55,  3.47it/s] 52%|█████▏    | 204/395 [03:15<00:59,  3.22it/s] 52%|█████▏    | 205/395 [03:15<01:03,  2.99it/s] 52%|█████▏    | 206/395 [03:15<01:00,  3.12it/s] 52%|█████▏    | 207/395 [03:16<00:58,  3.22it/s] 53%|█████▎    | 208/395 [03:16<00:56,  3.29it/s] 53%|█████▎    | 209/395 [03:16<00:55,  3.34it/s] 53%|█████▎    | 210/395 [03:16<00:54,  3.38it/s] 53%|█████▎    | 211/395 [03:18<01:49,  1.68it/s] 54%|█████▎    | 212/395 [03:18<01:37,  1.88it/s] 54%|█████▍    | 213/395 [03:18<01:23,  2.18it/s] 54%|█████▍    | 214/395 [03:19<01:13,  2.45it/s] 54%|█████▍    | 215/395 [03:19<01:06,  2.69it/s] 55%|█████▍    | 216/395 [03:19<01:02,  2.88it/s] 55%|█████▍    | 217/395 [03:19<00:58,  3.04it/s] 55%|█████▌    | 218/395 [03:20<00:56,  3.16it/s] 55%|█████▌    | 219/395 [03:20<00:54,  3.24it/s] 56%|█████▌    | 220/395 [03:20<00:52,  3.31it/s] 56%|█████▌    | 221/395 [03:21<00:51,  3.35it/s] 56%|█████▌    | 222/395 [03:21<00:51,  3.38it/s] 56%|█████▋    | 223/395 [03:21<00:54,  3.13it/s] 57%|█████▋    | 224/395 [03:22<00:53,  3.22it/s] 57%|█████▋    | 225/395 [03:22<00:51,  3.29it/s] 57%|█████▋    | 226/395 [03:22<00:50,  3.34it/s] 57%|█████▋    | 227/395 [03:22<00:49,  3.38it/s] 58%|█████▊    | 228/395 [03:23<00:49,  3.40it/s] 58%|█████▊    | 229/395 [03:23<00:48,  3.42it/s] 58%|█████▊    | 230/395 [03:23<00:48,  3.44it/s] 58%|█████▊    | 231/395 [03:24<00:47,  3.45it/s] 59%|█████▊    | 232/395 [03:24<00:47,  3.45it/s] 59%|█████▉    | 233/395 [03:24<00:46,  3.46it/s] 59%|█████▉    | 234/395 [03:25<00:47,  3.38it/s] 59%|█████▉    | 235/395 [03:25<00:47,  3.40it/s] 60%|█████▉    | 236/395 [03:25<00:46,  3.42it/s] 60%|██████    | 237/395 [03:25<00:42,  3.69it/s][INFO|trainer.py:2140] 2023-08-27 22:51:15,172 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:51:15,172 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-27 22:51:15,172 >>   Batch size = 8
{'eval_loss': 1.0147062540054321, 'eval_runtime': 9.9535, 'eval_samples_per_second': 351.334, 'eval_steps_per_second': 44.005, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.46it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.13it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.54it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.77it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.48it/s][A
  7%|▋         | 32/438 [00:00<00:08, 46.11it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.77it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.17it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.27it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.23it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.27it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.38it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.45it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.46it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.22it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 45.07it/s][A
 21%|██        | 92/438 [00:02<00:08, 43.07it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.88it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.40it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.73it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.98it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.13it/s][A
 28%|██▊       | 122/438 [00:02<00:06, 45.19it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 45.14it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.87it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.84it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.96it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 45.16it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.38it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.36it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.22it/s][A
 38%|███▊      | 167/438 [00:03<00:05, 45.26it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.03it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.77it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.89it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.97it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.16it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.30it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.31it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.30it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 45.14it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.94it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.80it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 37.65it/s][A
 53%|█████▎    | 232/438 [00:05<00:05, 39.79it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 41.43it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 42.64it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 43.56it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.13it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.64it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.92it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.49it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.32it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.49it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.70it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.98it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.13it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 45.37it/s][A
 69%|██████▉   | 302/438 [00:06<00:02, 45.37it/s][A
 70%|███████   | 307/438 [00:06<00:02, 45.45it/s][A
 71%|███████   | 312/438 [00:06<00:02, 45.17it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.83it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.81it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.86it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 45.08it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 45.19it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 45.25it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 45.42it/s][A
 80%|████████  | 352/438 [00:07<00:01, 45.38it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.17it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 40.89it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 42.16it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 43.15it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 43.86it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.27it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.67it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.93it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 45.05it/s][A
 92%|█████████▏| 402/438 [00:08<00:00, 44.73it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.56it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.77it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.93it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 45.04it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 45.12it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 45.31it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 45.35it/s][A                                                 
                                                 [A 60%|██████    | 237/395 [03:35<00:42,  3.69it/s]
100%|██████████| 438/438 [00:09<00:00, 45.35it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:51:25,504 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237
[INFO|configuration_utils.py:351] 2023-08-27 22:51:25,858 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:51:45,864 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:51:46,243 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:51:46,446 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237/special_tokens_map.json
 60%|██████    | 238/395 [04:42<1:00:53, 23.27s/it] 61%|██████    | 239/395 [04:43<42:38, 16.40s/it]   61%|██████    | 240/395 [04:43<29:53, 11.57s/it] 61%|██████    | 241/395 [04:43<21:00,  8.18s/it] 61%|██████▏   | 242/395 [04:43<14:49,  5.82s/it] 62%|██████▏   | 243/395 [04:44<10:31,  4.16s/it] 62%|██████▏   | 244/395 [04:44<07:32,  3.00s/it] 62%|██████▏   | 245/395 [04:44<05:27,  2.18s/it] 62%|██████▏   | 246/395 [04:45<04:00,  1.61s/it] 63%|██████▎   | 247/395 [04:45<03:00,  1.22s/it] 63%|██████▎   | 248/395 [04:45<02:17,  1.07it/s] 63%|██████▎   | 249/395 [04:45<01:48,  1.35it/s] 63%|██████▎   | 250/395 [04:46<01:32,  1.57it/s] 64%|██████▎   | 251/395 [04:46<01:16,  1.88it/s] 64%|██████▍   | 252/395 [04:46<01:05,  2.18it/s] 64%|██████▍   | 253/395 [04:47<00:57,  2.45it/s] 64%|██████▍   | 254/395 [04:47<00:52,  2.69it/s] 65%|██████▍   | 255/395 [04:47<00:48,  2.88it/s] 65%|██████▍   | 256/395 [04:48<00:45,  3.04it/s] 65%|██████▌   | 257/395 [04:48<00:43,  3.16it/s] 65%|██████▌   | 258/395 [04:48<00:42,  3.25it/s] 66%|██████▌   | 259/395 [04:48<00:41,  3.31it/s] 66%|██████▌   | 260/395 [04:49<00:40,  3.36it/s] 66%|██████▌   | 261/395 [04:49<00:42,  3.14it/s] 66%|██████▋   | 262/395 [04:49<00:41,  3.24it/s] 67%|██████▋   | 263/395 [04:50<00:39,  3.30it/s] 67%|██████▋   | 264/395 [04:50<00:39,  3.35it/s] 67%|██████▋   | 265/395 [04:50<00:38,  3.39it/s] 67%|██████▋   | 266/395 [04:51<00:37,  3.42it/s] 68%|██████▊   | 267/395 [04:51<00:37,  3.44it/s] 68%|██████▊   | 268/395 [04:51<00:36,  3.45it/s] 68%|██████▊   | 269/395 [04:51<00:36,  3.46it/s] 68%|██████▊   | 270/395 [04:52<00:36,  3.47it/s] 69%|██████▊   | 271/395 [04:52<00:35,  3.48it/s] 69%|██████▉   | 272/395 [04:52<00:37,  3.30it/s] 69%|██████▉   | 273/395 [04:53<00:36,  3.36it/s] 69%|██████▉   | 274/395 [04:53<00:35,  3.39it/s] 70%|██████▉   | 275/395 [04:53<00:35,  3.42it/s] 70%|██████▉   | 276/395 [04:53<00:34,  3.44it/s] 70%|███████   | 277/395 [04:54<00:34,  3.45it/s] 70%|███████   | 278/395 [04:54<00:33,  3.46it/s] 71%|███████   | 279/395 [04:54<00:33,  3.47it/s] 71%|███████   | 280/395 [04:55<00:33,  3.47it/s] 71%|███████   | 281/395 [04:55<00:32,  3.46it/s] 71%|███████▏  | 282/395 [04:55<00:32,  3.45it/s] 72%|███████▏  | 283/395 [04:56<00:34,  3.22it/s] 72%|███████▏  | 284/395 [04:56<00:33,  3.28it/s] 72%|███████▏  | 285/395 [04:56<00:33,  3.32it/s] 72%|███████▏  | 286/395 [04:56<00:32,  3.36it/s] 73%|███████▎  | 287/395 [04:57<00:31,  3.38it/s] 73%|███████▎  | 288/395 [04:57<00:31,  3.40it/s] 73%|███████▎  | 289/395 [04:57<00:31,  3.41it/s] 73%|███████▎  | 290/395 [04:58<00:30,  3.42it/s] 74%|███████▎  | 291/395 [04:58<00:30,  3.42it/s] 74%|███████▍  | 292/395 [04:58<00:30,  3.43it/s] 74%|███████▍  | 293/395 [04:58<00:29,  3.43it/s] 74%|███████▍  | 294/395 [04:59<00:31,  3.25it/s] 75%|███████▍  | 295/395 [04:59<00:30,  3.31it/s] 75%|███████▍  | 296/395 [04:59<00:29,  3.36it/s] 75%|███████▌  | 297/395 [05:00<00:28,  3.40it/s] 75%|███████▌  | 298/395 [05:00<00:28,  3.42it/s] 76%|███████▌  | 299/395 [05:00<00:27,  3.44it/s] 76%|███████▌  | 300/395 [05:01<00:27,  3.45it/s] 76%|███████▌  | 301/395 [05:01<00:27,  3.46it/s] 76%|███████▋  | 302/395 [05:01<00:26,  3.47it/s] 77%|███████▋  | 303/395 [05:01<00:26,  3.47it/s] 77%|███████▋  | 304/395 [05:02<00:26,  3.47it/s] 77%|███████▋  | 305/395 [05:02<00:28,  3.20it/s] 77%|███████▋  | 306/395 [05:02<00:27,  3.28it/s] 78%|███████▊  | 307/395 [05:03<00:26,  3.34it/s] 78%|███████▊  | 308/395 [05:03<00:25,  3.38it/s] 78%|███████▊  | 309/395 [05:03<00:25,  3.41it/s] 78%|███████▊  | 310/395 [05:04<00:24,  3.43it/s] 79%|███████▊  | 311/395 [05:04<00:24,  3.44it/s] 79%|███████▉  | 312/395 [05:04<00:24,  3.45it/s] 79%|███████▉  | 313/395 [05:04<00:23,  3.46it/s] 79%|███████▉  | 314/395 [05:05<00:23,  3.46it/s] 80%|███████▉  | 315/395 [05:05<00:23,  3.47it/s] 80%|████████  | 316/395 [05:05<00:22,  3.49it/s][INFO|trainer.py:2140] 2023-08-27 22:52:55,102 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:52:55,102 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-27 22:52:55,102 >>   Batch size = 8
{'eval_loss': 1.0163915157318115, 'eval_runtime': 9.8009, 'eval_samples_per_second': 356.805, 'eval_steps_per_second': 44.69, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.05it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.67it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.68it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.09it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.60it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.92it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.19it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.01it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.04it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.21it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.22it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.31it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.47it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.61it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.43it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.11it/s][A
 20%|██        | 88/438 [00:01<00:07, 44.89it/s][A
 21%|██        | 93/438 [00:02<00:07, 44.99it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 44.94it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.13it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 45.21it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 45.33it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 45.40it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 45.33it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 45.17it/s][A
 30%|███       | 133/438 [00:02<00:07, 39.46it/s][A
 32%|███▏      | 138/438 [00:03<00:07, 41.19it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 42.39it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 43.36it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 44.04it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.53it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.79it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.91it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 44.75it/s][A
 41%|████      | 178/438 [00:03<00:05, 44.55it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 44.74it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 44.95it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.05it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.12it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.37it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.19it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.20it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 44.74it/s][A
 51%|█████     | 223/438 [00:04<00:04, 44.70it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 44.85it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.00it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 45.11it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 45.24it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 45.39it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 45.39it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 45.30it/s][A
 60%|██████    | 263/438 [00:05<00:03, 44.96it/s][A
 61%|██████    | 268/438 [00:06<00:04, 40.15it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 41.73it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 42.76it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 43.57it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.11it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.70it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.91it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 44.89it/s][A
 70%|███████   | 308/438 [00:06<00:02, 44.63it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 44.51it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 44.53it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 44.66it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 44.99it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.18it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 45.30it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 45.41it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 45.40it/s][A
 81%|████████  | 353/438 [00:07<00:01, 45.06it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 44.85it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 44.87it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 44.82it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 45.07it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 45.20it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 45.40it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 45.33it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 45.28it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 45.12it/s][A
 92%|█████████▏| 403/438 [00:08<00:00, 44.95it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 44.73it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.86it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 45.02it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 45.13it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 38.83it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 41.45it/s][A
100%|██████████| 438/438 [00:09<00:00, 42.60it/s][A                                                 
                                                 [A 80%|████████  | 316/395 [05:15<00:22,  3.49it/s]
100%|██████████| 438/438 [00:09<00:00, 42.60it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:53:04,994 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-27 22:53:05,348 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:53:20,757 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:53:21,175 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:53:21,366 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316/special_tokens_map.json
 80%|████████  | 317/395 [06:12<26:07, 20.09s/it] 81%|████████  | 318/395 [06:12<18:10, 14.17s/it] 81%|████████  | 319/395 [06:12<12:40, 10.01s/it] 81%|████████  | 320/395 [06:12<08:51,  7.09s/it] 81%|████████▏ | 321/395 [06:13<06:13,  5.05s/it] 82%|████████▏ | 322/395 [06:13<04:24,  3.62s/it] 82%|████████▏ | 323/395 [06:13<03:08,  2.62s/it] 82%|████████▏ | 324/395 [06:14<02:16,  1.93s/it] 82%|████████▏ | 325/395 [06:14<01:40,  1.44s/it] 83%|████████▎ | 326/395 [06:14<01:15,  1.09s/it] 83%|████████▎ | 327/395 [06:15<00:57,  1.17it/s] 83%|████████▎ | 328/395 [06:15<00:45,  1.46it/s] 83%|████████▎ | 329/395 [06:15<00:39,  1.66it/s] 84%|████████▎ | 330/395 [06:16<00:33,  1.97it/s] 84%|████████▍ | 331/395 [06:16<00:28,  2.26it/s] 84%|████████▍ | 332/395 [06:16<00:25,  2.51it/s] 84%|████████▍ | 333/395 [06:16<00:22,  2.73it/s] 85%|████████▍ | 334/395 [06:17<00:20,  2.91it/s] 85%|████████▍ | 335/395 [06:17<00:19,  3.05it/s] 85%|████████▌ | 336/395 [06:17<00:18,  3.15it/s] 85%|████████▌ | 337/395 [06:18<00:17,  3.24it/s] 86%|████████▌ | 338/395 [06:18<00:17,  3.29it/s] 86%|████████▌ | 339/395 [06:18<00:18,  3.06it/s] 86%|████████▌ | 340/395 [06:19<00:17,  3.16it/s] 86%|████████▋ | 341/395 [06:19<00:16,  3.24it/s] 87%|████████▋ | 342/395 [06:19<00:16,  3.29it/s] 87%|████████▋ | 343/395 [06:19<00:15,  3.33it/s] 87%|████████▋ | 344/395 [06:20<00:15,  3.36it/s] 87%|████████▋ | 345/395 [06:20<00:14,  3.38it/s] 88%|████████▊ | 346/395 [06:20<00:14,  3.39it/s] 88%|████████▊ | 347/395 [06:21<00:14,  3.41it/s] 88%|████████▊ | 348/395 [06:21<00:13,  3.41it/s] 88%|████████▊ | 349/395 [06:21<00:14,  3.18it/s] 89%|████████▊ | 350/395 [06:22<00:13,  3.25it/s] 89%|████████▉ | 351/395 [06:22<00:13,  3.30it/s] 89%|████████▉ | 352/395 [06:22<00:12,  3.34it/s] 89%|████████▉ | 353/395 [06:22<00:12,  3.36it/s] 90%|████████▉ | 354/395 [06:23<00:12,  3.38it/s] 90%|████████▉ | 355/395 [06:23<00:11,  3.39it/s] 90%|█████████ | 356/395 [06:23<00:11,  3.40it/s] 90%|█████████ | 357/395 [06:24<00:11,  3.41it/s] 91%|█████████ | 358/395 [06:24<00:10,  3.42it/s] 91%|█████████ | 359/395 [06:24<00:10,  3.42it/s] 91%|█████████ | 360/395 [06:25<00:11,  3.14it/s] 91%|█████████▏| 361/395 [06:25<00:10,  3.23it/s] 92%|█████████▏| 362/395 [06:25<00:10,  3.28it/s] 92%|█████████▏| 363/395 [06:25<00:09,  3.33it/s] 92%|█████████▏| 364/395 [06:26<00:09,  3.35it/s] 92%|█████████▏| 365/395 [06:26<00:08,  3.37it/s] 93%|█████████▎| 366/395 [06:26<00:08,  3.39it/s] 93%|█████████▎| 367/395 [06:27<00:08,  3.40it/s] 93%|█████████▎| 368/395 [06:27<00:07,  3.41it/s] 93%|█████████▎| 369/395 [06:27<00:07,  3.41it/s] 94%|█████████▎| 370/395 [06:28<00:07,  3.18it/s] 94%|█████████▍| 371/395 [06:28<00:07,  3.25it/s] 94%|█████████▍| 372/395 [06:28<00:06,  3.30it/s] 94%|█████████▍| 373/395 [06:28<00:06,  3.33it/s] 95%|█████████▍| 374/395 [06:29<00:06,  3.37it/s] 95%|█████████▍| 375/395 [06:29<00:05,  3.40it/s] 95%|█████████▌| 376/395 [06:29<00:05,  3.42it/s] 95%|█████████▌| 377/395 [06:30<00:05,  3.44it/s] 96%|█████████▌| 378/395 [06:30<00:04,  3.45it/s] 96%|█████████▌| 379/395 [06:30<00:04,  3.45it/s] 96%|█████████▌| 380/395 [06:30<00:04,  3.46it/s] 96%|█████████▋| 381/395 [06:31<00:04,  3.30it/s] 97%|█████████▋| 382/395 [06:31<00:03,  3.35it/s] 97%|█████████▋| 383/395 [06:31<00:03,  3.38it/s] 97%|█████████▋| 384/395 [06:32<00:03,  3.41it/s] 97%|█████████▋| 385/395 [06:32<00:02,  3.42it/s] 98%|█████████▊| 386/395 [06:32<00:02,  3.43it/s] 98%|█████████▊| 387/395 [06:32<00:02,  3.44it/s] 98%|█████████▊| 388/395 [06:33<00:02,  3.45it/s] 98%|█████████▊| 389/395 [06:33<00:01,  3.45it/s] 99%|█████████▊| 390/395 [06:33<00:01,  3.46it/s] 99%|█████████▉| 391/395 [06:34<00:01,  3.46it/s] 99%|█████████▉| 392/395 [06:34<00:00,  3.27it/s] 99%|█████████▉| 393/395 [06:34<00:00,  3.33it/s]100%|█████████▉| 394/395 [06:35<00:00,  3.37it/s]100%|██████████| 395/395 [06:35<00:00,  3.65it/s][INFO|trainer.py:2140] 2023-08-27 22:54:24,623 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:54:24,624 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-27 22:54:24,624 >>   Batch size = 8
{'eval_loss': 1.0183770656585693, 'eval_runtime': 9.8299, 'eval_samples_per_second': 355.752, 'eval_steps_per_second': 44.558, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.78it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.44it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.48it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.82it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.19it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.66it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.26it/s][A
 10%|▉         | 43/438 [00:00<00:08, 44.88it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.04it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.23it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.31it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.41it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.38it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.27it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.27it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 44.98it/s][A
 20%|██        | 88/438 [00:01<00:07, 44.72it/s][A
 21%|██        | 93/438 [00:02<00:08, 40.25it/s][A
 22%|██▏       | 98/438 [00:02<00:08, 41.83it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 42.93it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 43.79it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.25it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.69it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.97it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.87it/s][A
 30%|███       | 133/438 [00:02<00:06, 44.58it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.42it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.57it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.84it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.06it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 45.21it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 45.04it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 45.27it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 45.14it/s][A
 41%|████      | 178/438 [00:03<00:05, 44.86it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 44.72it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 44.74it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 44.88it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.18it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.25it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.43it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.41it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.12it/s][A
 51%|█████     | 223/438 [00:05<00:04, 45.06it/s][A
 52%|█████▏    | 228/438 [00:05<00:06, 34.66it/s][A
 53%|█████▎    | 233/438 [00:05<00:05, 37.33it/s][A
 54%|█████▍    | 238/438 [00:05<00:05, 39.51it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 41.06it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 42.30it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 43.29it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 43.83it/s][A
 60%|██████    | 263/438 [00:05<00:03, 44.15it/s][A
 61%|██████    | 268/438 [00:06<00:03, 44.13it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 44.33it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 44.62it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 44.80it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.95it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 45.08it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 45.22it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 45.15it/s][A
 70%|███████   | 308/438 [00:06<00:02, 44.99it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 44.87it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 44.88it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 44.94it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.06it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.08it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 45.24it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 45.23it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 45.22it/s][A
 81%|████████  | 353/438 [00:07<00:01, 44.96it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 44.84it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 38.21it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 40.15it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 41.62it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 42.63it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 43.54it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 44.13it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 44.46it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 44.71it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.38it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 44.56it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.76it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.98it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 44.90it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 44.98it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 45.05it/s][A
100%|██████████| 438/438 [00:09<00:00, 45.22it/s][A                                                 
                                                 [A100%|██████████| 395/395 [06:45<00:00,  3.65it/s]
100%|██████████| 438/438 [00:09<00:00, 45.22it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-27 22:54:34,687 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395
[INFO|configuration_utils.py:351] 2023-08-27 22:54:35,265 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:54:50,729 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:54:51,031 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:54:51,183 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-27 22:55:05,646 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-27 22:55:05,693 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158 (score: 1.0147062540054321).
                                                 100%|██████████| 395/395 [07:25<00:00,  3.65it/s]100%|██████████| 395/395 [07:25<00:00,  1.13s/it]
[INFO|trainer.py:1894] 2023-08-27 22:55:15,363 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-27 22:55:15,508 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-27 22:55:20,267 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-27 22:55:20,705 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-27 22:55:20,962 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:55:22,230 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:22,230 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:22,230 >>   train_loss               =     0.7841
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:22,230 >>   train_runtime            = 0:07:25.93
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:22,230 >>   train_samples            =       5038
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:22,230 >>   train_samples_per_second =     56.488
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:22,230 >>   train_steps_per_second   =      0.886
{'eval_loss': 1.0216940641403198, 'eval_runtime': 9.9103, 'eval_samples_per_second': 352.864, 'eval_steps_per_second': 44.196, 'epoch': 5.0}
{'train_runtime': 445.9329, 'train_samples_per_second': 56.488, 'train_steps_per_second': 0.886, 'train_loss': 0.7841492471815664, 'epoch': 5.0}
08/27/2023 22:55:22 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-27 22:55:22,921 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-27 22:55:22,921 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-27 22:55:22,922 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 56.11it/s]  3%|▎         | 12/438 [00:00<00:08, 49.97it/s]  4%|▍         | 18/438 [00:00<00:08, 48.10it/s]  5%|▌         | 23/438 [00:00<00:11, 36.23it/s]  6%|▋         | 28/438 [00:00<00:10, 38.93it/s]  8%|▊         | 33/438 [00:00<00:09, 40.85it/s]  9%|▊         | 38/438 [00:00<00:09, 42.42it/s] 10%|▉         | 43/438 [00:01<00:09, 43.47it/s] 11%|█         | 48/438 [00:01<00:08, 44.22it/s] 12%|█▏        | 53/438 [00:01<00:08, 44.80it/s] 13%|█▎        | 58/438 [00:01<00:08, 44.98it/s] 14%|█▍        | 63/438 [00:01<00:08, 44.83it/s] 16%|█▌        | 68/438 [00:01<00:08, 44.58it/s] 17%|█▋        | 73/438 [00:01<00:08, 44.68it/s] 18%|█▊        | 78/438 [00:01<00:08, 44.95it/s] 19%|█▉        | 83/438 [00:01<00:07, 45.10it/s] 20%|██        | 88/438 [00:01<00:07, 45.35it/s] 21%|██        | 93/438 [00:02<00:07, 45.44it/s] 22%|██▏       | 98/438 [00:02<00:07, 45.55it/s] 24%|██▎       | 103/438 [00:02<00:07, 45.60it/s] 25%|██▍       | 108/438 [00:02<00:07, 45.38it/s] 26%|██▌       | 113/438 [00:02<00:07, 45.23it/s] 27%|██▋       | 118/438 [00:02<00:07, 45.13it/s] 28%|██▊       | 123/438 [00:02<00:06, 45.13it/s] 29%|██▉       | 128/438 [00:02<00:06, 45.11it/s] 30%|███       | 133/438 [00:02<00:06, 45.26it/s] 32%|███▏      | 138/438 [00:03<00:06, 45.44it/s] 33%|███▎      | 143/438 [00:03<00:06, 45.65it/s] 34%|███▍      | 148/438 [00:03<00:06, 45.61it/s] 35%|███▍      | 153/438 [00:03<00:06, 45.41it/s] 36%|███▌      | 158/438 [00:03<00:07, 38.57it/s] 37%|███▋      | 163/438 [00:03<00:06, 40.56it/s] 38%|███▊      | 168/438 [00:03<00:06, 41.98it/s] 39%|███▉      | 173/438 [00:03<00:06, 43.13it/s] 41%|████      | 178/438 [00:04<00:05, 43.90it/s] 42%|████▏     | 183/438 [00:04<00:05, 44.46it/s] 43%|████▎     | 188/438 [00:04<00:05, 44.75it/s] 44%|████▍     | 193/438 [00:04<00:05, 45.02it/s] 45%|████▌     | 198/438 [00:04<00:05, 44.66it/s] 46%|████▋     | 203/438 [00:04<00:05, 44.58it/s] 47%|████▋     | 208/438 [00:04<00:05, 44.58it/s] 49%|████▊     | 213/438 [00:04<00:05, 44.95it/s] 50%|████▉     | 218/438 [00:04<00:04, 45.26it/s] 51%|█████     | 223/438 [00:05<00:04, 45.48it/s] 52%|█████▏    | 228/438 [00:05<00:04, 45.59it/s] 53%|█████▎    | 233/438 [00:05<00:04, 45.62it/s] 54%|█████▍    | 238/438 [00:05<00:04, 45.51it/s] 55%|█████▌    | 243/438 [00:05<00:04, 45.12it/s] 57%|█████▋    | 248/438 [00:05<00:04, 45.02it/s] 58%|█████▊    | 253/438 [00:05<00:04, 44.95it/s] 59%|█████▉    | 258/438 [00:05<00:03, 45.01it/s] 60%|██████    | 263/438 [00:05<00:03, 45.21it/s] 61%|██████    | 268/438 [00:06<00:03, 45.24it/s] 62%|██████▏   | 273/438 [00:06<00:03, 45.53it/s] 63%|██████▎   | 278/438 [00:06<00:03, 45.55it/s] 65%|██████▍   | 283/438 [00:06<00:03, 45.52it/s] 66%|██████▌   | 288/438 [00:06<00:03, 45.20it/s] 67%|██████▋   | 293/438 [00:06<00:03, 39.89it/s] 68%|██████▊   | 298/438 [00:06<00:03, 41.58it/s] 69%|██████▉   | 303/438 [00:06<00:03, 42.79it/s] 70%|███████   | 308/438 [00:06<00:02, 43.58it/s] 71%|███████▏  | 313/438 [00:07<00:02, 44.28it/s] 73%|███████▎  | 318/438 [00:07<00:02, 44.65it/s] 74%|███████▎  | 323/438 [00:07<00:02, 45.02it/s] 75%|███████▍  | 328/438 [00:07<00:02, 45.18it/s] 76%|███████▌  | 333/438 [00:07<00:02, 44.81it/s] 77%|███████▋  | 338/438 [00:07<00:02, 44.67it/s] 78%|███████▊  | 343/438 [00:07<00:02, 44.80it/s] 79%|███████▉  | 348/438 [00:07<00:02, 44.91it/s] 81%|████████  | 353/438 [00:07<00:01, 45.13it/s] 82%|████████▏ | 358/438 [00:08<00:01, 45.38it/s] 83%|████████▎ | 363/438 [00:08<00:01, 45.33it/s] 84%|████████▍ | 368/438 [00:08<00:01, 45.46it/s] 85%|████████▌ | 373/438 [00:08<00:01, 45.50it/s] 86%|████████▋ | 378/438 [00:08<00:01, 45.18it/s] 87%|████████▋ | 383/438 [00:08<00:01, 44.91it/s] 89%|████████▊ | 388/438 [00:08<00:01, 44.89it/s] 90%|████████▉ | 393/438 [00:08<00:00, 45.07it/s] 91%|█████████ | 398/438 [00:08<00:00, 45.29it/s] 92%|█████████▏| 403/438 [00:09<00:00, 45.38it/s] 93%|█████████▎| 408/438 [00:09<00:00, 45.50it/s] 94%|█████████▍| 413/438 [00:09<00:00, 45.59it/s] 95%|█████████▌| 418/438 [00:09<00:00, 45.41it/s] 97%|█████████▋| 423/438 [00:09<00:00, 45.31it/s] 98%|█████████▊| 428/438 [00:09<00:00, 37.78it/s] 99%|█████████▉| 433/438 [00:09<00:00, 39.81it/s]100%|██████████| 438/438 [00:09<00:00, 41.58it/s]100%|██████████| 438/438 [00:09<00:00, 44.26it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-27 22:55:32,837 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:32,837 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:32,837 >>   eval_loss               =     1.0147
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:32,837 >>   eval_runtime            = 0:00:09.91
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:32,837 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:32,837 >>   eval_samples_per_second =    352.681
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:32,837 >>   eval_steps_per_second   =     44.173
[INFO|trainer_pt_utils.py:913] 2023-08-27 22:55:32,837 >>   perplexity              =     2.7586
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_synthetic_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 648, in main_dual
    path_test=path_dev, labels=labels_dev, mode='all_single', is_eval=True, model_size=model_size)
TypeError: run_eval() missing 1 required positional argument: 'model_size'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_4', 'type': 'train', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_5_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:43<06:32, 43.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:59<03:36, 27.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:17<02:43, 23.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:34<02:04, 20.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:51<01:37, 19.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [02:06<01:11, 17.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:21<00:50, 16.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:37<00:33, 16.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:54<00:16, 16.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:16<00:00, 18.25s/it]Generating: 100%|██████████| 10/10 [03:16<00:00, 19.62s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : director .', 'success_rate': 0.8138020833333334, 'errors': {'', "('\\n', 'director', 'Scott Zandt', 'It was written and directed by Scott Zandt ( a.k.a .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8505434782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 359, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 499, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7439903846153846, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : part of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'too many values to unpack (expected 2)', '(\'1958 contest\', \'part of\', \'\', \'He was succeeded as Dutch representative at its 1958 contest by Johannes Eichhorn with " Allende en seine " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a deal to turn " Ingress " into a mobile game . Head Entity : Ingress , Tail Entity : Ingress Studio .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.7981770833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8764204545454546, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n']
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n"]
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n", "Relation : member of political party . Context : This was the first coalition government which was elected in 1998 , and led by then - Prime Minister Naguib Sawiris of the People 's Alliance . Head Entity : Naguib Sawiris , Tail Entity : People ' Alliance .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Tom Blomkamp\', \'member of political party\', \'\', \'" My Life ( " ; ) is a 2015 English language English language documentary film directed by Tom Blomkamp and starring Emma Thompson and Tom Hardy .\')'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 58, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 96, 'raw': 160}
{'target': 600, 'success': 114, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 154, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 223, 'raw': 384}
{'target': 600, 'success': 245, 'raw': 416}
{'target': 600, 'success': 263, 'raw': 448}
{'target': 600, 'success': 276, 'raw': 480}
{'target': 600, 'success': 297, 'raw': 512}
{'target': 600, 'success': 318, 'raw': 544}
{'target': 600, 'success': 335, 'raw': 576}
{'target': 600, 'success': 359, 'raw': 608}
{'target': 600, 'success': 381, 'raw': 640}
{'target': 600, 'success': 398, 'raw': 672}
{'target': 600, 'success': 410, 'raw': 704}
{'target': 600, 'success': 431, 'raw': 736}
{'target': 600, 'success': 451, 'raw': 768}
{'target': 600, 'success': 470, 'raw': 800}
{'target': 600, 'success': 494, 'raw': 832}
{'target': 600, 'success': 516, 'raw': 864}
{'target': 600, 'success': 533, 'raw': 896}
{'target': 600, 'success': 552, 'raw': 928}
{'target': 600, 'success': 566, 'raw': 960}
{'target': 600, 'success': 584, 'raw': 992}
{'target': 600, 'success': 599, 'raw': 1024}
{'target': 600, 'success': 621, 'raw': 1056}
{'prompt': 'Relation : position held .', 'success_rate': 0.5880681818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 12256
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12356, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_train_large/unseen_5_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [00:25, 25.24s/it]Extractor Estimating: 2it [00:26, 11.25s/it]Extractor Estimating: 3it [00:27,  6.42s/it]Extractor Estimating: 4it [00:28,  4.16s/it]Extractor Estimating: 5it [00:28,  2.89s/it]Extractor Estimating: 6it [00:29,  2.14s/it]Extractor Estimating: 7it [00:30,  1.68s/it]Extractor Estimating: 8it [00:31,  1.54s/it]Extractor Estimating: 9it [00:32,  1.27s/it]Extractor Estimating: 10it [00:32,  1.10s/it]Extractor Estimating: 11it [00:33,  1.05it/s]Extractor Estimating: 12it [00:34,  1.16it/s]Extractor Estimating: 13it [00:34,  1.25it/s]Extractor Estimating: 14it [00:35,  1.27it/s]Extractor Estimating: 15it [00:36,  1.31it/s]Extractor Estimating: 16it [00:36,  1.36it/s]Extractor Estimating: 17it [00:37,  1.38it/s]Extractor Estimating: 18it [00:38,  1.44it/s]Extractor Estimating: 19it [00:39,  1.13it/s]Extractor Estimating: 20it [00:40,  1.18it/s]Extractor Estimating: 21it [00:40,  1.24it/s]Extractor Estimating: 22it [00:41,  1.33it/s]Extractor Estimating: 23it [00:42,  1.38it/s]Extractor Estimating: 24it [00:42,  1.37it/s]Extractor Estimating: 25it [00:43,  1.32it/s]Extractor Estimating: 26it [00:44,  1.40it/s]Extractor Estimating: 27it [00:45,  1.45it/s]Extractor Estimating: 28it [00:45,  1.54it/s]Extractor Estimating: 29it [00:46,  1.54it/s]Extractor Estimating: 30it [00:46,  1.53it/s]Extractor Estimating: 31it [00:47,  1.57it/s]Extractor Estimating: 32it [00:48,  1.58it/s]Extractor Estimating: 33it [00:48,  1.57it/s]Extractor Estimating: 34it [00:49,  1.64it/s]Extractor Estimating: 35it [00:49,  1.63it/s]Extractor Estimating: 36it [00:50,  1.62it/s]Extractor Estimating: 37it [00:51,  1.60it/s]Extractor Estimating: 38it [00:51,  1.59it/s]Extractor Estimating: 39it [00:52,  1.56it/s]Extractor Estimating: 40it [00:53,  1.58it/s]Extractor Estimating: 41it [00:53,  1.56it/s]Extractor Estimating: 42it [00:54,  1.58it/s]Extractor Estimating: 43it [00:54,  1.62it/s]Extractor Estimating: 44it [00:55,  1.62it/s]Extractor Estimating: 45it [00:56,  1.59it/s]Extractor Estimating: 46it [00:56,  1.64it/s]Extractor Estimating: 47it [00:57,  1.61it/s]Extractor Estimating: 48it [00:58,  1.54it/s]Extractor Estimating: 49it [00:58,  1.61it/s]Extractor Estimating: 50it [00:59,  1.62it/s]Extractor Estimating: 51it [01:00,  1.57it/s]Extractor Estimating: 52it [01:00,  1.57it/s]Extractor Estimating: 53it [01:01,  1.60it/s]Extractor Estimating: 54it [01:01,  1.64it/s]Extractor Estimating: 55it [01:02,  1.57it/s]Extractor Estimating: 56it [01:03,  1.57it/s]Extractor Estimating: 57it [01:03,  1.58it/s]Extractor Estimating: 58it [01:04,  1.64it/s]Extractor Estimating: 59it [01:05,  1.60it/s]Extractor Estimating: 60it [01:05,  1.59it/s]Extractor Estimating: 61it [01:08,  1.19s/it]Extractor Estimating: 62it [01:08,  1.02s/it]Extractor Estimating: 63it [01:09,  1.12it/s]Extractor Estimating: 64it [01:10,  1.21it/s]Extractor Estimating: 65it [01:10,  1.26it/s]Extractor Estimating: 66it [01:11,  1.32it/s]Extractor Estimating: 67it [01:12,  1.38it/s]Extractor Estimating: 68it [01:12,  1.42it/s]Extractor Estimating: 69it [01:13,  1.41it/s]Extractor Estimating: 70it [01:14,  1.48it/s]Extractor Estimating: 71it [01:14,  1.51it/s]Extractor Estimating: 72it [01:15,  1.50it/s]Extractor Estimating: 73it [01:16,  1.50it/s]Extractor Estimating: 74it [01:16,  1.48it/s]Extractor Estimating: 75it [01:17,  1.41it/s]Extractor Estimating: 76it [01:18,  1.43it/s]Extractor Estimating: 77it [01:18,  1.46it/s]Extractor Estimating: 78it [01:19,  1.47it/s]Extractor Estimating: 79it [01:20,  1.45it/s]Extractor Estimating: 80it [01:20,  1.52it/s]Extractor Estimating: 81it [01:21,  1.49it/s]Extractor Estimating: 82it [01:22,  1.52it/s]Extractor Estimating: 83it [01:22,  1.49it/s]Extractor Estimating: 84it [01:23,  1.44it/s]Extractor Estimating: 85it [01:24,  1.44it/s]Extractor Estimating: 86it [01:24,  1.48it/s]Extractor Estimating: 87it [01:25,  1.53it/s]Extractor Estimating: 88it [01:26,  1.46it/s]Extractor Estimating: 89it [01:27,  1.42it/s]Extractor Estimating: 90it [01:27,  1.44it/s]Extractor Estimating: 91it [01:28,  1.46it/s]Extractor Estimating: 92it [01:28,  1.49it/s]Extractor Estimating: 93it [01:29,  1.52it/s]Extractor Estimating: 94it [01:30,  1.44it/s]Extractor Estimating: 95it [01:30,  1.52it/s]Extractor Estimating: 96it [01:31,  1.49it/s]Extractor Estimating: 97it [01:32,  1.49it/s]Extractor Estimating: 98it [01:33,  1.50it/s]Extractor Estimating: 99it [01:33,  1.48it/s]Extractor Estimating: 100it [01:34,  1.50it/s]Extractor Estimating: 101it [01:34,  1.54it/s]Extractor Estimating: 102it [01:35,  1.56it/s]Extractor Estimating: 103it [01:36,  1.54it/s]Extractor Estimating: 104it [01:36,  1.47it/s]Extractor Estimating: 105it [01:37,  1.43it/s]Extractor Estimating: 106it [01:38,  1.46it/s]Extractor Estimating: 107it [01:39,  1.48it/s]Extractor Estimating: 108it [01:39,  1.48it/s]Extractor Estimating: 109it [01:40,  1.50it/s]Extractor Estimating: 110it [01:41,  1.48it/s]Extractor Estimating: 111it [01:41,  1.50it/s]Extractor Estimating: 112it [01:42,  1.50it/s]Extractor Estimating: 113it [01:43,  1.53it/s]Extractor Estimating: 114it [01:43,  1.52it/s]Extractor Estimating: 115it [01:44,  1.48it/s]Extractor Estimating: 116it [01:45,  1.49it/s]Extractor Estimating: 117it [01:45,  1.52it/s]Extractor Estimating: 118it [01:46,  1.47it/s]Extractor Estimating: 119it [01:47,  1.53it/s]Extractor Estimating: 120it [01:47,  1.50it/s]Extractor Estimating: 121it [01:48,  1.41it/s]Extractor Estimating: 122it [01:49,  1.48it/s]Extractor Estimating: 123it [01:49,  1.51it/s]Extractor Estimating: 124it [01:50,  1.53it/s]Extractor Estimating: 125it [01:51,  1.51it/s]Extractor Estimating: 126it [01:51,  1.50it/s]Extractor Estimating: 127it [01:52,  1.48it/s]Extractor Estimating: 128it [01:53,  1.50it/s]Extractor Estimating: 129it [01:53,  1.55it/s]Extractor Estimating: 130it [01:54,  1.52it/s]Extractor Estimating: 131it [01:54,  1.58it/s]Extractor Estimating: 132it [01:55,  1.54it/s]Extractor Estimating: 133it [01:56,  1.55it/s]Extractor Estimating: 134it [01:56,  1.52it/s]Extractor Estimating: 135it [01:57,  1.49it/s]Extractor Estimating: 136it [01:58,  1.52it/s]Extractor Estimating: 137it [01:58,  1.56it/s]Extractor Estimating: 138it [01:59,  1.56it/s]Extractor Estimating: 139it [02:00,  1.58it/s]Extractor Estimating: 140it [02:00,  1.50it/s]Extractor Estimating: 141it [02:01,  1.56it/s]Extractor Estimating: 142it [02:02,  1.54it/s]Extractor Estimating: 143it [02:02,  1.54it/s]Extractor Estimating: 144it [02:03,  1.58it/s]Extractor Estimating: 145it [02:04,  1.53it/s]Extractor Estimating: 146it [02:04,  1.52it/s]Extractor Estimating: 147it [02:05,  1.56it/s]Extractor Estimating: 148it [02:05,  1.54it/s]Extractor Estimating: 149it [02:06,  1.57it/s]Extractor Estimating: 150it [02:07,  1.52it/s]Extractor Estimating: 151it [02:07,  1.60it/s]Extractor Estimating: 152it [02:08,  1.59it/s]Extractor Estimating: 153it [02:09,  1.65it/s]Extractor Estimating: 154it [02:09,  1.71it/s]Extractor Estimating: 155it [02:10,  1.75it/s]Extractor Estimating: 156it [02:10,  1.78it/s]Extractor Estimating: 157it [02:11,  1.79it/s]Extractor Estimating: 158it [02:11,  1.69it/s]Extractor Estimating: 159it [02:12,  1.74it/s]Extractor Estimating: 160it [02:12,  1.77it/s]Extractor Estimating: 161it [02:13,  1.84it/s]Extractor Estimating: 162it [02:13,  1.91it/s]Extractor Estimating: 163it [02:14,  1.85it/s]Extractor Estimating: 164it [02:15,  1.76it/s]Extractor Estimating: 165it [02:15,  1.83it/s]Extractor Estimating: 166it [02:16,  1.81it/s]Extractor Estimating: 167it [02:16,  1.89it/s]Extractor Estimating: 168it [02:17,  1.86it/s]Extractor Estimating: 169it [02:17,  1.87it/s]Extractor Estimating: 170it [02:18,  1.76it/s]Extractor Estimating: 171it [02:18,  1.76it/s]Extractor Estimating: 172it [02:19,  1.79it/s]Extractor Estimating: 173it [02:20,  1.83it/s]Extractor Estimating: 174it [02:20,  1.86it/s]Extractor Estimating: 175it [02:21,  1.88it/s]Extractor Estimating: 176it [02:21,  1.66it/s]Extractor Estimating: 177it [02:22,  1.65it/s]Extractor Estimating: 178it [02:23,  1.67it/s]Extractor Estimating: 179it [02:23,  1.61it/s]Extractor Estimating: 180it [02:24,  1.64it/s]Extractor Estimating: 181it [02:24,  1.60it/s]Extractor Estimating: 182it [02:25,  1.64it/s]Extractor Estimating: 183it [02:26,  1.57it/s]Extractor Estimating: 184it [02:26,  1.61it/s]Extractor Estimating: 185it [02:27,  1.60it/s]Extractor Estimating: 186it [02:28,  1.49it/s]Extractor Estimating: 187it [02:28,  1.54it/s]Extractor Estimating: 188it [02:29,  1.58it/s]Extractor Estimating: 189it [02:30,  1.58it/s]Extractor Estimating: 190it [02:30,  1.57it/s]Extractor Estimating: 191it [02:31,  1.47it/s]Extractor Estimating: 192it [02:32,  1.50it/s]Extractor Estimating: 193it [02:32,  1.55it/s]Extractor Estimating: 194it [02:33,  1.58it/s]Extractor Estimating: 195it [02:33,  1.63it/s]Extractor Estimating: 196it [02:34,  1.48it/s]Extractor Estimating: 197it [02:35,  1.56it/s]Extractor Estimating: 198it [02:35,  1.52it/s]Extractor Estimating: 199it [02:36,  1.58it/s]Extractor Estimating: 200it [02:37,  1.60it/s]Extractor Estimating: 201it [02:37,  1.55it/s]Extractor Estimating: 202it [02:38,  1.57it/s]Extractor Estimating: 203it [02:39,  1.41it/s]Extractor Estimating: 204it [02:39,  1.45it/s]Extractor Estimating: 205it [02:40,  1.47it/s]Extractor Estimating: 206it [02:41,  1.49it/s]Extractor Estimating: 207it [02:41,  1.51it/s]Extractor Estimating: 208it [02:42,  1.46it/s]Extractor Estimating: 209it [02:43,  1.48it/s]Extractor Estimating: 210it [02:43,  1.57it/s]Extractor Estimating: 211it [02:44,  1.54it/s]Extractor Estimating: 212it [02:45,  1.56it/s]Extractor Estimating: 213it [02:45,  1.54it/s]Extractor Estimating: 214it [02:46,  1.50it/s]Extractor Estimating: 215it [02:47,  1.55it/s]Extractor Estimating: 216it [02:47,  1.56it/s]Extractor Estimating: 217it [02:48,  1.59it/s]Extractor Estimating: 218it [02:49,  1.54it/s]Extractor Estimating: 219it [02:49,  1.54it/s]Extractor Estimating: 220it [02:50,  1.53it/s]Extractor Estimating: 221it [02:51,  1.54it/s]Extractor Estimating: 222it [02:51,  1.56it/s]Extractor Estimating: 223it [02:52,  1.55it/s]Extractor Estimating: 224it [02:52,  1.62it/s]Extractor Estimating: 225it [02:53,  1.55it/s]Extractor Estimating: 226it [02:54,  1.59it/s]Extractor Estimating: 227it [02:54,  1.49it/s]Extractor Estimating: 228it [02:55,  1.45it/s]Extractor Estimating: 229it [02:56,  1.49it/s]Extractor Estimating: 230it [02:56,  1.56it/s]Extractor Estimating: 231it [02:57,  1.56it/s]Extractor Estimating: 232it [02:58,  1.57it/s]Extractor Estimating: 233it [02:58,  1.51it/s]Extractor Estimating: 234it [02:59,  1.56it/s]Extractor Estimating: 235it [03:00,  1.60it/s]Extractor Estimating: 236it [03:00,  1.64it/s]Extractor Estimating: 237it [03:01,  1.62it/s]Extractor Estimating: 238it [03:01,  1.60it/s]Extractor Estimating: 239it [03:02,  1.62it/s]Extractor Estimating: 240it [03:03,  1.65it/s]Extractor Estimating: 241it [03:03,  1.62it/s]Extractor Estimating: 242it [03:04,  1.70it/s]Extractor Estimating: 243it [03:04,  1.64it/s]Extractor Estimating: 244it [03:05,  1.59it/s]Extractor Estimating: 245it [03:06,  1.58it/s]Extractor Estimating: 246it [03:06,  1.56it/s]Extractor Estimating: 247it [03:07,  1.58it/s]Extractor Estimating: 248it [03:08,  1.63it/s]Extractor Estimating: 249it [03:08,  1.58it/s]Extractor Estimating: 250it [03:09,  1.50it/s]Extractor Estimating: 250it [03:09,  1.32it/s]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/numpy/core/_methods.py:230: RuntimeWarning: invalid value encountered in subtract
  x = asanyarray(arr - arrmean)
wrapper.py:469: RuntimeWarning: invalid value encountered in double_scalars
  std_func = lambda x, mean, std: ((x - mean) / std) if std != 0 else (x - mean)
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5151 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 24873
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24973, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_train_large/unseen_5_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24973, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.379, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.996, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 85, avg_time 0.999, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 185, avg_time 1.019, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 70, avg_time 0.995, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 170, avg_time 2.337, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 55, avg_time 0.991, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 155, avg_time 1.007, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 40, avg_time 0.990, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 140, avg_time 1.003, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 25, avg_time 2.093, loss:nan
g_step 1200, step 125, avg_time 1.016, loss:nan
g_step 1300, step 10, avg_time 0.996, loss:nan
g_step 1400, step 110, avg_time 1.004, loss:nan
g_step 1500, step 210, avg_time 1.005, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 95, avg_time 2.072, loss:nan
g_step 1700, step 195, avg_time 1.000, loss:nan
g_step 1800, step 80, avg_time 0.999, loss:nan
g_step 1900, step 180, avg_time 1.022, loss:nan
g_step 2000, step 65, avg_time 1.010, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 165, avg_time 2.093, loss:nan
g_step 2200, step 50, avg_time 1.018, loss:nan
g_step 2300, step 150, avg_time 1.015, loss:nan
g_step 2400, step 35, avg_time 1.001, loss:nan
g_step 2500, step 135, avg_time 1.005, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 20, avg_time 2.089, loss:nan
g_step 2700, step 120, avg_time 1.033, loss:nan
g_step 2800, step 5, avg_time 1.013, loss:nan
g_step 2900, step 105, avg_time 1.016, loss:nan
g_step 3000, step 205, avg_time 1.018, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 90, avg_time 2.091, loss:nan
g_step 3200, step 190, avg_time 1.015, loss:nan
g_step 3300, step 75, avg_time 0.997, loss:nan
g_step 3400, step 175, avg_time 1.025, loss:nan
g_step 3500, step 60, avg_time 1.013, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 160, avg_time 2.085, loss:nan
g_step 3700, step 45, avg_time 1.017, loss:nan
g_step 3800, step 145, avg_time 1.013, loss:nan
g_step 3900, step 30, avg_time 1.018, loss:nan
g_step 4000, step 130, avg_time 1.014, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 15, avg_time 2.089, loss:nan
g_step 4200, step 115, avg_time 1.018, loss:nan
g_step 4300, step 215, avg_time 1.016, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 00:51:43 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 00:51:43 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_00-51-42_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 00:51:46 - WARNING - datasets.builder -   Using custom data configuration default-d73a9f9910b36037
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-d73a9f9910b36037/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 00:51:54,692 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:51:54,804 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 00:51:54,805 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 00:51:54,806 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 00:51:55,098 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:51:55,486 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:51:55,486 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:51:55,486 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:51:55,486 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:51:55,486 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 00:51:55,486 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 00:51:56,153 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 00:51:59,429 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 00:51:59,519 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-d73a9f9910b36037/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 00:51:59 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x149de0754200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:01<00:05,  1.19s/ba] 33%|███▎      | 2/6 [00:01<00:02,  1.62ba/s] 50%|█████     | 3/6 [00:01<00:01,  2.29ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.84ba/s] 83%|████████▎ | 5/6 [00:02<00:00,  3.28ba/s]100%|██████████| 6/6 [00:02<00:00,  2.84ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.72ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.69ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.82ba/s]100%|██████████| 4/4 [00:01<00:00,  3.85ba/s]100%|██████████| 4/4 [00:01<00:00,  3.19ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.00ba/s] 50%|█████     | 3/6 [00:00<00:00,  6.46ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  8.25ba/s]100%|██████████| 6/6 [00:00<00:00,  8.41ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.92ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.57ba/s]100%|██████████| 4/4 [00:00<00:00,  6.67ba/s]100%|██████████| 4/4 [00:00<00:00,  5.14ba/s]
[INFO|trainer.py:414] 2023-08-28 00:52:09,589 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 00:52:09,919 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 00:52:09,920 >>   Num examples = 5160
[INFO|trainer.py:1149] 2023-08-28 00:52:09,920 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 00:52:09,920 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 00:52:09,920 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 00:52:09,920 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 00:52:09,920 >>   Total optimization steps = 405
  0%|          | 0/405 [00:00<?, ?it/s]  0%|          | 1/405 [00:00<01:55,  3.49it/s]  0%|          | 2/405 [00:00<02:18,  2.91it/s]  1%|          | 3/405 [00:00<02:05,  3.19it/s]  1%|          | 4/405 [00:01<01:59,  3.34it/s]  1%|          | 5/405 [00:01<01:56,  3.43it/s]  1%|▏         | 6/405 [00:01<01:54,  3.49it/s]  2%|▏         | 7/405 [00:02<01:52,  3.52it/s]  2%|▏         | 8/405 [00:02<01:51,  3.54it/s]  2%|▏         | 9/405 [00:02<01:51,  3.56it/s]  2%|▏         | 10/405 [00:02<01:50,  3.57it/s]  3%|▎         | 11/405 [00:03<01:50,  3.57it/s]  3%|▎         | 12/405 [00:03<01:49,  3.58it/s]  3%|▎         | 13/405 [00:03<02:01,  3.21it/s]  3%|▎         | 14/405 [00:04<01:57,  3.32it/s]  4%|▎         | 15/405 [00:04<01:54,  3.39it/s]  4%|▍         | 16/405 [00:04<01:52,  3.45it/s]  4%|▍         | 17/405 [00:04<01:51,  3.49it/s]  4%|▍         | 18/405 [00:05<01:50,  3.51it/s]  5%|▍         | 19/405 [00:05<01:49,  3.54it/s]  5%|▍         | 20/405 [00:05<01:48,  3.55it/s]  5%|▌         | 21/405 [00:06<01:47,  3.57it/s]  5%|▌         | 22/405 [00:06<01:47,  3.57it/s]  6%|▌         | 23/405 [00:06<01:46,  3.58it/s]  6%|▌         | 24/405 [00:06<01:46,  3.58it/s]  6%|▌         | 25/405 [00:07<01:46,  3.58it/s]  6%|▋         | 26/405 [00:07<01:59,  3.18it/s]  7%|▋         | 27/405 [00:07<01:54,  3.29it/s]  7%|▋         | 28/405 [00:08<01:51,  3.37it/s]  7%|▋         | 29/405 [00:08<01:49,  3.43it/s]  7%|▋         | 30/405 [00:08<01:47,  3.48it/s]  8%|▊         | 31/405 [00:08<01:46,  3.51it/s]  8%|▊         | 32/405 [00:09<01:45,  3.53it/s]  8%|▊         | 33/405 [00:09<01:44,  3.55it/s]  8%|▊         | 34/405 [00:09<01:44,  3.56it/s]  9%|▊         | 35/405 [00:10<01:43,  3.56it/s]  9%|▉         | 36/405 [00:10<01:43,  3.57it/s]  9%|▉         | 37/405 [00:10<01:52,  3.27it/s]  9%|▉         | 38/405 [00:11<01:49,  3.36it/s] 10%|▉         | 39/405 [00:11<01:47,  3.42it/s] 10%|▉         | 40/405 [00:11<01:45,  3.46it/s] 10%|█         | 41/405 [00:11<01:44,  3.49it/s] 10%|█         | 42/405 [00:12<01:43,  3.52it/s] 11%|█         | 43/405 [00:12<01:42,  3.54it/s] 11%|█         | 44/405 [00:12<01:41,  3.55it/s] 11%|█         | 45/405 [00:12<01:41,  3.56it/s] 11%|█▏        | 46/405 [00:13<01:40,  3.56it/s] 12%|█▏        | 47/405 [00:13<01:40,  3.57it/s] 12%|█▏        | 48/405 [00:13<01:48,  3.29it/s] 12%|█▏        | 49/405 [00:14<01:45,  3.37it/s] 12%|█▏        | 50/405 [00:14<01:43,  3.43it/s] 13%|█▎        | 51/405 [00:14<01:42,  3.47it/s] 13%|█▎        | 52/405 [00:15<01:40,  3.50it/s] 13%|█▎        | 53/405 [00:15<01:40,  3.52it/s] 13%|█▎        | 54/405 [00:15<01:39,  3.54it/s] 14%|█▎        | 55/405 [00:15<01:38,  3.55it/s] 14%|█▍        | 56/405 [00:16<01:38,  3.55it/s] 14%|█▍        | 57/405 [00:16<01:37,  3.56it/s] 14%|█▍        | 58/405 [00:16<01:37,  3.57it/s] 15%|█▍        | 59/405 [00:17<01:46,  3.24it/s] 15%|█▍        | 60/405 [00:17<01:43,  3.33it/s] 15%|█▌        | 61/405 [00:17<01:41,  3.40it/s] 15%|█▌        | 62/405 [00:17<01:39,  3.45it/s] 16%|█▌        | 63/405 [00:18<01:38,  3.48it/s] 16%|█▌        | 64/405 [00:18<01:37,  3.51it/s] 16%|█▌        | 65/405 [00:18<01:36,  3.53it/s] 16%|█▋        | 66/405 [00:19<01:35,  3.54it/s] 17%|█▋        | 67/405 [00:19<01:35,  3.55it/s] 17%|█▋        | 68/405 [00:19<01:34,  3.55it/s] 17%|█▋        | 69/405 [00:19<01:34,  3.56it/s] 17%|█▋        | 70/405 [00:20<01:41,  3.29it/s] 18%|█▊        | 71/405 [00:20<01:39,  3.37it/s] 18%|█▊        | 72/405 [00:20<01:37,  3.43it/s] 18%|█▊        | 73/405 [00:21<01:35,  3.49it/s] 18%|█▊        | 74/405 [00:21<01:33,  3.53it/s] 19%|█▊        | 75/405 [00:21<01:32,  3.56it/s] 19%|█▉        | 76/405 [00:21<01:31,  3.58it/s] 19%|█▉        | 77/405 [00:22<01:31,  3.58it/s] 19%|█▉        | 78/405 [00:22<01:31,  3.58it/s] 20%|█▉        | 79/405 [00:22<01:31,  3.57it/s] 20%|█▉        | 80/405 [00:23<01:30,  3.57it/s] 20%|██        | 81/405 [00:23<01:30,  3.59it/s][INFO|trainer.py:2140] 2023-08-28 00:52:33,209 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:52:33,210 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 00:52:33,210 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.35it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.07it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.52it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.83it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.36it/s][A
  7%|▋         | 32/438 [00:00<00:08, 46.07it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.35it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.82it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.72it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.82it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.97it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.99it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.12it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.24it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.21it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.06it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.61it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.69it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.69it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.92it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.99it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.10it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.13it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 45.14it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.98it/s][A
 30%|███       | 132/438 [00:02<00:08, 36.85it/s][A
 31%|███▏      | 137/438 [00:03<00:07, 39.12it/s][A
 32%|███▏      | 142/438 [00:03<00:07, 40.74it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 42.11it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 43.14it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 43.82it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.32it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.51it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.25it/s][A
 40%|████      | 177/438 [00:03<00:05, 43.98it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.20it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.51it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.78it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.04it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.17it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.16it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 45.05it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.72it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.41it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.57it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.65it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.94it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 45.05it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 45.24it/s][A
 58%|█████▊    | 252/438 [00:05<00:05, 33.90it/s][A
 59%|█████▉    | 258/438 [00:06<00:04, 38.10it/s][A
 60%|██████    | 263/438 [00:06<00:05, 29.54it/s][A
 61%|██████    | 268/438 [00:06<00:05, 32.89it/s][A
 62%|██████▏   | 273/438 [00:06<00:04, 35.83it/s][A
 63%|██████▎   | 278/438 [00:06<00:04, 38.25it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 40.12it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 41.61it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 42.71it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 43.35it/s][A
 69%|██████▉   | 303/438 [00:07<00:03, 43.43it/s][A
 70%|███████   | 308/438 [00:07<00:02, 43.54it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 43.67it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 44.06it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 44.43it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 44.82it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.06it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 45.14it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 45.14it/s][A
 79%|███████▉  | 348/438 [00:08<00:02, 44.85it/s][A
 81%|████████  | 353/438 [00:08<00:01, 44.49it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 44.44it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 44.47it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 44.75it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.93it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 45.01it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 45.22it/s][A
 89%|████████▊ | 388/438 [00:09<00:01, 45.19it/s][A
 90%|████████▉ | 393/438 [00:09<00:01, 34.58it/s][A
 91%|█████████ | 398/438 [00:09<00:01, 37.35it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 39.44it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 40.98it/s][A
 94%|█████████▍| 413/438 [00:10<00:02, 11.11it/s][A
 95%|█████████▌| 418/438 [00:10<00:01, 14.40it/s][A
 97%|█████████▋| 423/438 [00:10<00:00, 18.10it/s][A
 98%|█████████▊| 428/438 [00:11<00:00, 22.10it/s][A
 99%|█████████▉| 433/438 [00:11<00:00, 26.10it/s][A
100%|██████████| 438/438 [00:11<00:00, 29.90it/s][A                                                
                                                 [A 20%|██        | 81/405 [00:34<01:30,  3.59it/s]
100%|██████████| 438/438 [00:11<00:00, 29.90it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:52:46,077 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81
[INFO|configuration_utils.py:351] 2023-08-28 00:52:46,922 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:53:18,742 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:53:19,998 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:53:20,290 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81/special_tokens_map.json
 20%|██        | 82/405 [01:14<1:23:43, 15.55s/it] 20%|██        | 83/405 [01:14<59:04, 11.01s/it]   21%|██        | 84/405 [01:15<41:39,  7.79s/it] 21%|██        | 85/405 [01:15<29:31,  5.53s/it] 21%|██        | 86/405 [01:15<21:02,  3.96s/it] 21%|██▏       | 87/405 [01:15<15:07,  2.85s/it] 22%|██▏       | 88/405 [01:16<10:59,  2.08s/it] 22%|██▏       | 89/405 [01:16<08:06,  1.54s/it] 22%|██▏       | 90/405 [01:16<06:05,  1.16s/it] 22%|██▏       | 91/405 [01:17<04:41,  1.12it/s] 23%|██▎       | 92/405 [01:17<03:42,  1.41it/s] 23%|██▎       | 93/405 [01:17<03:01,  1.72it/s] 23%|██▎       | 94/405 [01:18<02:47,  1.86it/s] 23%|██▎       | 95/405 [01:18<02:22,  2.18it/s] 24%|██▎       | 96/405 [01:18<02:04,  2.48it/s] 24%|██▍       | 97/405 [01:18<01:52,  2.74it/s] 24%|██▍       | 98/405 [01:19<01:43,  2.96it/s] 24%|██▍       | 99/405 [01:19<01:37,  3.14it/s] 25%|██▍       | 100/405 [01:19<01:33,  3.28it/s] 25%|██▍       | 101/405 [01:20<01:29,  3.38it/s] 25%|██▌       | 102/405 [01:20<01:27,  3.46it/s] 25%|██▌       | 103/405 [01:20<01:26,  3.51it/s] 26%|██▌       | 104/405 [01:20<01:24,  3.55it/s] 26%|██▌       | 105/405 [01:21<01:34,  3.16it/s] 26%|██▌       | 106/405 [01:21<01:30,  3.29it/s] 26%|██▋       | 107/405 [01:21<01:27,  3.39it/s] 27%|██▋       | 108/405 [01:22<01:25,  3.46it/s] 27%|██▋       | 109/405 [01:22<01:24,  3.52it/s] 27%|██▋       | 110/405 [01:22<01:23,  3.55it/s] 27%|██▋       | 111/405 [01:22<01:22,  3.58it/s] 28%|██▊       | 112/405 [01:23<01:21,  3.60it/s] 28%|██▊       | 113/405 [01:23<01:20,  3.61it/s] 28%|██▊       | 114/405 [01:23<01:20,  3.62it/s] 28%|██▊       | 115/405 [01:23<01:19,  3.63it/s] 29%|██▊       | 116/405 [01:24<01:38,  2.94it/s] 29%|██▉       | 117/405 [01:24<01:32,  3.12it/s] 29%|██▉       | 118/405 [01:25<01:28,  3.26it/s] 29%|██▉       | 119/405 [01:25<01:25,  3.36it/s] 30%|██▉       | 120/405 [01:25<01:22,  3.45it/s] 30%|██▉       | 121/405 [01:25<01:21,  3.50it/s] 30%|███       | 122/405 [01:26<01:19,  3.54it/s] 30%|███       | 123/405 [01:26<01:18,  3.57it/s] 31%|███       | 124/405 [01:26<01:18,  3.59it/s] 31%|███       | 125/405 [01:26<01:17,  3.61it/s] 31%|███       | 126/405 [01:27<01:17,  3.62it/s] 31%|███▏      | 127/405 [01:27<01:27,  3.19it/s] 32%|███▏      | 128/405 [01:27<01:23,  3.31it/s] 32%|███▏      | 129/405 [01:28<01:21,  3.40it/s] 32%|███▏      | 130/405 [01:28<01:19,  3.47it/s] 32%|███▏      | 131/405 [01:28<01:17,  3.52it/s] 33%|███▎      | 132/405 [01:29<01:29,  3.05it/s] 33%|███▎      | 133/405 [01:29<01:25,  3.20it/s] 33%|███▎      | 134/405 [01:29<01:21,  3.32it/s] 33%|███▎      | 135/405 [01:29<01:19,  3.41it/s] 34%|███▎      | 136/405 [01:30<01:17,  3.47it/s] 34%|███▍      | 137/405 [01:30<01:30,  2.95it/s] 34%|███▍      | 138/405 [01:30<01:25,  3.13it/s] 34%|███▍      | 139/405 [01:31<01:21,  3.27it/s] 35%|███▍      | 140/405 [01:31<01:18,  3.37it/s] 35%|███▍      | 141/405 [01:31<01:16,  3.44it/s] 35%|███▌      | 142/405 [01:32<01:15,  3.50it/s] 35%|███▌      | 143/405 [01:32<01:14,  3.54it/s] 36%|███▌      | 144/405 [01:32<01:13,  3.57it/s] 36%|███▌      | 145/405 [01:34<03:09,  1.37it/s] 36%|███▌      | 146/405 [01:34<02:42,  1.60it/s] 36%|███▋      | 147/405 [01:35<02:14,  1.91it/s] 37%|███▋      | 148/405 [01:35<01:55,  2.23it/s] 37%|███▋      | 149/405 [01:35<01:41,  2.51it/s] 37%|███▋      | 150/405 [01:35<01:32,  2.76it/s] 37%|███▋      | 151/405 [01:36<01:25,  2.96it/s] 38%|███▊      | 152/405 [01:36<01:20,  3.12it/s] 38%|███▊      | 153/405 [01:36<01:22,  3.04it/s] 38%|███▊      | 154/405 [01:37<01:18,  3.19it/s] 38%|███▊      | 155/405 [01:37<01:15,  3.30it/s] 39%|███▊      | 156/405 [01:37<01:13,  3.38it/s] 39%|███▉      | 157/405 [01:37<01:12,  3.44it/s] 39%|███▉      | 158/405 [01:38<01:10,  3.48it/s] 39%|███▉      | 159/405 [01:38<01:10,  3.51it/s] 40%|███▉      | 160/405 [01:38<01:09,  3.54it/s] 40%|███▉      | 161/405 [01:39<01:08,  3.55it/s] 40%|████      | 162/405 [01:39<01:01,  3.95it/s][INFO|trainer.py:2140] 2023-08-28 00:53:49,159 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:53:49,159 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 00:53:49,159 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 11.2708, 'eval_samples_per_second': 310.27, 'eval_steps_per_second': 38.861, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.63it/s][A
  3%|▎         | 12/438 [00:00<00:13, 31.14it/s][A
  4%|▍         | 17/438 [00:00<00:11, 35.78it/s][A
  5%|▌         | 22/438 [00:00<00:10, 38.94it/s][A
  6%|▌         | 27/438 [00:00<00:10, 41.00it/s][A
  7%|▋         | 32/438 [00:00<00:09, 42.36it/s][A
  8%|▊         | 37/438 [00:00<00:09, 43.29it/s][A
 10%|▉         | 42/438 [00:01<00:09, 43.93it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.30it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.15it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.13it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.41it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.67it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.91it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 45.08it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.21it/s][A
 20%|█▉        | 87/438 [00:02<00:07, 45.24it/s][A
 21%|██        | 92/438 [00:02<00:07, 45.09it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.81it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.68it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.79it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.97it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.09it/s][A
 28%|██▊       | 122/438 [00:02<00:06, 45.21it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 45.25it/s][A
 30%|███       | 132/438 [00:03<00:06, 45.21it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 45.07it/s][A
 32%|███▏      | 142/438 [00:03<00:07, 37.17it/s][A
 34%|███▎      | 147/438 [00:03<00:07, 39.37it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 41.00it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 42.34it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 43.25it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 43.94it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.37it/s][A
 40%|████      | 177/438 [00:04<00:05, 44.55it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.24it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.20it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.38it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.56it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.89it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.04it/s][A
 48%|████▊     | 212/438 [00:04<00:04, 45.21it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.32it/s][A
 51%|█████     | 222/438 [00:05<00:04, 45.18it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.90it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.68it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.59it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.73it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.90it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 45.04it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 45.21it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 45.31it/s][A
 61%|██████    | 267/438 [00:06<00:03, 45.16it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.89it/s][A
 63%|██████▎   | 277/438 [00:06<00:04, 37.79it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 39.77it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 41.39it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 42.54it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 43.37it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 43.69it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.49it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.59it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.40it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.16it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.43it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.68it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.82it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 45.09it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 45.15it/s][A
 80%|████████  | 352/438 [00:08<00:01, 45.31it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.09it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.60it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.56it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.58it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.61it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.85it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 45.10it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 45.17it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 45.25it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 45.16it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.89it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 40.13it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 41.60it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 42.57it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.50it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.05it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.48it/s][A                                                 
                                                 [A 40%|████      | 162/405 [01:49<01:01,  3.95it/s]
100%|██████████| 438/438 [00:10<00:00, 44.48it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:53:59,796 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162
[INFO|configuration_utils.py:351] 2023-08-28 00:54:00,532 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:54:27,432 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:54:28,553 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:54:28,895 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162/special_tokens_map.json
 40%|████      | 163/405 [02:23<53:42, 13.32s/it] 40%|████      | 164/405 [02:23<37:51,  9.43s/it] 41%|████      | 165/405 [02:23<26:43,  6.68s/it] 41%|████      | 166/405 [02:23<18:57,  4.76s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/405 [02:24<14:07,  3.56s/it] 41%|████▏     | 168/405 [02:24<10:10,  2.58s/it] 42%|████▏     | 169/405 [02:25<07:25,  1.89s/it] 42%|████▏     | 170/405 [02:25<05:29,  1.40s/it] 42%|████▏     | 171/405 [02:25<04:08,  1.06s/it] 42%|████▏     | 172/405 [02:26<03:12,  1.21it/s] 43%|████▎     | 173/405 [02:26<02:39,  1.45it/s] 43%|████▎     | 174/405 [02:26<02:10,  1.77it/s] 43%|████▎     | 175/405 [02:26<01:49,  2.10it/s] 43%|████▎     | 176/405 [02:27<01:35,  2.40it/s] 44%|████▎     | 177/405 [02:27<01:25,  2.68it/s] 44%|████▍     | 178/405 [02:27<01:18,  2.91it/s] 44%|████▍     | 179/405 [02:28<01:13,  3.10it/s] 44%|████▍     | 180/405 [02:28<01:09,  3.25it/s] 45%|████▍     | 181/405 [02:28<01:06,  3.35it/s] 45%|████▍     | 182/405 [02:28<01:04,  3.44it/s] 45%|████▌     | 183/405 [02:29<01:07,  3.28it/s] 45%|████▌     | 184/405 [02:29<01:09,  3.19it/s] 46%|████▌     | 185/405 [02:29<01:06,  3.31it/s] 46%|████▌     | 186/405 [02:30<01:04,  3.40it/s] 46%|████▌     | 187/405 [02:30<01:02,  3.47it/s] 46%|████▋     | 188/405 [02:30<01:01,  3.52it/s] 47%|████▋     | 189/405 [02:30<01:00,  3.56it/s] 47%|████▋     | 190/405 [02:31<00:59,  3.58it/s] 47%|████▋     | 191/405 [02:31<00:59,  3.60it/s] 47%|████▋     | 192/405 [02:31<00:58,  3.62it/s] 48%|████▊     | 193/405 [02:32<00:58,  3.62it/s] 48%|████▊     | 194/405 [02:32<00:58,  3.63it/s] 48%|████▊     | 195/405 [02:32<01:02,  3.38it/s] 48%|████▊     | 196/405 [02:32<01:04,  3.26it/s] 49%|████▊     | 197/405 [02:33<01:04,  3.20it/s] 49%|████▉     | 198/405 [02:33<01:11,  2.90it/s] 49%|████▉     | 199/405 [02:34<01:24,  2.42it/s] 49%|████▉     | 200/405 [02:34<01:20,  2.53it/s] 50%|████▉     | 201/405 [02:34<01:13,  2.77it/s] 50%|████▉     | 202/405 [02:35<01:08,  2.99it/s] 50%|█████     | 203/405 [02:35<01:03,  3.16it/s] 50%|█████     | 204/405 [02:35<01:03,  3.15it/s] 51%|█████     | 205/405 [02:36<01:00,  3.28it/s] 51%|█████     | 206/405 [02:36<00:58,  3.38it/s] 51%|█████     | 207/405 [02:36<00:57,  3.46it/s] 51%|█████▏    | 208/405 [02:36<00:56,  3.51it/s] 52%|█████▏    | 209/405 [02:37<00:55,  3.55it/s] 52%|█████▏    | 210/405 [02:37<00:54,  3.58it/s] 52%|█████▏    | 211/405 [02:37<00:53,  3.60it/s] 52%|█████▏    | 212/405 [02:38<00:53,  3.61it/s] 53%|█████▎    | 213/405 [02:38<00:53,  3.62it/s] 53%|█████▎    | 214/405 [02:38<00:52,  3.62it/s] 53%|█████▎    | 215/405 [02:38<00:52,  3.63it/s] 53%|█████▎    | 216/405 [02:39<00:51,  3.64it/s] 54%|█████▎    | 217/405 [02:39<00:51,  3.64it/s] 54%|█████▍    | 218/405 [02:39<00:51,  3.64it/s] 54%|█████▍    | 219/405 [02:39<00:51,  3.65it/s] 54%|█████▍    | 220/405 [02:40<00:50,  3.64it/s] 55%|█████▍    | 221/405 [02:40<00:50,  3.65it/s] 55%|█████▍    | 222/405 [02:40<00:50,  3.64it/s] 55%|█████▌    | 223/405 [02:41<00:52,  3.47it/s] 55%|█████▌    | 224/405 [02:41<00:51,  3.52it/s] 56%|█████▌    | 225/405 [02:41<00:50,  3.55it/s] 56%|█████▌    | 226/405 [02:41<00:50,  3.57it/s] 56%|█████▌    | 227/405 [02:42<00:49,  3.59it/s] 56%|█████▋    | 228/405 [02:42<00:49,  3.61it/s] 57%|█████▋    | 229/405 [02:42<00:48,  3.61it/s] 57%|█████▋    | 230/405 [02:42<00:48,  3.61it/s] 57%|█████▋    | 231/405 [02:43<00:48,  3.62it/s] 57%|█████▋    | 232/405 [02:43<00:47,  3.63it/s] 58%|█████▊    | 233/405 [02:43<00:47,  3.63it/s] 58%|█████▊    | 234/405 [02:44<00:47,  3.62it/s] 58%|█████▊    | 235/405 [02:44<00:46,  3.63it/s] 58%|█████▊    | 236/405 [02:44<00:46,  3.63it/s] 59%|█████▊    | 237/405 [02:44<00:46,  3.64it/s] 59%|█████▉    | 238/405 [02:45<00:45,  3.63it/s] 59%|█████▉    | 239/405 [02:45<00:45,  3.64it/s] 59%|█████▉    | 240/405 [02:45<00:45,  3.64it/s] 60%|█████▉    | 241/405 [02:46<00:45,  3.64it/s] 60%|█████▉    | 242/405 [02:46<00:44,  3.64it/s] 60%|██████    | 243/405 [02:46<00:40,  4.05it/s][INFO|trainer.py:2140] 2023-08-28 00:54:56,405 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:54:56,405 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 00:54:56,405 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.0184, 'eval_samples_per_second': 349.059, 'eval_steps_per_second': 43.72, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 55.98it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.25it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.68it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.85it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.34it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.93it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.38it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.98it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.87it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.88it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.13it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.26it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.33it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.33it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.13it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.83it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.65it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.75it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.88it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 45.14it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 45.27it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.37it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.34it/s][A
 28%|██▊       | 122/438 [00:02<00:06, 45.15it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.87it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.80it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.85it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.94it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 45.13it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.25it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.33it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.20it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 45.02it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.82it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.79it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.73it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.90it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.11it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.27it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.24it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.19it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 45.01it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.89it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.87it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.89it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 45.04it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 45.13it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 45.24it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 45.20it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 45.02it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.80it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.73it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.78it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.92it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 45.02it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 45.10it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 45.15it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.18it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 36.82it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 39.10it/s][A
 70%|███████   | 307/438 [00:06<00:03, 40.84it/s][A
 71%|███████   | 312/438 [00:06<00:02, 42.16it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 43.13it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 43.85it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.24it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.49it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.30it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.13it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.38it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.61it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 44.83it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.02it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.20it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 45.21it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 45.14it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.95it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.66it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.77it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.76it/s][A
 92%|█████████▏| 402/438 [00:08<00:00, 44.90it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 45.08it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 45.21it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 45.26it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 45.08it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.87it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 38.19it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 40.15it/s][A                                                 
                                                 [A 60%|██████    | 243/405 [02:56<00:40,  4.05it/s]
100%|██████████| 438/438 [00:09<00:00, 40.15it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:55:06,803 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243
[INFO|configuration_utils.py:351] 2023-08-28 00:55:07,595 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:55:16,324 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:55:16,401 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:55:16,422 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243/special_tokens_map.json
 60%|██████    | 244/405 [03:07<17:05,  6.37s/it] 60%|██████    | 245/405 [03:07<12:07,  4.54s/it] 61%|██████    | 246/405 [03:07<08:38,  3.26s/it] 61%|██████    | 247/405 [03:07<06:14,  2.37s/it] 61%|██████    | 248/405 [03:08<04:33,  1.74s/it] 61%|██████▏   | 249/405 [03:08<03:23,  1.30s/it] 62%|██████▏   | 250/405 [03:08<02:34,  1.00it/s] 62%|██████▏   | 251/405 [03:09<02:00,  1.28it/s] 62%|██████▏   | 252/405 [03:09<01:36,  1.59it/s] 62%|██████▏   | 253/405 [03:09<01:19,  1.90it/s] 63%|██████▎   | 254/405 [03:10<01:16,  1.98it/s] 63%|██████▎   | 255/405 [03:10<01:05,  2.29it/s] 63%|██████▎   | 256/405 [03:10<00:58,  2.57it/s] 63%|██████▎   | 257/405 [03:10<00:52,  2.82it/s] 64%|██████▎   | 258/405 [03:11<00:48,  3.02it/s] 64%|██████▍   | 259/405 [03:11<00:45,  3.18it/s] 64%|██████▍   | 260/405 [03:11<00:43,  3.30it/s] 64%|██████▍   | 261/405 [03:12<00:42,  3.40it/s] 65%|██████▍   | 262/405 [03:12<00:41,  3.47it/s] 65%|██████▍   | 263/405 [03:12<00:40,  3.52it/s] 65%|██████▌   | 264/405 [03:12<00:39,  3.55it/s] 65%|██████▌   | 265/405 [03:13<00:44,  3.15it/s] 66%|██████▌   | 266/405 [03:13<00:42,  3.28it/s] 66%|██████▌   | 267/405 [03:13<00:40,  3.38it/s] 66%|██████▌   | 268/405 [03:14<00:39,  3.45it/s] 66%|██████▋   | 269/405 [03:14<00:38,  3.50it/s] 67%|██████▋   | 270/405 [03:14<00:38,  3.54it/s] 67%|██████▋   | 271/405 [03:14<00:37,  3.57it/s] 67%|██████▋   | 272/405 [03:15<00:37,  3.59it/s] 67%|██████▋   | 273/405 [03:15<00:36,  3.60it/s] 68%|██████▊   | 274/405 [03:15<00:36,  3.61it/s] 68%|██████▊   | 275/405 [03:16<00:35,  3.62it/s] 68%|██████▊   | 276/405 [03:16<00:39,  3.30it/s] 68%|██████▊   | 277/405 [03:16<00:37,  3.40it/s] 69%|██████▊   | 278/405 [03:16<00:36,  3.47it/s] 69%|██████▉   | 279/405 [03:17<00:35,  3.51it/s] 69%|██████▉   | 280/405 [03:17<00:35,  3.55it/s] 69%|██████▉   | 281/405 [03:17<00:34,  3.57it/s] 70%|██████▉   | 282/405 [03:18<00:34,  3.59it/s] 70%|██████▉   | 283/405 [03:18<00:33,  3.60it/s] 70%|███████   | 284/405 [03:18<00:33,  3.61it/s] 70%|███████   | 285/405 [03:18<00:33,  3.62it/s] 71%|███████   | 286/405 [03:19<00:32,  3.62it/s] 71%|███████   | 287/405 [03:19<00:38,  3.10it/s] 71%|███████   | 288/405 [03:19<00:36,  3.24it/s] 71%|███████▏  | 289/405 [03:20<00:34,  3.35it/s] 72%|███████▏  | 290/405 [03:20<00:33,  3.43it/s] 72%|███████▏  | 291/405 [03:20<00:32,  3.49it/s] 72%|███████▏  | 292/405 [03:20<00:32,  3.53it/s] 72%|███████▏  | 293/405 [03:21<00:31,  3.56it/s] 73%|███████▎  | 294/405 [03:21<00:30,  3.58it/s] 73%|███████▎  | 295/405 [03:21<00:30,  3.60it/s] 73%|███████▎  | 296/405 [03:22<00:30,  3.61it/s] 73%|███████▎  | 297/405 [03:22<00:29,  3.61it/s] 74%|███████▎  | 298/405 [03:22<00:33,  3.22it/s] 74%|███████▍  | 299/405 [03:22<00:31,  3.34it/s] 74%|███████▍  | 300/405 [03:23<00:30,  3.41it/s] 74%|███████▍  | 301/405 [03:23<00:29,  3.48it/s] 75%|███████▍  | 302/405 [03:23<00:29,  3.52it/s] 75%|███████▍  | 303/405 [03:24<00:28,  3.56it/s] 75%|███████▌  | 304/405 [03:24<00:28,  3.57it/s] 75%|███████▌  | 305/405 [03:24<00:27,  3.59it/s] 76%|███████▌  | 306/405 [03:24<00:27,  3.60it/s] 76%|███████▌  | 307/405 [03:25<00:27,  3.62it/s] 76%|███████▌  | 308/405 [03:25<00:26,  3.62it/s] 76%|███████▋  | 309/405 [03:25<00:30,  3.19it/s] 77%|███████▋  | 310/405 [03:26<00:28,  3.31it/s] 77%|███████▋  | 311/405 [03:26<00:27,  3.40it/s] 77%|███████▋  | 312/405 [03:26<00:26,  3.47it/s] 77%|███████▋  | 313/405 [03:26<00:26,  3.51it/s] 78%|███████▊  | 314/405 [03:27<00:25,  3.55it/s] 78%|███████▊  | 315/405 [03:27<00:25,  3.58it/s] 78%|███████▊  | 316/405 [03:27<00:24,  3.59it/s] 78%|███████▊  | 317/405 [03:28<00:24,  3.60it/s] 79%|███████▊  | 318/405 [03:28<00:24,  3.61it/s] 79%|███████▉  | 319/405 [03:28<00:23,  3.62it/s] 79%|███████▉  | 320/405 [03:28<00:25,  3.32it/s] 79%|███████▉  | 321/405 [03:29<00:31,  2.65it/s] 80%|███████▉  | 322/405 [03:29<00:28,  2.86it/s] 80%|███████▉  | 323/405 [03:30<00:26,  3.06it/s] 80%|████████  | 324/405 [03:30<00:23,  3.52it/s][INFO|trainer.py:2140] 2023-08-28 00:55:40,202 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:55:40,202 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 00:55:40,202 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 9.8553, 'eval_samples_per_second': 354.833, 'eval_steps_per_second': 44.443, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.13it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.03it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.59it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.66it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.25it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.73it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.16it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.72it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.71it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.94it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.07it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.17it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.27it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.11it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 45.06it/s][A
 19%|█▊        | 82/438 [00:01<00:09, 37.38it/s][A
 20%|█▉        | 87/438 [00:01<00:08, 39.51it/s][A
 21%|██        | 92/438 [00:02<00:08, 41.15it/s][A
 22%|██▏       | 97/438 [00:02<00:08, 42.32it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 43.29it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 43.92it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.17it/s][A
 27%|██▋       | 117/438 [00:02<00:09, 33.88it/s][A
 28%|██▊       | 123/438 [00:02<00:08, 37.89it/s][A
 29%|██▉       | 128/438 [00:03<00:10, 30.99it/s][A
 31%|███       | 134/438 [00:03<00:08, 35.34it/s][A
 32%|███▏      | 138/438 [00:03<00:12, 23.22it/s][A
 33%|███▎      | 143/438 [00:03<00:10, 27.49it/s][A
 34%|███▎      | 147/438 [00:04<00:26, 10.91it/s][A
 34%|███▍      | 150/438 [00:04<00:24, 11.62it/s][A
 35%|███▌      | 155/438 [00:05<00:18, 15.53it/s][A
 37%|███▋      | 160/438 [00:05<00:14, 19.74it/s][A
 38%|███▊      | 165/438 [00:05<00:11, 24.06it/s][A
 39%|███▉      | 170/438 [00:05<00:09, 28.20it/s][A
 40%|███▉      | 175/438 [00:05<00:08, 31.93it/s][A
 41%|████      | 180/438 [00:05<00:07, 35.16it/s][A
 42%|████▏     | 185/438 [00:05<00:06, 37.78it/s][A
 43%|████▎     | 190/438 [00:05<00:06, 39.48it/s][A
 45%|████▍     | 195/438 [00:05<00:05, 40.60it/s][A
 46%|████▌     | 200/438 [00:06<00:05, 41.55it/s][A
 47%|████▋     | 205/438 [00:06<00:05, 42.35it/s][A
 48%|████▊     | 210/438 [00:06<00:05, 43.24it/s][A
 49%|████▉     | 215/438 [00:06<00:05, 43.80it/s][A
 50%|█████     | 220/438 [00:06<00:04, 44.35it/s][A
 51%|█████▏    | 225/438 [00:06<00:04, 44.76it/s][A
 53%|█████▎    | 230/438 [00:06<00:04, 44.88it/s][A
 54%|█████▎    | 235/438 [00:06<00:04, 44.75it/s][A
 55%|█████▍    | 240/438 [00:06<00:04, 44.49it/s][A
 56%|█████▌    | 245/438 [00:07<00:04, 44.44it/s][A
 57%|█████▋    | 250/438 [00:07<00:04, 44.47it/s][A
 58%|█████▊    | 255/438 [00:07<00:04, 44.59it/s][A
 59%|█████▉    | 260/438 [00:07<00:03, 44.76it/s][A
 61%|██████    | 265/438 [00:07<00:03, 45.05it/s][A
 62%|██████▏   | 270/438 [00:07<00:03, 45.20it/s][A
 63%|██████▎   | 275/438 [00:07<00:03, 45.22it/s][A
 64%|██████▍   | 280/438 [00:07<00:03, 45.06it/s][A
 65%|██████▌   | 285/438 [00:08<00:04, 35.98it/s][A
 66%|██████▌   | 290/438 [00:08<00:03, 38.38it/s][A
 67%|██████▋   | 295/438 [00:08<00:03, 40.33it/s][A
 68%|██████▊   | 300/438 [00:08<00:03, 41.74it/s][A
 70%|██████▉   | 305/438 [00:08<00:03, 42.79it/s][A
 71%|███████   | 310/438 [00:08<00:02, 43.58it/s][A
 72%|███████▏  | 315/438 [00:08<00:02, 44.14it/s][A
 73%|███████▎  | 320/438 [00:08<00:02, 44.30it/s][A
 74%|███████▍  | 325/438 [00:08<00:02, 44.12it/s][A
 75%|███████▌  | 330/438 [00:09<00:02, 43.99it/s][A
 76%|███████▋  | 335/438 [00:09<00:02, 44.20it/s][A
 78%|███████▊  | 340/438 [00:09<00:02, 44.52it/s][A
 79%|███████▉  | 345/438 [00:09<00:02, 44.74it/s][A
 80%|███████▉  | 350/438 [00:09<00:01, 45.04it/s][A
 81%|████████  | 355/438 [00:09<00:01, 45.20it/s][A
 82%|████████▏ | 360/438 [00:09<00:01, 45.19it/s][A
 83%|████████▎ | 365/438 [00:09<00:01, 45.03it/s][A
 84%|████████▍ | 370/438 [00:09<00:01, 44.71it/s][A
 86%|████████▌ | 375/438 [00:10<00:01, 44.41it/s][A
 87%|████████▋ | 380/438 [00:10<00:01, 44.51it/s][A
 88%|████████▊ | 385/438 [00:10<00:01, 44.64it/s][A
 89%|████████▉ | 390/438 [00:10<00:01, 44.89it/s][A
 90%|█████████ | 395/438 [00:10<00:00, 45.05it/s][A
 91%|█████████▏| 400/438 [00:10<00:00, 45.20it/s][A
 92%|█████████▏| 405/438 [00:10<00:00, 45.10it/s][A
 94%|█████████▎| 410/438 [00:10<00:00, 44.87it/s][A
 95%|█████████▍| 415/438 [00:10<00:00, 44.60it/s][A
 96%|█████████▌| 420/438 [00:11<00:00, 36.36it/s][A
 97%|█████████▋| 425/438 [00:11<00:00, 38.80it/s][A
 98%|█████████▊| 430/438 [00:11<00:00, 40.53it/s][A
 99%|█████████▉| 435/438 [00:11<00:00, 41.95it/s][A                                                 
                                                 [A 80%|████████  | 324/405 [03:41<00:23,  3.52it/s]
100%|██████████| 438/438 [00:11<00:00, 41.95it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:55:52,435 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324
[INFO|configuration_utils.py:351] 2023-08-28 00:55:53,269 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:56:24,724 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:56:26,998 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:56:27,374 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324/special_tokens_map.json
 80%|████████  | 325/405 [04:22<21:07, 15.85s/it] 80%|████████  | 326/405 [04:22<14:48, 11.24s/it] 81%|████████  | 327/405 [04:23<10:20,  7.95s/it] 81%|████████  | 328/405 [04:23<07:15,  5.65s/it] 81%|████████  | 329/405 [04:23<05:06,  4.04s/it] 81%|████████▏ | 330/405 [04:24<03:38,  2.91s/it] 82%|████████▏ | 331/405 [04:24<02:36,  2.12s/it] 82%|████████▏ | 332/405 [04:24<01:54,  1.57s/it] 82%|████████▏ | 333/405 [04:24<01:25,  1.18s/it] 82%|████████▏ | 334/405 [04:25<01:04,  1.10it/s] 83%|████████▎ | 335/405 [04:25<00:50,  1.39it/s] 83%|████████▎ | 336/405 [04:25<00:43,  1.60it/s] 83%|████████▎ | 337/405 [04:26<00:35,  1.92it/s] 83%|████████▎ | 338/405 [04:26<00:29,  2.24it/s] 84%|████████▎ | 339/405 [04:26<00:26,  2.52it/s] 84%|████████▍ | 340/405 [04:26<00:23,  2.77it/s] 84%|████████▍ | 341/405 [04:27<00:21,  2.97it/s] 84%|████████▍ | 342/405 [04:27<00:20,  3.14it/s] 85%|████████▍ | 343/405 [04:27<00:19,  3.25it/s] 85%|████████▍ | 344/405 [04:28<00:18,  3.34it/s] 85%|████████▌ | 345/405 [04:28<00:17,  3.42it/s] 85%|████████▌ | 346/405 [04:28<00:17,  3.46it/s] 86%|████████▌ | 347/405 [04:28<00:18,  3.21it/s] 86%|████████▌ | 348/405 [04:29<00:17,  3.31it/s] 86%|████████▌ | 349/405 [04:29<00:18,  2.96it/s] 86%|████████▋ | 350/405 [04:29<00:17,  3.12it/s] 87%|████████▋ | 351/405 [04:30<00:16,  3.25it/s] 87%|████████▋ | 352/405 [04:30<00:15,  3.34it/s] 87%|████████▋ | 353/405 [04:30<00:15,  3.27it/s] 87%|████████▋ | 354/405 [04:31<00:15,  3.36it/s] 88%|████████▊ | 355/405 [04:31<00:14,  3.42it/s] 88%|████████▊ | 356/405 [04:31<00:14,  3.47it/s] 88%|████████▊ | 357/405 [04:31<00:13,  3.50it/s] 88%|████████▊ | 358/405 [04:32<00:13,  3.53it/s] 89%|████████▊ | 359/405 [04:32<00:12,  3.54it/s] 89%|████████▉ | 360/405 [04:32<00:12,  3.56it/s] 89%|████████▉ | 361/405 [04:33<00:13,  3.25it/s] 89%|████████▉ | 362/405 [04:33<00:14,  2.96it/s] 90%|████████▉ | 363/405 [04:34<00:19,  2.16it/s] 90%|████████▉ | 364/405 [04:35<00:24,  1.70it/s] 90%|█████████ | 365/405 [04:35<00:19,  2.01it/s] 90%|█████████ | 366/405 [04:35<00:16,  2.32it/s] 91%|█████████ | 367/405 [04:36<00:14,  2.59it/s] 91%|█████████ | 368/405 [04:36<00:13,  2.83it/s] 91%|█████████ | 369/405 [04:36<00:11,  3.02it/s] 91%|█████████▏| 370/405 [04:37<00:12,  2.82it/s] 92%|█████████▏| 371/405 [04:37<00:11,  3.02it/s] 92%|█████████▏| 372/405 [04:37<00:10,  3.17it/s] 92%|█████████▏| 373/405 [04:37<00:09,  3.29it/s] 92%|█████████▏| 374/405 [04:38<00:09,  3.37it/s] 93%|█████████▎| 375/405 [04:38<00:08,  3.43it/s] 93%|█████████▎| 376/405 [04:38<00:08,  3.48it/s] 93%|█████████▎| 377/405 [04:38<00:07,  3.51it/s] 93%|█████████▎| 378/405 [04:39<00:07,  3.54it/s] 94%|█████████▎| 379/405 [04:39<00:07,  3.55it/s] 94%|█████████▍| 380/405 [04:39<00:07,  3.57it/s] 94%|█████████▍| 381/405 [04:40<00:07,  3.16it/s] 94%|█████████▍| 382/405 [04:40<00:07,  3.27it/s] 95%|█████████▍| 383/405 [04:40<00:06,  3.36it/s] 95%|█████████▍| 384/405 [04:41<00:06,  3.43it/s] 95%|█████████▌| 385/405 [04:41<00:05,  3.47it/s] 95%|█████████▌| 386/405 [04:41<00:05,  3.50it/s] 96%|█████████▌| 387/405 [04:41<00:05,  3.53it/s] 96%|█████████▌| 388/405 [04:42<00:04,  3.54it/s] 96%|█████████▌| 389/405 [04:42<00:04,  3.55it/s] 96%|█████████▋| 390/405 [04:42<00:04,  3.56it/s] 97%|█████████▋| 391/405 [04:43<00:03,  3.57it/s] 97%|█████████▋| 392/405 [04:43<00:04,  3.24it/s] 97%|█████████▋| 393/405 [04:43<00:03,  3.34it/s] 97%|█████████▋| 394/405 [04:43<00:03,  3.41it/s] 98%|█████████▊| 395/405 [04:44<00:02,  3.46it/s] 98%|█████████▊| 396/405 [04:44<00:02,  3.50it/s] 98%|█████████▊| 397/405 [04:44<00:02,  3.53it/s] 98%|█████████▊| 398/405 [04:45<00:01,  3.54it/s] 99%|█████████▊| 399/405 [04:45<00:01,  3.55it/s] 99%|█████████▉| 400/405 [04:45<00:01,  3.56it/s] 99%|█████████▉| 401/405 [04:45<00:01,  3.57it/s] 99%|█████████▉| 402/405 [04:46<00:00,  3.57it/s]100%|█████████▉| 403/405 [04:46<00:00,  3.20it/s]100%|█████████▉| 404/405 [04:46<00:00,  3.31it/s]100%|██████████| 405/405 [04:47<00:00,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 00:56:56,945 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:56:56,945 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 00:56:56,945 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 11.5028, 'eval_samples_per_second': 304.013, 'eval_steps_per_second': 38.078, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.29it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.02it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.69it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.74it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.29it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.91it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.41it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.06it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.12it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.18it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.21it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.25it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.26it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.24it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.13it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.66it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.72it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.86it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 45.00it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 45.13it/s][A
 24%|██▍       | 107/438 [00:02<00:08, 37.07it/s][A
 26%|██▌       | 112/438 [00:02<00:08, 39.30it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 41.01it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 42.27it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.20it/s][A
 30%|███       | 132/438 [00:02<00:06, 43.88it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.33it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.57it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.31it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.20it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.44it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.67it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.90it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.02it/s][A
 40%|████      | 177/438 [00:03<00:05, 45.20it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 45.37it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 45.28it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.86it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.77it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.80it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.87it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 45.05it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.16it/s][A
 51%|█████     | 222/438 [00:04<00:04, 45.23it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 45.36it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 45.16it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.92it/s][A
 55%|█████▌    | 242/438 [00:05<00:05, 36.23it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 38.61it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 40.47it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 41.76it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 42.83it/s][A
 61%|██████    | 267/438 [00:06<00:03, 43.61it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.19it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.43it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.15it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.13it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.47it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.66it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.87it/s][A
 70%|███████   | 307/438 [00:06<00:02, 45.06it/s][A
 71%|███████   | 312/438 [00:07<00:02, 45.26it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.38it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 45.17it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.81it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.64it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.71it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.80it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.92it/s][A
 80%|████████  | 352/438 [00:07<00:01, 45.04it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.21it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.27it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.20it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.98it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 37.69it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 39.76it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 41.34it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 42.54it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 43.42it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.03it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.53it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.61it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.42it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.19it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.39it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.47it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.80it/s][A                                                 
                                                 [A100%|██████████| 405/405 [04:56<00:00,  3.73it/s]
100%|██████████| 438/438 [00:09<00:00, 44.80it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 00:57:07,494 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405
[INFO|configuration_utils.py:351] 2023-08-28 00:57:08,336 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:57:30,946 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:57:31,493 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:57:31,672 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 00:57:34,095 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 00:57:34,168 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81 (score: 1.0607956647872925).
                                                 100%|██████████| 405/405 [05:44<00:00,  3.73it/s]100%|██████████| 405/405 [05:44<00:00,  1.18it/s]
[INFO|trainer.py:1894] 2023-08-28 00:57:54,333 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 00:57:54,453 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 00:58:00,146 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 00:58:00,722 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 00:58:00,967 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:58:02,276 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:02,277 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:02,277 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:02,277 >>   train_runtime            = 0:05:44.10
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:02,277 >>   train_samples            =       5160
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:02,277 >>   train_samples_per_second =     74.978
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:02,277 >>   train_steps_per_second   =      1.177
{'eval_loss': 1.0607956647872925, 'eval_runtime': 9.9434, 'eval_samples_per_second': 351.69, 'eval_steps_per_second': 44.049, 'epoch': 5.0}
{'train_runtime': 344.1022, 'train_samples_per_second': 74.978, 'train_steps_per_second': 1.177, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 00:58:03 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 00:58:03,023 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 00:58:03,023 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 00:58:03,023 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 55.30it/s]  3%|▎         | 12/438 [00:00<00:10, 39.33it/s]  4%|▍         | 17/438 [00:00<00:10, 41.88it/s]  5%|▌         | 22/438 [00:00<00:09, 43.10it/s]  6%|▌         | 27/438 [00:00<00:09, 44.07it/s]  7%|▋         | 32/438 [00:00<00:09, 44.65it/s]  8%|▊         | 37/438 [00:00<00:08, 44.99it/s] 10%|▉         | 42/438 [00:00<00:08, 45.18it/s] 11%|█         | 47/438 [00:01<00:08, 45.11it/s] 12%|█▏        | 52/438 [00:01<00:08, 44.67it/s] 13%|█▎        | 57/438 [00:01<00:08, 44.44it/s] 14%|█▍        | 62/438 [00:01<00:08, 44.61it/s] 15%|█▌        | 67/438 [00:01<00:08, 44.92it/s] 16%|█▋        | 72/438 [00:01<00:08, 45.18it/s] 18%|█▊        | 77/438 [00:01<00:07, 45.35it/s] 19%|█▊        | 82/438 [00:01<00:07, 45.57it/s] 20%|█▉        | 87/438 [00:01<00:07, 45.55it/s] 21%|██        | 92/438 [00:02<00:07, 45.55it/s] 22%|██▏       | 97/438 [00:02<00:07, 45.21it/s] 23%|██▎       | 102/438 [00:02<00:07, 45.04it/s] 24%|██▍       | 107/438 [00:02<00:07, 45.10it/s] 26%|██▌       | 112/438 [00:02<00:07, 45.19it/s] 27%|██▋       | 117/438 [00:02<00:07, 45.35it/s] 28%|██▊       | 122/438 [00:02<00:06, 45.52it/s] 29%|██▉       | 127/438 [00:02<00:06, 45.66it/s] 30%|███       | 132/438 [00:02<00:06, 45.54it/s] 31%|███▏      | 137/438 [00:03<00:06, 45.52it/s] 32%|███▏      | 142/438 [00:03<00:06, 45.31it/s] 34%|███▎      | 147/438 [00:03<00:07, 39.43it/s] 35%|███▍      | 152/438 [00:03<00:06, 41.20it/s] 36%|███▌      | 157/438 [00:03<00:06, 42.50it/s] 37%|███▋      | 162/438 [00:03<00:06, 43.44it/s] 38%|███▊      | 167/438 [00:03<00:06, 44.17it/s] 39%|███▉      | 172/438 [00:03<00:05, 44.61it/s] 40%|████      | 177/438 [00:03<00:05, 44.97it/s] 42%|████▏     | 182/438 [00:04<00:05, 45.14it/s] 43%|████▎     | 187/438 [00:04<00:05, 44.90it/s] 44%|████▍     | 192/438 [00:04<00:05, 44.71it/s] 45%|████▍     | 197/438 [00:04<00:05, 44.73it/s] 46%|████▌     | 202/438 [00:04<00:05, 44.91it/s] 47%|████▋     | 207/438 [00:04<00:05, 45.12it/s] 48%|████▊     | 212/438 [00:04<00:04, 45.41it/s] 50%|████▉     | 217/438 [00:04<00:04, 45.54it/s] 51%|█████     | 222/438 [00:04<00:04, 45.58it/s] 52%|█████▏    | 227/438 [00:05<00:04, 45.53it/s] 53%|█████▎    | 232/438 [00:05<00:04, 45.32it/s] 54%|█████▍    | 237/438 [00:05<00:04, 45.13it/s] 55%|█████▌    | 242/438 [00:05<00:04, 44.98it/s] 56%|█████▋    | 247/438 [00:05<00:04, 45.09it/s] 58%|█████▊    | 252/438 [00:05<00:04, 45.20it/s] 59%|█████▊    | 257/438 [00:05<00:03, 45.39it/s] 60%|█████▉    | 262/438 [00:05<00:03, 45.53it/s] 61%|██████    | 267/438 [00:05<00:03, 45.58it/s] 62%|██████▏   | 272/438 [00:06<00:03, 45.56it/s] 63%|██████▎   | 277/438 [00:06<00:03, 45.29it/s] 64%|██████▍   | 282/438 [00:06<00:04, 34.88it/s] 66%|██████▌   | 287/438 [00:06<00:04, 37.46it/s] 67%|██████▋   | 292/438 [00:06<00:03, 39.59it/s] 68%|██████▊   | 297/438 [00:06<00:03, 41.30it/s] 69%|██████▉   | 302/438 [00:06<00:03, 42.58it/s] 70%|███████   | 307/438 [00:06<00:03, 43.46it/s] 71%|███████   | 312/438 [00:07<00:02, 44.20it/s] 72%|███████▏  | 317/438 [00:07<00:02, 44.59it/s] 74%|███████▎  | 322/438 [00:07<00:02, 44.49it/s] 75%|███████▍  | 327/438 [00:07<00:02, 44.35it/s] 76%|███████▌  | 332/438 [00:07<00:02, 44.27it/s] 77%|███████▋  | 337/438 [00:07<00:02, 44.64it/s] 78%|███████▊  | 342/438 [00:07<00:02, 44.97it/s] 79%|███████▉  | 347/438 [00:07<00:02, 45.24it/s] 80%|████████  | 352/438 [00:07<00:01, 45.41it/s] 82%|████████▏ | 357/438 [00:08<00:01, 45.51it/s] 83%|████████▎ | 362/438 [00:08<00:01, 45.56it/s] 84%|████████▍ | 367/438 [00:08<00:01, 45.37it/s] 85%|████████▍ | 372/438 [00:08<00:01, 45.07it/s] 86%|████████▌ | 377/438 [00:08<00:01, 44.87it/s] 87%|████████▋ | 382/438 [00:08<00:01, 45.00it/s] 88%|████████▊ | 387/438 [00:08<00:01, 45.03it/s] 89%|████████▉ | 392/438 [00:08<00:01, 45.35it/s] 91%|█████████ | 397/438 [00:08<00:00, 45.46it/s] 92%|█████████▏| 402/438 [00:09<00:00, 45.43it/s] 93%|█████████▎| 407/438 [00:09<00:00, 45.60it/s] 94%|█████████▍| 412/438 [00:09<00:00, 45.33it/s] 95%|█████████▌| 417/438 [00:09<00:00, 22.20it/s] 96%|█████████▋| 422/438 [00:09<00:00, 26.21it/s] 97%|█████████▋| 427/438 [00:09<00:00, 30.03it/s] 99%|█████████▊| 432/438 [00:10<00:00, 33.40it/s]100%|█████████▉| 437/438 [00:10<00:00, 36.37it/s]100%|██████████| 438/438 [00:10<00:00, 42.82it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 00:58:13,271 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:13,271 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:13,271 >>   eval_loss               =     1.0608
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:13,271 >>   eval_runtime            = 0:00:10.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:13,271 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:13,271 >>   eval_samples_per_second =    341.269
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:13,271 >>   eval_steps_per_second   =     42.744
[INFO|trainer_pt_utils.py:913] 2023-08-28 00:58:13,271 >>   perplexity              =     2.8887
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:39,203 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:39,367 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:39,367 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:39,368 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:39,368 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 00:58:40,954 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 00:58:40,955 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:58:41,711 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 00:58:42,977 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:58:43,062 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:46,541 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:46,645 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:46,645 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:46,645 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 00:58:46,646 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 00:58:48,045 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 00:58:48,046 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 00:58:48,934 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 00:58:49,366 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 00:58:49,366 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-81
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-324
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-243
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-405
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/checkpoint-162
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.70it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:04,  1.75it/s]Extractor Predicting: 9it [00:05,  1.74it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:07,  1.61it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.67it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:10,  1.66it/s]Extractor Predicting: 19it [00:11,  1.68it/s]Extractor Predicting: 20it [00:12,  1.66it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.72it/s]Extractor Predicting: 23it [00:13,  1.73it/s]Extractor Predicting: 24it [00:14,  1.80it/s]Extractor Predicting: 25it [00:14,  1.81it/s]Extractor Predicting: 26it [00:15,  1.72it/s]Extractor Predicting: 27it [00:16,  1.71it/s]Extractor Predicting: 28it [00:16,  1.72it/s]Extractor Predicting: 29it [00:17,  1.71it/s]Extractor Predicting: 30it [00:17,  1.69it/s]Extractor Predicting: 31it [00:18,  1.58it/s]Extractor Predicting: 32it [00:19,  1.58it/s]Extractor Predicting: 33it [00:19,  1.61it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:20,  1.68it/s]Extractor Predicting: 36it [00:21,  1.57it/s]Extractor Predicting: 37it [00:22,  1.62it/s]Extractor Predicting: 38it [00:22,  1.65it/s]Extractor Predicting: 39it [00:23,  1.61it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:24,  1.58it/s]Extractor Predicting: 42it [00:25,  1.61it/s]Extractor Predicting: 43it [00:25,  1.64it/s]Extractor Predicting: 44it [00:26,  1.65it/s]Extractor Predicting: 45it [00:27,  1.65it/s]Extractor Predicting: 46it [00:27,  1.46it/s]Extractor Predicting: 47it [00:28,  1.49it/s]Extractor Predicting: 48it [00:29,  1.58it/s]Extractor Predicting: 49it [00:29,  1.59it/s]Extractor Predicting: 50it [00:30,  1.62it/s]Extractor Predicting: 51it [00:31,  1.59it/s]Extractor Predicting: 52it [00:31,  1.60it/s]Extractor Predicting: 53it [00:32,  1.61it/s]Extractor Predicting: 54it [00:32,  1.66it/s]Extractor Predicting: 55it [00:33,  1.64it/s]Extractor Predicting: 56it [00:34,  1.56it/s]Extractor Predicting: 57it [00:34,  1.62it/s]Extractor Predicting: 58it [00:35,  1.62it/s]Extractor Predicting: 59it [00:35,  1.60it/s]Extractor Predicting: 60it [00:36,  1.60it/s]Extractor Predicting: 61it [00:37,  1.52it/s]Extractor Predicting: 62it [00:37,  1.60it/s]Extractor Predicting: 63it [00:38,  1.65it/s]Extractor Predicting: 64it [00:39,  1.62it/s]Extractor Predicting: 65it [00:39,  1.67it/s]Extractor Predicting: 66it [00:40,  1.60it/s]Extractor Predicting: 67it [00:40,  1.63it/s]Extractor Predicting: 68it [00:41,  1.68it/s]Extractor Predicting: 69it [00:42,  1.68it/s]Extractor Predicting: 70it [00:42,  1.56it/s]Extractor Predicting: 71it [00:43,  1.62it/s]Extractor Predicting: 72it [00:43,  1.69it/s]Extractor Predicting: 73it [00:44,  1.68it/s]Extractor Predicting: 74it [00:45,  1.66it/s]Extractor Predicting: 75it [00:45,  1.60it/s]Extractor Predicting: 76it [00:46,  1.65it/s]Extractor Predicting: 77it [00:46,  1.72it/s]Extractor Predicting: 78it [00:47,  1.71it/s]Extractor Predicting: 79it [00:47,  1.76it/s]Extractor Predicting: 80it [00:48,  1.78it/s]Extractor Predicting: 81it [00:49,  1.70it/s]Extractor Predicting: 82it [00:49,  1.68it/s]Extractor Predicting: 83it [00:50,  1.68it/s]Extractor Predicting: 84it [00:50,  1.68it/s]Extractor Predicting: 85it [00:51,  1.66it/s]Extractor Predicting: 86it [00:52,  1.61it/s]Extractor Predicting: 87it [00:52,  1.65it/s]Extractor Predicting: 88it [00:53,  1.66it/s]Extractor Predicting: 89it [00:54,  1.68it/s]Extractor Predicting: 90it [00:54,  1.70it/s]Extractor Predicting: 91it [00:55,  1.68it/s]Extractor Predicting: 92it [00:55,  1.61it/s]Extractor Predicting: 93it [00:56,  1.63it/s]Extractor Predicting: 94it [00:57,  1.68it/s]Extractor Predicting: 95it [00:57,  1.72it/s]Extractor Predicting: 96it [00:58,  1.70it/s]Extractor Predicting: 97it [00:58,  1.67it/s]Extractor Predicting: 98it [00:59,  1.59it/s]Extractor Predicting: 99it [01:00,  1.60it/s]Extractor Predicting: 100it [01:00,  1.63it/s]Extractor Predicting: 101it [01:01,  1.69it/s]Extractor Predicting: 102it [01:01,  1.68it/s]Extractor Predicting: 103it [01:02,  1.62it/s]Extractor Predicting: 104it [01:03,  1.62it/s]Extractor Predicting: 105it [01:03,  1.65it/s]Extractor Predicting: 106it [01:04,  1.65it/s]Extractor Predicting: 107it [01:04,  1.63it/s]Extractor Predicting: 108it [01:05,  1.57it/s]Extractor Predicting: 109it [01:06,  1.58it/s]Extractor Predicting: 110it [01:06,  1.61it/s]Extractor Predicting: 111it [01:07,  1.62it/s]Extractor Predicting: 112it [01:08,  1.65it/s]Extractor Predicting: 113it [01:08,  1.56it/s]Extractor Predicting: 114it [01:09,  1.58it/s]Extractor Predicting: 115it [01:09,  1.60it/s]Extractor Predicting: 116it [01:10,  1.58it/s]Extractor Predicting: 117it [01:11,  1.60it/s]Extractor Predicting: 118it [01:11,  1.54it/s]Extractor Predicting: 119it [01:12,  1.58it/s]Extractor Predicting: 120it [01:13,  1.59it/s]Extractor Predicting: 121it [01:13,  1.62it/s]Extractor Predicting: 122it [01:14,  1.65it/s]Extractor Predicting: 123it [01:14,  1.65it/s]Extractor Predicting: 124it [01:15,  1.66it/s]Extractor Predicting: 125it [01:16,  1.63it/s]Extractor Predicting: 126it [01:16,  1.57it/s]Extractor Predicting: 127it [01:17,  1.63it/s]Extractor Predicting: 128it [01:18,  1.63it/s]Extractor Predicting: 129it [01:18,  1.63it/s]Extractor Predicting: 130it [01:19,  1.64it/s]Extractor Predicting: 131it [01:19,  1.55it/s]Extractor Predicting: 132it [01:20,  1.45it/s]Extractor Predicting: 133it [01:21,  1.52it/s]Extractor Predicting: 134it [01:21,  1.57it/s]Extractor Predicting: 135it [01:22,  1.55it/s]Extractor Predicting: 136it [01:23,  1.45it/s]Extractor Predicting: 137it [01:23,  1.53it/s]Extractor Predicting: 138it [01:24,  1.56it/s]Extractor Predicting: 139it [01:25,  1.57it/s]Extractor Predicting: 140it [01:25,  1.61it/s]Extractor Predicting: 141it [01:26,  1.55it/s]Extractor Predicting: 142it [01:27,  1.53it/s]Extractor Predicting: 143it [01:27,  1.56it/s]Extractor Predicting: 144it [01:28,  1.62it/s]Extractor Predicting: 145it [01:28,  1.87it/s]Extractor Predicting: 145it [01:28,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:47,620 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:47,717 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:47,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:47,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:47,718 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:00:49,121 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:00:49,122 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:00:49,956 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:00:51,320 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:00:51,456 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:55,690 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:55,790 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:55,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:55,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:00:55,791 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:00:57,397 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:00:57,398 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:00:58,287 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:00:58,795 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:00:58,940 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.68it/s]Extractor Predicting: 4it [00:02,  1.71it/s]Extractor Predicting: 5it [00:02,  1.74it/s]Extractor Predicting: 6it [00:03,  1.65it/s]Extractor Predicting: 7it [00:04,  1.67it/s]Extractor Predicting: 8it [00:05,  1.43it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:06,  1.68it/s]Extractor Predicting: 12it [00:07,  1.70it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.66it/s]Extractor Predicting: 15it [00:09,  1.63it/s]Extractor Predicting: 16it [00:09,  1.66it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:10,  1.64it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.69it/s]Extractor Predicting: 23it [00:14,  1.61it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.65it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.72it/s]Extractor Predicting: 28it [00:17,  1.57it/s]Extractor Predicting: 29it [00:17,  1.61it/s]Extractor Predicting: 30it [00:18,  1.66it/s]Extractor Predicting: 31it [00:18,  1.70it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:20,  1.58it/s]Extractor Predicting: 34it [00:20,  1.61it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:22,  1.59it/s]Extractor Predicting: 37it [00:22,  1.64it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:23,  1.65it/s]Extractor Predicting: 40it [00:24,  1.64it/s]Extractor Predicting: 41it [00:25,  1.66it/s]Extractor Predicting: 42it [00:25,  1.68it/s]Extractor Predicting: 43it [00:26,  1.58it/s]Extractor Predicting: 44it [00:26,  1.62it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:28,  1.69it/s]Extractor Predicting: 47it [00:28,  1.70it/s]Extractor Predicting: 48it [00:29,  1.61it/s]Extractor Predicting: 49it [00:29,  1.67it/s]Extractor Predicting: 50it [00:30,  1.67it/s]Extractor Predicting: 51it [00:31,  1.68it/s]Extractor Predicting: 52it [00:31,  1.72it/s]Extractor Predicting: 53it [00:32,  1.72it/s]Extractor Predicting: 54it [00:32,  1.63it/s]Extractor Predicting: 55it [00:33,  1.65it/s]Extractor Predicting: 56it [00:34,  1.63it/s]Extractor Predicting: 57it [00:34,  1.62it/s]Extractor Predicting: 58it [00:35,  1.65it/s]Extractor Predicting: 59it [00:35,  1.62it/s]Extractor Predicting: 60it [00:36,  1.65it/s]Extractor Predicting: 61it [00:37,  1.66it/s]Extractor Predicting: 62it [00:37,  1.70it/s]Extractor Predicting: 63it [00:38,  1.68it/s]Extractor Predicting: 64it [00:38,  1.63it/s]Extractor Predicting: 65it [00:39,  1.64it/s]Extractor Predicting: 66it [00:40,  1.67it/s]Extractor Predicting: 67it [00:40,  1.69it/s]Extractor Predicting: 68it [00:41,  1.73it/s]Extractor Predicting: 69it [00:41,  1.77it/s]Extractor Predicting: 70it [00:42,  1.64it/s]Extractor Predicting: 71it [00:43,  1.68it/s]Extractor Predicting: 72it [00:43,  1.72it/s]Extractor Predicting: 73it [00:44,  1.73it/s]Extractor Predicting: 74it [00:44,  1.70it/s]Extractor Predicting: 75it [00:45,  1.55it/s]Extractor Predicting: 76it [00:46,  1.62it/s]Extractor Predicting: 77it [00:46,  1.69it/s]Extractor Predicting: 78it [00:47,  1.70it/s]Extractor Predicting: 79it [00:47,  1.73it/s]Extractor Predicting: 80it [00:48,  1.65it/s]Extractor Predicting: 81it [00:48,  1.68it/s]Extractor Predicting: 82it [00:49,  1.69it/s]Extractor Predicting: 83it [00:50,  1.69it/s]Extractor Predicting: 84it [00:50,  1.71it/s]Extractor Predicting: 85it [00:51,  1.73it/s]Extractor Predicting: 86it [00:51,  1.67it/s]Extractor Predicting: 87it [00:52,  1.71it/s]Extractor Predicting: 88it [00:53,  1.71it/s]Extractor Predicting: 89it [00:53,  1.67it/s]Extractor Predicting: 90it [00:54,  1.71it/s]Extractor Predicting: 91it [00:54,  1.76it/s]Extractor Predicting: 92it [00:55,  1.58it/s]Extractor Predicting: 93it [00:56,  1.60it/s]Extractor Predicting: 94it [00:56,  1.62it/s]Extractor Predicting: 95it [00:57,  1.65it/s]Extractor Predicting: 96it [00:58,  1.54it/s]Extractor Predicting: 97it [00:58,  1.49it/s]Extractor Predicting: 98it [00:59,  1.52it/s]Extractor Predicting: 99it [01:00,  1.59it/s]Extractor Predicting: 100it [01:00,  1.62it/s]Extractor Predicting: 101it [01:01,  1.68it/s]Extractor Predicting: 102it [01:01,  1.54it/s]Extractor Predicting: 103it [01:02,  1.60it/s]Extractor Predicting: 104it [01:03,  1.61it/s]Extractor Predicting: 105it [01:03,  1.61it/s]Extractor Predicting: 106it [01:04,  1.63it/s]Extractor Predicting: 107it [01:05,  1.56it/s]Extractor Predicting: 108it [01:05,  1.62it/s]Extractor Predicting: 109it [01:06,  1.65it/s]Extractor Predicting: 110it [01:06,  1.70it/s]Extractor Predicting: 111it [01:07,  1.76it/s]Extractor Predicting: 112it [01:07,  1.73it/s]Extractor Predicting: 113it [01:08,  1.72it/s]Extractor Predicting: 114it [01:09,  1.69it/s]Extractor Predicting: 115it [01:09,  1.62it/s]Extractor Predicting: 116it [01:10,  1.66it/s]Extractor Predicting: 117it [01:10,  1.71it/s]Extractor Predicting: 118it [01:11,  1.70it/s]Extractor Predicting: 119it [01:12,  1.71it/s]Extractor Predicting: 120it [01:12,  1.69it/s]Extractor Predicting: 121it [01:13,  1.62it/s]Extractor Predicting: 122it [01:13,  1.68it/s]Extractor Predicting: 123it [01:14,  1.71it/s]Extractor Predicting: 124it [01:14,  1.71it/s]Extractor Predicting: 125it [01:15,  1.69it/s]Extractor Predicting: 126it [01:16,  1.67it/s]Extractor Predicting: 127it [01:16,  1.62it/s]Extractor Predicting: 128it [01:17,  1.66it/s]Extractor Predicting: 129it [01:17,  1.72it/s]Extractor Predicting: 130it [01:18,  1.70it/s]Extractor Predicting: 131it [01:19,  1.75it/s]Extractor Predicting: 132it [01:19,  1.75it/s]Extractor Predicting: 133it [01:20,  1.65it/s]Extractor Predicting: 134it [01:20,  1.64it/s]Extractor Predicting: 135it [01:21,  1.64it/s]Extractor Predicting: 136it [01:22,  1.66it/s]Extractor Predicting: 137it [01:22,  1.69it/s]Extractor Predicting: 138it [01:23,  1.62it/s]Extractor Predicting: 139it [01:24,  1.64it/s]Extractor Predicting: 140it [01:24,  1.65it/s]Extractor Predicting: 141it [01:25,  1.70it/s]Extractor Predicting: 142it [01:25,  1.67it/s]Extractor Predicting: 143it [01:26,  1.60it/s]Extractor Predicting: 144it [01:27,  1.63it/s]Extractor Predicting: 145it [01:27,  2.10it/s]Extractor Predicting: 145it [01:27,  1.66it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:45,019 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:45,022 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:45,022 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:45,022 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:45,022 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:02:45,694 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:02:45,695 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:02:46,337 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:02:47,383 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:02:47,467 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:51,004 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:51,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:51,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:51,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:02:51,127 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:02:52,269 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:02:52,271 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:02:52,973 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:02:53,387 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:02:53,387 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.57it/s]Extractor Predicting: 1it [00:00,  1.57it/s]
[INFO|configuration_utils.py:515] 2023-08-28 01:02:59,501 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:02:59,502 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 01:02:59,704 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:02:59,815 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 01:03:00,077 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 01:03:41,438 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 01:03:41,539 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 01:03:42,639 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 01:03:42,641 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 01:03:43,332 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:03:43,586 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:03:43,586 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:03:43,586 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:03:43,586 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:03:43,586 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 01:03:43,586 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 01:03:44,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:45,182 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:45,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:46,539 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:47,213 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:47,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:48,428 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:49,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:49,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:50,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:51,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:51,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:52,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:53,346 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:54,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:54,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:55,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:55,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:56,802 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:57,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:58,448 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:59,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:03:59,906 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:00,585 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:16<02:30, 16.76s/it][WARNING|generation_utils.py:914] 2023-08-28 01:04:01,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:01,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:02,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:03,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:03,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:04,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:05,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:05,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:06,495 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:07,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:07,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:08,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:09,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:10,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:10,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:11,684 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:12,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:13,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:13,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:14,403 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:15,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:15,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:16,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:32<02:10, 16.26s/it][WARNING|generation_utils.py:914] 2023-08-28 01:04:17,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:17,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:18,617 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:19,196 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:19,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:20,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:21,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:22,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:22,755 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:23,402 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:24,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:24,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:25,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:26,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:27,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:27,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:28,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:29,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:30,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:30,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:31,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:32,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:33,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:33,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:34,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:35,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:51<02:02, 17.55s/it][WARNING|generation_utils.py:914] 2023-08-28 01:04:36,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:37,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:37,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:38,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:39,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:39,798 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:40,419 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:41,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:42,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:42,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:43,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:44,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:44,984 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:45,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:46,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:46,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:47,642 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:48,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:49,070 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:49,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:50,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:51,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:51,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:52,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:08<01:43, 17.28s/it][WARNING|generation_utils.py:914] 2023-08-28 01:04:53,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:53,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:54,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:55,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:56,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:56,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:57,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:58,124 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:58,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:04:59,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:00,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:01,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:01,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:02,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:03,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:03,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:04,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:05,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:06,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:06,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:07,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:08,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:08,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:09,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:25<01:26, 17.23s/it][WARNING|generation_utils.py:914] 2023-08-28 01:05:10,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:10,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:11,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:12,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:12,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:13,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:14,095 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:14,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:15,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:15,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:16,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:17,103 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:17,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:18,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:19,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:19,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:20,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:21,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:21,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:22,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:22,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:23,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:24,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:24,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:41<01:06, 16.56s/it][WARNING|generation_utils.py:914] 2023-08-28 01:05:25,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:26,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:26,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:27,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:27,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:28,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:29,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:30,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:30,741 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:31,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:32,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:32,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:33,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:33,863 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:34,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:35,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:35,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:36,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:37,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:37,654 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:38,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:38,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:54<00:47, 15.69s/it][WARNING|generation_utils.py:914] 2023-08-28 01:05:39,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:40,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:40,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:41,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:42,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:42,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:43,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:44,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:45,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:45,640 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:46,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:47,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:47,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:48,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:49,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:50,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:50,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:51,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:52,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:53,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:53,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:54,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:55,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:55,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:12<00:32, 16.20s/it][WARNING|generation_utils.py:914] 2023-08-28 01:05:56,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:57,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:57,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:58,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:05:59,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:00,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:00,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:01,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:02,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:02,918 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:03,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:04,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:04,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:05,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:06,229 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:06,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:07,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:08,351 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:09,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:09,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:10,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:11,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:11,943 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:28<00:16, 16.15s/it][WARNING|generation_utils.py:914] 2023-08-28 01:06:12,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:13,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:14,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:14,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:15,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:16,019 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:16,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:17,338 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:18,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:18,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:19,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:20,023 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:20,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:21,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:21,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:22,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:23,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:23,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:24,454 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:25,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:25,661 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:26,286 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:26,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:27,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:28,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:28,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:29,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:30,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:30,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:31,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:31,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:32,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 01:06:33,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:49<00:00, 17.80s/it]Generating: 100%|██████████| 10/10 [02:49<00:00, 16.97s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:49,358 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:49,434 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:49,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:49,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:49,435 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:06:50,903 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:06:50,904 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:06:51,674 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:06:52,982 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:06:53,060 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:56,002 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:56,088 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:56,088 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:56,089 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:06:56,089 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:06:56,929 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:06:56,930 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:06:57,338 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:06:57,659 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:06:57,660 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : director .', 'success_rate': 0.8138020833333334, 'errors': {'', "('\\n', 'director', 'Scott Zandt', 'It was written and directed by Scott Zandt ( a.k.a .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8505434782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 359, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 499, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7439903846153846, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : part of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'too many values to unpack (expected 2)', '(\'1958 contest\', \'part of\', \'\', \'He was succeeded as Dutch representative at its 1958 contest by Johannes Eichhorn with " Allende en seine " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a deal to turn " Ingress " into a mobile game . Head Entity : Ingress , Tail Entity : Ingress Studio .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.7981770833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8764204545454546, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n']
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n"]
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n", "Relation : member of political party . Context : This was the first coalition government which was elected in 1998 , and led by then - Prime Minister Naguib Sawiris of the People 's Alliance . Head Entity : Naguib Sawiris , Tail Entity : People ' Alliance .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Tom Blomkamp\', \'member of political party\', \'\', \'" My Life ( " ; ) is a 2015 English language English language documentary film directed by Tom Blomkamp and starring Emma Thompson and Tom Hardy .\')'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 58, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 96, 'raw': 160}
{'target': 600, 'success': 114, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 154, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 223, 'raw': 384}
{'target': 600, 'success': 245, 'raw': 416}
{'target': 600, 'success': 263, 'raw': 448}
{'target': 600, 'success': 276, 'raw': 480}
{'target': 600, 'success': 297, 'raw': 512}
{'target': 600, 'success': 318, 'raw': 544}
{'target': 600, 'success': 335, 'raw': 576}
{'target': 600, 'success': 359, 'raw': 608}
{'target': 600, 'success': 381, 'raw': 640}
{'target': 600, 'success': 398, 'raw': 672}
{'target': 600, 'success': 410, 'raw': 704}
{'target': 600, 'success': 431, 'raw': 736}
{'target': 600, 'success': 451, 'raw': 768}
{'target': 600, 'success': 470, 'raw': 800}
{'target': 600, 'success': 494, 'raw': 832}
{'target': 600, 'success': 516, 'raw': 864}
{'target': 600, 'success': 533, 'raw': 896}
{'target': 600, 'success': 552, 'raw': 928}
{'target': 600, 'success': 566, 'raw': 960}
{'target': 600, 'success': 584, 'raw': 992}
{'target': 600, 'success': 599, 'raw': 1024}
{'target': 600, 'success': 621, 'raw': 1056}
{'prompt': 'Relation : position held .', 'success_rate': 0.5880681818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 12256
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12356, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.25it/s]Extractor Estimating: 2it [00:01,  1.24it/s]Extractor Estimating: 3it [00:02,  1.33it/s]Extractor Estimating: 4it [00:02,  1.37it/s]Extractor Estimating: 5it [00:03,  1.36it/s]Extractor Estimating: 6it [00:04,  1.46it/s]Extractor Estimating: 7it [00:05,  1.43it/s]Extractor Estimating: 8it [00:05,  1.41it/s]Extractor Estimating: 9it [00:06,  1.44it/s]Extractor Estimating: 10it [00:07,  1.37it/s]Extractor Estimating: 11it [00:07,  1.44it/s]Extractor Estimating: 12it [00:08,  1.46it/s]Extractor Estimating: 13it [00:09,  1.48it/s]Extractor Estimating: 14it [00:09,  1.43it/s]Extractor Estimating: 15it [00:10,  1.41it/s]Extractor Estimating: 16it [00:11,  1.42it/s]Extractor Estimating: 17it [00:12,  1.43it/s]Extractor Estimating: 18it [00:12,  1.47it/s]Extractor Estimating: 19it [00:13,  1.45it/s]Extractor Estimating: 20it [00:14,  1.46it/s]Extractor Estimating: 21it [00:14,  1.43it/s]Extractor Estimating: 22it [00:15,  1.48it/s]Extractor Estimating: 23it [00:16,  1.49it/s]Extractor Estimating: 24it [00:16,  1.39it/s]Extractor Estimating: 25it [00:17,  1.37it/s]Extractor Estimating: 26it [00:18,  1.44it/s]Extractor Estimating: 27it [00:18,  1.48it/s]Extractor Estimating: 28it [00:19,  1.55it/s]Extractor Estimating: 29it [00:20,  1.48it/s]Extractor Estimating: 30it [00:20,  1.54it/s]Extractor Estimating: 31it [00:21,  1.57it/s]Extractor Estimating: 32it [00:22,  1.59it/s]Extractor Estimating: 33it [00:22,  1.58it/s]Extractor Estimating: 34it [00:23,  1.48it/s]Extractor Estimating: 35it [00:24,  1.56it/s]Extractor Estimating: 36it [00:24,  1.57it/s]Extractor Estimating: 37it [00:25,  1.57it/s]Extractor Estimating: 38it [00:25,  1.57it/s]Extractor Estimating: 39it [00:26,  1.50it/s]Extractor Estimating: 40it [00:27,  1.58it/s]Extractor Estimating: 41it [00:27,  1.55it/s]Extractor Estimating: 42it [00:28,  1.57it/s]Extractor Estimating: 43it [00:29,  1.61it/s]Extractor Estimating: 44it [00:29,  1.50it/s]Extractor Estimating: 45it [00:30,  1.53it/s]Extractor Estimating: 46it [00:31,  1.61it/s]Extractor Estimating: 47it [00:31,  1.58it/s]Extractor Estimating: 48it [00:32,  1.62it/s]Extractor Estimating: 49it [00:32,  1.59it/s]Extractor Estimating: 50it [00:33,  1.66it/s]Extractor Estimating: 51it [00:34,  1.59it/s]Extractor Estimating: 52it [00:34,  1.58it/s]Extractor Estimating: 53it [00:35,  1.61it/s]Extractor Estimating: 54it [00:36,  1.56it/s]Extractor Estimating: 55it [00:36,  1.56it/s]Extractor Estimating: 56it [00:37,  1.56it/s]Extractor Estimating: 57it [00:37,  1.57it/s]Extractor Estimating: 58it [00:38,  1.64it/s]Extractor Estimating: 59it [00:39,  1.50it/s]Extractor Estimating: 60it [00:39,  1.56it/s]Extractor Estimating: 61it [00:40,  1.44it/s]Extractor Estimating: 62it [00:41,  1.48it/s]Extractor Estimating: 63it [00:41,  1.53it/s]Extractor Estimating: 64it [00:42,  1.50it/s]Extractor Estimating: 65it [00:43,  1.46it/s]Extractor Estimating: 66it [00:44,  1.45it/s]Extractor Estimating: 67it [00:44,  1.48it/s]Extractor Estimating: 68it [00:45,  1.50it/s]Extractor Estimating: 69it [00:46,  1.51it/s]Extractor Estimating: 70it [00:46,  1.45it/s]Extractor Estimating: 71it [00:47,  1.39it/s]Extractor Estimating: 72it [00:48,  1.41it/s]Extractor Estimating: 73it [00:48,  1.43it/s]Extractor Estimating: 74it [00:49,  1.50it/s]Extractor Estimating: 75it [00:50,  1.42it/s]Extractor Estimating: 76it [00:51,  1.35it/s]Extractor Estimating: 77it [00:51,  1.39it/s]Extractor Estimating: 78it [00:52,  1.42it/s]Extractor Estimating: 79it [00:53,  1.45it/s]Extractor Estimating: 80it [00:53,  1.51it/s]Extractor Estimating: 81it [00:54,  1.39it/s]Extractor Estimating: 82it [00:55,  1.45it/s]Extractor Estimating: 83it [00:55,  1.44it/s]Extractor Estimating: 84it [00:56,  1.46it/s]Extractor Estimating: 85it [00:57,  1.45it/s]Extractor Estimating: 86it [00:58,  1.42it/s]Extractor Estimating: 87it [00:58,  1.48it/s]Extractor Estimating: 88it [00:59,  1.43it/s]Extractor Estimating: 89it [01:00,  1.46it/s]Extractor Estimating: 90it [01:00,  1.47it/s]Extractor Estimating: 91it [01:01,  1.38it/s]Extractor Estimating: 92it [01:02,  1.43it/s]Extractor Estimating: 93it [01:02,  1.48it/s]Extractor Estimating: 94it [01:03,  1.49it/s]Extractor Estimating: 95it [01:04,  1.56it/s]Extractor Estimating: 96it [01:04,  1.43it/s]Extractor Estimating: 97it [01:05,  1.45it/s]Extractor Estimating: 98it [01:06,  1.47it/s]Extractor Estimating: 99it [01:06,  1.51it/s]Extractor Estimating: 100it [01:07,  1.53it/s]Extractor Estimating: 101it [01:08,  1.45it/s]Extractor Estimating: 102it [01:08,  1.48it/s]Extractor Estimating: 103it [01:09,  1.49it/s]Extractor Estimating: 104it [01:10,  1.50it/s]Extractor Estimating: 105it [01:10,  1.53it/s]Extractor Estimating: 106it [01:11,  1.45it/s]Extractor Estimating: 107it [01:12,  1.48it/s]Extractor Estimating: 108it [01:12,  1.47it/s]Extractor Estimating: 109it [01:13,  1.50it/s]Extractor Estimating: 110it [01:14,  1.53it/s]Extractor Estimating: 111it [01:14,  1.54it/s]Extractor Estimating: 112it [01:15,  1.53it/s]Extractor Estimating: 113it [01:16,  1.55it/s]Extractor Estimating: 114it [01:16,  1.53it/s]Extractor Estimating: 115it [01:17,  1.58it/s]Extractor Estimating: 116it [01:18,  1.47it/s]Extractor Estimating: 117it [01:18,  1.51it/s]Extractor Estimating: 118it [01:19,  1.45it/s]Extractor Estimating: 119it [01:20,  1.51it/s]Extractor Estimating: 120it [01:20,  1.54it/s]Extractor Estimating: 121it [01:21,  1.46it/s]Extractor Estimating: 122it [01:22,  1.52it/s]Extractor Estimating: 123it [01:22,  1.54it/s]Extractor Estimating: 124it [01:23,  1.54it/s]Extractor Estimating: 125it [01:23,  1.59it/s]Extractor Estimating: 126it [01:24,  1.48it/s]Extractor Estimating: 127it [01:25,  1.47it/s]Extractor Estimating: 128it [01:26,  1.49it/s]Extractor Estimating: 129it [01:26,  1.55it/s]Extractor Estimating: 130it [01:27,  1.61it/s]Extractor Estimating: 131it [01:27,  1.57it/s]Extractor Estimating: 132it [01:28,  1.53it/s]Extractor Estimating: 133it [01:29,  1.55it/s]Extractor Estimating: 134it [01:29,  1.52it/s]Extractor Estimating: 135it [01:30,  1.55it/s]Extractor Estimating: 136it [01:31,  1.48it/s]Extractor Estimating: 137it [01:31,  1.53it/s]Extractor Estimating: 138it [01:32,  1.54it/s]Extractor Estimating: 139it [01:33,  1.44it/s]Extractor Estimating: 140it [01:33,  1.45it/s]Extractor Estimating: 141it [01:34,  1.46it/s]Extractor Estimating: 142it [01:35,  1.47it/s]Extractor Estimating: 143it [01:35,  1.50it/s]Extractor Estimating: 144it [01:36,  1.54it/s]Extractor Estimating: 145it [01:37,  1.58it/s]Extractor Estimating: 146it [01:37,  1.47it/s]Extractor Estimating: 147it [01:38,  1.53it/s]Extractor Estimating: 148it [01:39,  1.51it/s]Extractor Estimating: 149it [01:39,  1.55it/s]Extractor Estimating: 150it [01:40,  1.57it/s]Extractor Estimating: 151it [01:41,  1.56it/s]Extractor Estimating: 152it [01:41,  1.62it/s]Extractor Estimating: 153it [01:42,  1.68it/s]Extractor Estimating: 154it [01:42,  1.74it/s]Extractor Estimating: 155it [01:43,  1.76it/s]Extractor Estimating: 156it [01:43,  1.80it/s]Extractor Estimating: 157it [01:44,  1.66it/s]Extractor Estimating: 158it [01:45,  1.68it/s]Extractor Estimating: 159it [01:45,  1.73it/s]Extractor Estimating: 160it [01:46,  1.76it/s]Extractor Estimating: 161it [01:46,  1.83it/s]Extractor Estimating: 162it [01:47,  1.90it/s]Extractor Estimating: 163it [01:47,  1.84it/s]Extractor Estimating: 164it [01:48,  1.78it/s]Extractor Estimating: 165it [01:48,  1.84it/s]Extractor Estimating: 166it [01:49,  1.81it/s]Extractor Estimating: 167it [01:49,  1.89it/s]Extractor Estimating: 168it [01:50,  1.85it/s]Extractor Estimating: 169it [01:51,  1.87it/s]Extractor Estimating: 170it [01:51,  1.72it/s]Extractor Estimating: 171it [01:52,  1.74it/s]Extractor Estimating: 172it [01:52,  1.78it/s]Extractor Estimating: 173it [01:53,  1.82it/s]Extractor Estimating: 174it [01:53,  1.84it/s]Extractor Estimating: 175it [01:54,  1.85it/s]Extractor Estimating: 176it [01:55,  1.56it/s]Extractor Estimating: 177it [01:55,  1.57it/s]Extractor Estimating: 178it [01:56,  1.62it/s]Extractor Estimating: 179it [01:57,  1.58it/s]Extractor Estimating: 180it [01:57,  1.62it/s]Extractor Estimating: 181it [01:58,  1.49it/s]Extractor Estimating: 182it [01:59,  1.56it/s]Extractor Estimating: 183it [01:59,  1.53it/s]Extractor Estimating: 184it [02:00,  1.58it/s]Extractor Estimating: 185it [02:00,  1.58it/s]Extractor Estimating: 186it [02:01,  1.46it/s]Extractor Estimating: 187it [02:02,  1.51it/s]Extractor Estimating: 188it [02:02,  1.56it/s]Extractor Estimating: 189it [02:03,  1.57it/s]Extractor Estimating: 190it [02:04,  1.56it/s]Extractor Estimating: 191it [02:05,  1.47it/s]Extractor Estimating: 192it [02:05,  1.49it/s]Extractor Estimating: 193it [02:06,  1.55it/s]Extractor Estimating: 194it [02:06,  1.58it/s]Extractor Estimating: 195it [02:07,  1.63it/s]Extractor Estimating: 196it [02:08,  1.58it/s]Extractor Estimating: 197it [02:08,  1.63it/s]Extractor Estimating: 198it [02:09,  1.57it/s]Extractor Estimating: 199it [02:09,  1.62it/s]Extractor Estimating: 200it [02:10,  1.62it/s]Extractor Estimating: 201it [02:11,  1.51it/s]Extractor Estimating: 202it [02:11,  1.55it/s]Extractor Estimating: 203it [02:12,  1.55it/s]Extractor Estimating: 204it [02:13,  1.55it/s]Extractor Estimating: 205it [02:13,  1.54it/s]Extractor Estimating: 206it [02:14,  1.43it/s]Extractor Estimating: 207it [02:15,  1.47it/s]Extractor Estimating: 208it [02:15,  1.48it/s]Extractor Estimating: 209it [02:16,  1.50it/s]Extractor Estimating: 210it [02:17,  1.59it/s]Extractor Estimating: 211it [02:17,  1.55it/s]Extractor Estimating: 212it [02:18,  1.57it/s]Extractor Estimating: 213it [02:19,  1.52it/s]Extractor Estimating: 214it [02:19,  1.49it/s]Extractor Estimating: 215it [02:20,  1.55it/s]Extractor Estimating: 216it [02:21,  1.55it/s]Extractor Estimating: 217it [02:21,  1.46it/s]Extractor Estimating: 218it [02:22,  1.42it/s]Extractor Estimating: 219it [02:23,  1.45it/s]Extractor Estimating: 220it [02:23,  1.47it/s]Extractor Estimating: 221it [02:24,  1.50it/s]Extractor Estimating: 222it [02:25,  1.53it/s]Extractor Estimating: 223it [02:25,  1.51it/s]Extractor Estimating: 224it [02:26,  1.59it/s]Extractor Estimating: 225it [02:27,  1.53it/s]Extractor Estimating: 226it [02:27,  1.57it/s]Extractor Estimating: 227it [02:28,  1.48it/s]Extractor Estimating: 228it [02:29,  1.42it/s]Extractor Estimating: 229it [02:29,  1.47it/s]Extractor Estimating: 230it [02:30,  1.55it/s]Extractor Estimating: 231it [02:31,  1.55it/s]Extractor Estimating: 232it [02:31,  1.56it/s]Extractor Estimating: 233it [02:32,  1.50it/s]Extractor Estimating: 234it [02:33,  1.54it/s]Extractor Estimating: 235it [02:33,  1.59it/s]Extractor Estimating: 236it [02:34,  1.63it/s]Extractor Estimating: 237it [02:34,  1.61it/s]Extractor Estimating: 238it [02:35,  1.59it/s]Extractor Estimating: 239it [02:36,  1.62it/s]Extractor Estimating: 240it [02:36,  1.64it/s]Extractor Estimating: 241it [02:37,  1.62it/s]Extractor Estimating: 242it [02:37,  1.69it/s]Extractor Estimating: 243it [02:38,  1.55it/s]Extractor Estimating: 244it [02:39,  1.59it/s]Extractor Estimating: 245it [02:39,  1.58it/s]Extractor Estimating: 246it [02:40,  1.56it/s]Extractor Estimating: 247it [02:41,  1.57it/s]Extractor Estimating: 248it [02:41,  1.56it/s]Extractor Estimating: 249it [02:42,  1.60it/s]Extractor Estimating: 250it [02:43,  1.49it/s]Extractor Estimating: 250it [02:43,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:18,020 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:18,106 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:18,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:18,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:18,107 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 01:10:19,745 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 01:10:19,746 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:10:20,660 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 01:10:21,956 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:10:22,046 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:24,844 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:24,846 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:24,846 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:24,847 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 01:10:24,847 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 01:10:26,223 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 01:10:26,331 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 01:10:27,220 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 01:10:27,687 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 01:10:27,687 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 02:39:30,903 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 02:39:31,873 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5232 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 23801
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23901, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23901, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.010, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.039, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 82, avg_time 1.018, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 182, avg_time 1.021, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 64, avg_time 1.035, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 164, avg_time 2.135, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 46, avg_time 1.022, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 146, avg_time 1.047, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 28, avg_time 1.010, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 128, avg_time 1.022, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 10, avg_time 2.090, loss:nan
g_step 1200, step 110, avg_time 1.031, loss:nan
g_step 1300, step 210, avg_time 1.017, loss:nan
g_step 1400, step 92, avg_time 1.014, loss:nan
g_step 1500, step 192, avg_time 1.023, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 74, avg_time 2.116, loss:nan
g_step 1700, step 174, avg_time 1.027, loss:nan
g_step 1800, step 56, avg_time 1.010, loss:nan
g_step 1900, step 156, avg_time 1.023, loss:nan
g_step 2000, step 38, avg_time 1.045, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 138, avg_time 2.113, loss:nan
g_step 2200, step 20, avg_time 1.006, loss:nan
g_step 2300, step 120, avg_time 1.020, loss:nan
g_step 2400, step 2, avg_time 1.025, loss:nan
g_step 2500, step 102, avg_time 1.046, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 202, avg_time 2.092, loss:nan
g_step 2700, step 84, avg_time 1.023, loss:nan
g_step 2800, step 184, avg_time 1.025, loss:nan
g_step 2900, step 66, avg_time 1.016, loss:nan
g_step 3000, step 166, avg_time 1.031, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 48, avg_time 2.103, loss:nan
g_step 3200, step 148, avg_time 1.030, loss:nan
g_step 3300, step 30, avg_time 1.012, loss:nan
g_step 3400, step 130, avg_time 1.036, loss:nan
g_step 3500, step 12, avg_time 1.022, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 112, avg_time 2.097, loss:nan
g_step 3700, step 212, avg_time 1.024, loss:nan
g_step 3800, step 94, avg_time 1.003, loss:nan
g_step 3900, step 194, avg_time 1.024, loss:nan
g_step 4000, step 76, avg_time 1.019, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 176, avg_time 2.081, loss:nan
g_step 4200, step 58, avg_time 1.003, loss:nan
g_step 4300, step 158, avg_time 1.025, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 02:39:31 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 02:39:31 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_02-39-30_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 02:39:34 - WARNING - datasets.builder -   Using custom data configuration default-cf81d7fe29d4f693
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-cf81d7fe29d4f693/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 02:39:40,313 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:39:40,314 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:39:40,315 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:39:40,316 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:39:40,516 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:40,601 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:40,602 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:40,602 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:40,602 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:40,602 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:39:40,602 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 02:39:41,264 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:39:44,488 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 02:39:44,537 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-cf81d7fe29d4f693/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.19ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.61ba/s] 50%|█████     | 3/6 [00:01<00:00,  3.23ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.63ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.90ba/s]100%|██████████| 6/6 [00:01<00:00,  3.96ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.36ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.25ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.36ba/s]100%|██████████| 4/4 [00:01<00:00,  4.43ba/s]100%|██████████| 4/4 [00:01<00:00,  3.81ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.94ba/s] 50%|█████     | 3/6 [00:00<00:00,  6.45ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  8.21ba/s]100%|██████████| 6/6 [00:00<00:00,  8.29ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.70ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.50ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.76ba/s]100%|██████████| 4/4 [00:01<00:00,  3.96ba/s]
[INFO|trainer.py:414] 2023-08-28 02:39:53,371 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 02:39:53,720 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 02:39:53,720 >>   Num examples = 5240
[INFO|trainer.py:1149] 2023-08-28 02:39:53,720 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 02:39:53,720 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 02:39:53,720 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 02:39:53,720 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 02:39:53,720 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:00<01:57,  3.47it/s]  0%|          | 2/410 [00:00<02:21,  2.88it/s]  1%|          | 3/410 [00:00<02:08,  3.16it/s]  1%|          | 4/410 [00:01<02:02,  3.32it/s]  1%|          | 5/410 [00:01<01:58,  3.42it/s]  1%|▏         | 6/410 [00:01<01:55,  3.49it/s]  2%|▏         | 7/410 [00:02<01:54,  3.53it/s]  2%|▏         | 8/410 [00:02<01:53,  3.55it/s]  2%|▏         | 9/410 [00:02<01:52,  3.57it/s]  2%|▏         | 10/410 [00:02<01:51,  3.58it/s]  3%|▎         | 11/410 [00:03<01:51,  3.59it/s]  3%|▎         | 12/410 [00:03<01:50,  3.59it/s]  3%|▎         | 13/410 [00:03<02:04,  3.19it/s]  3%|▎         | 14/410 [00:04<01:59,  3.31it/s]  4%|▎         | 15/410 [00:04<01:56,  3.39it/s]  4%|▍         | 16/410 [00:04<01:54,  3.45it/s]  4%|▍         | 17/410 [00:04<01:52,  3.50it/s]  4%|▍         | 18/410 [00:05<01:51,  3.52it/s]  5%|▍         | 19/410 [00:05<01:49,  3.56it/s]  5%|▍         | 20/410 [00:05<01:48,  3.59it/s]  5%|▌         | 21/410 [00:06<01:47,  3.61it/s]  5%|▌         | 22/410 [00:06<01:47,  3.62it/s]  6%|▌         | 23/410 [00:06<01:46,  3.63it/s]  6%|▌         | 24/410 [00:06<01:55,  3.34it/s]  6%|▌         | 25/410 [00:07<01:52,  3.43it/s]  6%|▋         | 26/410 [00:07<01:49,  3.50it/s]  7%|▋         | 27/410 [00:07<01:48,  3.54it/s]  7%|▋         | 28/410 [00:08<01:46,  3.58it/s]  7%|▋         | 29/410 [00:08<01:45,  3.60it/s]  7%|▋         | 30/410 [00:08<01:44,  3.62it/s]  8%|▊         | 31/410 [00:08<01:44,  3.63it/s]  8%|▊         | 32/410 [00:09<01:43,  3.64it/s]  8%|▊         | 33/410 [00:09<01:43,  3.64it/s]  8%|▊         | 34/410 [00:09<01:43,  3.64it/s]  9%|▊         | 35/410 [00:10<01:50,  3.40it/s]  9%|▉         | 36/410 [00:10<01:47,  3.47it/s]  9%|▉         | 37/410 [00:10<01:45,  3.52it/s]  9%|▉         | 38/410 [00:10<01:44,  3.55it/s] 10%|▉         | 39/410 [00:11<01:43,  3.58it/s] 10%|▉         | 40/410 [00:11<01:42,  3.60it/s] 10%|█         | 41/410 [00:11<01:42,  3.61it/s] 10%|█         | 42/410 [00:11<01:41,  3.62it/s] 10%|█         | 43/410 [00:12<01:41,  3.63it/s] 11%|█         | 44/410 [00:12<01:40,  3.64it/s] 11%|█         | 45/410 [00:12<01:40,  3.64it/s] 11%|█         | 46/410 [00:13<01:39,  3.65it/s] 11%|█▏        | 47/410 [00:13<01:39,  3.65it/s] 12%|█▏        | 48/410 [00:13<01:39,  3.65it/s] 12%|█▏        | 49/410 [00:13<01:38,  3.65it/s] 12%|█▏        | 50/410 [00:14<01:38,  3.65it/s] 12%|█▏        | 51/410 [00:14<01:38,  3.65it/s] 13%|█▎        | 52/410 [00:14<01:38,  3.65it/s] 13%|█▎        | 53/410 [00:14<01:37,  3.65it/s] 13%|█▎        | 54/410 [00:15<01:37,  3.64it/s] 13%|█▎        | 55/410 [00:15<01:44,  3.40it/s] 14%|█▎        | 56/410 [00:15<01:42,  3.47it/s] 14%|█▍        | 57/410 [00:16<01:40,  3.52it/s] 14%|█▍        | 58/410 [00:16<01:38,  3.56it/s] 14%|█▍        | 59/410 [00:16<01:37,  3.59it/s] 15%|█▍        | 60/410 [00:16<01:37,  3.60it/s] 15%|█▍        | 61/410 [00:17<01:36,  3.61it/s] 15%|█▌        | 62/410 [00:17<01:35,  3.63it/s] 15%|█▌        | 63/410 [00:17<01:35,  3.63it/s] 16%|█▌        | 64/410 [00:18<01:35,  3.63it/s] 16%|█▌        | 65/410 [00:18<01:34,  3.64it/s] 16%|█▌        | 66/410 [00:18<01:38,  3.48it/s] 16%|█▋        | 67/410 [00:18<01:37,  3.53it/s] 17%|█▋        | 68/410 [00:19<01:36,  3.56it/s] 17%|█▋        | 69/410 [00:19<01:35,  3.59it/s] 17%|█▋        | 70/410 [00:19<01:34,  3.60it/s] 17%|█▋        | 71/410 [00:20<01:33,  3.61it/s] 18%|█▊        | 72/410 [00:20<01:33,  3.62it/s] 18%|█▊        | 73/410 [00:20<01:32,  3.63it/s] 18%|█▊        | 74/410 [00:20<01:32,  3.63it/s] 18%|█▊        | 75/410 [00:21<01:32,  3.63it/s] 19%|█▊        | 76/410 [00:21<01:31,  3.63it/s] 19%|█▉        | 77/410 [00:21<01:35,  3.48it/s] 19%|█▉        | 78/410 [00:21<01:34,  3.53it/s] 19%|█▉        | 79/410 [00:22<01:32,  3.56it/s] 20%|█▉        | 80/410 [00:22<01:32,  3.59it/s] 20%|█▉        | 81/410 [00:22<01:31,  3.61it/s] 20%|██        | 82/410 [00:23<01:27,  3.75it/s][INFO|trainer.py:2140] 2023-08-28 02:40:16,764 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:40:16,764 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 02:40:16,764 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.55it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.41it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.74it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.63it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.99it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.42it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.08it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.79it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.79it/s][A
 12%|█▏        | 52/438 [00:01<00:10, 36.70it/s][A
 13%|█▎        | 57/438 [00:01<00:09, 38.93it/s][A
 14%|█▍        | 62/438 [00:01<00:09, 40.84it/s][A
 15%|█▌        | 67/438 [00:01<00:11, 33.11it/s][A
 16%|█▋        | 72/438 [00:01<00:10, 35.99it/s][A
 18%|█▊        | 77/438 [00:01<00:09, 38.45it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 40.21it/s][A
 20%|█▉        | 87/438 [00:02<00:08, 41.63it/s][A
 21%|██        | 92/438 [00:02<00:08, 42.67it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.51it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 43.94it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 43.84it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.03it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.37it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.53it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.78it/s][A
 30%|███       | 132/438 [00:03<00:06, 44.97it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 45.04it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 45.07it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.91it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.64it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.70it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.71it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.93it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.98it/s][A
 40%|████      | 177/438 [00:04<00:06, 37.59it/s][A
 42%|████▏     | 182/438 [00:04<00:06, 38.74it/s][A
 43%|████▎     | 187/438 [00:04<00:06, 40.84it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 42.14it/s][A
 45%|████▍     | 197/438 [00:04<00:06, 35.98it/s][A
 46%|████▌     | 202/438 [00:04<00:06, 38.40it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 40.32it/s][A
 48%|████▊     | 212/438 [00:05<00:05, 41.67it/s][A
 50%|████▉     | 217/438 [00:05<00:05, 42.70it/s][A
 51%|█████     | 222/438 [00:05<00:04, 43.46it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.08it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.35it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.17it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.05it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.20it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.55it/s][A
 59%|█████▊    | 257/438 [00:06<00:04, 44.73it/s][A
 60%|█████▉    | 262/438 [00:06<00:03, 44.89it/s][A
 61%|██████    | 267/438 [00:06<00:03, 45.04it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 45.15it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 45.14it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.74it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.53it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.61it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.70it/s][A
 69%|██████▉   | 302/438 [00:07<00:03, 38.20it/s][A
 70%|███████   | 307/438 [00:07<00:03, 40.13it/s][A
 71%|███████   | 312/438 [00:07<00:03, 41.64it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 42.66it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 43.55it/s][A
 75%|███████▍  | 327/438 [00:07<00:03, 34.61it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 37.18it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 39.30it/s][A
 78%|███████▊  | 342/438 [00:08<00:02, 40.87it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 42.14it/s][A
 80%|████████  | 352/438 [00:08<00:02, 42.98it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 43.69it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.10it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.04it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.14it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.24it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.56it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 44.76it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 44.94it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.91it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 37.50it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 39.85it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 41.34it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 42.47it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.28it/s][A
 97%|█████████▋| 427/438 [00:10<00:00, 43.93it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 44.38it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.70it/s][A                                                
                                                 [A 20%|██        | 82/410 [00:33<01:27,  3.75it/s]
100%|██████████| 438/438 [00:10<00:00, 44.70it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:40:27,542 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 02:40:29,205 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:40:54,944 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:40:55,595 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:40:55,768 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [01:06<1:11:55, 13.20s/it] 20%|██        | 84/410 [01:06<50:46,  9.35s/it]   21%|██        | 85/410 [01:07<35:53,  6.63s/it] 21%|██        | 86/410 [01:07<25:29,  4.72s/it] 21%|██        | 87/410 [01:07<18:14,  3.39s/it] 21%|██▏       | 88/410 [01:07<13:10,  2.45s/it] 22%|██▏       | 89/410 [01:08<09:38,  1.80s/it] 22%|██▏       | 90/410 [01:08<07:10,  1.34s/it] 22%|██▏       | 91/410 [01:08<05:26,  1.02s/it] 22%|██▏       | 92/410 [01:08<04:14,  1.25it/s] 23%|██▎       | 93/410 [01:09<03:23,  1.55it/s] 23%|██▎       | 94/410 [01:09<02:48,  1.87it/s] 23%|██▎       | 95/410 [01:09<02:34,  2.03it/s] 23%|██▎       | 96/410 [01:10<02:14,  2.34it/s] 24%|██▎       | 97/410 [01:10<01:59,  2.62it/s] 24%|██▍       | 98/410 [01:10<01:49,  2.85it/s] 24%|██▍       | 99/410 [01:11<01:42,  3.04it/s] 24%|██▍       | 100/410 [01:11<01:37,  3.19it/s] 25%|██▍       | 101/410 [01:11<01:33,  3.31it/s] 25%|██▍       | 102/410 [01:11<01:30,  3.39it/s] 25%|██▌       | 103/410 [01:12<01:28,  3.45it/s] 25%|██▌       | 104/410 [01:12<01:27,  3.49it/s] 26%|██▌       | 105/410 [01:12<01:26,  3.52it/s] 26%|██▌       | 106/410 [01:13<01:35,  3.19it/s] 26%|██▌       | 107/410 [01:13<01:31,  3.30it/s] 26%|██▋       | 108/410 [01:13<01:29,  3.39it/s] 27%|██▋       | 109/410 [01:13<01:28,  3.41it/s] 27%|██▋       | 110/410 [01:14<01:26,  3.46it/s] 27%|██▋       | 111/410 [01:14<01:25,  3.50it/s] 27%|██▋       | 112/410 [01:14<01:24,  3.53it/s] 28%|██▊       | 113/410 [01:15<01:23,  3.55it/s] 28%|██▊       | 114/410 [01:15<01:23,  3.56it/s] 28%|██▊       | 115/410 [01:15<01:22,  3.57it/s] 28%|██▊       | 116/410 [01:15<01:22,  3.58it/s] 29%|██▊       | 117/410 [01:16<01:21,  3.59it/s] 29%|██▉       | 118/410 [01:16<01:21,  3.59it/s] 29%|██▉       | 119/410 [01:16<01:21,  3.59it/s] 29%|██▉       | 120/410 [01:17<01:29,  3.25it/s] 30%|██▉       | 121/410 [01:17<01:26,  3.34it/s] 30%|██▉       | 122/410 [01:17<01:24,  3.41it/s] 30%|███       | 123/410 [01:17<01:22,  3.46it/s] 30%|███       | 124/410 [01:18<01:21,  3.50it/s] 30%|███       | 125/410 [01:18<01:20,  3.53it/s] 31%|███       | 126/410 [01:18<01:20,  3.55it/s] 31%|███       | 127/410 [01:19<01:19,  3.56it/s] 31%|███       | 128/410 [01:19<01:18,  3.57it/s] 31%|███▏      | 129/410 [01:19<01:18,  3.58it/s] 32%|███▏      | 130/410 [01:19<01:18,  3.58it/s] 32%|███▏      | 131/410 [01:20<01:29,  3.13it/s] 32%|███▏      | 132/410 [01:20<01:25,  3.25it/s] 32%|███▏      | 133/410 [01:20<01:22,  3.34it/s] 33%|███▎      | 134/410 [01:21<01:20,  3.41it/s] 33%|███▎      | 135/410 [01:21<01:19,  3.46it/s] 33%|███▎      | 136/410 [01:21<01:18,  3.50it/s] 33%|███▎      | 137/410 [01:21<01:17,  3.52it/s] 34%|███▎      | 138/410 [01:22<01:16,  3.54it/s] 34%|███▍      | 139/410 [01:22<01:16,  3.56it/s] 34%|███▍      | 140/410 [01:22<01:15,  3.57it/s] 34%|███▍      | 141/410 [01:23<01:15,  3.58it/s] 35%|███▍      | 142/410 [01:23<01:22,  3.25it/s] 35%|███▍      | 143/410 [01:23<01:19,  3.35it/s] 35%|███▌      | 144/410 [01:24<01:17,  3.43it/s] 35%|███▌      | 145/410 [01:24<01:15,  3.49it/s] 36%|███▌      | 146/410 [01:24<01:14,  3.54it/s] 36%|███▌      | 147/410 [01:24<01:13,  3.57it/s] 36%|███▌      | 148/410 [01:25<01:12,  3.59it/s] 36%|███▋      | 149/410 [01:25<01:12,  3.61it/s] 37%|███▋      | 150/410 [01:25<01:11,  3.62it/s] 37%|███▋      | 151/410 [01:25<01:11,  3.63it/s] 37%|███▋      | 152/410 [01:26<01:11,  3.63it/s] 37%|███▋      | 153/410 [01:26<01:18,  3.28it/s] 38%|███▊      | 154/410 [01:26<01:15,  3.38it/s] 38%|███▊      | 155/410 [01:27<01:13,  3.45it/s] 38%|███▊      | 156/410 [01:27<01:12,  3.51it/s] 38%|███▊      | 157/410 [01:27<01:11,  3.55it/s] 39%|███▊      | 158/410 [01:27<01:10,  3.58it/s] 39%|███▉      | 159/410 [01:28<01:09,  3.59it/s] 39%|███▉      | 160/410 [01:28<01:09,  3.61it/s] 39%|███▉      | 161/410 [01:28<01:08,  3.62it/s] 40%|███▉      | 162/410 [01:29<01:08,  3.62it/s] 40%|███▉      | 163/410 [01:29<01:08,  3.63it/s] 40%|████      | 164/410 [01:29<01:13,  3.37it/s][INFO|trainer.py:2140] 2023-08-28 02:41:23,393 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:41:23,393 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 02:41:23,393 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.2862, 'eval_samples_per_second': 339.97, 'eval_steps_per_second': 42.581, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.54it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.17it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.64it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.88it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.42it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.83it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.14it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.67it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.79it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.87it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.00it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.17it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.25it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.35it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.15it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.84it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.49it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.68it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.76it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.80it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.97it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.14it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.24it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 45.12it/s][A
 29%|██▉       | 127/438 [00:03<00:10, 28.57it/s][A
 30%|███       | 132/438 [00:03<00:09, 32.14it/s][A
 31%|███▏      | 137/438 [00:03<00:08, 35.24it/s][A
 32%|███▏      | 142/438 [00:03<00:07, 37.82it/s][A
 34%|███▎      | 147/438 [00:03<00:07, 39.75it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 41.29it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 42.44it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 43.30it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 43.35it/s][A
 39%|███▉      | 172/438 [00:04<00:06, 43.52it/s][A
 40%|████      | 177/438 [00:04<00:05, 44.10it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.44it/s][A
 43%|████▎     | 187/438 [00:04<00:06, 36.76it/s][A
 44%|████▍     | 192/438 [00:04<00:06, 39.05it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 40.80it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 42.07it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 43.08it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 43.71it/s][A
 50%|████▉     | 217/438 [00:05<00:05, 44.18it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.62it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.35it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.14it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.11it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.43it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.66it/s][A
 58%|█████▊    | 252/438 [00:05<00:05, 35.83it/s][A
 59%|█████▊    | 257/438 [00:06<00:04, 38.27it/s][A
 60%|█████▉    | 262/438 [00:06<00:04, 40.19it/s][A
 61%|██████    | 267/438 [00:06<00:04, 41.58it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 42.67it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 43.47it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.07it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.37it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.07it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 43.62it/s][A
 69%|██████▉   | 302/438 [00:07<00:03, 44.31it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.57it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.79it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.98it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 45.10it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 45.30it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.99it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.71it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.62it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 44.44it/s][A
 80%|████████  | 352/438 [00:08<00:01, 44.69it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.87it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.06it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.20it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 45.25it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 45.01it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.71it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 37.28it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 39.30it/s][A
 91%|█████████ | 397/438 [00:09<00:01, 40.96it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 42.11it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 43.03it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 43.71it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.11it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.34it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.12it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 44.34it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.52it/s][A                                                 
                                                 [A 40%|████      | 164/410 [01:39<01:13,  3.37it/s]
100%|██████████| 438/438 [00:10<00:00, 44.52it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:41:34,273 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 02:41:35,005 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:41:57,341 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:41:57,796 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:41:57,993 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [02:08<48:33, 11.89s/it] 40%|████      | 166/410 [02:08<34:16,  8.43s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [02:09<24:14,  5.98s/it] 41%|████      | 168/410 [02:09<17:13,  4.27s/it] 41%|████      | 169/410 [02:09<12:20,  3.07s/it] 41%|████▏     | 170/410 [02:10<08:55,  2.23s/it] 42%|████▏     | 171/410 [02:10<06:33,  1.64s/it] 42%|████▏     | 172/410 [02:10<04:53,  1.23s/it] 42%|████▏     | 173/410 [02:10<03:44,  1.06it/s] 42%|████▏     | 174/410 [02:11<02:55,  1.34it/s] 43%|████▎     | 175/410 [02:11<02:21,  1.66it/s] 43%|████▎     | 176/410 [02:11<01:57,  1.98it/s] 43%|████▎     | 177/410 [02:12<01:48,  2.15it/s] 43%|████▎     | 178/410 [02:12<01:34,  2.45it/s] 44%|████▎     | 179/410 [02:12<01:24,  2.72it/s] 44%|████▍     | 180/410 [02:12<01:18,  2.95it/s] 44%|████▍     | 181/410 [02:13<01:13,  3.13it/s] 44%|████▍     | 182/410 [02:13<01:09,  3.27it/s] 45%|████▍     | 183/410 [02:13<01:07,  3.37it/s] 45%|████▍     | 184/410 [02:14<01:05,  3.45it/s] 45%|████▌     | 185/410 [02:14<01:04,  3.51it/s] 45%|████▌     | 186/410 [02:14<01:03,  3.55it/s] 46%|████▌     | 187/410 [02:14<01:02,  3.58it/s] 46%|████▌     | 188/410 [02:15<01:19,  2.78it/s] 46%|████▌     | 189/410 [02:15<01:13,  2.99it/s] 46%|████▋     | 190/410 [02:15<01:09,  3.16it/s] 47%|████▋     | 191/410 [02:16<01:06,  3.29it/s] 47%|████▋     | 192/410 [02:16<01:04,  3.39it/s] 47%|████▋     | 193/410 [02:16<01:02,  3.47it/s] 47%|████▋     | 194/410 [02:17<01:01,  3.52it/s] 48%|████▊     | 195/410 [02:17<01:00,  3.56it/s] 48%|████▊     | 196/410 [02:17<00:59,  3.58it/s] 48%|████▊     | 197/410 [02:17<00:59,  3.60it/s] 48%|████▊     | 198/410 [02:18<00:58,  3.62it/s] 49%|████▊     | 199/410 [02:18<01:05,  3.21it/s] 49%|████▉     | 200/410 [02:18<01:03,  3.33it/s] 49%|████▉     | 201/410 [02:19<01:01,  3.42it/s] 49%|████▉     | 202/410 [02:19<00:59,  3.48it/s] 50%|████▉     | 203/410 [02:19<00:58,  3.53it/s] 50%|████▉     | 204/410 [02:19<00:57,  3.57it/s] 50%|█████     | 205/410 [02:20<00:57,  3.59it/s] 50%|█████     | 206/410 [02:20<00:56,  3.60it/s] 50%|█████     | 207/410 [02:20<00:56,  3.62it/s] 51%|█████     | 208/410 [02:20<00:55,  3.63it/s] 51%|█████     | 209/410 [02:21<00:55,  3.63it/s] 51%|█████     | 210/410 [02:21<01:02,  3.20it/s] 51%|█████▏    | 211/410 [02:21<00:59,  3.32it/s] 52%|█████▏    | 212/410 [02:22<00:58,  3.41it/s] 52%|█████▏    | 213/410 [02:22<00:56,  3.48it/s] 52%|█████▏    | 214/410 [02:22<00:55,  3.53it/s] 52%|█████▏    | 215/410 [02:23<00:54,  3.56it/s] 53%|█████▎    | 216/410 [02:23<00:54,  3.59it/s] 53%|█████▎    | 217/410 [02:23<00:53,  3.60it/s] 53%|█████▎    | 218/410 [02:23<00:53,  3.62it/s] 53%|█████▎    | 219/410 [02:24<00:52,  3.63it/s] 54%|█████▎    | 220/410 [02:24<00:52,  3.63it/s] 54%|█████▍    | 221/410 [02:24<00:57,  3.29it/s] 54%|█████▍    | 222/410 [02:25<00:55,  3.39it/s] 54%|█████▍    | 223/410 [02:25<00:53,  3.46it/s] 55%|█████▍    | 224/410 [02:25<00:52,  3.52it/s] 55%|█████▍    | 225/410 [02:25<00:51,  3.56it/s] 55%|█████▌    | 226/410 [02:26<00:51,  3.58it/s] 55%|█████▌    | 227/410 [02:26<00:50,  3.60it/s] 56%|█████▌    | 228/410 [02:26<00:50,  3.62it/s] 56%|█████▌    | 229/410 [02:26<00:49,  3.63it/s] 56%|█████▌    | 230/410 [02:27<00:49,  3.63it/s] 56%|█████▋    | 231/410 [02:27<00:49,  3.64it/s] 57%|█████▋    | 232/410 [02:28<01:10,  2.51it/s] 57%|█████▋    | 233/410 [02:28<01:03,  2.77it/s] 57%|█████▋    | 234/410 [02:28<00:58,  2.98it/s] 57%|█████▋    | 235/410 [02:29<00:55,  3.16it/s] 58%|█████▊    | 236/410 [02:29<00:52,  3.29it/s] 58%|█████▊    | 237/410 [02:29<00:51,  3.39it/s] 58%|█████▊    | 238/410 [02:29<00:49,  3.46it/s] 58%|█████▊    | 239/410 [02:30<00:48,  3.51it/s] 59%|█████▊    | 240/410 [02:30<00:47,  3.55it/s] 59%|█████▉    | 241/410 [02:30<00:47,  3.58it/s] 59%|█████▉    | 242/410 [02:31<00:53,  3.16it/s] 59%|█████▉    | 243/410 [02:31<00:50,  3.29it/s] 60%|█████▉    | 244/410 [02:31<00:48,  3.39it/s] 60%|█████▉    | 245/410 [02:31<00:47,  3.46it/s] 60%|██████    | 246/410 [02:32<00:45,  3.64it/s][INFO|trainer.py:2140] 2023-08-28 02:42:25,850 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:42:25,850 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 02:42:25,850 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.1828, 'eval_samples_per_second': 343.422, 'eval_steps_per_second': 43.014, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.44it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.47it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.48it/s][A
  5%|▌         | 23/438 [00:00<00:11, 35.69it/s][A
  7%|▋         | 29/438 [00:00<00:10, 39.97it/s][A
  8%|▊         | 34/438 [00:00<00:09, 41.61it/s][A
  9%|▉         | 39/438 [00:00<00:09, 42.69it/s][A
 10%|█         | 44/438 [00:01<00:09, 43.48it/s][A
 11%|█         | 49/438 [00:01<00:08, 44.08it/s][A
 12%|█▏        | 54/438 [00:01<00:08, 44.47it/s][A
 13%|█▎        | 59/438 [00:01<00:08, 44.76it/s][A
 15%|█▍        | 64/438 [00:01<00:08, 44.75it/s][A
 16%|█▌        | 69/438 [00:01<00:08, 44.29it/s][A
 17%|█▋        | 74/438 [00:01<00:10, 35.03it/s][A
 18%|█▊        | 78/438 [00:02<00:12, 28.92it/s][A
 19%|█▉        | 83/438 [00:02<00:10, 32.68it/s][A
 20%|██        | 88/438 [00:02<00:09, 35.83it/s][A
 21%|██        | 93/438 [00:02<00:09, 38.25it/s][A
 22%|██▏       | 98/438 [00:02<00:08, 40.22it/s][A
 24%|██▎       | 103/438 [00:02<00:08, 41.62it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 42.68it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 43.47it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 43.53it/s][A
 28%|██▊       | 123/438 [00:03<00:07, 43.76it/s][A
 29%|██▉       | 128/438 [00:03<00:07, 44.08it/s][A
 30%|███       | 133/438 [00:03<00:06, 44.44it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.59it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.90it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.98it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.10it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.99it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.81it/s][A
 38%|███▊      | 168/438 [00:04<00:06, 44.65it/s][A
 39%|███▉      | 173/438 [00:04<00:05, 44.67it/s][A
 41%|████      | 178/438 [00:04<00:05, 44.87it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 44.91it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.13it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.13it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.02it/s][A
 46%|████▋     | 203/438 [00:04<00:06, 37.47it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 39.52it/s][A
 49%|████▊     | 213/438 [00:05<00:05, 41.10it/s][A
 50%|████▉     | 218/438 [00:05<00:05, 42.26it/s][A
 51%|█████     | 223/438 [00:05<00:04, 43.22it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 43.81it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 44.27it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 44.54it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 44.30it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 44.17it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.37it/s][A
 59%|█████▉    | 258/438 [00:06<00:04, 44.70it/s][A
 60%|██████    | 263/438 [00:06<00:03, 44.84it/s][A
 61%|██████    | 268/438 [00:06<00:03, 45.05it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 45.20it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 45.07it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 44.92it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.65it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.48it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.45it/s][A
 69%|██████▉   | 303/438 [00:07<00:03, 44.65it/s][A
 70%|███████   | 308/438 [00:07<00:02, 44.82it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 44.96it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.09it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.28it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.03it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 44.83it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 35.92it/s][A
 78%|███████▊  | 343/438 [00:08<00:02, 38.45it/s][A
 79%|███████▉  | 348/438 [00:08<00:02, 40.31it/s][A
 81%|████████  | 353/438 [00:08<00:02, 41.71it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 42.82it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 43.59it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 44.06it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.37it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 44.15it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 43.95it/s][A
 89%|████████▊ | 388/438 [00:09<00:01, 44.01it/s][A
 90%|████████▉ | 393/438 [00:09<00:01, 44.21it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 44.72it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.94it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 45.06it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 45.23it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 45.21it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 44.87it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 44.64it/s][A
 99%|█████████▉| 433/438 [00:10<00:00, 44.56it/s][A
100%|██████████| 438/438 [00:10<00:00, 44.65it/s][A                                                 
                                                 [A 60%|██████    | 246/410 [02:42<00:45,  3.64it/s]
100%|██████████| 438/438 [00:10<00:00, 44.65it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:42:36,817 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 02:42:37,542 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:42:53,826 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:42:54,209 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:42:54,438 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [03:03<26:20,  9.69s/it] 60%|██████    | 248/410 [03:04<18:35,  6.89s/it] 61%|██████    | 249/410 [03:04<13:09,  4.91s/it] 61%|██████    | 250/410 [03:04<09:22,  3.52s/it] 61%|██████    | 251/410 [03:04<06:44,  2.55s/it] 61%|██████▏   | 252/410 [03:05<04:54,  1.87s/it] 62%|██████▏   | 253/410 [03:06<04:27,  1.71s/it] 62%|██████▏   | 254/410 [03:06<03:19,  1.28s/it] 62%|██████▏   | 255/410 [03:07<02:35,  1.00s/it] 62%|██████▏   | 256/410 [03:07<02:00,  1.28it/s] 63%|██████▎   | 257/410 [03:07<01:36,  1.58it/s] 63%|██████▎   | 258/410 [03:08<01:19,  1.90it/s] 63%|██████▎   | 259/410 [03:08<01:08,  2.21it/s] 63%|██████▎   | 260/410 [03:08<00:59,  2.50it/s] 64%|██████▎   | 261/410 [03:08<00:54,  2.76it/s] 64%|██████▍   | 262/410 [03:09<00:49,  2.96it/s] 64%|██████▍   | 263/410 [03:09<00:47,  3.13it/s] 64%|██████▍   | 264/410 [03:09<00:44,  3.26it/s] 65%|██████▍   | 265/410 [03:09<00:43,  3.35it/s] 65%|██████▍   | 266/410 [03:10<00:46,  3.10it/s] 65%|██████▌   | 267/410 [03:10<00:44,  3.23it/s] 65%|██████▌   | 268/410 [03:10<00:42,  3.33it/s] 66%|██████▌   | 269/410 [03:11<00:41,  3.40it/s] 66%|██████▌   | 270/410 [03:11<00:40,  3.46it/s] 66%|██████▌   | 271/410 [03:11<00:39,  3.50it/s] 66%|██████▋   | 272/410 [03:12<00:39,  3.52it/s] 67%|██████▋   | 273/410 [03:12<00:38,  3.55it/s] 67%|██████▋   | 274/410 [03:12<00:38,  3.56it/s] 67%|██████▋   | 275/410 [03:12<00:37,  3.57it/s] 67%|██████▋   | 276/410 [03:13<00:37,  3.58it/s] 68%|██████▊   | 277/410 [03:13<00:40,  3.26it/s] 68%|██████▊   | 278/410 [03:13<00:39,  3.36it/s] 68%|██████▊   | 279/410 [03:14<00:38,  3.43it/s] 68%|██████▊   | 280/410 [03:14<00:37,  3.48it/s] 69%|██████▊   | 281/410 [03:14<00:36,  3.51it/s] 69%|██████▉   | 282/410 [03:14<00:36,  3.54it/s] 69%|██████▉   | 283/410 [03:15<00:35,  3.55it/s] 69%|██████▉   | 284/410 [03:15<00:35,  3.56it/s] 70%|██████▉   | 285/410 [03:15<00:35,  3.56it/s] 70%|██████▉   | 286/410 [03:16<00:34,  3.57it/s] 70%|███████   | 287/410 [03:16<00:34,  3.57it/s] 70%|███████   | 288/410 [03:16<00:43,  2.83it/s] 70%|███████   | 289/410 [03:17<00:40,  3.02it/s] 71%|███████   | 290/410 [03:17<00:37,  3.17it/s] 71%|███████   | 291/410 [03:17<00:36,  3.29it/s] 71%|███████   | 292/410 [03:17<00:34,  3.37it/s] 71%|███████▏  | 293/410 [03:18<00:34,  3.43it/s] 72%|███████▏  | 294/410 [03:18<00:33,  3.48it/s] 72%|███████▏  | 295/410 [03:18<00:32,  3.51it/s] 72%|███████▏  | 296/410 [03:19<00:32,  3.54it/s] 72%|███████▏  | 297/410 [03:19<00:31,  3.57it/s] 73%|███████▎  | 298/410 [03:19<00:31,  3.59it/s] 73%|███████▎  | 299/410 [03:20<00:34,  3.23it/s] 73%|███████▎  | 300/410 [03:20<00:32,  3.34it/s] 73%|███████▎  | 301/410 [03:20<00:31,  3.43it/s] 74%|███████▎  | 302/410 [03:20<00:30,  3.49it/s] 74%|███████▍  | 303/410 [03:21<00:30,  3.54it/s] 74%|███████▍  | 304/410 [03:21<00:29,  3.57it/s] 74%|███████▍  | 305/410 [03:21<00:29,  3.59it/s] 75%|███████▍  | 306/410 [03:21<00:28,  3.61it/s] 75%|███████▍  | 307/410 [03:22<00:28,  3.62it/s] 75%|███████▌  | 308/410 [03:22<00:28,  3.63it/s] 75%|███████▌  | 309/410 [03:22<00:27,  3.63it/s] 76%|███████▌  | 310/410 [03:23<00:30,  3.23it/s] 76%|███████▌  | 311/410 [03:23<00:29,  3.34it/s] 76%|███████▌  | 312/410 [03:23<00:28,  3.43it/s] 76%|███████▋  | 313/410 [03:23<00:27,  3.49it/s] 77%|███████▋  | 314/410 [03:24<00:27,  3.53it/s] 77%|███████▋  | 315/410 [03:24<00:26,  3.56it/s] 77%|███████▋  | 316/410 [03:24<00:26,  3.59it/s] 77%|███████▋  | 317/410 [03:25<00:25,  3.61it/s] 78%|███████▊  | 318/410 [03:25<00:25,  3.62it/s] 78%|███████▊  | 319/410 [03:25<00:25,  3.62it/s] 78%|███████▊  | 320/410 [03:25<00:24,  3.63it/s] 78%|███████▊  | 321/410 [03:26<00:27,  3.22it/s] 79%|███████▊  | 322/410 [03:26<00:26,  3.34it/s] 79%|███████▉  | 323/410 [03:26<00:25,  3.43it/s] 79%|███████▉  | 324/410 [03:27<00:24,  3.49it/s] 79%|███████▉  | 325/410 [03:27<00:24,  3.53it/s] 80%|███████▉  | 326/410 [03:27<00:23,  3.56it/s] 80%|███████▉  | 327/410 [03:27<00:23,  3.59it/s] 80%|████████  | 328/410 [03:28<00:21,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 02:43:21,884 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:43:21,884 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 02:43:21,884 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.1856, 'eval_samples_per_second': 343.327, 'eval_steps_per_second': 43.002, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.70it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.39it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.67it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.72it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.91it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.43it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.00it/s][A
 10%|▉         | 42/438 [00:01<00:11, 33.09it/s][A
 11%|█         | 47/438 [00:01<00:10, 36.14it/s][A
 12%|█▏        | 52/438 [00:01<00:10, 38.53it/s][A
 13%|█▎        | 57/438 [00:01<00:09, 40.33it/s][A
 14%|█▍        | 62/438 [00:01<00:09, 41.76it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 42.76it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 43.48it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 43.99it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 43.95it/s][A
 20%|█▉        | 87/438 [00:02<00:07, 44.07it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.34it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.63it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.85it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.98it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.07it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.08it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.82it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.70it/s][A
 30%|███       | 132/438 [00:03<00:06, 44.75it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.83it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.91it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.93it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.01it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.08it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.02it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.92it/s][A
 39%|███▉      | 172/438 [00:04<00:07, 36.83it/s][A
 40%|████      | 177/438 [00:04<00:06, 39.03it/s][A
 42%|████▏     | 182/438 [00:04<00:06, 40.80it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 42.10it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 43.05it/s][A
 45%|████▍     | 197/438 [00:04<00:07, 33.25it/s][A
 46%|████▌     | 202/438 [00:04<00:06, 36.78it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 38.98it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 40.64it/s][A
 50%|████▉     | 217/438 [00:05<00:05, 42.06it/s][A
 51%|█████     | 222/438 [00:05<00:05, 43.01it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 43.71it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.18it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.20it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.01it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.23it/s][A
 58%|█████▊    | 252/438 [00:05<00:05, 36.48it/s][A
 59%|█████▊    | 257/438 [00:06<00:04, 38.78it/s][A
 60%|█████▉    | 262/438 [00:06<00:04, 40.63it/s][A
 61%|██████    | 267/438 [00:06<00:04, 41.95it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 42.96it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 43.72it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.27it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.56it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.36it/s][A
 68%|██████▊   | 297/438 [00:07<00:04, 32.85it/s][A
 69%|██████▉   | 302/438 [00:07<00:03, 35.74it/s][A
 70%|███████   | 307/438 [00:07<00:03, 38.14it/s][A
 71%|███████   | 312/438 [00:07<00:03, 40.08it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 41.47it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 42.59it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 43.39it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 43.73it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 43.74it/s][A
 78%|███████▊  | 342/438 [00:08<00:02, 44.04it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 44.38it/s][A
 80%|████████  | 352/438 [00:08<00:01, 44.56it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.83it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.93it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.95it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.98it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.84it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.69it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 44.59it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 44.83it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.78it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.93it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 45.13it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 45.10it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.99it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.85it/s][A
 97%|█████████▋| 427/438 [00:10<00:00, 38.23it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 40.07it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 41.55it/s][A                                                 
                                                 [A 80%|████████  | 328/410 [03:38<00:21,  3.74it/s]
100%|██████████| 438/438 [00:10<00:00, 41.55it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:43:32,560 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 02:43:33,192 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:43:49,366 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:43:50,094 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:43:50,350 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [03:59<12:51,  9.52s/it] 80%|████████  | 330/410 [03:59<09:01,  6.77s/it] 81%|████████  | 331/410 [03:59<06:21,  4.82s/it] 81%|████████  | 332/410 [04:00<04:29,  3.46s/it] 81%|████████  | 333/410 [04:00<03:12,  2.51s/it] 81%|████████▏ | 334/410 [04:00<02:19,  1.84s/it] 82%|████████▏ | 335/410 [04:01<01:42,  1.37s/it] 82%|████████▏ | 336/410 [04:01<01:17,  1.04s/it] 82%|████████▏ | 337/410 [04:01<00:59,  1.23it/s] 82%|████████▏ | 338/410 [04:01<00:46,  1.53it/s] 83%|████████▎ | 339/410 [04:02<00:38,  1.85it/s] 83%|████████▎ | 340/410 [04:02<00:32,  2.17it/s] 83%|████████▎ | 341/410 [04:02<00:30,  2.29it/s] 83%|████████▎ | 342/410 [04:03<00:26,  2.57it/s] 84%|████████▎ | 343/410 [04:03<00:23,  2.81it/s] 84%|████████▍ | 344/410 [04:03<00:21,  3.01it/s] 84%|████████▍ | 345/410 [04:03<00:20,  3.16it/s] 84%|████████▍ | 346/410 [04:04<00:19,  3.28it/s] 85%|████████▍ | 347/410 [04:04<00:18,  3.36it/s] 85%|████████▍ | 348/410 [04:04<00:18,  3.43it/s] 85%|████████▌ | 349/410 [04:05<00:17,  3.48it/s] 85%|████████▌ | 350/410 [04:05<00:17,  3.51it/s] 86%|████████▌ | 351/410 [04:06<00:34,  1.69it/s] 86%|████████▌ | 352/410 [04:06<00:30,  1.90it/s] 86%|████████▌ | 353/410 [04:07<00:25,  2.22it/s] 86%|████████▋ | 354/410 [04:07<00:22,  2.51it/s] 87%|████████▋ | 355/410 [04:07<00:19,  2.76it/s] 87%|████████▋ | 356/410 [04:08<00:18,  2.97it/s] 87%|████████▋ | 357/410 [04:08<00:16,  3.13it/s] 87%|████████▋ | 358/410 [04:08<00:15,  3.26it/s] 88%|████████▊ | 359/410 [04:08<00:15,  3.35it/s] 88%|████████▊ | 360/410 [04:09<00:14,  3.42it/s] 88%|████████▊ | 361/410 [04:09<00:14,  3.47it/s] 88%|████████▊ | 362/410 [04:09<00:13,  3.50it/s] 89%|████████▊ | 363/410 [04:10<00:14,  3.23it/s] 89%|████████▉ | 364/410 [04:10<00:13,  3.33it/s] 89%|████████▉ | 365/410 [04:10<00:13,  3.40it/s] 89%|████████▉ | 366/410 [04:10<00:12,  3.46it/s] 90%|████████▉ | 367/410 [04:11<00:12,  3.50it/s] 90%|████████▉ | 368/410 [04:11<00:11,  3.52it/s] 90%|█████████ | 369/410 [04:11<00:11,  3.54it/s] 90%|█████████ | 370/410 [04:12<00:11,  3.56it/s] 90%|█████████ | 371/410 [04:12<00:10,  3.57it/s] 91%|█████████ | 372/410 [04:12<00:10,  3.58it/s] 91%|█████████ | 373/410 [04:12<00:10,  3.58it/s] 91%|█████████ | 374/410 [04:13<00:11,  3.18it/s] 91%|█████████▏| 375/410 [04:13<00:10,  3.30it/s] 92%|█████████▏| 376/410 [04:13<00:10,  3.38it/s] 92%|█████████▏| 377/410 [04:14<00:09,  3.44it/s] 92%|█████████▏| 378/410 [04:14<00:09,  3.48it/s] 92%|█████████▏| 379/410 [04:14<00:08,  3.51it/s] 93%|█████████▎| 380/410 [04:14<00:08,  3.53it/s] 93%|█████████▎| 381/410 [04:15<00:08,  3.55it/s] 93%|█████████▎| 382/410 [04:15<00:07,  3.56it/s] 93%|█████████▎| 383/410 [04:15<00:07,  3.57it/s] 94%|█████████▎| 384/410 [04:16<00:07,  3.57it/s] 94%|█████████▍| 385/410 [04:16<00:07,  3.27it/s] 94%|█████████▍| 386/410 [04:16<00:07,  3.36it/s] 94%|█████████▍| 387/410 [04:17<00:06,  3.42it/s] 95%|█████████▍| 388/410 [04:17<00:06,  3.47it/s] 95%|█████████▍| 389/410 [04:17<00:05,  3.50it/s] 95%|█████████▌| 390/410 [04:17<00:05,  3.53it/s] 95%|█████████▌| 391/410 [04:18<00:05,  3.24it/s] 96%|█████████▌| 392/410 [04:18<00:05,  3.33it/s] 96%|█████████▌| 393/410 [04:18<00:04,  3.41it/s] 96%|█████████▌| 394/410 [04:19<00:04,  3.46it/s] 96%|█████████▋| 395/410 [04:19<00:04,  3.49it/s] 97%|█████████▋| 396/410 [04:19<00:03,  3.52it/s] 97%|█████████▋| 397/410 [04:19<00:03,  3.54it/s] 97%|█████████▋| 398/410 [04:20<00:03,  3.55it/s] 97%|█████████▋| 399/410 [04:20<00:03,  3.56it/s] 98%|█████████▊| 400/410 [04:20<00:02,  3.57it/s] 98%|█████████▊| 401/410 [04:21<00:02,  3.57it/s] 98%|█████████▊| 402/410 [04:21<00:02,  3.11it/s] 98%|█████████▊| 403/410 [04:21<00:02,  3.24it/s] 99%|█████████▊| 404/410 [04:21<00:01,  3.34it/s] 99%|█████████▉| 405/410 [04:22<00:01,  3.41it/s] 99%|█████████▉| 406/410 [04:22<00:01,  3.47it/s] 99%|█████████▉| 407/410 [04:22<00:00,  3.51it/s]100%|█████████▉| 408/410 [04:23<00:00,  3.53it/s]100%|█████████▉| 409/410 [04:23<00:00,  3.55it/s]100%|██████████| 410/410 [04:23<00:00,  3.69it/s][INFO|trainer.py:2140] 2023-08-28 02:44:17,346 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:44:17,346 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 02:44:17,346 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.3104, 'eval_samples_per_second': 339.174, 'eval_steps_per_second': 42.482, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.23it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.14it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.63it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.65it/s][A
  6%|▌         | 27/438 [00:00<00:10, 37.44it/s][A
  7%|▋         | 32/438 [00:00<00:10, 39.81it/s][A
  8%|▊         | 37/438 [00:00<00:09, 41.17it/s][A
 10%|▉         | 42/438 [00:00<00:09, 42.42it/s][A
 11%|█         | 47/438 [00:01<00:09, 43.38it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 43.95it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.32it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.51it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.36it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.47it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.50it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.66it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.81it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.95it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 45.07it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 45.11it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.88it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.80it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.72it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.71it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.92it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.78it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 45.00it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 45.06it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 45.05it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.85it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.70it/s][A
 37%|███▋      | 162/438 [00:03<00:07, 37.59it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 39.68it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 41.23it/s][A
 40%|████      | 177/438 [00:04<00:06, 42.40it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 43.32it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 43.89it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.48it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.65it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.27it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.15it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.24it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.44it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.77it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.95it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 45.05it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 45.19it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 45.01it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.74it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.35it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.34it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.64it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.75it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.93it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 45.08it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 45.31it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 45.04it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.84it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 36.26it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 38.59it/s][A
 70%|███████   | 307/438 [00:07<00:03, 40.34it/s][A
 71%|███████   | 312/438 [00:07<00:03, 41.77it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 42.82it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 43.57it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.11it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.42it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.12it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.09it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.16it/s][A
 80%|████████  | 352/438 [00:08<00:01, 44.26it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.72it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.89it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.08it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 45.22it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 45.03it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.76it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.64it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.62it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.56it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.73it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 39.34it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 41.06it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 42.28it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.18it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 36.65it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 38.96it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 40.66it/s][A                                                 
                                                 [A100%|██████████| 410/410 [04:33<00:00,  3.69it/s]
100%|██████████| 438/438 [00:10<00:00, 40.66it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 02:44:27,994 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 02:44:28,506 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:44:35,902 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:44:36,083 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:44:36,177 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 02:44:37,849 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 02:44:37,849 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82 (score: 1.0607956647872925).
                                                 100%|██████████| 410/410 [04:59<00:00,  3.69it/s]100%|██████████| 410/410 [04:59<00:00,  1.37it/s]
[INFO|trainer.py:1894] 2023-08-28 02:44:54,159 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 02:44:54,689 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 02:45:22,471 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 02:45:23,032 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 02:45:23,386 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:45:26,150 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:26,279 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:26,279 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:26,279 >>   train_runtime            = 0:04:59.63
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:26,279 >>   train_samples            =       5240
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:26,279 >>   train_samples_per_second =      87.44
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:26,279 >>   train_steps_per_second   =      1.368
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.0804, 'eval_samples_per_second': 346.912, 'eval_steps_per_second': 43.451, 'epoch': 5.0}
{'train_runtime': 299.6351, 'train_samples_per_second': 87.44, 'train_steps_per_second': 1.368, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 02:45:27 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 02:45:27,403 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 02:45:27,403 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 02:45:27,403 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 56.07it/s]  3%|▎         | 12/438 [00:00<00:08, 50.20it/s]  4%|▍         | 18/438 [00:00<00:11, 36.95it/s]  5%|▌         | 23/438 [00:00<00:10, 39.87it/s]  6%|▋         | 28/438 [00:00<00:09, 41.69it/s]  8%|▊         | 33/438 [00:00<00:09, 42.92it/s]  9%|▊         | 38/438 [00:00<00:09, 43.72it/s] 10%|▉         | 43/438 [00:00<00:08, 44.02it/s] 11%|█         | 48/438 [00:01<00:08, 44.54it/s] 12%|█▏        | 53/438 [00:01<00:08, 44.78it/s] 13%|█▎        | 58/438 [00:01<00:08, 44.85it/s] 14%|█▍        | 63/438 [00:01<00:08, 44.62it/s] 16%|█▌        | 68/438 [00:01<00:08, 44.85it/s] 17%|█▋        | 73/438 [00:01<00:08, 45.13it/s] 18%|█▊        | 78/438 [00:01<00:07, 45.44it/s] 19%|█▉        | 83/438 [00:01<00:09, 38.68it/s] 20%|██        | 88/438 [00:02<00:08, 40.60it/s] 21%|██        | 93/438 [00:02<00:08, 42.23it/s] 22%|██▏       | 98/438 [00:02<00:07, 43.27it/s] 24%|██▎       | 103/438 [00:02<00:07, 43.98it/s] 25%|██▍       | 108/438 [00:02<00:07, 44.53it/s] 26%|██▌       | 113/438 [00:02<00:07, 44.92it/s] 27%|██▋       | 118/438 [00:02<00:07, 45.09it/s] 28%|██▊       | 123/438 [00:02<00:07, 44.97it/s] 29%|██▉       | 128/438 [00:02<00:06, 44.72it/s] 30%|███       | 133/438 [00:03<00:06, 44.74it/s] 32%|███▏      | 138/438 [00:03<00:06, 45.01it/s] 33%|███▎      | 143/438 [00:03<00:06, 45.24it/s] 34%|███▍      | 148/438 [00:03<00:06, 45.36it/s] 35%|███▍      | 153/438 [00:03<00:06, 45.54it/s] 36%|███▌      | 158/438 [00:03<00:06, 45.61it/s] 37%|███▋      | 163/438 [00:03<00:06, 45.66it/s] 38%|███▊      | 168/438 [00:03<00:05, 45.49it/s] 39%|███▉      | 173/438 [00:03<00:05, 45.27it/s] 41%|████      | 178/438 [00:04<00:05, 45.16it/s] 42%|████▏     | 183/438 [00:04<00:05, 45.20it/s] 43%|████▎     | 188/438 [00:04<00:05, 45.29it/s] 44%|████▍     | 193/438 [00:04<00:05, 45.37it/s] 45%|████▌     | 198/438 [00:04<00:05, 45.54it/s] 46%|████▋     | 203/438 [00:04<00:05, 45.59it/s] 47%|████▋     | 208/438 [00:04<00:05, 45.70it/s] 49%|████▊     | 213/438 [00:04<00:04, 45.54it/s] 50%|████▉     | 218/438 [00:04<00:05, 38.19it/s] 51%|█████     | 223/438 [00:05<00:05, 40.12it/s] 52%|█████▏    | 228/438 [00:05<00:05, 41.73it/s] 53%|█████▎    | 233/438 [00:05<00:04, 42.88it/s] 54%|█████▍    | 238/438 [00:05<00:04, 43.66it/s] 55%|█████▌    | 243/438 [00:05<00:04, 44.24it/s] 57%|█████▋    | 248/438 [00:05<00:04, 44.78it/s] 58%|█████▊    | 253/438 [00:05<00:04, 45.02it/s] 59%|█████▉    | 258/438 [00:05<00:04, 44.81it/s] 60%|██████    | 263/438 [00:05<00:03, 44.82it/s] 61%|██████    | 268/438 [00:06<00:03, 44.87it/s] 62%|██████▏   | 273/438 [00:06<00:03, 45.17it/s] 63%|██████▎   | 278/438 [00:06<00:03, 45.29it/s] 65%|██████▍   | 283/438 [00:06<00:03, 45.35it/s] 66%|██████▌   | 288/438 [00:06<00:03, 45.44it/s] 67%|██████▋   | 293/438 [00:06<00:03, 45.42it/s] 68%|██████▊   | 298/438 [00:06<00:03, 45.38it/s] 69%|██████▉   | 303/438 [00:06<00:02, 45.12it/s] 70%|███████   | 308/438 [00:06<00:02, 45.03it/s] 71%|███████▏  | 313/438 [00:07<00:02, 45.03it/s] 73%|███████▎  | 318/438 [00:07<00:02, 45.24it/s] 74%|███████▎  | 323/438 [00:07<00:02, 45.33it/s] 75%|███████▍  | 328/438 [00:07<00:02, 45.56it/s] 76%|███████▌  | 333/438 [00:07<00:02, 45.59it/s] 77%|███████▋  | 338/438 [00:07<00:02, 45.58it/s] 78%|███████▊  | 343/438 [00:07<00:02, 45.58it/s] 79%|███████▉  | 348/438 [00:07<00:01, 45.30it/s] 81%|████████  | 353/438 [00:08<00:02, 39.59it/s] 82%|████████▏ | 358/438 [00:08<00:01, 41.34it/s] 83%|████████▎ | 363/438 [00:08<00:01, 42.61it/s] 84%|████████▍ | 368/438 [00:08<00:01, 43.43it/s] 85%|████████▌ | 373/438 [00:08<00:01, 44.15it/s] 86%|████████▋ | 378/438 [00:08<00:01, 44.58it/s] 87%|████████▋ | 383/438 [00:08<00:01, 44.99it/s] 89%|████████▊ | 388/438 [00:08<00:01, 45.10it/s] 90%|████████▉ | 393/438 [00:08<00:01, 44.84it/s] 91%|█████████ | 398/438 [00:08<00:00, 44.66it/s] 92%|█████████▏| 403/438 [00:09<00:00, 44.87it/s] 93%|█████████▎| 408/438 [00:09<00:00, 45.01it/s] 94%|█████████▍| 413/438 [00:09<00:00, 45.12it/s] 95%|█████████▌| 418/438 [00:09<00:00, 45.41it/s] 97%|█████████▋| 423/438 [00:09<00:00, 45.52it/s] 98%|█████████▊| 428/438 [00:09<00:00, 45.53it/s] 99%|█████████▉| 433/438 [00:09<00:00, 45.59it/s]100%|██████████| 438/438 [00:09<00:00, 45.35it/s]100%|██████████| 438/438 [00:09<00:00, 44.36it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 02:45:37,313 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:37,313 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:37,313 >>   eval_loss               =     1.0608
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:37,313 >>   eval_runtime            = 0:00:09.91
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:37,313 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:37,313 >>   eval_samples_per_second =    352.863
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:37,313 >>   eval_steps_per_second   =     44.196
[INFO|trainer_pt_utils.py:913] 2023-08-28 02:45:37,313 >>   perplexity              =     2.8887
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:56,728 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:56,783 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:56,783 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:56,783 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:45:56,783 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:45:57,577 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:45:57,578 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:45:58,401 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:45:59,457 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:45:59,457 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:46:03,298 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:46:03,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:46:03,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:46:03,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:46:03,349 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:46:04,330 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:46:04,332 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:46:05,087 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:46:05,263 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:46:05,263 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-410
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-82
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/checkpoint-328
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.55it/s]Extractor Predicting: 3it [00:01,  1.63it/s]Extractor Predicting: 4it [00:02,  1.67it/s]Extractor Predicting: 5it [00:03,  1.70it/s]Extractor Predicting: 6it [00:03,  1.66it/s]Extractor Predicting: 7it [00:04,  1.69it/s]Extractor Predicting: 8it [00:04,  1.76it/s]Extractor Predicting: 9it [00:05,  1.74it/s]Extractor Predicting: 10it [00:05,  1.71it/s]Extractor Predicting: 11it [00:06,  1.72it/s]Extractor Predicting: 12it [00:07,  1.69it/s]Extractor Predicting: 13it [00:07,  1.67it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:08,  1.66it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.69it/s]Extractor Predicting: 18it [00:10,  1.71it/s]Extractor Predicting: 19it [00:11,  1.72it/s]Extractor Predicting: 20it [00:11,  1.65it/s]Extractor Predicting: 21it [00:12,  1.69it/s]Extractor Predicting: 22it [00:13,  1.72it/s]Extractor Predicting: 23it [00:13,  1.72it/s]Extractor Predicting: 24it [00:14,  1.79it/s]Extractor Predicting: 25it [00:14,  1.82it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:15,  1.69it/s]Extractor Predicting: 28it [00:16,  1.71it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:17,  1.68it/s]Extractor Predicting: 31it [00:18,  1.57it/s]Extractor Predicting: 32it [00:19,  1.56it/s]Extractor Predicting: 33it [00:19,  1.60it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:20,  1.67it/s]Extractor Predicting: 36it [00:21,  1.55it/s]Extractor Predicting: 37it [00:22,  1.61it/s]Extractor Predicting: 38it [00:22,  1.64it/s]Extractor Predicting: 39it [00:23,  1.60it/s]Extractor Predicting: 40it [00:23,  1.64it/s]Extractor Predicting: 41it [00:24,  1.58it/s]Extractor Predicting: 42it [00:25,  1.61it/s]Extractor Predicting: 43it [00:25,  1.65it/s]Extractor Predicting: 44it [00:26,  1.67it/s]Extractor Predicting: 45it [00:26,  1.67it/s]Extractor Predicting: 46it [00:27,  1.43it/s]Extractor Predicting: 47it [00:28,  1.46it/s]Extractor Predicting: 48it [00:29,  1.56it/s]Extractor Predicting: 49it [00:29,  1.56it/s]Extractor Predicting: 50it [00:30,  1.61it/s]Extractor Predicting: 51it [00:30,  1.58it/s]Extractor Predicting: 52it [00:31,  1.59it/s]Extractor Predicting: 53it [00:32,  1.60it/s]Extractor Predicting: 54it [00:32,  1.65it/s]Extractor Predicting: 55it [00:33,  1.63it/s]Extractor Predicting: 56it [00:34,  1.59it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:35,  1.63it/s]Extractor Predicting: 59it [00:35,  1.60it/s]Extractor Predicting: 60it [00:36,  1.60it/s]Extractor Predicting: 61it [00:37,  1.58it/s]Extractor Predicting: 62it [00:37,  1.63it/s]Extractor Predicting: 63it [00:38,  1.66it/s]Extractor Predicting: 64it [00:39,  1.57it/s]Extractor Predicting: 65it [00:39,  1.63it/s]Extractor Predicting: 66it [00:40,  1.68it/s]Extractor Predicting: 67it [00:40,  1.69it/s]Extractor Predicting: 68it [00:41,  1.72it/s]Extractor Predicting: 69it [00:41,  1.70it/s]Extractor Predicting: 70it [00:42,  1.67it/s]Extractor Predicting: 71it [00:43,  1.69it/s]Extractor Predicting: 72it [00:43,  1.74it/s]Extractor Predicting: 73it [00:44,  1.71it/s]Extractor Predicting: 74it [00:44,  1.68it/s]Extractor Predicting: 75it [00:45,  1.71it/s]Extractor Predicting: 76it [00:46,  1.63it/s]Extractor Predicting: 77it [00:46,  1.70it/s]Extractor Predicting: 78it [00:47,  1.69it/s]Extractor Predicting: 79it [00:47,  1.74it/s]Extractor Predicting: 80it [00:48,  1.76it/s]Extractor Predicting: 81it [00:48,  1.76it/s]Extractor Predicting: 82it [00:49,  1.66it/s]Extractor Predicting: 83it [00:50,  1.67it/s]Extractor Predicting: 84it [00:50,  1.67it/s]Extractor Predicting: 85it [00:51,  1.53it/s]Extractor Predicting: 86it [00:52,  1.58it/s]Extractor Predicting: 87it [00:52,  1.54it/s]Extractor Predicting: 88it [00:53,  1.59it/s]Extractor Predicting: 89it [00:53,  1.62it/s]Extractor Predicting: 90it [00:54,  1.67it/s]Extractor Predicting: 91it [00:55,  1.67it/s]Extractor Predicting: 92it [00:55,  1.61it/s]Extractor Predicting: 93it [00:56,  1.63it/s]Extractor Predicting: 94it [00:56,  1.67it/s]Extractor Predicting: 95it [00:57,  1.70it/s]Extractor Predicting: 96it [00:58,  1.69it/s]Extractor Predicting: 97it [00:58,  1.66it/s]Extractor Predicting: 98it [00:59,  1.57it/s]Extractor Predicting: 99it [01:00,  1.59it/s]Extractor Predicting: 100it [01:00,  1.61it/s]Extractor Predicting: 101it [01:01,  1.68it/s]Extractor Predicting: 102it [01:01,  1.68it/s]Extractor Predicting: 103it [01:02,  1.58it/s]Extractor Predicting: 104it [01:03,  1.59it/s]Extractor Predicting: 105it [01:03,  1.62it/s]Extractor Predicting: 106it [01:04,  1.64it/s]Extractor Predicting: 107it [01:04,  1.61it/s]Extractor Predicting: 108it [01:05,  1.57it/s]Extractor Predicting: 109it [01:06,  1.45it/s]Extractor Predicting: 110it [01:07,  1.51it/s]Extractor Predicting: 111it [01:07,  1.56it/s]Extractor Predicting: 112it [01:08,  1.62it/s]Extractor Predicting: 113it [01:08,  1.61it/s]Extractor Predicting: 114it [01:09,  1.56it/s]Extractor Predicting: 115it [01:10,  1.59it/s]Extractor Predicting: 116it [01:10,  1.58it/s]Extractor Predicting: 117it [01:11,  1.60it/s]Extractor Predicting: 118it [01:11,  1.63it/s]Extractor Predicting: 119it [01:12,  1.53it/s]Extractor Predicting: 120it [01:13,  1.56it/s]Extractor Predicting: 121it [01:13,  1.60it/s]Extractor Predicting: 122it [01:14,  1.63it/s]Extractor Predicting: 123it [01:15,  1.64it/s]Extractor Predicting: 124it [01:15,  1.56it/s]Extractor Predicting: 125it [01:16,  1.57it/s]Extractor Predicting: 126it [01:17,  1.59it/s]Extractor Predicting: 127it [01:17,  1.64it/s]Extractor Predicting: 128it [01:18,  1.63it/s]Extractor Predicting: 129it [01:18,  1.56it/s]Extractor Predicting: 130it [01:19,  1.59it/s]Extractor Predicting: 131it [01:20,  1.58it/s]Extractor Predicting: 132it [01:20,  1.61it/s]Extractor Predicting: 133it [01:21,  1.64it/s]Extractor Predicting: 134it [01:22,  1.57it/s]Extractor Predicting: 135it [01:22,  1.55it/s]Extractor Predicting: 136it [01:23,  1.53it/s]Extractor Predicting: 137it [01:24,  1.59it/s]Extractor Predicting: 138it [01:24,  1.60it/s]Extractor Predicting: 139it [01:25,  1.53it/s]Extractor Predicting: 140it [01:25,  1.58it/s]Extractor Predicting: 141it [01:26,  1.59it/s]Extractor Predicting: 142it [01:27,  1.56it/s]Extractor Predicting: 143it [01:27,  1.57it/s]Extractor Predicting: 144it [01:28,  1.54it/s]Extractor Predicting: 145it [01:28,  1.77it/s]Extractor Predicting: 145it [01:28,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:52,982 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:53,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:53,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:53,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:53,035 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:50:54,195 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:50:54,196 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:50:54,860 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:50:56,076 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:50:56,155 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:59,735 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:59,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:59,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:59,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:50:59,821 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:51:00,953 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:51:00,954 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:51:01,717 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:51:02,084 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:51:02,084 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.60it/s]Extractor Predicting: 3it [00:01,  1.69it/s]Extractor Predicting: 4it [00:02,  1.72it/s]Extractor Predicting: 5it [00:02,  1.74it/s]Extractor Predicting: 6it [00:03,  1.56it/s]Extractor Predicting: 7it [00:04,  1.62it/s]Extractor Predicting: 8it [00:04,  1.62it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.76it/s]Extractor Predicting: 11it [00:06,  1.63it/s]Extractor Predicting: 12it [00:07,  1.68it/s]Extractor Predicting: 13it [00:07,  1.69it/s]Extractor Predicting: 14it [00:08,  1.73it/s]Extractor Predicting: 15it [00:08,  1.68it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.67it/s]Extractor Predicting: 18it [00:10,  1.70it/s]Extractor Predicting: 19it [00:11,  1.69it/s]Extractor Predicting: 20it [00:11,  1.69it/s]Extractor Predicting: 21it [00:12,  1.70it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:13,  1.67it/s]Extractor Predicting: 24it [00:14,  1.65it/s]Extractor Predicting: 25it [00:14,  1.68it/s]Extractor Predicting: 26it [00:15,  1.69it/s]Extractor Predicting: 27it [00:16,  1.73it/s]Extractor Predicting: 28it [00:16,  1.71it/s]Extractor Predicting: 29it [00:17,  1.70it/s]Extractor Predicting: 30it [00:17,  1.65it/s]Extractor Predicting: 31it [00:18,  1.68it/s]Extractor Predicting: 32it [00:19,  1.69it/s]Extractor Predicting: 33it [00:19,  1.67it/s]Extractor Predicting: 34it [00:20,  1.68it/s]Extractor Predicting: 35it [00:20,  1.64it/s]Extractor Predicting: 36it [00:21,  1.68it/s]Extractor Predicting: 37it [00:22,  1.73it/s]Extractor Predicting: 38it [00:22,  1.74it/s]Extractor Predicting: 39it [00:23,  1.75it/s]Extractor Predicting: 40it [00:23,  1.72it/s]Extractor Predicting: 41it [00:24,  1.60it/s]Extractor Predicting: 42it [00:25,  1.64it/s]Extractor Predicting: 43it [00:25,  1.63it/s]Extractor Predicting: 44it [00:26,  1.66it/s]Extractor Predicting: 45it [00:26,  1.65it/s]Extractor Predicting: 46it [00:27,  1.64it/s]Extractor Predicting: 47it [00:28,  1.67it/s]Extractor Predicting: 48it [00:28,  1.69it/s]Extractor Predicting: 49it [00:29,  1.72it/s]Extractor Predicting: 50it [00:29,  1.71it/s]Extractor Predicting: 51it [00:30,  1.71it/s]Extractor Predicting: 52it [00:31,  1.68it/s]Extractor Predicting: 53it [00:31,  1.68it/s]Extractor Predicting: 54it [00:32,  1.69it/s]Extractor Predicting: 55it [00:32,  1.69it/s]Extractor Predicting: 56it [00:33,  1.67it/s]Extractor Predicting: 57it [00:34,  1.60it/s]Extractor Predicting: 58it [00:34,  1.64it/s]Extractor Predicting: 59it [00:35,  1.62it/s]Extractor Predicting: 60it [00:35,  1.65it/s]Extractor Predicting: 61it [00:36,  1.66it/s]Extractor Predicting: 62it [00:37,  1.66it/s]Extractor Predicting: 63it [00:37,  1.66it/s]Extractor Predicting: 64it [00:38,  1.70it/s]Extractor Predicting: 65it [00:38,  1.70it/s]Extractor Predicting: 66it [00:39,  1.57it/s]Extractor Predicting: 67it [00:40,  1.56it/s]Extractor Predicting: 68it [00:40,  1.63it/s]Extractor Predicting: 69it [00:41,  1.69it/s]Extractor Predicting: 70it [00:41,  1.71it/s]Extractor Predicting: 71it [00:42,  1.74it/s]Extractor Predicting: 72it [00:42,  1.75it/s]Extractor Predicting: 73it [00:43,  1.69it/s]Extractor Predicting: 74it [00:44,  1.67it/s]Extractor Predicting: 75it [00:44,  1.65it/s]Extractor Predicting: 76it [00:45,  1.68it/s]Extractor Predicting: 77it [00:45,  1.74it/s]Extractor Predicting: 78it [00:46,  1.74it/s]Extractor Predicting: 79it [00:47,  1.75it/s]Extractor Predicting: 80it [00:47,  1.78it/s]Extractor Predicting: 81it [00:48,  1.68it/s]Extractor Predicting: 82it [00:48,  1.70it/s]Extractor Predicting: 83it [00:49,  1.69it/s]Extractor Predicting: 84it [00:50,  1.72it/s]Extractor Predicting: 85it [00:50,  1.74it/s]Extractor Predicting: 86it [00:51,  1.78it/s]Extractor Predicting: 87it [00:51,  1.71it/s]Extractor Predicting: 88it [00:52,  1.71it/s]Extractor Predicting: 89it [00:53,  1.67it/s]Extractor Predicting: 90it [00:53,  1.70it/s]Extractor Predicting: 91it [00:54,  1.75it/s]Extractor Predicting: 92it [00:54,  1.72it/s]Extractor Predicting: 93it [00:55,  1.62it/s]Extractor Predicting: 94it [00:56,  1.63it/s]Extractor Predicting: 95it [00:56,  1.66it/s]Extractor Predicting: 96it [00:57,  1.70it/s]Extractor Predicting: 97it [00:57,  1.70it/s]Extractor Predicting: 98it [00:58,  1.59it/s]Extractor Predicting: 99it [00:59,  1.64it/s]Extractor Predicting: 100it [00:59,  1.67it/s]Extractor Predicting: 101it [01:00,  1.71it/s]Extractor Predicting: 102it [01:00,  1.65it/s]Extractor Predicting: 103it [01:01,  1.59it/s]Extractor Predicting: 104it [01:02,  1.62it/s]Extractor Predicting: 105it [01:02,  1.61it/s]Extractor Predicting: 106it [01:03,  1.63it/s]Extractor Predicting: 107it [01:03,  1.64it/s]Extractor Predicting: 108it [01:04,  1.60it/s]Extractor Predicting: 109it [01:05,  1.64it/s]Extractor Predicting: 110it [01:05,  1.70it/s]Extractor Predicting: 111it [01:06,  1.75it/s]Extractor Predicting: 112it [01:06,  1.73it/s]Extractor Predicting: 113it [01:07,  1.72it/s]Extractor Predicting: 114it [01:08,  1.58it/s]Extractor Predicting: 115it [01:08,  1.61it/s]Extractor Predicting: 116it [01:09,  1.66it/s]Extractor Predicting: 117it [01:09,  1.70it/s]Extractor Predicting: 118it [01:10,  1.70it/s]Extractor Predicting: 119it [01:11,  1.63it/s]Extractor Predicting: 120it [01:11,  1.65it/s]Extractor Predicting: 121it [01:12,  1.67it/s]Extractor Predicting: 122it [01:12,  1.72it/s]Extractor Predicting: 123it [01:13,  1.74it/s]Extractor Predicting: 124it [01:13,  1.73it/s]Extractor Predicting: 125it [01:14,  1.64it/s]Extractor Predicting: 126it [01:15,  1.63it/s]Extractor Predicting: 127it [01:15,  1.65it/s]Extractor Predicting: 128it [01:16,  1.68it/s]Extractor Predicting: 129it [01:16,  1.73it/s]Extractor Predicting: 130it [01:17,  1.71it/s]Extractor Predicting: 131it [01:18,  1.75it/s]Extractor Predicting: 132it [01:18,  1.76it/s]Extractor Predicting: 133it [01:19,  1.61it/s]Extractor Predicting: 134it [01:20,  1.61it/s]Extractor Predicting: 135it [01:20,  1.63it/s]Extractor Predicting: 136it [01:21,  1.65it/s]Extractor Predicting: 137it [01:21,  1.69it/s]Extractor Predicting: 138it [01:22,  1.62it/s]Extractor Predicting: 139it [01:23,  1.64it/s]Extractor Predicting: 140it [01:23,  1.65it/s]Extractor Predicting: 141it [01:24,  1.69it/s]Extractor Predicting: 142it [01:24,  1.67it/s]Extractor Predicting: 143it [01:25,  1.47it/s]Extractor Predicting: 144it [01:26,  1.54it/s]Extractor Predicting: 145it [01:26,  1.98it/s]Extractor Predicting: 145it [01:26,  1.68it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:46,520 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:46,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:46,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:46,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:46,584 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:53:47,527 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:53:47,529 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:53:48,021 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:53:49,196 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:53:49,197 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:51,736 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:51,854 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:51,854 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:51,854 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:53:51,854 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:53:52,787 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:53:52,788 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:53:53,350 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:53:53,709 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:53:53,709 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.54it/s]Extractor Predicting: 1it [00:00,  1.54it/s]
[INFO|configuration_utils.py:515] 2023-08-28 02:53:58,592 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:53:58,593 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 02:53:58,710 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:53:58,711 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 02:53:58,805 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 02:54:33,787 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 02:54:33,861 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 02:54:34,241 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 02:54:34,242 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 02:54:34,491 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:54:34,696 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:54:34,696 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:54:34,696 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:54:34,696 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:54:34,696 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 02:54:34,696 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 02:54:35,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:36,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:37,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:37,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:38,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:39,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:39,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:40,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:41,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:41,877 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:42,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:43,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:43,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:44,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:45,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:45,987 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:46,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:47,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:48,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:49,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:50,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:50,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:51,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:52,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:17<02:34, 17.20s/it][WARNING|generation_utils.py:914] 2023-08-28 02:54:52,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:53,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:54,175 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:54,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:55,571 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:56,244 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:56,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:57,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:58,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:58,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:54:59,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:00,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:00,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:01,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:02,519 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:03,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:03,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:04,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:05,141 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:05,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:06,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:07,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:07,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:32<02:09, 16.21s/it][WARNING|generation_utils.py:914] 2023-08-28 02:55:08,330 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:09,082 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:09,871 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:10,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:11,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:11,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:12,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:13,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:14,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:14,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:15,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:16,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:16,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:17,524 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:18,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:19,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:19,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:20,639 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:21,255 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:21,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:22,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:23,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:24,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:25,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:25,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:26,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:51<02:02, 17.53s/it][WARNING|generation_utils.py:914] 2023-08-28 02:55:27,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:28,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:29,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:29,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:30,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:31,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:31,711 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:32,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:33,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:34,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:34,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:35,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:36,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:37,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:37,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:38,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:39,028 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:39,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:40,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:41,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:41,712 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:42,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:43,174 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:43,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:08<01:43, 17.30s/it][WARNING|generation_utils.py:914] 2023-08-28 02:55:44,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:44,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:45,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:46,627 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:47,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:47,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:48,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:49,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:50,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:50,709 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:51,410 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:52,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:52,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:53,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:54,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:54,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:55,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:56,424 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:57,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:57,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:58,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:59,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:55:59,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:00,587 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:25<01:25, 17.14s/it][WARNING|generation_utils.py:914] 2023-08-28 02:56:01,239 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:01,884 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:02,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:03,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:03,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:04,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:05,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:05,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:06,392 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:06,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:07,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:08,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:08,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:09,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:10,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:10,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:11,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:11,909 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:12,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:13,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:13,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:14,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:14,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:15,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:40<01:05, 16.45s/it][WARNING|generation_utils.py:914] 2023-08-28 02:56:16,354 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:16,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:17,489 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:18,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:18,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:19,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:20,230 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:20,896 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:21,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:22,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:22,794 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:23,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:24,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:24,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:25,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:25,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:26,714 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:27,334 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:27,953 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:28,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:29,177 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:29,825 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:54<00:46, 15.66s/it][WARNING|generation_utils.py:914] 2023-08-28 02:56:30,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:31,090 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:31,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:32,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:33,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:33,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:34,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:35,302 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:36,003 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:36,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:37,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:38,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:38,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:39,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:40,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:41,345 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:42,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:42,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:43,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:44,199 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:44,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:45,478 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:46,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:46,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:12<00:32, 16.16s/it][WARNING|generation_utils.py:914] 2023-08-28 02:56:47,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:48,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:48,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:49,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:50,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:50,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:51,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:52,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:53,062 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:53,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:54,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:55,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:55,722 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:56,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:56,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:57,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:58,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:59,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:56:59,893 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:00,537 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:01,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:01,935 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:02,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:27<00:16, 16.03s/it][WARNING|generation_utils.py:914] 2023-08-28 02:57:03,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:03,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:04,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:05,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:06,071 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:06,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:07,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:07,995 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:08,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:09,422 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:10,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:10,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:11,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:11,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:12,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:13,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:13,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:14,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:15,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:15,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:16,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:16,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:17,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:18,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:18,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:19,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:20,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:20,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:21,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:21,999 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:22,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:23,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 02:57:23,851 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:49<00:00, 17.67s/it]Generating: 100%|██████████| 10/10 [02:49<00:00, 16.92s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:37,954 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:38,048 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:38,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:38,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:38,049 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 02:57:39,452 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 02:57:39,453 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:57:40,349 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 02:57:41,674 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:57:41,773 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:45,331 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:45,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:45,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:45,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 02:57:45,455 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 02:57:46,723 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 02:57:46,724 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 02:57:47,493 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 02:57:47,904 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 02:57:47,904 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : director .', 'success_rate': 0.8138020833333334, 'errors': {'', "('\\n', 'director', 'Scott Zandt', 'It was written and directed by Scott Zandt ( a.k.a .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8505434782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 359, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 499, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7439903846153846, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : part of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'too many values to unpack (expected 2)', '(\'1958 contest\', \'part of\', \'\', \'He was succeeded as Dutch representative at its 1958 contest by Johannes Eichhorn with " Allende en seine " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a deal to turn " Ingress " into a mobile game . Head Entity : Ingress , Tail Entity : Ingress Studio .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.7981770833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8764204545454546, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n']
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n"]
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n", "Relation : member of political party . Context : This was the first coalition government which was elected in 1998 , and led by then - Prime Minister Naguib Sawiris of the People 's Alliance . Head Entity : Naguib Sawiris , Tail Entity : People ' Alliance .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Tom Blomkamp\', \'member of political party\', \'\', \'" My Life ( " ; ) is a 2015 English language English language documentary film directed by Tom Blomkamp and starring Emma Thompson and Tom Hardy .\')'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 58, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 96, 'raw': 160}
{'target': 600, 'success': 114, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 154, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 223, 'raw': 384}
{'target': 600, 'success': 245, 'raw': 416}
{'target': 600, 'success': 263, 'raw': 448}
{'target': 600, 'success': 276, 'raw': 480}
{'target': 600, 'success': 297, 'raw': 512}
{'target': 600, 'success': 318, 'raw': 544}
{'target': 600, 'success': 335, 'raw': 576}
{'target': 600, 'success': 359, 'raw': 608}
{'target': 600, 'success': 381, 'raw': 640}
{'target': 600, 'success': 398, 'raw': 672}
{'target': 600, 'success': 410, 'raw': 704}
{'target': 600, 'success': 431, 'raw': 736}
{'target': 600, 'success': 451, 'raw': 768}
{'target': 600, 'success': 470, 'raw': 800}
{'target': 600, 'success': 494, 'raw': 832}
{'target': 600, 'success': 516, 'raw': 864}
{'target': 600, 'success': 533, 'raw': 896}
{'target': 600, 'success': 552, 'raw': 928}
{'target': 600, 'success': 566, 'raw': 960}
{'target': 600, 'success': 584, 'raw': 992}
{'target': 600, 'success': 599, 'raw': 1024}
{'target': 600, 'success': 621, 'raw': 1056}
{'prompt': 'Relation : position held .', 'success_rate': 0.5880681818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 12256
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12356, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.23it/s]Extractor Estimating: 2it [00:01,  1.23it/s]Extractor Estimating: 3it [00:02,  1.32it/s]Extractor Estimating: 4it [00:03,  1.36it/s]Extractor Estimating: 5it [00:03,  1.35it/s]Extractor Estimating: 6it [00:04,  1.45it/s]Extractor Estimating: 7it [00:05,  1.42it/s]Extractor Estimating: 8it [00:05,  1.40it/s]Extractor Estimating: 9it [00:06,  1.43it/s]Extractor Estimating: 10it [00:07,  1.45it/s]Extractor Estimating: 11it [00:07,  1.50it/s]Extractor Estimating: 12it [00:08,  1.51it/s]Extractor Estimating: 13it [00:09,  1.43it/s]Extractor Estimating: 14it [00:09,  1.40it/s]Extractor Estimating: 15it [00:10,  1.45it/s]Extractor Estimating: 16it [00:11,  1.45it/s]Extractor Estimating: 17it [00:11,  1.45it/s]Extractor Estimating: 18it [00:12,  1.44it/s]Extractor Estimating: 19it [00:13,  1.43it/s]Extractor Estimating: 20it [00:14,  1.45it/s]Extractor Estimating: 21it [00:14,  1.43it/s]Extractor Estimating: 22it [00:15,  1.48it/s]Extractor Estimating: 23it [00:16,  1.41it/s]Extractor Estimating: 24it [00:16,  1.39it/s]Extractor Estimating: 25it [00:17,  1.38it/s]Extractor Estimating: 26it [00:18,  1.44it/s]Extractor Estimating: 27it [00:18,  1.48it/s]Extractor Estimating: 28it [00:19,  1.47it/s]Extractor Estimating: 29it [00:20,  1.50it/s]Extractor Estimating: 30it [00:20,  1.55it/s]Extractor Estimating: 31it [00:21,  1.59it/s]Extractor Estimating: 32it [00:22,  1.59it/s]Extractor Estimating: 33it [00:22,  1.52it/s]Extractor Estimating: 34it [00:23,  1.60it/s]Extractor Estimating: 35it [00:23,  1.66it/s]Extractor Estimating: 36it [00:24,  1.64it/s]Extractor Estimating: 37it [00:25,  1.61it/s]Extractor Estimating: 38it [00:25,  1.54it/s]Extractor Estimating: 39it [00:26,  1.53it/s]Extractor Estimating: 40it [00:27,  1.61it/s]Extractor Estimating: 41it [00:27,  1.57it/s]Extractor Estimating: 42it [00:28,  1.59it/s]Extractor Estimating: 43it [00:29,  1.54it/s]Extractor Estimating: 44it [00:29,  1.57it/s]Extractor Estimating: 45it [00:30,  1.58it/s]Extractor Estimating: 46it [00:30,  1.64it/s]Extractor Estimating: 47it [00:31,  1.60it/s]Extractor Estimating: 48it [00:32,  1.55it/s]Extractor Estimating: 49it [00:32,  1.62it/s]Extractor Estimating: 50it [00:33,  1.57it/s]Extractor Estimating: 51it [00:34,  1.53it/s]Extractor Estimating: 52it [00:34,  1.54it/s]Extractor Estimating: 53it [00:35,  1.49it/s]Extractor Estimating: 54it [00:36,  1.55it/s]Extractor Estimating: 55it [00:36,  1.56it/s]Extractor Estimating: 56it [00:37,  1.55it/s]Extractor Estimating: 57it [00:37,  1.57it/s]Extractor Estimating: 58it [00:38,  1.63it/s]Extractor Estimating: 59it [00:39,  1.59it/s]Extractor Estimating: 60it [00:39,  1.54it/s]Extractor Estimating: 61it [00:40,  1.42it/s]Extractor Estimating: 62it [00:41,  1.46it/s]Extractor Estimating: 63it [00:41,  1.52it/s]Extractor Estimating: 64it [00:42,  1.58it/s]Extractor Estimating: 65it [00:43,  1.31it/s]Extractor Estimating: 66it [00:44,  1.36it/s]Extractor Estimating: 67it [00:44,  1.41it/s]Extractor Estimating: 68it [00:45,  1.44it/s]Extractor Estimating: 69it [00:46,  1.40it/s]Extractor Estimating: 70it [00:46,  1.47it/s]Extractor Estimating: 71it [00:47,  1.51it/s]Extractor Estimating: 72it [00:48,  1.49it/s]Extractor Estimating: 73it [00:48,  1.49it/s]Extractor Estimating: 74it [00:49,  1.44it/s]Extractor Estimating: 75it [00:50,  1.38it/s]Extractor Estimating: 76it [00:51,  1.40it/s]Extractor Estimating: 77it [00:51,  1.43it/s]Extractor Estimating: 78it [00:52,  1.44it/s]Extractor Estimating: 79it [00:53,  1.41it/s]Extractor Estimating: 80it [00:53,  1.48it/s]Extractor Estimating: 81it [00:54,  1.46it/s]Extractor Estimating: 82it [00:55,  1.50it/s]Extractor Estimating: 83it [00:55,  1.48it/s]Extractor Estimating: 84it [00:56,  1.41it/s]Extractor Estimating: 85it [00:57,  1.41it/s]Extractor Estimating: 86it [00:58,  1.46it/s]Extractor Estimating: 87it [00:58,  1.51it/s]Extractor Estimating: 88it [00:59,  1.44it/s]Extractor Estimating: 89it [01:00,  1.40it/s]Extractor Estimating: 90it [01:00,  1.43it/s]Extractor Estimating: 91it [01:01,  1.44it/s]Extractor Estimating: 92it [01:02,  1.48it/s]Extractor Estimating: 93it [01:02,  1.50it/s]Extractor Estimating: 94it [01:03,  1.45it/s]Extractor Estimating: 95it [01:04,  1.53it/s]Extractor Estimating: 96it [01:04,  1.50it/s]Extractor Estimating: 97it [01:05,  1.49it/s]Extractor Estimating: 98it [01:06,  1.50it/s]Extractor Estimating: 99it [01:06,  1.46it/s]Extractor Estimating: 100it [01:07,  1.39it/s]Extractor Estimating: 101it [01:08,  1.45it/s]Extractor Estimating: 102it [01:08,  1.48it/s]Extractor Estimating: 103it [01:09,  1.49it/s]Extractor Estimating: 104it [01:10,  1.50it/s]Extractor Estimating: 105it [01:10,  1.47it/s]Extractor Estimating: 106it [01:11,  1.50it/s]Extractor Estimating: 107it [01:12,  1.51it/s]Extractor Estimating: 108it [01:12,  1.49it/s]Extractor Estimating: 109it [01:13,  1.51it/s]Extractor Estimating: 110it [01:14,  1.49it/s]Extractor Estimating: 111it [01:14,  1.51it/s]Extractor Estimating: 112it [01:15,  1.50it/s]Extractor Estimating: 113it [01:16,  1.53it/s]Extractor Estimating: 114it [01:16,  1.52it/s]Extractor Estimating: 115it [01:17,  1.48it/s]Extractor Estimating: 116it [01:18,  1.49it/s]Extractor Estimating: 117it [01:18,  1.52it/s]Extractor Estimating: 118it [01:19,  1.47it/s]Extractor Estimating: 119it [01:20,  1.53it/s]Extractor Estimating: 120it [01:20,  1.48it/s]Extractor Estimating: 121it [01:21,  1.49it/s]Extractor Estimating: 122it [01:22,  1.54it/s]Extractor Estimating: 123it [01:22,  1.44it/s]Extractor Estimating: 124it [01:23,  1.48it/s]Extractor Estimating: 125it [01:24,  1.47it/s]Extractor Estimating: 126it [01:24,  1.47it/s]Extractor Estimating: 127it [01:25,  1.47it/s]Extractor Estimating: 128it [01:26,  1.49it/s]Extractor Estimating: 129it [01:26,  1.55it/s]Extractor Estimating: 130it [01:27,  1.54it/s]Extractor Estimating: 131it [01:28,  1.59it/s]Extractor Estimating: 132it [01:28,  1.55it/s]Extractor Estimating: 133it [01:29,  1.56it/s]Extractor Estimating: 134it [01:30,  1.52it/s]Extractor Estimating: 135it [01:30,  1.48it/s]Extractor Estimating: 136it [01:31,  1.51it/s]Extractor Estimating: 137it [01:32,  1.56it/s]Extractor Estimating: 138it [01:32,  1.56it/s]Extractor Estimating: 139it [01:33,  1.58it/s]Extractor Estimating: 140it [01:34,  1.47it/s]Extractor Estimating: 141it [01:34,  1.54it/s]Extractor Estimating: 142it [01:35,  1.53it/s]Extractor Estimating: 143it [01:36,  1.53it/s]Extractor Estimating: 144it [01:36,  1.56it/s]Extractor Estimating: 145it [01:37,  1.53it/s]Extractor Estimating: 146it [01:37,  1.52it/s]Extractor Estimating: 147it [01:38,  1.56it/s]Extractor Estimating: 148it [01:39,  1.53it/s]Extractor Estimating: 149it [01:39,  1.56it/s]Extractor Estimating: 150it [01:40,  1.58it/s]Extractor Estimating: 151it [01:41,  1.63it/s]Extractor Estimating: 152it [01:41,  1.56it/s]Extractor Estimating: 153it [01:42,  1.63it/s]Extractor Estimating: 154it [01:42,  1.70it/s]Extractor Estimating: 155it [01:43,  1.74it/s]Extractor Estimating: 156it [01:43,  1.78it/s]Extractor Estimating: 157it [01:44,  1.78it/s]Extractor Estimating: 158it [01:45,  1.66it/s]Extractor Estimating: 159it [01:45,  1.70it/s]Extractor Estimating: 160it [01:46,  1.73it/s]Extractor Estimating: 161it [01:46,  1.81it/s]Extractor Estimating: 162it [01:47,  1.88it/s]Extractor Estimating: 163it [01:47,  1.83it/s]Extractor Estimating: 164it [01:48,  1.72it/s]Extractor Estimating: 165it [01:49,  1.80it/s]Extractor Estimating: 166it [01:49,  1.79it/s]Extractor Estimating: 167it [01:50,  1.87it/s]Extractor Estimating: 168it [01:50,  1.84it/s]Extractor Estimating: 169it [01:51,  1.85it/s]Extractor Estimating: 170it [01:51,  1.64it/s]Extractor Estimating: 171it [01:52,  1.68it/s]Extractor Estimating: 172it [01:53,  1.73it/s]Extractor Estimating: 173it [01:53,  1.79it/s]Extractor Estimating: 174it [01:54,  1.81it/s]Extractor Estimating: 175it [01:54,  1.84it/s]Extractor Estimating: 176it [01:55,  1.60it/s]Extractor Estimating: 177it [01:56,  1.60it/s]Extractor Estimating: 178it [01:56,  1.64it/s]Extractor Estimating: 179it [01:57,  1.59it/s]Extractor Estimating: 180it [01:57,  1.63it/s]Extractor Estimating: 181it [01:58,  1.58it/s]Extractor Estimating: 182it [01:59,  1.63it/s]Extractor Estimating: 183it [01:59,  1.56it/s]Extractor Estimating: 184it [02:00,  1.60it/s]Extractor Estimating: 185it [02:01,  1.60it/s]Extractor Estimating: 186it [02:01,  1.44it/s]Extractor Estimating: 187it [02:02,  1.50it/s]Extractor Estimating: 188it [02:03,  1.54it/s]Extractor Estimating: 189it [02:03,  1.55it/s]Extractor Estimating: 190it [02:04,  1.56it/s]Extractor Estimating: 191it [02:05,  1.47it/s]Extractor Estimating: 192it [02:05,  1.50it/s]Extractor Estimating: 193it [02:06,  1.56it/s]Extractor Estimating: 194it [02:06,  1.59it/s]Extractor Estimating: 195it [02:07,  1.63it/s]Extractor Estimating: 196it [02:08,  1.57it/s]Extractor Estimating: 197it [02:08,  1.62it/s]Extractor Estimating: 198it [02:09,  1.44it/s]Extractor Estimating: 199it [02:10,  1.51it/s]Extractor Estimating: 200it [02:10,  1.55it/s]Extractor Estimating: 201it [02:11,  1.57it/s]Extractor Estimating: 202it [02:12,  1.48it/s]Extractor Estimating: 203it [02:12,  1.48it/s]Extractor Estimating: 204it [02:13,  1.50it/s]Extractor Estimating: 205it [02:14,  1.51it/s]Extractor Estimating: 206it [02:14,  1.52it/s]Extractor Estimating: 207it [02:15,  1.47it/s]Extractor Estimating: 208it [02:16,  1.47it/s]Extractor Estimating: 209it [02:16,  1.48it/s]Extractor Estimating: 210it [02:17,  1.58it/s]Extractor Estimating: 211it [02:18,  1.54it/s]Extractor Estimating: 212it [02:18,  1.49it/s]Extractor Estimating: 213it [02:19,  1.52it/s]Extractor Estimating: 214it [02:20,  1.49it/s]Extractor Estimating: 215it [02:20,  1.55it/s]Extractor Estimating: 216it [02:21,  1.55it/s]Extractor Estimating: 217it [02:22,  1.49it/s]Extractor Estimating: 218it [02:22,  1.50it/s]Extractor Estimating: 219it [02:23,  1.51it/s]Extractor Estimating: 220it [02:24,  1.51it/s]Extractor Estimating: 221it [02:24,  1.53it/s]Extractor Estimating: 222it [02:25,  1.47it/s]Extractor Estimating: 223it [02:26,  1.53it/s]Extractor Estimating: 224it [02:26,  1.60it/s]Extractor Estimating: 225it [02:27,  1.53it/s]Extractor Estimating: 226it [02:27,  1.57it/s]Extractor Estimating: 227it [02:28,  1.40it/s]Extractor Estimating: 228it [02:29,  1.41it/s]Extractor Estimating: 229it [02:30,  1.47it/s]Extractor Estimating: 230it [02:30,  1.55it/s]Extractor Estimating: 231it [02:31,  1.54it/s]Extractor Estimating: 232it [02:32,  1.48it/s]Extractor Estimating: 233it [02:32,  1.50it/s]Extractor Estimating: 234it [02:33,  1.55it/s]Extractor Estimating: 235it [02:33,  1.59it/s]Extractor Estimating: 236it [02:34,  1.63it/s]Extractor Estimating: 237it [02:35,  1.53it/s]Extractor Estimating: 238it [02:35,  1.61it/s]Extractor Estimating: 239it [02:36,  1.63it/s]Extractor Estimating: 240it [02:37,  1.65it/s]Extractor Estimating: 241it [02:37,  1.61it/s]Extractor Estimating: 242it [02:38,  1.61it/s]Extractor Estimating: 243it [02:38,  1.57it/s]Extractor Estimating: 244it [02:39,  1.61it/s]Extractor Estimating: 245it [02:40,  1.59it/s]Extractor Estimating: 246it [02:40,  1.57it/s]Extractor Estimating: 247it [02:41,  1.58it/s]Extractor Estimating: 248it [02:42,  1.63it/s]Extractor Estimating: 249it [02:42,  1.54it/s]Extractor Estimating: 250it [02:43,  1.44it/s]Extractor Estimating: 250it [02:43,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:07,655 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:07,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:07,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:07,658 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:07,658 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 03:01:09,387 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 03:01:09,487 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:01:10,431 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 03:01:11,702 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:01:11,702 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:15,927 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:16,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:16,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:16,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 03:01:16,051 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 03:01:17,409 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 03:01:17,410 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 03:01:18,253 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 03:01:18,690 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 03:01:18,690 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 04:34:25,051 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 04:34:25,725 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 5297 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 22828
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 22928, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=22928, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.086, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.083, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 79, avg_time 1.058, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 179, avg_time 1.069, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 58, avg_time 1.057, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 158, avg_time 2.174, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 37, avg_time 1.073, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 137, avg_time 1.068, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 16, avg_time 1.063, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 116, avg_time 1.060, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 216, avg_time 2.143, loss:nan
g_step 1200, step 95, avg_time 1.042, loss:nan
g_step 1300, step 195, avg_time 1.073, loss:nan
g_step 1400, step 74, avg_time 1.083, loss:nan
g_step 1500, step 174, avg_time 1.073, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 53, avg_time 2.130, loss:nan
g_step 1700, step 153, avg_time 1.064, loss:nan
g_step 1800, step 32, avg_time 1.084, loss:nan
g_step 1900, step 132, avg_time 1.052, loss:nan
g_step 2000, step 11, avg_time 1.077, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 111, avg_time 2.149, loss:nan
g_step 2200, step 211, avg_time 1.075, loss:nan
g_step 2300, step 90, avg_time 1.074, loss:nan
g_step 2400, step 190, avg_time 1.052, loss:nan
g_step 2500, step 69, avg_time 1.062, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 169, avg_time 2.160, loss:nan
g_step 2700, step 48, avg_time 1.038, loss:nan
g_step 2800, step 148, avg_time 1.043, loss:nan
g_step 2900, step 27, avg_time 1.064, loss:nan
g_step 3000, step 127, avg_time 1.059, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 6, avg_time 2.135, loss:nan
g_step 3200, step 106, avg_time 1.046, loss:nan
g_step 3300, step 206, avg_time 1.054, loss:nan
g_step 3400, step 85, avg_time 1.028, loss:nan
g_step 3500, step 185, avg_time 1.056, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 64, avg_time 2.161, loss:nan
g_step 3700, step 164, avg_time 1.062, loss:nan
g_step 3800, step 43, avg_time 1.065, loss:nan
g_step 3900, step 143, avg_time 1.066, loss:nan
g_step 4000, step 22, avg_time 1.064, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 122, avg_time 2.152, loss:nan
g_step 4200, step 1, avg_time 1.060, loss:nan
g_step 4300, step 101, avg_time 1.078, loss:nan
g_step 4400, step 201, avg_time 1.057, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 04:34:25 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 04:34:25 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_04-34-25_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 04:34:27 - WARNING - datasets.builder -   Using custom data configuration default-88949b62b12912dc
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-88949b62b12912dc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 04:34:32,699 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:34:32,700 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:34:32,700 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:34:32,701 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:34:32,956 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:34:33,094 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:34:33,165 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:34:33,166 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:34:33,166 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:34:33,166 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:34:33,166 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 04:34:34,052 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:34:37,283 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 04:34:37,357 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-88949b62b12912dc/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  1.97ba/s] 33%|███▎      | 2/6 [00:00<00:01,  2.99ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.57ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.89ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.53ba/s]100%|██████████| 6/6 [00:01<00:00,  3.84ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.16ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.72ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.30ba/s]100%|██████████| 4/4 [00:01<00:00,  4.37ba/s]100%|██████████| 4/4 [00:01<00:00,  3.62ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.81ba/s] 50%|█████     | 3/6 [00:00<00:00,  6.28ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  8.07ba/s]100%|██████████| 6/6 [00:00<00:00,  8.05ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.01ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.70ba/s]100%|██████████| 4/4 [00:00<00:00,  6.88ba/s]100%|██████████| 4/4 [00:00<00:00,  5.33ba/s]
[INFO|trainer.py:414] 2023-08-28 04:34:46,263 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 04:34:46,489 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 04:34:46,489 >>   Num examples = 5300
[INFO|trainer.py:1149] 2023-08-28 04:34:46,489 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 04:34:46,490 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 04:34:46,490 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 04:34:46,490 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 04:34:46,490 >>   Total optimization steps = 415
  0%|          | 0/415 [00:00<?, ?it/s]  0%|          | 1/415 [00:02<15:36,  2.26s/it]  0%|          | 2/415 [00:03<10:17,  1.50s/it]  1%|          | 3/415 [00:03<06:52,  1.00s/it]  1%|          | 4/415 [00:03<05:06,  1.34it/s]  1%|          | 5/415 [00:04<03:56,  1.73it/s]  1%|▏         | 6/415 [00:04<03:14,  2.10it/s]  2%|▏         | 7/415 [00:04<02:47,  2.44it/s]  2%|▏         | 8/415 [00:05<02:29,  2.73it/s]  2%|▏         | 9/415 [00:05<02:37,  2.58it/s]  2%|▏         | 10/415 [00:05<02:42,  2.50it/s]  3%|▎         | 11/415 [00:06<02:39,  2.53it/s]  3%|▎         | 12/415 [00:06<02:25,  2.78it/s]  3%|▎         | 13/415 [00:06<02:14,  3.00it/s]  3%|▎         | 14/415 [00:07<02:06,  3.17it/s]  4%|▎         | 15/415 [00:07<02:01,  3.29it/s]  4%|▍         | 16/415 [00:07<01:57,  3.39it/s]  4%|▍         | 17/415 [00:07<01:54,  3.46it/s]  4%|▍         | 18/415 [00:08<01:53,  3.51it/s]  5%|▍         | 19/415 [00:08<01:51,  3.55it/s]  5%|▍         | 20/415 [00:08<02:02,  3.21it/s]  5%|▌         | 21/415 [00:09<01:58,  3.33it/s]  5%|▌         | 22/415 [00:09<01:55,  3.41it/s]  6%|▌         | 23/415 [00:09<01:52,  3.48it/s]  6%|▌         | 24/415 [00:10<01:50,  3.52it/s]  6%|▌         | 25/415 [00:10<01:49,  3.56it/s]  6%|▋         | 26/415 [00:10<01:48,  3.58it/s]  7%|▋         | 27/415 [00:10<01:47,  3.60it/s]  7%|▋         | 28/415 [00:11<01:47,  3.61it/s]  7%|▋         | 29/415 [00:11<01:46,  3.62it/s]  7%|▋         | 30/415 [00:11<01:46,  3.62it/s]  7%|▋         | 31/415 [00:11<01:45,  3.62it/s]  8%|▊         | 32/415 [00:12<01:59,  3.21it/s]  8%|▊         | 33/415 [00:12<01:54,  3.33it/s]  8%|▊         | 34/415 [00:12<01:51,  3.41it/s]  8%|▊         | 35/415 [00:13<01:49,  3.48it/s]  9%|▊         | 36/415 [00:13<01:47,  3.52it/s]  9%|▉         | 37/415 [00:13<01:46,  3.55it/s]  9%|▉         | 38/415 [00:13<01:45,  3.58it/s]  9%|▉         | 39/415 [00:14<01:44,  3.60it/s] 10%|▉         | 40/415 [00:14<01:43,  3.61it/s] 10%|▉         | 41/415 [00:14<01:43,  3.61it/s] 10%|█         | 42/415 [00:15<01:43,  3.62it/s] 10%|█         | 43/415 [00:15<01:53,  3.29it/s] 11%|█         | 44/415 [00:15<01:49,  3.38it/s] 11%|█         | 45/415 [00:16<01:47,  3.46it/s] 11%|█         | 46/415 [00:16<01:45,  3.51it/s] 11%|█▏        | 47/415 [00:16<01:43,  3.54it/s] 12%|█▏        | 48/415 [00:16<01:42,  3.56it/s] 12%|█▏        | 49/415 [00:17<01:42,  3.58it/s] 12%|█▏        | 50/415 [00:17<01:41,  3.60it/s] 12%|█▏        | 51/415 [00:17<01:40,  3.61it/s] 13%|█▎        | 52/415 [00:17<01:40,  3.60it/s] 13%|█▎        | 53/415 [00:18<01:40,  3.59it/s] 13%|█▎        | 54/415 [00:18<01:48,  3.33it/s] 13%|█▎        | 55/415 [00:18<01:45,  3.40it/s] 13%|█▎        | 56/415 [00:19<01:43,  3.45it/s] 14%|█▎        | 57/415 [00:19<01:42,  3.49it/s] 14%|█▍        | 58/415 [00:19<01:41,  3.52it/s] 14%|█▍        | 59/415 [00:19<01:40,  3.54it/s] 14%|█▍        | 60/415 [00:20<01:40,  3.55it/s] 15%|█▍        | 61/415 [00:20<01:39,  3.56it/s] 15%|█▍        | 62/415 [00:20<01:38,  3.58it/s] 15%|█▌        | 63/415 [00:21<01:37,  3.60it/s] 15%|█▌        | 64/415 [00:21<01:37,  3.60it/s] 16%|█▌        | 65/415 [00:21<01:44,  3.35it/s] 16%|█▌        | 66/415 [00:21<01:41,  3.43it/s] 16%|█▌        | 67/415 [00:22<01:39,  3.48it/s] 16%|█▋        | 68/415 [00:22<01:38,  3.53it/s] 17%|█▋        | 69/415 [00:22<01:37,  3.56it/s] 17%|█▋        | 70/415 [00:23<01:36,  3.58it/s] 17%|█▋        | 71/415 [00:23<01:35,  3.59it/s] 17%|█▋        | 72/415 [00:23<01:35,  3.59it/s] 18%|█▊        | 73/415 [00:23<01:35,  3.60it/s] 18%|█▊        | 74/415 [00:24<01:34,  3.61it/s] 18%|█▊        | 75/415 [00:24<01:34,  3.62it/s] 18%|█▊        | 76/415 [00:24<01:38,  3.45it/s] 19%|█▊        | 77/415 [00:25<01:36,  3.50it/s] 19%|█▉        | 78/415 [00:25<01:35,  3.54it/s] 19%|█▉        | 79/415 [00:25<01:34,  3.56it/s] 19%|█▉        | 80/415 [00:25<01:33,  3.58it/s] 20%|█▉        | 81/415 [00:26<01:32,  3.59it/s] 20%|█▉        | 82/415 [00:26<01:32,  3.60it/s] 20%|██        | 83/415 [00:26<01:27,  3.80it/s][INFO|trainer.py:2140] 2023-08-28 04:35:13,174 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:35:13,174 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 04:35:13,174 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.45it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.18it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.61it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.83it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.14it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.73it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.23it/s][A
 10%|▉         | 42/438 [00:00<00:09, 41.87it/s][A
 11%|█         | 47/438 [00:01<00:09, 42.96it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 43.71it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.26it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.66it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.87it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.98it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.89it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.49it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.51it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.66it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.79it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.86it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 45.05it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.18it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.15it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 45.09it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.81it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.68it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.74it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.81it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 45.09it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.16it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.24it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.17it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.94it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.89it/s][A
 40%|████      | 177/438 [00:04<00:06, 37.46it/s][A
 42%|████▏     | 182/438 [00:04<00:06, 39.49it/s][A
 43%|████▎     | 187/438 [00:04<00:06, 41.15it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 42.26it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 43.20it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 43.87it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.34it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.57it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.28it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.25it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.54it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.69it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.86it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 45.00it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 45.17it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 45.14it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 45.08it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.77it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.58it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.63it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.76it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.92it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.95it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.04it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 45.25it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 45.15it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.83it/s][A
 71%|███████   | 312/438 [00:07<00:03, 35.70it/s][A
 72%|███████▏  | 317/438 [00:07<00:03, 38.12it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 40.02it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 41.55it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 42.69it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 43.45it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.02it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.28it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.23it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.11it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.17it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.56it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.75it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.98it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 45.07it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 45.09it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 45.14it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.87it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.63it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 38.55it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 40.58it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 41.94it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 42.98it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.62it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.17it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.56it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:10<00:00, 44.56it/s][A 20%|██        | 83/415 [00:36<01:27,  3.80it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:35:23,637 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83
[INFO|configuration_utils.py:351] 2023-08-28 04:35:24,073 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:35:57,181 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:35:58,184 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:35:58,520 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83/special_tokens_map.json
 20%|██        | 84/415 [01:18<1:26:19, 15.65s/it] 20%|██        | 85/415 [01:18<1:00:52, 11.07s/it] 21%|██        | 86/415 [01:18<42:56,  7.83s/it]   21%|██        | 87/415 [01:19<30:25,  5.57s/it] 21%|██        | 88/415 [01:19<21:41,  3.98s/it] 21%|██▏       | 89/415 [01:19<15:35,  2.87s/it] 22%|██▏       | 90/415 [01:19<11:19,  2.09s/it] 22%|██▏       | 91/415 [01:20<08:21,  1.55s/it] 22%|██▏       | 92/415 [01:20<06:16,  1.17s/it] 22%|██▏       | 93/415 [01:20<04:49,  1.11it/s] 23%|██▎       | 94/415 [01:21<03:49,  1.40it/s] 23%|██▎       | 95/415 [01:21<03:06,  1.71it/s] 23%|██▎       | 96/415 [01:21<02:45,  1.93it/s] 23%|██▎       | 97/415 [01:22<02:22,  2.24it/s] 24%|██▎       | 98/415 [01:22<02:05,  2.52it/s] 24%|██▍       | 99/415 [01:22<01:54,  2.77it/s] 24%|██▍       | 100/415 [01:22<01:45,  2.97it/s] 24%|██▍       | 101/415 [01:23<01:40,  3.13it/s] 25%|██▍       | 102/415 [01:23<01:36,  3.25it/s] 25%|██▍       | 103/415 [01:23<01:33,  3.34it/s] 25%|██▌       | 104/415 [01:23<01:31,  3.41it/s] 25%|██▌       | 105/415 [01:24<01:29,  3.46it/s] 26%|██▌       | 106/415 [01:24<01:28,  3.49it/s] 26%|██▌       | 107/415 [01:24<01:36,  3.18it/s] 26%|██▌       | 108/415 [01:25<01:33,  3.29it/s] 26%|██▋       | 109/415 [01:25<01:30,  3.37it/s] 27%|██▋       | 110/415 [01:25<01:28,  3.43it/s] 27%|██▋       | 111/415 [01:26<01:27,  3.48it/s] 27%|██▋       | 112/415 [01:26<01:25,  3.53it/s] 27%|██▋       | 113/415 [01:26<01:24,  3.56it/s] 27%|██▋       | 114/415 [01:26<01:24,  3.58it/s] 28%|██▊       | 115/415 [01:27<01:23,  3.60it/s] 28%|██▊       | 116/415 [01:27<01:22,  3.61it/s] 28%|██▊       | 117/415 [01:27<01:22,  3.62it/s] 28%|██▊       | 118/415 [01:28<01:32,  3.23it/s] 29%|██▊       | 119/415 [01:28<01:28,  3.34it/s] 29%|██▉       | 120/415 [01:28<01:26,  3.43it/s] 29%|██▉       | 121/415 [01:28<01:24,  3.49it/s] 29%|██▉       | 122/415 [01:29<01:23,  3.53it/s] 30%|██▉       | 123/415 [01:29<01:22,  3.56it/s] 30%|██▉       | 124/415 [01:29<01:21,  3.58it/s] 30%|███       | 125/415 [01:30<01:20,  3.59it/s] 30%|███       | 126/415 [01:30<01:20,  3.60it/s] 31%|███       | 127/415 [01:30<01:19,  3.61it/s] 31%|███       | 128/415 [01:30<01:19,  3.62it/s] 31%|███       | 129/415 [01:31<01:23,  3.41it/s] 31%|███▏      | 130/415 [01:31<01:22,  3.47it/s] 32%|███▏      | 131/415 [01:31<01:20,  3.52it/s] 32%|███▏      | 132/415 [01:31<01:19,  3.55it/s] 32%|███▏      | 133/415 [01:32<01:18,  3.58it/s] 32%|███▏      | 134/415 [01:32<01:18,  3.59it/s] 33%|███▎      | 135/415 [01:32<01:17,  3.60it/s] 33%|███▎      | 136/415 [01:33<01:17,  3.61it/s] 33%|███▎      | 137/415 [01:33<01:16,  3.62it/s] 33%|███▎      | 138/415 [01:33<01:16,  3.62it/s] 33%|███▎      | 139/415 [01:33<01:16,  3.62it/s] 34%|███▎      | 140/415 [01:34<01:22,  3.33it/s] 34%|███▍      | 141/415 [01:34<01:20,  3.42it/s] 34%|███▍      | 142/415 [01:34<01:18,  3.48it/s] 34%|███▍      | 143/415 [01:35<01:17,  3.52it/s] 35%|███▍      | 144/415 [01:35<01:16,  3.56it/s] 35%|███▍      | 145/415 [01:35<01:15,  3.58it/s] 35%|███▌      | 146/415 [01:35<01:18,  3.42it/s] 35%|███▌      | 147/415 [01:36<01:17,  3.47it/s] 36%|███▌      | 148/415 [01:36<01:16,  3.50it/s] 36%|███▌      | 149/415 [01:36<01:15,  3.54it/s] 36%|███▌      | 150/415 [01:37<01:14,  3.57it/s] 36%|███▋      | 151/415 [01:37<01:17,  3.40it/s] 37%|███▋      | 152/415 [01:37<01:15,  3.47it/s] 37%|███▋      | 153/415 [01:37<01:14,  3.51it/s] 37%|███▋      | 154/415 [01:38<01:13,  3.54it/s] 37%|███▋      | 155/415 [01:38<01:12,  3.56it/s] 38%|███▊      | 156/415 [01:38<01:12,  3.58it/s] 38%|███▊      | 157/415 [01:39<01:11,  3.60it/s] 38%|███▊      | 158/415 [01:39<01:11,  3.60it/s] 38%|███▊      | 159/415 [01:39<01:10,  3.61it/s] 39%|███▊      | 160/415 [01:39<01:10,  3.61it/s] 39%|███▉      | 161/415 [01:40<01:10,  3.61it/s] 39%|███▉      | 162/415 [01:40<01:15,  3.35it/s] 39%|███▉      | 163/415 [01:40<01:13,  3.43it/s] 40%|███▉      | 164/415 [01:41<01:12,  3.48it/s] 40%|███▉      | 165/415 [01:41<01:10,  3.52it/s] 40%|████      | 166/415 [01:41<01:06,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 04:36:28,073 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:36:28,073 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 04:36:28,073 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.0226, 'eval_samples_per_second': 348.912, 'eval_steps_per_second': 43.701, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.30it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.17it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.59it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.83it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.26it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.61it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.26it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.72it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.86it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.93it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.12it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.19it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.33it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.36it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.17it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.01it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.76it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.77it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.82it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.98it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 45.14it/s][A
 26%|██▌       | 112/438 [00:02<00:08, 37.72it/s][A
 27%|██▋       | 117/438 [00:02<00:08, 39.75it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 41.34it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 42.47it/s][A
 30%|███       | 132/438 [00:02<00:07, 43.26it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.93it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.40it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.63it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.38it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.34it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.39it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.62it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.80it/s][A
 40%|████      | 177/438 [00:03<00:05, 45.01it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 45.15it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 45.24it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.24it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.04it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.80it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.69it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.75it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.84it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.96it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 45.15it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 45.21it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 45.09it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.93it/s][A
 56%|█████▋    | 247/438 [00:05<00:05, 37.99it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 39.93it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 41.47it/s][A
 60%|█████▉    | 262/438 [00:05<00:04, 42.56it/s][A
 61%|██████    | 267/438 [00:06<00:03, 43.41it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 43.97it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.42it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.65it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.33it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.37it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.33it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.77it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.88it/s][A
 71%|███████   | 312/438 [00:07<00:02, 45.10it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.21it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 45.20it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 45.09it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.79it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.60it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.59it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.82it/s][A
 80%|████████  | 352/438 [00:07<00:01, 44.76it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.05it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.20it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.17it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 45.04it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.94it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 28.80it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 32.34it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 35.34it/s][A
 91%|█████████ | 397/438 [00:09<00:01, 37.92it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 39.81it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 41.35it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 42.42it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 43.25it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.39it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 43.68it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.07it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.44it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.44it/s][A 40%|████      | 166/415 [01:51<01:06,  3.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:36:38,663 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166
[INFO|configuration_utils.py:351] 2023-08-28 04:36:39,417 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:37:10,065 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:37:11,029 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:37:11,317 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166/special_tokens_map.json
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 40%|████      | 167/415 [02:29<59:49, 14.47s/it] 40%|████      | 168/415 [02:29<42:08, 10.24s/it] 41%|████      | 169/415 [02:29<29:43,  7.25s/it] 41%|████      | 170/415 [02:30<21:03,  5.16s/it] 41%|████      | 171/415 [02:30<15:01,  3.69s/it] 41%|████▏     | 172/415 [02:30<10:48,  2.67s/it] 42%|████▏     | 173/415 [02:30<07:52,  1.95s/it] 42%|████▏     | 174/415 [02:31<05:48,  1.45s/it] 42%|████▏     | 175/415 [02:31<04:22,  1.10s/it] 42%|████▏     | 176/415 [02:31<03:22,  1.18it/s] 43%|████▎     | 177/415 [02:32<02:41,  1.48it/s] 43%|████▎     | 178/415 [02:32<02:11,  1.80it/s] 43%|████▎     | 179/415 [02:32<01:58,  1.99it/s] 43%|████▎     | 180/415 [02:32<01:42,  2.30it/s] 44%|████▎     | 181/415 [02:33<01:30,  2.58it/s] 44%|████▍     | 182/415 [02:33<01:22,  2.83it/s] 44%|████▍     | 183/415 [02:33<01:16,  3.03it/s] 44%|████▍     | 184/415 [02:34<01:12,  3.19it/s] 45%|████▍     | 185/415 [02:34<01:09,  3.31it/s] 45%|████▍     | 186/415 [02:34<01:07,  3.40it/s] 45%|████▌     | 187/415 [02:34<01:05,  3.47it/s] 45%|████▌     | 188/415 [02:35<01:04,  3.52it/s] 46%|████▌     | 189/415 [02:35<01:03,  3.55it/s] 46%|████▌     | 190/415 [02:35<01:11,  3.14it/s] 46%|████▌     | 191/415 [02:36<01:12,  3.09it/s] 46%|████▋     | 192/415 [02:36<01:09,  3.22it/s] 47%|████▋     | 193/415 [02:36<01:06,  3.33it/s] 47%|████▋     | 194/415 [02:37<01:04,  3.42it/s] 47%|████▋     | 195/415 [02:37<01:03,  3.48it/s] 47%|████▋     | 196/415 [02:37<01:02,  3.52it/s] 47%|████▋     | 197/415 [02:37<01:01,  3.56it/s] 48%|████▊     | 198/415 [02:38<01:00,  3.58it/s] 48%|████▊     | 199/415 [02:38<01:00,  3.60it/s] 48%|████▊     | 200/415 [02:38<00:59,  3.61it/s] 48%|████▊     | 201/415 [02:39<01:05,  3.26it/s] 49%|████▊     | 202/415 [02:39<01:03,  3.36it/s] 49%|████▉     | 203/415 [02:39<01:01,  3.44it/s] 49%|████▉     | 204/415 [02:39<01:00,  3.50it/s] 49%|████▉     | 205/415 [02:40<00:59,  3.54it/s] 50%|████▉     | 206/415 [02:40<00:58,  3.57it/s] 50%|████▉     | 207/415 [02:40<00:57,  3.59it/s] 50%|█████     | 208/415 [02:40<00:57,  3.60it/s] 50%|█████     | 209/415 [02:41<00:57,  3.61it/s] 51%|█████     | 210/415 [02:41<00:56,  3.61it/s] 51%|█████     | 211/415 [02:41<00:56,  3.62it/s] 51%|█████     | 212/415 [02:42<01:03,  3.21it/s] 51%|█████▏    | 213/415 [02:42<01:00,  3.32it/s] 52%|█████▏    | 214/415 [02:42<01:04,  3.13it/s] 52%|█████▏    | 215/415 [02:43<01:01,  3.26it/s] 52%|█████▏    | 216/415 [02:43<00:59,  3.36it/s] 52%|█████▏    | 217/415 [02:43<00:57,  3.44it/s] 53%|█████▎    | 218/415 [02:43<00:56,  3.50it/s] 53%|█████▎    | 219/415 [02:44<00:55,  3.54it/s] 53%|█████▎    | 220/415 [02:44<00:54,  3.56it/s] 53%|█████▎    | 221/415 [02:44<00:54,  3.58it/s] 53%|█████▎    | 222/415 [02:45<00:53,  3.60it/s] 54%|█████▎    | 223/415 [02:45<00:53,  3.61it/s] 54%|█████▍    | 224/415 [02:45<00:52,  3.61it/s] 54%|█████▍    | 225/415 [02:45<00:57,  3.29it/s] 54%|█████▍    | 226/415 [02:46<00:55,  3.39it/s] 55%|█████▍    | 227/415 [02:46<00:54,  3.45it/s] 55%|█████▍    | 228/415 [02:46<00:53,  3.50it/s] 55%|█████▌    | 229/415 [02:47<00:52,  3.54it/s] 55%|█████▌    | 230/415 [02:47<00:51,  3.56it/s] 56%|█████▌    | 231/415 [02:47<00:51,  3.58it/s] 56%|█████▌    | 232/415 [02:47<00:50,  3.59it/s] 56%|█████▌    | 233/415 [02:48<00:50,  3.60it/s] 56%|█████▋    | 234/415 [02:48<00:50,  3.61it/s] 57%|█████▋    | 235/415 [02:48<00:49,  3.61it/s] 57%|█████▋    | 236/415 [02:49<00:55,  3.20it/s] 57%|█████▋    | 237/415 [02:49<00:53,  3.32it/s] 57%|█████▋    | 238/415 [02:49<00:52,  3.40it/s] 58%|█████▊    | 239/415 [02:49<00:50,  3.47it/s] 58%|█████▊    | 240/415 [02:50<00:49,  3.51it/s] 58%|█████▊    | 241/415 [02:50<00:49,  3.54it/s] 58%|█████▊    | 242/415 [02:50<00:48,  3.57it/s] 59%|█████▊    | 243/415 [02:51<00:47,  3.59it/s] 59%|█████▉    | 244/415 [02:51<00:47,  3.60it/s] 59%|█████▉    | 245/415 [02:51<00:47,  3.61it/s] 59%|█████▉    | 246/415 [02:51<00:46,  3.61it/s] 60%|█████▉    | 247/415 [02:52<00:49,  3.38it/s] 60%|█████▉    | 248/415 [02:52<00:48,  3.45it/s] 60%|██████    | 249/415 [02:52<00:45,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 04:37:39,201 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:37:39,201 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 04:37:39,201 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.061, 'eval_samples_per_second': 347.58, 'eval_steps_per_second': 43.534, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.38it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.09it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.27it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.56it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.23it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.88it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.40it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.94it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.90it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.97it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.10it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.12it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.14it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.25it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.16it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.88it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.72it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.67it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.85it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.93it/s][A
 24%|██▍       | 107/438 [00:02<00:08, 39.27it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 40.93it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 42.24it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.13it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.85it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.18it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.51it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.42it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.27it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.17it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.32it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.45it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.79it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.00it/s][A
 40%|████      | 177/438 [00:03<00:05, 45.18it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 45.07it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 45.06it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.72it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.63it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.65it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.73it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.87it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.03it/s][A
 51%|█████     | 222/438 [00:04<00:04, 45.23it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 45.20it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 45.17it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.68it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 41.47it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 42.60it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 43.37it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.02it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.37it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.68it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.90it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.80it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.40it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.38it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.54it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.85it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.93it/s][A
 70%|███████   | 307/438 [00:06<00:02, 45.09it/s][A
 71%|███████   | 312/438 [00:06<00:02, 45.18it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.20it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.93it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.68it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.63it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.72it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.76it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.89it/s][A
 80%|████████  | 352/438 [00:07<00:01, 45.14it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 45.18it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.13it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.00it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.86it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 37.90it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 39.84it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 41.38it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 42.52it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 43.36it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.04it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.50it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.54it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.35it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.17it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.34it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.61it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.78it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.78it/s][A 60%|██████    | 249/415 [03:02<00:45,  3.68it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:37:49,531 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249
[INFO|configuration_utils.py:351] 2023-08-28 04:37:50,104 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:38:17,267 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:38:18,001 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:38:18,266 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249/special_tokens_map.json
 60%|██████    | 250/415 [03:36<37:00, 13.46s/it] 60%|██████    | 251/415 [03:37<26:04,  9.54s/it] 61%|██████    | 252/415 [03:37<18:21,  6.76s/it] 61%|██████    | 253/415 [03:37<13:00,  4.82s/it] 61%|██████    | 254/415 [03:38<09:16,  3.45s/it] 61%|██████▏   | 255/415 [03:38<06:40,  2.50s/it] 62%|██████▏   | 256/415 [03:38<04:51,  1.83s/it] 62%|██████▏   | 257/415 [03:38<03:36,  1.37s/it] 62%|██████▏   | 258/415 [03:39<02:43,  1.04s/it] 62%|██████▏   | 259/415 [03:39<02:06,  1.23it/s] 63%|██████▎   | 260/415 [03:39<01:40,  1.53it/s] 63%|██████▎   | 261/415 [03:40<01:23,  1.85it/s] 63%|██████▎   | 262/415 [03:40<01:13,  2.09it/s] 63%|██████▎   | 263/415 [03:40<01:03,  2.39it/s] 64%|██████▎   | 264/415 [03:41<00:56,  2.65it/s] 64%|██████▍   | 265/415 [03:41<00:52,  2.87it/s] 64%|██████▍   | 266/415 [03:41<00:48,  3.05it/s] 64%|██████▍   | 267/415 [03:41<00:46,  3.19it/s] 65%|██████▍   | 268/415 [03:42<00:44,  3.30it/s] 65%|██████▍   | 269/415 [03:42<00:43,  3.38it/s] 65%|██████▌   | 270/415 [03:42<00:42,  3.43it/s] 65%|██████▌   | 271/415 [03:42<00:41,  3.47it/s] 66%|██████▌   | 272/415 [03:43<00:40,  3.50it/s] 66%|██████▌   | 273/415 [03:43<00:42,  3.33it/s] 66%|██████▌   | 274/415 [03:43<00:41,  3.40it/s] 66%|██████▋   | 275/415 [03:44<00:43,  3.25it/s] 67%|██████▋   | 276/415 [03:44<00:41,  3.34it/s] 67%|██████▋   | 277/415 [03:44<00:40,  3.41it/s] 67%|██████▋   | 278/415 [03:45<00:39,  3.46it/s] 67%|██████▋   | 279/415 [03:45<00:38,  3.49it/s] 67%|██████▋   | 280/415 [03:45<00:38,  3.52it/s] 68%|██████▊   | 281/415 [03:45<00:37,  3.53it/s] 68%|██████▊   | 282/415 [03:46<00:37,  3.55it/s] 68%|██████▊   | 283/415 [03:46<00:37,  3.56it/s] 68%|██████▊   | 284/415 [03:46<00:36,  3.57it/s] 69%|██████▊   | 285/415 [03:46<00:36,  3.57it/s] 69%|██████▉   | 286/415 [03:47<00:38,  3.37it/s] 69%|██████▉   | 287/415 [03:47<00:37,  3.43it/s] 69%|██████▉   | 288/415 [03:47<00:36,  3.48it/s] 70%|██████▉   | 289/415 [03:48<00:35,  3.51it/s] 70%|██████▉   | 290/415 [03:48<00:35,  3.53it/s] 70%|███████   | 291/415 [03:48<00:34,  3.55it/s] 70%|███████   | 292/415 [03:49<00:34,  3.55it/s] 71%|███████   | 293/415 [03:49<00:34,  3.56it/s] 71%|███████   | 294/415 [03:49<00:33,  3.56it/s] 71%|███████   | 295/415 [03:49<00:33,  3.57it/s] 71%|███████▏  | 296/415 [03:50<00:33,  3.57it/s] 72%|███████▏  | 297/415 [03:50<00:35,  3.31it/s] 72%|███████▏  | 298/415 [03:50<00:34,  3.39it/s] 72%|███████▏  | 299/415 [03:51<00:33,  3.45it/s] 72%|███████▏  | 300/415 [03:51<00:32,  3.51it/s] 73%|███████▎  | 301/415 [03:51<00:32,  3.54it/s] 73%|███████▎  | 302/415 [03:51<00:31,  3.56it/s] 73%|███████▎  | 303/415 [03:52<00:31,  3.58it/s] 73%|███████▎  | 304/415 [03:52<00:30,  3.60it/s] 73%|███████▎  | 305/415 [03:52<00:30,  3.61it/s] 74%|███████▎  | 306/415 [03:52<00:30,  3.61it/s] 74%|███████▍  | 307/415 [03:53<00:29,  3.61it/s] 74%|███████▍  | 308/415 [03:53<00:31,  3.42it/s] 74%|███████▍  | 309/415 [03:53<00:30,  3.48it/s] 75%|███████▍  | 310/415 [03:54<00:29,  3.52it/s] 75%|███████▍  | 311/415 [03:54<00:29,  3.55it/s] 75%|███████▌  | 312/415 [03:54<00:28,  3.57it/s] 75%|███████▌  | 313/415 [03:54<00:28,  3.59it/s] 76%|███████▌  | 314/415 [03:55<00:28,  3.60it/s] 76%|███████▌  | 315/415 [03:55<00:27,  3.60it/s] 76%|███████▌  | 316/415 [03:55<00:27,  3.61it/s] 76%|███████▋  | 317/415 [03:56<00:27,  3.61it/s] 77%|███████▋  | 318/415 [03:56<00:26,  3.61it/s] 77%|███████▋  | 319/415 [03:56<00:35,  2.74it/s] 77%|███████▋  | 320/415 [03:57<00:32,  2.96it/s] 77%|███████▋  | 321/415 [03:57<00:30,  3.13it/s] 78%|███████▊  | 322/415 [03:57<00:28,  3.27it/s] 78%|███████▊  | 323/415 [03:58<00:27,  3.37it/s] 78%|███████▊  | 324/415 [03:58<00:26,  3.44it/s] 78%|███████▊  | 325/415 [03:58<00:25,  3.49it/s] 79%|███████▊  | 326/415 [03:58<00:25,  3.53it/s] 79%|███████▉  | 327/415 [03:59<00:24,  3.56it/s] 79%|███████▉  | 328/415 [03:59<00:24,  3.59it/s] 79%|███████▉  | 329/415 [03:59<00:25,  3.41it/s] 80%|███████▉  | 330/415 [03:59<00:24,  3.47it/s] 80%|███████▉  | 331/415 [04:00<00:23,  3.52it/s] 80%|████████  | 332/415 [04:00<00:22,  3.73it/s][INFO|trainer.py:2140] 2023-08-28 04:38:46,979 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:38:46,980 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 04:38:46,980 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 9.8774, 'eval_samples_per_second': 354.042, 'eval_steps_per_second': 44.344, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.37it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.99it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.54it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.80it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.35it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.86it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.39it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.85it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.86it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.05it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.16it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.21it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.28it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.34it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.21it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.94it/s][A
 20%|█▉        | 87/438 [00:02<00:07, 44.73it/s][A
 21%|██        | 92/438 [00:02<00:09, 36.75it/s][A
 22%|██▏       | 97/438 [00:02<00:08, 39.07it/s][A
 23%|██▎       | 102/438 [00:02<00:08, 40.73it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 42.13it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 43.08it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 43.78it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.24it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.42it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.26it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.19it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.44it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.50it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.76it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.98it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.15it/s][A
 38%|███▊      | 167/438 [00:03<00:05, 45.20it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.17it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.87it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.64it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.67it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.80it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.94it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.04it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.22it/s][A
 48%|████▊     | 212/438 [00:04<00:04, 45.24it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.10it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.99it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 37.10it/s][A
 53%|█████▎    | 232/438 [00:05<00:05, 39.24it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 40.93it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 42.18it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 43.14it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 43.84it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.27it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.56it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.24it/s][A
 62%|██████▏   | 272/438 [00:06<00:04, 38.80it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 41.31it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 42.57it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 43.33it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 43.96it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.33it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.74it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.94it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.77it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.38it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.39it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.62it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.95it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 45.05it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 45.08it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 45.21it/s][A
 80%|████████  | 352/438 [00:08<00:01, 45.23it/s][A
 82%|████████▏ | 357/438 [00:08<00:02, 34.93it/s][A
 83%|████████▎ | 362/438 [00:08<00:02, 37.47it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 39.58it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 41.16it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 42.36it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 43.20it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 43.92it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.30it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.11it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.04it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.07it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.25it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.43it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.65it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.91it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 45.09it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 45.18it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 45.18it/s][A 80%|████████  | 332/415 [04:10<00:22,  3.73it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:38:57,715 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332
[INFO|configuration_utils.py:351] 2023-08-28 04:38:58,489 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:39:26,530 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:39:27,074 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:39:27,324 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332/special_tokens_map.json
 80%|████████  | 333/415 [04:44<18:29, 13.53s/it] 80%|████████  | 334/415 [04:45<12:55,  9.57s/it] 81%|████████  | 335/415 [04:45<09:07,  6.84s/it] 81%|████████  | 336/415 [04:46<06:24,  4.87s/it] 81%|████████  | 337/415 [04:46<04:32,  3.49s/it] 81%|████████▏ | 338/415 [04:46<03:14,  2.53s/it] 82%|████████▏ | 339/415 [04:46<02:20,  1.85s/it] 82%|████████▏ | 340/415 [04:47<01:43,  1.38s/it] 82%|████████▏ | 341/415 [04:47<01:17,  1.05s/it] 82%|████████▏ | 342/415 [04:47<00:59,  1.22it/s] 83%|████████▎ | 343/415 [04:47<00:47,  1.52it/s] 83%|████████▎ | 344/415 [04:48<00:38,  1.84it/s] 83%|████████▎ | 345/415 [04:48<00:33,  2.07it/s] 83%|████████▎ | 346/415 [04:48<00:29,  2.37it/s] 84%|████████▎ | 347/415 [04:49<00:25,  2.64it/s] 84%|████████▍ | 348/415 [04:49<00:23,  2.87it/s] 84%|████████▍ | 349/415 [04:49<00:21,  3.05it/s] 84%|████████▍ | 350/415 [04:50<00:20,  3.19it/s] 85%|████████▍ | 351/415 [04:50<00:19,  3.30it/s] 85%|████████▍ | 352/415 [04:50<00:18,  3.39it/s] 85%|████████▌ | 353/415 [04:50<00:17,  3.46it/s] 85%|████████▌ | 354/415 [04:51<00:17,  3.51it/s] 86%|████████▌ | 355/415 [04:51<00:16,  3.55it/s] 86%|████████▌ | 356/415 [04:51<00:18,  3.11it/s] 86%|████████▌ | 357/415 [04:52<00:17,  3.26it/s] 86%|████████▋ | 358/415 [04:52<00:16,  3.37it/s] 87%|████████▋ | 359/415 [04:52<00:16,  3.45it/s] 87%|████████▋ | 360/415 [04:52<00:15,  3.50it/s] 87%|████████▋ | 361/415 [04:53<00:15,  3.54it/s] 87%|████████▋ | 362/415 [04:53<00:14,  3.57it/s] 87%|████████▋ | 363/415 [04:53<00:14,  3.59it/s] 88%|████████▊ | 364/415 [04:53<00:14,  3.60it/s] 88%|████████▊ | 365/415 [04:54<00:13,  3.61it/s] 88%|████████▊ | 366/415 [04:54<00:13,  3.62it/s] 88%|████████▊ | 367/415 [04:54<00:15,  3.17it/s] 89%|████████▊ | 368/415 [04:55<00:14,  3.30it/s] 89%|████████▉ | 369/415 [04:55<00:13,  3.40it/s] 89%|████████▉ | 370/415 [04:55<00:12,  3.46it/s] 89%|████████▉ | 371/415 [04:56<00:12,  3.51it/s] 90%|████████▉ | 372/415 [04:56<00:12,  3.55it/s] 90%|████████▉ | 373/415 [04:56<00:11,  3.58it/s] 90%|█████████ | 374/415 [04:56<00:11,  3.59it/s] 90%|█████████ | 375/415 [04:57<00:11,  3.61it/s] 91%|█████████ | 376/415 [04:57<00:10,  3.62it/s] 91%|█████████ | 377/415 [04:57<00:10,  3.62it/s] 91%|█████████ | 378/415 [04:58<00:11,  3.30it/s] 91%|█████████▏| 379/415 [04:58<00:10,  3.40it/s] 92%|█████████▏| 380/415 [04:58<00:10,  3.47it/s] 92%|█████████▏| 381/415 [04:58<00:09,  3.51it/s] 92%|█████████▏| 382/415 [04:59<00:09,  3.55it/s] 92%|█████████▏| 383/415 [04:59<00:08,  3.57it/s] 93%|█████████▎| 384/415 [04:59<00:08,  3.60it/s] 93%|█████████▎| 385/415 [04:59<00:08,  3.61it/s] 93%|█████████▎| 386/415 [05:00<00:08,  3.62it/s] 93%|█████████▎| 387/415 [05:00<00:07,  3.62it/s] 93%|█████████▎| 388/415 [05:00<00:07,  3.63it/s] 94%|█████████▎| 389/415 [05:01<00:08,  3.18it/s] 94%|█████████▍| 390/415 [05:01<00:07,  3.30it/s] 94%|█████████▍| 391/415 [05:01<00:07,  3.40it/s] 94%|█████████▍| 392/415 [05:02<00:06,  3.47it/s] 95%|█████████▍| 393/415 [05:02<00:06,  3.51it/s] 95%|█████████▍| 394/415 [05:02<00:05,  3.54it/s] 95%|█████████▌| 395/415 [05:02<00:05,  3.57it/s] 95%|█████████▌| 396/415 [05:03<00:05,  3.59it/s] 96%|█████████▌| 397/415 [05:03<00:04,  3.61it/s] 96%|█████████▌| 398/415 [05:03<00:04,  3.62it/s] 96%|█████████▌| 399/415 [05:03<00:04,  3.62it/s] 96%|█████████▋| 400/415 [05:04<00:04,  3.35it/s] 97%|█████████▋| 401/415 [05:04<00:04,  3.43it/s] 97%|█████████▋| 402/415 [05:04<00:03,  3.49it/s] 97%|█████████▋| 403/415 [05:05<00:03,  3.53it/s] 97%|█████████▋| 404/415 [05:05<00:03,  3.56it/s] 98%|█████████▊| 405/415 [05:05<00:02,  3.58it/s] 98%|█████████▊| 406/415 [05:05<00:02,  3.60it/s] 98%|█████████▊| 407/415 [05:06<00:02,  3.62it/s] 98%|█████████▊| 408/415 [05:06<00:01,  3.62it/s] 99%|█████████▊| 409/415 [05:06<00:01,  3.31it/s] 99%|█████████▉| 410/415 [05:07<00:01,  3.39it/s] 99%|█████████▉| 411/415 [05:07<00:01,  3.12it/s] 99%|█████████▉| 412/415 [05:07<00:00,  3.26it/s]100%|█████████▉| 413/415 [05:08<00:00,  3.37it/s]100%|█████████▉| 414/415 [05:08<00:00,  3.44it/s]100%|██████████| 415/415 [05:08<00:00,  3.68it/s][INFO|trainer.py:2140] 2023-08-28 04:39:55,084 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:39:55,084 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 04:39:55,084 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.0202, 'eval_samples_per_second': 348.997, 'eval_steps_per_second': 43.712, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.91it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.47it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.45it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.50it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.01it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.42it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.26it/s][A
 10%|▉         | 43/438 [00:00<00:08, 44.71it/s][A
 11%|█         | 48/438 [00:01<00:08, 44.96it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.12it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.19it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.32it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.32it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.18it/s][A
 18%|█▊        | 78/438 [00:01<00:09, 38.82it/s][A
 19%|█▉        | 83/438 [00:01<00:08, 40.62it/s][A
 20%|██        | 88/438 [00:01<00:08, 41.99it/s][A
 21%|██        | 93/438 [00:02<00:08, 42.93it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 43.63it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 44.18it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 44.59it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.84it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.33it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.31it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.44it/s][A
 30%|███       | 133/438 [00:02<00:06, 44.79it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.85it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 45.05it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 45.26it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.33it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 45.12it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.73it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.65it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 44.64it/s][A
 41%|████      | 178/438 [00:03<00:05, 44.92it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 44.89it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.05it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.22it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.26it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.14it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 44.85it/s][A
 49%|████▊     | 213/438 [00:04<00:06, 34.07it/s][A
 50%|████▉     | 218/438 [00:04<00:05, 36.86it/s][A
 51%|█████     | 223/438 [00:05<00:05, 39.00it/s][A
 52%|█████▏    | 228/438 [00:05<00:05, 40.73it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 42.13it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 42.91it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 43.69it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 44.04it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.09it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.20it/s][A
 60%|██████    | 263/438 [00:05<00:03, 44.42it/s][A
 61%|██████    | 268/438 [00:06<00:03, 44.68it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 44.79it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 45.03it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 45.01it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 45.13it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 45.00it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.73it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 44.75it/s][A
 70%|███████   | 308/438 [00:06<00:02, 44.74it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 44.96it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.04it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.13it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.10it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 38.60it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 40.44it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 41.91it/s][A
 79%|███████▉  | 348/438 [00:07<00:02, 42.85it/s][A
 81%|████████  | 353/438 [00:08<00:01, 43.65it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 44.11it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 44.58it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 44.73it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.41it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 44.22it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 44.26it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 44.53it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 44.77it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 45.05it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 45.12it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 45.25it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 45.09it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.80it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 44.53it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 44.56it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 44.70it/s][A
100%|██████████| 438/438 [00:09<00:00, 45.03it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 45.03it/s][A100%|██████████| 415/415 [05:18<00:00,  3.68it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 04:40:05,639 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415
[INFO|configuration_utils.py:351] 2023-08-28 04:40:06,354 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:40:30,215 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:40:31,037 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:40:31,314 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 04:40:35,478 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 04:40:35,478 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83 (score: 1.0607956647872925).
                                                 100%|██████████| 415/415 [06:13<00:00,  3.68it/s]100%|██████████| 415/415 [06:13<00:00,  1.11it/s]
[INFO|trainer.py:1894] 2023-08-28 04:40:59,887 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 04:41:00,494 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 04:41:30,794 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 04:41:32,138 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 04:41:32,502 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:41:35,034 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:35,034 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:35,034 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:35,034 >>   train_runtime            = 0:06:13.13
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:35,034 >>   train_samples            =       5300
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:35,034 >>   train_samples_per_second =     71.021
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:35,034 >>   train_steps_per_second   =      1.112
{'eval_loss': 1.0607956647872925, 'eval_runtime': 9.9512, 'eval_samples_per_second': 351.415, 'eval_steps_per_second': 44.015, 'epoch': 5.0}
{'train_runtime': 373.1311, 'train_samples_per_second': 71.021, 'train_steps_per_second': 1.112, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 04:41:36 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 04:41:36,259 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 04:41:36,259 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 04:41:36,259 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 56.49it/s]  3%|▎         | 12/438 [00:00<00:08, 49.66it/s]  4%|▍         | 18/438 [00:00<00:08, 48.15it/s]  5%|▌         | 23/438 [00:00<00:08, 47.26it/s]  6%|▋         | 28/438 [00:00<00:08, 46.75it/s]  8%|▊         | 33/438 [00:00<00:08, 46.45it/s]  9%|▊         | 38/438 [00:00<00:08, 46.22it/s] 10%|▉         | 43/438 [00:00<00:08, 45.78it/s] 11%|█         | 48/438 [00:01<00:08, 45.34it/s] 12%|█▏        | 53/438 [00:01<00:08, 45.13it/s] 13%|█▎        | 58/438 [00:01<00:08, 45.22it/s] 14%|█▍        | 63/438 [00:01<00:08, 45.26it/s] 16%|█▌        | 68/438 [00:01<00:09, 37.71it/s] 17%|█▋        | 73/438 [00:01<00:09, 39.88it/s] 18%|█▊        | 78/438 [00:01<00:08, 41.46it/s] 19%|█▉        | 83/438 [00:01<00:08, 42.65it/s] 20%|██        | 88/438 [00:01<00:08, 43.53it/s] 21%|██        | 93/438 [00:02<00:07, 44.20it/s] 22%|██▏       | 98/438 [00:02<00:07, 44.68it/s] 24%|██▎       | 103/438 [00:02<00:07, 44.85it/s] 25%|██▍       | 108/438 [00:02<00:07, 44.65it/s] 26%|██▌       | 113/438 [00:02<00:07, 44.56it/s] 27%|██▋       | 118/438 [00:02<00:07, 44.71it/s] 28%|██▊       | 123/438 [00:02<00:07, 44.86it/s] 29%|██▉       | 128/438 [00:02<00:06, 45.05it/s] 30%|███       | 133/438 [00:02<00:06, 45.27it/s] 32%|███▏      | 138/438 [00:03<00:06, 45.47it/s] 33%|███▎      | 143/438 [00:03<00:06, 45.54it/s] 34%|███▍      | 148/438 [00:03<00:06, 45.51it/s] 35%|███▍      | 153/438 [00:03<00:06, 45.23it/s] 36%|███▌      | 158/438 [00:03<00:06, 45.00it/s] 37%|███▋      | 163/438 [00:03<00:06, 44.93it/s] 38%|███▊      | 168/438 [00:03<00:05, 45.06it/s] 39%|███▉      | 173/438 [00:03<00:05, 44.96it/s] 41%|████      | 178/438 [00:03<00:05, 45.22it/s] 42%|████▏     | 183/438 [00:04<00:05, 45.43it/s] 43%|████▎     | 188/438 [00:04<00:05, 45.47it/s] 44%|████▍     | 193/438 [00:04<00:05, 45.52it/s] 45%|████▌     | 198/438 [00:04<00:05, 45.36it/s] 46%|████▋     | 203/438 [00:04<00:06, 34.57it/s] 47%|████▋     | 208/438 [00:04<00:06, 37.33it/s] 49%|████▊     | 213/438 [00:04<00:05, 39.33it/s] 50%|████▉     | 218/438 [00:04<00:05, 41.02it/s] 51%|█████     | 223/438 [00:05<00:05, 42.33it/s] 52%|█████▏    | 228/438 [00:05<00:04, 43.33it/s] 53%|█████▎    | 233/438 [00:05<00:04, 43.98it/s] 54%|█████▍    | 238/438 [00:05<00:04, 44.41it/s] 55%|█████▌    | 243/438 [00:05<00:04, 44.22it/s] 57%|█████▋    | 248/438 [00:05<00:04, 44.32it/s] 58%|█████▊    | 253/438 [00:05<00:04, 44.44it/s] 59%|█████▉    | 258/438 [00:05<00:04, 44.69it/s] 60%|██████    | 263/438 [00:05<00:03, 45.07it/s] 61%|██████    | 268/438 [00:06<00:03, 45.29it/s] 62%|██████▏   | 273/438 [00:06<00:03, 45.21it/s] 63%|██████▎   | 278/438 [00:06<00:03, 45.36it/s] 65%|██████▍   | 283/438 [00:06<00:03, 45.35it/s] 66%|██████▌   | 288/438 [00:06<00:03, 44.97it/s] 67%|██████▋   | 293/438 [00:06<00:03, 44.82it/s] 68%|██████▊   | 298/438 [00:06<00:03, 44.81it/s] 69%|██████▉   | 303/438 [00:06<00:03, 44.96it/s] 70%|███████   | 308/438 [00:06<00:02, 45.18it/s] 71%|███████▏  | 313/438 [00:07<00:02, 45.42it/s] 73%|███████▎  | 318/438 [00:07<00:02, 45.43it/s] 74%|███████▎  | 323/438 [00:07<00:02, 45.45it/s] 75%|███████▍  | 328/438 [00:07<00:02, 45.29it/s] 76%|███████▌  | 333/438 [00:07<00:02, 45.06it/s] 77%|███████▋  | 338/438 [00:07<00:02, 37.27it/s] 78%|███████▊  | 343/438 [00:07<00:02, 39.45it/s] 79%|███████▉  | 348/438 [00:07<00:02, 41.15it/s] 81%|████████  | 353/438 [00:08<00:02, 42.41it/s] 82%|████████▏ | 358/438 [00:08<00:01, 43.44it/s] 83%|████████▎ | 363/438 [00:08<00:01, 44.06it/s] 84%|████████▍ | 368/438 [00:08<00:01, 44.68it/s] 85%|████████▌ | 373/438 [00:08<00:01, 45.01it/s] 86%|████████▋ | 378/438 [00:08<00:01, 44.78it/s] 87%|████████▋ | 383/438 [00:08<00:01, 44.65it/s] 89%|████████▊ | 388/438 [00:08<00:01, 44.77it/s] 90%|████████▉ | 393/438 [00:08<00:00, 45.06it/s] 91%|█████████ | 398/438 [00:09<00:00, 45.29it/s] 92%|█████████▏| 403/438 [00:09<00:00, 45.46it/s] 93%|█████████▎| 408/438 [00:09<00:00, 45.64it/s] 94%|█████████▍| 413/438 [00:09<00:00, 45.74it/s] 95%|█████████▌| 418/438 [00:09<00:00, 45.54it/s] 97%|█████████▋| 423/438 [00:09<00:00, 45.35it/s] 98%|█████████▊| 428/438 [00:09<00:00, 45.06it/s] 99%|█████████▉| 433/438 [00:09<00:00, 45.08it/s]100%|██████████| 438/438 [00:09<00:00, 45.21it/s]100%|██████████| 438/438 [00:09<00:00, 44.24it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 04:41:46,195 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:46,195 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:46,195 >>   eval_loss               =     1.0608
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:46,195 >>   eval_runtime            = 0:00:09.93
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:46,195 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:46,195 >>   eval_samples_per_second =     351.94
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:46,195 >>   eval_steps_per_second   =     44.081
[INFO|trainer_pt_utils.py:913] 2023-08-28 04:41:46,195 >>   perplexity              =     2.8887
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:11,430 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:11,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:11,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:11,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:11,530 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:42:12,855 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:42:12,856 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:42:13,617 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:42:14,918 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:42:15,016 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:18,625 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:18,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:18,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:18,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:42:18,730 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:42:19,972 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:42:19,973 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:42:20,729 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:42:21,067 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:42:21,067 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-249
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-415
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-166
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-83
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/checkpoint-332
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.43it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.67it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.65it/s]Extractor Predicting: 9it [00:05,  1.66it/s]Extractor Predicting: 10it [00:06,  1.65it/s]Extractor Predicting: 11it [00:06,  1.61it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.64it/s]Extractor Predicting: 15it [00:09,  1.56it/s]Extractor Predicting: 16it [00:10,  1.51it/s]Extractor Predicting: 17it [00:10,  1.56it/s]Extractor Predicting: 18it [00:11,  1.61it/s]Extractor Predicting: 19it [00:11,  1.64it/s]Extractor Predicting: 20it [00:12,  1.67it/s]Extractor Predicting: 21it [00:13,  1.60it/s]Extractor Predicting: 22it [00:13,  1.65it/s]Extractor Predicting: 23it [00:14,  1.67it/s]Extractor Predicting: 24it [00:14,  1.75it/s]Extractor Predicting: 25it [00:15,  1.77it/s]Extractor Predicting: 26it [00:15,  1.76it/s]Extractor Predicting: 27it [00:16,  1.61it/s]Extractor Predicting: 28it [00:17,  1.65it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:19,  1.63it/s]Extractor Predicting: 32it [00:19,  1.50it/s]Extractor Predicting: 33it [00:20,  1.55it/s]Extractor Predicting: 34it [00:21,  1.61it/s]Extractor Predicting: 35it [00:21,  1.63it/s]Extractor Predicting: 36it [00:22,  1.60it/s]Extractor Predicting: 37it [00:22,  1.56it/s]Extractor Predicting: 38it [00:23,  1.60it/s]Extractor Predicting: 39it [00:24,  1.57it/s]Extractor Predicting: 40it [00:24,  1.61it/s]Extractor Predicting: 41it [00:25,  1.61it/s]Extractor Predicting: 42it [00:26,  1.57it/s]Extractor Predicting: 43it [00:26,  1.61it/s]Extractor Predicting: 44it [00:27,  1.62it/s]Extractor Predicting: 45it [00:27,  1.63it/s]Extractor Predicting: 46it [00:28,  1.64it/s]Extractor Predicting: 47it [00:29,  1.52it/s]Extractor Predicting: 48it [00:29,  1.59it/s]Extractor Predicting: 49it [00:30,  1.59it/s]Extractor Predicting: 50it [00:31,  1.63it/s]Extractor Predicting: 51it [00:31,  1.67it/s]Extractor Predicting: 52it [00:32,  1.58it/s]Extractor Predicting: 53it [00:32,  1.58it/s]Extractor Predicting: 54it [00:33,  1.64it/s]Extractor Predicting: 55it [00:34,  1.63it/s]Extractor Predicting: 56it [00:34,  1.64it/s]Extractor Predicting: 57it [00:35,  1.68it/s]Extractor Predicting: 58it [00:35,  1.66it/s]Extractor Predicting: 59it [00:36,  1.62it/s]Extractor Predicting: 60it [00:37,  1.62it/s]Extractor Predicting: 61it [00:37,  1.52it/s]Extractor Predicting: 62it [00:38,  1.60it/s]Extractor Predicting: 63it [00:39,  1.65it/s]Extractor Predicting: 64it [00:39,  1.62it/s]Extractor Predicting: 65it [00:40,  1.67it/s]Extractor Predicting: 66it [00:40,  1.63it/s]Extractor Predicting: 67it [00:41,  1.66it/s]Extractor Predicting: 68it [00:42,  1.69it/s]Extractor Predicting: 69it [00:42,  1.68it/s]Extractor Predicting: 70it [00:43,  1.72it/s]Extractor Predicting: 71it [00:43,  1.75it/s]Extractor Predicting: 72it [00:44,  1.52it/s]Extractor Predicting: 73it [00:45,  1.57it/s]Extractor Predicting: 74it [00:45,  1.58it/s]Extractor Predicting: 75it [00:46,  1.63it/s]Extractor Predicting: 76it [00:46,  1.68it/s]Extractor Predicting: 77it [00:47,  1.62it/s]Extractor Predicting: 78it [00:48,  1.64it/s]Extractor Predicting: 79it [00:48,  1.71it/s]Extractor Predicting: 80it [00:49,  1.74it/s]Extractor Predicting: 81it [00:49,  1.75it/s]Extractor Predicting: 82it [00:50,  1.72it/s]Extractor Predicting: 83it [00:51,  1.63it/s]Extractor Predicting: 84it [00:51,  1.64it/s]Extractor Predicting: 85it [00:52,  1.63it/s]Extractor Predicting: 86it [00:52,  1.65it/s]Extractor Predicting: 87it [00:53,  1.68it/s]Extractor Predicting: 88it [00:54,  1.59it/s]Extractor Predicting: 89it [00:54,  1.62it/s]Extractor Predicting: 90it [00:55,  1.66it/s]Extractor Predicting: 91it [00:55,  1.66it/s]Extractor Predicting: 92it [00:56,  1.66it/s]Extractor Predicting: 93it [00:57,  1.59it/s]Extractor Predicting: 94it [00:57,  1.64it/s]Extractor Predicting: 95it [00:58,  1.68it/s]Extractor Predicting: 96it [00:58,  1.68it/s]Extractor Predicting: 97it [00:59,  1.49it/s]Extractor Predicting: 98it [01:00,  1.45it/s]Extractor Predicting: 99it [01:01,  1.49it/s]Extractor Predicting: 100it [01:01,  1.54it/s]Extractor Predicting: 101it [01:02,  1.62it/s]Extractor Predicting: 102it [01:02,  1.62it/s]Extractor Predicting: 103it [01:03,  1.57it/s]Extractor Predicting: 104it [01:04,  1.58it/s]Extractor Predicting: 105it [01:04,  1.61it/s]Extractor Predicting: 106it [01:05,  1.62it/s]Extractor Predicting: 107it [01:06,  1.60it/s]Extractor Predicting: 108it [01:06,  1.64it/s]Extractor Predicting: 109it [01:07,  1.62it/s]Extractor Predicting: 110it [01:07,  1.63it/s]Extractor Predicting: 111it [01:08,  1.59it/s]Extractor Predicting: 112it [01:09,  1.64it/s]Extractor Predicting: 113it [01:09,  1.62it/s]Extractor Predicting: 114it [01:10,  1.63it/s]Extractor Predicting: 115it [01:10,  1.63it/s]Extractor Predicting: 116it [01:11,  1.54it/s]Extractor Predicting: 117it [01:12,  1.56it/s]Extractor Predicting: 118it [01:12,  1.59it/s]Extractor Predicting: 119it [01:13,  1.61it/s]Extractor Predicting: 120it [01:14,  1.61it/s]Extractor Predicting: 121it [01:14,  1.55it/s]Extractor Predicting: 122it [01:15,  1.59it/s]Extractor Predicting: 123it [01:16,  1.61it/s]Extractor Predicting: 124it [01:16,  1.61it/s]Extractor Predicting: 125it [01:17,  1.59it/s]Extractor Predicting: 126it [01:18,  1.54it/s]Extractor Predicting: 127it [01:18,  1.60it/s]Extractor Predicting: 128it [01:19,  1.60it/s]Extractor Predicting: 129it [01:19,  1.62it/s]Extractor Predicting: 130it [01:20,  1.63it/s]Extractor Predicting: 131it [01:21,  1.52it/s]Extractor Predicting: 132it [01:21,  1.55it/s]Extractor Predicting: 133it [01:22,  1.59it/s]Extractor Predicting: 134it [01:22,  1.62it/s]Extractor Predicting: 135it [01:23,  1.58it/s]Extractor Predicting: 136it [01:24,  1.46it/s]Extractor Predicting: 137it [01:25,  1.53it/s]Extractor Predicting: 138it [01:25,  1.56it/s]Extractor Predicting: 139it [01:26,  1.57it/s]Extractor Predicting: 140it [01:26,  1.60it/s]Extractor Predicting: 141it [01:27,  1.52it/s]Extractor Predicting: 142it [01:28,  1.51it/s]Extractor Predicting: 143it [01:28,  1.54it/s]Extractor Predicting: 144it [01:29,  1.60it/s]Extractor Predicting: 145it [01:29,  1.81it/s]Extractor Predicting: 145it [01:29,  1.61it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:42,776 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:42,894 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:42,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:42,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:42,895 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:45:44,194 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:45:44,195 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:45:44,924 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:45:46,177 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:45:46,379 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:49,812 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:49,892 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:49,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:49,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:45:49,893 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:45:51,024 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:45:51,025 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:45:51,736 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:45:52,021 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:45:52,021 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.70it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.65it/s]Extractor Predicting: 8it [00:04,  1.65it/s]Extractor Predicting: 9it [00:05,  1.70it/s]Extractor Predicting: 10it [00:05,  1.76it/s]Extractor Predicting: 11it [00:06,  1.75it/s]Extractor Predicting: 12it [00:07,  1.63it/s]Extractor Predicting: 13it [00:07,  1.65it/s]Extractor Predicting: 14it [00:08,  1.60it/s]Extractor Predicting: 15it [00:09,  1.59it/s]Extractor Predicting: 16it [00:09,  1.63it/s]Extractor Predicting: 17it [00:10,  1.59it/s]Extractor Predicting: 18it [00:10,  1.63it/s]Extractor Predicting: 19it [00:11,  1.63it/s]Extractor Predicting: 20it [00:12,  1.63it/s]Extractor Predicting: 21it [00:12,  1.66it/s]Extractor Predicting: 22it [00:13,  1.59it/s]Extractor Predicting: 23it [00:14,  1.63it/s]Extractor Predicting: 24it [00:14,  1.61it/s]Extractor Predicting: 25it [00:15,  1.64it/s]Extractor Predicting: 26it [00:15,  1.65it/s]Extractor Predicting: 27it [00:16,  1.69it/s]Extractor Predicting: 28it [00:17,  1.67it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.69it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.66it/s]Extractor Predicting: 33it [00:20,  1.64it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:21,  1.68it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:22,  1.69it/s]Extractor Predicting: 38it [00:22,  1.71it/s]Extractor Predicting: 39it [00:23,  1.73it/s]Extractor Predicting: 40it [00:24,  1.70it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.59it/s]Extractor Predicting: 43it [00:26,  1.60it/s]Extractor Predicting: 44it [00:26,  1.63it/s]Extractor Predicting: 45it [00:27,  1.63it/s]Extractor Predicting: 46it [00:27,  1.69it/s]Extractor Predicting: 47it [00:28,  1.65it/s]Extractor Predicting: 48it [00:29,  1.67it/s]Extractor Predicting: 49it [00:29,  1.71it/s]Extractor Predicting: 50it [00:30,  1.70it/s]Extractor Predicting: 51it [00:30,  1.71it/s]Extractor Predicting: 52it [00:31,  1.74it/s]Extractor Predicting: 53it [00:31,  1.68it/s]Extractor Predicting: 54it [00:32,  1.70it/s]Extractor Predicting: 55it [00:33,  1.69it/s]Extractor Predicting: 56it [00:33,  1.66it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:35,  1.62it/s]Extractor Predicting: 59it [00:35,  1.61it/s]Extractor Predicting: 60it [00:36,  1.64it/s]Extractor Predicting: 61it [00:36,  1.65it/s]Extractor Predicting: 62it [00:37,  1.69it/s]Extractor Predicting: 63it [00:38,  1.60it/s]Extractor Predicting: 64it [00:38,  1.66it/s]Extractor Predicting: 65it [00:39,  1.68it/s]Extractor Predicting: 66it [00:39,  1.70it/s]Extractor Predicting: 67it [00:40,  1.73it/s]Extractor Predicting: 68it [00:40,  1.77it/s]Extractor Predicting: 69it [00:47,  2.28s/it]Extractor Predicting: 70it [00:47,  1.77s/it]Extractor Predicting: 71it [00:48,  1.41s/it]Extractor Predicting: 72it [00:48,  1.15s/it]Extractor Predicting: 73it [00:49,  1.01it/s]Extractor Predicting: 74it [00:50,  1.14it/s]Extractor Predicting: 75it [00:50,  1.25it/s]Extractor Predicting: 76it [00:51,  1.37it/s]Extractor Predicting: 77it [00:51,  1.50it/s]Extractor Predicting: 78it [00:52,  1.42it/s]Extractor Predicting: 79it [00:53,  1.44it/s]Extractor Predicting: 80it [00:53,  1.53it/s]Extractor Predicting: 81it [00:54,  1.59it/s]Extractor Predicting: 82it [00:54,  1.62it/s]Extractor Predicting: 83it [00:55,  1.64it/s]Extractor Predicting: 84it [00:56,  1.68it/s]Extractor Predicting: 85it [00:56,  1.63it/s]Extractor Predicting: 86it [00:57,  1.69it/s]Extractor Predicting: 87it [00:57,  1.72it/s]Extractor Predicting: 88it [00:58,  1.72it/s]Extractor Predicting: 89it [00:59,  1.66it/s]Extractor Predicting: 90it [00:59,  1.70it/s]Extractor Predicting: 91it [01:00,  1.66it/s]Extractor Predicting: 92it [01:00,  1.64it/s]Extractor Predicting: 93it [01:01,  1.64it/s]Extractor Predicting: 94it [01:02,  1.63it/s]Extractor Predicting: 95it [01:02,  1.65it/s]Extractor Predicting: 96it [01:03,  1.59it/s]Extractor Predicting: 97it [01:04,  1.63it/s]Extractor Predicting: 98it [01:04,  1.62it/s]Extractor Predicting: 99it [01:05,  1.66it/s]Extractor Predicting: 100it [01:05,  1.67it/s]Extractor Predicting: 101it [01:06,  1.57it/s]Extractor Predicting: 102it [01:07,  1.55it/s]Extractor Predicting: 103it [01:07,  1.61it/s]Extractor Predicting: 104it [01:08,  1.62it/s]Extractor Predicting: 105it [01:09,  1.61it/s]Extractor Predicting: 106it [01:09,  1.52it/s]Extractor Predicting: 107it [01:10,  1.56it/s]Extractor Predicting: 108it [01:10,  1.62it/s]Extractor Predicting: 109it [01:11,  1.66it/s]Extractor Predicting: 110it [01:12,  1.70it/s]Extractor Predicting: 111it [01:12,  1.66it/s]Extractor Predicting: 112it [01:13,  1.68it/s]Extractor Predicting: 113it [01:13,  1.68it/s]Extractor Predicting: 114it [01:14,  1.67it/s]Extractor Predicting: 115it [01:15,  1.68it/s]Extractor Predicting: 116it [01:15,  1.63it/s]Extractor Predicting: 117it [01:16,  1.68it/s]Extractor Predicting: 118it [01:16,  1.68it/s]Extractor Predicting: 119it [01:17,  1.70it/s]Extractor Predicting: 120it [01:18,  1.66it/s]Extractor Predicting: 121it [01:18,  1.68it/s]Extractor Predicting: 122it [01:19,  1.73it/s]Extractor Predicting: 123it [01:19,  1.74it/s]Extractor Predicting: 124it [01:20,  1.73it/s]Extractor Predicting: 125it [01:21,  1.62it/s]Extractor Predicting: 126it [01:21,  1.61it/s]Extractor Predicting: 127it [01:22,  1.63it/s]Extractor Predicting: 128it [01:22,  1.67it/s]Extractor Predicting: 129it [01:23,  1.72it/s]Extractor Predicting: 130it [01:24,  1.61it/s]Extractor Predicting: 131it [01:24,  1.68it/s]Extractor Predicting: 132it [01:25,  1.71it/s]Extractor Predicting: 133it [01:25,  1.70it/s]Extractor Predicting: 134it [01:26,  1.68it/s]Extractor Predicting: 135it [01:27,  1.60it/s]Extractor Predicting: 136it [01:27,  1.63it/s]Extractor Predicting: 137it [01:28,  1.67it/s]Extractor Predicting: 138it [01:28,  1.54it/s]Extractor Predicting: 139it [01:29,  1.57it/s]Extractor Predicting: 140it [01:30,  1.54it/s]Extractor Predicting: 141it [01:30,  1.61it/s]Extractor Predicting: 142it [01:31,  1.61it/s]Extractor Predicting: 143it [01:32,  1.64it/s]Extractor Predicting: 144it [01:32,  1.67it/s]Extractor Predicting: 145it [01:32,  2.00it/s]Extractor Predicting: 145it [01:32,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:10,259 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:10,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:10,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:10,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:10,346 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:52:11,543 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:52:11,544 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:52:12,270 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:52:13,488 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:52:13,554 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:16,981 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:17,096 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:17,096 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:17,096 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:52:17,096 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:52:18,168 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:52:18,169 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:52:18,905 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:52:19,244 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:52:19,244 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 1it [00:00,  1.52it/s]
[INFO|configuration_utils.py:515] 2023-08-28 04:52:24,123 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:52:24,124 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 04:52:24,250 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:52:24,251 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 04:52:24,309 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 04:52:53,369 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 04:52:53,498 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 04:52:54,041 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 04:52:54,042 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 04:52:54,373 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:52:54,577 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:52:54,577 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:52:54,577 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:52:54,577 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:52:54,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 04:52:54,578 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 04:52:55,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:56,041 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:56,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:57,435 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:58,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:58,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:52:59,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:00,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:01,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:01,796 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:02,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:03,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:03,833 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:04,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:05,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:05,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:06,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:07,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:08,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:08,931 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:09,681 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:10,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:11,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:11,830 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:17<02:34, 17.20s/it][WARNING|generation_utils.py:914] 2023-08-28 04:53:12,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:13,119 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:13,859 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:14,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:15,178 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:15,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:16,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:17,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:17,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:18,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:19,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:19,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:20,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:21,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:22,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:22,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:23,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:24,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:24,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:25,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:26,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:27,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:27,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:32<02:10, 16.36s/it][WARNING|generation_utils.py:914] 2023-08-28 04:53:28,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:28,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:29,738 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:30,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:31,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:31,839 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:32,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:33,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:34,029 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:34,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:35,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:36,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:36,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:37,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:38,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:39,063 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:39,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:40,632 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:41,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:42,042 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:42,985 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:43,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:44,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:45,208 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:45,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:46,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:52<02:03, 17.67s/it][WARNING|generation_utils.py:914] 2023-08-28 04:53:47,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:48,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:49,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:49,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:50,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:51,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:51,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:52,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:53,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:54,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:54,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:55,485 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:56,336 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:57,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:57,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:58,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:59,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:53:59,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:00,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:01,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:01,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:02,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:03,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:03,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:09<01:44, 17.37s/it][WARNING|generation_utils.py:914] 2023-08-28 04:54:04,493 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:05,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:05,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:06,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:07,451 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:08,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:08,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:09,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:10,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:10,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:11,764 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:12,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:13,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:13,883 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:14,674 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:15,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:16,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:16,954 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:17,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:18,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:18,914 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:19,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:20,353 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:20,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:26<01:26, 17.32s/it][WARNING|generation_utils.py:914] 2023-08-28 04:54:21,715 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:22,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:23,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:23,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:24,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:24,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:25,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:26,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:26,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:27,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:27,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:28,591 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:29,205 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:29,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:30,520 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:31,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:31,752 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:32,399 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:32,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:33,529 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:34,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:34,767 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:35,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:36,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:41<01:06, 16.55s/it][WARNING|generation_utils.py:914] 2023-08-28 04:54:36,656 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:37,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:37,917 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:38,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:39,209 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:39,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:40,560 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:41,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:42,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:42,690 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:43,248 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:43,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:44,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:45,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:45,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:46,438 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:47,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:47,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:48,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:48,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:49,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:50,097 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:55<00:47, 15.72s/it][WARNING|generation_utils.py:914] 2023-08-28 04:54:50,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:51,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:52,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:52,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:53,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:54,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:54,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:55,643 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:56,446 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:57,050 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:58,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:58,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:54:59,408 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:00,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:00,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:01,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:02,475 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:03,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:03,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:04,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:05,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:06,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:06,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:07,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:12<00:32, 16.27s/it][WARNING|generation_utils.py:914] 2023-08-28 04:55:08,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:08,783 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:09,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:10,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:10,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:11,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:12,192 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:13,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:13,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:14,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:15,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:15,595 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:16,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:16,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:17,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:18,319 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:19,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:19,724 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:20,609 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:21,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:21,864 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:22,551 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:23,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:28<00:16, 16.20s/it][WARNING|generation_utils.py:914] 2023-08-28 04:55:24,158 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:24,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:25,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:26,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:26,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:27,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:28,188 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:28,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:29,458 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:30,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:30,972 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:31,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:32,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:32,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:33,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:34,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:34,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:35,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:36,242 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:36,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:37,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:38,109 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:38,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:39,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:40,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:40,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:41,326 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:41,962 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:42,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:43,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:43,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:44,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 04:55:45,290 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:50<00:00, 17.95s/it]Generating: 100%|██████████| 10/10 [02:50<00:00, 17.09s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:55:58,634 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:55:58,753 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:55:58,753 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:55:58,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:55:58,754 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:56:00,188 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:56:00,189 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:56:00,900 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:56:02,203 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:56:02,270 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:05,860 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:05,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:05,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:05,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:56:05,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:56:06,975 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:56:06,977 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:56:07,683 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:56:08,006 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:56:08,006 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : director .', 'success_rate': 0.8138020833333334, 'errors': {'', "('\\n', 'director', 'Scott Zandt', 'It was written and directed by Scott Zandt ( a.k.a .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8505434782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 359, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 499, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7439903846153846, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : part of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'too many values to unpack (expected 2)', '(\'1958 contest\', \'part of\', \'\', \'He was succeeded as Dutch representative at its 1958 contest by Johannes Eichhorn with " Allende en seine " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a deal to turn " Ingress " into a mobile game . Head Entity : Ingress , Tail Entity : Ingress Studio .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.7981770833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8764204545454546, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n']
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n"]
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n", "Relation : member of political party . Context : This was the first coalition government which was elected in 1998 , and led by then - Prime Minister Naguib Sawiris of the People 's Alliance . Head Entity : Naguib Sawiris , Tail Entity : People ' Alliance .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Tom Blomkamp\', \'member of political party\', \'\', \'" My Life ( " ; ) is a 2015 English language English language documentary film directed by Tom Blomkamp and starring Emma Thompson and Tom Hardy .\')'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 58, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 96, 'raw': 160}
{'target': 600, 'success': 114, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 154, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 223, 'raw': 384}
{'target': 600, 'success': 245, 'raw': 416}
{'target': 600, 'success': 263, 'raw': 448}
{'target': 600, 'success': 276, 'raw': 480}
{'target': 600, 'success': 297, 'raw': 512}
{'target': 600, 'success': 318, 'raw': 544}
{'target': 600, 'success': 335, 'raw': 576}
{'target': 600, 'success': 359, 'raw': 608}
{'target': 600, 'success': 381, 'raw': 640}
{'target': 600, 'success': 398, 'raw': 672}
{'target': 600, 'success': 410, 'raw': 704}
{'target': 600, 'success': 431, 'raw': 736}
{'target': 600, 'success': 451, 'raw': 768}
{'target': 600, 'success': 470, 'raw': 800}
{'target': 600, 'success': 494, 'raw': 832}
{'target': 600, 'success': 516, 'raw': 864}
{'target': 600, 'success': 533, 'raw': 896}
{'target': 600, 'success': 552, 'raw': 928}
{'target': 600, 'success': 566, 'raw': 960}
{'target': 600, 'success': 584, 'raw': 992}
{'target': 600, 'success': 599, 'raw': 1024}
{'target': 600, 'success': 621, 'raw': 1056}
{'prompt': 'Relation : position held .', 'success_rate': 0.5880681818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 12256
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12356, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.25it/s]Extractor Estimating: 2it [00:01,  1.22it/s]Extractor Estimating: 3it [00:02,  1.31it/s]Extractor Estimating: 4it [00:03,  1.33it/s]Extractor Estimating: 5it [00:03,  1.36it/s]Extractor Estimating: 6it [00:04,  1.46it/s]Extractor Estimating: 7it [00:05,  1.42it/s]Extractor Estimating: 8it [00:05,  1.38it/s]Extractor Estimating: 9it [00:06,  1.41it/s]Extractor Estimating: 10it [00:07,  1.39it/s]Extractor Estimating: 11it [00:07,  1.44it/s]Extractor Estimating: 12it [00:08,  1.47it/s]Extractor Estimating: 13it [00:09,  1.48it/s]Extractor Estimating: 14it [00:10,  1.42it/s]Extractor Estimating: 15it [00:10,  1.42it/s]Extractor Estimating: 16it [00:11,  1.42it/s]Extractor Estimating: 17it [00:12,  1.42it/s]Extractor Estimating: 18it [00:12,  1.46it/s]Extractor Estimating: 19it [00:13,  1.43it/s]Extractor Estimating: 20it [00:14,  1.39it/s]Extractor Estimating: 21it [00:14,  1.39it/s]Extractor Estimating: 22it [00:15,  1.44it/s]Extractor Estimating: 23it [00:16,  1.46it/s]Extractor Estimating: 24it [00:17,  1.42it/s]Extractor Estimating: 25it [00:17,  1.31it/s]Extractor Estimating: 26it [00:18,  1.39it/s]Extractor Estimating: 27it [00:19,  1.44it/s]Extractor Estimating: 28it [00:19,  1.53it/s]Extractor Estimating: 29it [00:20,  1.54it/s]Extractor Estimating: 30it [00:21,  1.51it/s]Extractor Estimating: 31it [00:21,  1.55it/s]Extractor Estimating: 32it [00:22,  1.57it/s]Extractor Estimating: 33it [00:22,  1.56it/s]Extractor Estimating: 34it [00:23,  1.63it/s]Extractor Estimating: 35it [00:24,  1.60it/s]Extractor Estimating: 36it [00:24,  1.60it/s]Extractor Estimating: 37it [00:25,  1.58it/s]Extractor Estimating: 38it [00:26,  1.58it/s]Extractor Estimating: 39it [00:26,  1.55it/s]Extractor Estimating: 40it [00:27,  1.55it/s]Extractor Estimating: 41it [00:28,  1.53it/s]Extractor Estimating: 42it [00:28,  1.56it/s]Extractor Estimating: 43it [00:29,  1.59it/s]Extractor Estimating: 44it [00:29,  1.61it/s]Extractor Estimating: 45it [00:30,  1.55it/s]Extractor Estimating: 46it [00:31,  1.62it/s]Extractor Estimating: 47it [00:31,  1.59it/s]Extractor Estimating: 48it [00:32,  1.63it/s]Extractor Estimating: 49it [00:32,  1.67it/s]Extractor Estimating: 50it [00:33,  1.52it/s]Extractor Estimating: 51it [00:34,  1.49it/s]Extractor Estimating: 52it [00:35,  1.51it/s]Extractor Estimating: 53it [00:35,  1.55it/s]Extractor Estimating: 54it [00:36,  1.59it/s]Extractor Estimating: 55it [00:36,  1.51it/s]Extractor Estimating: 56it [00:37,  1.52it/s]Extractor Estimating: 57it [00:38,  1.54it/s]Extractor Estimating: 58it [00:38,  1.61it/s]Extractor Estimating: 59it [00:39,  1.57it/s]Extractor Estimating: 60it [00:40,  1.54it/s]Extractor Estimating: 61it [00:41,  1.41it/s]Extractor Estimating: 62it [00:41,  1.45it/s]Extractor Estimating: 63it [00:42,  1.50it/s]Extractor Estimating: 64it [00:42,  1.56it/s]Extractor Estimating: 65it [00:43,  1.45it/s]Extractor Estimating: 66it [00:44,  1.45it/s]Extractor Estimating: 67it [00:45,  1.48it/s]Extractor Estimating: 68it [00:45,  1.48it/s]Extractor Estimating: 69it [00:46,  1.50it/s]Extractor Estimating: 70it [00:47,  1.46it/s]Extractor Estimating: 71it [00:47,  1.50it/s]Extractor Estimating: 72it [00:48,  1.49it/s]Extractor Estimating: 73it [00:49,  1.48it/s]Extractor Estimating: 74it [00:49,  1.54it/s]Extractor Estimating: 75it [00:50,  1.44it/s]Extractor Estimating: 76it [00:51,  1.45it/s]Extractor Estimating: 77it [00:51,  1.40it/s]Extractor Estimating: 78it [00:52,  1.43it/s]Extractor Estimating: 79it [00:53,  1.45it/s]Extractor Estimating: 80it [00:53,  1.52it/s]Extractor Estimating: 81it [00:54,  1.48it/s]Extractor Estimating: 82it [00:55,  1.46it/s]Extractor Estimating: 83it [00:55,  1.45it/s]Extractor Estimating: 84it [00:56,  1.47it/s]Extractor Estimating: 85it [00:57,  1.45it/s]Extractor Estimating: 86it [00:57,  1.49it/s]Extractor Estimating: 87it [00:58,  1.49it/s]Extractor Estimating: 88it [00:59,  1.43it/s]Extractor Estimating: 89it [01:00,  1.45it/s]Extractor Estimating: 90it [01:00,  1.47it/s]Extractor Estimating: 91it [01:01,  1.47it/s]Extractor Estimating: 92it [01:02,  1.45it/s]Extractor Estimating: 93it [01:02,  1.49it/s]Extractor Estimating: 94it [01:03,  1.49it/s]Extractor Estimating: 95it [01:03,  1.57it/s]Extractor Estimating: 96it [01:04,  1.51it/s]Extractor Estimating: 97it [01:05,  1.44it/s]Extractor Estimating: 98it [01:06,  1.46it/s]Extractor Estimating: 99it [01:06,  1.49it/s]Extractor Estimating: 100it [01:07,  1.52it/s]Extractor Estimating: 101it [01:07,  1.55it/s]Extractor Estimating: 102it [01:08,  1.46it/s]Extractor Estimating: 103it [01:09,  1.47it/s]Extractor Estimating: 104it [01:10,  1.48it/s]Extractor Estimating: 105it [01:10,  1.52it/s]Extractor Estimating: 106it [01:11,  1.53it/s]Extractor Estimating: 107it [01:12,  1.44it/s]Extractor Estimating: 108it [01:12,  1.45it/s]Extractor Estimating: 109it [01:13,  1.48it/s]Extractor Estimating: 110it [01:14,  1.53it/s]Extractor Estimating: 111it [01:14,  1.53it/s]Extractor Estimating: 112it [01:15,  1.44it/s]Extractor Estimating: 113it [01:16,  1.48it/s]Extractor Estimating: 114it [01:16,  1.49it/s]Extractor Estimating: 115it [01:17,  1.54it/s]Extractor Estimating: 116it [01:18,  1.53it/s]Extractor Estimating: 117it [01:18,  1.45it/s]Extractor Estimating: 118it [01:19,  1.40it/s]Extractor Estimating: 119it [01:20,  1.37it/s]Extractor Estimating: 120it [01:20,  1.44it/s]Extractor Estimating: 121it [01:21,  1.46it/s]Extractor Estimating: 122it [01:22,  1.44it/s]Extractor Estimating: 123it [01:22,  1.48it/s]Extractor Estimating: 124it [01:23,  1.50it/s]Extractor Estimating: 125it [01:24,  1.55it/s]Extractor Estimating: 126it [01:24,  1.51it/s]Extractor Estimating: 127it [01:25,  1.43it/s]Extractor Estimating: 128it [01:26,  1.45it/s]Extractor Estimating: 129it [01:26,  1.51it/s]Extractor Estimating: 130it [01:27,  1.58it/s]Extractor Estimating: 131it [01:28,  1.62it/s]Extractor Estimating: 132it [01:28,  1.50it/s]Extractor Estimating: 133it [01:29,  1.52it/s]Extractor Estimating: 134it [01:30,  1.49it/s]Extractor Estimating: 135it [01:30,  1.53it/s]Extractor Estimating: 136it [01:31,  1.54it/s]Extractor Estimating: 137it [01:32,  1.48it/s]Extractor Estimating: 138it [01:32,  1.50it/s]Extractor Estimating: 139it [01:33,  1.53it/s]Extractor Estimating: 140it [01:34,  1.51it/s]Extractor Estimating: 141it [01:34,  1.57it/s]Extractor Estimating: 142it [01:35,  1.46it/s]Extractor Estimating: 143it [01:36,  1.48it/s]Extractor Estimating: 144it [01:36,  1.52it/s]Extractor Estimating: 145it [01:37,  1.56it/s]Extractor Estimating: 146it [01:38,  1.54it/s]Extractor Estimating: 147it [01:38,  1.51it/s]Extractor Estimating: 148it [01:39,  1.50it/s]Extractor Estimating: 149it [01:40,  1.54it/s]Extractor Estimating: 150it [01:40,  1.56it/s]Extractor Estimating: 151it [01:41,  1.62it/s]Extractor Estimating: 152it [01:41,  1.60it/s]Extractor Estimating: 153it [01:42,  1.66it/s]Extractor Estimating: 154it [01:42,  1.71it/s]Extractor Estimating: 155it [01:43,  1.74it/s]Extractor Estimating: 156it [01:44,  1.77it/s]Extractor Estimating: 157it [01:44,  1.77it/s]Extractor Estimating: 158it [01:45,  1.66it/s]Extractor Estimating: 159it [01:45,  1.72it/s]Extractor Estimating: 160it [01:46,  1.75it/s]Extractor Estimating: 161it [01:46,  1.82it/s]Extractor Estimating: 162it [01:47,  1.89it/s]Extractor Estimating: 163it [01:47,  1.83it/s]Extractor Estimating: 164it [01:48,  1.72it/s]Extractor Estimating: 165it [01:49,  1.80it/s]Extractor Estimating: 166it [01:49,  1.78it/s]Extractor Estimating: 167it [01:50,  1.86it/s]Extractor Estimating: 168it [01:50,  1.83it/s]Extractor Estimating: 169it [01:51,  1.85it/s]Extractor Estimating: 170it [01:51,  1.80it/s]Extractor Estimating: 171it [01:52,  1.79it/s]Extractor Estimating: 172it [01:53,  1.75it/s]Extractor Estimating: 173it [01:53,  1.79it/s]Extractor Estimating: 174it [01:54,  1.82it/s]Extractor Estimating: 175it [01:54,  1.83it/s]Extractor Estimating: 176it [01:55,  1.71it/s]Extractor Estimating: 177it [01:55,  1.67it/s]Extractor Estimating: 178it [01:56,  1.60it/s]Extractor Estimating: 179it [01:57,  1.56it/s]Extractor Estimating: 180it [01:57,  1.61it/s]Extractor Estimating: 181it [01:58,  1.63it/s]Extractor Estimating: 182it [01:59,  1.66it/s]Extractor Estimating: 183it [01:59,  1.53it/s]Extractor Estimating: 184it [02:00,  1.58it/s]Extractor Estimating: 185it [02:01,  1.58it/s]Extractor Estimating: 186it [02:01,  1.51it/s]Extractor Estimating: 187it [02:02,  1.55it/s]Extractor Estimating: 188it [02:03,  1.58it/s]Extractor Estimating: 189it [02:03,  1.53it/s]Extractor Estimating: 190it [02:04,  1.54it/s]Extractor Estimating: 191it [02:05,  1.53it/s]Extractor Estimating: 192it [02:05,  1.41it/s]Extractor Estimating: 193it [02:06,  1.47it/s]Extractor Estimating: 194it [02:07,  1.49it/s]Extractor Estimating: 195it [02:07,  1.55it/s]Extractor Estimating: 196it [02:08,  1.60it/s]Extractor Estimating: 197it [02:08,  1.64it/s]Extractor Estimating: 198it [02:09,  1.57it/s]Extractor Estimating: 199it [02:10,  1.57it/s]Extractor Estimating: 200it [02:10,  1.58it/s]Extractor Estimating: 201it [02:11,  1.59it/s]Extractor Estimating: 202it [02:12,  1.59it/s]Extractor Estimating: 203it [02:12,  1.57it/s]Extractor Estimating: 204it [02:13,  1.52it/s]Extractor Estimating: 205it [02:14,  1.51it/s]Extractor Estimating: 206it [02:14,  1.52it/s]Extractor Estimating: 207it [02:15,  1.53it/s]Extractor Estimating: 208it [02:16,  1.51it/s]Extractor Estimating: 209it [02:16,  1.47it/s]Extractor Estimating: 210it [02:17,  1.56it/s]Extractor Estimating: 211it [02:18,  1.51it/s]Extractor Estimating: 212it [02:18,  1.54it/s]Extractor Estimating: 213it [02:19,  1.55it/s]Extractor Estimating: 214it [02:20,  1.48it/s]Extractor Estimating: 215it [02:20,  1.54it/s]Extractor Estimating: 216it [02:21,  1.54it/s]Extractor Estimating: 217it [02:21,  1.57it/s]Extractor Estimating: 218it [02:22,  1.56it/s]Extractor Estimating: 219it [02:23,  1.52it/s]Extractor Estimating: 220it [02:23,  1.52it/s]Extractor Estimating: 221it [02:24,  1.54it/s]Extractor Estimating: 222it [02:25,  1.56it/s]Extractor Estimating: 223it [02:25,  1.59it/s]Extractor Estimating: 224it [02:26,  1.63it/s]Extractor Estimating: 225it [02:27,  1.55it/s]Extractor Estimating: 226it [02:27,  1.59it/s]Extractor Estimating: 227it [02:28,  1.48it/s]Extractor Estimating: 228it [02:29,  1.47it/s]Extractor Estimating: 229it [02:29,  1.48it/s]Extractor Estimating: 230it [02:30,  1.56it/s]Extractor Estimating: 231it [02:30,  1.55it/s]Extractor Estimating: 232it [02:31,  1.57it/s]Extractor Estimating: 233it [02:32,  1.57it/s]Extractor Estimating: 234it [02:32,  1.59it/s]Extractor Estimating: 235it [02:33,  1.62it/s]Extractor Estimating: 236it [02:34,  1.61it/s]Extractor Estimating: 237it [02:34,  1.61it/s]Extractor Estimating: 238it [02:35,  1.67it/s]Extractor Estimating: 239it [02:35,  1.68it/s]Extractor Estimating: 240it [02:36,  1.68it/s]Extractor Estimating: 241it [02:37,  1.64it/s]Extractor Estimating: 242it [02:37,  1.65it/s]Extractor Estimating: 243it [02:38,  1.61it/s]Extractor Estimating: 244it [02:38,  1.64it/s]Extractor Estimating: 245it [02:39,  1.61it/s]Extractor Estimating: 246it [02:40,  1.58it/s]Extractor Estimating: 247it [02:40,  1.52it/s]Extractor Estimating: 248it [02:41,  1.58it/s]Extractor Estimating: 249it [02:42,  1.61it/s]Extractor Estimating: 250it [02:42,  1.47it/s]Extractor Estimating: 250it [02:42,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:20,143 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:20,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:20,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:20,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:20,193 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 04:59:20,965 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 04:59:20,966 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:59:21,768 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 04:59:23,083 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:59:23,083 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:25,802 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:25,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:25,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:25,895 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 04:59:25,896 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 04:59:26,911 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 04:59:26,912 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 04:59:27,341 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 04:59:27,965 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 04:59:27,966 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 06:33:41,858 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 06:33:42,625 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 5238 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 21562
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21662, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21662, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.082, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.101, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 81, avg_time 1.090, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 181, avg_time 1.097, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 62, avg_time 1.102, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 162, avg_time 2.244, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 43, avg_time 1.061, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 143, avg_time 1.068, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 24, avg_time 1.105, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 124, avg_time 1.084, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 5, avg_time 2.145, loss:nan
g_step 1200, step 105, avg_time 1.102, loss:nan
g_step 1300, step 205, avg_time 1.079, loss:nan
g_step 1400, step 86, avg_time 1.082, loss:nan
g_step 1500, step 186, avg_time 1.079, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 67, avg_time 2.189, loss:nan
g_step 1700, step 167, avg_time 1.095, loss:nan
g_step 1800, step 48, avg_time 1.074, loss:nan
g_step 1900, step 148, avg_time 1.080, loss:nan
g_step 2000, step 29, avg_time 1.076, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 129, avg_time 2.159, loss:nan
g_step 2200, step 10, avg_time 1.094, loss:nan
g_step 2300, step 110, avg_time 1.080, loss:nan
g_step 2400, step 210, avg_time 1.096, loss:nan
g_step 2500, step 91, avg_time 1.069, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 191, avg_time 2.133, loss:nan
g_step 2700, step 72, avg_time 1.080, loss:nan
g_step 2800, step 172, avg_time 1.064, loss:nan
g_step 2900, step 53, avg_time 1.053, loss:nan
g_step 3000, step 153, avg_time 1.450, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 34, avg_time 2.228, loss:nan
g_step 3200, step 134, avg_time 1.073, loss:nan
g_step 3300, step 15, avg_time 1.059, loss:nan
g_step 3400, step 115, avg_time 1.076, loss:nan
g_step 3500, step 215, avg_time 1.067, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 96, avg_time 2.143, loss:nan
g_step 3700, step 196, avg_time 1.073, loss:nan
g_step 3800, step 77, avg_time 1.078, loss:nan
g_step 3900, step 177, avg_time 1.056, loss:nan
g_step 4000, step 58, avg_time 1.074, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 158, avg_time 2.147, loss:nan
g_step 4200, step 39, avg_time 1.061, loss:nan
g_step 4300, step 139, avg_time 1.082, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 06:33:42 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 06:33:42 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_06-33-41_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 06:33:47 - WARNING - datasets.builder -   Using custom data configuration default-b3398930fcb20b21
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-b3398930fcb20b21/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]1 tables [00:03,  3.58s/ tables]                                0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 06:33:59,826 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:33:59,894 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:33:59,895 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:33:59,896 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:34:00,157 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:34:00,294 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:34:00,294 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:34:00,294 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:34:00,294 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:34:00,294 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:34:00,294 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 06:34:01,303 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:34:04,999 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 06:34:05,028 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-b3398930fcb20b21/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.28ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.29ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.82ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.11ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.30ba/s]100%|██████████| 6/6 [00:01<00:00,  4.47ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.28ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.61ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.22ba/s]100%|██████████| 4/4 [00:01<00:00,  3.88ba/s]100%|██████████| 4/4 [00:01<00:00,  3.38ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  3.55ba/s] 33%|███▎      | 2/6 [00:00<00:00,  5.44ba/s] 67%|██████▋   | 4/6 [00:00<00:00,  7.88ba/s]100%|██████████| 6/6 [00:00<00:00, 10.66ba/s]100%|██████████| 6/6 [00:00<00:00,  8.61ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  4.32ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.95ba/s]100%|██████████| 4/4 [00:00<00:00,  8.81ba/s]
[INFO|trainer.py:414] 2023-08-28 06:34:20,813 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 06:34:21,000 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 06:34:21,000 >>   Num examples = 5239
[INFO|trainer.py:1149] 2023-08-28 06:34:21,000 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 06:34:21,000 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 06:34:21,000 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 06:34:21,000 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 06:34:21,000 >>   Total optimization steps = 410
  0%|          | 0/410 [00:00<?, ?it/s]  0%|          | 1/410 [00:01<12:10,  1.79s/it]  0%|          | 2/410 [00:02<06:13,  1.09it/s]  1%|          | 3/410 [00:02<04:15,  1.59it/s]  1%|          | 4/410 [00:02<03:18,  2.04it/s]  1%|          | 5/410 [00:02<02:47,  2.41it/s]  1%|▏         | 6/410 [00:03<02:28,  2.72it/s]  2%|▏         | 7/410 [00:03<02:16,  2.95it/s]  2%|▏         | 8/410 [00:03<02:10,  3.09it/s]  2%|▏         | 9/410 [00:04<02:04,  3.23it/s]  2%|▏         | 10/410 [00:04<01:59,  3.34it/s]  3%|▎         | 11/410 [00:04<01:59,  3.35it/s]  3%|▎         | 12/410 [00:04<01:56,  3.42it/s]  3%|▎         | 13/410 [00:05<01:54,  3.47it/s]  3%|▎         | 14/410 [00:05<01:52,  3.51it/s]  4%|▎         | 15/410 [00:05<01:51,  3.54it/s]  4%|▍         | 16/410 [00:06<01:50,  3.56it/s]  4%|▍         | 17/410 [00:06<01:49,  3.58it/s]  4%|▍         | 18/410 [00:06<01:49,  3.58it/s]  5%|▍         | 19/410 [00:06<01:48,  3.59it/s]  5%|▍         | 20/410 [00:07<01:48,  3.59it/s]  5%|▌         | 21/410 [00:07<01:48,  3.59it/s]  5%|▌         | 22/410 [00:07<01:53,  3.41it/s]  6%|▌         | 23/410 [00:08<01:51,  3.46it/s]  6%|▌         | 24/410 [00:08<01:50,  3.50it/s]  6%|▌         | 25/410 [00:08<01:48,  3.54it/s]  6%|▋         | 26/410 [00:08<01:48,  3.55it/s]  7%|▋         | 27/410 [00:09<01:47,  3.57it/s]  7%|▋         | 28/410 [00:09<01:46,  3.58it/s]  7%|▋         | 29/410 [00:09<01:46,  3.59it/s]  7%|▋         | 30/410 [00:09<01:45,  3.60it/s]  8%|▊         | 31/410 [00:10<01:45,  3.60it/s]  8%|▊         | 32/410 [00:10<01:45,  3.60it/s]  8%|▊         | 33/410 [00:10<01:49,  3.45it/s]  8%|▊         | 34/410 [00:11<01:47,  3.50it/s]  9%|▊         | 35/410 [00:11<01:46,  3.53it/s]  9%|▉         | 36/410 [00:11<01:45,  3.55it/s]  9%|▉         | 37/410 [00:11<01:44,  3.57it/s]  9%|▉         | 38/410 [00:12<01:43,  3.58it/s] 10%|▉         | 39/410 [00:12<01:43,  3.58it/s] 10%|▉         | 40/410 [00:12<01:43,  3.59it/s] 10%|█         | 41/410 [00:13<01:42,  3.59it/s] 10%|█         | 42/410 [00:13<01:42,  3.59it/s] 10%|█         | 43/410 [00:13<01:42,  3.59it/s] 11%|█         | 44/410 [00:13<01:51,  3.29it/s] 11%|█         | 45/410 [00:14<01:48,  3.37it/s] 11%|█         | 46/410 [00:14<01:45,  3.44it/s] 11%|█▏        | 47/410 [00:14<01:44,  3.48it/s] 12%|█▏        | 48/410 [00:15<01:43,  3.51it/s] 12%|█▏        | 49/410 [00:15<01:41,  3.54it/s] 12%|█▏        | 50/410 [00:15<01:41,  3.55it/s] 12%|█▏        | 51/410 [00:15<01:40,  3.57it/s] 13%|█▎        | 52/410 [00:16<01:40,  3.58it/s] 13%|█▎        | 53/410 [00:16<01:39,  3.58it/s] 13%|█▎        | 54/410 [00:16<01:39,  3.58it/s] 13%|█▎        | 55/410 [00:17<01:53,  3.13it/s] 14%|█▎        | 56/410 [00:17<02:00,  2.94it/s] 14%|█▍        | 57/410 [00:17<01:53,  3.10it/s] 14%|█▍        | 58/410 [00:18<01:48,  3.24it/s] 14%|█▍        | 59/410 [00:18<01:45,  3.34it/s] 15%|█▍        | 60/410 [00:18<01:42,  3.41it/s] 15%|█▍        | 61/410 [00:18<01:40,  3.46it/s] 15%|█▌        | 62/410 [00:19<01:39,  3.50it/s] 15%|█▌        | 63/410 [00:19<01:38,  3.53it/s] 16%|█▌        | 64/410 [00:19<01:37,  3.54it/s] 16%|█▌        | 65/410 [00:20<01:48,  3.18it/s] 16%|█▌        | 66/410 [00:20<01:44,  3.30it/s] 16%|█▋        | 67/410 [00:20<01:40,  3.40it/s] 17%|█▋        | 68/410 [00:21<03:07,  1.83it/s] 17%|█▋        | 69/410 [00:22<02:39,  2.14it/s] 17%|█▋        | 70/410 [00:22<02:19,  2.44it/s] 17%|█▋        | 71/410 [00:22<02:04,  2.71it/s] 18%|█▊        | 72/410 [00:22<01:54,  2.94it/s] 18%|█▊        | 73/410 [00:23<01:58,  2.83it/s] 18%|█▊        | 74/410 [00:23<01:50,  3.04it/s] 18%|█▊        | 75/410 [00:23<01:44,  3.20it/s] 19%|█▊        | 76/410 [00:24<01:40,  3.33it/s] 19%|█▉        | 77/410 [00:24<01:37,  3.42it/s] 19%|█▉        | 78/410 [00:24<01:35,  3.49it/s] 19%|█▉        | 79/410 [00:24<01:33,  3.54it/s] 20%|█▉        | 80/410 [00:25<01:32,  3.57it/s] 20%|█▉        | 81/410 [00:25<01:31,  3.59it/s] 20%|██        | 82/410 [00:25<01:27,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 06:34:46,780 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:34:46,780 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 06:34:46,780 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.92it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.61it/s][A
  4%|▍         | 18/438 [00:00<00:11, 37.96it/s][A
  5%|▌         | 23/438 [00:00<00:10, 40.38it/s][A
  6%|▋         | 28/438 [00:00<00:09, 42.05it/s][A
  8%|▊         | 33/438 [00:00<00:09, 43.16it/s][A
  9%|▊         | 38/438 [00:00<00:09, 43.95it/s][A
 10%|▉         | 43/438 [00:00<00:08, 44.38it/s][A
 11%|█         | 48/438 [00:01<00:08, 44.67it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 44.72it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 44.50it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 44.35it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 44.63it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 44.83it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.03it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.24it/s][A
 20%|██        | 88/438 [00:01<00:07, 45.25it/s][A
 21%|██        | 93/438 [00:02<00:07, 45.32it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.06it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 44.53it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 44.54it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.73it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.90it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 45.08it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 45.18it/s][A
 30%|███       | 133/438 [00:02<00:06, 45.39it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 45.38it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 45.18it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.82it/s][A
 35%|███▍      | 153/438 [00:03<00:07, 37.79it/s][A
 36%|███▌      | 158/438 [00:03<00:07, 39.90it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 41.41it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 42.61it/s][A
 39%|███▉      | 173/438 [00:03<00:06, 43.35it/s][A
 41%|████      | 178/438 [00:04<00:05, 44.02it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 44.41it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 44.64it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 44.40it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 44.25it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 44.51it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 44.75it/s][A
 49%|████▊     | 213/438 [00:04<00:05, 44.99it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.09it/s][A
 51%|█████     | 223/438 [00:05<00:04, 45.26it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 45.30it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.25it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 44.91it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 44.71it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 44.72it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.87it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 45.13it/s][A
 60%|██████    | 263/438 [00:05<00:03, 45.27it/s][A
 61%|██████    | 268/438 [00:06<00:03, 45.28it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 45.25it/s][A
 63%|██████▎   | 278/438 [00:06<00:04, 32.44it/s][A
 65%|██████▍   | 283/438 [00:06<00:04, 35.31it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 37.93it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 39.96it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 41.44it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 42.57it/s][A
 70%|███████   | 308/438 [00:07<00:02, 43.38it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 43.97it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 43.87it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 43.99it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 44.08it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 44.45it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 44.78it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 45.04it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 45.23it/s][A
 81%|████████  | 353/438 [00:08<00:01, 45.23it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 45.14it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 44.81it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 44.64it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.50it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 44.80it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 45.02it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 45.20it/s][A
 90%|████████▉ | 393/438 [00:08<00:00, 45.34it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 45.22it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 45.04it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 39.23it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 40.93it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 42.14it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 43.10it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 43.88it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 44.33it/s][A
100%|██████████| 438/438 [00:10<00:00, 44.72it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:10<00:00, 44.72it/s][A 20%|██        | 82/410 [00:35<01:27,  3.76it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:34:57,694 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82
[INFO|configuration_utils.py:351] 2023-08-28 06:34:58,103 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:35:22,824 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:35:23,174 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:35:23,349 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82/special_tokens_map.json
 20%|██        | 83/410 [01:05<1:06:08, 12.14s/it] 20%|██        | 84/410 [01:05<46:36,  8.58s/it]   21%|██        | 85/410 [01:06<33:06,  6.11s/it] 21%|██        | 86/410 [01:06<23:33,  4.36s/it] 21%|██        | 87/410 [01:06<16:53,  3.14s/it] 21%|██▏       | 88/410 [01:07<12:13,  2.28s/it] 22%|██▏       | 89/410 [01:07<08:58,  1.68s/it] 22%|██▏       | 90/410 [01:07<06:42,  1.26s/it] 22%|██▏       | 91/410 [01:07<05:07,  1.04it/s] 22%|██▏       | 92/410 [01:08<04:01,  1.32it/s] 23%|██▎       | 93/410 [01:08<03:14,  1.63it/s] 23%|██▎       | 94/410 [01:08<02:41,  1.95it/s] 23%|██▎       | 95/410 [01:09<02:18,  2.27it/s] 23%|██▎       | 96/410 [01:09<02:11,  2.39it/s] 24%|██▎       | 97/410 [01:09<01:57,  2.66it/s] 24%|██▍       | 98/410 [01:09<01:48,  2.88it/s] 24%|██▍       | 99/410 [01:10<01:41,  3.06it/s] 24%|██▍       | 100/410 [01:10<01:36,  3.21it/s] 25%|██▍       | 101/410 [01:10<01:33,  3.31it/s] 25%|██▍       | 102/410 [01:11<01:30,  3.39it/s] 25%|██▌       | 103/410 [01:11<01:28,  3.46it/s] 25%|██▌       | 104/410 [01:11<01:27,  3.50it/s] 26%|██▌       | 105/410 [01:11<01:26,  3.53it/s] 26%|██▌       | 106/410 [01:12<01:25,  3.55it/s] 26%|██▌       | 107/410 [01:12<01:34,  3.21it/s] 26%|██▋       | 108/410 [01:12<01:31,  3.32it/s] 27%|██▋       | 109/410 [01:13<01:28,  3.40it/s] 27%|██▋       | 110/410 [01:13<01:26,  3.46it/s] 27%|██▋       | 111/410 [01:13<01:25,  3.49it/s] 27%|██▋       | 112/410 [01:13<01:24,  3.52it/s] 28%|██▊       | 113/410 [01:14<01:23,  3.54it/s] 28%|██▊       | 114/410 [01:14<01:23,  3.56it/s] 28%|██▊       | 115/410 [01:14<01:22,  3.57it/s] 28%|██▊       | 116/410 [01:15<01:22,  3.58it/s] 29%|██▊       | 117/410 [01:15<01:21,  3.58it/s] 29%|██▉       | 118/410 [01:15<01:30,  3.22it/s] 29%|██▉       | 119/410 [01:15<01:27,  3.32it/s] 29%|██▉       | 120/410 [01:16<01:25,  3.40it/s] 30%|██▉       | 121/410 [01:16<01:23,  3.46it/s] 30%|██▉       | 122/410 [01:16<01:22,  3.50it/s] 30%|███       | 123/410 [01:17<01:21,  3.53it/s] 30%|███       | 124/410 [01:17<01:20,  3.55it/s] 30%|███       | 125/410 [01:17<01:26,  3.29it/s] 31%|███       | 126/410 [01:18<01:24,  3.37it/s] 31%|███       | 127/410 [01:18<01:22,  3.42it/s] 31%|███       | 128/410 [01:18<01:21,  3.47it/s] 31%|███▏      | 129/410 [01:18<01:27,  3.22it/s] 32%|███▏      | 130/410 [01:19<01:24,  3.32it/s] 32%|███▏      | 131/410 [01:19<01:22,  3.40it/s] 32%|███▏      | 132/410 [01:19<01:20,  3.45it/s] 32%|███▏      | 133/410 [01:20<01:19,  3.50it/s] 33%|███▎      | 134/410 [01:20<01:18,  3.53it/s] 33%|███▎      | 135/410 [01:20<01:17,  3.57it/s] 33%|███▎      | 136/410 [01:20<01:16,  3.60it/s] 33%|███▎      | 137/410 [01:22<02:34,  1.77it/s] 34%|███▎      | 138/410 [01:22<02:15,  2.00it/s] 34%|███▍      | 139/410 [01:22<01:56,  2.32it/s] 34%|███▍      | 140/410 [01:22<01:43,  2.60it/s] 34%|███▍      | 141/410 [01:23<01:34,  2.85it/s] 35%|███▍      | 142/410 [01:23<01:27,  3.05it/s] 35%|███▍      | 143/410 [01:23<01:23,  3.21it/s] 35%|███▌      | 144/410 [01:24<01:19,  3.33it/s] 35%|███▌      | 145/410 [01:24<01:17,  3.42it/s] 36%|███▌      | 146/410 [01:24<01:15,  3.49it/s] 36%|███▌      | 147/410 [01:24<01:14,  3.54it/s] 36%|███▌      | 148/410 [01:25<01:13,  3.57it/s] 36%|███▋      | 149/410 [01:25<01:21,  3.20it/s] 37%|███▋      | 150/410 [01:25<01:18,  3.32it/s] 37%|███▋      | 151/410 [01:26<01:15,  3.41it/s] 37%|███▋      | 152/410 [01:26<01:14,  3.48it/s] 37%|███▋      | 153/410 [01:26<01:12,  3.53it/s] 38%|███▊      | 154/410 [01:26<01:11,  3.56it/s] 38%|███▊      | 155/410 [01:27<01:11,  3.59it/s] 38%|███▊      | 156/410 [01:27<01:10,  3.61it/s] 38%|███▊      | 157/410 [01:27<01:09,  3.62it/s] 39%|███▊      | 158/410 [01:28<01:09,  3.63it/s] 39%|███▉      | 159/410 [01:28<01:08,  3.64it/s] 39%|███▉      | 160/410 [01:28<01:15,  3.32it/s] 39%|███▉      | 161/410 [01:28<01:12,  3.41it/s] 40%|███▉      | 162/410 [01:29<01:11,  3.48it/s] 40%|███▉      | 163/410 [01:29<01:09,  3.53it/s] 40%|████      | 164/410 [01:29<01:06,  3.71it/s][INFO|trainer.py:2140] 2023-08-28 06:35:50,739 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:35:50,740 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 06:35:50,740 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.0288, 'eval_samples_per_second': 348.696, 'eval_steps_per_second': 43.674, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.81it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.48it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.24it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.27it/s][A
  6%|▋         | 28/438 [00:00<00:08, 45.79it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.37it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.23it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.28it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.26it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.43it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.56it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.47it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.29it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.18it/s][A
 18%|█▊        | 78/438 [00:01<00:10, 34.18it/s][A
 19%|█▉        | 83/438 [00:01<00:09, 37.06it/s][A
 20%|██        | 88/438 [00:02<00:08, 39.25it/s][A
 21%|██        | 93/438 [00:02<00:08, 41.01it/s][A
 22%|██▏       | 98/438 [00:02<00:08, 42.30it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 43.31it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 44.03it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.44it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.26it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.26it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.39it/s][A
 30%|███       | 133/438 [00:03<00:06, 44.71it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.83it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 45.11it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 45.29it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.44it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 45.44it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 45.08it/s][A
 38%|███▊      | 168/438 [00:03<00:07, 37.35it/s][A
 39%|███▉      | 173/438 [00:03<00:06, 39.58it/s][A
 41%|████      | 178/438 [00:04<00:06, 41.24it/s][A
 42%|████▏     | 183/438 [00:04<00:06, 42.46it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 43.39it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 44.03it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 44.47it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 44.76it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 44.52it/s][A
 49%|████▊     | 213/438 [00:04<00:05, 44.39it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 44.48it/s][A
 51%|█████     | 223/438 [00:05<00:04, 44.72it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 44.98it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.14it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 45.34it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 45.46it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 45.42it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 45.13it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.79it/s][A
 60%|██████    | 263/438 [00:05<00:03, 44.76it/s][A
 61%|██████    | 268/438 [00:06<00:03, 44.82it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 45.07it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 45.19it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 45.30it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 45.33it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 45.29it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 37.91it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 39.98it/s][A
 70%|███████   | 308/438 [00:07<00:03, 41.61it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 42.76it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 43.63it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 44.29it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 44.71it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 44.81it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 44.42it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 44.42it/s][A
 79%|███████▉  | 348/438 [00:07<00:02, 44.46it/s][A
 81%|████████  | 353/438 [00:08<00:01, 44.63it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 44.86it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 45.10it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 45.33it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 45.55it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 45.41it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 45.06it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 44.61it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 44.68it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 44.77it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.86it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 45.13it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 45.24it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 45.35it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 45.34it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 45.21it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 39.33it/s][A
100%|██████████| 438/438 [00:09<00:00, 41.00it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 41.00it/s][A 40%|████      | 164/410 [01:39<01:06,  3.71it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:36:01,398 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164
[INFO|configuration_utils.py:351] 2023-08-28 06:36:01,781 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:36:26,762 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:36:27,164 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:36:27,344 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164/special_tokens_map.json
 40%|████      | 165/410 [02:09<49:44, 12.18s/it] 40%|████      | 166/410 [02:10<35:06,  8.63s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 41%|████      | 167/410 [02:10<24:48,  6.13s/it] 41%|████      | 168/410 [02:10<17:37,  4.37s/it] 41%|████      | 169/410 [02:10<12:37,  3.14s/it] 41%|████▏     | 170/410 [02:11<09:08,  2.28s/it] 42%|████▏     | 171/410 [02:11<06:42,  1.68s/it] 42%|████▏     | 172/410 [02:11<05:00,  1.26s/it] 42%|████▏     | 173/410 [02:12<03:48,  1.04it/s] 42%|████▏     | 174/410 [02:12<02:59,  1.32it/s] 43%|████▎     | 175/410 [02:12<02:24,  1.63it/s] 43%|████▎     | 176/410 [02:12<02:00,  1.95it/s] 43%|████▎     | 177/410 [02:13<01:53,  2.06it/s] 43%|████▎     | 178/410 [02:13<01:38,  2.36it/s] 44%|████▎     | 179/410 [02:13<01:27,  2.64it/s] 44%|████▍     | 180/410 [02:14<01:20,  2.87it/s] 44%|████▍     | 181/410 [02:14<01:14,  3.06it/s] 44%|████▍     | 182/410 [02:14<01:10,  3.22it/s] 45%|████▍     | 183/410 [02:14<01:08,  3.34it/s] 45%|████▍     | 184/410 [02:15<01:06,  3.42it/s] 45%|████▌     | 185/410 [02:15<01:04,  3.49it/s] 45%|████▌     | 186/410 [02:15<01:03,  3.54it/s] 46%|████▌     | 187/410 [02:16<01:02,  3.57it/s] 46%|████▌     | 188/410 [02:16<01:06,  3.35it/s] 46%|████▌     | 189/410 [02:16<01:04,  3.43it/s] 46%|████▋     | 190/410 [02:16<01:02,  3.50it/s] 47%|████▋     | 191/410 [02:17<01:01,  3.55it/s] 47%|████▋     | 192/410 [02:17<01:00,  3.58it/s] 47%|████▋     | 193/410 [02:17<01:05,  3.29it/s] 47%|████▋     | 194/410 [02:18<01:04,  3.37it/s] 48%|████▊     | 195/410 [02:18<01:02,  3.45it/s] 48%|████▊     | 196/410 [02:18<01:00,  3.51it/s] 48%|████▊     | 197/410 [02:18<00:59,  3.55it/s] 48%|████▊     | 198/410 [02:19<00:59,  3.59it/s] 49%|████▊     | 199/410 [02:19<01:04,  3.28it/s] 49%|████▉     | 200/410 [02:19<01:02,  3.38it/s] 49%|████▉     | 201/410 [02:20<01:00,  3.46it/s] 49%|████▉     | 202/410 [02:20<00:59,  3.52it/s] 50%|████▉     | 203/410 [02:20<00:58,  3.56it/s] 50%|████▉     | 204/410 [02:20<00:57,  3.59it/s] 50%|█████     | 205/410 [02:21<01:35,  2.14it/s] 50%|█████     | 206/410 [02:22<01:34,  2.16it/s] 50%|█████     | 207/410 [02:22<01:24,  2.39it/s] 51%|█████     | 208/410 [02:22<01:15,  2.67it/s] 51%|█████     | 209/410 [02:23<01:09,  2.90it/s] 51%|█████     | 210/410 [02:23<01:04,  3.10it/s] 51%|█████▏    | 211/410 [02:23<01:01,  3.25it/s] 52%|█████▏    | 212/410 [02:23<00:58,  3.36it/s] 52%|█████▏    | 213/410 [02:24<00:57,  3.44it/s] 52%|█████▏    | 214/410 [02:24<00:55,  3.51it/s] 52%|█████▏    | 215/410 [02:24<00:54,  3.55it/s] 53%|█████▎    | 216/410 [02:25<00:54,  3.58it/s] 53%|█████▎    | 217/410 [02:25<00:53,  3.60it/s] 53%|█████▎    | 218/410 [02:25<00:57,  3.31it/s] 53%|█████▎    | 219/410 [02:25<00:56,  3.41it/s] 54%|█████▎    | 220/410 [02:26<00:54,  3.48it/s] 54%|█████▍    | 221/410 [02:26<00:53,  3.53it/s] 54%|█████▍    | 222/410 [02:26<00:52,  3.57it/s] 54%|█████▍    | 223/410 [02:27<00:52,  3.59it/s] 55%|█████▍    | 224/410 [02:27<00:51,  3.61it/s] 55%|█████▍    | 225/410 [02:27<00:51,  3.62it/s] 55%|█████▌    | 226/410 [02:27<00:50,  3.63it/s] 55%|█████▌    | 227/410 [02:28<00:50,  3.64it/s] 56%|█████▌    | 228/410 [02:28<00:49,  3.64it/s] 56%|█████▌    | 229/410 [02:28<00:55,  3.27it/s] 56%|█████▌    | 230/410 [02:29<00:53,  3.38it/s] 56%|█████▋    | 231/410 [02:29<00:51,  3.45it/s] 57%|█████▋    | 232/410 [02:29<00:50,  3.51it/s] 57%|█████▋    | 233/410 [02:29<00:49,  3.56it/s] 57%|█████▋    | 234/410 [02:30<00:49,  3.58it/s] 57%|█████▋    | 235/410 [02:30<00:48,  3.61it/s] 58%|█████▊    | 236/410 [02:30<00:48,  3.62it/s] 58%|█████▊    | 237/410 [02:30<00:47,  3.63it/s] 58%|█████▊    | 238/410 [02:31<00:47,  3.64it/s] 58%|█████▊    | 239/410 [02:31<00:46,  3.65it/s] 59%|█████▊    | 240/410 [02:31<00:50,  3.39it/s] 59%|█████▉    | 241/410 [02:32<00:48,  3.47it/s] 59%|█████▉    | 242/410 [02:32<00:47,  3.52it/s] 59%|█████▉    | 243/410 [02:32<00:46,  3.57it/s] 60%|█████▉    | 244/410 [02:32<00:46,  3.59it/s] 60%|█████▉    | 245/410 [02:33<00:45,  3.61it/s] 60%|██████    | 246/410 [02:33<00:43,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 06:36:54,479 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:36:54,479 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 06:36:54,479 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 9.9926, 'eval_samples_per_second': 349.96, 'eval_steps_per_second': 43.833, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.16it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.41it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.24it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.28it/s][A
  6%|▋         | 28/438 [00:00<00:08, 45.86it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.41it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.34it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.31it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.17it/s][A
 12%|█▏        | 53/438 [00:01<00:09, 38.64it/s][A
 13%|█▎        | 58/438 [00:01<00:09, 40.56it/s][A
 14%|█▍        | 63/438 [00:01<00:12, 29.21it/s][A
 16%|█▌        | 68/438 [00:01<00:11, 32.78it/s][A
 17%|█▋        | 73/438 [00:01<00:10, 35.81it/s][A
 18%|█▊        | 78/438 [00:01<00:09, 38.35it/s][A
 19%|█▉        | 83/438 [00:02<00:08, 40.30it/s][A
 20%|██        | 88/438 [00:02<00:08, 41.84it/s][A
 21%|██        | 93/438 [00:02<00:08, 42.90it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 43.67it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 43.72it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 43.90it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.09it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.46it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.75it/s][A
 29%|██▉       | 128/438 [00:03<00:06, 45.01it/s][A
 30%|███       | 133/438 [00:03<00:06, 45.12it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 45.36it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 45.36it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 45.03it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 44.86it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.84it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.86it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 45.12it/s][A
 39%|███▉      | 173/438 [00:04<00:05, 45.27it/s][A
 41%|████      | 178/438 [00:04<00:05, 45.36it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.36it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.32it/s][A
 44%|████▍     | 193/438 [00:04<00:06, 38.26it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 40.31it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 41.68it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 42.91it/s][A
 49%|████▊     | 213/438 [00:04<00:05, 43.77it/s][A
 50%|████▉     | 218/438 [00:05<00:04, 44.38it/s][A
 51%|█████     | 223/438 [00:05<00:04, 44.76it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 44.94it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 44.57it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 44.45it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 44.55it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 44.65it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 45.06it/s][A
 59%|█████▉    | 258/438 [00:05<00:03, 45.24it/s][A
 60%|██████    | 263/438 [00:06<00:03, 45.33it/s][A
 61%|██████    | 268/438 [00:06<00:03, 45.55it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 45.43it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 45.09it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 44.93it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.83it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.85it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.93it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 45.26it/s][A
 70%|███████   | 308/438 [00:07<00:02, 45.41it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 45.52it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.35it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.11it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 38.98it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 40.82it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 42.08it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 43.23it/s][A
 79%|███████▉  | 348/438 [00:08<00:02, 43.94it/s][A
 81%|████████  | 353/438 [00:08<00:01, 44.50it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 44.89it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 45.00it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 44.55it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.38it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 44.52it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 44.65it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 44.99it/s][A
 90%|████████▉ | 393/438 [00:09<00:00, 45.14it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 45.32it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 45.40it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 45.30it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.96it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.57it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 44.67it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 44.69it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 45.05it/s][A
100%|██████████| 438/438 [00:10<00:00, 45.20it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 45.20it/s][A 60%|██████    | 246/410 [02:43<00:43,  3.77it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:37:05,111 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246
[INFO|configuration_utils.py:351] 2023-08-28 06:37:05,665 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:37:30,114 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:37:30,568 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:37:30,764 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246/special_tokens_map.json
 60%|██████    | 247/410 [03:13<33:01, 12.15s/it] 60%|██████    | 248/410 [03:13<23:15,  8.62s/it] 61%|██████    | 249/410 [03:14<16:24,  6.11s/it] 61%|██████    | 250/410 [03:14<11:38,  4.36s/it] 61%|██████    | 251/410 [03:14<08:18,  3.14s/it] 61%|██████▏   | 252/410 [03:14<06:00,  2.28s/it] 62%|██████▏   | 253/410 [03:15<04:23,  1.68s/it] 62%|██████▏   | 254/410 [03:15<03:16,  1.26s/it] 62%|██████▏   | 255/410 [03:15<02:29,  1.04it/s] 62%|██████▏   | 256/410 [03:15<01:56,  1.32it/s] 63%|██████▎   | 257/410 [03:16<01:36,  1.59it/s] 63%|██████▎   | 258/410 [03:16<01:19,  1.91it/s] 63%|██████▎   | 259/410 [03:16<01:07,  2.22it/s] 63%|██████▎   | 260/410 [03:17<00:59,  2.51it/s] 64%|██████▎   | 261/410 [03:17<00:54,  2.76it/s] 64%|██████▍   | 262/410 [03:17<00:49,  2.97it/s] 64%|██████▍   | 263/410 [03:18<00:51,  2.85it/s] 64%|██████▍   | 264/410 [03:18<00:48,  3.02it/s] 65%|██████▍   | 265/410 [03:18<00:45,  3.18it/s] 65%|██████▍   | 266/410 [03:18<00:43,  3.29it/s] 65%|██████▌   | 267/410 [03:19<00:48,  2.95it/s] 65%|██████▌   | 268/410 [03:19<00:45,  3.12it/s] 66%|██████▌   | 269/410 [03:19<00:43,  3.25it/s] 66%|██████▌   | 270/410 [03:20<00:41,  3.36it/s] 66%|██████▌   | 271/410 [03:20<00:40,  3.45it/s] 66%|██████▋   | 272/410 [03:20<00:39,  3.51it/s] 67%|██████▋   | 273/410 [03:20<00:38,  3.55it/s] 67%|██████▋   | 274/410 [03:21<00:43,  3.12it/s] 67%|██████▋   | 275/410 [03:21<00:50,  2.66it/s] 67%|██████▋   | 276/410 [03:22<00:54,  2.46it/s] 68%|██████▊   | 277/410 [03:22<00:51,  2.59it/s] 68%|██████▊   | 278/410 [03:22<00:46,  2.84it/s] 68%|██████▊   | 279/410 [03:23<00:43,  3.04it/s] 68%|██████▊   | 280/410 [03:23<00:40,  3.20it/s] 69%|██████▊   | 281/410 [03:23<00:38,  3.33it/s] 69%|██████▉   | 282/410 [03:24<00:37,  3.42it/s] 69%|██████▉   | 283/410 [03:24<00:36,  3.49it/s] 69%|██████▉   | 284/410 [03:24<00:35,  3.54it/s] 70%|██████▉   | 285/410 [03:24<00:35,  3.57it/s] 70%|██████▉   | 286/410 [03:25<00:34,  3.60it/s] 70%|███████   | 287/410 [03:25<00:34,  3.61it/s] 70%|███████   | 288/410 [03:25<00:36,  3.34it/s] 70%|███████   | 289/410 [03:26<00:35,  3.43it/s] 71%|███████   | 290/410 [03:26<00:34,  3.49it/s] 71%|███████   | 291/410 [03:26<00:33,  3.53it/s] 71%|███████   | 292/410 [03:26<00:33,  3.57it/s] 71%|███████▏  | 293/410 [03:27<00:32,  3.60it/s] 72%|███████▏  | 294/410 [03:27<00:32,  3.60it/s] 72%|███████▏  | 295/410 [03:27<00:31,  3.62it/s] 72%|███████▏  | 296/410 [03:27<00:31,  3.63it/s] 72%|███████▏  | 297/410 [03:28<00:31,  3.63it/s] 73%|███████▎  | 298/410 [03:28<00:30,  3.63it/s] 73%|███████▎  | 299/410 [03:28<00:34,  3.23it/s] 73%|███████▎  | 300/410 [03:29<00:32,  3.35it/s] 73%|███████▎  | 301/410 [03:29<00:31,  3.43it/s] 74%|███████▎  | 302/410 [03:29<00:30,  3.49it/s] 74%|███████▍  | 303/410 [03:30<00:30,  3.53it/s] 74%|███████▍  | 304/410 [03:30<00:29,  3.56it/s] 74%|███████▍  | 305/410 [03:30<00:29,  3.58it/s] 75%|███████▍  | 306/410 [03:30<00:28,  3.60it/s] 75%|███████▍  | 307/410 [03:31<00:28,  3.61it/s] 75%|███████▌  | 308/410 [03:31<00:28,  3.62it/s] 75%|███████▌  | 309/410 [03:31<00:27,  3.62it/s] 76%|███████▌  | 310/410 [03:32<00:30,  3.24it/s] 76%|███████▌  | 311/410 [03:32<00:29,  3.35it/s] 76%|███████▌  | 312/410 [03:32<00:28,  3.43it/s] 76%|███████▋  | 313/410 [03:32<00:27,  3.49it/s] 77%|███████▋  | 314/410 [03:33<00:27,  3.53it/s] 77%|███████▋  | 315/410 [03:33<00:26,  3.56it/s] 77%|███████▋  | 316/410 [03:33<00:26,  3.58it/s] 77%|███████▋  | 317/410 [03:33<00:25,  3.59it/s] 78%|███████▊  | 318/410 [03:34<00:25,  3.61it/s] 78%|███████▊  | 319/410 [03:34<00:25,  3.61it/s] 78%|███████▊  | 320/410 [03:34<00:24,  3.62it/s] 78%|███████▊  | 321/410 [03:35<00:27,  3.25it/s] 79%|███████▊  | 322/410 [03:35<00:26,  3.36it/s] 79%|███████▉  | 323/410 [03:35<00:25,  3.44it/s] 79%|███████▉  | 324/410 [03:36<00:24,  3.50it/s] 79%|███████▉  | 325/410 [03:36<00:24,  3.53it/s] 80%|███████▉  | 326/410 [03:36<00:23,  3.56it/s] 80%|███████▉  | 327/410 [03:36<00:23,  3.58it/s] 80%|████████  | 328/410 [03:37<00:21,  3.74it/s][INFO|trainer.py:2140] 2023-08-28 06:37:58,076 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:37:58,076 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 06:37:58,076 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.032, 'eval_samples_per_second': 348.584, 'eval_steps_per_second': 43.66, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.41it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.11it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.58it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.78it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.22it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.66it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.22it/s][A
 10%|▉         | 42/438 [00:00<00:10, 37.10it/s][A
 11%|█         | 47/438 [00:01<00:09, 39.37it/s][A
 12%|█▏        | 52/438 [00:01<00:09, 41.16it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 42.41it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 43.23it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 43.88it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.34it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.57it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 44.37it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.23it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.25it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.48it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.84it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 45.04it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.17it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.19it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 45.13it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.90it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.58it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.50it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.62it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.87it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.96it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.05it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.22it/s][A
 38%|███▊      | 167/438 [00:03<00:05, 45.29it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.07it/s][A
 40%|████      | 177/438 [00:04<00:07, 35.85it/s][A
 42%|████▏     | 182/438 [00:04<00:06, 38.18it/s][A
 43%|████▎     | 187/438 [00:04<00:06, 40.13it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 41.63it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 42.57it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 43.42it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 44.03it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.36it/s][A
 50%|████▉     | 217/438 [00:04<00:05, 44.12it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.18it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.30it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.56it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.75it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.93it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 45.12it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 45.29it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 45.12it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.76it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.75it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.81it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.83it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.92it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 45.16it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.20it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 45.28it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.92it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.85it/s][A
 71%|███████   | 312/438 [00:07<00:03, 37.31it/s][A
 72%|███████▏  | 317/438 [00:07<00:03, 39.38it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 41.09it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 42.32it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 43.26it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 43.89it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.20it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 44.55it/s][A
 80%|████████  | 352/438 [00:08<00:01, 44.32it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 44.29it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.39it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.70it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.88it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.98it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 45.07it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 45.17it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 45.11it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 45.09it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.84it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.72it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.85it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.94it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 39.11it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 40.87it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 42.17it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 43.09it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 43.09it/s][A 80%|████████  | 328/410 [03:47<00:21,  3.74it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:38:08,949 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328
[INFO|configuration_utils.py:351] 2023-08-28 06:38:09,571 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:38:36,325 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:38:36,859 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:38:37,094 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328/special_tokens_map.json
 80%|████████  | 329/410 [04:20<17:39, 13.08s/it] 80%|████████  | 330/410 [04:20<12:19,  9.24s/it] 81%|████████  | 331/410 [04:20<08:40,  6.59s/it] 81%|████████  | 332/410 [04:21<06:06,  4.70s/it] 81%|████████  | 333/410 [04:21<04:26,  3.46s/it] 81%|████████▏ | 334/410 [04:23<03:45,  2.97s/it] 82%|████████▏ | 335/410 [04:23<02:44,  2.20s/it] 82%|████████▏ | 336/410 [04:24<02:00,  1.62s/it] 82%|████████▏ | 337/410 [04:24<01:28,  1.22s/it] 82%|████████▏ | 338/410 [04:24<01:07,  1.07it/s] 83%|████████▎ | 339/410 [04:24<00:52,  1.35it/s] 83%|████████▎ | 340/410 [04:25<00:42,  1.67it/s] 83%|████████▎ | 341/410 [04:25<00:34,  1.99it/s] 83%|████████▎ | 342/410 [04:25<00:29,  2.30it/s] 84%|████████▎ | 343/410 [04:26<00:25,  2.58it/s] 84%|████████▍ | 344/410 [04:26<00:23,  2.82it/s] 84%|████████▍ | 345/410 [04:26<00:21,  3.02it/s] 84%|████████▍ | 346/410 [04:26<00:21,  2.97it/s] 85%|████████▍ | 347/410 [04:27<00:20,  3.13it/s] 85%|████████▍ | 348/410 [04:27<00:19,  3.26it/s] 85%|████████▌ | 349/410 [04:27<00:18,  3.36it/s] 85%|████████▌ | 350/410 [04:28<00:17,  3.43it/s] 86%|████████▌ | 351/410 [04:28<00:16,  3.48it/s] 86%|████████▌ | 352/410 [04:28<00:16,  3.52it/s] 86%|████████▌ | 353/410 [04:28<00:16,  3.54it/s] 86%|████████▋ | 354/410 [04:29<00:15,  3.56it/s] 87%|████████▋ | 355/410 [04:29<00:15,  3.58it/s] 87%|████████▋ | 356/410 [04:29<00:15,  3.59it/s] 87%|████████▋ | 357/410 [04:30<00:15,  3.40it/s] 87%|████████▋ | 358/410 [04:30<00:15,  3.46it/s] 88%|████████▊ | 359/410 [04:30<00:14,  3.50it/s] 88%|████████▊ | 360/410 [04:30<00:14,  3.54it/s] 88%|████████▊ | 361/410 [04:31<00:13,  3.55it/s] 88%|████████▊ | 362/410 [04:31<00:13,  3.57it/s] 89%|████████▊ | 363/410 [04:31<00:13,  3.57it/s] 89%|████████▉ | 364/410 [04:31<00:12,  3.58it/s] 89%|████████▉ | 365/410 [04:32<00:12,  3.59it/s] 89%|████████▉ | 366/410 [04:32<00:12,  3.59it/s] 90%|████████▉ | 367/410 [04:32<00:11,  3.59it/s] 90%|████████▉ | 368/410 [04:33<00:13,  3.16it/s] 90%|█████████ | 369/410 [04:33<00:12,  3.28it/s] 90%|█████████ | 370/410 [04:33<00:11,  3.37it/s] 90%|█████████ | 371/410 [04:34<00:11,  3.44it/s] 91%|█████████ | 372/410 [04:34<00:10,  3.49it/s] 91%|█████████ | 373/410 [04:34<00:10,  3.52it/s] 91%|█████████ | 374/410 [04:34<00:10,  3.55it/s] 91%|█████████▏| 375/410 [04:35<00:09,  3.56it/s] 92%|█████████▏| 376/410 [04:35<00:09,  3.58it/s] 92%|█████████▏| 377/410 [04:35<00:09,  3.58it/s] 92%|█████████▏| 378/410 [04:36<00:08,  3.58it/s] 92%|█████████▏| 379/410 [04:36<00:09,  3.17it/s] 93%|█████████▎| 380/410 [04:36<00:09,  3.29it/s] 93%|█████████▎| 381/410 [04:36<00:08,  3.38it/s] 93%|█████████▎| 382/410 [04:37<00:08,  3.44it/s] 93%|█████████▎| 383/410 [04:37<00:07,  3.50it/s] 94%|█████████▎| 384/410 [04:37<00:07,  3.52it/s] 94%|█████████▍| 385/410 [04:38<00:07,  3.55it/s] 94%|█████████▍| 386/410 [04:38<00:06,  3.56it/s] 94%|█████████▍| 387/410 [04:38<00:06,  3.57it/s] 95%|█████████▍| 388/410 [04:38<00:06,  3.58it/s] 95%|█████████▍| 389/410 [04:39<00:05,  3.58it/s] 95%|█████████▌| 390/410 [04:39<00:06,  3.18it/s] 95%|█████████▌| 391/410 [04:39<00:05,  3.31it/s] 96%|█████████▌| 392/410 [04:40<00:05,  3.40it/s] 96%|█████████▌| 393/410 [04:40<00:04,  3.47it/s] 96%|█████████▌| 394/410 [04:40<00:04,  3.52it/s] 96%|█████████▋| 395/410 [04:40<00:04,  3.56it/s] 97%|█████████▋| 396/410 [04:41<00:03,  3.58it/s] 97%|█████████▋| 397/410 [04:41<00:03,  3.60it/s] 97%|█████████▋| 398/410 [04:41<00:03,  3.61it/s] 97%|█████████▋| 399/410 [04:42<00:03,  3.62it/s] 98%|█████████▊| 400/410 [04:42<00:02,  3.62it/s] 98%|█████████▊| 401/410 [04:42<00:02,  3.42it/s] 98%|█████████▊| 402/410 [04:42<00:02,  3.49it/s] 98%|█████████▊| 403/410 [04:43<00:01,  3.53it/s] 99%|█████████▊| 404/410 [04:43<00:01,  3.56it/s] 99%|█████████▉| 405/410 [04:43<00:01,  3.59it/s] 99%|█████████▉| 406/410 [04:44<00:01,  3.61it/s] 99%|█████████▉| 407/410 [04:44<00:00,  3.61it/s]100%|█████████▉| 408/410 [04:44<00:00,  3.62it/s]100%|█████████▉| 409/410 [04:44<00:00,  3.63it/s]100%|██████████| 410/410 [04:45<00:00,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 06:39:06,095 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:39:06,095 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 06:39:06,095 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.0101, 'eval_samples_per_second': 349.348, 'eval_steps_per_second': 43.756, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.31it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.24it/s][A
  4%|▍         | 17/438 [00:00<00:12, 33.64it/s][A
  5%|▌         | 22/438 [00:00<00:11, 37.11it/s][A
  6%|▌         | 27/438 [00:00<00:10, 39.67it/s][A
  7%|▋         | 32/438 [00:00<00:09, 41.45it/s][A
  8%|▊         | 37/438 [00:00<00:09, 42.67it/s][A
 10%|▉         | 42/438 [00:01<00:09, 43.49it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.10it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.42it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 44.19it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 44.21it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 44.41it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 44.64it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 44.86it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.11it/s][A
 20%|█▉        | 87/438 [00:02<00:07, 45.16it/s][A
 21%|██        | 92/438 [00:02<00:07, 45.34it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 45.15it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.88it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.68it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.60it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.86it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.99it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 45.15it/s][A
 30%|███       | 132/438 [00:03<00:08, 36.68it/s][A
 31%|███▏      | 137/438 [00:03<00:07, 38.99it/s][A
 32%|███▏      | 142/438 [00:03<00:07, 40.77it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 42.04it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 43.09it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 43.69it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.24it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.47it/s][A
 39%|███▉      | 172/438 [00:03<00:06, 44.26it/s][A
 40%|████      | 177/438 [00:04<00:05, 44.18it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.21it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.51it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.79it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.06it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.19it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.23it/s][A
 48%|████▊     | 212/438 [00:04<00:06, 36.36it/s][A
 50%|████▉     | 218/438 [00:05<00:05, 40.26it/s][A
 51%|█████     | 223/438 [00:05<00:05, 41.62it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 42.76it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 43.54it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 44.12it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 44.49it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 44.86it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.76it/s][A
 59%|█████▉    | 258/438 [00:06<00:04, 37.40it/s][A
 60%|██████    | 263/438 [00:06<00:04, 39.57it/s][A
 61%|██████    | 268/438 [00:06<00:04, 41.15it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 42.30it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 43.27it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 43.86it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.43it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.60it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.37it/s][A
 69%|██████▉   | 303/438 [00:07<00:03, 44.18it/s][A
 70%|███████   | 308/438 [00:07<00:02, 44.42it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 44.70it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 44.90it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.01it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.11it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.15it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 44.98it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 44.70it/s][A
 79%|███████▉  | 348/438 [00:08<00:02, 44.50it/s][A
 81%|████████  | 353/438 [00:08<00:01, 44.51it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 44.88it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 45.00it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 45.20it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 45.19it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 45.20it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 44.87it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 44.57it/s][A
 90%|████████▉ | 393/438 [00:09<00:01, 41.10it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 42.39it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 43.20it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 43.84it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.37it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.74it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 44.83it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 44.81it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 44.54it/s][A
100%|██████████| 438/438 [00:10<00:00, 44.43it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.43it/s][A100%|██████████| 410/410 [04:55<00:00,  3.78it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 06:39:16,648 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410
[INFO|configuration_utils.py:351] 2023-08-28 06:39:16,995 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:39:46,841 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:39:47,733 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:39:47,902 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 06:39:51,661 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 06:39:51,743 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82 (score: 1.0607956647872925).
                                                 100%|██████████| 410/410 [05:58<00:00,  3.78it/s]100%|██████████| 410/410 [05:58<00:00,  1.14it/s]
[INFO|trainer.py:1894] 2023-08-28 06:40:20,328 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 06:40:20,672 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 06:40:35,057 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 06:40:35,463 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 06:40:35,629 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:40:37,147 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:37,148 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:37,148 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:37,148 >>   train_runtime            = 0:05:58.77
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:37,148 >>   train_samples            =       5239
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:37,148 >>   train_samples_per_second =     73.012
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:37,148 >>   train_steps_per_second   =      1.143
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.0718, 'eval_samples_per_second': 347.208, 'eval_steps_per_second': 43.488, 'epoch': 5.0}
{'train_runtime': 358.7748, 'train_samples_per_second': 73.012, 'train_steps_per_second': 1.143, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 06:40:38 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 06:40:38,158 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 06:40:38,158 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 06:40:38,158 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 56.02it/s]  3%|▎         | 12/438 [00:00<00:08, 49.81it/s]  4%|▍         | 18/438 [00:00<00:08, 47.72it/s]  5%|▌         | 23/438 [00:00<00:08, 46.95it/s]  6%|▋         | 28/438 [00:00<00:08, 46.54it/s]  8%|▊         | 33/438 [00:00<00:08, 46.28it/s]  9%|▊         | 38/438 [00:00<00:08, 46.12it/s] 10%|▉         | 43/438 [00:00<00:08, 45.61it/s] 11%|█         | 48/438 [00:01<00:08, 45.08it/s] 12%|█▏        | 53/438 [00:01<00:08, 44.96it/s] 13%|█▎        | 58/438 [00:01<00:08, 45.09it/s] 14%|█▍        | 63/438 [00:01<00:08, 45.04it/s] 16%|█▌        | 68/438 [00:01<00:08, 45.25it/s] 17%|█▋        | 73/438 [00:01<00:08, 45.25it/s] 18%|█▊        | 78/438 [00:01<00:07, 45.39it/s] 19%|█▉        | 83/438 [00:01<00:07, 45.35it/s] 20%|██        | 88/438 [00:01<00:07, 45.34it/s] 21%|██        | 93/438 [00:02<00:07, 45.23it/s] 22%|██▏       | 98/438 [00:02<00:07, 45.08it/s] 24%|██▎       | 103/438 [00:02<00:07, 45.10it/s] 25%|██▍       | 108/438 [00:02<00:07, 44.98it/s] 26%|██▌       | 113/438 [00:02<00:07, 45.24it/s] 27%|██▋       | 118/438 [00:02<00:07, 45.15it/s] 28%|██▊       | 123/438 [00:02<00:06, 45.31it/s] 29%|██▉       | 128/438 [00:02<00:06, 45.38it/s] 30%|███       | 133/438 [00:02<00:06, 45.40it/s] 32%|███▏      | 138/438 [00:03<00:08, 35.50it/s] 33%|███▎      | 143/438 [00:03<00:07, 38.07it/s] 34%|███▍      | 148/438 [00:03<00:07, 40.06it/s] 35%|███▍      | 153/438 [00:03<00:06, 41.53it/s] 36%|███▌      | 158/438 [00:03<00:06, 42.64it/s] 37%|███▋      | 163/438 [00:03<00:06, 43.51it/s] 38%|███▊      | 168/438 [00:03<00:06, 44.01it/s] 39%|███▉      | 173/438 [00:03<00:05, 44.43it/s] 41%|████      | 178/438 [00:04<00:05, 44.23it/s] 42%|████▏     | 183/438 [00:04<00:05, 44.16it/s] 43%|████▎     | 188/438 [00:04<00:05, 44.34it/s] 44%|████▍     | 193/438 [00:04<00:05, 44.63it/s] 45%|████▌     | 198/438 [00:04<00:05, 44.91it/s] 46%|████▋     | 203/438 [00:04<00:05, 44.91it/s] 47%|████▋     | 208/438 [00:04<00:05, 45.15it/s] 49%|████▊     | 213/438 [00:04<00:04, 45.19it/s] 50%|████▉     | 218/438 [00:04<00:04, 45.30it/s] 51%|█████     | 223/438 [00:05<00:04, 45.14it/s] 52%|█████▏    | 228/438 [00:05<00:04, 45.13it/s] 53%|█████▎    | 233/438 [00:05<00:04, 45.14it/s] 54%|█████▍    | 238/438 [00:05<00:04, 45.04it/s] 55%|█████▌    | 243/438 [00:05<00:04, 45.33it/s] 57%|█████▋    | 248/438 [00:05<00:04, 45.39it/s] 58%|█████▊    | 253/438 [00:05<00:04, 45.54it/s] 59%|█████▉    | 258/438 [00:05<00:03, 45.67it/s] 60%|██████    | 263/438 [00:05<00:03, 45.58it/s] 61%|██████    | 268/438 [00:05<00:03, 45.45it/s] 62%|██████▏   | 273/438 [00:06<00:04, 33.80it/s] 63%|██████▎   | 278/438 [00:06<00:04, 36.67it/s] 65%|██████▍   | 283/438 [00:06<00:03, 38.98it/s] 66%|██████▌   | 288/438 [00:06<00:03, 40.80it/s] 67%|██████▋   | 293/438 [00:06<00:03, 42.21it/s] 68%|██████▊   | 298/438 [00:06<00:03, 43.15it/s] 69%|██████▉   | 303/438 [00:06<00:03, 43.99it/s] 70%|███████   | 308/438 [00:06<00:02, 44.35it/s] 71%|███████▏  | 313/438 [00:07<00:02, 44.36it/s] 73%|███████▎  | 318/438 [00:07<00:02, 44.46it/s] 74%|███████▎  | 323/438 [00:07<00:02, 44.76it/s] 75%|███████▍  | 328/438 [00:07<00:02, 44.95it/s] 76%|███████▌  | 333/438 [00:07<00:02, 45.18it/s] 77%|███████▋  | 338/438 [00:07<00:02, 45.26it/s] 78%|███████▊  | 343/438 [00:07<00:02, 45.43it/s] 79%|███████▉  | 348/438 [00:07<00:01, 45.37it/s] 81%|████████  | 353/438 [00:07<00:01, 45.39it/s] 82%|████████▏ | 358/438 [00:08<00:01, 45.03it/s] 83%|████████▎ | 363/438 [00:08<00:01, 44.93it/s] 84%|████████▍ | 368/438 [00:08<00:01, 45.04it/s] 85%|████████▌ | 373/438 [00:08<00:01, 45.18it/s] 86%|████████▋ | 378/438 [00:08<00:01, 45.23it/s] 87%|████████▋ | 383/438 [00:08<00:01, 45.39it/s] 89%|████████▊ | 388/438 [00:08<00:01, 36.11it/s] 90%|████████▉ | 394/438 [00:08<00:01, 39.85it/s] 91%|█████████ | 399/438 [00:09<00:00, 41.33it/s] 92%|█████████▏| 404/438 [00:09<00:00, 36.78it/s] 93%|█████████▎| 409/438 [00:09<00:00, 38.96it/s] 95%|█████████▍| 414/438 [00:09<00:00, 40.68it/s] 96%|█████████▌| 419/438 [00:09<00:00, 42.12it/s] 97%|█████████▋| 424/438 [00:09<00:00, 43.00it/s] 98%|█████████▊| 429/438 [00:09<00:00, 43.82it/s] 99%|█████████▉| 434/438 [00:09<00:00, 44.29it/s]100%|██████████| 438/438 [00:10<00:00, 43.76it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 06:40:48,186 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:48,186 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:48,187 >>   eval_loss               =     1.0608
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:48,187 >>   eval_runtime            = 0:00:10.02
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:48,187 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:48,187 >>   eval_samples_per_second =    348.706
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:48,187 >>   eval_steps_per_second   =     43.675
[INFO|trainer_pt_utils.py:913] 2023-08-28 06:40:48,187 >>   perplexity              =     2.8887
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:26,743 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:26,869 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:26,870 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:26,870 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:26,870 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:43:28,473 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:43:28,474 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:43:29,017 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:43:56,743 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:43:56,819 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:59,781 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:59,887 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:59,887 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:59,887 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:43:59,887 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:44:02,076 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:44:02,077 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:44:02,994 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:44:07,321 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:44:07,402 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-328
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-82
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-246
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-164
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/checkpoint-410
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.65it/s]Extractor Predicting: 5it [00:03,  1.69it/s]Extractor Predicting: 6it [00:03,  1.57it/s]Extractor Predicting: 7it [00:04,  1.63it/s]Extractor Predicting: 8it [00:04,  1.72it/s]Extractor Predicting: 9it [00:05,  1.71it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.59it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.60it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.56it/s]Extractor Predicting: 17it [00:10,  1.61it/s]Extractor Predicting: 18it [00:11,  1.65it/s]Extractor Predicting: 19it [00:11,  1.67it/s]Extractor Predicting: 20it [00:12,  1.72it/s]Extractor Predicting: 21it [00:12,  1.63it/s]Extractor Predicting: 22it [00:13,  1.68it/s]Extractor Predicting: 23it [00:13,  1.70it/s]Extractor Predicting: 24it [00:14,  1.77it/s]Extractor Predicting: 25it [00:15,  1.80it/s]Extractor Predicting: 26it [00:15,  1.78it/s]Extractor Predicting: 27it [00:16,  1.68it/s]Extractor Predicting: 28it [00:16,  1.70it/s]Extractor Predicting: 29it [00:17,  1.69it/s]Extractor Predicting: 30it [00:18,  1.67it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.55it/s]Extractor Predicting: 33it [00:19,  1.60it/s]Extractor Predicting: 34it [00:20,  1.65it/s]Extractor Predicting: 35it [00:21,  1.67it/s]Extractor Predicting: 36it [00:21,  1.63it/s]Extractor Predicting: 37it [00:22,  1.29it/s]Extractor Predicting: 38it [00:23,  1.39it/s]Extractor Predicting: 39it [00:24,  1.43it/s]Extractor Predicting: 40it [00:24,  1.43it/s]Extractor Predicting: 41it [00:25,  1.41it/s]Extractor Predicting: 42it [00:26,  1.49it/s]Extractor Predicting: 43it [00:26,  1.55it/s]Extractor Predicting: 44it [00:27,  1.58it/s]Extractor Predicting: 45it [00:27,  1.60it/s]Extractor Predicting: 46it [00:28,  1.54it/s]Extractor Predicting: 47it [00:29,  1.55it/s]Extractor Predicting: 48it [00:29,  1.62it/s]Extractor Predicting: 49it [00:30,  1.60it/s]Extractor Predicting: 50it [00:31,  1.64it/s]Extractor Predicting: 51it [00:31,  1.58it/s]Extractor Predicting: 52it [00:32,  1.59it/s]Extractor Predicting: 53it [00:33,  1.59it/s]Extractor Predicting: 54it [00:33,  1.64it/s]Extractor Predicting: 55it [00:34,  1.63it/s]Extractor Predicting: 56it [00:34,  1.56it/s]Extractor Predicting: 57it [00:35,  1.62it/s]Extractor Predicting: 58it [00:36,  1.62it/s]Extractor Predicting: 59it [00:36,  1.60it/s]Extractor Predicting: 60it [00:37,  1.60it/s]Extractor Predicting: 61it [00:38,  1.51it/s]Extractor Predicting: 62it [00:38,  1.59it/s]Extractor Predicting: 63it [00:39,  1.65it/s]Extractor Predicting: 64it [00:39,  1.63it/s]Extractor Predicting: 65it [00:40,  1.68it/s]Extractor Predicting: 66it [00:41,  1.62it/s]Extractor Predicting: 67it [00:41,  1.65it/s]Extractor Predicting: 68it [00:42,  1.69it/s]Extractor Predicting: 69it [00:42,  1.70it/s]Extractor Predicting: 70it [00:43,  1.74it/s]Extractor Predicting: 71it [00:44,  1.56it/s]Extractor Predicting: 72it [00:44,  1.64it/s]Extractor Predicting: 73it [00:45,  1.65it/s]Extractor Predicting: 74it [00:45,  1.64it/s]Extractor Predicting: 75it [00:46,  1.68it/s]Extractor Predicting: 76it [00:47,  1.61it/s]Extractor Predicting: 77it [00:47,  1.68it/s]Extractor Predicting: 78it [00:48,  1.68it/s]Extractor Predicting: 79it [00:48,  1.74it/s]Extractor Predicting: 80it [00:49,  1.77it/s]Extractor Predicting: 81it [00:49,  1.76it/s]Extractor Predicting: 82it [00:50,  1.63it/s]Extractor Predicting: 83it [00:51,  1.64it/s]Extractor Predicting: 84it [00:51,  1.65it/s]Extractor Predicting: 85it [00:52,  1.64it/s]Extractor Predicting: 86it [00:53,  1.66it/s]Extractor Predicting: 87it [00:53,  1.62it/s]Extractor Predicting: 88it [00:54,  1.64it/s]Extractor Predicting: 89it [00:54,  1.66it/s]Extractor Predicting: 90it [00:55,  1.69it/s]Extractor Predicting: 91it [00:56,  1.68it/s]Extractor Predicting: 92it [00:56,  1.64it/s]Extractor Predicting: 93it [00:57,  1.65it/s]Extractor Predicting: 94it [00:57,  1.69it/s]Extractor Predicting: 95it [00:58,  1.72it/s]Extractor Predicting: 96it [00:58,  1.70it/s]Extractor Predicting: 97it [00:59,  1.68it/s]Extractor Predicting: 98it [01:00,  1.60it/s]Extractor Predicting: 99it [01:00,  1.62it/s]Extractor Predicting: 100it [01:01,  1.64it/s]Extractor Predicting: 101it [01:02,  1.70it/s]Extractor Predicting: 102it [01:02,  1.69it/s]Extractor Predicting: 103it [01:03,  1.63it/s]Extractor Predicting: 104it [01:03,  1.63it/s]Extractor Predicting: 105it [01:04,  1.65it/s]Extractor Predicting: 106it [01:05,  1.66it/s]Extractor Predicting: 107it [01:05,  1.64it/s]Extractor Predicting: 108it [01:06,  1.60it/s]Extractor Predicting: 109it [01:07,  1.60it/s]Extractor Predicting: 110it [01:07,  1.62it/s]Extractor Predicting: 111it [01:08,  1.65it/s]Extractor Predicting: 112it [01:08,  1.69it/s]Extractor Predicting: 113it [01:09,  1.60it/s]Extractor Predicting: 114it [01:10,  1.62it/s]Extractor Predicting: 115it [01:10,  1.64it/s]Extractor Predicting: 116it [01:11,  1.61it/s]Extractor Predicting: 117it [01:11,  1.62it/s]Extractor Predicting: 118it [01:12,  1.45it/s]Extractor Predicting: 119it [01:13,  1.51it/s]Extractor Predicting: 120it [01:13,  1.53it/s]Extractor Predicting: 121it [01:14,  1.58it/s]Extractor Predicting: 122it [01:15,  1.61it/s]Extractor Predicting: 123it [01:15,  1.62it/s]Extractor Predicting: 124it [01:16,  1.62it/s]Extractor Predicting: 125it [01:17,  1.60it/s]Extractor Predicting: 126it [01:17,  1.61it/s]Extractor Predicting: 127it [01:18,  1.58it/s]Extractor Predicting: 128it [01:18,  1.59it/s]Extractor Predicting: 129it [01:19,  1.61it/s]Extractor Predicting: 130it [01:20,  1.62it/s]Extractor Predicting: 131it [01:20,  1.60it/s]Extractor Predicting: 132it [01:21,  1.55it/s]Extractor Predicting: 133it [01:22,  1.59it/s]Extractor Predicting: 134it [01:22,  1.62it/s]Extractor Predicting: 135it [01:23,  1.57it/s]Extractor Predicting: 136it [01:24,  1.54it/s]Extractor Predicting: 137it [01:24,  1.53it/s]Extractor Predicting: 138it [01:25,  1.56it/s]Extractor Predicting: 139it [01:25,  1.58it/s]Extractor Predicting: 140it [01:26,  1.61it/s]Extractor Predicting: 141it [01:27,  1.61it/s]Extractor Predicting: 142it [01:27,  1.49it/s]Extractor Predicting: 143it [01:28,  1.53it/s]Extractor Predicting: 144it [01:29,  1.59it/s]Extractor Predicting: 145it [01:29,  1.80it/s]Extractor Predicting: 145it [01:29,  1.62it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:56,321 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:56,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:56,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:56,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:48:56,417 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:48:57,754 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:48:57,755 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:48:58,768 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:48:59,936 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:49:00,022 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:03,491 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:03,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:03,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:03,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:49:03,587 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:49:04,877 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:49:04,878 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:49:05,640 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:49:05,999 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:49:05,999 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.66it/s]Extractor Predicting: 4it [00:02,  1.70it/s]Extractor Predicting: 5it [00:02,  1.72it/s]Extractor Predicting: 6it [00:03,  1.60it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.69it/s]Extractor Predicting: 10it [00:05,  1.74it/s]Extractor Predicting: 11it [00:06,  1.74it/s]Extractor Predicting: 12it [00:07,  1.66it/s]Extractor Predicting: 13it [00:07,  1.68it/s]Extractor Predicting: 14it [00:08,  1.71it/s]Extractor Predicting: 15it [00:08,  1.66it/s]Extractor Predicting: 16it [00:09,  1.69it/s]Extractor Predicting: 17it [00:10,  1.63it/s]Extractor Predicting: 18it [00:10,  1.67it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.65it/s]Extractor Predicting: 21it [00:12,  1.68it/s]Extractor Predicting: 22it [00:13,  1.60it/s]Extractor Predicting: 23it [00:13,  1.65it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.66it/s]Extractor Predicting: 26it [00:15,  1.67it/s]Extractor Predicting: 27it [00:16,  1.72it/s]Extractor Predicting: 28it [00:16,  1.63it/s]Extractor Predicting: 29it [00:17,  1.65it/s]Extractor Predicting: 30it [00:18,  1.63it/s]Extractor Predicting: 31it [00:18,  1.67it/s]Extractor Predicting: 32it [00:19,  1.68it/s]Extractor Predicting: 33it [00:19,  1.66it/s]Extractor Predicting: 34it [00:20,  1.67it/s]Extractor Predicting: 35it [00:21,  1.69it/s]Extractor Predicting: 36it [00:21,  1.64it/s]Extractor Predicting: 37it [00:22,  1.68it/s]Extractor Predicting: 38it [00:22,  1.69it/s]Extractor Predicting: 39it [00:23,  1.71it/s]Extractor Predicting: 40it [00:23,  1.69it/s]Extractor Predicting: 41it [00:24,  1.69it/s]Extractor Predicting: 42it [00:25,  1.63it/s]Extractor Predicting: 43it [00:25,  1.62it/s]Extractor Predicting: 44it [00:26,  1.65it/s]Extractor Predicting: 45it [00:27,  1.65it/s]Extractor Predicting: 46it [00:27,  1.71it/s]Extractor Predicting: 47it [00:28,  1.66it/s]Extractor Predicting: 48it [00:28,  1.67it/s]Extractor Predicting: 49it [00:29,  1.71it/s]Extractor Predicting: 50it [00:29,  1.70it/s]Extractor Predicting: 51it [00:30,  1.70it/s]Extractor Predicting: 52it [00:31,  1.75it/s]Extractor Predicting: 53it [00:31,  1.64it/s]Extractor Predicting: 54it [00:32,  1.67it/s]Extractor Predicting: 55it [00:32,  1.67it/s]Extractor Predicting: 56it [00:33,  1.65it/s]Extractor Predicting: 57it [00:34,  1.64it/s]Extractor Predicting: 58it [00:34,  1.59it/s]Extractor Predicting: 59it [00:35,  1.59it/s]Extractor Predicting: 60it [00:36,  1.63it/s]Extractor Predicting: 61it [00:36,  1.64it/s]Extractor Predicting: 62it [00:37,  1.68it/s]Extractor Predicting: 63it [00:37,  1.62it/s]Extractor Predicting: 64it [00:38,  1.68it/s]Extractor Predicting: 65it [00:39,  1.68it/s]Extractor Predicting: 66it [00:39,  1.70it/s]Extractor Predicting: 67it [00:40,  1.73it/s]Extractor Predicting: 68it [00:40,  1.76it/s]Extractor Predicting: 69it [00:41,  1.72it/s]Extractor Predicting: 70it [00:41,  1.72it/s]Extractor Predicting: 71it [00:42,  1.75it/s]Extractor Predicting: 72it [00:43,  1.76it/s]Extractor Predicting: 73it [00:43,  1.76it/s]Extractor Predicting: 74it [00:44,  1.72it/s]Extractor Predicting: 75it [00:44,  1.57it/s]Extractor Predicting: 76it [00:45,  1.63it/s]Extractor Predicting: 77it [00:46,  1.71it/s]Extractor Predicting: 78it [00:46,  1.72it/s]Extractor Predicting: 79it [00:47,  1.74it/s]Extractor Predicting: 80it [00:47,  1.77it/s]Extractor Predicting: 81it [00:48,  1.77it/s]Extractor Predicting: 82it [00:48,  1.67it/s]Extractor Predicting: 83it [00:49,  1.68it/s]Extractor Predicting: 84it [00:50,  1.71it/s]Extractor Predicting: 85it [00:50,  1.74it/s]Extractor Predicting: 86it [00:51,  1.78it/s]Extractor Predicting: 87it [00:51,  1.80it/s]Extractor Predicting: 88it [00:52,  1.52it/s]Extractor Predicting: 89it [00:53,  1.54it/s]Extractor Predicting: 90it [00:53,  1.62it/s]Extractor Predicting: 91it [00:54,  1.69it/s]Extractor Predicting: 92it [00:54,  1.68it/s]Extractor Predicting: 93it [00:55,  1.56it/s]Extractor Predicting: 94it [00:56,  1.58it/s]Extractor Predicting: 95it [00:56,  1.63it/s]Extractor Predicting: 96it [00:57,  1.68it/s]Extractor Predicting: 97it [00:57,  1.69it/s]Extractor Predicting: 98it [00:58,  1.59it/s]Extractor Predicting: 99it [00:59,  1.63it/s]Extractor Predicting: 100it [00:59,  1.65it/s]Extractor Predicting: 101it [01:00,  1.70it/s]Extractor Predicting: 102it [01:01,  1.64it/s]Extractor Predicting: 103it [01:01,  1.60it/s]Extractor Predicting: 104it [01:02,  1.62it/s]Extractor Predicting: 105it [01:03,  1.47it/s]Extractor Predicting: 106it [01:03,  1.53it/s]Extractor Predicting: 107it [01:04,  1.56it/s]Extractor Predicting: 108it [01:05,  1.54it/s]Extractor Predicting: 109it [01:05,  1.59it/s]Extractor Predicting: 110it [01:06,  1.66it/s]Extractor Predicting: 111it [01:06,  1.71it/s]Extractor Predicting: 112it [01:07,  1.70it/s]Extractor Predicting: 113it [01:07,  1.69it/s]Extractor Predicting: 114it [01:08,  1.59it/s]Extractor Predicting: 115it [01:09,  1.62it/s]Extractor Predicting: 116it [01:09,  1.66it/s]Extractor Predicting: 117it [01:10,  1.70it/s]Extractor Predicting: 118it [01:10,  1.69it/s]Extractor Predicting: 119it [01:11,  1.64it/s]Extractor Predicting: 120it [01:12,  1.65it/s]Extractor Predicting: 121it [01:12,  1.67it/s]Extractor Predicting: 122it [01:13,  1.72it/s]Extractor Predicting: 123it [01:13,  1.73it/s]Extractor Predicting: 124it [01:14,  1.72it/s]Extractor Predicting: 125it [01:15,  1.61it/s]Extractor Predicting: 126it [01:15,  1.61it/s]Extractor Predicting: 127it [01:16,  1.55it/s]Extractor Predicting: 128it [01:17,  1.60it/s]Extractor Predicting: 129it [01:17,  1.67it/s]Extractor Predicting: 130it [01:18,  1.67it/s]Extractor Predicting: 131it [01:18,  1.73it/s]Extractor Predicting: 132it [01:19,  1.68it/s]Extractor Predicting: 133it [01:19,  1.68it/s]Extractor Predicting: 134it [01:20,  1.67it/s]Extractor Predicting: 135it [01:21,  1.67it/s]Extractor Predicting: 136it [01:21,  1.68it/s]Extractor Predicting: 137it [01:22,  1.61it/s]Extractor Predicting: 138it [01:23,  1.65it/s]Extractor Predicting: 139it [01:23,  1.66it/s]Extractor Predicting: 140it [01:24,  1.67it/s]Extractor Predicting: 141it [01:24,  1.72it/s]Extractor Predicting: 142it [01:25,  1.69it/s]Extractor Predicting: 143it [01:26,  1.65it/s]Extractor Predicting: 144it [01:26,  1.67it/s]Extractor Predicting: 145it [01:26,  2.14it/s]Extractor Predicting: 145it [01:26,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:51,349 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:51,389 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:51,390 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:51,390 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:51,390 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:50:52,045 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:50:52,046 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:50:52,688 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:50:53,742 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:50:53,743 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:57,630 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:57,716 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:57,717 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:57,717 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:50:57,717 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:50:59,015 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:50:59,017 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:50:59,821 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:51:00,215 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:51:00,316 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.41it/s]Extractor Predicting: 1it [00:00,  1.31it/s]
[INFO|configuration_utils.py:515] 2023-08-28 06:51:05,891 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:51:05,892 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 06:51:06,136 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:51:06,137 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 06:51:06,263 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 06:51:39,944 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 06:51:40,023 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 06:51:40,564 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 06:51:40,565 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 06:51:41,685 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:51:41,967 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:51:41,967 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:51:41,968 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:51:41,968 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:51:41,968 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 06:51:41,968 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 06:51:43,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:44,327 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:45,026 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:45,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:46,296 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:46,991 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:47,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:48,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:49,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:49,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:50,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:51,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:51,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:52,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:53,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:53,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:54,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:55,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:55,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:56,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:57,613 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:58,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:58,961 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:51:59,649 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:17<02:34, 17.17s/it][WARNING|generation_utils.py:914] 2023-08-28 06:52:00,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:00,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:01,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:02,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:03,065 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:03,732 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:04,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:04,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:05,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:06,315 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:07,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:07,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:08,381 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:09,166 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:10,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:10,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:11,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:11,964 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:12,677 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:13,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:14,033 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:14,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:15,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:32<02:10, 16.27s/it][WARNING|generation_utils.py:914] 2023-08-28 06:52:15,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:16,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:17,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:17,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:18,562 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:19,415 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:20,030 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:20,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:21,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:22,111 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:22,840 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:23,612 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:24,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:24,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:25,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:26,391 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:27,115 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:27,944 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:28,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:29,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:30,210 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:30,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:31,607 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:32,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:33,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:34,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:51<02:01, 17.42s/it][WARNING|generation_utils.py:914] 2023-08-28 06:52:34,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:35,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:36,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:36,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:37,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:38,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:38,817 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:39,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:40,543 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:41,195 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:41,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:42,517 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:43,316 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:44,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:44,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:45,252 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:45,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:46,527 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:47,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:47,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:48,506 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:49,180 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:49,928 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:50,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:08<01:42, 17.05s/it][WARNING|generation_utils.py:914] 2023-08-28 06:52:51,154 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:51,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:52,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:53,413 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:54,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:54,757 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:55,459 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:56,102 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:56,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:57,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:58,215 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:59,057 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:52:59,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:00,372 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:01,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:01,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:02,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:03,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:04,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:04,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:05,236 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:06,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:06,803 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:07,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:24<01:24, 16.99s/it][WARNING|generation_utils.py:914] 2023-08-28 06:53:08,058 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:08,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:09,382 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:10,012 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:10,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:11,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:11,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:12,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:13,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:13,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:14,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:14,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:15,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:16,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:16,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:17,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:18,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:18,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:19,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:19,933 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:20,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:21,039 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:21,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:22,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:39<01:05, 16.28s/it][WARNING|generation_utils.py:914] 2023-08-28 06:53:22,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:23,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:24,098 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:24,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:25,341 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:26,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:26,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:27,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:28,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:28,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:29,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:30,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:30,631 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:31,167 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:31,777 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:32,492 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:33,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:33,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:34,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:35,001 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:35,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:36,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:53<00:46, 15.42s/it][WARNING|generation_utils.py:914] 2023-08-28 06:53:36,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:37,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:38,008 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:38,651 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:39,241 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:39,923 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:40,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:41,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:42,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:42,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:43,781 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:44,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:45,114 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:45,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:46,628 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:47,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:48,087 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:48,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:49,511 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:50,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:50,860 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:51,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:52,238 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:52,853 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:10<00:31, 15.88s/it][WARNING|generation_utils.py:914] 2023-08-28 06:53:53,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:54,129 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:54,797 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:55,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:56,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:56,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:57,423 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:58,288 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:58,937 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:53:59,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:00,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:00,848 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:01,664 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:02,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:02,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:03,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:04,254 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:05,067 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:05,832 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:06,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:07,079 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:07,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:08,597 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:26<00:15, 15.86s/it][WARNING|generation_utils.py:914] 2023-08-28 06:54:09,299 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:09,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:10,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:11,342 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:11,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:12,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:13,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:13,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:14,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:15,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:15,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:16,452 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:17,040 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:17,601 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:18,194 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:18,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:19,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:20,086 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:20,775 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:21,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:22,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:22,564 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:23,163 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:23,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:24,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:25,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:25,670 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:26,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:26,925 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:27,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:28,126 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:28,821 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 06:54:29,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:47<00:00, 17.41s/it]Generating: 100%|██████████| 10/10 [02:47<00:00, 16.71s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:40,076 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:40,167 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:40,167 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:40,167 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:40,167 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:54:41,493 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:54:41,495 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:54:41,905 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:54:43,283 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:54:43,388 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:45,220 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:45,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:45,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:45,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:54:45,223 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:54:46,505 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:54:46,593 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:54:47,014 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:54:47,317 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:54:47,317 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : director .', 'success_rate': 0.8138020833333334, 'errors': {'', "('\\n', 'director', 'Scott Zandt', 'It was written and directed by Scott Zandt ( a.k.a .')", 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8505434782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 359, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 499, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7439903846153846, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : part of .', 'success_rate': 0.8020833333333334, 'errors': {'', 'too many values to unpack (expected 2)', '(\'1958 contest\', \'part of\', \'\', \'He was succeeded as Dutch representative at its 1958 contest by Johannes Eichhorn with " Allende en seine " .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a deal to turn " Ingress " into a mobile game . Head Entity : Ingress , Tail Entity : Ingress Studio .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.7981770833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8764204545454546, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n']
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n"]
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n", "Relation : member of political party . Context : This was the first coalition government which was elected in 1998 , and led by then - Prime Minister Naguib Sawiris of the People 's Alliance . Head Entity : Naguib Sawiris , Tail Entity : People ' Alliance .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8059895833333334, 'errors': {'', 'not enough values to unpack (expected 2, got 1)', '(\'Tom Blomkamp\', \'member of political party\', \'\', \'" My Life ( " ; ) is a 2015 English language English language documentary film directed by Tom Blomkamp and starring Emma Thompson and Tom Hardy .\')'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 58, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 96, 'raw': 160}
{'target': 600, 'success': 114, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 154, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 223, 'raw': 384}
{'target': 600, 'success': 245, 'raw': 416}
{'target': 600, 'success': 263, 'raw': 448}
{'target': 600, 'success': 276, 'raw': 480}
{'target': 600, 'success': 297, 'raw': 512}
{'target': 600, 'success': 318, 'raw': 544}
{'target': 600, 'success': 335, 'raw': 576}
{'target': 600, 'success': 359, 'raw': 608}
{'target': 600, 'success': 381, 'raw': 640}
{'target': 600, 'success': 398, 'raw': 672}
{'target': 600, 'success': 410, 'raw': 704}
{'target': 600, 'success': 431, 'raw': 736}
{'target': 600, 'success': 451, 'raw': 768}
{'target': 600, 'success': 470, 'raw': 800}
{'target': 600, 'success': 494, 'raw': 832}
{'target': 600, 'success': 516, 'raw': 864}
{'target': 600, 'success': 533, 'raw': 896}
{'target': 600, 'success': 552, 'raw': 928}
{'target': 600, 'success': 566, 'raw': 960}
{'target': 600, 'success': 584, 'raw': 992}
{'target': 600, 'success': 599, 'raw': 1024}
{'target': 600, 'success': 621, 'raw': 1056}
{'prompt': 'Relation : position held .', 'success_rate': 0.5880681818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 12256
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12356, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.07it/s]Extractor Estimating: 2it [00:01,  1.17it/s]Extractor Estimating: 3it [00:02,  1.28it/s]Extractor Estimating: 4it [00:03,  1.33it/s]Extractor Estimating: 5it [00:03,  1.33it/s]Extractor Estimating: 6it [00:04,  1.44it/s]Extractor Estimating: 7it [00:05,  1.41it/s]Extractor Estimating: 8it [00:06,  1.34it/s]Extractor Estimating: 9it [00:06,  1.38it/s]Extractor Estimating: 10it [00:07,  1.36it/s]Extractor Estimating: 11it [00:08,  1.43it/s]Extractor Estimating: 12it [00:08,  1.45it/s]Extractor Estimating: 13it [00:09,  1.48it/s]Extractor Estimating: 14it [00:10,  1.42it/s]Extractor Estimating: 15it [00:10,  1.42it/s]Extractor Estimating: 16it [00:11,  1.43it/s]Extractor Estimating: 17it [00:12,  1.44it/s]Extractor Estimating: 18it [00:12,  1.48it/s]Extractor Estimating: 19it [00:13,  1.45it/s]Extractor Estimating: 20it [00:14,  1.47it/s]Extractor Estimating: 21it [00:14,  1.44it/s]Extractor Estimating: 22it [00:15,  1.45it/s]Extractor Estimating: 23it [00:16,  1.45it/s]Extractor Estimating: 24it [00:17,  1.41it/s]Extractor Estimating: 25it [00:17,  1.39it/s]Extractor Estimating: 26it [00:18,  1.44it/s]Extractor Estimating: 27it [00:19,  1.44it/s]Extractor Estimating: 28it [00:19,  1.53it/s]Extractor Estimating: 29it [00:20,  1.53it/s]Extractor Estimating: 30it [00:20,  1.58it/s]Extractor Estimating: 31it [00:21,  1.61it/s]Extractor Estimating: 32it [00:22,  1.55it/s]Extractor Estimating: 33it [00:22,  1.55it/s]Extractor Estimating: 34it [00:23,  1.62it/s]Extractor Estimating: 35it [00:24,  1.68it/s]Extractor Estimating: 36it [00:24,  1.65it/s]Extractor Estimating: 37it [00:25,  1.57it/s]Extractor Estimating: 38it [00:25,  1.57it/s]Extractor Estimating: 39it [00:26,  1.56it/s]Extractor Estimating: 40it [00:27,  1.62it/s]Extractor Estimating: 41it [00:27,  1.59it/s]Extractor Estimating: 42it [00:28,  1.52it/s]Extractor Estimating: 43it [00:29,  1.58it/s]Extractor Estimating: 44it [00:29,  1.60it/s]Extractor Estimating: 45it [00:30,  1.61it/s]Extractor Estimating: 46it [00:30,  1.66it/s]Extractor Estimating: 47it [00:31,  1.57it/s]Extractor Estimating: 48it [00:32,  1.62it/s]Extractor Estimating: 49it [00:32,  1.68it/s]Extractor Estimating: 50it [00:33,  1.73it/s]Extractor Estimating: 51it [00:33,  1.64it/s]Extractor Estimating: 52it [00:34,  1.55it/s]Extractor Estimating: 53it [00:35,  1.59it/s]Extractor Estimating: 54it [00:35,  1.62it/s]Extractor Estimating: 55it [00:36,  1.60it/s]Extractor Estimating: 56it [00:37,  1.58it/s]Extractor Estimating: 57it [00:37,  1.55it/s]Extractor Estimating: 58it [00:38,  1.62it/s]Extractor Estimating: 59it [00:39,  1.58it/s]Extractor Estimating: 60it [00:39,  1.62it/s]Extractor Estimating: 61it [00:40,  1.46it/s]Extractor Estimating: 62it [00:41,  1.45it/s]Extractor Estimating: 63it [00:41,  1.50it/s]Extractor Estimating: 64it [00:42,  1.57it/s]Extractor Estimating: 65it [00:43,  1.51it/s]Extractor Estimating: 66it [00:43,  1.50it/s]Extractor Estimating: 67it [00:44,  1.52it/s]Extractor Estimating: 68it [00:45,  1.52it/s]Extractor Estimating: 69it [00:45,  1.53it/s]Extractor Estimating: 70it [00:46,  1.49it/s]Extractor Estimating: 71it [00:47,  1.52it/s]Extractor Estimating: 72it [00:47,  1.51it/s]Extractor Estimating: 73it [00:48,  1.50it/s]Extractor Estimating: 74it [00:49,  1.55it/s]Extractor Estimating: 75it [00:49,  1.40it/s]Extractor Estimating: 76it [00:50,  1.42it/s]Extractor Estimating: 77it [00:51,  1.45it/s]Extractor Estimating: 78it [00:51,  1.46it/s]Extractor Estimating: 79it [00:52,  1.48it/s]Extractor Estimating: 80it [00:53,  1.47it/s]Extractor Estimating: 81it [00:54,  1.34it/s]Extractor Estimating: 82it [00:54,  1.41it/s]Extractor Estimating: 83it [00:55,  1.41it/s]Extractor Estimating: 84it [00:56,  1.44it/s]Extractor Estimating: 85it [00:56,  1.36it/s]Extractor Estimating: 86it [00:57,  1.42it/s]Extractor Estimating: 87it [00:58,  1.48it/s]Extractor Estimating: 88it [00:58,  1.42it/s]Extractor Estimating: 89it [00:59,  1.44it/s]Extractor Estimating: 90it [01:00,  1.42it/s]Extractor Estimating: 91it [01:01,  1.44it/s]Extractor Estimating: 92it [01:01,  1.47it/s]Extractor Estimating: 93it [01:02,  1.50it/s]Extractor Estimating: 94it [01:02,  1.51it/s]Extractor Estimating: 95it [01:03,  1.50it/s]Extractor Estimating: 96it [01:04,  1.48it/s]Extractor Estimating: 97it [01:05,  1.48it/s]Extractor Estimating: 98it [01:05,  1.49it/s]Extractor Estimating: 99it [01:06,  1.53it/s]Extractor Estimating: 100it [01:07,  1.43it/s]Extractor Estimating: 101it [01:07,  1.49it/s]Extractor Estimating: 102it [01:08,  1.51it/s]Extractor Estimating: 103it [01:09,  1.51it/s]Extractor Estimating: 104it [01:09,  1.52it/s]Extractor Estimating: 105it [01:10,  1.47it/s]Extractor Estimating: 106it [01:11,  1.49it/s]Extractor Estimating: 107it [01:11,  1.51it/s]Extractor Estimating: 108it [01:12,  1.50it/s]Extractor Estimating: 109it [01:13,  1.52it/s]Extractor Estimating: 110it [01:13,  1.49it/s]Extractor Estimating: 111it [01:14,  1.51it/s]Extractor Estimating: 112it [01:15,  1.51it/s]Extractor Estimating: 113it [01:15,  1.54it/s]Extractor Estimating: 114it [01:16,  1.54it/s]Extractor Estimating: 115it [01:16,  1.58it/s]Extractor Estimating: 116it [01:17,  1.50it/s]Extractor Estimating: 117it [01:18,  1.54it/s]Extractor Estimating: 118it [01:18,  1.48it/s]Extractor Estimating: 119it [01:19,  1.54it/s]Extractor Estimating: 120it [01:20,  1.56it/s]Extractor Estimating: 121it [01:20,  1.48it/s]Extractor Estimating: 122it [01:21,  1.53it/s]Extractor Estimating: 123it [01:22,  1.55it/s]Extractor Estimating: 124it [01:22,  1.56it/s]Extractor Estimating: 125it [01:23,  1.60it/s]Extractor Estimating: 126it [01:24,  1.40it/s]Extractor Estimating: 127it [01:25,  1.41it/s]Extractor Estimating: 128it [01:25,  1.45it/s]Extractor Estimating: 129it [01:26,  1.52it/s]Extractor Estimating: 130it [01:26,  1.59it/s]Extractor Estimating: 131it [01:27,  1.56it/s]Extractor Estimating: 132it [01:28,  1.52it/s]Extractor Estimating: 133it [01:28,  1.54it/s]Extractor Estimating: 134it [01:29,  1.51it/s]Extractor Estimating: 135it [01:30,  1.55it/s]Extractor Estimating: 136it [01:30,  1.47it/s]Extractor Estimating: 137it [01:31,  1.53it/s]Extractor Estimating: 138it [01:32,  1.55it/s]Extractor Estimating: 139it [01:32,  1.57it/s]Extractor Estimating: 140it [01:33,  1.51it/s]Extractor Estimating: 141it [01:34,  1.52it/s]Extractor Estimating: 142it [01:34,  1.51it/s]Extractor Estimating: 143it [01:35,  1.52it/s]Extractor Estimating: 144it [01:35,  1.56it/s]Extractor Estimating: 145it [01:36,  1.59it/s]Extractor Estimating: 146it [01:37,  1.49it/s]Extractor Estimating: 147it [01:37,  1.53it/s]Extractor Estimating: 148it [01:38,  1.52it/s]Extractor Estimating: 149it [01:39,  1.55it/s]Extractor Estimating: 150it [01:40,  1.44it/s]Extractor Estimating: 151it [01:40,  1.48it/s]Extractor Estimating: 152it [01:41,  1.56it/s]Extractor Estimating: 153it [01:41,  1.63it/s]Extractor Estimating: 154it [01:42,  1.69it/s]Extractor Estimating: 155it [01:42,  1.73it/s]Extractor Estimating: 156it [01:43,  1.76it/s]Extractor Estimating: 157it [01:44,  1.68it/s]Extractor Estimating: 158it [01:44,  1.53it/s]Extractor Estimating: 159it [01:45,  1.61it/s]Extractor Estimating: 160it [01:45,  1.67it/s]Extractor Estimating: 161it [01:46,  1.76it/s]Extractor Estimating: 162it [01:46,  1.84it/s]Extractor Estimating: 163it [01:47,  1.80it/s]Extractor Estimating: 164it [01:48,  1.72it/s]Extractor Estimating: 165it [01:48,  1.79it/s]Extractor Estimating: 166it [01:49,  1.78it/s]Extractor Estimating: 167it [01:49,  1.86it/s]Extractor Estimating: 168it [01:50,  1.83it/s]Extractor Estimating: 169it [01:50,  1.85it/s]Extractor Estimating: 170it [01:51,  1.70it/s]Extractor Estimating: 171it [01:52,  1.72it/s]Extractor Estimating: 172it [01:52,  1.76it/s]Extractor Estimating: 173it [01:53,  1.80it/s]Extractor Estimating: 174it [01:53,  1.83it/s]Extractor Estimating: 175it [01:54,  1.85it/s]Extractor Estimating: 176it [01:54,  1.64it/s]Extractor Estimating: 177it [01:55,  1.63it/s]Extractor Estimating: 178it [01:56,  1.66it/s]Extractor Estimating: 179it [01:56,  1.60it/s]Extractor Estimating: 180it [01:57,  1.64it/s]Extractor Estimating: 181it [01:58,  1.58it/s]Extractor Estimating: 182it [01:58,  1.63it/s]Extractor Estimating: 183it [01:59,  1.56it/s]Extractor Estimating: 184it [01:59,  1.60it/s]Extractor Estimating: 185it [02:00,  1.60it/s]Extractor Estimating: 186it [02:01,  1.49it/s]Extractor Estimating: 187it [02:01,  1.54it/s]Extractor Estimating: 188it [02:02,  1.57it/s]Extractor Estimating: 189it [02:03,  1.58it/s]Extractor Estimating: 190it [02:03,  1.57it/s]Extractor Estimating: 191it [02:04,  1.50it/s]Extractor Estimating: 192it [02:05,  1.52it/s]Extractor Estimating: 193it [02:05,  1.57it/s]Extractor Estimating: 194it [02:06,  1.60it/s]Extractor Estimating: 195it [02:06,  1.64it/s]Extractor Estimating: 196it [02:07,  1.61it/s]Extractor Estimating: 197it [02:08,  1.65it/s]Extractor Estimating: 198it [02:08,  1.58it/s]Extractor Estimating: 199it [02:09,  1.63it/s]Extractor Estimating: 200it [02:10,  1.62it/s]Extractor Estimating: 201it [02:10,  1.58it/s]Extractor Estimating: 202it [02:11,  1.59it/s]Extractor Estimating: 203it [02:12,  1.58it/s]Extractor Estimating: 204it [02:12,  1.58it/s]Extractor Estimating: 205it [02:13,  1.55it/s]Extractor Estimating: 206it [02:14,  1.50it/s]Extractor Estimating: 207it [02:14,  1.52it/s]Extractor Estimating: 208it [02:15,  1.51it/s]Extractor Estimating: 209it [02:16,  1.50it/s]Extractor Estimating: 210it [02:16,  1.59it/s]Extractor Estimating: 211it [02:17,  1.55it/s]Extractor Estimating: 212it [02:17,  1.57it/s]Extractor Estimating: 213it [02:18,  1.53it/s]Extractor Estimating: 214it [02:19,  1.49it/s]Extractor Estimating: 215it [02:19,  1.55it/s]Extractor Estimating: 216it [02:20,  1.55it/s]Extractor Estimating: 217it [02:21,  1.58it/s]Extractor Estimating: 218it [02:21,  1.50it/s]Extractor Estimating: 219it [02:22,  1.51it/s]Extractor Estimating: 220it [02:23,  1.51it/s]Extractor Estimating: 221it [02:23,  1.53it/s]Extractor Estimating: 222it [02:24,  1.56it/s]Extractor Estimating: 223it [02:25,  1.52it/s]Extractor Estimating: 224it [02:25,  1.60it/s]Extractor Estimating: 225it [02:26,  1.53it/s]Extractor Estimating: 226it [02:27,  1.46it/s]Extractor Estimating: 227it [02:27,  1.41it/s]Extractor Estimating: 228it [02:28,  1.35it/s]Extractor Estimating: 229it [02:29,  1.42it/s]Extractor Estimating: 230it [02:29,  1.52it/s]Extractor Estimating: 231it [02:30,  1.52it/s]Extractor Estimating: 232it [02:31,  1.55it/s]Extractor Estimating: 233it [02:31,  1.48it/s]Extractor Estimating: 234it [02:32,  1.53it/s]Extractor Estimating: 235it [02:33,  1.57it/s]Extractor Estimating: 236it [02:33,  1.61it/s]Extractor Estimating: 237it [02:34,  1.60it/s]Extractor Estimating: 238it [02:34,  1.59it/s]Extractor Estimating: 239it [02:35,  1.62it/s]Extractor Estimating: 240it [02:36,  1.64it/s]Extractor Estimating: 241it [02:36,  1.61it/s]Extractor Estimating: 242it [02:37,  1.68it/s]Extractor Estimating: 243it [02:38,  1.56it/s]Extractor Estimating: 244it [02:38,  1.60it/s]Extractor Estimating: 245it [02:39,  1.58it/s]Extractor Estimating: 246it [02:39,  1.56it/s]Extractor Estimating: 247it [02:40,  1.58it/s]Extractor Estimating: 248it [02:41,  1.56it/s]Extractor Estimating: 249it [02:41,  1.60it/s]Extractor Estimating: 250it [02:42,  1.46it/s]Extractor Estimating: 250it [02:42,  1.54it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:00,187 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:00,295 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:00,296 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:00,296 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:00,296 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 06:58:01,643 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 06:58:01,644 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:58:02,506 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 06:58:03,748 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:58:03,817 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:07,823 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:07,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:07,903 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:07,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 06:58:07,904 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 06:58:09,208 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 06:58:09,209 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 06:58:10,232 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 06:58:10,688 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 06:58:10,688 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 08:29:53,330 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 08:29:54,670 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5099 mean pseudo reward: nan
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 20142
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 20242, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=20242, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.085, loss:nan
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.084, loss:nan
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 87, avg_time 1.107, loss:nan
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 187, avg_time 1.087, loss:nan
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 74, avg_time 1.077, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 174, avg_time 2.200, loss:nan
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 61, avg_time 1.081, loss:nan
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 161, avg_time 1.097, loss:nan
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 48, avg_time 1.076, loss:nan
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 148, avg_time 1.096, loss:nan
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1100, step 35, avg_time 2.161, loss:nan
g_step 1200, step 135, avg_time 1.102, loss:nan
g_step 1300, step 22, avg_time 1.087, loss:nan
g_step 1400, step 122, avg_time 1.088, loss:nan
g_step 1500, step 9, avg_time 1.071, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 1600, step 109, avg_time 2.179, loss:nan
g_step 1700, step 209, avg_time 1.087, loss:nan
g_step 1800, step 96, avg_time 1.081, loss:nan
g_step 1900, step 196, avg_time 1.096, loss:nan
g_step 2000, step 83, avg_time 1.071, loss:nan
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2100, step 183, avg_time 2.181, loss:nan
g_step 2200, step 70, avg_time 1.098, loss:nan
g_step 2300, step 170, avg_time 1.088, loss:nan
g_step 2400, step 57, avg_time 1.079, loss:nan
g_step 2500, step 157, avg_time 1.090, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 2600, step 44, avg_time 2.153, loss:nan
g_step 2700, step 144, avg_time 1.097, loss:nan
g_step 2800, step 31, avg_time 1.088, loss:nan
g_step 2900, step 131, avg_time 1.074, loss:nan
g_step 3000, step 18, avg_time 1.093, loss:nan
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3100, step 118, avg_time 2.153, loss:nan
g_step 3200, step 5, avg_time 1.063, loss:nan
g_step 3300, step 105, avg_time 1.071, loss:nan
g_step 3400, step 205, avg_time 1.075, loss:nan
g_step 3500, step 92, avg_time 1.106, loss:nan
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 3600, step 192, avg_time 2.133, loss:nan
g_step 3700, step 79, avg_time 1.088, loss:nan
g_step 3800, step 179, avg_time 1.067, loss:nan
g_step 3900, step 66, avg_time 1.089, loss:nan
g_step 4000, step 166, avg_time 1.090, loss:nan
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation prec:0.0000, rec:0.0000, f1:0.0000
>> valid relation with NER prec:0.0000, rec:0.0000, f1:0.0000
g_step 4100, step 53, avg_time 2.145, loss:nan
g_step 4200, step 153, avg_time 1.103, loss:nan
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 08:29:54 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 08:29:54 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_08-29-53_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 08:29:56 - WARNING - datasets.builder -   Using custom data configuration default-8daf78cc43f82836
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8daf78cc43f82836/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 08:30:02,019 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:30:02,020 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 08:30:02,020 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 08:30:02,022 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 08:30:02,282 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:30:02,439 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:30:02,439 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:30:02,439 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:30:02,439 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:30:02,439 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 08:30:02,439 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 08:30:03,392 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 08:30:06,865 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 08:30:06,918 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8daf78cc43f82836/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.29ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.31ba/s] 50%|█████     | 3/6 [00:00<00:00,  3.86ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  4.16ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  4.36ba/s]100%|██████████| 6/6 [00:01<00:00,  4.62ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.09ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.03ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.54ba/s]100%|██████████| 4/4 [00:01<00:00,  3.99ba/s]100%|██████████| 4/4 [00:01<00:00,  3.53ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.22ba/s] 50%|█████     | 3/6 [00:00<00:00,  5.45ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  7.41ba/s]100%|██████████| 6/6 [00:00<00:00,  7.33ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.70ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.82ba/s]100%|██████████| 4/4 [00:00<00:00,  7.16ba/s]100%|██████████| 4/4 [00:00<00:00,  5.79ba/s]
[INFO|trainer.py:414] 2023-08-28 08:30:15,150 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 08:30:15,388 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 08:30:15,388 >>   Num examples = 5100
[INFO|trainer.py:1149] 2023-08-28 08:30:15,388 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 08:30:15,388 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 08:30:15,388 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 08:30:15,388 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 08:30:15,388 >>   Total optimization steps = 400
  0%|          | 0/400 [00:00<?, ?it/s]  0%|          | 1/400 [00:00<01:58,  3.37it/s]  0%|          | 2/400 [00:00<01:53,  3.50it/s]  1%|          | 3/400 [00:00<01:51,  3.55it/s]  1%|          | 4/400 [00:01<02:11,  3.01it/s]  1%|▏         | 5/400 [00:01<02:03,  3.19it/s]  2%|▏         | 6/400 [00:01<01:58,  3.32it/s]  2%|▏         | 7/400 [00:02<01:55,  3.41it/s]  2%|▏         | 8/400 [00:02<01:53,  3.47it/s]  2%|▏         | 9/400 [00:02<01:51,  3.51it/s]  2%|▎         | 10/400 [00:02<01:50,  3.54it/s]  3%|▎         | 11/400 [00:03<01:48,  3.58it/s]  3%|▎         | 12/400 [00:03<01:47,  3.60it/s]  3%|▎         | 13/400 [00:03<01:46,  3.62it/s]  4%|▎         | 14/400 [00:04<01:46,  3.64it/s]  4%|▍         | 15/400 [00:04<02:03,  3.12it/s]  4%|▍         | 16/400 [00:04<01:57,  3.27it/s]  4%|▍         | 17/400 [00:04<01:53,  3.38it/s]  4%|▍         | 18/400 [00:05<01:50,  3.45it/s]  5%|▍         | 19/400 [00:05<01:48,  3.51it/s]  5%|▌         | 20/400 [00:05<01:46,  3.56it/s]  5%|▌         | 21/400 [00:06<01:45,  3.59it/s]  6%|▌         | 22/400 [00:06<01:44,  3.61it/s]  6%|▌         | 23/400 [00:06<01:44,  3.62it/s]  6%|▌         | 24/400 [00:06<01:43,  3.63it/s]  6%|▋         | 25/400 [00:07<01:42,  3.65it/s]  6%|▋         | 26/400 [00:07<01:49,  3.42it/s]  7%|▋         | 27/400 [00:07<01:46,  3.49it/s]  7%|▋         | 28/400 [00:08<01:45,  3.54it/s]  7%|▋         | 29/400 [00:08<01:43,  3.58it/s]  8%|▊         | 30/400 [00:08<01:42,  3.60it/s]  8%|▊         | 31/400 [00:08<01:42,  3.62it/s]  8%|▊         | 32/400 [00:09<01:41,  3.63it/s]  8%|▊         | 33/400 [00:09<01:40,  3.64it/s]  8%|▊         | 34/400 [00:09<01:40,  3.65it/s]  9%|▉         | 35/400 [00:09<01:39,  3.65it/s]  9%|▉         | 36/400 [00:10<01:39,  3.65it/s]  9%|▉         | 37/400 [00:10<01:47,  3.38it/s] 10%|▉         | 38/400 [00:10<01:44,  3.46it/s] 10%|▉         | 39/400 [00:11<01:42,  3.52it/s] 10%|█         | 40/400 [00:11<01:41,  3.56it/s] 10%|█         | 41/400 [00:11<01:40,  3.58it/s] 10%|█         | 42/400 [00:11<01:39,  3.60it/s] 11%|█         | 43/400 [00:12<01:38,  3.62it/s] 11%|█         | 44/400 [00:12<01:38,  3.63it/s] 11%|█▏        | 45/400 [00:12<01:37,  3.64it/s] 12%|█▏        | 46/400 [00:13<01:37,  3.65it/s] 12%|█▏        | 47/400 [00:13<01:36,  3.65it/s] 12%|█▏        | 48/400 [00:13<01:36,  3.65it/s] 12%|█▏        | 49/400 [00:13<01:36,  3.65it/s] 12%|█▎        | 50/400 [00:14<01:35,  3.65it/s] 13%|█▎        | 51/400 [00:14<01:35,  3.65it/s] 13%|█▎        | 52/400 [00:14<01:35,  3.66it/s] 13%|█▎        | 53/400 [00:14<01:34,  3.66it/s] 14%|█▎        | 54/400 [00:15<01:34,  3.66it/s] 14%|█▍        | 55/400 [00:15<01:34,  3.66it/s] 14%|█▍        | 56/400 [00:15<01:33,  3.66it/s] 14%|█▍        | 57/400 [00:16<01:33,  3.66it/s] 14%|█▍        | 58/400 [00:16<01:33,  3.66it/s] 15%|█▍        | 59/400 [00:16<01:37,  3.48it/s] 15%|█▌        | 60/400 [00:16<01:36,  3.53it/s] 15%|█▌        | 61/400 [00:17<01:35,  3.56it/s] 16%|█▌        | 62/400 [00:17<01:34,  3.58it/s] 16%|█▌        | 63/400 [00:17<01:33,  3.61it/s] 16%|█▌        | 64/400 [00:18<01:33,  3.61it/s] 16%|█▋        | 65/400 [00:18<01:32,  3.62it/s] 16%|█▋        | 66/400 [00:18<01:32,  3.63it/s] 17%|█▋        | 67/400 [00:18<01:31,  3.63it/s] 17%|█▋        | 68/400 [00:19<01:31,  3.63it/s] 17%|█▋        | 69/400 [00:19<01:30,  3.64it/s] 18%|█▊        | 70/400 [00:19<01:35,  3.45it/s] 18%|█▊        | 71/400 [00:20<01:34,  3.50it/s] 18%|█▊        | 72/400 [00:20<01:32,  3.54it/s] 18%|█▊        | 73/400 [00:20<01:31,  3.57it/s] 18%|█▊        | 74/400 [00:20<01:30,  3.59it/s] 19%|█▉        | 75/400 [00:21<01:30,  3.60it/s] 19%|█▉        | 76/400 [00:21<01:29,  3.62it/s] 19%|█▉        | 77/400 [00:21<01:29,  3.63it/s] 20%|█▉        | 78/400 [00:21<01:28,  3.63it/s] 20%|█▉        | 79/400 [00:22<01:28,  3.63it/s] 20%|██        | 80/400 [00:22<01:20,  3.98it/s][INFO|trainer.py:2140] 2023-08-28 08:30:37,786 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:30:37,787 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 08:30:37,787 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  2%|▏         | 7/438 [00:00<00:07, 59.20it/s][A
  3%|▎         | 13/438 [00:00<00:08, 50.58it/s][A
  4%|▍         | 19/438 [00:00<00:08, 48.42it/s][A
  5%|▌         | 24/438 [00:00<00:08, 47.40it/s][A
  7%|▋         | 29/438 [00:00<00:08, 46.74it/s][A
  8%|▊         | 34/438 [00:00<00:08, 46.46it/s][A
  9%|▉         | 39/438 [00:00<00:08, 46.06it/s][A
 10%|█         | 44/438 [00:00<00:08, 45.48it/s][A
 11%|█         | 49/438 [00:01<00:09, 39.85it/s][A
 12%|█▏        | 54/438 [00:01<00:09, 42.11it/s][A
 13%|█▎        | 59/438 [00:01<00:08, 43.19it/s][A
 15%|█▍        | 64/438 [00:01<00:08, 43.85it/s][A
 16%|█▌        | 69/438 [00:01<00:08, 44.36it/s][A
 17%|█▋        | 74/438 [00:01<00:08, 44.75it/s][A
 18%|█▊        | 79/438 [00:01<00:07, 45.01it/s][A
 19%|█▉        | 84/438 [00:01<00:07, 45.11it/s][A
 20%|██        | 89/438 [00:01<00:07, 44.80it/s][A
 21%|██▏       | 94/438 [00:02<00:07, 44.53it/s][A
 23%|██▎       | 99/438 [00:02<00:07, 44.51it/s][A
 24%|██▎       | 104/438 [00:02<00:07, 44.69it/s][A
 25%|██▍       | 109/438 [00:02<00:07, 44.98it/s][A
 26%|██▌       | 114/438 [00:02<00:07, 45.08it/s][A
 27%|██▋       | 119/438 [00:02<00:07, 45.27it/s][A
 28%|██▊       | 124/438 [00:02<00:06, 45.26it/s][A
 29%|██▉       | 129/438 [00:02<00:06, 45.39it/s][A
 31%|███       | 134/438 [00:03<00:06, 45.14it/s][A
 32%|███▏      | 139/438 [00:03<00:07, 42.47it/s][A
 33%|███▎      | 144/438 [00:03<00:06, 43.31it/s][A
 34%|███▍      | 149/438 [00:03<00:06, 43.85it/s][A
 35%|███▌      | 154/438 [00:03<00:06, 44.30it/s][A
 36%|███▋      | 159/438 [00:03<00:06, 44.73it/s][A
 37%|███▋      | 164/438 [00:03<00:06, 44.99it/s][A
 39%|███▊      | 169/438 [00:03<00:05, 45.16it/s][A
 40%|███▉      | 174/438 [00:03<00:06, 42.87it/s][A
 41%|████      | 179/438 [00:03<00:05, 44.00it/s][A
 42%|████▏     | 184/438 [00:04<00:05, 44.27it/s][A
 43%|████▎     | 189/438 [00:04<00:05, 44.49it/s][A
 44%|████▍     | 194/438 [00:04<00:05, 44.69it/s][A
 45%|████▌     | 199/438 [00:04<00:05, 44.84it/s][A
 47%|████▋     | 204/438 [00:04<00:05, 44.95it/s][A
 48%|████▊     | 209/438 [00:04<00:05, 45.23it/s][A
 49%|████▉     | 214/438 [00:04<00:04, 45.27it/s][A
 50%|█████     | 219/438 [00:04<00:04, 44.85it/s][A
 51%|█████     | 224/438 [00:04<00:04, 44.98it/s][A
 52%|█████▏    | 229/438 [00:05<00:04, 44.96it/s][A
 53%|█████▎    | 234/438 [00:05<00:04, 44.98it/s][A
 55%|█████▍    | 239/438 [00:05<00:04, 45.07it/s][A
 56%|█████▌    | 244/438 [00:05<00:04, 45.19it/s][A
 57%|█████▋    | 249/438 [00:05<00:04, 45.20it/s][A
 58%|█████▊    | 254/438 [00:05<00:04, 45.27it/s][A
 59%|█████▉    | 259/438 [00:05<00:03, 45.17it/s][A
 60%|██████    | 264/438 [00:05<00:03, 44.95it/s][A
 61%|██████▏   | 269/438 [00:06<00:03, 44.90it/s][A
 63%|██████▎   | 274/438 [00:06<00:03, 44.86it/s][A
 64%|██████▎   | 279/438 [00:06<00:03, 44.88it/s][A
 65%|██████▍   | 284/438 [00:06<00:03, 44.99it/s][A
 66%|██████▌   | 289/438 [00:06<00:03, 45.16it/s][A
 67%|██████▋   | 294/438 [00:06<00:03, 45.23it/s][A
 68%|██████▊   | 299/438 [00:06<00:03, 45.21it/s][A
 69%|██████▉   | 304/438 [00:06<00:02, 45.23it/s][A
 71%|███████   | 309/438 [00:06<00:02, 45.08it/s][A
 72%|███████▏  | 314/438 [00:06<00:02, 45.03it/s][A
 73%|███████▎  | 319/438 [00:07<00:02, 44.88it/s][A
 74%|███████▍  | 324/438 [00:07<00:02, 44.79it/s][A
 75%|███████▌  | 329/438 [00:07<00:02, 44.98it/s][A
 76%|███████▋  | 334/438 [00:07<00:02, 45.19it/s][A
 77%|███████▋  | 339/438 [00:07<00:02, 45.21it/s][A
 79%|███████▊  | 344/438 [00:07<00:02, 45.23it/s][A
 80%|███████▉  | 349/438 [00:07<00:01, 45.09it/s][A
 81%|████████  | 354/438 [00:07<00:01, 44.98it/s][A
 82%|████████▏ | 359/438 [00:07<00:01, 44.89it/s][A
 83%|████████▎ | 364/438 [00:08<00:01, 44.84it/s][A
 84%|████████▍ | 369/438 [00:08<00:01, 44.87it/s][A
 85%|████████▌ | 374/438 [00:08<00:01, 44.96it/s][A
 87%|████████▋ | 379/438 [00:08<00:01, 45.15it/s][A
 88%|████████▊ | 384/438 [00:08<00:01, 45.22it/s][A
 89%|████████▉ | 389/438 [00:08<00:01, 45.27it/s][A
 90%|████████▉ | 394/438 [00:08<00:00, 45.21it/s][A
 91%|█████████ | 399/438 [00:08<00:00, 45.10it/s][A
 92%|█████████▏| 404/438 [00:08<00:00, 45.04it/s][A
 93%|█████████▎| 409/438 [00:09<00:00, 44.90it/s][A
 95%|█████████▍| 414/438 [00:09<00:00, 43.69it/s][A
 96%|█████████▌| 419/438 [00:09<00:00, 44.25it/s][A
 97%|█████████▋| 424/438 [00:09<00:00, 44.65it/s][A
 98%|█████████▊| 429/438 [00:09<00:00, 44.96it/s][A
 99%|█████████▉| 434/438 [00:09<00:00, 45.01it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:09<00:00, 45.01it/s][A 20%|██        | 80/400 [00:32<01:20,  3.98it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:30:47,918 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80
[INFO|configuration_utils.py:351] 2023-08-28 08:30:48,385 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:31:07,742 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:31:08,791 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:31:09,088 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80/special_tokens_map.json
 20%|██        | 81/400 [00:58<58:03, 10.92s/it] 20%|██        | 82/400 [00:58<41:04,  7.75s/it] 21%|██        | 83/400 [00:58<29:06,  5.51s/it] 21%|██        | 84/400 [00:59<20:44,  3.94s/it] 21%|██▏       | 85/400 [00:59<14:54,  2.84s/it] 22%|██▏       | 86/400 [00:59<10:50,  2.07s/it] 22%|██▏       | 87/400 [00:59<07:59,  1.53s/it] 22%|██▏       | 88/400 [01:00<06:00,  1.15s/it] 22%|██▏       | 89/400 [01:00<04:36,  1.12it/s] 22%|██▎       | 90/400 [01:00<03:38,  1.42it/s] 23%|██▎       | 91/400 [01:01<02:57,  1.74it/s] 23%|██▎       | 92/400 [01:01<02:29,  2.06it/s] 23%|██▎       | 93/400 [01:01<02:15,  2.26it/s] 24%|██▎       | 94/400 [01:01<01:59,  2.55it/s] 24%|██▍       | 95/400 [01:02<01:48,  2.81it/s] 24%|██▍       | 96/400 [01:02<01:40,  3.02it/s] 24%|██▍       | 97/400 [01:02<01:35,  3.19it/s] 24%|██▍       | 98/400 [01:03<01:31,  3.31it/s] 25%|██▍       | 99/400 [01:03<01:28,  3.41it/s] 25%|██▌       | 100/400 [01:03<01:26,  3.48it/s] 25%|██▌       | 101/400 [01:03<01:24,  3.53it/s] 26%|██▌       | 102/400 [01:04<01:23,  3.57it/s] 26%|██▌       | 103/400 [01:04<01:22,  3.59it/s] 26%|██▌       | 104/400 [01:04<01:38,  3.00it/s] 26%|██▋       | 105/400 [01:05<01:33,  3.17it/s] 26%|██▋       | 106/400 [01:05<01:29,  3.30it/s] 27%|██▋       | 107/400 [01:05<01:26,  3.40it/s] 27%|██▋       | 108/400 [01:05<01:24,  3.48it/s] 27%|██▋       | 109/400 [01:06<01:22,  3.53it/s] 28%|██▊       | 110/400 [01:06<01:21,  3.56it/s] 28%|██▊       | 111/400 [01:06<01:20,  3.59it/s] 28%|██▊       | 112/400 [01:07<01:19,  3.61it/s] 28%|██▊       | 113/400 [01:07<01:19,  3.63it/s] 28%|██▊       | 114/400 [01:07<01:25,  3.33it/s] 29%|██▉       | 115/400 [01:07<01:23,  3.43it/s] 29%|██▉       | 116/400 [01:08<01:21,  3.50it/s] 29%|██▉       | 117/400 [01:08<01:19,  3.54it/s] 30%|██▉       | 118/400 [01:08<01:18,  3.58it/s] 30%|██▉       | 119/400 [01:09<01:18,  3.60it/s] 30%|███       | 120/400 [01:09<01:17,  3.62it/s] 30%|███       | 121/400 [01:09<01:16,  3.63it/s] 30%|███       | 122/400 [01:09<01:16,  3.63it/s] 31%|███       | 123/400 [01:10<01:16,  3.64it/s] 31%|███       | 124/400 [01:10<01:15,  3.65it/s] 31%|███▏      | 125/400 [01:10<01:22,  3.32it/s] 32%|███▏      | 126/400 [01:11<01:20,  3.41it/s] 32%|███▏      | 127/400 [01:11<01:18,  3.48it/s] 32%|███▏      | 128/400 [01:11<01:17,  3.53it/s] 32%|███▏      | 129/400 [01:11<01:16,  3.56it/s] 32%|███▎      | 130/400 [01:12<01:15,  3.59it/s] 33%|███▎      | 131/400 [01:12<01:14,  3.60it/s] 33%|███▎      | 132/400 [01:12<01:14,  3.61it/s] 33%|███▎      | 133/400 [01:12<01:13,  3.62it/s] 34%|███▎      | 134/400 [01:13<01:13,  3.63it/s] 34%|███▍      | 135/400 [01:13<01:13,  3.63it/s] 34%|███▍      | 136/400 [01:13<01:20,  3.26it/s] 34%|███▍      | 137/400 [01:14<01:18,  3.37it/s] 34%|███▍      | 138/400 [01:14<01:16,  3.44it/s] 35%|███▍      | 139/400 [01:14<01:14,  3.50it/s] 35%|███▌      | 140/400 [01:14<01:13,  3.54it/s] 35%|███▌      | 141/400 [01:15<01:12,  3.57it/s] 36%|███▌      | 142/400 [01:15<01:11,  3.59it/s] 36%|███▌      | 143/400 [01:15<01:11,  3.61it/s] 36%|███▌      | 144/400 [01:16<01:10,  3.62it/s] 36%|███▋      | 145/400 [01:16<01:10,  3.63it/s] 36%|███▋      | 146/400 [01:16<01:10,  3.63it/s] 37%|███▋      | 147/400 [01:16<01:14,  3.41it/s] 37%|███▋      | 148/400 [01:17<01:12,  3.48it/s] 37%|███▋      | 149/400 [01:17<01:11,  3.52it/s] 38%|███▊      | 150/400 [01:17<01:10,  3.56it/s] 38%|███▊      | 151/400 [01:18<01:09,  3.58it/s] 38%|███▊      | 152/400 [01:18<01:08,  3.60it/s] 38%|███▊      | 153/400 [01:18<01:08,  3.61it/s] 38%|███▊      | 154/400 [01:18<01:07,  3.62it/s] 39%|███▉      | 155/400 [01:19<01:07,  3.63it/s] 39%|███▉      | 156/400 [01:19<01:07,  3.63it/s] 39%|███▉      | 157/400 [01:19<01:06,  3.63it/s] 40%|███▉      | 158/400 [01:20<01:19,  3.06it/s] 40%|███▉      | 159/400 [01:20<01:14,  3.21it/s] 40%|████      | 160/400 [01:20<01:06,  3.62it/s][INFO|trainer.py:2140] 2023-08-28 08:31:36,026 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:31:36,027 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 08:31:36,027 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 9.8193, 'eval_samples_per_second': 356.136, 'eval_steps_per_second': 44.606, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.32it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.27it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.44it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.87it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.31it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.84it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.47it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.15it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.15it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.13it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.15it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.26it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.33it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.30it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.25it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.06it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.91it/s][A
 21%|██        | 92/438 [00:02<00:07, 45.02it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 45.01it/s][A
 23%|██▎       | 102/438 [00:02<00:09, 37.06it/s][A
 24%|██▍       | 107/438 [00:02<00:08, 39.18it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 40.92it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 42.19it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.16it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.85it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.34it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.63it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.44it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.28it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.57it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.75it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.95it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 45.12it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.25it/s][A
 40%|████      | 177/438 [00:03<00:05, 45.30it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 45.16it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.92it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 44.85it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.81it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.93it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.00it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 45.19it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.25it/s][A
 51%|█████     | 222/438 [00:04<00:04, 45.35it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 45.18it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 45.06it/s][A
 54%|█████▍    | 237/438 [00:05<00:05, 38.27it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 40.18it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 41.65it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 42.73it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 43.67it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.25it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.69it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.77it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.53it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.30it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.46it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.74it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.82it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 45.12it/s][A
 70%|███████   | 307/438 [00:06<00:02, 45.23it/s][A
 71%|███████   | 312/438 [00:07<00:02, 45.35it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.16it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.94it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.83it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.73it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.88it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 45.04it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 45.17it/s][A
 80%|████████  | 352/438 [00:07<00:01, 45.31it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.40it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.25it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.03it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 35.86it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 38.25it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 40.16it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 41.57it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 42.68it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 43.52it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.12it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.47it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.20it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.29it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.39it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.61it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.80it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 45.09it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 45.09it/s][A 40%|████      | 160/400 [01:30<01:06,  3.62it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:31:46,574 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160
[INFO|configuration_utils.py:351] 2023-08-28 08:31:47,297 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:32:10,391 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:32:11,468 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:32:11,690 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160/special_tokens_map.json
 40%|████      | 161/400 [01:59<47:31, 11.93s/it] 40%|████      | 162/400 [02:00<33:32,  8.46s/it] 41%|████      | 163/400 [02:00<23:42,  6.00s/it] 41%|████      | 164/400 [02:00<16:51,  4.28s/it] 41%|████▏     | 165/400 [02:00<12:04,  3.08s/it] 42%|████▏     | 166/400 [02:01<08:43,  2.24s/it]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
 42%|████▏     | 167/400 [02:01<06:24,  1.65s/it] 42%|████▏     | 168/400 [02:01<04:46,  1.24s/it] 42%|████▏     | 169/400 [02:02<03:38,  1.06it/s] 42%|████▎     | 170/400 [02:02<02:51,  1.34it/s] 43%|████▎     | 171/400 [02:02<02:18,  1.66it/s] 43%|████▎     | 172/400 [02:02<01:54,  1.98it/s] 43%|████▎     | 173/400 [02:03<01:45,  2.15it/s] 44%|████▎     | 174/400 [02:03<01:32,  2.45it/s] 44%|████▍     | 175/400 [02:03<01:22,  2.73it/s] 44%|████▍     | 176/400 [02:04<01:15,  2.95it/s] 44%|████▍     | 177/400 [02:04<01:11,  3.13it/s] 44%|████▍     | 178/400 [02:04<01:07,  3.28it/s] 45%|████▍     | 179/400 [02:04<01:05,  3.39it/s] 45%|████▌     | 180/400 [02:05<01:03,  3.46it/s] 45%|████▌     | 181/400 [02:05<01:02,  3.52it/s] 46%|████▌     | 182/400 [02:05<01:01,  3.56it/s] 46%|████▌     | 183/400 [02:05<01:00,  3.60it/s] 46%|████▌     | 184/400 [02:06<00:59,  3.62it/s] 46%|████▋     | 185/400 [02:06<00:59,  3.63it/s] 46%|████▋     | 186/400 [02:06<00:58,  3.64it/s] 47%|████▋     | 187/400 [02:07<00:58,  3.64it/s] 47%|████▋     | 188/400 [02:07<00:58,  3.65it/s] 47%|████▋     | 189/400 [02:07<00:57,  3.65it/s] 48%|████▊     | 190/400 [02:07<00:57,  3.65it/s] 48%|████▊     | 191/400 [02:08<00:57,  3.65it/s] 48%|████▊     | 192/400 [02:08<00:56,  3.65it/s] 48%|████▊     | 193/400 [02:08<00:56,  3.65it/s] 48%|████▊     | 194/400 [02:08<00:56,  3.65it/s] 49%|████▉     | 195/400 [02:09<01:01,  3.34it/s] 49%|████▉     | 196/400 [02:09<00:59,  3.43it/s] 49%|████▉     | 197/400 [02:09<00:58,  3.50it/s] 50%|████▉     | 198/400 [02:10<00:56,  3.55it/s] 50%|████▉     | 199/400 [02:10<00:56,  3.58it/s] 50%|█████     | 200/400 [02:10<00:55,  3.60it/s] 50%|█████     | 201/400 [02:10<00:54,  3.62it/s] 50%|█████     | 202/400 [02:11<00:54,  3.63it/s] 51%|█████     | 203/400 [02:11<00:54,  3.64it/s] 51%|█████     | 204/400 [02:11<00:53,  3.64it/s] 51%|█████▏    | 205/400 [02:12<00:53,  3.65it/s] 52%|█████▏    | 206/400 [02:12<00:59,  3.27it/s] 52%|█████▏    | 207/400 [02:12<00:57,  3.38it/s] 52%|█████▏    | 208/400 [02:12<00:55,  3.46it/s] 52%|█████▏    | 209/400 [02:13<00:54,  3.51it/s] 52%|█████▎    | 210/400 [02:13<00:53,  3.55it/s] 53%|█████▎    | 211/400 [02:13<00:52,  3.58it/s] 53%|█████▎    | 212/400 [02:14<00:52,  3.61it/s] 53%|█████▎    | 213/400 [02:14<00:51,  3.62it/s] 54%|█████▎    | 214/400 [02:14<00:51,  3.63it/s] 54%|█████▍    | 215/400 [02:14<00:50,  3.64it/s] 54%|█████▍    | 216/400 [02:15<00:50,  3.64it/s] 54%|█████▍    | 217/400 [02:15<00:55,  3.30it/s] 55%|█████▍    | 218/400 [02:15<00:53,  3.39it/s] 55%|█████▍    | 219/400 [02:16<00:52,  3.47it/s] 55%|█████▌    | 220/400 [02:16<00:51,  3.52it/s] 55%|█████▌    | 221/400 [02:16<00:50,  3.55it/s] 56%|█████▌    | 222/400 [02:16<00:49,  3.58it/s] 56%|█████▌    | 223/400 [02:17<00:49,  3.60it/s] 56%|█████▌    | 224/400 [02:17<00:48,  3.61it/s] 56%|█████▋    | 225/400 [02:17<00:48,  3.62it/s] 56%|█████▋    | 226/400 [02:17<00:47,  3.63it/s] 57%|█████▋    | 227/400 [02:18<00:47,  3.63it/s] 57%|█████▋    | 228/400 [02:18<00:51,  3.37it/s] 57%|█████▋    | 229/400 [02:18<00:49,  3.45it/s] 57%|█████▊    | 230/400 [02:19<00:48,  3.50it/s] 58%|█████▊    | 231/400 [02:19<00:47,  3.54it/s] 58%|█████▊    | 232/400 [02:19<00:47,  3.56it/s] 58%|█████▊    | 233/400 [02:19<00:46,  3.58it/s] 58%|█████▊    | 234/400 [02:20<00:46,  3.60it/s] 59%|█████▉    | 235/400 [02:20<00:45,  3.61it/s] 59%|█████▉    | 236/400 [02:20<00:45,  3.62it/s] 59%|█████▉    | 237/400 [02:21<00:44,  3.63it/s] 60%|█████▉    | 238/400 [02:21<00:44,  3.63it/s] 60%|█████▉    | 239/400 [02:21<00:47,  3.41it/s] 60%|██████    | 240/400 [02:21<00:42,  3.78it/s][INFO|trainer.py:2140] 2023-08-28 08:32:37,296 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:32:37,296 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 08:32:37,296 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 9.9302, 'eval_samples_per_second': 352.16, 'eval_steps_per_second': 44.108, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.65it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.50it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.48it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.88it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.39it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.96it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.54it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.11it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.15it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.36it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.37it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.43it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.32it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.43it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.31it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.01it/s][A
 20%|██        | 88/438 [00:01<00:07, 44.96it/s][A
 21%|██        | 93/438 [00:02<00:07, 44.93it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.06it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.24it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 45.33it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 45.45it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 40.40it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 41.85it/s][A
 29%|██▉       | 128/438 [00:02<00:07, 42.99it/s][A
 30%|███       | 133/438 [00:02<00:06, 43.69it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.29it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.63it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.87it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 44.96it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.64it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.62it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.74it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 45.00it/s][A
 41%|████      | 178/438 [00:03<00:05, 45.19it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.31it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.37it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.42it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.23it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.01it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 44.95it/s][A
 49%|████▊     | 213/438 [00:04<00:05, 44.92it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.04it/s][A
 51%|█████     | 223/438 [00:04<00:04, 45.20it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 45.39it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.40it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 45.35it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 45.30it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 45.08it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 39.63it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 41.39it/s][A
 60%|██████    | 263/438 [00:05<00:04, 42.57it/s][A
 61%|██████    | 268/438 [00:05<00:03, 43.38it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 44.12it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 44.55it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 44.95it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.97it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.73it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.50it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 44.77it/s][A
 70%|███████   | 308/438 [00:06<00:02, 44.93it/s][A
 71%|███████▏  | 313/438 [00:06<00:02, 45.13it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.26it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.07it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.22it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.24it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 45.01it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 44.93it/s][A
 79%|███████▉  | 348/438 [00:07<00:02, 44.89it/s][A
 81%|████████  | 353/438 [00:07<00:01, 44.88it/s][A
 82%|████████▏ | 358/438 [00:07<00:01, 45.21it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 45.28it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 45.35it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 45.39it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 45.25it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 45.18it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 36.59it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 38.93it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 40.75it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 42.14it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 43.12it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 43.93it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.49it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 44.71it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 44.47it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 44.30it/s][A
100%|██████████| 438/438 [00:09<00:00, 44.57it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.57it/s][A 60%|██████    | 240/400 [02:31<00:42,  3.78it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:32:47,631 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240
[INFO|configuration_utils.py:351] 2023-08-28 08:32:48,283 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:33:13,434 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:33:14,643 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:33:14,944 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240/special_tokens_map.json
 60%|██████    | 241/400 [03:03<33:30, 12.65s/it] 60%|██████    | 242/400 [03:03<23:37,  8.97s/it] 61%|██████    | 243/400 [03:04<16:38,  6.36s/it] 61%|██████    | 244/400 [03:04<11:47,  4.54s/it] 61%|██████▏   | 245/400 [03:04<08:25,  3.26s/it] 62%|██████▏   | 246/400 [03:04<06:04,  2.36s/it] 62%|██████▏   | 247/400 [03:05<04:25,  1.74s/it] 62%|██████▏   | 248/400 [03:05<03:17,  1.30s/it] 62%|██████▏   | 249/400 [03:05<02:29,  1.01it/s] 62%|██████▎   | 250/400 [03:06<01:56,  1.28it/s] 63%|██████▎   | 251/400 [03:06<01:33,  1.59it/s] 63%|██████▎   | 252/400 [03:06<01:17,  1.92it/s] 63%|██████▎   | 253/400 [03:06<01:09,  2.13it/s] 64%|██████▎   | 254/400 [03:07<00:59,  2.44it/s] 64%|██████▍   | 255/400 [03:07<01:04,  2.25it/s] 64%|██████▍   | 256/400 [03:08<00:56,  2.54it/s] 64%|██████▍   | 257/400 [03:08<00:51,  2.80it/s] 64%|██████▍   | 258/400 [03:08<00:47,  3.01it/s] 65%|██████▍   | 259/400 [03:08<00:44,  3.19it/s] 65%|██████▌   | 260/400 [03:09<00:42,  3.32it/s] 65%|██████▌   | 261/400 [03:09<00:40,  3.42it/s] 66%|██████▌   | 262/400 [03:09<00:39,  3.49it/s] 66%|██████▌   | 263/400 [03:09<00:38,  3.54it/s] 66%|██████▌   | 264/400 [03:10<00:38,  3.58it/s] 66%|██████▋   | 265/400 [03:10<00:41,  3.27it/s] 66%|██████▋   | 266/400 [03:10<00:39,  3.38it/s] 67%|██████▋   | 267/400 [03:11<00:38,  3.46it/s] 67%|██████▋   | 268/400 [03:11<00:37,  3.52it/s] 67%|██████▋   | 269/400 [03:11<00:36,  3.56it/s] 68%|██████▊   | 270/400 [03:11<00:36,  3.60it/s] 68%|██████▊   | 271/400 [03:12<00:35,  3.61it/s] 68%|██████▊   | 272/400 [03:12<00:35,  3.63it/s] 68%|██████▊   | 273/400 [03:12<00:34,  3.64it/s] 68%|██████▊   | 274/400 [03:13<00:34,  3.65it/s] 69%|██████▉   | 275/400 [03:13<00:34,  3.65it/s] 69%|██████▉   | 276/400 [03:13<00:36,  3.44it/s] 69%|██████▉   | 277/400 [03:13<00:35,  3.51it/s] 70%|██████▉   | 278/400 [03:14<00:34,  3.55it/s] 70%|██████▉   | 279/400 [03:14<00:33,  3.59it/s] 70%|███████   | 280/400 [03:14<00:33,  3.61it/s] 70%|███████   | 281/400 [03:14<00:32,  3.62it/s] 70%|███████   | 282/400 [03:15<00:32,  3.64it/s] 71%|███████   | 283/400 [03:15<00:32,  3.64it/s] 71%|███████   | 284/400 [03:15<00:31,  3.65it/s] 71%|███████▏  | 285/400 [03:16<00:31,  3.65it/s] 72%|███████▏  | 286/400 [03:16<00:31,  3.66it/s] 72%|███████▏  | 287/400 [03:16<00:34,  3.31it/s] 72%|███████▏  | 288/400 [03:16<00:32,  3.41it/s] 72%|███████▏  | 289/400 [03:17<00:31,  3.48it/s] 72%|███████▎  | 290/400 [03:17<00:31,  3.54it/s] 73%|███████▎  | 291/400 [03:17<00:30,  3.57it/s] 73%|███████▎  | 292/400 [03:18<00:30,  3.60it/s] 73%|███████▎  | 293/400 [03:18<00:29,  3.62it/s] 74%|███████▎  | 294/400 [03:18<00:29,  3.63it/s] 74%|███████▍  | 295/400 [03:18<00:28,  3.63it/s] 74%|███████▍  | 296/400 [03:19<00:28,  3.64it/s] 74%|███████▍  | 297/400 [03:19<00:28,  3.65it/s] 74%|███████▍  | 298/400 [03:19<00:30,  3.31it/s] 75%|███████▍  | 299/400 [03:20<00:29,  3.41it/s] 75%|███████▌  | 300/400 [03:20<00:28,  3.48it/s] 75%|███████▌  | 301/400 [03:20<00:28,  3.53it/s] 76%|███████▌  | 302/400 [03:20<00:27,  3.57it/s] 76%|███████▌  | 303/400 [03:21<00:26,  3.60it/s] 76%|███████▌  | 304/400 [03:21<00:26,  3.61it/s] 76%|███████▋  | 305/400 [03:21<00:26,  3.63it/s] 76%|███████▋  | 306/400 [03:22<00:25,  3.64it/s] 77%|███████▋  | 307/400 [03:22<00:25,  3.64it/s] 77%|███████▋  | 308/400 [03:22<00:25,  3.65it/s] 77%|███████▋  | 309/400 [03:22<00:28,  3.19it/s] 78%|███████▊  | 310/400 [03:23<00:27,  3.32it/s] 78%|███████▊  | 311/400 [03:23<00:26,  3.41it/s] 78%|███████▊  | 312/400 [03:23<00:25,  3.48it/s] 78%|███████▊  | 313/400 [03:24<00:24,  3.53it/s] 78%|███████▊  | 314/400 [03:24<00:24,  3.56it/s] 79%|███████▉  | 315/400 [03:24<00:23,  3.58it/s] 79%|███████▉  | 316/400 [03:24<00:23,  3.60it/s] 79%|███████▉  | 317/400 [03:25<00:22,  3.62it/s] 80%|███████▉  | 318/400 [03:25<00:22,  3.62it/s] 80%|███████▉  | 319/400 [03:25<00:22,  3.63it/s] 80%|████████  | 320/400 [03:26<00:22,  3.50it/s][INFO|trainer.py:2140] 2023-08-28 08:33:41,404 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:33:41,405 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 08:33:41,405 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 9.8495, 'eval_samples_per_second': 355.043, 'eval_steps_per_second': 44.469, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.55it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.30it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.69it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.96it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.55it/s][A
  7%|▋         | 32/438 [00:00<00:08, 46.27it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.64it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.19it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.23it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.22it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.33it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.20it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.47it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.57it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.54it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.23it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 45.12it/s][A
 21%|██        | 92/438 [00:02<00:07, 45.13it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 45.25it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 45.13it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 45.19it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.28it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.39it/s][A
 28%|██▊       | 122/438 [00:02<00:06, 45.42it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 45.35it/s][A
 30%|███       | 132/438 [00:02<00:07, 41.02it/s][A
 31%|███▏      | 137/438 [00:03<00:07, 42.38it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 43.32it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.06it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.48it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.80it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.02it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 45.12it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.73it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.69it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.90it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 45.06it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.31it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.41it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.48it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.47it/s][A
 48%|████▊     | 212/438 [00:04<00:04, 45.25it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.01it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.89it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.99it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 45.13it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 45.27it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 45.39it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 45.37it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 45.39it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 45.19it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.95it/s][A
 61%|██████    | 267/438 [00:05<00:04, 41.24it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 42.57it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 43.45it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.04it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.50it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.88it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 45.13it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 45.13it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.74it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.61it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.78it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 45.02it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 45.20it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 45.34it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 45.45it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 45.47it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 45.26it/s][A
 80%|████████  | 352/438 [00:07<00:01, 45.05it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 44.90it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.86it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.17it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 45.28it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 45.35it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 45.14it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 45.28it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 45.08it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 45.00it/s][A
 92%|█████████▏| 402/438 [00:08<00:00, 39.33it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 41.06it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 42.31it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 43.18it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.87it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.31it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.69it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.93it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 44.93it/s][A 80%|████████  | 320/400 [03:35<00:22,  3.50it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:33:51,650 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320
[INFO|configuration_utils.py:351] 2023-08-28 08:33:52,406 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:34:19,568 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:34:20,810 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:34:21,094 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320/special_tokens_map.json
 80%|████████  | 321/400 [04:12<18:27, 14.02s/it] 80%|████████  | 322/400 [04:12<12:54,  9.93s/it] 81%|████████  | 323/400 [04:12<09:01,  7.04s/it] 81%|████████  | 324/400 [04:13<06:20,  5.01s/it] 81%|████████▏ | 325/400 [04:13<04:29,  3.59s/it] 82%|████████▏ | 326/400 [04:13<03:12,  2.60s/it] 82%|████████▏ | 327/400 [04:13<02:18,  1.90s/it] 82%|████████▏ | 328/400 [04:14<01:41,  1.41s/it] 82%|████████▏ | 329/400 [04:14<01:16,  1.07s/it] 82%|████████▎ | 330/400 [04:14<00:58,  1.20it/s] 83%|████████▎ | 331/400 [04:14<00:46,  1.50it/s] 83%|████████▎ | 332/400 [04:15<00:37,  1.82it/s] 83%|████████▎ | 333/400 [04:15<00:33,  2.02it/s] 84%|████████▎ | 334/400 [04:15<00:28,  2.33it/s] 84%|████████▍ | 335/400 [04:16<00:24,  2.60it/s] 84%|████████▍ | 336/400 [04:16<00:22,  2.84it/s] 84%|████████▍ | 337/400 [04:16<00:20,  3.03it/s] 84%|████████▍ | 338/400 [04:17<00:19,  3.18it/s] 85%|████████▍ | 339/400 [04:17<00:18,  3.29it/s] 85%|████████▌ | 340/400 [04:17<00:17,  3.38it/s] 85%|████████▌ | 341/400 [04:17<00:17,  3.45it/s] 86%|████████▌ | 342/400 [04:18<00:16,  3.49it/s] 86%|████████▌ | 343/400 [04:18<00:16,  3.52it/s] 86%|████████▌ | 344/400 [04:18<00:17,  3.14it/s] 86%|████████▋ | 345/400 [04:19<00:16,  3.26it/s] 86%|████████▋ | 346/400 [04:19<00:16,  3.36it/s] 87%|████████▋ | 347/400 [04:19<00:15,  3.43it/s] 87%|████████▋ | 348/400 [04:19<00:14,  3.47it/s] 87%|████████▋ | 349/400 [04:20<00:14,  3.51it/s] 88%|████████▊ | 350/400 [04:20<00:14,  3.54it/s] 88%|████████▊ | 351/400 [04:20<00:13,  3.56it/s] 88%|████████▊ | 352/400 [04:21<00:13,  3.57it/s] 88%|████████▊ | 353/400 [04:21<00:13,  3.58it/s] 88%|████████▊ | 354/400 [04:21<00:12,  3.59it/s] 89%|████████▉ | 355/400 [04:21<00:13,  3.26it/s] 89%|████████▉ | 356/400 [04:22<00:13,  3.35it/s] 89%|████████▉ | 357/400 [04:22<00:12,  3.43it/s] 90%|████████▉ | 358/400 [04:22<00:12,  3.48it/s] 90%|████████▉ | 359/400 [04:23<00:11,  3.52it/s] 90%|█████████ | 360/400 [04:23<00:11,  3.54it/s] 90%|█████████ | 361/400 [04:23<00:10,  3.55it/s] 90%|█████████ | 362/400 [04:23<00:10,  3.57it/s] 91%|█████████ | 363/400 [04:24<00:10,  3.57it/s] 91%|█████████ | 364/400 [04:24<00:10,  3.58it/s] 91%|█████████▏| 365/400 [04:24<00:09,  3.58it/s] 92%|█████████▏| 366/400 [04:25<00:10,  3.29it/s] 92%|█████████▏| 367/400 [04:25<00:09,  3.37it/s] 92%|█████████▏| 368/400 [04:25<00:09,  3.44it/s] 92%|█████████▏| 369/400 [04:25<00:08,  3.48it/s] 92%|█████████▎| 370/400 [04:26<00:08,  3.52it/s] 93%|█████████▎| 371/400 [04:26<00:08,  3.54it/s] 93%|█████████▎| 372/400 [04:26<00:07,  3.56it/s] 93%|█████████▎| 373/400 [04:27<00:07,  3.57it/s] 94%|█████████▎| 374/400 [04:27<00:07,  3.58it/s] 94%|█████████▍| 375/400 [04:27<00:06,  3.58it/s] 94%|█████████▍| 376/400 [04:27<00:06,  3.59it/s] 94%|█████████▍| 377/400 [04:28<00:07,  3.28it/s] 94%|█████████▍| 378/400 [04:28<00:06,  3.37it/s] 95%|█████████▍| 379/400 [04:28<00:06,  3.43it/s] 95%|█████████▌| 380/400 [04:29<00:05,  3.48it/s] 95%|█████████▌| 381/400 [04:29<00:05,  3.51it/s] 96%|█████████▌| 382/400 [04:29<00:05,  3.53it/s] 96%|█████████▌| 383/400 [04:29<00:04,  3.54it/s] 96%|█████████▌| 384/400 [04:30<00:04,  3.55it/s] 96%|█████████▋| 385/400 [04:30<00:04,  3.56it/s] 96%|█████████▋| 386/400 [04:30<00:03,  3.57it/s] 97%|█████████▋| 387/400 [04:31<00:03,  3.57it/s] 97%|█████████▋| 388/400 [04:31<00:03,  3.16it/s] 97%|█████████▋| 389/400 [04:31<00:03,  3.28it/s] 98%|█████████▊| 390/400 [04:31<00:02,  3.36it/s] 98%|█████████▊| 391/400 [04:32<00:02,  3.42it/s] 98%|█████████▊| 392/400 [04:32<00:02,  3.46it/s] 98%|█████████▊| 393/400 [04:32<00:02,  3.50it/s] 98%|█████████▊| 394/400 [04:33<00:01,  3.52it/s] 99%|█████████▉| 395/400 [04:33<00:01,  3.54it/s] 99%|█████████▉| 396/400 [04:33<00:01,  3.55it/s] 99%|█████████▉| 397/400 [04:33<00:00,  3.55it/s]100%|█████████▉| 398/400 [04:34<00:00,  3.56it/s]100%|█████████▉| 399/400 [04:34<00:00,  3.16it/s]100%|██████████| 400/400 [04:34<00:00,  3.55it/s][INFO|trainer.py:2140] 2023-08-28 08:34:50,216 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:34:50,216 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 08:34:50,216 >>   Batch size = 8
{'eval_loss': 1.0607956647872925, 'eval_runtime': 9.7856, 'eval_samples_per_second': 357.36, 'eval_steps_per_second': 44.759, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.23it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.11it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.52it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.92it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.39it/s][A
  7%|▋         | 32/438 [00:00<00:08, 46.00it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.39it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.88it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.99it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.10it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.17it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.24it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.27it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.36it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.29it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.95it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.86it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.81it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.78it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 45.00it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 45.21it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.32it/s][A
 27%|██▋       | 117/438 [00:02<00:08, 39.27it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 40.93it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 42.24it/s][A
 30%|███       | 132/438 [00:02<00:07, 43.18it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.90it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.39it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.65it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.82it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.51it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.41it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.48it/s][A
 39%|███▉      | 172/438 [00:03<00:07, 36.33it/s][A
 41%|████      | 178/438 [00:04<00:06, 40.32it/s][A
 42%|████▏     | 183/438 [00:04<00:06, 41.69it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 42.77it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 43.54it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 44.10it/s][A
 46%|████▋     | 203/438 [00:04<00:07, 31.81it/s][A
 47%|████▋     | 208/438 [00:04<00:06, 34.91it/s][A
 49%|████▊     | 213/438 [00:04<00:05, 37.53it/s][A
 50%|████▉     | 218/438 [00:05<00:05, 39.54it/s][A
 51%|█████     | 223/438 [00:05<00:05, 41.22it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 42.35it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 43.27it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 43.82it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 43.77it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 43.84it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.10it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.35it/s][A
 60%|██████    | 263/438 [00:06<00:03, 44.72it/s][A
 61%|██████    | 268/438 [00:06<00:03, 44.92it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 45.12it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 45.17it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 45.15it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.92it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.67it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.66it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 44.68it/s][A
 70%|███████   | 308/438 [00:07<00:02, 44.98it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 44.98it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.19it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.05it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.15it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 38.08it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 40.06it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 41.53it/s][A
 79%|███████▉  | 348/438 [00:08<00:02, 42.55it/s][A
 81%|████████  | 353/438 [00:08<00:01, 43.34it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 43.91it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 44.38it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 44.66it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.26it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 44.28it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 44.45it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 44.72it/s][A
 90%|████████▉ | 393/438 [00:09<00:01, 44.89it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 45.02it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 45.07it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 45.13it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 45.02it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.76it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 44.63it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 44.48it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 44.89it/s][A
100%|██████████| 438/438 [00:10<00:00, 45.18it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 45.18it/s][A100%|██████████| 400/400 [04:44<00:00,  3.55it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 08:35:00,691 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400
[INFO|configuration_utils.py:351] 2023-08-28 08:35:01,290 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:35:23,068 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:35:24,066 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:35:24,394 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 08:35:28,395 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 08:35:28,395 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80 (score: 1.0607956647872925).
                                                 100%|██████████| 400/400 [05:33<00:00,  3.55it/s]100%|██████████| 400/400 [05:33<00:00,  1.20it/s]
[INFO|trainer.py:1894] 2023-08-28 08:35:48,533 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 08:35:48,754 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 08:35:53,618 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 08:35:53,895 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 08:35:54,041 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:35:54,812 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:35:54,812 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:35:54,812 >>   train_loss               =        nan
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:35:54,812 >>   train_runtime            = 0:05:33.06
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:35:54,812 >>   train_samples            =       5100
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:35:54,812 >>   train_samples_per_second =     76.561
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:35:54,812 >>   train_steps_per_second   =      1.201
{'eval_loss': 1.0607956647872925, 'eval_runtime': 10.046, 'eval_samples_per_second': 348.1, 'eval_steps_per_second': 43.6, 'epoch': 5.0}
{'train_runtime': 333.0693, 'train_samples_per_second': 76.561, 'train_steps_per_second': 1.201, 'train_loss': nan, 'epoch': 5.0}
08/28/2023 08:35:55 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 08:35:55,199 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 08:35:55,199 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 08:35:55,199 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 56.37it/s]  3%|▎         | 12/438 [00:00<00:08, 49.72it/s]  4%|▍         | 18/438 [00:00<00:08, 47.82it/s]  5%|▌         | 23/438 [00:00<00:08, 47.07it/s]  6%|▋         | 28/438 [00:00<00:08, 46.53it/s]  8%|▊         | 33/438 [00:00<00:08, 46.19it/s]  9%|▊         | 38/438 [00:00<00:08, 45.99it/s] 10%|▉         | 43/438 [00:00<00:08, 45.58it/s] 11%|█         | 48/438 [00:01<00:08, 45.19it/s] 12%|█▏        | 53/438 [00:01<00:08, 45.12it/s] 13%|█▎        | 58/438 [00:01<00:08, 45.21it/s] 14%|█▍        | 63/438 [00:01<00:08, 45.35it/s] 16%|█▌        | 68/438 [00:01<00:08, 45.48it/s] 17%|█▋        | 73/438 [00:01<00:08, 45.57it/s] 18%|█▊        | 78/438 [00:01<00:07, 45.71it/s] 19%|█▉        | 83/438 [00:01<00:07, 45.77it/s] 20%|██        | 88/438 [00:01<00:07, 45.54it/s] 21%|██        | 93/438 [00:02<00:07, 45.21it/s] 22%|██▏       | 98/438 [00:02<00:07, 45.14it/s] 24%|██▎       | 103/438 [00:02<00:07, 45.11it/s] 25%|██▍       | 108/438 [00:02<00:07, 45.24it/s] 26%|██▌       | 113/438 [00:02<00:07, 45.30it/s] 27%|██▋       | 118/438 [00:02<00:07, 45.53it/s] 28%|██▊       | 123/438 [00:02<00:06, 45.59it/s] 29%|██▉       | 128/438 [00:02<00:06, 45.68it/s] 30%|███       | 133/438 [00:02<00:06, 45.57it/s] 32%|███▏      | 138/438 [00:03<00:06, 45.26it/s] 33%|███▎      | 143/438 [00:03<00:06, 45.13it/s] 34%|███▍      | 148/438 [00:03<00:06, 45.16it/s] 35%|███▍      | 153/438 [00:03<00:06, 45.28it/s] 36%|███▌      | 158/438 [00:03<00:06, 45.34it/s] 37%|███▋      | 163/438 [00:03<00:06, 45.48it/s] 38%|███▊      | 168/438 [00:03<00:05, 45.64it/s] 39%|███▉      | 173/438 [00:03<00:05, 45.57it/s] 41%|████      | 178/438 [00:03<00:05, 45.51it/s] 42%|████▏     | 183/438 [00:04<00:05, 45.31it/s] 43%|████▎     | 188/438 [00:04<00:05, 42.85it/s] 44%|████▍     | 193/438 [00:04<00:05, 43.64it/s] 45%|████▌     | 198/438 [00:04<00:05, 44.27it/s] 46%|████▋     | 203/438 [00:04<00:05, 44.57it/s] 47%|████▋     | 208/438 [00:04<00:05, 44.86it/s] 49%|████▊     | 213/438 [00:04<00:04, 45.17it/s] 50%|████▉     | 218/438 [00:04<00:04, 45.37it/s] 51%|█████     | 223/438 [00:04<00:04, 45.33it/s] 52%|█████▏    | 228/438 [00:05<00:04, 44.97it/s] 53%|█████▎    | 233/438 [00:05<00:04, 44.96it/s] 54%|█████▍    | 238/438 [00:05<00:04, 44.98it/s] 55%|█████▌    | 243/438 [00:05<00:04, 45.28it/s] 57%|█████▋    | 248/438 [00:05<00:04, 45.36it/s] 58%|█████▊    | 253/438 [00:05<00:04, 45.54it/s] 59%|█████▉    | 258/438 [00:05<00:03, 45.59it/s] 60%|██████    | 263/438 [00:05<00:03, 45.50it/s] 61%|██████    | 268/438 [00:05<00:03, 45.29it/s] 62%|██████▏   | 273/438 [00:06<00:03, 45.04it/s] 63%|██████▎   | 278/438 [00:06<00:03, 44.96it/s] 65%|██████▍   | 283/438 [00:06<00:03, 45.03it/s] 66%|██████▌   | 288/438 [00:06<00:03, 45.19it/s] 67%|██████▋   | 293/438 [00:06<00:03, 45.29it/s] 68%|██████▊   | 298/438 [00:06<00:03, 45.52it/s] 69%|██████▉   | 303/438 [00:06<00:02, 45.60it/s] 70%|███████   | 308/438 [00:06<00:02, 45.65it/s] 71%|███████▏  | 313/438 [00:06<00:02, 45.41it/s] 73%|███████▎  | 318/438 [00:07<00:02, 45.18it/s] 74%|███████▎  | 323/438 [00:07<00:02, 44.93it/s] 75%|███████▍  | 328/438 [00:07<00:02, 45.02it/s] 76%|███████▌  | 333/438 [00:07<00:02, 45.18it/s] 77%|███████▋  | 338/438 [00:07<00:02, 45.33it/s] 78%|███████▊  | 343/438 [00:07<00:02, 45.45it/s] 79%|███████▉  | 348/438 [00:07<00:01, 45.57it/s] 81%|████████  | 353/438 [00:07<00:01, 45.55it/s] 82%|████████▏ | 358/438 [00:07<00:01, 45.38it/s] 83%|████████▎ | 363/438 [00:07<00:01, 45.16it/s] 84%|████████▍ | 368/438 [00:08<00:01, 45.01it/s] 85%|████████▌ | 373/438 [00:08<00:01, 45.01it/s] 86%|████████▋ | 378/438 [00:08<00:01, 45.18it/s] 87%|████████▋ | 383/438 [00:08<00:01, 45.35it/s] 89%|████████▊ | 388/438 [00:08<00:01, 45.52it/s] 90%|████████▉ | 393/438 [00:08<00:00, 45.51it/s] 91%|█████████ | 398/438 [00:08<00:00, 45.52it/s] 92%|█████████▏| 403/438 [00:08<00:00, 45.29it/s] 93%|█████████▎| 408/438 [00:08<00:00, 45.17it/s] 94%|█████████▍| 413/438 [00:09<00:00, 45.03it/s] 95%|█████████▌| 418/438 [00:09<00:00, 45.02it/s] 97%|█████████▋| 423/438 [00:09<00:00, 45.13it/s] 98%|█████████▊| 428/438 [00:09<00:00, 45.25it/s] 99%|█████████▉| 433/438 [00:09<00:00, 45.45it/s]100%|██████████| 438/438 [00:09<00:00, 45.61it/s]100%|██████████| 438/438 [00:09<00:00, 45.38it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 08:36:04,871 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:36:04,871 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:36:04,871 >>   eval_loss               =     1.0608
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:36:04,871 >>   eval_runtime            = 0:00:09.67
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:36:04,871 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:36:04,871 >>   eval_samples_per_second =    361.579
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:36:04,871 >>   eval_steps_per_second   =     45.288
[INFO|trainer_pt_utils.py:913] 2023-08-28 08:36:04,871 >>   perplexity              =     2.8887
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,345 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:19,433 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:36:20,370 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:36:20,371 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:36:20,744 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:36:21,960 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:36:21,960 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:24,163 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:24,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:24,268 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:24,269 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:36:24,269 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:36:25,206 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:36:25,207 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:36:25,610 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:36:25,961 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:36:25,961 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-240
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-320
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-80
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-400
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/generator/iter5/model/checkpoint-160
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.42it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.66it/s]Extractor Predicting: 5it [00:03,  1.68it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.66it/s]Extractor Predicting: 8it [00:04,  1.74it/s]Extractor Predicting: 9it [00:05,  1.72it/s]Extractor Predicting: 10it [00:06,  1.69it/s]Extractor Predicting: 11it [00:06,  1.70it/s]Extractor Predicting: 12it [00:07,  1.60it/s]Extractor Predicting: 13it [00:07,  1.60it/s]Extractor Predicting: 14it [00:08,  1.65it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.67it/s]Extractor Predicting: 17it [00:10,  1.60it/s]Extractor Predicting: 18it [00:10,  1.64it/s]Extractor Predicting: 19it [00:11,  1.66it/s]Extractor Predicting: 20it [00:12,  1.69it/s]Extractor Predicting: 21it [00:12,  1.72it/s]Extractor Predicting: 22it [00:13,  1.74it/s]Extractor Predicting: 23it [00:13,  1.67it/s]Extractor Predicting: 24it [00:14,  1.75it/s]Extractor Predicting: 25it [00:14,  1.78it/s]Extractor Predicting: 26it [00:15,  1.76it/s]Extractor Predicting: 27it [00:16,  1.74it/s]Extractor Predicting: 28it [00:16,  1.75it/s]Extractor Predicting: 29it [00:17,  1.64it/s]Extractor Predicting: 30it [00:17,  1.64it/s]Extractor Predicting: 31it [00:18,  1.65it/s]Extractor Predicting: 32it [00:19,  1.61it/s]Extractor Predicting: 33it [00:19,  1.56it/s]Extractor Predicting: 34it [00:20,  1.62it/s]Extractor Predicting: 35it [00:21,  1.65it/s]Extractor Predicting: 36it [00:21,  1.62it/s]Extractor Predicting: 37it [00:22,  1.65it/s]Extractor Predicting: 38it [00:22,  1.61it/s]Extractor Predicting: 39it [00:23,  1.58it/s]Extractor Predicting: 40it [00:24,  1.62it/s]Extractor Predicting: 41it [00:24,  1.62it/s]Extractor Predicting: 42it [00:25,  1.64it/s]Extractor Predicting: 43it [00:26,  1.56it/s]Extractor Predicting: 44it [00:26,  1.60it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:27,  1.63it/s]Extractor Predicting: 47it [00:28,  1.62it/s]Extractor Predicting: 48it [00:29,  1.60it/s]Extractor Predicting: 49it [00:29,  1.60it/s]Extractor Predicting: 50it [00:30,  1.64it/s]Extractor Predicting: 51it [00:30,  1.68it/s]Extractor Predicting: 52it [00:31,  1.66it/s]Extractor Predicting: 53it [00:32,  1.57it/s]Extractor Predicting: 54it [00:32,  1.63it/s]Extractor Predicting: 55it [00:33,  1.62it/s]Extractor Predicting: 56it [00:34,  1.63it/s]Extractor Predicting: 57it [00:34,  1.67it/s]Extractor Predicting: 58it [00:35,  1.60it/s]Extractor Predicting: 59it [00:35,  1.59it/s]Extractor Predicting: 60it [00:36,  1.60it/s]Extractor Predicting: 61it [00:37,  1.59it/s]Extractor Predicting: 62it [00:37,  1.65it/s]Extractor Predicting: 63it [00:38,  1.62it/s]Extractor Predicting: 64it [00:39,  1.61it/s]Extractor Predicting: 65it [00:39,  1.67it/s]Extractor Predicting: 66it [00:40,  1.71it/s]Extractor Predicting: 67it [00:40,  1.71it/s]Extractor Predicting: 68it [00:41,  1.73it/s]Extractor Predicting: 69it [00:41,  1.67it/s]Extractor Predicting: 70it [00:42,  1.72it/s]Extractor Predicting: 71it [00:43,  1.59it/s]Extractor Predicting: 72it [00:43,  1.66it/s]Extractor Predicting: 73it [00:44,  1.66it/s]Extractor Predicting: 74it [00:45,  1.59it/s]Extractor Predicting: 75it [00:45,  1.64it/s]Extractor Predicting: 76it [00:46,  1.68it/s]Extractor Predicting: 77it [00:46,  1.74it/s]Extractor Predicting: 78it [00:47,  1.71it/s]Extractor Predicting: 79it [00:47,  1.76it/s]Extractor Predicting: 80it [00:48,  1.71it/s]Extractor Predicting: 81it [00:49,  1.72it/s]Extractor Predicting: 82it [00:49,  1.69it/s]Extractor Predicting: 83it [00:50,  1.68it/s]Extractor Predicting: 84it [00:50,  1.67it/s]Extractor Predicting: 85it [00:51,  1.65it/s]Extractor Predicting: 86it [00:52,  1.66it/s]Extractor Predicting: 87it [00:52,  1.68it/s]Extractor Predicting: 88it [00:53,  1.68it/s]Extractor Predicting: 89it [00:53,  1.59it/s]Extractor Predicting: 90it [00:54,  1.64it/s]Extractor Predicting: 91it [00:55,  1.63it/s]Extractor Predicting: 92it [00:55,  1.65it/s]Extractor Predicting: 93it [00:56,  1.65it/s]Extractor Predicting: 94it [00:56,  1.59it/s]Extractor Predicting: 95it [00:57,  1.65it/s]Extractor Predicting: 96it [00:58,  1.65it/s]Extractor Predicting: 97it [00:58,  1.62it/s]Extractor Predicting: 98it [00:59,  1.62it/s]Extractor Predicting: 99it [01:00,  1.55it/s]Extractor Predicting: 100it [01:00,  1.59it/s]Extractor Predicting: 101it [01:01,  1.65it/s]Extractor Predicting: 102it [01:01,  1.66it/s]Extractor Predicting: 103it [01:02,  1.66it/s]Extractor Predicting: 104it [01:03,  1.57it/s]Extractor Predicting: 105it [01:03,  1.60it/s]Extractor Predicting: 106it [01:04,  1.61it/s]Extractor Predicting: 107it [01:05,  1.60it/s]Extractor Predicting: 108it [01:05,  1.63it/s]Extractor Predicting: 109it [01:06,  1.53it/s]Extractor Predicting: 110it [01:06,  1.57it/s]Extractor Predicting: 111it [01:07,  1.60it/s]Extractor Predicting: 112it [01:08,  1.65it/s]Extractor Predicting: 113it [01:08,  1.62it/s]Extractor Predicting: 114it [01:09,  1.59it/s]Extractor Predicting: 115it [01:10,  1.60it/s]Extractor Predicting: 116it [01:10,  1.59it/s]Extractor Predicting: 117it [01:11,  1.60it/s]Extractor Predicting: 118it [01:11,  1.63it/s]Extractor Predicting: 119it [01:12,  1.56it/s]Extractor Predicting: 120it [01:13,  1.57it/s]Extractor Predicting: 121it [01:13,  1.61it/s]Extractor Predicting: 122it [01:14,  1.64it/s]Extractor Predicting: 123it [01:14,  1.64it/s]Extractor Predicting: 124it [01:15,  1.59it/s]Extractor Predicting: 125it [01:16,  1.59it/s]Extractor Predicting: 126it [01:16,  1.61it/s]Extractor Predicting: 127it [01:17,  1.66it/s]Extractor Predicting: 128it [01:18,  1.65it/s]Extractor Predicting: 129it [01:18,  1.56it/s]Extractor Predicting: 130it [01:19,  1.59it/s]Extractor Predicting: 131it [01:20,  1.58it/s]Extractor Predicting: 132it [01:20,  1.60it/s]Extractor Predicting: 133it [01:21,  1.57it/s]Extractor Predicting: 134it [01:21,  1.61it/s]Extractor Predicting: 135it [01:22,  1.58it/s]Extractor Predicting: 136it [01:23,  1.55it/s]Extractor Predicting: 137it [01:23,  1.61it/s]Extractor Predicting: 138it [01:24,  1.55it/s]Extractor Predicting: 139it [01:25,  1.57it/s]Extractor Predicting: 140it [01:25,  1.61it/s]Extractor Predicting: 141it [01:26,  1.61it/s]Extractor Predicting: 142it [01:26,  1.58it/s]Extractor Predicting: 143it [01:27,  1.53it/s]Extractor Predicting: 144it [01:28,  1.59it/s]Extractor Predicting: 145it [01:28,  1.79it/s]Extractor Predicting: 145it [01:28,  1.64it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:21,218 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:21,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:21,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:21,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:21,276 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:38:22,108 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:38:22,109 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:38:22,519 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:38:23,735 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:38:23,735 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:26,258 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:26,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:26,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:26,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:38:26,339 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:38:26,938 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:38:26,939 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:38:27,316 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:38:27,588 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:38:27,588 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.58it/s]Extractor Predicting: 3it [00:01,  1.65it/s]Extractor Predicting: 4it [00:02,  1.68it/s]Extractor Predicting: 5it [00:03,  1.70it/s]Extractor Predicting: 6it [00:03,  1.62it/s]Extractor Predicting: 7it [00:04,  1.64it/s]Extractor Predicting: 8it [00:04,  1.63it/s]Extractor Predicting: 9it [00:05,  1.68it/s]Extractor Predicting: 10it [00:05,  1.74it/s]Extractor Predicting: 11it [00:06,  1.74it/s]Extractor Predicting: 12it [00:07,  1.65it/s]Extractor Predicting: 13it [00:07,  1.66it/s]Extractor Predicting: 14it [00:08,  1.69it/s]Extractor Predicting: 15it [00:09,  1.66it/s]Extractor Predicting: 16it [00:09,  1.68it/s]Extractor Predicting: 17it [00:10,  1.62it/s]Extractor Predicting: 18it [00:10,  1.65it/s]Extractor Predicting: 19it [00:11,  1.65it/s]Extractor Predicting: 20it [00:12,  1.64it/s]Extractor Predicting: 21it [00:12,  1.67it/s]Extractor Predicting: 22it [00:13,  1.63it/s]Extractor Predicting: 23it [00:13,  1.66it/s]Extractor Predicting: 24it [00:14,  1.64it/s]Extractor Predicting: 25it [00:15,  1.66it/s]Extractor Predicting: 26it [00:15,  1.68it/s]Extractor Predicting: 27it [00:16,  1.65it/s]Extractor Predicting: 28it [00:16,  1.65it/s]Extractor Predicting: 29it [00:17,  1.66it/s]Extractor Predicting: 30it [00:18,  1.69it/s]Extractor Predicting: 31it [00:18,  1.72it/s]Extractor Predicting: 32it [00:19,  1.71it/s]Extractor Predicting: 33it [00:19,  1.65it/s]Extractor Predicting: 34it [00:20,  1.66it/s]Extractor Predicting: 35it [00:21,  1.69it/s]Extractor Predicting: 36it [00:21,  1.72it/s]Extractor Predicting: 37it [00:22,  1.75it/s]Extractor Predicting: 38it [00:22,  1.75it/s]Extractor Predicting: 39it [00:23,  1.70it/s]Extractor Predicting: 40it [00:23,  1.67it/s]Extractor Predicting: 41it [00:24,  1.68it/s]Extractor Predicting: 42it [00:25,  1.69it/s]Extractor Predicting: 43it [00:25,  1.66it/s]Extractor Predicting: 44it [00:26,  1.60it/s]Extractor Predicting: 45it [00:27,  1.62it/s]Extractor Predicting: 46it [00:27,  1.69it/s]Extractor Predicting: 47it [00:28,  1.71it/s]Extractor Predicting: 48it [00:28,  1.72it/s]Extractor Predicting: 49it [00:29,  1.74it/s]Extractor Predicting: 50it [00:29,  1.64it/s]Extractor Predicting: 51it [00:30,  1.66it/s]Extractor Predicting: 52it [00:31,  1.72it/s]Extractor Predicting: 53it [00:31,  1.71it/s]Extractor Predicting: 54it [00:32,  1.72it/s]Extractor Predicting: 55it [00:32,  1.71it/s]Extractor Predicting: 56it [00:33,  1.59it/s]Extractor Predicting: 57it [00:34,  1.60it/s]Extractor Predicting: 58it [00:34,  1.64it/s]Extractor Predicting: 59it [00:35,  1.51it/s]Extractor Predicting: 60it [00:36,  1.57it/s]Extractor Predicting: 61it [00:36,  1.53it/s]Extractor Predicting: 62it [00:37,  1.60it/s]Extractor Predicting: 63it [00:37,  1.61it/s]Extractor Predicting: 64it [00:38,  1.66it/s]Extractor Predicting: 65it [00:39,  1.66it/s]Extractor Predicting: 66it [00:39,  1.61it/s]Extractor Predicting: 67it [00:40,  1.65it/s]Extractor Predicting: 68it [00:40,  1.69it/s]Extractor Predicting: 69it [00:41,  1.72it/s]Extractor Predicting: 70it [00:42,  1.73it/s]Extractor Predicting: 71it [00:42,  1.74it/s]Extractor Predicting: 72it [00:43,  1.65it/s]Extractor Predicting: 73it [00:43,  1.67it/s]Extractor Predicting: 74it [00:44,  1.66it/s]Extractor Predicting: 75it [00:45,  1.63it/s]Extractor Predicting: 76it [00:45,  1.67it/s]Extractor Predicting: 77it [00:46,  1.64it/s]Extractor Predicting: 78it [00:46,  1.66it/s]Extractor Predicting: 79it [00:47,  1.69it/s]Extractor Predicting: 80it [00:48,  1.73it/s]Extractor Predicting: 81it [00:48,  1.73it/s]Extractor Predicting: 82it [00:49,  1.73it/s]Extractor Predicting: 83it [00:49,  1.65it/s]Extractor Predicting: 84it [00:50,  1.69it/s]Extractor Predicting: 85it [00:50,  1.71it/s]Extractor Predicting: 86it [00:51,  1.76it/s]Extractor Predicting: 87it [00:52,  1.77it/s]Extractor Predicting: 88it [00:52,  1.75it/s]Extractor Predicting: 89it [00:53,  1.61it/s]Extractor Predicting: 90it [00:53,  1.67it/s]Extractor Predicting: 91it [00:54,  1.72it/s]Extractor Predicting: 92it [00:55,  1.69it/s]Extractor Predicting: 93it [00:55,  1.67it/s]Extractor Predicting: 94it [00:56,  1.59it/s]Extractor Predicting: 95it [00:56,  1.63it/s]Extractor Predicting: 96it [00:57,  1.54it/s]Extractor Predicting: 97it [00:58,  1.59it/s]Extractor Predicting: 98it [00:58,  1.60it/s]Extractor Predicting: 99it [00:59,  1.65it/s]Extractor Predicting: 100it [01:00,  1.67it/s]Extractor Predicting: 101it [01:00,  1.59it/s]Extractor Predicting: 102it [01:01,  1.57it/s]Extractor Predicting: 103it [01:01,  1.64it/s]Extractor Predicting: 104it [01:02,  1.65it/s]Extractor Predicting: 105it [01:03,  1.63it/s]Extractor Predicting: 106it [01:03,  1.60it/s]Extractor Predicting: 107it [01:04,  1.61it/s]Extractor Predicting: 108it [01:05,  1.67it/s]Extractor Predicting: 109it [01:05,  1.69it/s]Extractor Predicting: 110it [01:06,  1.74it/s]Extractor Predicting: 111it [01:06,  1.78it/s]Extractor Predicting: 112it [01:07,  1.66it/s]Extractor Predicting: 113it [01:07,  1.67it/s]Extractor Predicting: 114it [01:08,  1.66it/s]Extractor Predicting: 115it [01:09,  1.68it/s]Extractor Predicting: 116it [01:09,  1.71it/s]Extractor Predicting: 117it [01:10,  1.67it/s]Extractor Predicting: 118it [01:10,  1.67it/s]Extractor Predicting: 119it [01:11,  1.69it/s]Extractor Predicting: 120it [01:12,  1.69it/s]Extractor Predicting: 121it [01:12,  1.70it/s]Extractor Predicting: 122it [01:13,  1.74it/s]Extractor Predicting: 123it [01:13,  1.66it/s]Extractor Predicting: 124it [01:14,  1.68it/s]Extractor Predicting: 125it [01:15,  1.67it/s]Extractor Predicting: 126it [01:15,  1.66it/s]Extractor Predicting: 127it [01:16,  1.68it/s]Extractor Predicting: 128it [01:16,  1.59it/s]Extractor Predicting: 129it [01:17,  1.66it/s]Extractor Predicting: 130it [01:18,  1.67it/s]Extractor Predicting: 131it [01:18,  1.72it/s]Extractor Predicting: 132it [01:19,  1.74it/s]Extractor Predicting: 133it [01:19,  1.73it/s]Extractor Predicting: 134it [01:20,  1.62it/s]Extractor Predicting: 135it [01:21,  1.49it/s]Extractor Predicting: 136it [01:21,  1.55it/s]Extractor Predicting: 137it [01:22,  1.62it/s]Extractor Predicting: 138it [01:22,  1.65it/s]Extractor Predicting: 139it [01:23,  1.61it/s]Extractor Predicting: 140it [01:24,  1.63it/s]Extractor Predicting: 141it [01:24,  1.67it/s]Extractor Predicting: 142it [01:25,  1.65it/s]Extractor Predicting: 143it [01:26,  1.67it/s]Extractor Predicting: 144it [01:26,  1.62it/s]Extractor Predicting: 145it [01:26,  2.07it/s]Extractor Predicting: 145it [01:26,  1.67it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:12,695 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:12,763 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:12,764 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:12,764 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:12,764 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 08:40:13,977 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 08:40:13,978 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:40:14,735 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 08:40:15,929 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:40:16,006 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:19,393 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:19,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:19,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:19,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 08:40:19,493 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 08:40:20,219 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 08:40:20,220 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 08:40:20,845 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 08:40:21,005 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 08:40:21,005 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 1it [00:00,  1.48it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0,
  "recall": 0,
  "score": 0,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_train_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
Warn: we adopt CRF implemented by allennlp, please install it first before using CRF.
{'main_dual': {'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/', 'num_iter': 5, 'data_name': 'fewrel', 'split': 'unseen_5_seed_4', 'type': 'filtered', 'model_size': 'large', 'with_train': True, 'by_rel': False, 'rl_version': 'all', 'rescale_train': False, 'score_only_ext': True, 'limit': 5000, 'g_encoder_name': 'generate', 'num_gen_per_label': 500, 'diverse': False}}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', data_dir='outputs/wrapper/fewrel/unseen_5_seed_4/generator/data', model_name='gpt2', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:36<05:31, 36.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:52<03:13, 24.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [01:10<02:31, 21.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [01:27<01:58, 19.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:44<01:33, 18.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:59<01:09, 17.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [02:12<00:48, 16.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [02:29<00:32, 16.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:45<00:16, 16.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [03:06<00:00, 17.65s/it]Generating: 100%|██████████| 10/10 [03:06<00:00, 18.63s/it]
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 130, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 210, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 345, 'raw': 416}
{'target': 600, 'success': 369, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 439, 'raw': 544}
{'target': 600, 'success': 465, 'raw': 576}
{'target': 600, 'success': 491, 'raw': 608}
{'target': 600, 'success': 518, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 596, 'raw': 736}
{'target': 600, 'success': 625, 'raw': 768}
{'prompt': 'Relation : director .', 'success_rate': 0.8138020833333334, 'errors': {'', 'too many values to unpack (expected 2)', "('\\n', 'director', 'Scott Zandt', 'It was written and directed by Scott Zandt ( a.k.a .')", 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 326, 'raw': 384}
{'target': 600, 'success': 355, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 412, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 466, 'raw': 544}
{'target': 600, 'success': 494, 'raw': 576}
{'target': 600, 'success': 519, 'raw': 608}
{'target': 600, 'success': 546, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8505434782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 46, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 99, 'raw': 128}
{'target': 600, 'success': 123, 'raw': 160}
{'target': 600, 'success': 149, 'raw': 192}
{'target': 600, 'success': 170, 'raw': 224}
{'target': 600, 'success': 193, 'raw': 256}
{'target': 600, 'success': 218, 'raw': 288}
{'target': 600, 'success': 240, 'raw': 320}
{'target': 600, 'success': 267, 'raw': 352}
{'target': 600, 'success': 291, 'raw': 384}
{'target': 600, 'success': 311, 'raw': 416}
{'target': 600, 'success': 335, 'raw': 448}
{'target': 600, 'success': 359, 'raw': 480}
{'target': 600, 'success': 381, 'raw': 512}
{'target': 600, 'success': 405, 'raw': 544}
{'target': 600, 'success': 430, 'raw': 576}
{'target': 600, 'success': 453, 'raw': 608}
{'target': 600, 'success': 475, 'raw': 640}
{'target': 600, 'success': 499, 'raw': 672}
{'target': 600, 'success': 522, 'raw': 704}
{'target': 600, 'success': 545, 'raw': 736}
{'target': 600, 'success': 567, 'raw': 768}
{'target': 600, 'success': 592, 'raw': 800}
{'target': 600, 'success': 619, 'raw': 832}
{'prompt': 'Relation : mother .', 'success_rate': 0.7439903846153846, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 155, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 207, 'raw': 256}
{'target': 600, 'success': 234, 'raw': 288}
{'target': 600, 'success': 258, 'raw': 320}
{'target': 600, 'success': 279, 'raw': 352}
{'target': 600, 'success': 307, 'raw': 384}
{'target': 600, 'success': 334, 'raw': 416}
{'target': 600, 'success': 360, 'raw': 448}
{'target': 600, 'success': 389, 'raw': 480}
{'target': 600, 'success': 413, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 467, 'raw': 576}
{'target': 600, 'success': 489, 'raw': 608}
{'target': 600, 'success': 513, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 591, 'raw': 736}
{'target': 600, 'success': 616, 'raw': 768}
{'prompt': 'Relation : part of .', 'success_rate': 0.8020833333333334, 'errors': {'', '(\'1958 contest\', \'part of\', \'\', \'He was succeeded as Dutch representative at its 1958 contest by Johannes Eichhorn with " Allende en seine " .\')', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 25, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 76, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 124, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 175, 'raw': 224}
{'target': 600, 'success': 201, 'raw': 256}
{'target': 600, 'success': 227, 'raw': 288}
{'target': 600, 'success': 252, 'raw': 320}
{'target': 600, 'success': 278, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 331, 'raw': 416}
{'target': 600, 'success': 358, 'raw': 448}
{'target': 600, 'success': 387, 'raw': 480}
{'target': 600, 'success': 417, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 470, 'raw': 576}
{'target': 600, 'success': 494, 'raw': 608}
{'target': 600, 'success': 521, 'raw': 640}
{'target': 600, 'success': 545, 'raw': 672}
{'target': 600, 'success': 572, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 622, 'raw': 768}
{'prompt': 'Relation : residence .', 'success_rate': 0.8098958333333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the project became a part of a deal to turn " Ingress " into a mobile game . Head Entity : Ingress , Tail Entity : Ingress Studio .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 105, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 319, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 392, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 441, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 488, 'raw': 608}
{'target': 600, 'success': 511, 'raw': 640}
{'target': 600, 'success': 538, 'raw': 672}
{'target': 600, 'success': 566, 'raw': 704}
{'target': 600, 'success': 588, 'raw': 736}
{'target': 600, 'success': 613, 'raw': 768}
{'prompt': 'Relation : developer .', 'success_rate': 0.7981770833333334, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 139, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 250, 'raw': 288}
{'target': 600, 'success': 274, 'raw': 320}
{'target': 600, 'success': 303, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 361, 'raw': 416}
{'target': 600, 'success': 392, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 476, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 527, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 587, 'raw': 672}
{'target': 600, 'success': 617, 'raw': 704}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.8764204545454546, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n']
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n"]
['Relation : member of political party . Context : Later in the year , the party formed a new cabinet with two notable members of its cabinet : John Breenus and Barry Ritchie . Head Entity : John Breenus , Tail Entity : political party .\n', "Relation : member of political party . Context : After the death of former Prime Minister Paul VandenBerg , Sommers began a relationship with the SPD 's Peter Van Buren . Head Entity : Peter van Buren , Tail Entity : SPD .\n", "Relation : member of political party . Context : This was the first coalition government which was elected in 1998 , and led by then - Prime Minister Naguib Sawiris of the People 's Alliance . Head Entity : Naguib Sawiris , Tail Entity : People ' Alliance .\n"]
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 71, 'raw': 96}
{'target': 600, 'success': 96, 'raw': 128}
{'target': 600, 'success': 119, 'raw': 160}
{'target': 600, 'success': 144, 'raw': 192}
{'target': 600, 'success': 172, 'raw': 224}
{'target': 600, 'success': 194, 'raw': 256}
{'target': 600, 'success': 222, 'raw': 288}
{'target': 600, 'success': 249, 'raw': 320}
{'target': 600, 'success': 275, 'raw': 352}
{'target': 600, 'success': 306, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 359, 'raw': 448}
{'target': 600, 'success': 384, 'raw': 480}
{'target': 600, 'success': 408, 'raw': 512}
{'target': 600, 'success': 437, 'raw': 544}
{'target': 600, 'success': 464, 'raw': 576}
{'target': 600, 'success': 492, 'raw': 608}
{'target': 600, 'success': 519, 'raw': 640}
{'target': 600, 'success': 543, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 594, 'raw': 736}
{'target': 600, 'success': 619, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8059895833333334, 'errors': {'', '(\'Tom Blomkamp\', \'member of political party\', \'\', \'" My Life ( " ; ) is a 2015 English language English language documentary film directed by Tom Blomkamp and starring Emma Thompson and Tom Hardy .\')', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 161, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 294, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 346, 'raw': 416}
{'target': 600, 'success': 372, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 449, 'raw': 544}
{'target': 600, 'success': 478, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 534, 'raw': 640}
{'target': 600, 'success': 556, 'raw': 672}
{'target': 600, 'success': 582, 'raw': 704}
{'target': 600, 'success': 611, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8301630434782609, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 39, 'raw': 64}
{'target': 600, 'success': 58, 'raw': 96}
{'target': 600, 'success': 78, 'raw': 128}
{'target': 600, 'success': 96, 'raw': 160}
{'target': 600, 'success': 114, 'raw': 192}
{'target': 600, 'success': 134, 'raw': 224}
{'target': 600, 'success': 154, 'raw': 256}
{'target': 600, 'success': 173, 'raw': 288}
{'target': 600, 'success': 188, 'raw': 320}
{'target': 600, 'success': 207, 'raw': 352}
{'target': 600, 'success': 223, 'raw': 384}
{'target': 600, 'success': 245, 'raw': 416}
{'target': 600, 'success': 263, 'raw': 448}
{'target': 600, 'success': 276, 'raw': 480}
{'target': 600, 'success': 297, 'raw': 512}
{'target': 600, 'success': 318, 'raw': 544}
{'target': 600, 'success': 335, 'raw': 576}
{'target': 600, 'success': 359, 'raw': 608}
{'target': 600, 'success': 381, 'raw': 640}
{'target': 600, 'success': 398, 'raw': 672}
{'target': 600, 'success': 410, 'raw': 704}
{'target': 600, 'success': 431, 'raw': 736}
{'target': 600, 'success': 451, 'raw': 768}
{'target': 600, 'success': 470, 'raw': 800}
{'target': 600, 'success': 494, 'raw': 832}
{'target': 600, 'success': 516, 'raw': 864}
{'target': 600, 'success': 533, 'raw': 896}
{'target': 600, 'success': 552, 'raw': 928}
{'target': 600, 'success': 566, 'raw': 960}
{'target': 600, 'success': 584, 'raw': 992}
{'target': 600, 'success': 599, 'raw': 1024}
{'target': 600, 'success': 621, 'raw': 1056}
{'prompt': 'Relation : position held .', 'success_rate': 0.5880681818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/0.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/0_ext.jsonl'}}
estimate vocab size: 12256
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 12356, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_filtered_large/unseen_5_seed_4/extractor/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)
  return forward_call(*input, **kwargs)
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() profile_node %91 : int[] = prim::profile_ivalue[profile_failed="varying profile values"](%89)
 does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
Extractor Estimating: 1it [01:01, 61.21s/it]Extractor Estimating: 2it [01:02, 26.13s/it]Extractor Estimating: 3it [01:03, 14.51s/it]Extractor Estimating: 4it [01:04,  9.06s/it]Extractor Estimating: 5it [01:04,  6.02s/it]Extractor Estimating: 6it [01:05,  4.20s/it]Extractor Estimating: 7it [01:06,  3.07s/it]Extractor Estimating: 8it [01:07,  2.48s/it]Extractor Estimating: 9it [01:08,  1.91s/it]Extractor Estimating: 10it [01:08,  1.56s/it]Extractor Estimating: 11it [01:09,  1.27s/it]Extractor Estimating: 12it [01:10,  1.08s/it]Extractor Estimating: 13it [01:10,  1.05it/s]Extractor Estimating: 14it [01:11,  1.13it/s]Extractor Estimating: 15it [01:12,  1.20it/s]Extractor Estimating: 16it [01:12,  1.27it/s]Extractor Estimating: 17it [01:13,  1.32it/s]Extractor Estimating: 18it [01:14,  1.39it/s]Extractor Estimating: 19it [01:15,  1.15it/s]Extractor Estimating: 20it [01:16,  1.18it/s]Extractor Estimating: 21it [01:16,  1.24it/s]Extractor Estimating: 22it [01:17,  1.33it/s]Extractor Estimating: 23it [01:18,  1.38it/s]Extractor Estimating: 24it [01:18,  1.38it/s]Extractor Estimating: 25it [01:19,  1.31it/s]Extractor Estimating: 26it [01:20,  1.39it/s]Extractor Estimating: 27it [01:21,  1.44it/s]Extractor Estimating: 28it [01:21,  1.53it/s]Extractor Estimating: 29it [01:22,  1.55it/s]Extractor Estimating: 30it [01:23,  1.45it/s]Extractor Estimating: 31it [01:23,  1.51it/s]Extractor Estimating: 32it [01:24,  1.54it/s]Extractor Estimating: 33it [01:24,  1.54it/s]Extractor Estimating: 34it [01:25,  1.62it/s]Extractor Estimating: 35it [01:26,  1.59it/s]Extractor Estimating: 36it [01:26,  1.60it/s]Extractor Estimating: 37it [01:27,  1.59it/s]Extractor Estimating: 38it [01:27,  1.60it/s]Extractor Estimating: 39it [01:28,  1.58it/s]Extractor Estimating: 40it [01:29,  1.51it/s]Extractor Estimating: 41it [01:29,  1.51it/s]Extractor Estimating: 42it [01:30,  1.55it/s]Extractor Estimating: 43it [01:31,  1.60it/s]Extractor Estimating: 44it [01:31,  1.62it/s]Extractor Estimating: 45it [01:32,  1.62it/s]Extractor Estimating: 46it [01:32,  1.67it/s]Extractor Estimating: 47it [01:33,  1.63it/s]Extractor Estimating: 48it [01:34,  1.56it/s]Extractor Estimating: 49it [01:34,  1.62it/s]Extractor Estimating: 50it [01:35,  1.61it/s]Extractor Estimating: 51it [01:36,  1.56it/s]Extractor Estimating: 52it [01:36,  1.56it/s]Extractor Estimating: 53it [01:37,  1.60it/s]Extractor Estimating: 54it [01:37,  1.63it/s]Extractor Estimating: 55it [01:38,  1.53it/s]Extractor Estimating: 56it [01:39,  1.54it/s]Extractor Estimating: 57it [01:40,  1.56it/s]Extractor Estimating: 58it [01:40,  1.63it/s]Extractor Estimating: 59it [01:41,  1.59it/s]Extractor Estimating: 60it [01:41,  1.56it/s]Extractor Estimating: 61it [01:44,  1.20s/it]Extractor Estimating: 62it [01:45,  1.07s/it]Extractor Estimating: 63it [01:45,  1.08it/s]Extractor Estimating: 64it [01:46,  1.22it/s]Extractor Estimating: 65it [01:47,  1.27it/s]Extractor Estimating: 66it [01:47,  1.33it/s]Extractor Estimating: 67it [01:48,  1.34it/s]Extractor Estimating: 68it [01:49,  1.39it/s]Extractor Estimating: 69it [01:49,  1.44it/s]Extractor Estimating: 70it [01:50,  1.50it/s]Extractor Estimating: 71it [01:50,  1.53it/s]Extractor Estimating: 72it [01:51,  1.45it/s]Extractor Estimating: 73it [01:52,  1.46it/s]Extractor Estimating: 74it [01:52,  1.52it/s]Extractor Estimating: 75it [01:53,  1.44it/s]Extractor Estimating: 76it [01:54,  1.44it/s]Extractor Estimating: 77it [01:55,  1.43it/s]Extractor Estimating: 78it [01:55,  1.45it/s]Extractor Estimating: 79it [01:56,  1.48it/s]Extractor Estimating: 80it [01:57,  1.55it/s]Extractor Estimating: 81it [01:57,  1.51it/s]Extractor Estimating: 82it [01:58,  1.49it/s]Extractor Estimating: 83it [01:59,  1.47it/s]Extractor Estimating: 84it [01:59,  1.49it/s]Extractor Estimating: 85it [02:00,  1.47it/s]Extractor Estimating: 86it [02:01,  1.50it/s]Extractor Estimating: 87it [02:01,  1.50it/s]Extractor Estimating: 88it [02:02,  1.45it/s]Extractor Estimating: 89it [02:03,  1.41it/s]Extractor Estimating: 90it [02:03,  1.45it/s]Extractor Estimating: 91it [02:04,  1.46it/s]Extractor Estimating: 92it [02:05,  1.49it/s]Extractor Estimating: 93it [02:05,  1.52it/s]Extractor Estimating: 94it [02:06,  1.44it/s]Extractor Estimating: 95it [02:07,  1.53it/s]Extractor Estimating: 96it [02:07,  1.50it/s]Extractor Estimating: 97it [02:08,  1.50it/s]Extractor Estimating: 98it [02:09,  1.51it/s]Extractor Estimating: 99it [02:09,  1.47it/s]Extractor Estimating: 100it [02:10,  1.50it/s]Extractor Estimating: 101it [02:11,  1.54it/s]Extractor Estimating: 102it [02:11,  1.55it/s]Extractor Estimating: 103it [02:12,  1.54it/s]Extractor Estimating: 104it [02:13,  1.48it/s]Extractor Estimating: 105it [02:13,  1.51it/s]Extractor Estimating: 106it [02:14,  1.53it/s]Extractor Estimating: 107it [02:15,  1.53it/s]Extractor Estimating: 108it [02:15,  1.51it/s]Extractor Estimating: 109it [02:16,  1.46it/s]Extractor Estimating: 110it [02:17,  1.51it/s]Extractor Estimating: 111it [02:17,  1.52it/s]Extractor Estimating: 112it [02:18,  1.52it/s]Extractor Estimating: 113it [02:19,  1.54it/s]Extractor Estimating: 114it [02:19,  1.48it/s]Extractor Estimating: 115it [02:20,  1.53it/s]Extractor Estimating: 116it [02:21,  1.52it/s]Extractor Estimating: 117it [02:21,  1.55it/s]Extractor Estimating: 118it [02:22,  1.49it/s]Extractor Estimating: 119it [02:23,  1.50it/s]Extractor Estimating: 120it [02:23,  1.54it/s]Extractor Estimating: 121it [02:24,  1.43it/s]Extractor Estimating: 122it [02:25,  1.50it/s]Extractor Estimating: 123it [02:25,  1.53it/s]Extractor Estimating: 124it [02:26,  1.46it/s]Extractor Estimating: 125it [02:27,  1.52it/s]Extractor Estimating: 126it [02:27,  1.51it/s]Extractor Estimating: 127it [02:28,  1.49it/s]Extractor Estimating: 128it [02:29,  1.51it/s]Extractor Estimating: 129it [02:29,  1.48it/s]Extractor Estimating: 130it [02:30,  1.56it/s]Extractor Estimating: 131it [02:31,  1.61it/s]Extractor Estimating: 132it [02:31,  1.55it/s]Extractor Estimating: 133it [02:32,  1.56it/s]Extractor Estimating: 134it [02:33,  1.47it/s]Extractor Estimating: 135it [02:33,  1.52it/s]Extractor Estimating: 136it [02:34,  1.54it/s]Extractor Estimating: 137it [02:34,  1.58it/s]Extractor Estimating: 138it [02:35,  1.58it/s]Extractor Estimating: 139it [02:36,  1.59it/s]Extractor Estimating: 140it [02:36,  1.55it/s]Extractor Estimating: 141it [02:37,  1.53it/s]Extractor Estimating: 142it [02:38,  1.52it/s]Extractor Estimating: 143it [02:38,  1.53it/s]Extractor Estimating: 144it [02:39,  1.57it/s]Extractor Estimating: 145it [02:40,  1.60it/s]Extractor Estimating: 146it [02:40,  1.50it/s]Extractor Estimating: 147it [02:41,  1.55it/s]Extractor Estimating: 148it [02:42,  1.53it/s]Extractor Estimating: 149it [02:42,  1.57it/s]Extractor Estimating: 150it [02:43,  1.58it/s]Extractor Estimating: 151it [02:43,  1.57it/s]Extractor Estimating: 152it [02:44,  1.63it/s]Extractor Estimating: 153it [02:45,  1.68it/s]Extractor Estimating: 154it [02:45,  1.74it/s]Extractor Estimating: 155it [02:46,  1.77it/s]Extractor Estimating: 156it [02:46,  1.79it/s]Extractor Estimating: 157it [02:47,  1.71it/s]Extractor Estimating: 158it [02:47,  1.71it/s]Extractor Estimating: 159it [02:48,  1.75it/s]Extractor Estimating: 160it [02:48,  1.78it/s]Extractor Estimating: 161it [02:49,  1.84it/s]Extractor Estimating: 162it [02:49,  1.91it/s]Extractor Estimating: 163it [02:50,  1.76it/s]Extractor Estimating: 164it [02:51,  1.79it/s]Extractor Estimating: 165it [02:51,  1.86it/s]Extractor Estimating: 166it [02:52,  1.83it/s]Extractor Estimating: 167it [02:52,  1.90it/s]Extractor Estimating: 168it [02:53,  1.86it/s]Extractor Estimating: 169it [02:53,  1.79it/s]Extractor Estimating: 170it [02:54,  1.76it/s]Extractor Estimating: 171it [02:55,  1.77it/s]Extractor Estimating: 172it [02:55,  1.81it/s]Extractor Estimating: 173it [02:56,  1.84it/s]Extractor Estimating: 174it [02:56,  1.86it/s]Extractor Estimating: 175it [02:57,  1.77it/s]Extractor Estimating: 176it [02:57,  1.68it/s]Extractor Estimating: 177it [02:58,  1.66it/s]Extractor Estimating: 178it [02:59,  1.69it/s]Extractor Estimating: 179it [02:59,  1.62it/s]Extractor Estimating: 180it [03:00,  1.61it/s]Extractor Estimating: 181it [03:00,  1.63it/s]Extractor Estimating: 182it [03:01,  1.67it/s]Extractor Estimating: 183it [03:02,  1.61it/s]Extractor Estimating: 184it [03:02,  1.64it/s]Extractor Estimating: 185it [03:03,  1.57it/s]Extractor Estimating: 186it [03:04,  1.52it/s]Extractor Estimating: 187it [03:04,  1.49it/s]Extractor Estimating: 188it [03:05,  1.55it/s]Extractor Estimating: 189it [03:06,  1.56it/s]Extractor Estimating: 190it [03:06,  1.56it/s]Extractor Estimating: 191it [03:07,  1.56it/s]Extractor Estimating: 192it [03:08,  1.46it/s]Extractor Estimating: 193it [03:08,  1.53it/s]Extractor Estimating: 194it [03:09,  1.57it/s]Extractor Estimating: 195it [03:09,  1.62it/s]Extractor Estimating: 196it [03:10,  1.52it/s]Extractor Estimating: 197it [03:11,  1.49it/s]Extractor Estimating: 198it [03:12,  1.48it/s]Extractor Estimating: 199it [03:12,  1.55it/s]Extractor Estimating: 200it [03:13,  1.58it/s]Extractor Estimating: 201it [03:13,  1.60it/s]Extractor Estimating: 202it [03:14,  1.53it/s]Extractor Estimating: 203it [03:15,  1.53it/s]Extractor Estimating: 204it [03:15,  1.54it/s]Extractor Estimating: 205it [03:16,  1.54it/s]Extractor Estimating: 206it [03:17,  1.55it/s]Extractor Estimating: 207it [03:17,  1.50it/s]Extractor Estimating: 208it [03:18,  1.50it/s]Extractor Estimating: 209it [03:19,  1.51it/s]Extractor Estimating: 210it [03:19,  1.60it/s]Extractor Estimating: 211it [03:20,  1.56it/s]Extractor Estimating: 212it [03:21,  1.50it/s]Extractor Estimating: 213it [03:21,  1.53it/s]Extractor Estimating: 214it [03:22,  1.50it/s]Extractor Estimating: 215it [03:23,  1.56it/s]Extractor Estimating: 216it [03:23,  1.56it/s]Extractor Estimating: 217it [03:24,  1.51it/s]Extractor Estimating: 218it [03:25,  1.52it/s]Extractor Estimating: 219it [03:25,  1.53it/s]Extractor Estimating: 220it [03:26,  1.52it/s]Extractor Estimating: 221it [03:26,  1.54it/s]Extractor Estimating: 222it [03:27,  1.49it/s]Extractor Estimating: 223it [03:28,  1.55it/s]Extractor Estimating: 224it [03:28,  1.61it/s]Extractor Estimating: 225it [03:29,  1.55it/s]Extractor Estimating: 226it [03:30,  1.59it/s]Extractor Estimating: 227it [03:31,  1.43it/s]Extractor Estimating: 228it [03:31,  1.44it/s]Extractor Estimating: 229it [03:32,  1.49it/s]Extractor Estimating: 230it [03:32,  1.57it/s]Extractor Estimating: 231it [03:33,  1.57it/s]Extractor Estimating: 232it [03:34,  1.51it/s]Extractor Estimating: 233it [03:34,  1.53it/s]Extractor Estimating: 234it [03:35,  1.42it/s]Extractor Estimating: 235it [03:36,  1.49it/s]Extractor Estimating: 236it [03:36,  1.56it/s]Extractor Estimating: 237it [03:37,  1.57it/s]Extractor Estimating: 238it [03:38,  1.64it/s]Extractor Estimating: 239it [03:38,  1.58it/s]Extractor Estimating: 240it [03:39,  1.62it/s]Extractor Estimating: 241it [03:39,  1.60it/s]Extractor Estimating: 242it [03:40,  1.68it/s]Extractor Estimating: 243it [03:41,  1.63it/s]Extractor Estimating: 244it [03:41,  1.57it/s]Extractor Estimating: 245it [03:42,  1.56it/s]Extractor Estimating: 246it [03:43,  1.55it/s]Extractor Estimating: 247it [03:43,  1.57it/s]Extractor Estimating: 248it [03:44,  1.63it/s]Extractor Estimating: 249it [03:44,  1.57it/s]Extractor Estimating: 250it [03:45,  1.48it/s]Extractor Estimating: 250it [03:45,  1.11it/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/0_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/0.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.2, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 1000, 'num_train': 4000}
num of filtered data: 5019 mean pseudo reward: 0.91312067235841
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/0.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 24588
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 24688, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_filtered_large/unseen_5_seed_4/extractor/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=24688, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 1.323, loss:1455.1935
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.959, loss:1352.2837
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 90, avg_time 0.967, loss:1309.2432
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 190, avg_time 0.957, loss:1295.7762
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 80, avg_time 0.960, loss:1270.7136
>> valid entity prec:0.5871, rec:0.5246, f1:0.5541
>> valid relation prec:0.8276, rec:0.0480, f1:0.0908
>> valid relation with NER prec:0.8276, rec:0.0480, f1:0.0908
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 180, avg_time 2.299, loss:1185.2110
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 70, avg_time 0.960, loss:1186.4724
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 170, avg_time 0.963, loss:1188.7007
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 60, avg_time 0.954, loss:1119.5152
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 160, avg_time 0.965, loss:1105.5183
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6023, rec:0.6170, f1:0.6096
>> valid relation prec:0.7526, rec:0.0418, f1:0.0791
>> valid relation with NER prec:0.7526, rec:0.0418, f1:0.0791
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 50, avg_time 2.283, loss:1103.5633
g_step 1200, step 150, avg_time 0.959, loss:1045.2166
g_step 1300, step 40, avg_time 0.964, loss:1004.2671
g_step 1400, step 140, avg_time 0.962, loss:1023.1033
g_step 1500, step 30, avg_time 0.956, loss:937.7178
>> valid entity prec:0.5222, rec:0.6493, f1:0.5789
>> valid relation prec:0.5193, rec:0.1078, f1:0.1785
>> valid relation with NER prec:0.5193, rec:0.1078, f1:0.1785
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 130, avg_time 2.286, loss:945.6055
g_step 1700, step 20, avg_time 0.958, loss:930.0769
g_step 1800, step 120, avg_time 0.966, loss:901.6490
g_step 1900, step 10, avg_time 0.953, loss:892.0232
g_step 2000, step 110, avg_time 0.964, loss:860.0165
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5274, rec:0.6554, f1:0.5845
>> valid relation prec:0.3543, rec:0.0741, f1:0.1225
>> valid relation with NER prec:0.3543, rec:0.0741, f1:0.1225
g_step 2100, step 210, avg_time 2.246, loss:836.8477
g_step 2200, step 100, avg_time 0.956, loss:804.0609
g_step 2300, step 200, avg_time 0.962, loss:807.4892
g_step 2400, step 90, avg_time 0.955, loss:765.5091
g_step 2500, step 190, avg_time 0.969, loss:784.0949
>> valid entity prec:0.5693, rec:0.5926, f1:0.5807
>> valid relation prec:0.3888, rec:0.0855, f1:0.1402
>> valid relation with NER prec:0.3888, rec:0.0855, f1:0.1402
g_step 2600, step 80, avg_time 2.230, loss:740.2022
g_step 2700, step 180, avg_time 0.961, loss:762.6694
g_step 2800, step 70, avg_time 0.959, loss:684.2204
g_step 2900, step 170, avg_time 0.962, loss:729.5502
g_step 3000, step 60, avg_time 0.960, loss:697.7005
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5745, rec:0.5796, f1:0.5770
>> valid relation prec:0.3998, rec:0.0998, f1:0.1597
>> valid relation with NER prec:0.3998, rec:0.0998, f1:0.1597
g_step 3100, step 160, avg_time 2.230, loss:677.2784
g_step 3200, step 50, avg_time 0.957, loss:653.7249
g_step 3300, step 150, avg_time 0.963, loss:644.9210
g_step 3400, step 40, avg_time 0.964, loss:633.7817
g_step 3500, step 140, avg_time 0.960, loss:616.7728
>> valid entity prec:0.5931, rec:0.6213, f1:0.6069
>> valid relation prec:0.3279, rec:0.0972, f1:0.1500
>> valid relation with NER prec:0.3279, rec:0.0972, f1:0.1500
g_step 3600, step 30, avg_time 2.229, loss:607.3559
g_step 3700, step 130, avg_time 0.963, loss:587.6775
g_step 3800, step 20, avg_time 0.964, loss:587.1985
g_step 3900, step 120, avg_time 0.954, loss:560.9119
g_step 4000, step 10, avg_time 0.961, loss:576.6391
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5816, rec:0.5806, f1:0.5811
>> valid relation prec:0.2371, rec:0.0698, f1:0.1078
>> valid relation with NER prec:0.2371, rec:0.0698, f1:0.1078
g_step 4100, step 110, avg_time 2.234, loss:542.9978
g_step 4200, step 210, avg_time 0.959, loss:551.3966
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 10:32:17 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 10:32:17 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_10-32-16_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 10:32:21 - WARNING - datasets.builder -   Using custom data configuration default-8d81060038fdc379
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-8d81060038fdc379/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 10:32:30,845 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:32:30,963 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:32:30,964 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:32:30,965 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:32:31,358 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:32:31,594 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:32:31,594 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:32:31,594 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:32:31,595 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:32:31,595 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:32:31,595 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 10:32:32,276 >> loading weights file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:32:35,596 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 10:32:35,596 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel/unseen_5_seed_4/generator/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-8d81060038fdc379/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
08/28/2023 10:32:35 - WARNING - datasets.fingerprint -   Parameter 'function'=<function main.<locals>.tokenize_function at 0x14ef43e91f80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:01<00:08,  1.77s/ba] 33%|███▎      | 2/6 [00:02<00:03,  1.15ba/s] 50%|█████     | 3/6 [00:02<00:01,  1.74ba/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.32ba/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.83ba/s]100%|██████████| 6/6 [00:02<00:00,  2.24ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.42ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.36ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  3.02ba/s]100%|██████████| 4/4 [00:01<00:00,  4.06ba/s]100%|██████████| 4/4 [00:01<00:00,  3.15ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  2.35ba/s] 50%|█████     | 3/6 [00:00<00:00,  5.60ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  7.45ba/s]100%|██████████| 6/6 [00:00<00:00,  7.51ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.60ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.43ba/s]100%|██████████| 4/4 [00:00<00:00,  7.78ba/s]100%|██████████| 4/4 [00:00<00:00,  6.24ba/s]
[INFO|trainer.py:414] 2023-08-28 10:32:46,086 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 10:32:46,376 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 10:32:46,376 >>   Num examples = 5027
[INFO|trainer.py:1149] 2023-08-28 10:32:46,377 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 10:32:46,377 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 10:32:46,377 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 10:32:46,377 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 10:32:46,377 >>   Total optimization steps = 395
  0%|          | 0/395 [00:00<?, ?it/s]  0%|          | 1/395 [00:00<01:59,  3.31it/s]  1%|          | 2/395 [00:00<01:55,  3.40it/s]  1%|          | 3/395 [00:00<01:53,  3.45it/s]  1%|          | 4/395 [00:01<01:52,  3.47it/s]  1%|▏         | 5/395 [00:01<01:52,  3.48it/s]  2%|▏         | 6/395 [00:01<01:51,  3.49it/s]  2%|▏         | 7/395 [00:02<01:51,  3.49it/s]  2%|▏         | 8/395 [00:02<01:50,  3.49it/s]  2%|▏         | 9/395 [00:02<01:50,  3.49it/s]  3%|▎         | 10/395 [00:02<01:50,  3.50it/s]  3%|▎         | 11/395 [00:03<01:49,  3.50it/s]  3%|▎         | 12/395 [00:03<01:49,  3.49it/s]  3%|▎         | 13/395 [00:03<01:49,  3.50it/s]  4%|▎         | 14/395 [00:04<01:48,  3.50it/s]  4%|▍         | 15/395 [00:04<01:48,  3.50it/s]  4%|▍         | 16/395 [00:04<01:48,  3.50it/s]  4%|▍         | 17/395 [00:04<01:48,  3.50it/s]  5%|▍         | 18/395 [00:05<01:47,  3.50it/s]  5%|▍         | 19/395 [00:05<01:47,  3.50it/s]  5%|▌         | 20/395 [00:05<01:47,  3.50it/s]  5%|▌         | 21/395 [00:06<01:46,  3.50it/s]  6%|▌         | 22/395 [00:06<01:46,  3.49it/s]  6%|▌         | 23/395 [00:06<01:46,  3.49it/s]  6%|▌         | 24/395 [00:06<01:46,  3.50it/s]  6%|▋         | 25/395 [00:07<01:45,  3.49it/s]  7%|▋         | 26/395 [00:07<01:45,  3.49it/s]  7%|▋         | 27/395 [00:07<01:45,  3.49it/s]  7%|▋         | 28/395 [00:08<01:44,  3.50it/s]  7%|▋         | 29/395 [00:08<01:44,  3.49it/s]  8%|▊         | 30/395 [00:08<01:44,  3.49it/s]  8%|▊         | 31/395 [00:08<01:44,  3.49it/s]  8%|▊         | 32/395 [00:09<01:43,  3.50it/s]  8%|▊         | 33/395 [00:09<01:43,  3.49it/s]  9%|▊         | 34/395 [00:09<01:43,  3.49it/s]  9%|▉         | 35/395 [00:10<01:43,  3.49it/s]  9%|▉         | 36/395 [00:10<01:42,  3.49it/s]  9%|▉         | 37/395 [00:10<01:42,  3.49it/s] 10%|▉         | 38/395 [00:10<01:42,  3.49it/s] 10%|▉         | 39/395 [00:11<01:42,  3.49it/s] 10%|█         | 40/395 [00:11<01:41,  3.49it/s] 10%|█         | 41/395 [00:11<01:41,  3.49it/s] 11%|█         | 42/395 [00:12<01:41,  3.49it/s] 11%|█         | 43/395 [00:12<01:40,  3.49it/s] 11%|█         | 44/395 [00:12<01:40,  3.49it/s] 11%|█▏        | 45/395 [00:12<01:40,  3.49it/s] 12%|█▏        | 46/395 [00:13<01:40,  3.49it/s] 12%|█▏        | 47/395 [00:13<01:39,  3.49it/s] 12%|█▏        | 48/395 [00:13<01:39,  3.49it/s] 12%|█▏        | 49/395 [00:14<01:39,  3.49it/s] 13%|█▎        | 50/395 [00:14<01:38,  3.49it/s] 13%|█▎        | 51/395 [00:14<01:38,  3.49it/s] 13%|█▎        | 52/395 [00:14<01:38,  3.48it/s] 13%|█▎        | 53/395 [00:15<01:38,  3.48it/s] 14%|█▎        | 54/395 [00:15<01:37,  3.48it/s] 14%|█▍        | 55/395 [00:15<01:37,  3.49it/s] 14%|█▍        | 56/395 [00:16<01:37,  3.48it/s] 14%|█▍        | 57/395 [00:16<01:36,  3.49it/s] 15%|█▍        | 58/395 [00:16<01:36,  3.48it/s] 15%|█▍        | 59/395 [00:16<01:36,  3.49it/s] 15%|█▌        | 60/395 [00:17<01:36,  3.49it/s] 15%|█▌        | 61/395 [00:17<01:35,  3.48it/s] 16%|█▌        | 62/395 [00:17<01:35,  3.49it/s] 16%|█▌        | 63/395 [00:18<01:41,  3.26it/s] 16%|█▌        | 64/395 [00:18<01:39,  3.32it/s] 16%|█▋        | 65/395 [00:18<01:38,  3.37it/s] 17%|█▋        | 66/395 [00:18<01:36,  3.40it/s] 17%|█▋        | 67/395 [00:19<01:35,  3.42it/s] 17%|█▋        | 68/395 [00:19<01:35,  3.44it/s] 17%|█▋        | 69/395 [00:19<01:34,  3.45it/s] 18%|█▊        | 70/395 [00:20<01:34,  3.45it/s] 18%|█▊        | 71/395 [00:20<01:33,  3.46it/s] 18%|█▊        | 72/395 [00:20<01:33,  3.46it/s] 18%|█▊        | 73/395 [00:21<01:32,  3.47it/s] 19%|█▊        | 74/395 [00:21<01:32,  3.47it/s] 19%|█▉        | 75/395 [00:21<01:32,  3.47it/s] 19%|█▉        | 76/395 [00:21<01:31,  3.47it/s] 19%|█▉        | 77/395 [00:22<01:31,  3.48it/s] 20%|█▉        | 78/395 [00:22<01:31,  3.46it/s] 20%|██        | 79/395 [00:22<01:21,  3.89it/s][INFO|trainer.py:2140] 2023-08-28 10:33:09,013 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:33:09,013 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 10:33:09,013 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.92it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.18it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.47it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.69it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.13it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.77it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.18it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.82it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.74it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 44.98it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.13it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.22it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.16it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.15it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.13it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 44.83it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 44.72it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.60it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.76it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.87it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.97it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 45.19it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 45.21it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 45.11it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.92it/s][A
 30%|███       | 132/438 [00:02<00:07, 39.95it/s][A
 31%|███▏      | 137/438 [00:03<00:07, 41.54it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 42.66it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 43.47it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.03it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.43it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.55it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.83it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.60it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.38it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.40it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.63it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.00it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.15it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.30it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.33it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 45.18it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 44.80it/s][A
 51%|█████     | 222/438 [00:04<00:04, 44.58it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.48it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.70it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.88it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 45.07it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 45.21it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 45.26it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 45.17it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.94it/s][A
 61%|██████    | 267/438 [00:05<00:03, 44.63it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.50it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.67it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.81it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 45.06it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.18it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 45.25it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 45.04it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.92it/s][A
 71%|███████   | 312/438 [00:06<00:02, 44.66it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.52it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.65it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.40it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.91it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 45.26it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 45.33it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 45.22it/s][A
 80%|████████  | 352/438 [00:07<00:01, 45.03it/s][A
 82%|████████▏ | 357/438 [00:07<00:01, 44.76it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 44.59it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.61it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 38.11it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 41.37it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 42.42it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 43.37it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 43.84it/s][A
 91%|█████████ | 398/438 [00:08<00:00, 44.42it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.60it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 44.74it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.52it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.17it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 44.40it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 34.44it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 37.10it/s][A
100%|██████████| 438/438 [00:09<00:00, 39.25it/s][A                                                
                                                 [A 20%|██        | 79/395 [00:32<01:21,  3.89it/s]
100%|██████████| 438/438 [00:09<00:00, 39.25it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:33:20,110 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79
[INFO|configuration_utils.py:351] 2023-08-28 10:33:20,576 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:33:49,220 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:33:49,914 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:33:50,333 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79/special_tokens_map.json
 20%|██        | 80/395 [01:58<2:32:30, 29.05s/it] 21%|██        | 81/395 [01:59<1:47:02, 20.46s/it] 21%|██        | 82/395 [01:59<1:15:09, 14.41s/it] 21%|██        | 83/395 [01:59<52:53, 10.17s/it]   21%|██▏       | 84/395 [02:00<37:21,  7.21s/it] 22%|██▏       | 85/395 [02:00<26:31,  5.13s/it] 22%|██▏       | 86/395 [02:00<18:57,  3.68s/it] 22%|██▏       | 87/395 [02:01<13:40,  2.66s/it] 22%|██▏       | 88/395 [02:01<09:58,  1.95s/it] 23%|██▎       | 89/395 [02:01<07:24,  1.45s/it] 23%|██▎       | 90/395 [02:01<05:36,  1.10s/it] 23%|██▎       | 91/395 [02:02<04:32,  1.12it/s] 23%|██▎       | 92/395 [02:02<03:36,  1.40it/s] 24%|██▎       | 93/395 [02:02<02:56,  1.71it/s] 24%|██▍       | 94/395 [02:03<02:29,  2.01it/s] 24%|██▍       | 95/395 [02:03<02:10,  2.31it/s] 24%|██▍       | 96/395 [02:03<01:56,  2.57it/s] 25%|██▍       | 97/395 [02:04<01:46,  2.79it/s] 25%|██▍       | 98/395 [02:04<01:40,  2.96it/s] 25%|██▌       | 99/395 [02:04<01:35,  3.10it/s] 25%|██▌       | 100/395 [02:04<01:32,  3.21it/s] 26%|██▌       | 101/395 [02:05<01:36,  3.05it/s] 26%|██▌       | 102/395 [02:05<01:32,  3.17it/s] 26%|██▌       | 103/395 [02:05<01:29,  3.26it/s] 26%|██▋       | 104/395 [02:06<01:27,  3.33it/s] 27%|██▋       | 105/395 [02:06<01:25,  3.38it/s] 27%|██▋       | 106/395 [02:06<01:24,  3.41it/s] 27%|██▋       | 107/395 [02:06<01:23,  3.44it/s] 27%|██▋       | 108/395 [02:07<01:23,  3.45it/s] 28%|██▊       | 109/395 [02:07<01:22,  3.46it/s] 28%|██▊       | 110/395 [02:07<01:22,  3.47it/s] 28%|██▊       | 111/395 [02:08<01:21,  3.48it/s] 28%|██▊       | 112/395 [02:08<01:31,  3.09it/s] 29%|██▊       | 113/395 [02:08<01:28,  3.20it/s] 29%|██▉       | 114/395 [02:09<01:25,  3.28it/s] 29%|██▉       | 115/395 [02:09<01:23,  3.34it/s] 29%|██▉       | 116/395 [02:09<01:22,  3.38it/s] 30%|██▉       | 117/395 [02:09<01:21,  3.42it/s] 30%|██▉       | 118/395 [02:10<01:20,  3.43it/s] 30%|███       | 119/395 [02:10<01:19,  3.45it/s] 30%|███       | 120/395 [02:10<01:19,  3.46it/s] 31%|███       | 121/395 [02:11<01:19,  3.46it/s] 31%|███       | 122/395 [02:11<01:18,  3.47it/s] 31%|███       | 123/395 [02:11<01:27,  3.09it/s] 31%|███▏      | 124/395 [02:12<01:24,  3.20it/s] 32%|███▏      | 125/395 [02:12<01:22,  3.28it/s] 32%|███▏      | 126/395 [02:12<01:20,  3.34it/s] 32%|███▏      | 127/395 [02:12<01:19,  3.38it/s] 32%|███▏      | 128/395 [02:13<01:18,  3.41it/s] 33%|███▎      | 129/395 [02:13<01:17,  3.43it/s] 33%|███▎      | 130/395 [02:13<01:16,  3.45it/s] 33%|███▎      | 131/395 [02:14<01:16,  3.46it/s] 33%|███▎      | 132/395 [02:14<01:15,  3.46it/s] 34%|███▎      | 133/395 [02:14<01:15,  3.47it/s] 34%|███▍      | 134/395 [02:15<01:24,  3.10it/s] 34%|███▍      | 135/395 [02:15<01:21,  3.20it/s] 34%|███▍      | 136/395 [02:15<01:18,  3.29it/s] 35%|███▍      | 137/395 [02:15<01:17,  3.34it/s] 35%|███▍      | 138/395 [02:16<01:15,  3.38it/s] 35%|███▌      | 139/395 [02:16<01:15,  3.41it/s] 35%|███▌      | 140/395 [02:16<01:14,  3.44it/s] 36%|███▌      | 141/395 [02:17<01:13,  3.45it/s] 36%|███▌      | 142/395 [02:17<01:13,  3.46it/s] 36%|███▌      | 143/395 [02:17<01:12,  3.46it/s] 36%|███▋      | 144/395 [02:17<01:12,  3.47it/s] 37%|███▋      | 145/395 [02:18<01:19,  3.15it/s] 37%|███▋      | 146/395 [02:18<01:16,  3.24it/s] 37%|███▋      | 147/395 [02:18<01:14,  3.31it/s] 37%|███▋      | 148/395 [02:19<01:13,  3.36it/s] 38%|███▊      | 149/395 [02:19<01:12,  3.39it/s] 38%|███▊      | 150/395 [02:19<01:11,  3.42it/s] 38%|███▊      | 151/395 [02:20<01:11,  3.43it/s] 38%|███▊      | 152/395 [02:20<01:10,  3.45it/s] 39%|███▊      | 153/395 [02:20<01:09,  3.46it/s] 39%|███▉      | 154/395 [02:20<01:09,  3.46it/s] 39%|███▉      | 155/395 [02:21<01:09,  3.47it/s] 39%|███▉      | 156/395 [02:21<01:13,  3.27it/s] 40%|███▉      | 157/395 [02:21<01:11,  3.33it/s] 40%|████      | 158/395 [02:22<01:02,  3.77it/s][INFO|trainer.py:2140] 2023-08-28 10:35:08,392 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:35:08,392 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 10:35:08,392 >>   Batch size = 8
{'eval_loss': 1.0127758979797363, 'eval_runtime': 9.9371, 'eval_samples_per_second': 351.914, 'eval_steps_per_second': 44.077, 'epoch': 1.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.89it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.45it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.55it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.82it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.38it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.90it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.39it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.01it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.05it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.04it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.08it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.23it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.21it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.32it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.16it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 44.91it/s][A
 20%|██        | 88/438 [00:01<00:07, 44.93it/s][A
 21%|██        | 93/438 [00:02<00:07, 44.85it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.04it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.14it/s][A
 25%|██▍       | 108/438 [00:02<00:08, 38.19it/s][A
 26%|██▌       | 113/438 [00:02<00:08, 40.22it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 41.72it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 42.85it/s][A
 29%|██▉       | 128/438 [00:02<00:07, 43.76it/s][A
 30%|███       | 133/438 [00:02<00:06, 44.36it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.69it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.88it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.57it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 44.32it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.45it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.57it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.84it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 45.05it/s][A
 41%|████      | 178/438 [00:03<00:05, 45.07it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.34it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.33it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 44.99it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 44.82it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 44.72it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 44.77it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.01it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.21it/s][A
 51%|█████     | 223/438 [00:04<00:04, 45.32it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 45.42it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.31it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 45.10it/s][A
 55%|█████▌    | 243/438 [00:05<00:05, 37.32it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 39.50it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 41.17it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 42.47it/s][A
 60%|██████    | 263/438 [00:05<00:04, 43.34it/s][A
 61%|██████    | 268/438 [00:06<00:03, 43.96it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 44.47it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 44.65it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 44.40it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.17it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.42it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.67it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 44.91it/s][A
 70%|███████   | 308/438 [00:06<00:02, 45.15it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 45.10it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.30it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.15it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 44.86it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 44.67it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 44.75it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 44.85it/s][A
 79%|███████▉  | 348/438 [00:07<00:02, 44.96it/s][A
 81%|████████  | 353/438 [00:07<00:01, 45.11it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 45.25it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 45.31it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 45.18it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.95it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 35.23it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 37.74it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 39.78it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 41.36it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 42.51it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 43.40it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 43.98it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.39it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.19it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 43.98it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 44.30it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 44.65it/s][A
100%|██████████| 438/438 [00:09<00:00, 44.97it/s][A                                                 
                                                 [A 40%|████      | 158/395 [02:31<01:02,  3.77it/s]
100%|██████████| 438/438 [00:09<00:00, 44.97it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:35:18,661 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158
[INFO|configuration_utils.py:351] 2023-08-28 10:35:19,173 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:37:02,182 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:37:03,676 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:37:03,892 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158/special_tokens_map.json
 40%|████      | 159/395 [05:15<3:25:39, 52.28s/it] 41%|████      | 160/395 [05:16<2:23:58, 36.76s/it] 41%|████      | 161/395 [05:16<1:40:41, 25.82s/it] 41%|████      | 162/395 [05:16<1:10:30, 18.16s/it] 41%|████▏     | 163/395 [05:17<49:29, 12.80s/it]   42%|████▏     | 164/395 [05:17<34:49,  9.05s/it] 42%|████▏     | 165/395 [05:17<24:41,  6.44s/it] 42%|████▏     | 166/395 [05:18<17:32,  4.59s/it] 42%|████▏     | 167/395 [05:18<12:33,  3.30s/it] 43%|████▎     | 168/395 [05:18<09:04,  2.40s/it] 43%|████▎     | 169/395 [05:20<07:55,  2.10s/it] 43%|████▎     | 170/395 [05:20<05:50,  1.56s/it] 43%|████▎     | 171/395 [05:20<04:30,  1.21s/it] 44%|████▎     | 172/395 [05:20<03:27,  1.07it/s] 44%|████▍     | 173/395 [05:21<02:43,  1.36it/s] 44%|████▍     | 174/395 [05:21<02:13,  1.66it/s] 44%|████▍     | 175/395 [05:21<01:51,  1.97it/s] 45%|████▍     | 176/395 [05:22<01:36,  2.27it/s] 45%|████▍     | 177/395 [05:22<01:25,  2.54it/s] 45%|████▌     | 178/395 [05:22<01:18,  2.77it/s] 45%|████▌     | 179/395 [05:22<01:13,  2.95it/s] 46%|████▌     | 180/395 [05:23<01:09,  3.10it/s] 46%|████▌     | 181/395 [05:23<01:06,  3.21it/s] 46%|████▌     | 182/395 [05:23<01:13,  2.90it/s] 46%|████▋     | 183/395 [05:24<01:09,  3.06it/s] 47%|████▋     | 184/395 [05:24<01:06,  3.18it/s] 47%|████▋     | 185/395 [05:24<01:04,  3.27it/s] 47%|████▋     | 186/395 [05:25<01:02,  3.33it/s] 47%|████▋     | 187/395 [05:25<01:01,  3.38it/s] 48%|████▊     | 188/395 [05:25<01:00,  3.42it/s] 48%|████▊     | 189/395 [05:25<00:59,  3.44it/s] 48%|████▊     | 190/395 [05:26<00:59,  3.45it/s] 48%|████▊     | 191/395 [05:26<00:58,  3.47it/s] 49%|████▊     | 192/395 [05:26<00:58,  3.47it/s] 49%|████▉     | 193/395 [05:27<01:03,  3.16it/s] 49%|████▉     | 194/395 [05:27<01:01,  3.25it/s] 49%|████▉     | 195/395 [05:27<01:00,  3.32it/s] 50%|████▉     | 196/395 [05:28<00:59,  3.37it/s] 50%|████▉     | 197/395 [05:28<00:58,  3.41it/s] 50%|█████     | 198/395 [05:28<00:57,  3.44it/s] 50%|█████     | 199/395 [05:28<00:56,  3.45it/s] 51%|█████     | 200/395 [05:29<00:56,  3.46it/s] 51%|█████     | 201/395 [05:29<00:55,  3.48it/s] 51%|█████     | 202/395 [05:29<00:55,  3.48it/s] 51%|█████▏    | 203/395 [05:30<00:55,  3.48it/s] 52%|█████▏    | 204/395 [05:30<01:03,  3.02it/s] 52%|█████▏    | 205/395 [05:30<01:00,  3.14it/s] 52%|█████▏    | 206/395 [05:31<00:58,  3.24it/s] 52%|█████▏    | 207/395 [05:31<00:56,  3.31it/s] 53%|█████▎    | 208/395 [05:31<00:55,  3.37it/s] 53%|█████▎    | 209/395 [05:31<00:54,  3.41it/s] 53%|█████▎    | 210/395 [05:32<00:53,  3.43it/s] 53%|█████▎    | 211/395 [05:32<00:53,  3.45it/s] 54%|█████▎    | 212/395 [05:32<00:52,  3.47it/s] 54%|█████▍    | 213/395 [05:33<00:52,  3.47it/s] 54%|█████▍    | 214/395 [05:33<00:58,  3.09it/s] 54%|█████▍    | 215/395 [05:33<00:56,  3.21it/s] 55%|█████▍    | 216/395 [05:34<00:54,  3.28it/s] 55%|█████▍    | 217/395 [05:34<00:53,  3.35it/s] 55%|█████▌    | 218/395 [05:34<00:52,  3.39it/s] 55%|█████▌    | 219/395 [05:34<00:51,  3.42it/s] 56%|█████▌    | 220/395 [05:35<00:50,  3.44it/s] 56%|█████▌    | 221/395 [05:35<00:50,  3.45it/s] 56%|█████▌    | 222/395 [05:35<00:49,  3.47it/s] 56%|█████▋    | 223/395 [05:36<00:49,  3.48it/s] 57%|█████▋    | 224/395 [05:36<00:49,  3.48it/s] 57%|█████▋    | 225/395 [05:36<00:54,  3.13it/s] 57%|█████▋    | 226/395 [05:37<00:52,  3.23it/s] 57%|█████▋    | 227/395 [05:37<00:50,  3.31it/s] 58%|█████▊    | 228/395 [05:37<00:49,  3.36it/s] 58%|█████▊    | 229/395 [05:37<00:48,  3.41it/s] 58%|█████▊    | 230/395 [05:38<00:48,  3.43it/s] 58%|█████▊    | 231/395 [05:38<00:47,  3.46it/s] 59%|█████▊    | 232/395 [05:38<00:46,  3.47it/s] 59%|█████▉    | 233/395 [05:39<00:46,  3.48it/s] 59%|█████▉    | 234/395 [05:39<00:46,  3.49it/s] 59%|█████▉    | 235/395 [05:39<00:45,  3.49it/s] 60%|█████▉    | 236/395 [05:39<00:49,  3.19it/s] 60%|██████    | 237/395 [05:40<00:43,  3.65it/s][INFO|trainer.py:2140] 2023-08-28 10:38:26,533 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:38:26,533 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 10:38:26,533 >>   Batch size = 8
{'eval_loss': 1.0106321573257446, 'eval_runtime': 9.9387, 'eval_samples_per_second': 351.857, 'eval_steps_per_second': 44.07, 'epoch': 2.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.63it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.54it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.74it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.11it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.74it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.36it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.80it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.30it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.24it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.43it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.48it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.61it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.68it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 45.77it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.64it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.41it/s][A
 20%|██        | 88/438 [00:01<00:07, 45.11it/s][A
 21%|██        | 93/438 [00:02<00:07, 45.19it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.26it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.33it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 45.35it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 45.47it/s][A
 27%|██▋       | 118/438 [00:02<00:08, 38.64it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 40.55it/s][A
 29%|██▉       | 128/438 [00:02<00:07, 42.06it/s][A
 30%|███       | 133/438 [00:02<00:07, 43.16it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.01it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.57it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.93it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.13it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.80it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.67it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.60it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 44.81it/s][A
 41%|████      | 178/438 [00:03<00:05, 45.09it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.32it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.43it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.65it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.64it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.44it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.21it/s][A
 49%|████▊     | 213/438 [00:04<00:05, 44.95it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.02it/s][A
 51%|█████     | 223/438 [00:04<00:04, 45.10it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 45.28it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.47it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 45.46it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 45.64it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 45.56it/s][A
 58%|█████▊    | 253/438 [00:05<00:05, 36.87it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 39.13it/s][A
 60%|██████    | 263/438 [00:05<00:04, 40.95it/s][A
 61%|██████    | 268/438 [00:06<00:04, 42.31it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 43.34it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 44.02it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 44.61it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.82it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.65it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.55it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 44.62it/s][A
 70%|███████   | 308/438 [00:06<00:02, 44.79it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 45.07it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.35it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.44it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.58it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.52it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 45.31it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 45.05it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 45.07it/s][A
 81%|████████  | 353/438 [00:08<00:01, 45.16it/s][A
 82%|████████▏ | 358/438 [00:08<00:02, 33.07it/s][A
 83%|████████▎ | 363/438 [00:08<00:02, 35.99it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 38.39it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 40.36it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 41.87it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 43.01it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 43.88it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 44.32it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 44.36it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.28it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 44.15it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.45it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.69it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 45.04it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 45.27it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 45.43it/s][A
100%|██████████| 438/438 [00:09<00:00, 45.56it/s][A                                                 
                                                 [A 60%|██████    | 237/395 [05:50<00:43,  3.65it/s]
100%|██████████| 438/438 [00:09<00:00, 45.56it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:38:37,137 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237
[INFO|configuration_utils.py:351] 2023-08-28 10:38:37,698 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:39:00,586 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:39:01,264 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:39:01,538 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237/special_tokens_map.json
 60%|██████    | 238/395 [07:15<1:15:22, 28.80s/it] 61%|██████    | 239/395 [07:15<52:43, 20.28s/it]   61%|██████    | 240/395 [07:16<36:53, 14.28s/it] 61%|██████    | 241/395 [07:16<25:58, 10.12s/it] 61%|██████▏   | 242/395 [07:16<18:17,  7.17s/it] 62%|██████▏   | 243/395 [07:17<12:56,  5.11s/it] 62%|██████▏   | 244/395 [07:17<09:12,  3.66s/it] 62%|██████▏   | 245/395 [07:17<06:37,  2.65s/it] 62%|██████▏   | 246/395 [07:18<04:48,  1.94s/it] 63%|██████▎   | 247/395 [07:18<03:33,  1.44s/it] 63%|██████▎   | 248/395 [07:18<02:47,  1.14s/it] 63%|██████▎   | 249/395 [07:19<02:15,  1.08it/s] 63%|██████▎   | 250/395 [07:19<01:46,  1.36it/s] 64%|██████▎   | 251/395 [07:19<01:26,  1.67it/s] 64%|██████▍   | 252/395 [07:20<01:12,  1.98it/s] 64%|██████▍   | 253/395 [07:20<01:02,  2.27it/s] 64%|██████▍   | 254/395 [07:20<00:55,  2.54it/s] 65%|██████▍   | 255/395 [07:20<00:50,  2.76it/s] 65%|██████▍   | 256/395 [07:21<00:47,  2.95it/s] 65%|██████▌   | 257/395 [07:21<00:44,  3.09it/s] 65%|██████▌   | 258/395 [07:21<00:42,  3.20it/s] 66%|██████▌   | 259/395 [07:22<00:47,  2.88it/s] 66%|██████▌   | 260/395 [07:22<00:44,  3.04it/s] 66%|██████▌   | 261/395 [07:22<00:42,  3.16it/s] 66%|██████▋   | 262/395 [07:23<00:40,  3.25it/s] 67%|██████▋   | 263/395 [07:23<00:39,  3.31it/s] 67%|██████▋   | 264/395 [07:23<00:39,  3.36it/s] 67%|██████▋   | 265/395 [07:23<00:38,  3.39it/s] 67%|██████▋   | 266/395 [07:24<00:37,  3.42it/s] 68%|██████▊   | 267/395 [07:24<00:37,  3.44it/s] 68%|██████▊   | 268/395 [07:24<00:36,  3.45it/s] 68%|██████▊   | 269/395 [07:25<00:39,  3.16it/s] 68%|██████▊   | 270/395 [07:25<00:38,  3.26it/s] 69%|██████▊   | 271/395 [07:25<00:37,  3.33it/s] 69%|██████▉   | 272/395 [07:26<00:36,  3.38it/s] 69%|██████▉   | 273/395 [07:26<00:35,  3.41it/s] 69%|██████▉   | 274/395 [07:26<00:35,  3.44it/s] 70%|██████▉   | 275/395 [07:26<00:34,  3.46it/s] 70%|██████▉   | 276/395 [07:27<00:34,  3.47it/s] 70%|███████   | 277/395 [07:27<00:33,  3.48it/s] 70%|███████   | 278/395 [07:27<00:33,  3.49it/s] 71%|███████   | 279/395 [07:28<00:33,  3.49it/s] 71%|███████   | 280/395 [07:28<00:35,  3.25it/s] 71%|███████   | 281/395 [07:28<00:34,  3.32it/s] 71%|███████▏  | 282/395 [07:28<00:33,  3.37it/s] 72%|███████▏  | 283/395 [07:29<00:32,  3.41it/s] 72%|███████▏  | 284/395 [07:29<00:32,  3.43it/s] 72%|███████▏  | 285/395 [07:29<00:31,  3.45it/s] 72%|███████▏  | 286/395 [07:30<00:31,  3.47it/s] 73%|███████▎  | 287/395 [07:30<00:31,  3.48it/s] 73%|███████▎  | 288/395 [07:30<00:30,  3.48it/s] 73%|███████▎  | 289/395 [07:30<00:30,  3.49it/s] 73%|███████▎  | 290/395 [07:31<00:30,  3.49it/s] 74%|███████▎  | 291/395 [07:31<00:32,  3.19it/s] 74%|███████▍  | 292/395 [07:31<00:31,  3.28it/s] 74%|███████▍  | 293/395 [07:32<00:30,  3.34it/s] 74%|███████▍  | 294/395 [07:32<00:29,  3.39it/s] 75%|███████▍  | 295/395 [07:32<00:29,  3.42it/s] 75%|███████▍  | 296/395 [07:33<00:28,  3.44it/s] 75%|███████▌  | 297/395 [07:33<00:28,  3.46it/s] 75%|███████▌  | 298/395 [07:33<00:27,  3.47it/s] 76%|███████▌  | 299/395 [07:33<00:27,  3.48it/s] 76%|███████▌  | 300/395 [07:34<00:27,  3.48it/s] 76%|███████▌  | 301/395 [07:34<00:26,  3.49it/s] 76%|███████▋  | 302/395 [07:34<00:29,  3.17it/s] 77%|███████▋  | 303/395 [07:35<00:28,  3.26it/s] 77%|███████▋  | 304/395 [07:35<00:27,  3.33it/s] 77%|███████▋  | 305/395 [07:35<00:26,  3.38it/s] 77%|███████▋  | 306/395 [07:35<00:26,  3.41it/s] 78%|███████▊  | 307/395 [07:36<00:25,  3.44it/s] 78%|███████▊  | 308/395 [07:36<00:25,  3.46it/s] 78%|███████▊  | 309/395 [07:36<00:24,  3.47it/s] 78%|███████▊  | 310/395 [07:37<00:24,  3.48it/s] 79%|███████▊  | 311/395 [07:37<00:24,  3.48it/s] 79%|███████▉  | 312/395 [07:37<00:23,  3.48it/s] 79%|███████▉  | 313/395 [07:38<00:25,  3.17it/s] 79%|███████▉  | 314/395 [07:38<00:24,  3.26it/s] 80%|███████▉  | 315/395 [07:38<00:24,  3.32it/s] 80%|████████  | 316/395 [07:38<00:20,  3.76it/s][INFO|trainer.py:2140] 2023-08-28 10:40:25,234 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:40:25,234 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 10:40:25,234 >>   Batch size = 8
{'eval_loss': 1.0142228603363037, 'eval_runtime': 9.9183, 'eval_samples_per_second': 352.581, 'eval_steps_per_second': 44.161, 'epoch': 3.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.36it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.67it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.83it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.97it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.40it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.06it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.55it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.21it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.19it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.42it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.57it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.56it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.53it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.46it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.34it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.12it/s][A
 20%|██        | 88/438 [00:02<00:07, 45.05it/s][A
 21%|██        | 93/438 [00:02<00:09, 37.21it/s][A
 22%|██▏       | 98/438 [00:02<00:08, 39.34it/s][A
 24%|██▎       | 103/438 [00:02<00:08, 41.09it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 42.41it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 43.39it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.01it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.53it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.70it/s][A
 30%|███       | 133/438 [00:02<00:06, 44.59it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.53it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.67it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.87it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.25it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 45.37it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 45.52it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 45.42it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 45.33it/s][A
 41%|████      | 178/438 [00:03<00:05, 45.07it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 44.81it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 44.94it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.05it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.30it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.43it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.54it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.51it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.33it/s][A
 51%|█████     | 223/438 [00:05<00:04, 44.97it/s][A
 52%|█████▏    | 228/438 [00:05<00:05, 35.51it/s][A
 53%|█████▎    | 233/438 [00:05<00:05, 38.11it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 40.14it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 41.67it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 42.78it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 43.65it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.31it/s][A
 60%|██████    | 263/438 [00:05<00:03, 44.61it/s][A
 61%|██████    | 268/438 [00:06<00:03, 44.38it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 44.28it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 44.36it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 44.74it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 45.04it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 45.24it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 45.48it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 45.47it/s][A
 70%|███████   | 308/438 [00:07<00:02, 45.40it/s][A
 71%|███████▏  | 313/438 [00:07<00:04, 30.05it/s][A
 73%|███████▎  | 318/438 [00:07<00:03, 33.53it/s][A
 74%|███████▎  | 323/438 [00:07<00:03, 36.49it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 38.85it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 40.72it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 42.07it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 43.15it/s][A
 79%|███████▉  | 348/438 [00:08<00:02, 43.76it/s][A
 81%|████████  | 353/438 [00:08<00:01, 43.80it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 43.82it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 44.04it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 44.44it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.87it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 45.18it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 45.37it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 45.43it/s][A
 90%|████████▉ | 393/438 [00:09<00:00, 45.32it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 45.13it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.79it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 44.73it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.96it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 45.15it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 45.32it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 45.38it/s][A
 99%|█████████▉| 433/438 [00:10<00:00, 45.41it/s][A
100%|██████████| 438/438 [00:10<00:00, 34.45it/s][A                                                 
                                                 [A 80%|████████  | 316/395 [07:48<00:20,  3.76it/s]
100%|██████████| 438/438 [00:10<00:00, 34.45it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:40:35,640 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316
[INFO|configuration_utils.py:351] 2023-08-28 10:40:36,485 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:41:05,880 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:41:07,548 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:41:07,944 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316/special_tokens_map.json
 80%|████████  | 317/395 [08:41<24:47, 19.06s/it] 81%|████████  | 318/395 [08:42<17:16, 13.46s/it] 81%|████████  | 319/395 [08:42<12:02,  9.51s/it] 81%|████████  | 320/395 [08:42<08:25,  6.74s/it] 81%|████████▏ | 321/395 [08:43<05:55,  4.81s/it] 82%|████████▏ | 322/395 [08:43<04:11,  3.45s/it] 82%|████████▏ | 323/395 [08:43<03:00,  2.50s/it] 82%|████████▏ | 324/395 [08:43<02:10,  1.84s/it] 82%|████████▏ | 325/395 [08:44<01:36,  1.38s/it] 83%|████████▎ | 326/395 [08:44<01:12,  1.05s/it] 83%|████████▎ | 327/395 [08:44<00:55,  1.22it/s] 83%|████████▎ | 328/395 [08:45<00:44,  1.51it/s] 83%|████████▎ | 329/395 [08:45<00:37,  1.74it/s] 84%|████████▎ | 330/395 [08:45<00:31,  2.04it/s] 84%|████████▍ | 331/395 [08:46<00:27,  2.33it/s] 84%|████████▍ | 332/395 [08:46<00:24,  2.58it/s] 84%|████████▍ | 333/395 [08:46<00:22,  2.79it/s] 85%|████████▍ | 334/395 [08:46<00:20,  2.95it/s] 85%|████████▍ | 335/395 [08:47<00:19,  3.09it/s] 85%|████████▌ | 336/395 [08:47<00:18,  3.18it/s] 85%|████████▌ | 337/395 [08:47<00:19,  2.93it/s] 86%|████████▌ | 338/395 [08:48<00:18,  3.07it/s] 86%|████████▌ | 339/395 [08:48<00:17,  3.17it/s] 86%|████████▌ | 340/395 [08:48<00:16,  3.24it/s] 86%|████████▋ | 341/395 [08:49<00:16,  3.30it/s] 87%|████████▋ | 342/395 [08:49<00:15,  3.35it/s] 87%|████████▋ | 343/395 [08:49<00:15,  3.38it/s] 87%|████████▋ | 344/395 [08:49<00:14,  3.41it/s] 87%|████████▋ | 345/395 [08:50<00:14,  3.43it/s] 88%|████████▊ | 346/395 [08:50<00:14,  3.45it/s] 88%|████████▊ | 347/395 [08:50<00:14,  3.25it/s] 88%|████████▊ | 348/395 [08:51<00:14,  3.32it/s] 88%|████████▊ | 349/395 [08:51<00:13,  3.38it/s] 89%|████████▊ | 350/395 [08:51<00:13,  3.41it/s] 89%|████████▉ | 351/395 [08:51<00:12,  3.44it/s] 89%|████████▉ | 352/395 [08:52<00:12,  3.46it/s] 89%|████████▉ | 353/395 [08:52<00:12,  3.47it/s] 90%|████████▉ | 354/395 [08:52<00:11,  3.48it/s] 90%|████████▉ | 355/395 [08:53<00:11,  3.48it/s] 90%|█████████ | 356/395 [08:53<00:11,  3.49it/s] 90%|█████████ | 357/395 [08:53<00:10,  3.49it/s] 91%|█████████ | 358/395 [08:54<00:11,  3.30it/s] 91%|█████████ | 359/395 [08:54<00:10,  3.35it/s] 91%|█████████ | 360/395 [08:54<00:10,  3.39it/s] 91%|█████████▏| 361/395 [08:54<00:09,  3.42it/s] 92%|█████████▏| 362/395 [08:55<00:09,  3.44it/s] 92%|█████████▏| 363/395 [08:55<00:09,  3.46it/s] 92%|█████████▏| 364/395 [08:55<00:08,  3.46it/s] 92%|█████████▏| 365/395 [08:56<00:08,  3.47it/s] 93%|█████████▎| 366/395 [08:56<00:08,  3.47it/s] 93%|█████████▎| 367/395 [08:56<00:08,  3.48it/s] 93%|█████████▎| 368/395 [08:56<00:07,  3.48it/s] 93%|█████████▎| 369/395 [08:57<00:07,  3.37it/s] 94%|█████████▎| 370/395 [08:57<00:07,  3.40it/s] 94%|█████████▍| 371/395 [08:57<00:07,  3.41it/s] 94%|█████████▍| 372/395 [08:58<00:06,  3.42it/s] 94%|█████████▍| 373/395 [08:58<00:06,  3.42it/s] 95%|█████████▍| 374/395 [08:58<00:06,  3.43it/s] 95%|█████████▍| 375/395 [08:58<00:05,  3.43it/s] 95%|█████████▌| 376/395 [08:59<00:05,  3.43it/s] 95%|█████████▌| 377/395 [08:59<00:05,  3.43it/s] 96%|█████████▌| 378/395 [08:59<00:04,  3.43it/s] 96%|█████████▌| 379/395 [09:00<00:04,  3.43it/s] 96%|█████████▌| 380/395 [09:00<00:04,  3.27it/s] 96%|█████████▋| 381/395 [09:00<00:04,  3.31it/s] 97%|█████████▋| 382/395 [09:01<00:03,  3.35it/s] 97%|█████████▋| 383/395 [09:01<00:03,  3.38it/s] 97%|█████████▋| 384/395 [09:01<00:03,  3.39it/s] 97%|█████████▋| 385/395 [09:01<00:02,  3.41it/s] 98%|█████████▊| 386/395 [09:02<00:02,  3.41it/s] 98%|█████████▊| 387/395 [09:02<00:02,  3.42it/s] 98%|█████████▊| 388/395 [09:02<00:02,  3.42it/s] 98%|█████████▊| 389/395 [09:03<00:01,  3.43it/s] 99%|█████████▊| 390/395 [09:03<00:01,  3.43it/s] 99%|█████████▉| 391/395 [09:03<00:01,  3.27it/s] 99%|█████████▉| 392/395 [09:03<00:00,  3.31it/s] 99%|█████████▉| 393/395 [09:04<00:00,  3.35it/s]100%|█████████▉| 394/395 [09:04<00:00,  3.37it/s]100%|██████████| 395/395 [09:04<00:00,  3.83it/s][INFO|trainer.py:2140] 2023-08-28 10:41:51,139 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:41:51,139 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 10:41:51,139 >>   Batch size = 8
{'eval_loss': 1.018333911895752, 'eval_runtime': 10.1346, 'eval_samples_per_second': 345.054, 'eval_steps_per_second': 43.218, 'epoch': 4.0}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.63it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.27it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.55it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.82it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.26it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.80it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.39it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.08it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.95it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.14it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.28it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.30it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.25it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.30it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 45.06it/s][A
 19%|█▊        | 82/438 [00:01<00:09, 39.35it/s][A
 20%|█▉        | 87/438 [00:01<00:08, 40.99it/s][A
 21%|██        | 92/438 [00:02<00:08, 42.31it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.27it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 43.82it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.41it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.69it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.72it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.51it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.37it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.53it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.78it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 45.04it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 45.16it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.29it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.31it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.08it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.87it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.65it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.72it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.89it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.95it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.13it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.17it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.28it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.21it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 45.03it/s][A
 50%|████▉     | 217/438 [00:04<00:06, 33.36it/s][A
 51%|█████     | 222/438 [00:05<00:05, 36.24it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 38.66it/s][A
 53%|█████▎    | 232/438 [00:05<00:05, 40.36it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 41.84it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 42.87it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 43.59it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.01it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.05it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.13it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.37it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.79it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.73it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.91it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.92it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.11it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 45.06it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.89it/s][A
 70%|███████   | 307/438 [00:06<00:02, 44.84it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.90it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.07it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.98it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 45.08it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 45.11it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 45.08it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.98it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 34.50it/s][A
 80%|████████  | 352/438 [00:08<00:02, 37.21it/s][A
 82%|████████▏ | 357/438 [00:08<00:02, 39.34it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 40.99it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 42.29it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 43.20it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 43.91it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.15it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.05it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.05it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.21it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.59it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.81it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 45.11it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 45.18it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 45.20it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 45.01it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 44.81it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 44.64it/s][A                                                 
                                                 [A100%|██████████| 395/395 [09:14<00:00,  3.83it/s]
100%|██████████| 438/438 [00:09<00:00, 44.64it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 10:42:01,267 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395
[INFO|configuration_utils.py:351] 2023-08-28 10:42:01,773 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:42:31,478 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:42:33,367 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:42:33,636 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 10:43:27,148 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 10:43:27,220 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158 (score: 1.0106321573257446).
                                                 100%|██████████| 395/395 [11:17<00:00,  3.83it/s]100%|██████████| 395/395 [11:17<00:00,  1.71s/it]
[INFO|trainer.py:1894] 2023-08-28 10:44:04,768 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model
[INFO|configuration_utils.py:351] 2023-08-28 10:44:05,232 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 10:44:32,091 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 10:44:32,747 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 10:44:33,079 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:44:34,581 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:34,582 >>   epoch                    =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:34,582 >>   train_loss               =     0.7863
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:34,582 >>   train_runtime            = 0:11:17.01
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:34,582 >>   train_samples            =       5027
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:34,582 >>   train_samples_per_second =     37.126
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:34,582 >>   train_steps_per_second   =      0.583
{'eval_loss': 1.0194828510284424, 'eval_runtime': 9.9988, 'eval_samples_per_second': 349.741, 'eval_steps_per_second': 43.805, 'epoch': 5.0}
{'train_runtime': 677.0111, 'train_samples_per_second': 37.126, 'train_steps_per_second': 0.583, 'train_loss': 0.7863356191900712, 'epoch': 5.0}
08/28/2023 10:44:35 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 10:44:35,561 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 10:44:35,561 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 10:44:35,561 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 56.84it/s]  3%|▎         | 12/438 [00:00<00:08, 50.36it/s]  4%|▍         | 18/438 [00:00<00:08, 48.58it/s]  5%|▌         | 23/438 [00:00<00:08, 47.77it/s]  6%|▋         | 28/438 [00:00<00:10, 37.60it/s]  8%|▊         | 33/438 [00:00<00:10, 39.96it/s]  9%|▊         | 38/438 [00:00<00:09, 41.70it/s] 10%|▉         | 43/438 [00:00<00:09, 42.56it/s] 11%|█         | 48/438 [00:01<00:08, 43.37it/s] 12%|█▏        | 53/438 [00:01<00:08, 44.06it/s] 13%|█▎        | 58/438 [00:01<00:08, 44.60it/s] 14%|█▍        | 63/438 [00:01<00:08, 45.00it/s] 16%|█▌        | 68/438 [00:01<00:08, 44.82it/s] 17%|█▋        | 73/438 [00:01<00:08, 44.90it/s] 18%|█▊        | 78/438 [00:01<00:07, 45.05it/s] 19%|█▉        | 83/438 [00:01<00:07, 45.33it/s] 20%|██        | 88/438 [00:01<00:07, 45.32it/s] 21%|██        | 93/438 [00:02<00:07, 45.38it/s] 22%|██▏       | 98/438 [00:02<00:07, 45.41it/s] 24%|██▎       | 103/438 [00:02<00:07, 45.64it/s] 25%|██▍       | 108/438 [00:02<00:07, 45.46it/s] 26%|██▌       | 113/438 [00:02<00:07, 45.36it/s] 27%|██▋       | 118/438 [00:02<00:07, 45.27it/s] 28%|██▊       | 123/438 [00:02<00:06, 45.32it/s] 29%|██▉       | 128/438 [00:02<00:06, 45.35it/s] 30%|███       | 133/438 [00:02<00:06, 45.46it/s] 32%|███▏      | 138/438 [00:03<00:06, 45.42it/s] 33%|███▎      | 143/438 [00:03<00:06, 45.49it/s] 34%|███▍      | 148/438 [00:03<00:06, 45.45it/s] 35%|███▍      | 153/438 [00:03<00:06, 45.51it/s] 36%|███▌      | 158/438 [00:03<00:06, 45.36it/s] 37%|███▋      | 163/438 [00:03<00:07, 38.69it/s] 38%|███▊      | 168/438 [00:03<00:06, 40.65it/s] 39%|███▉      | 173/438 [00:03<00:06, 42.15it/s] 41%|████      | 178/438 [00:04<00:06, 43.13it/s] 42%|████▏     | 183/438 [00:04<00:05, 43.98it/s] 43%|████▎     | 188/438 [00:04<00:05, 44.45it/s] 44%|████▍     | 193/438 [00:04<00:05, 44.96it/s] 45%|████▌     | 198/438 [00:04<00:05, 45.16it/s] 46%|████▋     | 203/438 [00:04<00:05, 44.85it/s] 47%|████▋     | 208/438 [00:04<00:05, 44.63it/s] 49%|████▊     | 213/438 [00:04<00:05, 44.60it/s] 50%|████▉     | 218/438 [00:04<00:04, 44.84it/s] 51%|█████     | 223/438 [00:05<00:04, 45.00it/s] 52%|█████▏    | 228/438 [00:05<00:04, 45.21it/s] 53%|█████▎    | 233/438 [00:05<00:04, 45.38it/s] 54%|█████▍    | 238/438 [00:05<00:04, 45.51it/s] 55%|█████▌    | 243/438 [00:05<00:04, 45.68it/s] 57%|█████▋    | 248/438 [00:05<00:04, 45.37it/s] 58%|█████▊    | 253/438 [00:05<00:04, 45.28it/s] 59%|█████▉    | 258/438 [00:05<00:03, 45.10it/s] 60%|██████    | 263/438 [00:05<00:03, 45.10it/s] 61%|██████    | 268/438 [00:06<00:03, 45.16it/s] 62%|██████▏   | 273/438 [00:06<00:03, 45.32it/s] 63%|██████▎   | 278/438 [00:06<00:03, 45.43it/s] 65%|██████▍   | 283/438 [00:06<00:03, 45.50it/s] 66%|██████▌   | 288/438 [00:06<00:03, 45.62it/s] 67%|██████▋   | 293/438 [00:06<00:03, 45.44it/s] 68%|██████▊   | 298/438 [00:06<00:03, 37.44it/s] 69%|██████▉   | 303/438 [00:06<00:03, 39.43it/s] 70%|███████   | 308/438 [00:06<00:03, 41.26it/s] 71%|███████▏  | 313/438 [00:07<00:02, 42.39it/s] 73%|███████▎  | 318/438 [00:07<00:02, 43.35it/s] 74%|███████▎  | 323/438 [00:07<00:02, 44.08it/s] 75%|███████▍  | 328/438 [00:07<00:02, 44.50it/s] 76%|███████▌  | 333/438 [00:07<00:02, 44.89it/s] 77%|███████▋  | 338/438 [00:07<00:02, 44.59it/s] 78%|███████▊  | 343/438 [00:07<00:02, 44.77it/s] 79%|███████▉  | 348/438 [00:07<00:02, 44.97it/s] 81%|████████  | 353/438 [00:07<00:01, 45.26it/s] 82%|████████▏ | 358/438 [00:08<00:01, 45.35it/s] 83%|████████▎ | 363/438 [00:08<00:01, 45.35it/s] 84%|████████▍ | 368/438 [00:08<00:01, 45.47it/s] 85%|████████▌ | 373/438 [00:08<00:01, 45.43it/s] 86%|████████▋ | 378/438 [00:08<00:01, 45.32it/s] 87%|████████▋ | 383/438 [00:08<00:01, 45.02it/s] 89%|████████▊ | 388/438 [00:08<00:01, 44.97it/s] 90%|████████▉ | 393/438 [00:08<00:00, 45.11it/s] 91%|█████████ | 398/438 [00:08<00:00, 45.24it/s] 92%|█████████▏| 403/438 [00:09<00:00, 45.43it/s] 93%|█████████▎| 408/438 [00:09<00:00, 45.41it/s] 94%|█████████▍| 413/438 [00:09<00:00, 45.43it/s] 95%|█████████▌| 418/438 [00:09<00:00, 45.46it/s] 97%|█████████▋| 423/438 [00:09<00:00, 45.35it/s] 98%|█████████▊| 428/438 [00:09<00:00, 45.23it/s] 99%|█████████▉| 433/438 [00:09<00:00, 33.40it/s]100%|██████████| 438/438 [00:09<00:00, 36.30it/s]100%|██████████| 438/438 [00:09<00:00, 43.97it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 10:44:45,559 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:45,560 >>   epoch                   =        5.0
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:45,560 >>   eval_loss               =     1.0106
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:45,560 >>   eval_runtime            = 0:00:09.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:45,560 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:45,560 >>   eval_samples_per_second =    349.758
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:45,560 >>   eval_steps_per_second   =     43.807
[INFO|trainer_pt_utils.py:913] 2023-08-28 10:44:45,560 >>   perplexity              =     2.7473
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:09,091 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:09,179 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:09,179 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:09,179 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:09,179 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:45:10,415 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:45:10,416 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:45:11,120 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:45:12,335 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:45:12,381 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:15,577 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:15,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:15,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:15,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:45:15,627 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:45:16,578 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:45:16,580 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:45:17,252 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:45:17,527 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:45:17,527 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-395
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-237
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-79
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-316
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/checkpoint-158
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 2it [00:01,  1.52it/s]Extractor Predicting: 3it [00:01,  1.55it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.58it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.55it/s]Extractor Predicting: 8it [00:05,  1.61it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.58it/s]Extractor Predicting: 12it [00:07,  1.56it/s]Extractor Predicting: 13it [00:08,  1.48it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.53it/s]Extractor Predicting: 17it [00:10,  1.54it/s]Extractor Predicting: 18it [00:11,  1.48it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.57it/s]Extractor Predicting: 22it [00:14,  1.59it/s]Extractor Predicting: 23it [00:14,  1.53it/s]Extractor Predicting: 24it [00:15,  1.60it/s]Extractor Predicting: 25it [00:16,  1.63it/s]Extractor Predicting: 26it [00:16,  1.62it/s]Extractor Predicting: 27it [00:17,  1.60it/s]Extractor Predicting: 28it [00:18,  1.53it/s]Extractor Predicting: 29it [00:18,  1.54it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:19,  1.53it/s]Extractor Predicting: 32it [00:20,  1.50it/s]Extractor Predicting: 33it [00:21,  1.40it/s]Extractor Predicting: 34it [00:22,  1.46it/s]Extractor Predicting: 35it [00:22,  1.49it/s]Extractor Predicting: 36it [00:23,  1.48it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:24,  1.45it/s]Extractor Predicting: 39it [00:25,  1.44it/s]Extractor Predicting: 40it [00:26,  1.48it/s]Extractor Predicting: 41it [00:26,  1.48it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.47it/s]Extractor Predicting: 44it [00:28,  1.49it/s]Extractor Predicting: 45it [00:29,  1.50it/s]Extractor Predicting: 46it [00:30,  1.51it/s]Extractor Predicting: 47it [00:30,  1.49it/s]Extractor Predicting: 48it [00:31,  1.48it/s]Extractor Predicting: 49it [00:32,  1.48it/s]Extractor Predicting: 50it [00:32,  1.51it/s]Extractor Predicting: 51it [00:33,  1.54it/s]Extractor Predicting: 52it [00:34,  1.53it/s]Extractor Predicting: 53it [00:34,  1.47it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:36,  1.50it/s]Extractor Predicting: 56it [00:36,  1.50it/s]Extractor Predicting: 57it [00:37,  1.54it/s]Extractor Predicting: 58it [00:38,  1.53it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:39,  1.44it/s]Extractor Predicting: 61it [00:40,  1.43it/s]Extractor Predicting: 62it [00:40,  1.49it/s]Extractor Predicting: 63it [00:41,  1.54it/s]Extractor Predicting: 64it [00:42,  1.51it/s]Extractor Predicting: 65it [00:42,  1.46it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:44,  1.53it/s]Extractor Predicting: 68it [00:44,  1.56it/s]Extractor Predicting: 69it [00:45,  1.55it/s]Extractor Predicting: 70it [00:46,  1.54it/s]Extractor Predicting: 71it [00:46,  1.57it/s]Extractor Predicting: 72it [00:47,  1.61it/s]Extractor Predicting: 73it [00:47,  1.59it/s]Extractor Predicting: 74it [00:48,  1.56it/s]Extractor Predicting: 75it [00:49,  1.52it/s]Extractor Predicting: 76it [00:49,  1.55it/s]Extractor Predicting: 77it [00:50,  1.60it/s]Extractor Predicting: 78it [00:51,  1.58it/s]Extractor Predicting: 79it [00:51,  1.62it/s]Extractor Predicting: 80it [00:52,  1.57it/s]Extractor Predicting: 81it [00:53,  1.59it/s]Extractor Predicting: 82it [00:53,  1.57it/s]Extractor Predicting: 83it [00:54,  1.56it/s]Extractor Predicting: 84it [00:54,  1.55it/s]Extractor Predicting: 85it [00:55,  1.49it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:56,  1.53it/s]Extractor Predicting: 88it [00:57,  1.53it/s]Extractor Predicting: 89it [00:58,  1.54it/s]Extractor Predicting: 90it [00:59,  1.48it/s]Extractor Predicting: 91it [00:59,  1.50it/s]Extractor Predicting: 92it [01:00,  1.52it/s]Extractor Predicting: 93it [01:00,  1.52it/s]Extractor Predicting: 94it [01:01,  1.56it/s]Extractor Predicting: 95it [01:02,  1.53it/s]Extractor Predicting: 96it [01:02,  1.53it/s]Extractor Predicting: 97it [01:03,  1.51it/s]Extractor Predicting: 98it [01:04,  1.52it/s]Extractor Predicting: 99it [01:04,  1.51it/s]Extractor Predicting: 100it [01:05,  1.47it/s]Extractor Predicting: 101it [01:06,  1.53it/s]Extractor Predicting: 102it [01:06,  1.53it/s]Extractor Predicting: 103it [01:07,  1.53it/s]Extractor Predicting: 104it [01:08,  1.52it/s]Extractor Predicting: 105it [01:08,  1.53it/s]Extractor Predicting: 106it [01:09,  1.54it/s]Extractor Predicting: 107it [01:10,  1.51it/s]Extractor Predicting: 108it [01:10,  1.45it/s]Extractor Predicting: 109it [01:11,  1.45it/s]Extractor Predicting: 110it [01:12,  1.48it/s]Extractor Predicting: 111it [01:12,  1.50it/s]Extractor Predicting: 112it [01:13,  1.53it/s]Extractor Predicting: 113it [01:14,  1.44it/s]Extractor Predicting: 114it [01:14,  1.47it/s]Extractor Predicting: 115it [01:15,  1.48it/s]Extractor Predicting: 116it [01:16,  1.46it/s]Extractor Predicting: 117it [01:17,  1.47it/s]Extractor Predicting: 118it [01:17,  1.45it/s]Extractor Predicting: 119it [01:18,  1.47it/s]Extractor Predicting: 120it [01:19,  1.47it/s]Extractor Predicting: 121it [01:19,  1.49it/s]Extractor Predicting: 122it [01:20,  1.51it/s]Extractor Predicting: 123it [01:21,  1.45it/s]Extractor Predicting: 124it [01:21,  1.47it/s]Extractor Predicting: 125it [01:22,  1.46it/s]Extractor Predicting: 126it [01:23,  1.37it/s]Extractor Predicting: 127it [01:23,  1.43it/s]Extractor Predicting: 128it [01:24,  1.42it/s]Extractor Predicting: 129it [01:25,  1.45it/s]Extractor Predicting: 130it [01:25,  1.47it/s]Extractor Predicting: 131it [01:26,  1.45it/s]Extractor Predicting: 132it [01:27,  1.47it/s]Extractor Predicting: 133it [01:28,  1.43it/s]Extractor Predicting: 134it [01:28,  1.45it/s]Extractor Predicting: 135it [01:29,  1.43it/s]Extractor Predicting: 136it [01:30,  1.41it/s]Extractor Predicting: 137it [01:30,  1.45it/s]Extractor Predicting: 138it [01:31,  1.44it/s]Extractor Predicting: 139it [01:32,  1.45it/s]Extractor Predicting: 140it [01:32,  1.47it/s]Extractor Predicting: 141it [01:33,  1.48it/s]Extractor Predicting: 142it [01:34,  1.45it/s]Extractor Predicting: 143it [01:35,  1.42it/s]Extractor Predicting: 144it [01:35,  1.48it/s]Extractor Predicting: 145it [01:36,  1.68it/s]Extractor Predicting: 145it [01:36,  1.51it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:20,374 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:20,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:20,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:20,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:20,456 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:47:21,711 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:47:21,712 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:47:22,572 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:47:23,769 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:47:23,825 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:27,750 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:27,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:27,813 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:27,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:47:27,814 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:47:29,038 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:47:29,039 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:47:30,048 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:47:30,354 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:47:30,354 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.7026239067055393,
  "recall": 0.13783242779525307,
  "score": 0.23045661008845325,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 2it [00:01,  1.59it/s]Extractor Predicting: 3it [00:01,  1.60it/s]Extractor Predicting: 4it [00:02,  1.61it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:03,  1.53it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.54it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.64it/s]Extractor Predicting: 11it [00:07,  1.55it/s]Extractor Predicting: 12it [00:07,  1.57it/s]Extractor Predicting: 13it [00:08,  1.57it/s]Extractor Predicting: 14it [00:08,  1.59it/s]Extractor Predicting: 15it [00:09,  1.55it/s]Extractor Predicting: 16it [00:10,  1.50it/s]Extractor Predicting: 17it [00:10,  1.53it/s]Extractor Predicting: 18it [00:11,  1.56it/s]Extractor Predicting: 19it [00:12,  1.54it/s]Extractor Predicting: 20it [00:12,  1.54it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:14,  1.56it/s]Extractor Predicting: 24it [00:15,  1.54it/s]Extractor Predicting: 25it [00:16,  1.56it/s]Extractor Predicting: 26it [00:16,  1.51it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:17,  1.55it/s]Extractor Predicting: 29it [00:18,  1.55it/s]Extractor Predicting: 30it [00:19,  1.57it/s]Extractor Predicting: 31it [00:19,  1.56it/s]Extractor Predicting: 32it [00:20,  1.55it/s]Extractor Predicting: 33it [00:21,  1.54it/s]Extractor Predicting: 34it [00:21,  1.55it/s]Extractor Predicting: 35it [00:22,  1.56it/s]Extractor Predicting: 36it [00:23,  1.56it/s]Extractor Predicting: 37it [00:23,  1.59it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:24,  1.60it/s]Extractor Predicting: 40it [00:25,  1.57it/s]Extractor Predicting: 41it [00:26,  1.53it/s]Extractor Predicting: 42it [00:26,  1.55it/s]Extractor Predicting: 43it [00:27,  1.53it/s]Extractor Predicting: 44it [00:28,  1.55it/s]Extractor Predicting: 45it [00:28,  1.53it/s]Extractor Predicting: 46it [00:29,  1.51it/s]Extractor Predicting: 47it [00:30,  1.54it/s]Extractor Predicting: 48it [00:30,  1.56it/s]Extractor Predicting: 49it [00:31,  1.59it/s]Extractor Predicting: 50it [00:32,  1.58it/s]Extractor Predicting: 51it [00:32,  1.57it/s]Extractor Predicting: 52it [00:33,  1.60it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:34,  1.45it/s]Extractor Predicting: 55it [00:35,  1.48it/s]Extractor Predicting: 56it [00:36,  1.48it/s]Extractor Predicting: 57it [00:36,  1.48it/s]Extractor Predicting: 58it [00:37,  1.52it/s]Extractor Predicting: 59it [00:38,  1.46it/s]Extractor Predicting: 60it [00:38,  1.49it/s]Extractor Predicting: 61it [00:39,  1.50it/s]Extractor Predicting: 62it [00:40,  1.54it/s]Extractor Predicting: 63it [00:40,  1.53it/s]Extractor Predicting: 64it [00:41,  1.53it/s]Extractor Predicting: 65it [00:42,  1.53it/s]Extractor Predicting: 66it [00:42,  1.56it/s]Extractor Predicting: 67it [00:43,  1.57it/s]Extractor Predicting: 68it [00:43,  1.61it/s]Extractor Predicting: 69it [00:44,  1.60it/s]Extractor Predicting: 70it [00:45,  1.59it/s]Extractor Predicting: 71it [00:45,  1.60it/s]Extractor Predicting: 72it [00:46,  1.61it/s]Extractor Predicting: 73it [00:47,  1.62it/s]Extractor Predicting: 74it [00:47,  1.56it/s]Extractor Predicting: 75it [00:48,  1.54it/s]Extractor Predicting: 76it [00:48,  1.57it/s]Extractor Predicting: 77it [00:49,  1.61it/s]Extractor Predicting: 78it [00:50,  1.61it/s]Extractor Predicting: 79it [00:50,  1.55it/s]Extractor Predicting: 80it [00:51,  1.59it/s]Extractor Predicting: 81it [00:52,  1.59it/s]Extractor Predicting: 82it [00:52,  1.59it/s]Extractor Predicting: 83it [00:53,  1.58it/s]Extractor Predicting: 84it [00:54,  1.55it/s]Extractor Predicting: 85it [00:54,  1.57it/s]Extractor Predicting: 86it [00:55,  1.62it/s]Extractor Predicting: 87it [00:55,  1.63it/s]Extractor Predicting: 88it [00:56,  1.61it/s]Extractor Predicting: 89it [00:57,  1.51it/s]Extractor Predicting: 90it [00:57,  1.56it/s]Extractor Predicting: 91it [00:58,  1.60it/s]Extractor Predicting: 92it [00:59,  1.57it/s]Extractor Predicting: 93it [00:59,  1.55it/s]Extractor Predicting: 94it [01:00,  1.49it/s]Extractor Predicting: 95it [01:01,  1.51it/s]Extractor Predicting: 96it [01:01,  1.55it/s]Extractor Predicting: 97it [01:02,  1.56it/s]Extractor Predicting: 98it [01:03,  1.54it/s]Extractor Predicting: 99it [01:03,  1.56it/s]Extractor Predicting: 100it [01:04,  1.56it/s]Extractor Predicting: 101it [01:04,  1.53it/s]Extractor Predicting: 102it [01:05,  1.49it/s]Extractor Predicting: 103it [01:06,  1.52it/s]Extractor Predicting: 104it [01:06,  1.52it/s]Extractor Predicting: 105it [01:07,  1.51it/s]Extractor Predicting: 106it [01:08,  1.45it/s]Extractor Predicting: 107it [01:09,  1.47it/s]Extractor Predicting: 108it [01:09,  1.52it/s]Extractor Predicting: 109it [01:10,  1.54it/s]Extractor Predicting: 110it [01:10,  1.59it/s]Extractor Predicting: 111it [01:11,  1.56it/s]Extractor Predicting: 112it [01:12,  1.56it/s]Extractor Predicting: 113it [01:12,  1.56it/s]Extractor Predicting: 114it [01:13,  1.54it/s]Extractor Predicting: 115it [01:14,  1.55it/s]Extractor Predicting: 116it [01:14,  1.51it/s]Extractor Predicting: 117it [01:15,  1.55it/s]Extractor Predicting: 118it [01:16,  1.55it/s]Extractor Predicting: 119it [01:16,  1.56it/s]Extractor Predicting: 120it [01:17,  1.55it/s]Extractor Predicting: 121it [01:18,  1.50it/s]Extractor Predicting: 122it [01:18,  1.55it/s]Extractor Predicting: 123it [01:19,  1.57it/s]Extractor Predicting: 124it [01:19,  1.57it/s]Extractor Predicting: 125it [01:20,  1.56it/s]Extractor Predicting: 126it [01:21,  1.48it/s]Extractor Predicting: 127it [01:21,  1.50it/s]Extractor Predicting: 128it [01:22,  1.53it/s]Extractor Predicting: 129it [01:23,  1.58it/s]Extractor Predicting: 130it [01:23,  1.57it/s]Extractor Predicting: 131it [01:24,  1.53it/s]Extractor Predicting: 132it [01:25,  1.55it/s]Extractor Predicting: 133it [01:25,  1.54it/s]Extractor Predicting: 134it [01:26,  1.54it/s]Extractor Predicting: 135it [01:27,  1.54it/s]Extractor Predicting: 136it [01:27,  1.50it/s]Extractor Predicting: 137it [01:28,  1.53it/s]Extractor Predicting: 138it [01:29,  1.55it/s]Extractor Predicting: 139it [01:29,  1.55it/s]Extractor Predicting: 140it [01:30,  1.55it/s]Extractor Predicting: 141it [01:31,  1.51it/s]Extractor Predicting: 142it [01:31,  1.50it/s]Extractor Predicting: 143it [01:32,  1.52it/s]Extractor Predicting: 144it [01:33,  1.54it/s]Extractor Predicting: 145it [01:33,  1.94it/s]Extractor Predicting: 145it [01:33,  1.56it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:25,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:25,843 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:25,843 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:25,844 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:25,844 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:49:27,076 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:49:27,077 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:49:27,845 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:49:29,043 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:49:29,122 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:32,655 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:32,808 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:32,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:32,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:49:32,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:49:34,050 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:49:34,051 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:49:34,842 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:49:35,267 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:49:35,267 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4669652855543113,
  "recall": 0.12055507372072853,
  "score": 0.19163602941176472,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.56it/s]Extractor Predicting: 1it [00:00,  1.56it/s]
[INFO|configuration_utils.py:515] 2023-08-28 10:49:41,115 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:49:41,116 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 10:49:41,355 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:49:41,356 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 10:49:41,453 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 10:50:14,904 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 10:50:14,957 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 10:50:15,612 >> loading configuration file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 10:50:15,613 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 10:50:15,882 >> Didn't find file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:50:16,069 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:50:16,070 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:50:16,070 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:50:16,070 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:50:16,070 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 10:50:16,070 >> loading file outputs/wrapper/fewrel/unseen_5_seed_4/generator/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/pred_in_multi_is_eval_False.jsonl",
  "precision": 1.0,
  "recall": 0.04878048780487805,
  "score": 0.09302325581395349,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/data', model_name='outputs/wrapper/fewrel/unseen_5_seed_4/generator/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 10:50:16,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:17,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:17,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:18,665 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:19,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:19,827 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:20,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:21,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:21,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:22,243 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:22,900 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:23,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:24,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:24,790 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:25,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:26,035 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:26,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:27,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:27,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:28,528 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:29,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:29,899 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:13<02:04, 13.81s/it][WARNING|generation_utils.py:914] 2023-08-28 10:50:30,614 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:31,482 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:32,217 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:32,789 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:33,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:34,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:34,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:35,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:36,092 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:36,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:37,367 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:38,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:38,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:39,364 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:39,948 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:40,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:41,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:41,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:42,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:43,186 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:43,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:44,666 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:28<01:55, 14.40s/it][WARNING|generation_utils.py:914] 2023-08-28 10:50:45,481 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:46,120 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:46,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:47,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:48,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:48,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:49,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:50,278 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:50,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:51,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:52,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:53,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:53,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:54,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:54,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:55,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:56,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:56,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:57,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:58,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:59,105 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:50:59,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:00,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:44<01:45, 15.10s/it][WARNING|generation_utils.py:914] 2023-08-28 10:51:01,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:01,826 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:02,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:03,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:03,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:04,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:05,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:06,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:06,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:07,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:08,036 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:08,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:09,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:09,957 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:10,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:11,314 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:11,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:12,483 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:13,147 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:13,696 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:14,267 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:14,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:15,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:59<01:29, 14.93s/it][WARNING|generation_utils.py:914] 2023-08-28 10:51:16,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:16,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:17,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:17,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:18,470 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:19,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:19,922 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:20,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:21,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:21,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:22,471 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:23,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:23,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:24,959 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:25,605 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:26,429 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:27,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:27,762 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:28,365 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:28,996 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:29,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:30,328 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:30,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:14<01:15, 15.17s/it][WARNING|generation_utils.py:914] 2023-08-28 10:51:31,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:32,191 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:32,958 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:33,555 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:34,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:34,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:35,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:35,625 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:36,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:36,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:37,256 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:37,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:38,398 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:39,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:39,718 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:40,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:40,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:41,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:41,869 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:42,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:42,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:43,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:44,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:27<00:57, 14.47s/it][WARNING|generation_utils.py:914] 2023-08-28 10:51:44,657 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:45,294 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:45,813 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:46,295 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:46,819 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:47,584 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:48,125 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:48,721 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:49,253 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:49,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:50,401 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:51,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:51,731 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:52,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:52,881 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:53,375 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:53,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:54,703 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:55,221 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:55,795 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:56,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:40<00:41, 13.78s/it][WARNING|generation_utils.py:914] 2023-08-28 10:51:57,121 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:57,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:58,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:58,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:51:59,473 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:00,009 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:00,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:01,289 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:01,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:02,449 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:03,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:03,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:04,556 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:05,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:05,852 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:06,461 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:07,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:07,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:08,331 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:08,921 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:09,547 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:10,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:10,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:11,494 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:55<00:28, 14.17s/it][WARNING|generation_utils.py:914] 2023-08-28 10:52:12,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:12,619 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:13,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:14,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:14,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:15,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:16,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:16,558 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:17,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:17,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:18,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:19,190 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:19,805 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:20,472 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:21,149 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:21,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:22,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:22,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:23,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:24,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:24,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:25,486 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:26,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:10<00:14, 14.35s/it][WARNING|generation_utils.py:914] 2023-08-28 10:52:26,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:27,362 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:28,052 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:28,751 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:29,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:29,799 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:30,348 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:30,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:31,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:32,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:33,227 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:33,735 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:34,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:35,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:35,557 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:36,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:36,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:37,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:37,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:38,416 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:38,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:39,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:40,025 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:40,648 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:41,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:41,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:42,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:43,101 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:43,606 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:44,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:44,938 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:45,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 10:52:46,055 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:29<00:00, 16.07s/it]Generating: 100%|██████████| 10/10 [02:29<00:00, 15.00s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:52:56,641 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:52:56,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:52:56,746 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:52:56,747 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:52:56,747 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:52:58,076 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:52:58,077 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:52:58,681 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:53:00,080 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:53:00,200 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:53:02,344 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:53:02,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:53:02,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:53:02,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:53:02,346 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:53:03,335 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:53:03,407 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:53:03,871 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:53:04,207 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:53:04,207 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 227, 'raw': 256}
{'target': 600, 'success': 254, 'raw': 288}
{'target': 600, 'success': 281, 'raw': 320}
{'target': 600, 'success': 308, 'raw': 352}
{'target': 600, 'success': 333, 'raw': 384}
{'target': 600, 'success': 362, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 417, 'raw': 480}
{'target': 600, 'success': 446, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 496, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 580, 'raw': 672}
{'target': 600, 'success': 604, 'raw': 704}
{'prompt': 'Relation : director .', 'success_rate': 0.8579545454545454, 'errors': {''}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 110, 'raw': 128}
{'target': 600, 'success': 137, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 190, 'raw': 224}
{'target': 600, 'success': 217, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 270, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 331, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 385, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 469, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 524, 'raw': 608}
{'target': 600, 'success': 551, 'raw': 640}
{'target': 600, 'success': 579, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.8664772727272727, 'errors': {'', 'too many values to unpack (expected 2)'}}
['Relation : mother . Context : Later in Life , the children became members of the Church of All Saints . Head Entity : Lady Mary , Tail Entity : Church of All Saints .\n']
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 52, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 101, 'raw': 128}
{'target': 600, 'success': 128, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 180, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 262, 'raw': 320}
{'target': 600, 'success': 286, 'raw': 352}
{'target': 600, 'success': 314, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 368, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 420, 'raw': 512}
{'target': 600, 'success': 444, 'raw': 544}
{'target': 600, 'success': 469, 'raw': 576}
{'target': 600, 'success': 498, 'raw': 608}
{'target': 600, 'success': 528, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 577, 'raw': 704}
{'target': 600, 'success': 602, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8179347826086957, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 80, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 158, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 213, 'raw': 256}
{'target': 600, 'success': 241, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 291, 'raw': 352}
{'target': 600, 'success': 313, 'raw': 384}
{'target': 600, 'success': 340, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 393, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 452, 'raw': 544}
{'target': 600, 'success': 479, 'raw': 576}
{'target': 600, 'success': 508, 'raw': 608}
{'target': 600, 'success': 533, 'raw': 640}
{'target': 600, 'success': 558, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : part of .', 'success_rate': 0.8342391304347826, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 79, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 152, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 289, 'raw': 352}
{'target': 600, 'success': 315, 'raw': 384}
{'target': 600, 'success': 341, 'raw': 416}
{'target': 600, 'success': 367, 'raw': 448}
{'target': 600, 'success': 396, 'raw': 480}
{'target': 600, 'success': 423, 'raw': 512}
{'target': 600, 'success': 448, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 502, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 554, 'raw': 672}
{'target': 600, 'success': 581, 'raw': 704}
{'target': 600, 'success': 609, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8274456521739131, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : developer . Context : Later in 2008 , the studio announced that they were re - developing " The Last of Us " based upon the game and also included the first trailer . Head Entity : The Last of Us , Tail Entity : The studio .\n']
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 74, 'raw': 96}
{'target': 600, 'success': 98, 'raw': 128}
{'target': 600, 'success': 127, 'raw': 160}
{'target': 600, 'success': 156, 'raw': 192}
{'target': 600, 'success': 184, 'raw': 224}
{'target': 600, 'success': 211, 'raw': 256}
{'target': 600, 'success': 237, 'raw': 288}
{'target': 600, 'success': 264, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 318, 'raw': 384}
{'target': 600, 'success': 343, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 397, 'raw': 480}
{'target': 600, 'success': 419, 'raw': 512}
{'target': 600, 'success': 446, 'raw': 544}
{'target': 600, 'success': 473, 'raw': 576}
{'target': 600, 'success': 501, 'raw': 608}
{'target': 600, 'success': 527, 'raw': 640}
{'target': 600, 'success': 552, 'raw': 672}
{'target': 600, 'success': 580, 'raw': 704}
{'target': 600, 'success': 605, 'raw': 736}
{'prompt': 'Relation : developer .', 'success_rate': 0.8220108695652174, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 92, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 183, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 241, 'raw': 256}
{'target': 600, 'success': 268, 'raw': 288}
{'target': 600, 'success': 298, 'raw': 320}
{'target': 600, 'success': 329, 'raw': 352}
{'target': 600, 'success': 357, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 419, 'raw': 448}
{'target': 600, 'success': 450, 'raw': 480}
{'target': 600, 'success': 479, 'raw': 512}
{'target': 600, 'success': 508, 'raw': 544}
{'target': 600, 'success': 538, 'raw': 576}
{'target': 600, 'success': 568, 'raw': 608}
{'target': 600, 'success': 596, 'raw': 640}
{'target': 600, 'success': 626, 'raw': 672}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9315476190476191, 'errors': {''}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 131, 'raw': 160}
{'target': 600, 'success': 159, 'raw': 192}
{'target': 600, 'success': 189, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 293, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 371, 'raw': 448}
{'target': 600, 'success': 394, 'raw': 480}
{'target': 600, 'success': 418, 'raw': 512}
{'target': 600, 'success': 440, 'raw': 544}
{'target': 600, 'success': 468, 'raw': 576}
{'target': 600, 'success': 490, 'raw': 608}
{'target': 600, 'success': 517, 'raw': 640}
{'target': 600, 'success': 542, 'raw': 672}
{'target': 600, 'success': 570, 'raw': 704}
{'target': 600, 'success': 599, 'raw': 736}
{'target': 600, 'success': 621, 'raw': 768}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.80859375, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 51, 'raw': 64}
{'target': 600, 'success': 77, 'raw': 96}
{'target': 600, 'success': 102, 'raw': 128}
{'target': 600, 'success': 134, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 240, 'raw': 288}
{'target': 600, 'success': 268, 'raw': 320}
{'target': 600, 'success': 295, 'raw': 352}
{'target': 600, 'success': 321, 'raw': 384}
{'target': 600, 'success': 347, 'raw': 416}
{'target': 600, 'success': 373, 'raw': 448}
{'target': 600, 'success': 403, 'raw': 480}
{'target': 600, 'success': 427, 'raw': 512}
{'target': 600, 'success': 455, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 514, 'raw': 608}
{'target': 600, 'success': 542, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 620, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.842391304347826, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : On 31 March 2014 , the Armenian Football Association announced a deal to sign Armenian international Federico Rosimi from club Azeri Premier League side FC Karpatyız . Head Entity : Federico Rosimi , Tail Entity : forward .\n']
['Relation : position held . Context : On 31 March 2014 , the Armenian Football Association announced a deal to sign Armenian international Federico Rosimi from club Azeri Premier League side FC Karpatyız . Head Entity : Federico Rosimi , Tail Entity : forward .\n', 'Relation : position held . Context : James Harrison was the only active member of his team to play with 100 - or more yards receiving for the Bears team that finished 3 - 2 . Head Entity : James Harrison , Tail Entity : quarterback .\n']
['Relation : position held . Context : On 31 March 2014 , the Armenian Football Association announced a deal to sign Armenian international Federico Rosimi from club Azeri Premier League side FC Karpatyız . Head Entity : Federico Rosimi , Tail Entity : forward .\n', 'Relation : position held . Context : James Harrison was the only active member of his team to play with 100 - or more yards receiving for the Bears team that finished 3 - 2 . Head Entity : James Harrison , Tail Entity : quarterback .\n', "Relation : position held . Context : This was the first Russian representative of the National Bolshevik Party , and the third of the party 's members to represent Bulgaria in the World Congress . Head Entity : Nzhgoromir Stachuk , Tail Entity : representative .\n"]
{'target': 600, 'success': 18, 'raw': 32}
{'target': 600, 'success': 36, 'raw': 64}
{'target': 600, 'success': 57, 'raw': 96}
{'target': 600, 'success': 75, 'raw': 128}
{'target': 600, 'success': 95, 'raw': 160}
{'target': 600, 'success': 113, 'raw': 192}
{'target': 600, 'success': 133, 'raw': 224}
{'target': 600, 'success': 150, 'raw': 256}
{'target': 600, 'success': 172, 'raw': 288}
{'target': 600, 'success': 192, 'raw': 320}
{'target': 600, 'success': 206, 'raw': 352}
{'target': 600, 'success': 224, 'raw': 384}
{'target': 600, 'success': 240, 'raw': 416}
{'target': 600, 'success': 259, 'raw': 448}
{'target': 600, 'success': 281, 'raw': 480}
{'target': 600, 'success': 297, 'raw': 512}
{'target': 600, 'success': 312, 'raw': 544}
{'target': 600, 'success': 332, 'raw': 576}
{'target': 600, 'success': 350, 'raw': 608}
{'target': 600, 'success': 366, 'raw': 640}
{'target': 600, 'success': 385, 'raw': 672}
{'target': 600, 'success': 408, 'raw': 704}
{'target': 600, 'success': 427, 'raw': 736}
{'target': 600, 'success': 443, 'raw': 768}
{'target': 600, 'success': 463, 'raw': 800}
{'target': 600, 'success': 484, 'raw': 832}
{'target': 600, 'success': 497, 'raw': 864}
{'target': 600, 'success': 516, 'raw': 896}
{'target': 600, 'success': 534, 'raw': 928}
{'target': 600, 'success': 556, 'raw': 960}
{'target': 600, 'success': 572, 'raw': 992}
{'target': 600, 'success': 594, 'raw': 1024}
{'target': 600, 'success': 617, 'raw': 1056}
{'prompt': 'Relation : position held .', 'success_rate': 0.584280303030303, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/1.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/1_ext.jsonl'}}
estimate vocab size: 10507
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 10607, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.36it/s]Extractor Estimating: 2it [00:01,  1.43it/s]Extractor Estimating: 3it [00:02,  1.47it/s]Extractor Estimating: 4it [00:02,  1.50it/s]Extractor Estimating: 5it [00:03,  1.48it/s]Extractor Estimating: 6it [00:04,  1.46it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.50it/s]Extractor Estimating: 9it [00:06,  1.51it/s]Extractor Estimating: 10it [00:06,  1.49it/s]Extractor Estimating: 11it [00:07,  1.54it/s]Extractor Estimating: 12it [00:07,  1.57it/s]Extractor Estimating: 13it [00:08,  1.55it/s]Extractor Estimating: 14it [00:09,  1.59it/s]Extractor Estimating: 15it [00:09,  1.53it/s]Extractor Estimating: 16it [00:10,  1.54it/s]Extractor Estimating: 17it [00:11,  1.57it/s]Extractor Estimating: 18it [00:11,  1.53it/s]Extractor Estimating: 19it [00:12,  1.55it/s]Extractor Estimating: 20it [00:13,  1.48it/s]Extractor Estimating: 21it [00:13,  1.51it/s]Extractor Estimating: 22it [00:14,  1.51it/s]Extractor Estimating: 23it [00:15,  1.54it/s]Extractor Estimating: 24it [00:16,  1.17it/s]Extractor Estimating: 25it [00:17,  1.22it/s]Extractor Estimating: 26it [00:17,  1.29it/s]Extractor Estimating: 27it [00:18,  1.40it/s]Extractor Estimating: 28it [00:18,  1.52it/s]Extractor Estimating: 29it [00:19,  1.59it/s]Extractor Estimating: 30it [00:20,  1.52it/s]Extractor Estimating: 31it [00:20,  1.63it/s]Extractor Estimating: 32it [00:21,  1.66it/s]Extractor Estimating: 33it [00:22,  1.58it/s]Extractor Estimating: 34it [00:22,  1.65it/s]Extractor Estimating: 35it [00:23,  1.54it/s]Extractor Estimating: 36it [00:24,  1.48it/s]Extractor Estimating: 37it [00:24,  1.55it/s]Extractor Estimating: 38it [00:25,  1.61it/s]Extractor Estimating: 39it [00:25,  1.63it/s]Extractor Estimating: 40it [00:26,  1.67it/s]Extractor Estimating: 41it [00:27,  1.59it/s]Extractor Estimating: 42it [00:27,  1.68it/s]Extractor Estimating: 43it [00:28,  1.72it/s]Extractor Estimating: 44it [00:28,  1.71it/s]Extractor Estimating: 45it [00:29,  1.72it/s]Extractor Estimating: 46it [00:29,  1.78it/s]Extractor Estimating: 47it [00:30,  1.74it/s]Extractor Estimating: 48it [00:31,  1.69it/s]Extractor Estimating: 49it [00:31,  1.76it/s]Extractor Estimating: 50it [00:32,  1.77it/s]Extractor Estimating: 51it [00:32,  1.72it/s]Extractor Estimating: 52it [00:33,  1.73it/s]Extractor Estimating: 53it [00:33,  1.68it/s]Extractor Estimating: 54it [00:34,  1.68it/s]Extractor Estimating: 55it [00:35,  1.67it/s]Extractor Estimating: 56it [00:35,  1.66it/s]Extractor Estimating: 57it [00:36,  1.72it/s]Extractor Estimating: 58it [00:36,  1.76it/s]Extractor Estimating: 59it [00:37,  1.56it/s]Extractor Estimating: 60it [00:38,  1.62it/s]Extractor Estimating: 61it [00:38,  1.63it/s]Extractor Estimating: 62it [00:39,  1.59it/s]Extractor Estimating: 63it [00:40,  1.62it/s]Extractor Estimating: 64it [00:40,  1.59it/s]Extractor Estimating: 65it [00:41,  1.65it/s]Extractor Estimating: 66it [00:41,  1.65it/s]Extractor Estimating: 67it [00:42,  1.68it/s]Extractor Estimating: 68it [00:43,  1.63it/s]Extractor Estimating: 69it [00:43,  1.58it/s]Extractor Estimating: 70it [00:44,  1.54it/s]Extractor Estimating: 71it [00:45,  1.56it/s]Extractor Estimating: 72it [00:45,  1.64it/s]Extractor Estimating: 73it [00:46,  1.63it/s]Extractor Estimating: 74it [00:47,  1.55it/s]Extractor Estimating: 75it [00:47,  1.60it/s]Extractor Estimating: 76it [00:48,  1.62it/s]Extractor Estimating: 77it [00:48,  1.62it/s]Extractor Estimating: 78it [00:49,  1.62it/s]Extractor Estimating: 79it [00:50,  1.56it/s]Extractor Estimating: 80it [00:50,  1.64it/s]Extractor Estimating: 81it [00:51,  1.62it/s]Extractor Estimating: 82it [00:51,  1.60it/s]Extractor Estimating: 83it [00:52,  1.56it/s]Extractor Estimating: 84it [00:53,  1.51it/s]Extractor Estimating: 85it [00:54,  1.50it/s]Extractor Estimating: 86it [00:54,  1.54it/s]Extractor Estimating: 87it [00:55,  1.56it/s]Extractor Estimating: 88it [00:55,  1.57it/s]Extractor Estimating: 89it [00:56,  1.60it/s]Extractor Estimating: 90it [00:57,  1.64it/s]Extractor Estimating: 91it [00:57,  1.62it/s]Extractor Estimating: 92it [00:58,  1.53it/s]Extractor Estimating: 93it [00:58,  1.59it/s]Extractor Estimating: 94it [00:59,  1.60it/s]Extractor Estimating: 95it [01:00,  1.62it/s]Extractor Estimating: 96it [01:00,  1.62it/s]Extractor Estimating: 97it [01:01,  1.58it/s]Extractor Estimating: 98it [01:02,  1.58it/s]Extractor Estimating: 99it [01:02,  1.55it/s]Extractor Estimating: 100it [01:03,  1.62it/s]Extractor Estimating: 101it [01:03,  1.68it/s]Extractor Estimating: 102it [01:04,  1.58it/s]Extractor Estimating: 103it [01:05,  1.59it/s]Extractor Estimating: 104it [01:05,  1.66it/s]Extractor Estimating: 105it [01:06,  1.65it/s]Extractor Estimating: 106it [01:07,  1.64it/s]Extractor Estimating: 107it [01:07,  1.57it/s]Extractor Estimating: 108it [01:08,  1.57it/s]Extractor Estimating: 109it [01:08,  1.58it/s]Extractor Estimating: 110it [01:09,  1.57it/s]Extractor Estimating: 111it [01:10,  1.63it/s]Extractor Estimating: 112it [01:10,  1.58it/s]Extractor Estimating: 113it [01:11,  1.60it/s]Extractor Estimating: 114it [01:12,  1.62it/s]Extractor Estimating: 115it [01:12,  1.64it/s]Extractor Estimating: 116it [01:13,  1.65it/s]Extractor Estimating: 117it [01:14,  1.52it/s]Extractor Estimating: 118it [01:14,  1.54it/s]Extractor Estimating: 119it [01:15,  1.59it/s]Extractor Estimating: 120it [01:15,  1.52it/s]Extractor Estimating: 121it [01:16,  1.52it/s]Extractor Estimating: 122it [01:17,  1.53it/s]Extractor Estimating: 123it [01:17,  1.53it/s]Extractor Estimating: 124it [01:18,  1.57it/s]Extractor Estimating: 125it [01:19,  1.59it/s]Extractor Estimating: 126it [01:19,  1.57it/s]Extractor Estimating: 127it [01:20,  1.48it/s]Extractor Estimating: 128it [01:21,  1.49it/s]Extractor Estimating: 129it [01:21,  1.53it/s]Extractor Estimating: 130it [01:22,  1.60it/s]Extractor Estimating: 131it [01:22,  1.61it/s]Extractor Estimating: 132it [01:23,  1.55it/s]Extractor Estimating: 133it [01:24,  1.58it/s]Extractor Estimating: 134it [01:24,  1.60it/s]Extractor Estimating: 135it [01:25,  1.60it/s]Extractor Estimating: 136it [01:26,  1.63it/s]Extractor Estimating: 137it [01:26,  1.67it/s]Extractor Estimating: 138it [01:27,  1.68it/s]Extractor Estimating: 139it [01:27,  1.60it/s]Extractor Estimating: 140it [01:28,  1.54it/s]Extractor Estimating: 141it [01:29,  1.57it/s]Extractor Estimating: 142it [01:29,  1.56it/s]Extractor Estimating: 143it [01:30,  1.56it/s]Extractor Estimating: 144it [01:31,  1.61it/s]Extractor Estimating: 145it [01:31,  1.56it/s]Extractor Estimating: 146it [01:32,  1.61it/s]Extractor Estimating: 147it [01:33,  1.63it/s]Extractor Estimating: 148it [01:33,  1.62it/s]Extractor Estimating: 149it [01:34,  1.68it/s]Extractor Estimating: 150it [01:34,  1.55it/s]Extractor Estimating: 151it [01:35,  1.69it/s]Extractor Estimating: 152it [01:35,  1.77it/s]Extractor Estimating: 153it [01:36,  1.85it/s]Extractor Estimating: 154it [01:36,  1.97it/s]Extractor Estimating: 155it [01:37,  2.00it/s]Extractor Estimating: 156it [01:37,  1.91it/s]Extractor Estimating: 157it [01:38,  2.01it/s]Extractor Estimating: 158it [01:38,  2.07it/s]Extractor Estimating: 159it [01:39,  2.05it/s]Extractor Estimating: 160it [01:39,  2.08it/s]Extractor Estimating: 161it [01:40,  2.06it/s]Extractor Estimating: 162it [01:40,  2.06it/s]Extractor Estimating: 163it [01:41,  1.95it/s]Extractor Estimating: 164it [01:41,  2.01it/s]Extractor Estimating: 165it [01:42,  2.04it/s]Extractor Estimating: 166it [01:42,  2.05it/s]Extractor Estimating: 167it [01:43,  2.03it/s]Extractor Estimating: 168it [01:43,  2.10it/s]Extractor Estimating: 169it [01:44,  2.06it/s]Extractor Estimating: 170it [01:44,  1.88it/s]Extractor Estimating: 171it [01:45,  1.90it/s]Extractor Estimating: 172it [01:45,  1.98it/s]Extractor Estimating: 173it [01:46,  2.00it/s]Extractor Estimating: 174it [01:46,  1.95it/s]Extractor Estimating: 175it [01:47,  1.99it/s]Extractor Estimating: 176it [01:47,  1.78it/s]Extractor Estimating: 177it [01:48,  1.73it/s]Extractor Estimating: 178it [01:49,  1.77it/s]Extractor Estimating: 179it [01:49,  1.78it/s]Extractor Estimating: 180it [01:50,  1.78it/s]Extractor Estimating: 181it [01:50,  1.80it/s]Extractor Estimating: 182it [01:51,  1.67it/s]Extractor Estimating: 183it [01:52,  1.69it/s]Extractor Estimating: 184it [01:52,  1.69it/s]Extractor Estimating: 185it [01:53,  1.70it/s]Extractor Estimating: 186it [01:53,  1.72it/s]Extractor Estimating: 187it [01:54,  1.58it/s]Extractor Estimating: 188it [01:55,  1.63it/s]Extractor Estimating: 189it [01:55,  1.72it/s]Extractor Estimating: 190it [01:56,  1.57it/s]Extractor Estimating: 191it [01:56,  1.63it/s]Extractor Estimating: 192it [01:57,  1.70it/s]Extractor Estimating: 193it [01:58,  1.68it/s]Extractor Estimating: 194it [01:58,  1.65it/s]Extractor Estimating: 195it [01:59,  1.56it/s]Extractor Estimating: 196it [02:00,  1.58it/s]Extractor Estimating: 197it [02:00,  1.48it/s]Extractor Estimating: 198it [02:01,  1.57it/s]Extractor Estimating: 199it [02:01,  1.60it/s]Extractor Estimating: 200it [02:02,  1.58it/s]Extractor Estimating: 201it [02:03,  1.62it/s]Extractor Estimating: 202it [02:03,  1.55it/s]Extractor Estimating: 203it [02:04,  1.57it/s]Extractor Estimating: 204it [02:05,  1.55it/s]Extractor Estimating: 205it [02:05,  1.50it/s]Extractor Estimating: 206it [02:06,  1.52it/s]Extractor Estimating: 207it [02:07,  1.62it/s]Extractor Estimating: 208it [02:07,  1.64it/s]Extractor Estimating: 209it [02:08,  1.67it/s]Extractor Estimating: 210it [02:09,  1.51it/s]Extractor Estimating: 211it [02:09,  1.55it/s]Extractor Estimating: 212it [02:10,  1.56it/s]Extractor Estimating: 213it [02:10,  1.55it/s]Extractor Estimating: 214it [02:11,  1.62it/s]Extractor Estimating: 215it [02:12,  1.54it/s]Extractor Estimating: 216it [02:12,  1.52it/s]Extractor Estimating: 217it [02:13,  1.61it/s]Extractor Estimating: 218it [02:13,  1.69it/s]Extractor Estimating: 219it [02:14,  1.71it/s]Extractor Estimating: 220it [02:15,  1.61it/s]Extractor Estimating: 221it [02:15,  1.60it/s]Extractor Estimating: 222it [02:16,  1.58it/s]Extractor Estimating: 223it [02:17,  1.63it/s]Extractor Estimating: 224it [02:17,  1.64it/s]Extractor Estimating: 225it [02:18,  1.57it/s]Extractor Estimating: 226it [02:18,  1.63it/s]Extractor Estimating: 227it [02:19,  1.65it/s]Extractor Estimating: 228it [02:20,  1.66it/s]Extractor Estimating: 229it [02:20,  1.72it/s]Extractor Estimating: 230it [02:21,  1.73it/s]Extractor Estimating: 231it [02:21,  1.61it/s]Extractor Estimating: 232it [02:22,  1.67it/s]Extractor Estimating: 233it [02:23,  1.68it/s]Extractor Estimating: 234it [02:23,  1.69it/s]Extractor Estimating: 235it [02:24,  1.66it/s]Extractor Estimating: 236it [02:24,  1.66it/s]Extractor Estimating: 237it [02:25,  1.64it/s]Extractor Estimating: 238it [02:26,  1.71it/s]Extractor Estimating: 239it [02:26,  1.72it/s]Extractor Estimating: 240it [02:27,  1.74it/s]Extractor Estimating: 241it [02:27,  1.84it/s]Extractor Estimating: 242it [02:28,  1.63it/s]Extractor Estimating: 243it [02:29,  1.61it/s]Extractor Estimating: 244it [02:29,  1.68it/s]Extractor Estimating: 245it [02:30,  1.69it/s]Extractor Estimating: 246it [02:30,  1.75it/s]Extractor Estimating: 247it [02:31,  1.64it/s]Extractor Estimating: 248it [02:31,  1.67it/s]Extractor Estimating: 249it [02:32,  1.72it/s]Extractor Estimating: 250it [02:33,  1.70it/s]Extractor Estimating: 250it [02:33,  1.63it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:10,127 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:10,186 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:10,187 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:10,187 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:10,187 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 10:56:10,578 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 10:56:10,579 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:56:10,908 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 10:56:12,014 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:56:12,014 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:14,325 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:14,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:14,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:14,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 10:56:14,402 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 10:56:15,083 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 10:56:15,084 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 10:56:15,431 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 10:56:15,588 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 10:56:15,588 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 12:20:35,739 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 12:20:36,930 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/1_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/1.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.4, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 2000, 'num_train': 3000}
num of filtered data: 5015 mean pseudo reward: 0.9557706212979621
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/1.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 22973
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 23073, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=23073, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.940, loss:851.6122
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 1.023, loss:796.0859
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.966, loss:761.3223
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.943, loss:785.8107
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.947, loss:735.2318
>> valid entity prec:0.5806, rec:0.5669, f1:0.5737
>> valid relation prec:0.5043, rec:0.1184, f1:0.1918
>> valid relation with NER prec:0.5043, rec:0.1184, f1:0.1918
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.265, loss:773.3170
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 0.961, loss:731.6969
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.961, loss:750.3974
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.954, loss:755.9738
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.941, loss:763.9626
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6173, rec:0.6465, f1:0.6315
>> valid relation prec:0.5471, rec:0.1112, f1:0.1849
>> valid relation with NER prec:0.5471, rec:0.1112, f1:0.1849
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 55, avg_time 2.314, loss:715.6316
g_step 1200, step 155, avg_time 0.953, loss:736.2844
g_step 1300, step 46, avg_time 0.952, loss:727.5075
g_step 1400, step 146, avg_time 0.967, loss:700.4223
g_step 1500, step 37, avg_time 0.957, loss:710.0536
>> valid entity prec:0.6307, rec:0.5665, f1:0.5969
>> valid relation prec:0.5007, rec:0.1044, f1:0.1727
>> valid relation with NER prec:0.5007, rec:0.1044, f1:0.1727
g_step 1600, step 137, avg_time 2.225, loss:651.7691
g_step 1700, step 28, avg_time 0.967, loss:676.8050
g_step 1800, step 128, avg_time 0.955, loss:622.3808
g_step 1900, step 19, avg_time 0.971, loss:644.6754
g_step 2000, step 119, avg_time 0.953, loss:623.8334
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6034, rec:0.5873, f1:0.5953
>> valid relation prec:0.3823, rec:0.0826, f1:0.1359
>> valid relation with NER prec:0.3823, rec:0.0826, f1:0.1359
g_step 2100, step 10, avg_time 2.254, loss:597.4777
g_step 2200, step 110, avg_time 0.963, loss:578.1359
g_step 2300, step 1, avg_time 0.959, loss:584.8220
g_step 2400, step 101, avg_time 0.956, loss:530.7740
g_step 2500, step 201, avg_time 0.970, loss:579.7318
>> valid entity prec:0.6101, rec:0.5370, f1:0.5713
>> valid relation prec:0.3035, rec:0.0689, f1:0.1123
>> valid relation with NER prec:0.3035, rec:0.0689, f1:0.1123
g_step 2600, step 92, avg_time 2.229, loss:515.0872
g_step 2700, step 192, avg_time 0.958, loss:543.4829
g_step 2800, step 83, avg_time 0.951, loss:515.4363
g_step 2900, step 183, avg_time 0.962, loss:510.0269
g_step 3000, step 74, avg_time 0.959, loss:499.5503
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5771, rec:0.5158, f1:0.5447
>> valid relation prec:0.1874, rec:0.0486, f1:0.0772
>> valid relation with NER prec:0.1874, rec:0.0486, f1:0.0772
g_step 3100, step 174, avg_time 2.234, loss:491.2695
g_step 3200, step 65, avg_time 0.966, loss:490.6791
g_step 3300, step 165, avg_time 0.951, loss:466.5357
g_step 3400, step 56, avg_time 0.951, loss:468.9555
g_step 3500, step 156, avg_time 0.950, loss:459.9589
>> valid entity prec:0.5835, rec:0.5678, f1:0.5755
>> valid relation prec:0.2294, rec:0.0652, f1:0.1015
>> valid relation with NER prec:0.2294, rec:0.0652, f1:0.1015
g_step 3600, step 47, avg_time 2.228, loss:451.8860
g_step 3700, step 147, avg_time 0.961, loss:447.0894
g_step 3800, step 38, avg_time 0.959, loss:418.9427
g_step 3900, step 138, avg_time 0.961, loss:411.2398
g_step 4000, step 29, avg_time 0.963, loss:420.1417
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6162, rec:0.5405, f1:0.5759
>> valid relation prec:0.2632, rec:0.0915, f1:0.1358
>> valid relation with NER prec:0.2632, rec:0.0915, f1:0.1358
g_step 4100, step 129, avg_time 2.228, loss:396.8449
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 12:20:36 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 12:20:36 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_12-20-35_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 12:20:39 - WARNING - datasets.builder -   Using custom data configuration default-77b2320a22fcc8b1
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-77b2320a22fcc8b1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 12:20:48,210 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:20:48,212 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:20:48,212 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:20:48,213 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:20:48,731 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:20:48,980 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:20:48,980 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:20:48,980 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:20:48,980 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:20:48,980 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:20:48,980 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 12:20:50,269 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:20:53,601 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 12:20:53,712 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-77b2320a22fcc8b1/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:02,  1.94ba/s] 33%|███▎      | 2/6 [00:00<00:01,  3.01ba/s] 50%|█████     | 3/6 [00:01<00:00,  3.04ba/s] 67%|██████▋   | 4/6 [00:01<00:00,  3.53ba/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.90ba/s]100%|██████████| 6/6 [00:01<00:00,  4.06ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.87ba/s] 50%|█████     | 2/4 [00:00<00:00,  2.84ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  3.43ba/s]100%|██████████| 4/4 [00:01<00:00,  4.52ba/s]100%|██████████| 4/4 [00:01<00:00,  3.66ba/s]
  0%|          | 0/6 [00:00<?, ?ba/s] 17%|█▋        | 1/6 [00:00<00:01,  2.60ba/s] 50%|█████     | 3/6 [00:00<00:00,  5.97ba/s] 83%|████████▎ | 5/6 [00:00<00:00,  7.78ba/s]100%|██████████| 6/6 [00:00<00:00,  7.95ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.68ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  6.15ba/s]100%|██████████| 4/4 [00:00<00:00,  6.71ba/s]
[INFO|trainer.py:414] 2023-08-28 12:21:02,185 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 12:21:02,538 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 12:21:02,538 >>   Num examples = 5022
[INFO|trainer.py:1149] 2023-08-28 12:21:02,538 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 12:21:02,539 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 12:21:02,539 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 12:21:02,539 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 12:21:02,539 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:59,  3.26it/s]  1%|          | 2/390 [00:00<01:55,  3.37it/s]  1%|          | 3/390 [00:00<01:53,  3.40it/s]  1%|          | 4/390 [00:01<01:53,  3.41it/s]  1%|▏         | 5/390 [00:01<01:52,  3.42it/s]  2%|▏         | 6/390 [00:01<01:51,  3.43it/s]  2%|▏         | 7/390 [00:02<01:51,  3.44it/s]  2%|▏         | 8/390 [00:02<01:50,  3.46it/s]  2%|▏         | 9/390 [00:02<01:49,  3.47it/s]  3%|▎         | 10/390 [00:02<01:49,  3.48it/s]  3%|▎         | 11/390 [00:03<01:48,  3.48it/s]  3%|▎         | 12/390 [00:03<01:57,  3.21it/s]  3%|▎         | 13/390 [00:03<01:54,  3.29it/s]  4%|▎         | 14/390 [00:04<01:52,  3.34it/s]  4%|▍         | 15/390 [00:04<01:50,  3.38it/s]  4%|▍         | 16/390 [00:04<01:49,  3.41it/s]  4%|▍         | 17/390 [00:04<01:48,  3.43it/s]  5%|▍         | 18/390 [00:05<01:48,  3.44it/s]  5%|▍         | 19/390 [00:05<01:47,  3.45it/s]  5%|▌         | 20/390 [00:06<02:05,  2.95it/s]  5%|▌         | 21/390 [00:06<01:59,  3.09it/s]  6%|▌         | 22/390 [00:06<01:55,  3.19it/s]  6%|▌         | 23/390 [00:06<01:52,  3.28it/s]  6%|▌         | 24/390 [00:07<01:49,  3.33it/s]  6%|▋         | 25/390 [00:07<01:48,  3.37it/s]  7%|▋         | 26/390 [00:07<01:46,  3.40it/s]  7%|▋         | 27/390 [00:08<01:46,  3.42it/s]  7%|▋         | 28/390 [00:08<01:45,  3.44it/s]  7%|▋         | 29/390 [00:08<01:44,  3.45it/s]  8%|▊         | 30/390 [00:08<01:43,  3.46it/s]  8%|▊         | 31/390 [00:09<01:43,  3.47it/s]  8%|▊         | 32/390 [00:09<01:43,  3.47it/s]  8%|▊         | 33/390 [00:09<01:42,  3.47it/s]  9%|▊         | 34/390 [00:10<01:42,  3.47it/s]  9%|▉         | 35/390 [00:10<01:42,  3.47it/s]  9%|▉         | 36/390 [00:10<01:41,  3.48it/s]  9%|▉         | 37/390 [00:10<01:41,  3.48it/s] 10%|▉         | 38/390 [00:11<01:41,  3.48it/s] 10%|█         | 39/390 [00:11<01:40,  3.48it/s] 10%|█         | 40/390 [00:11<01:40,  3.48it/s] 11%|█         | 41/390 [00:12<01:40,  3.48it/s] 11%|█         | 42/390 [00:12<01:39,  3.48it/s] 11%|█         | 43/390 [00:12<01:39,  3.48it/s] 11%|█▏        | 44/390 [00:13<01:48,  3.19it/s] 12%|█▏        | 45/390 [00:13<01:45,  3.26it/s] 12%|█▏        | 46/390 [00:13<01:43,  3.32it/s] 12%|█▏        | 47/390 [00:13<01:42,  3.36it/s] 12%|█▏        | 48/390 [00:14<01:40,  3.40it/s] 13%|█▎        | 49/390 [00:14<01:39,  3.42it/s] 13%|█▎        | 50/390 [00:14<01:38,  3.44it/s] 13%|█▎        | 51/390 [00:15<01:38,  3.45it/s] 13%|█▎        | 52/390 [00:15<01:37,  3.45it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.46it/s] 14%|█▍        | 54/390 [00:15<01:36,  3.47it/s] 14%|█▍        | 55/390 [00:16<01:36,  3.47it/s] 14%|█▍        | 56/390 [00:16<01:36,  3.47it/s] 15%|█▍        | 57/390 [00:16<01:35,  3.47it/s] 15%|█▍        | 58/390 [00:17<01:35,  3.47it/s] 15%|█▌        | 59/390 [00:17<01:35,  3.47it/s] 15%|█▌        | 60/390 [00:17<01:35,  3.47it/s] 16%|█▌        | 61/390 [00:17<01:34,  3.47it/s] 16%|█▌        | 62/390 [00:18<01:34,  3.47it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.47it/s] 16%|█▋        | 64/390 [00:18<01:33,  3.47it/s] 17%|█▋        | 65/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 67/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 68/390 [00:19<01:32,  3.47it/s] 18%|█▊        | 69/390 [00:20<01:32,  3.47it/s] 18%|█▊        | 70/390 [00:20<01:32,  3.47it/s] 18%|█▊        | 71/390 [00:20<01:31,  3.47it/s] 18%|█▊        | 72/390 [00:21<01:31,  3.47it/s] 19%|█▊        | 73/390 [00:21<01:31,  3.47it/s] 19%|█▉        | 74/390 [00:21<01:31,  3.47it/s] 19%|█▉        | 75/390 [00:21<01:30,  3.47it/s] 19%|█▉        | 76/390 [00:22<01:30,  3.47it/s] 20%|█▉        | 77/390 [00:22<01:30,  3.47it/s] 20%|██        | 78/390 [00:22<01:29,  3.47it/s][INFO|trainer.py:2140] 2023-08-28 12:21:25,481 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:21:25,481 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 12:21:25,481 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.35it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.29it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.63it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.63it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.92it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.46it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.08it/s][A
 10%|▉         | 42/438 [00:00<00:08, 44.85it/s][A
 11%|█         | 47/438 [00:01<00:08, 44.96it/s][A
 12%|█▏        | 52/438 [00:01<00:10, 36.56it/s][A
 13%|█▎        | 57/438 [00:01<00:10, 36.08it/s][A
 14%|█▍        | 62/438 [00:01<00:09, 39.03it/s][A
 15%|█▌        | 67/438 [00:01<00:09, 40.80it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 42.06it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 43.08it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 43.72it/s][A
 20%|█▉        | 87/438 [00:02<00:07, 44.23it/s][A
 21%|██        | 92/438 [00:02<00:07, 44.11it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 44.09it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 43.82it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.27it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.62it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.88it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.98it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 45.17it/s][A
 30%|███       | 132/438 [00:03<00:06, 45.16it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 45.02it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.68it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.60it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.70it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.93it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.11it/s][A
 38%|███▊      | 167/438 [00:03<00:05, 45.21it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.16it/s][A
 40%|████      | 177/438 [00:04<00:07, 35.73it/s][A
 42%|████▏     | 182/438 [00:04<00:06, 37.61it/s][A
 43%|████▎     | 187/438 [00:04<00:06, 39.82it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 41.27it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 42.45it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 43.20it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 43.86it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 44.23it/s][A
 50%|████▉     | 217/438 [00:05<00:04, 44.24it/s][A
 51%|█████     | 222/438 [00:05<00:04, 44.31it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 44.56it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 44.64it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 44.77it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 44.87it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.99it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 45.04it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.96it/s][A
 60%|█████▉    | 262/438 [00:06<00:03, 44.89it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.84it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.90it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.89it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.98it/s][A
 66%|██████▌   | 287/438 [00:06<00:04, 33.44it/s][A
 66%|██████▋   | 291/438 [00:06<00:05, 28.48it/s][A
 68%|██████▊   | 296/438 [00:07<00:04, 32.25it/s][A
 68%|██████▊   | 300/438 [00:07<00:04, 29.87it/s][A
 70%|██████▉   | 305/438 [00:07<00:03, 33.59it/s][A
 71%|███████   | 310/438 [00:07<00:03, 36.57it/s][A
 72%|███████▏  | 315/438 [00:07<00:03, 38.97it/s][A
 73%|███████▎  | 320/438 [00:07<00:02, 40.77it/s][A
 74%|███████▍  | 325/438 [00:07<00:02, 42.14it/s][A
 75%|███████▌  | 330/438 [00:07<00:02, 43.04it/s][A
 76%|███████▋  | 335/438 [00:07<00:02, 43.76it/s][A
 78%|███████▊  | 340/438 [00:08<00:02, 43.80it/s][A
 79%|███████▉  | 345/438 [00:08<00:02, 43.76it/s][A
 80%|███████▉  | 350/438 [00:08<00:02, 43.79it/s][A
 81%|████████  | 355/438 [00:08<00:01, 44.13it/s][A
 82%|████████▏ | 360/438 [00:08<00:01, 44.42it/s][A
 83%|████████▎ | 365/438 [00:08<00:01, 44.74it/s][A
 84%|████████▍ | 370/438 [00:08<00:01, 44.98it/s][A
 86%|████████▌ | 375/438 [00:08<00:01, 45.18it/s][A
 87%|████████▋ | 380/438 [00:08<00:01, 45.16it/s][A
 88%|████████▊ | 385/438 [00:09<00:01, 44.99it/s][A
 89%|████████▉ | 390/438 [00:09<00:01, 44.64it/s][A
 90%|█████████ | 395/438 [00:09<00:00, 44.47it/s][A
 91%|█████████▏| 400/438 [00:09<00:00, 44.60it/s][A
 92%|█████████▏| 405/438 [00:09<00:00, 44.80it/s][A
 94%|█████████▎| 410/438 [00:09<00:00, 45.00it/s][A
 95%|█████████▍| 415/438 [00:09<00:00, 45.13it/s][A
 96%|█████████▌| 420/438 [00:09<00:00, 33.72it/s][A
 97%|█████████▋| 425/438 [00:10<00:00, 36.45it/s][A
 98%|█████████▊| 430/438 [00:10<00:00, 38.76it/s][A
 99%|█████████▉| 435/438 [00:10<00:00, 40.54it/s][A                                                
                                                 [A 20%|██        | 78/390 [00:33<01:29,  3.47it/s]
100%|██████████| 438/438 [00:10<00:00, 40.54it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:21:36,627 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 12:21:37,403 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:21:50,803 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:21:51,313 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:21:51,550 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [02:02<2:36:10, 30.13s/it] 21%|██        | 80/390 [02:02<1:49:36, 21.22s/it] 21%|██        | 81/390 [02:03<1:16:55, 14.94s/it] 21%|██        | 82/390 [02:03<54:07, 10.54s/it]   21%|██▏       | 83/390 [02:03<38:12,  7.47s/it] 22%|██▏       | 84/390 [02:04<27:06,  5.31s/it] 22%|██▏       | 85/390 [02:04<19:21,  3.81s/it] 22%|██▏       | 86/390 [02:04<13:56,  2.75s/it] 22%|██▏       | 87/390 [02:05<10:10,  2.01s/it] 23%|██▎       | 88/390 [02:05<07:32,  1.50s/it] 23%|██▎       | 89/390 [02:05<05:41,  1.14s/it] 23%|██▎       | 90/390 [02:06<04:37,  1.08it/s] 23%|██▎       | 91/390 [02:06<03:39,  1.36it/s] 24%|██▎       | 92/390 [02:06<02:59,  1.66it/s] 24%|██▍       | 93/390 [02:06<02:30,  1.97it/s] 24%|██▍       | 94/390 [02:07<02:10,  2.26it/s] 24%|██▍       | 95/390 [02:07<01:56,  2.53it/s] 25%|██▍       | 96/390 [02:07<01:46,  2.76it/s] 25%|██▍       | 97/390 [02:08<01:39,  2.94it/s] 25%|██▌       | 98/390 [02:08<01:34,  3.09it/s] 25%|██▌       | 99/390 [02:08<01:31,  3.20it/s] 26%|██▌       | 100/390 [02:09<01:38,  2.93it/s] 26%|██▌       | 101/390 [02:09<01:33,  3.08it/s] 26%|██▌       | 102/390 [02:09<01:30,  3.19it/s] 26%|██▋       | 103/390 [02:09<01:27,  3.27it/s] 27%|██▋       | 104/390 [02:10<01:25,  3.33it/s] 27%|██▋       | 105/390 [02:10<01:24,  3.38it/s] 27%|██▋       | 106/390 [02:10<01:23,  3.41it/s] 27%|██▋       | 107/390 [02:11<01:22,  3.43it/s] 28%|██▊       | 108/390 [02:11<01:21,  3.45it/s] 28%|██▊       | 109/390 [02:11<01:21,  3.46it/s] 28%|██▊       | 110/390 [02:11<01:20,  3.47it/s] 28%|██▊       | 111/390 [02:12<01:29,  3.13it/s] 29%|██▊       | 112/390 [02:12<01:26,  3.23it/s] 29%|██▉       | 113/390 [02:12<01:23,  3.30it/s] 29%|██▉       | 114/390 [02:13<01:22,  3.35it/s] 29%|██▉       | 115/390 [02:13<01:21,  3.39it/s] 30%|██▉       | 116/390 [02:13<01:20,  3.42it/s] 30%|███       | 117/390 [02:14<01:19,  3.45it/s] 30%|███       | 118/390 [02:14<01:18,  3.46it/s] 31%|███       | 119/390 [02:14<01:18,  3.47it/s] 31%|███       | 120/390 [02:14<01:17,  3.48it/s] 31%|███       | 121/390 [02:15<01:17,  3.49it/s] 31%|███▏      | 122/390 [02:15<01:25,  3.13it/s] 32%|███▏      | 123/390 [02:15<01:22,  3.23it/s] 32%|███▏      | 124/390 [02:16<01:20,  3.30it/s] 32%|███▏      | 125/390 [02:16<01:18,  3.35it/s] 32%|███▏      | 126/390 [02:16<01:17,  3.39it/s] 33%|███▎      | 127/390 [02:16<01:16,  3.42it/s] 33%|███▎      | 128/390 [02:17<01:16,  3.44it/s] 33%|███▎      | 129/390 [02:17<01:15,  3.46it/s] 33%|███▎      | 130/390 [02:17<01:14,  3.47it/s] 34%|███▎      | 131/390 [02:18<01:14,  3.47it/s] 34%|███▍      | 132/390 [02:18<01:14,  3.48it/s] 34%|███▍      | 133/390 [02:18<01:22,  3.10it/s] 34%|███▍      | 134/390 [02:19<01:19,  3.21it/s] 35%|███▍      | 135/390 [02:19<01:17,  3.29it/s] 35%|███▍      | 136/390 [02:19<01:15,  3.34it/s] 35%|███▌      | 137/390 [02:19<01:14,  3.39it/s] 35%|███▌      | 138/390 [02:20<01:13,  3.42it/s] 36%|███▌      | 139/390 [02:20<01:12,  3.44it/s] 36%|███▌      | 140/390 [02:20<01:12,  3.45it/s] 36%|███▌      | 141/390 [02:21<01:11,  3.46it/s] 36%|███▋      | 142/390 [02:21<01:11,  3.47it/s] 37%|███▋      | 143/390 [02:21<01:11,  3.48it/s] 37%|███▋      | 144/390 [02:22<01:19,  3.09it/s] 37%|███▋      | 145/390 [02:22<01:16,  3.20it/s] 37%|███▋      | 146/390 [02:22<01:14,  3.28it/s] 38%|███▊      | 147/390 [02:22<01:12,  3.34it/s] 38%|███▊      | 148/390 [02:23<01:11,  3.38it/s] 38%|███▊      | 149/390 [02:23<01:10,  3.41it/s] 38%|███▊      | 150/390 [02:23<01:09,  3.44it/s] 39%|███▊      | 151/390 [02:24<01:09,  3.45it/s] 39%|███▉      | 152/390 [02:24<01:08,  3.46it/s] 39%|███▉      | 153/390 [02:24<01:08,  3.47it/s] 39%|███▉      | 154/390 [02:24<01:07,  3.48it/s] 40%|███▉      | 155/390 [02:25<01:16,  3.07it/s] 40%|████      | 156/390 [02:25<01:13,  3.18it/s][INFO|trainer.py:2140] 2023-08-28 12:23:28,347 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:23:28,347 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 12:23:28,347 >>   Batch size = 8
{'eval_loss': 1.0145633220672607, 'eval_runtime': 10.3634, 'eval_samples_per_second': 337.439, 'eval_steps_per_second': 42.264, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.83it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.38it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.57it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.87it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.28it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.81it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.38it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.26it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.31it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.39it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.54it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.58it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.56it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.48it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.31it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.08it/s][A
 20%|██        | 88/438 [00:01<00:07, 45.00it/s][A
 21%|██        | 93/438 [00:02<00:07, 45.04it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.26it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.44it/s][A
 25%|██▍       | 108/438 [00:02<00:09, 34.25it/s][A
 26%|██▌       | 113/438 [00:02<00:08, 36.99it/s][A
 27%|██▋       | 118/438 [00:02<00:08, 39.23it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 40.98it/s][A
 29%|██▉       | 128/438 [00:02<00:07, 42.29it/s][A
 30%|███       | 133/438 [00:03<00:07, 43.38it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.05it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.48it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.28it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 44.18it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.37it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.59it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.93it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 45.18it/s][A
 41%|████      | 178/438 [00:04<00:05, 45.46it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.56it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.42it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.09it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 44.84it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 44.78it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 44.86it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.02it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.25it/s][A
 51%|█████     | 223/438 [00:05<00:04, 45.41it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 45.54it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.44it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 45.21it/s][A
 55%|█████▌    | 243/438 [00:05<00:05, 38.12it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 40.15it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 41.71it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 42.87it/s][A
 60%|██████    | 263/438 [00:05<00:04, 43.66it/s][A
 61%|██████    | 268/438 [00:06<00:03, 44.26it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 44.69it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 44.96it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 44.63it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.48it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.45it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.76it/s][A
 69%|██████▉   | 303/438 [00:06<00:03, 44.92it/s][A
 70%|███████   | 308/438 [00:06<00:02, 45.20it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 45.38it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.52it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.38it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.17it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 44.90it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 44.74it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 44.86it/s][A
 79%|███████▉  | 348/438 [00:07<00:02, 44.89it/s][A
 81%|████████  | 353/438 [00:07<00:01, 45.13it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 45.25it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 45.45it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 45.43it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 45.26it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 34.68it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 37.41it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 39.51it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 41.13it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 42.34it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 43.28it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 43.94it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 32.08it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 35.19it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 37.75it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 39.78it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 41.35it/s][A
100%|██████████| 438/438 [00:10<00:00, 42.66it/s][A                                                 
                                                 [A 40%|████      | 156/390 [02:35<01:13,  3.18it/s]
100%|██████████| 438/438 [00:10<00:00, 42.66it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:23:38,775 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 12:23:39,317 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:24:12,547 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:24:14,034 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:24:14,248 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [04:16<2:10:01, 33.48s/it] 41%|████      | 158/390 [04:16<1:31:05, 23.56s/it] 41%|████      | 159/390 [04:17<1:03:48, 16.57s/it] 41%|████      | 160/390 [04:17<44:48, 11.69s/it]   41%|████▏     | 161/390 [04:17<31:33,  8.27s/it] 42%|████▏     | 162/390 [04:18<22:19,  5.87s/it] 42%|████▏     | 163/390 [04:18<15:52,  4.20s/it] 42%|████▏     | 164/390 [04:18<11:23,  3.02s/it] 42%|████▏     | 165/390 [04:18<08:15,  2.20s/it] 43%|████▎     | 166/390 [04:19<06:04,  1.63s/it] 43%|████▎     | 167/390 [04:19<04:33,  1.23s/it] 43%|████▎     | 168/390 [04:19<03:29,  1.06it/s] 43%|████▎     | 169/390 [04:20<02:53,  1.28it/s] 44%|████▎     | 170/390 [04:20<02:19,  1.58it/s] 44%|████▍     | 171/390 [04:20<01:56,  1.89it/s] 44%|████▍     | 172/390 [04:21<01:39,  2.19it/s] 44%|████▍     | 173/390 [04:21<01:28,  2.46it/s] 45%|████▍     | 174/390 [04:21<01:19,  2.70it/s] 45%|████▍     | 175/390 [04:21<01:14,  2.90it/s] 45%|████▌     | 176/390 [04:22<01:10,  3.05it/s] 45%|████▌     | 177/390 [04:22<01:07,  3.17it/s] 46%|████▌     | 178/390 [04:22<01:05,  3.26it/s] 46%|████▌     | 179/390 [04:23<01:03,  3.32it/s] 46%|████▌     | 180/390 [04:23<01:08,  3.05it/s] 46%|████▋     | 181/390 [04:23<01:05,  3.17it/s] 47%|████▋     | 182/390 [04:24<01:03,  3.26it/s] 47%|████▋     | 183/390 [04:24<01:02,  3.32it/s] 47%|████▋     | 184/390 [04:24<01:01,  3.37it/s] 47%|████▋     | 185/390 [04:24<01:00,  3.41it/s] 48%|████▊     | 186/390 [04:25<00:59,  3.43it/s] 48%|████▊     | 187/390 [04:25<00:58,  3.45it/s] 48%|████▊     | 188/390 [04:25<00:58,  3.46it/s] 48%|████▊     | 189/390 [04:26<00:58,  3.47it/s] 49%|████▊     | 190/390 [04:26<00:57,  3.47it/s] 49%|████▉     | 191/390 [04:26<01:03,  3.11it/s] 49%|████▉     | 192/390 [04:27<01:01,  3.21it/s] 49%|████▉     | 193/390 [04:27<00:59,  3.29it/s] 50%|████▉     | 194/390 [04:27<00:58,  3.34it/s] 50%|█████     | 195/390 [04:27<00:57,  3.39it/s] 50%|█████     | 196/390 [04:28<00:56,  3.41it/s] 51%|█████     | 197/390 [04:28<00:56,  3.44it/s] 51%|█████     | 198/390 [04:28<00:55,  3.45it/s] 51%|█████     | 199/390 [04:29<00:55,  3.46it/s] 51%|█████▏    | 200/390 [04:29<00:54,  3.47it/s] 52%|█████▏    | 201/390 [04:29<00:54,  3.47it/s] 52%|█████▏    | 202/390 [04:30<01:02,  2.99it/s] 52%|█████▏    | 203/390 [04:30<00:59,  3.13it/s] 52%|█████▏    | 204/390 [04:30<00:57,  3.23it/s] 53%|█████▎    | 205/390 [04:30<00:56,  3.30it/s] 53%|█████▎    | 206/390 [04:31<00:54,  3.36it/s] 53%|█████▎    | 207/390 [04:31<00:53,  3.39it/s] 53%|█████▎    | 208/390 [04:31<00:53,  3.43it/s] 54%|█████▎    | 209/390 [04:32<00:52,  3.45it/s] 54%|█████▍    | 210/390 [04:32<00:52,  3.46it/s] 54%|█████▍    | 211/390 [04:32<00:51,  3.47it/s] 54%|█████▍    | 212/390 [04:33<00:58,  3.04it/s] 55%|█████▍    | 213/390 [04:33<00:55,  3.16it/s] 55%|█████▍    | 214/390 [04:33<00:54,  3.25it/s] 55%|█████▌    | 215/390 [04:33<00:52,  3.32it/s] 55%|█████▌    | 216/390 [04:34<00:51,  3.37it/s] 56%|█████▌    | 217/390 [04:34<00:50,  3.41it/s] 56%|█████▌    | 218/390 [04:34<00:50,  3.42it/s] 56%|█████▌    | 219/390 [04:35<00:49,  3.44it/s] 56%|█████▋    | 220/390 [04:35<00:49,  3.45it/s] 57%|█████▋    | 221/390 [04:35<00:48,  3.47it/s] 57%|█████▋    | 222/390 [04:36<00:54,  3.10it/s] 57%|█████▋    | 223/390 [04:36<00:52,  3.21it/s] 57%|█████▋    | 224/390 [04:36<00:50,  3.29it/s] 58%|█████▊    | 225/390 [04:36<00:49,  3.35it/s] 58%|█████▊    | 226/390 [04:37<00:48,  3.39it/s] 58%|█████▊    | 227/390 [04:37<00:47,  3.42it/s] 58%|█████▊    | 228/390 [04:37<00:47,  3.44it/s] 59%|█████▊    | 229/390 [04:38<00:46,  3.45it/s] 59%|█████▉    | 230/390 [04:38<00:46,  3.46it/s] 59%|█████▉    | 231/390 [04:38<00:45,  3.47it/s] 59%|█████▉    | 232/390 [04:38<00:45,  3.48it/s] 60%|█████▉    | 233/390 [04:39<00:51,  3.08it/s] 60%|██████    | 234/390 [04:39<00:48,  3.20it/s][INFO|trainer.py:2140] 2023-08-28 12:25:42,275 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:25:42,275 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 12:25:42,275 >>   Batch size = 8
{'eval_loss': 1.025793433189392, 'eval_runtime': 10.0977, 'eval_samples_per_second': 346.317, 'eval_steps_per_second': 43.376, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.31it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.44it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.81it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.80it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.27it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.83it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.47it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.18it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.30it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.35it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.37it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.59it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.65it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.53it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.43it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.14it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 45.08it/s][A
 21%|██        | 92/438 [00:02<00:07, 45.07it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 45.16it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 45.18it/s][A
 24%|██▍       | 107/438 [00:02<00:09, 36.44it/s][A
 26%|██▌       | 112/438 [00:02<00:08, 38.82it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 40.71it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 42.09it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 43.13it/s][A
 30%|███       | 132/438 [00:02<00:06, 43.94it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.51it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.80it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.57it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.46it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.58it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.77it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 45.08it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.27it/s][A
 40%|████      | 177/438 [00:03<00:05, 45.50it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 45.55it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 45.55it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.14it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.87it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.81it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.02it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 45.14it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.46it/s][A
 51%|█████     | 222/438 [00:04<00:04, 45.44it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 45.62it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 45.50it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 45.25it/s][A
 55%|█████▌    | 242/438 [00:05<00:06, 29.95it/s][A
 56%|█████▋    | 247/438 [00:05<00:05, 33.36it/s][A
 58%|█████▊    | 252/438 [00:05<00:05, 36.29it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 38.73it/s][A
 60%|█████▉    | 262/438 [00:06<00:04, 40.56it/s][A
 61%|██████    | 267/438 [00:06<00:04, 42.05it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 43.24it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 43.86it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 43.95it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 43.87it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.04it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.44it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.80it/s][A
 70%|███████   | 307/438 [00:07<00:02, 45.04it/s][A
 71%|███████   | 312/438 [00:07<00:02, 45.25it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.52it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 45.50it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 45.29it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.75it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.75it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 44.88it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 45.09it/s][A
 80%|████████  | 352/438 [00:08<00:01, 45.30it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.45it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.53it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 45.56it/s][A
 85%|████████▍ | 372/438 [00:08<00:03, 21.88it/s][A
 86%|████████▌ | 377/438 [00:08<00:02, 25.93it/s][A
 87%|████████▋ | 382/438 [00:09<00:01, 29.77it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 33.23it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 36.20it/s][A
 91%|█████████ | 397/438 [00:09<00:01, 38.65it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 40.54it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 41.93it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 42.50it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 43.07it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 43.46it/s][A
 97%|█████████▋| 427/438 [00:10<00:00, 44.05it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 44.46it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 44.86it/s][A                                                 
                                                 [A 60%|██████    | 234/390 [04:50<00:48,  3.20it/s]
100%|██████████| 438/438 [00:10<00:00, 44.86it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:25:53,020 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 12:25:53,981 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:26:31,698 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:26:33,422 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:26:33,841 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [06:39<1:33:24, 36.16s/it] 61%|██████    | 236/390 [06:39<1:05:16, 25.43s/it] 61%|██████    | 237/390 [06:40<45:37, 17.89s/it]   61%|██████    | 238/390 [06:40<31:56, 12.61s/it] 61%|██████▏   | 239/390 [06:40<22:26,  8.91s/it] 62%|██████▏   | 240/390 [06:40<15:49,  6.33s/it] 62%|██████▏   | 241/390 [06:41<11:12,  4.52s/it] 62%|██████▏   | 242/390 [06:41<08:00,  3.25s/it] 62%|██████▏   | 243/390 [06:41<05:47,  2.36s/it] 63%|██████▎   | 244/390 [06:42<04:13,  1.74s/it] 63%|██████▎   | 245/390 [06:42<03:09,  1.30s/it] 63%|██████▎   | 246/390 [06:42<02:28,  1.03s/it] 63%|██████▎   | 247/390 [06:43<01:55,  1.24it/s] 64%|██████▎   | 248/390 [06:43<01:32,  1.54it/s] 64%|██████▍   | 249/390 [06:43<01:16,  1.85it/s] 64%|██████▍   | 250/390 [06:43<01:05,  2.15it/s] 64%|██████▍   | 251/390 [06:44<00:57,  2.43it/s] 65%|██████▍   | 252/390 [06:44<00:51,  2.67it/s] 65%|██████▍   | 253/390 [06:44<00:47,  2.87it/s] 65%|██████▌   | 254/390 [06:45<00:44,  3.03it/s] 65%|██████▌   | 255/390 [06:45<00:42,  3.15it/s] 66%|██████▌   | 256/390 [06:45<00:41,  3.24it/s] 66%|██████▌   | 257/390 [06:46<00:45,  2.90it/s] 66%|██████▌   | 258/390 [06:46<00:43,  3.05it/s] 66%|██████▋   | 259/390 [06:46<00:41,  3.17it/s] 67%|██████▋   | 260/390 [06:46<00:39,  3.25it/s] 67%|██████▋   | 261/390 [06:47<00:38,  3.32it/s] 67%|██████▋   | 262/390 [06:47<00:38,  3.36it/s] 67%|██████▋   | 263/390 [06:47<00:37,  3.40it/s] 68%|██████▊   | 264/390 [06:48<00:36,  3.42it/s] 68%|██████▊   | 265/390 [06:48<00:36,  3.44it/s] 68%|██████▊   | 266/390 [06:48<00:35,  3.45it/s] 68%|██████▊   | 267/390 [06:49<00:38,  3.22it/s] 69%|██████▊   | 268/390 [06:49<00:37,  3.29it/s] 69%|██████▉   | 269/390 [06:49<00:36,  3.35it/s] 69%|██████▉   | 270/390 [06:49<00:35,  3.39it/s] 69%|██████▉   | 271/390 [06:50<00:34,  3.41it/s] 70%|██████▉   | 272/390 [06:50<00:34,  3.43it/s] 70%|███████   | 273/390 [06:50<00:33,  3.45it/s] 70%|███████   | 274/390 [06:51<00:33,  3.46it/s] 71%|███████   | 275/390 [06:51<00:33,  3.46it/s] 71%|███████   | 276/390 [06:51<00:32,  3.47it/s] 71%|███████   | 277/390 [06:51<00:32,  3.47it/s] 71%|███████▏  | 278/390 [06:52<00:35,  3.17it/s] 72%|███████▏  | 279/390 [06:52<00:34,  3.26it/s] 72%|███████▏  | 280/390 [06:52<00:33,  3.32it/s] 72%|███████▏  | 281/390 [06:53<00:32,  3.36it/s] 72%|███████▏  | 282/390 [06:53<00:31,  3.40it/s] 73%|███████▎  | 283/390 [06:53<00:31,  3.42it/s] 73%|███████▎  | 284/390 [06:54<00:30,  3.44it/s] 73%|███████▎  | 285/390 [06:54<00:30,  3.45it/s] 73%|███████▎  | 286/390 [06:54<00:30,  3.46it/s] 74%|███████▎  | 287/390 [06:54<00:29,  3.47it/s] 74%|███████▍  | 288/390 [06:55<00:29,  3.47it/s] 74%|███████▍  | 289/390 [06:55<00:34,  2.93it/s] 74%|███████▍  | 290/390 [06:55<00:32,  3.07it/s] 75%|███████▍  | 291/390 [06:56<00:31,  3.19it/s] 75%|███████▍  | 292/390 [06:56<00:29,  3.27it/s] 75%|███████▌  | 293/390 [06:56<00:29,  3.33it/s] 75%|███████▌  | 294/390 [06:57<00:28,  3.38it/s] 76%|███████▌  | 295/390 [06:57<00:27,  3.41it/s] 76%|███████▌  | 296/390 [06:57<00:27,  3.43it/s] 76%|███████▌  | 297/390 [06:57<00:26,  3.45it/s] 76%|███████▋  | 298/390 [06:58<00:26,  3.46it/s] 77%|███████▋  | 299/390 [06:58<00:29,  3.08it/s] 77%|███████▋  | 300/390 [06:58<00:28,  3.19it/s] 77%|███████▋  | 301/390 [06:59<00:27,  3.27it/s] 77%|███████▋  | 302/390 [06:59<00:26,  3.33it/s] 78%|███████▊  | 303/390 [06:59<00:25,  3.38it/s] 78%|███████▊  | 304/390 [07:00<00:25,  3.41it/s] 78%|███████▊  | 305/390 [07:00<00:24,  3.43it/s] 78%|███████▊  | 306/390 [07:00<00:24,  3.45it/s] 79%|███████▊  | 307/390 [07:00<00:24,  3.46it/s] 79%|███████▉  | 308/390 [07:01<00:23,  3.47it/s] 79%|███████▉  | 309/390 [07:01<00:23,  3.47it/s] 79%|███████▉  | 310/390 [07:01<00:25,  3.14it/s] 80%|███████▉  | 311/390 [07:02<00:24,  3.23it/s] 80%|████████  | 312/390 [07:02<00:23,  3.30it/s][INFO|trainer.py:2140] 2023-08-28 12:28:05,148 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:28:05,148 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 12:28:05,148 >>   Batch size = 8
{'eval_loss': 1.0342482328414917, 'eval_runtime': 10.3242, 'eval_samples_per_second': 338.719, 'eval_steps_per_second': 42.425, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.41it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.23it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.97it/s][A
  5%|▌         | 22/438 [00:00<00:08, 47.13it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.42it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.85it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.47it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.13it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.24it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.24it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.41it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.55it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.60it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.59it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.50it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.25it/s][A
 20%|█▉        | 87/438 [00:02<00:07, 45.04it/s][A
 21%|██        | 92/438 [00:02<00:10, 33.65it/s][A
 22%|██▏       | 97/438 [00:02<00:09, 36.48it/s][A
 23%|██▎       | 102/438 [00:02<00:08, 38.84it/s][A
 24%|██▍       | 107/438 [00:02<00:08, 40.68it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 42.10it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 43.14it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.86it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 44.37it/s][A
 30%|███       | 132/438 [00:03<00:06, 44.39it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.36it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.59it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.78it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.07it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.27it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.49it/s][A
 38%|███▊      | 167/438 [00:03<00:05, 45.53it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.48it/s][A
 40%|████      | 177/438 [00:04<00:05, 45.17it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 45.07it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 44.95it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.08it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.21it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.33it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.40it/s][A
 48%|████▊     | 212/438 [00:04<00:04, 45.56it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.48it/s][A
 51%|█████     | 222/438 [00:05<00:04, 45.30it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 37.69it/s][A
 53%|█████▎    | 232/438 [00:05<00:05, 39.85it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 41.41it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 42.63it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 43.41it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.27it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.58it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.79it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.60it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.65it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.98it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 45.17it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 45.27it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.42it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 45.44it/s][A
 69%|██████▉   | 302/438 [00:06<00:02, 45.57it/s][A
 70%|███████   | 307/438 [00:06<00:02, 45.28it/s][A
 71%|███████   | 312/438 [00:07<00:02, 45.03it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 44.90it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.95it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 45.13it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 45.36it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 45.37it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 45.50it/s][A
 79%|███████▉  | 347/438 [00:07<00:01, 45.51it/s][A
 80%|████████  | 352/438 [00:07<00:01, 45.34it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.11it/s][A
 83%|████████▎ | 362/438 [00:08<00:02, 33.41it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 36.29it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 38.67it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 40.54it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 41.95it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 43.06it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 43.85it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.32it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.28it/s][A
 93%|█████████▎| 407/438 [00:09<00:01, 23.49it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 27.48it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 31.15it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 34.47it/s][A
 97%|█████████▋| 427/438 [00:10<00:00, 37.25it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 39.44it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 41.23it/s][A                                                 
                                                 [A 80%|████████  | 312/390 [07:12<00:23,  3.30it/s]
100%|██████████| 438/438 [00:10<00:00, 41.23it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:28:15,580 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 12:28:16,312 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:28:44,649 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:28:46,829 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:28:47,261 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [08:55<43:39, 34.02s/it] 81%|████████  | 314/390 [08:55<30:19, 23.94s/it] 81%|████████  | 315/390 [08:55<21:03, 16.84s/it] 81%|████████  | 316/390 [08:56<14:38, 11.88s/it] 81%|████████▏ | 317/390 [08:56<10:13,  8.40s/it] 82%|████████▏ | 318/390 [08:56<07:09,  5.97s/it] 82%|████████▏ | 319/390 [08:57<05:02,  4.26s/it] 82%|████████▏ | 320/390 [08:57<03:35,  3.07s/it] 82%|████████▏ | 321/390 [08:57<02:34,  2.24s/it] 83%|████████▎ | 322/390 [08:57<01:52,  1.65s/it] 83%|████████▎ | 323/390 [08:58<01:23,  1.24s/it] 83%|████████▎ | 324/390 [08:58<01:06,  1.01s/it] 83%|████████▎ | 325/390 [08:58<00:51,  1.26it/s] 84%|████████▎ | 326/390 [08:59<00:41,  1.56it/s] 84%|████████▍ | 327/390 [08:59<00:33,  1.86it/s] 84%|████████▍ | 328/390 [08:59<00:28,  2.17it/s] 84%|████████▍ | 329/390 [09:00<00:24,  2.44it/s] 85%|████████▍ | 330/390 [09:00<00:22,  2.68it/s] 85%|████████▍ | 331/390 [09:00<00:20,  2.88it/s] 85%|████████▌ | 332/390 [09:00<00:19,  3.04it/s] 85%|████████▌ | 333/390 [09:01<00:18,  3.16it/s] 86%|████████▌ | 334/390 [09:01<00:19,  2.88it/s] 86%|████████▌ | 335/390 [09:01<00:18,  3.04it/s] 86%|████████▌ | 336/390 [09:02<00:17,  3.16it/s] 86%|████████▋ | 337/390 [09:02<00:16,  3.26it/s] 87%|████████▋ | 338/390 [09:02<00:15,  3.32it/s] 87%|████████▋ | 339/390 [09:03<00:15,  3.37it/s] 87%|████████▋ | 340/390 [09:03<00:14,  3.41it/s] 87%|████████▋ | 341/390 [09:03<00:14,  3.43it/s] 88%|████████▊ | 342/390 [09:03<00:13,  3.45it/s] 88%|████████▊ | 343/390 [09:04<00:13,  3.46it/s] 88%|████████▊ | 344/390 [09:04<00:13,  3.47it/s] 88%|████████▊ | 345/390 [09:04<00:15,  2.97it/s] 89%|████████▊ | 346/390 [09:05<00:14,  3.11it/s] 89%|████████▉ | 347/390 [09:05<00:13,  3.22it/s] 89%|████████▉ | 348/390 [09:05<00:12,  3.30it/s] 89%|████████▉ | 349/390 [09:06<00:12,  3.35it/s] 90%|████████▉ | 350/390 [09:06<00:11,  3.40it/s] 90%|█████████ | 351/390 [09:06<00:11,  3.43it/s] 90%|█████████ | 352/390 [09:06<00:11,  3.45it/s] 91%|█████████ | 353/390 [09:07<00:10,  3.45it/s] 91%|█████████ | 354/390 [09:07<00:10,  3.46it/s] 91%|█████████ | 355/390 [09:07<00:11,  3.09it/s] 91%|█████████▏| 356/390 [09:08<00:10,  3.20it/s] 92%|█████████▏| 357/390 [09:08<00:10,  3.28it/s] 92%|█████████▏| 358/390 [09:08<00:09,  3.34it/s] 92%|█████████▏| 359/390 [09:09<00:09,  3.39it/s] 92%|█████████▏| 360/390 [09:09<00:08,  3.41it/s] 93%|█████████▎| 361/390 [09:09<00:08,  3.44it/s] 93%|█████████▎| 362/390 [09:09<00:08,  3.46it/s] 93%|█████████▎| 363/390 [09:10<00:07,  3.47it/s] 93%|█████████▎| 364/390 [09:10<00:07,  3.47it/s] 94%|█████████▎| 365/390 [09:10<00:07,  3.48it/s] 94%|█████████▍| 366/390 [09:11<00:07,  3.08it/s] 94%|█████████▍| 367/390 [09:11<00:07,  3.19it/s] 94%|█████████▍| 368/390 [09:11<00:06,  3.28it/s] 95%|█████████▍| 369/390 [09:12<00:06,  3.34it/s] 95%|█████████▍| 370/390 [09:12<00:05,  3.38it/s] 95%|█████████▌| 371/390 [09:12<00:05,  3.41it/s] 95%|█████████▌| 372/390 [09:12<00:05,  3.43it/s] 96%|█████████▌| 373/390 [09:13<00:04,  3.45it/s] 96%|█████████▌| 374/390 [09:13<00:04,  3.46it/s] 96%|█████████▌| 375/390 [09:13<00:04,  3.47it/s] 96%|█████████▋| 376/390 [09:14<00:04,  3.48it/s] 97%|█████████▋| 377/390 [09:14<00:04,  3.18it/s] 97%|█████████▋| 378/390 [09:14<00:03,  3.27it/s] 97%|█████████▋| 379/390 [09:15<00:03,  3.34it/s] 97%|█████████▋| 380/390 [09:15<00:02,  3.38it/s] 98%|█████████▊| 381/390 [09:15<00:02,  3.41it/s] 98%|█████████▊| 382/390 [09:15<00:02,  3.43it/s] 98%|█████████▊| 383/390 [09:16<00:02,  3.45it/s] 98%|█████████▊| 384/390 [09:16<00:01,  3.46it/s] 99%|█████████▊| 385/390 [09:16<00:01,  3.47it/s] 99%|█████████▉| 386/390 [09:17<00:01,  3.48it/s] 99%|█████████▉| 387/390 [09:17<00:00,  3.48it/s] 99%|█████████▉| 388/390 [09:17<00:00,  3.14it/s]100%|█████████▉| 389/390 [09:18<00:00,  3.23it/s]100%|██████████| 390/390 [09:18<00:00,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 12:30:20,865 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:30:20,865 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 12:30:20,865 >>   Batch size = 8
{'eval_loss': 1.040327548980713, 'eval_runtime': 10.3058, 'eval_samples_per_second': 339.323, 'eval_steps_per_second': 42.5, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.38it/s][A
  3%|▎         | 12/438 [00:00<00:08, 48.92it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.79it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.99it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.52it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.90it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.55it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.25it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.28it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.32it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.39it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.35it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.45it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.42it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.38it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.16it/s][A
 20%|█▉        | 87/438 [00:01<00:07, 45.04it/s][A
 21%|██        | 92/438 [00:02<00:07, 45.07it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 45.12it/s][A
 23%|██▎       | 102/438 [00:02<00:11, 30.23it/s][A
 24%|██▍       | 107/438 [00:02<00:09, 33.58it/s][A
 26%|██▌       | 112/438 [00:02<00:08, 36.42it/s][A
 27%|██▋       | 117/438 [00:02<00:08, 38.76it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 40.65it/s][A
 29%|██▉       | 127/438 [00:02<00:07, 42.07it/s][A
 30%|███       | 132/438 [00:03<00:07, 43.17it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 43.80it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 43.89it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 43.97it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.15it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.45it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.68it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.97it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.21it/s][A
 40%|████      | 177/438 [00:04<00:05, 45.35it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 45.41it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 45.21it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.04it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 44.93it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.91it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.05it/s][A
 48%|████▊     | 212/438 [00:04<00:05, 45.20it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.32it/s][A
 51%|█████     | 222/438 [00:05<00:04, 45.38it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 45.28it/s][A
 53%|█████▎    | 232/438 [00:05<00:05, 36.07it/s][A
 54%|█████▍    | 237/438 [00:05<00:05, 38.47it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 40.23it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 41.80it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 42.92it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 43.82it/s][A
 60%|█████▉    | 262/438 [00:06<00:03, 44.33it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.68it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.42it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.40it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.48it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.63it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.83it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 45.25it/s][A
 69%|██████▉   | 302/438 [00:06<00:02, 45.37it/s][A
 70%|███████   | 307/438 [00:07<00:02, 45.46it/s][A
 71%|███████   | 312/438 [00:07<00:02, 45.32it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.07it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 44.84it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 44.74it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 44.93it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 44.97it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 45.17it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 45.25it/s][A
 80%|████████  | 352/438 [00:08<00:01, 45.45it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 45.38it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 45.24it/s][A
 84%|████████▍ | 367/438 [00:08<00:02, 33.15it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 36.12it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 38.53it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 40.33it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 41.71it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 42.82it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 43.56it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.08it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.05it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.30it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.57it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.59it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 45.17it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 45.22it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 45.19it/s][A                                                 
                                                 [A100%|██████████| 390/390 [09:28<00:00,  3.31it/s]
100%|██████████| 438/438 [00:10<00:00, 45.19it/s][A
                                                 [A[INFO|trainer.py:1894] 2023-08-28 12:30:31,105 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 12:30:31,994 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:32:34,216 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:32:36,160 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:32:36,553 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 12:33:26,806 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 12:33:26,853 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78 (score: 1.0145633220672607).
                                                 100%|██████████| 390/390 [12:36<00:00,  3.31it/s]100%|██████████| 390/390 [12:36<00:00,  1.94s/it]
[INFO|trainer.py:1894] 2023-08-28 12:33:39,047 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model
[INFO|configuration_utils.py:351] 2023-08-28 12:33:39,120 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 12:33:45,542 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 12:33:46,124 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 12:33:46,406 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:33:48,735 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:33:48,851 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:33:48,851 >>   train_loss               =     0.6867
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:33:48,851 >>   train_runtime            = 0:12:36.23
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:33:48,851 >>   train_samples            =       5022
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:33:48,851 >>   train_samples_per_second =     33.204
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:33:48,851 >>   train_steps_per_second   =      0.516
{'eval_loss': 1.0444555282592773, 'eval_runtime': 10.0774, 'eval_samples_per_second': 347.013, 'eval_steps_per_second': 43.463, 'epoch': 4.99}
{'train_runtime': 756.234, 'train_samples_per_second': 33.204, 'train_steps_per_second': 0.516, 'train_loss': 0.6866620968549679, 'epoch': 4.99}
08/28/2023 12:33:50 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 12:33:50,329 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 12:33:50,329 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 12:33:50,329 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 56.80it/s]  3%|▎         | 12/438 [00:00<00:08, 50.01it/s]  4%|▍         | 18/438 [00:00<00:08, 48.14it/s]  5%|▌         | 23/438 [00:00<00:08, 47.40it/s]  6%|▋         | 28/438 [00:00<00:08, 47.00it/s]  8%|▊         | 33/438 [00:00<00:08, 46.69it/s]  9%|▊         | 38/438 [00:00<00:08, 46.54it/s] 10%|▉         | 43/438 [00:00<00:08, 46.05it/s] 11%|█         | 48/438 [00:01<00:08, 45.61it/s] 12%|█▏        | 53/438 [00:01<00:08, 45.38it/s] 13%|█▎        | 58/438 [00:01<00:08, 45.44it/s] 14%|█▍        | 63/438 [00:01<00:08, 45.60it/s] 16%|█▌        | 68/438 [00:01<00:08, 45.62it/s] 17%|█▋        | 73/438 [00:01<00:10, 36.20it/s] 18%|█▊        | 78/438 [00:01<00:09, 38.77it/s] 19%|█▉        | 83/438 [00:01<00:08, 40.65it/s] 20%|██        | 88/438 [00:01<00:08, 42.15it/s] 21%|██        | 93/438 [00:02<00:07, 43.27it/s] 22%|██▏       | 98/438 [00:02<00:07, 44.09it/s] 24%|██▎       | 103/438 [00:02<00:07, 44.72it/s] 25%|██▍       | 108/438 [00:02<00:07, 45.00it/s] 26%|██▌       | 113/438 [00:02<00:07, 44.89it/s] 27%|██▋       | 118/438 [00:02<00:07, 44.78it/s] 28%|██▊       | 123/438 [00:02<00:07, 44.81it/s] 29%|██▉       | 128/438 [00:02<00:06, 45.14it/s] 30%|███       | 133/438 [00:02<00:06, 45.22it/s] 32%|███▏      | 138/438 [00:03<00:06, 45.53it/s] 33%|███▎      | 143/438 [00:03<00:06, 45.69it/s] 34%|███▍      | 148/438 [00:03<00:06, 45.78it/s] 35%|███▍      | 153/438 [00:03<00:06, 45.77it/s] 36%|███▌      | 158/438 [00:03<00:06, 45.50it/s] 37%|███▋      | 163/438 [00:03<00:06, 45.33it/s] 38%|███▊      | 168/438 [00:03<00:05, 45.24it/s] 39%|███▉      | 173/438 [00:03<00:05, 45.25it/s] 41%|████      | 178/438 [00:03<00:05, 45.31it/s] 42%|████▏     | 183/438 [00:04<00:05, 45.44it/s] 43%|████▎     | 188/438 [00:04<00:05, 45.57it/s] 44%|████▍     | 193/438 [00:04<00:05, 45.78it/s] 45%|████▌     | 198/438 [00:04<00:05, 45.80it/s] 46%|████▋     | 203/438 [00:04<00:05, 45.61it/s] 47%|████▋     | 208/438 [00:04<00:06, 36.18it/s] 49%|████▊     | 213/438 [00:04<00:05, 38.70it/s] 50%|████▉     | 218/438 [00:04<00:05, 40.66it/s] 51%|█████     | 223/438 [00:05<00:05, 42.03it/s] 52%|█████▏    | 228/438 [00:05<00:04, 43.24it/s] 53%|█████▎    | 233/438 [00:05<00:04, 44.01it/s] 54%|█████▍    | 238/438 [00:05<00:04, 44.59it/s] 55%|█████▌    | 243/438 [00:05<00:04, 44.99it/s] 57%|█████▋    | 248/438 [00:05<00:04, 44.79it/s] 58%|█████▊    | 253/438 [00:05<00:04, 44.60it/s] 59%|█████▉    | 258/438 [00:05<00:04, 44.71it/s] 60%|██████    | 263/438 [00:05<00:03, 45.03it/s] 61%|██████    | 268/438 [00:06<00:03, 45.31it/s] 62%|██████▏   | 273/438 [00:06<00:03, 45.39it/s] 63%|██████▎   | 278/438 [00:06<00:03, 45.58it/s] 65%|██████▍   | 283/438 [00:06<00:03, 45.64it/s] 66%|██████▌   | 288/438 [00:06<00:03, 45.74it/s] 67%|██████▋   | 293/438 [00:06<00:03, 45.30it/s] 68%|██████▊   | 298/438 [00:06<00:03, 44.98it/s] 69%|██████▉   | 303/438 [00:06<00:03, 44.86it/s] 70%|███████   | 308/438 [00:06<00:02, 45.00it/s] 71%|███████▏  | 313/438 [00:07<00:02, 45.26it/s] 73%|███████▎  | 318/438 [00:07<00:02, 45.36it/s] 74%|███████▎  | 323/438 [00:07<00:02, 45.46it/s] 75%|███████▍  | 328/438 [00:07<00:02, 45.65it/s] 76%|███████▌  | 333/438 [00:07<00:02, 45.73it/s] 77%|███████▋  | 338/438 [00:07<00:02, 45.59it/s] 78%|███████▊  | 343/438 [00:07<00:02, 38.88it/s] 79%|███████▉  | 348/438 [00:07<00:02, 40.77it/s] 81%|████████  | 353/438 [00:07<00:02, 42.19it/s] 82%|████████▏ | 358/438 [00:08<00:01, 43.25it/s] 83%|████████▎ | 363/438 [00:08<00:01, 44.07it/s] 84%|████████▍ | 368/438 [00:08<00:01, 44.54it/s] 85%|████████▌ | 373/438 [00:08<00:01, 44.80it/s] 86%|████████▋ | 378/438 [00:08<00:01, 45.09it/s] 87%|████████▋ | 383/438 [00:08<00:01, 44.82it/s] 89%|████████▊ | 388/438 [00:08<00:01, 44.78it/s] 90%|████████▉ | 393/438 [00:08<00:01, 44.97it/s] 91%|█████████ | 398/438 [00:08<00:00, 45.13it/s] 92%|█████████▏| 403/438 [00:09<00:00, 45.25it/s] 93%|█████████▎| 408/438 [00:09<00:00, 45.46it/s] 94%|█████████▍| 413/438 [00:09<00:00, 45.54it/s] 95%|█████████▌| 418/438 [00:09<00:00, 45.68it/s] 97%|█████████▋| 423/438 [00:09<00:00, 45.59it/s] 98%|█████████▊| 428/438 [00:09<00:00, 45.28it/s] 99%|█████████▉| 433/438 [00:09<00:00, 45.22it/s]100%|██████████| 438/438 [00:09<00:00, 45.25it/s]100%|██████████| 438/438 [00:09<00:00, 44.52it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 12:34:00,185 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:34:00,185 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:34:00,185 >>   eval_loss               =     1.0146
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:34:00,185 >>   eval_runtime            = 0:00:09.85
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:34:00,185 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:34:00,185 >>   eval_samples_per_second =    354.838
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:34:00,185 >>   eval_steps_per_second   =     44.444
[INFO|trainer_pt_utils.py:913] 2023-08-28 12:34:00,185 >>   perplexity              =     2.7582
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:26,803 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:26,939 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:26,940 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:26,940 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:26,940 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:34:28,215 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:34:28,216 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:34:28,750 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:34:30,052 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:34:30,187 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:33,253 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:33,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:33,323 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:33,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:34:33,324 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:34:34,489 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:34:34,490 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:34:35,330 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:34:35,681 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:34:35,681 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/checkpoint-312
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.36it/s]Extractor Predicting: 2it [00:01,  1.46it/s]Extractor Predicting: 3it [00:02,  1.51it/s]Extractor Predicting: 4it [00:02,  1.54it/s]Extractor Predicting: 5it [00:03,  1.55it/s]Extractor Predicting: 6it [00:03,  1.51it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.49it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.55it/s]Extractor Predicting: 21it [00:13,  1.50it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.61it/s]Extractor Predicting: 25it [00:16,  1.63it/s]Extractor Predicting: 26it [00:16,  1.56it/s]Extractor Predicting: 27it [00:17,  1.56it/s]Extractor Predicting: 28it [00:18,  1.57it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.53it/s]Extractor Predicting: 31it [00:20,  1.45it/s]Extractor Predicting: 32it [00:21,  1.44it/s]Extractor Predicting: 33it [00:21,  1.48it/s]Extractor Predicting: 34it [00:22,  1.52it/s]Extractor Predicting: 35it [00:22,  1.53it/s]Extractor Predicting: 36it [00:23,  1.45it/s]Extractor Predicting: 37it [00:24,  1.49it/s]Extractor Predicting: 38it [00:24,  1.52it/s]Extractor Predicting: 39it [00:25,  1.48it/s]Extractor Predicting: 40it [00:26,  1.51it/s]Extractor Predicting: 41it [00:27,  1.43it/s]Extractor Predicting: 42it [00:27,  1.46it/s]Extractor Predicting: 43it [00:28,  1.50it/s]Extractor Predicting: 44it [00:29,  1.51it/s]Extractor Predicting: 45it [00:29,  1.51it/s]Extractor Predicting: 46it [00:30,  1.47it/s]Extractor Predicting: 47it [00:31,  1.45it/s]Extractor Predicting: 48it [00:31,  1.51it/s]Extractor Predicting: 49it [00:32,  1.50it/s]Extractor Predicting: 50it [00:33,  1.52it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:34,  1.48it/s]Extractor Predicting: 53it [00:35,  1.48it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:36,  1.51it/s]Extractor Predicting: 56it [00:37,  1.45it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:38,  1.49it/s]Extractor Predicting: 59it [00:39,  1.47it/s]Extractor Predicting: 60it [00:39,  1.48it/s]Extractor Predicting: 61it [00:40,  1.42it/s]Extractor Predicting: 62it [00:41,  1.48it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.50it/s]Extractor Predicting: 65it [00:43,  1.54it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:44,  1.52it/s]Extractor Predicting: 68it [00:45,  1.55it/s]Extractor Predicting: 69it [00:45,  1.55it/s]Extractor Predicting: 70it [00:46,  1.59it/s]Extractor Predicting: 71it [00:46,  1.53it/s]Extractor Predicting: 72it [00:47,  1.58it/s]Extractor Predicting: 73it [00:48,  1.56it/s]Extractor Predicting: 74it [00:48,  1.54it/s]Extractor Predicting: 75it [00:49,  1.57it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:50,  1.53it/s]Extractor Predicting: 78it [00:51,  1.53it/s]Extractor Predicting: 79it [00:52,  1.59it/s]Extractor Predicting: 80it [00:52,  1.61it/s]Extractor Predicting: 81it [00:53,  1.62it/s]Extractor Predicting: 82it [00:54,  1.50it/s]Extractor Predicting: 83it [00:54,  1.52it/s]Extractor Predicting: 84it [00:55,  1.52it/s]Extractor Predicting: 85it [00:56,  1.51it/s]Extractor Predicting: 86it [00:56,  1.51it/s]Extractor Predicting: 87it [00:57,  1.48it/s]Extractor Predicting: 88it [00:58,  1.50it/s]Extractor Predicting: 89it [00:58,  1.52it/s]Extractor Predicting: 90it [00:59,  1.55it/s]Extractor Predicting: 91it [00:59,  1.54it/s]Extractor Predicting: 92it [01:00,  1.49it/s]Extractor Predicting: 93it [01:01,  1.50it/s]Extractor Predicting: 94it [01:01,  1.54it/s]Extractor Predicting: 95it [01:02,  1.58it/s]Extractor Predicting: 96it [01:03,  1.56it/s]Extractor Predicting: 97it [01:03,  1.48it/s]Extractor Predicting: 98it [01:04,  1.49it/s]Extractor Predicting: 99it [01:05,  1.38it/s]Extractor Predicting: 100it [01:06,  1.42it/s]Extractor Predicting: 101it [01:06,  1.48it/s]Extractor Predicting: 102it [01:07,  1.43it/s]Extractor Predicting: 103it [01:08,  1.46it/s]Extractor Predicting: 104it [01:08,  1.47it/s]Extractor Predicting: 105it [01:09,  1.49it/s]Extractor Predicting: 106it [01:10,  1.51it/s]Extractor Predicting: 107it [01:10,  1.42it/s]Extractor Predicting: 108it [01:11,  1.47it/s]Extractor Predicting: 109it [01:12,  1.46it/s]Extractor Predicting: 110it [01:12,  1.48it/s]Extractor Predicting: 111it [01:13,  1.51it/s]Extractor Predicting: 112it [01:14,  1.47it/s]Extractor Predicting: 113it [01:14,  1.47it/s]Extractor Predicting: 114it [01:15,  1.48it/s]Extractor Predicting: 115it [01:16,  1.49it/s]Extractor Predicting: 116it [01:16,  1.48it/s]Extractor Predicting: 117it [01:17,  1.40it/s]Extractor Predicting: 118it [01:18,  1.44it/s]Extractor Predicting: 119it [01:19,  1.46it/s]Extractor Predicting: 120it [01:19,  1.47it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.44it/s]Extractor Predicting: 123it [01:21,  1.46it/s]Extractor Predicting: 124it [01:22,  1.47it/s]Extractor Predicting: 125it [01:23,  1.47it/s]Extractor Predicting: 126it [01:23,  1.48it/s]Extractor Predicting: 127it [01:24,  1.52it/s]Extractor Predicting: 128it [01:25,  1.45it/s]Extractor Predicting: 129it [01:25,  1.47it/s]Extractor Predicting: 130it [01:26,  1.49it/s]Extractor Predicting: 131it [01:27,  1.47it/s]Extractor Predicting: 132it [01:27,  1.48it/s]Extractor Predicting: 133it [01:28,  1.43it/s]Extractor Predicting: 134it [01:29,  1.47it/s]Extractor Predicting: 135it [01:29,  1.44it/s]Extractor Predicting: 136it [01:30,  1.42it/s]Extractor Predicting: 137it [01:31,  1.47it/s]Extractor Predicting: 138it [01:32,  1.43it/s]Extractor Predicting: 139it [01:32,  1.45it/s]Extractor Predicting: 140it [01:33,  1.47it/s]Extractor Predicting: 141it [01:34,  1.48it/s]Extractor Predicting: 142it [01:34,  1.45it/s]Extractor Predicting: 143it [01:35,  1.41it/s]Extractor Predicting: 144it [01:36,  1.47it/s]Extractor Predicting: 145it [01:36,  1.67it/s]Extractor Predicting: 145it [01:36,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:38,567 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:38,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:38,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:38,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:38,657 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:38:40,150 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:38:40,151 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:38:40,901 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:38:42,145 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:38:42,227 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:45,773 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:45,863 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:45,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:45,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:38:45,864 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:38:47,075 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:38:47,077 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:38:47,808 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:38:48,128 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:38:48,128 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6926470588235294,
  "recall": 0.13468687446382613,
  "score": 0.22552070864256643,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.49it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.59it/s]Extractor Predicting: 6it [00:03,  1.48it/s]Extractor Predicting: 7it [00:04,  1.51it/s]Extractor Predicting: 8it [00:05,  1.50it/s]Extractor Predicting: 9it [00:05,  1.56it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.50it/s]Extractor Predicting: 12it [00:07,  1.53it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.49it/s]Extractor Predicting: 15it [00:09,  1.47it/s]Extractor Predicting: 16it [00:10,  1.44it/s]Extractor Predicting: 17it [00:11,  1.48it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.51it/s]Extractor Predicting: 20it [00:13,  1.51it/s]Extractor Predicting: 21it [00:13,  1.53it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.55it/s]Extractor Predicting: 24it [00:15,  1.53it/s]Extractor Predicting: 25it [00:16,  1.46it/s]Extractor Predicting: 26it [00:17,  1.47it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.52it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.53it/s]Extractor Predicting: 32it [00:21,  1.54it/s]Extractor Predicting: 33it [00:21,  1.53it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:23,  1.49it/s]Extractor Predicting: 36it [00:23,  1.54it/s]Extractor Predicting: 37it [00:24,  1.58it/s]Extractor Predicting: 38it [00:24,  1.59it/s]Extractor Predicting: 39it [00:25,  1.60it/s]Extractor Predicting: 40it [00:26,  1.50it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:27,  1.54it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.48it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.57it/s]Extractor Predicting: 49it [00:32,  1.59it/s]Extractor Predicting: 50it [00:32,  1.50it/s]Extractor Predicting: 51it [00:33,  1.53it/s]Extractor Predicting: 52it [00:34,  1.57it/s]Extractor Predicting: 53it [00:34,  1.57it/s]Extractor Predicting: 54it [00:35,  1.57it/s]Extractor Predicting: 55it [00:35,  1.54it/s]Extractor Predicting: 56it [00:36,  1.52it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:37,  1.53it/s]Extractor Predicting: 59it [00:38,  1.50it/s]Extractor Predicting: 60it [00:39,  1.45it/s]Extractor Predicting: 61it [00:40,  1.47it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:41,  1.57it/s]Extractor Predicting: 65it [00:42,  1.47it/s]Extractor Predicting: 66it [00:43,  1.51it/s]Extractor Predicting: 67it [00:43,  1.54it/s]Extractor Predicting: 68it [00:44,  1.58it/s]Extractor Predicting: 69it [00:45,  1.61it/s]Extractor Predicting: 70it [00:45,  1.60it/s]Extractor Predicting: 71it [00:46,  1.61it/s]Extractor Predicting: 72it [00:47,  1.54it/s]Extractor Predicting: 73it [00:47,  1.56it/s]Extractor Predicting: 74it [00:48,  1.54it/s]Extractor Predicting: 75it [00:49,  1.52it/s]Extractor Predicting: 76it [00:49,  1.56it/s]Extractor Predicting: 77it [00:50,  1.54it/s]Extractor Predicting: 78it [00:50,  1.56it/s]Extractor Predicting: 79it [00:51,  1.59it/s]Extractor Predicting: 80it [00:52,  1.61it/s]Extractor Predicting: 81it [00:52,  1.61it/s]Extractor Predicting: 82it [00:53,  1.51it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:54,  1.56it/s]Extractor Predicting: 85it [00:55,  1.58it/s]Extractor Predicting: 86it [00:55,  1.62it/s]Extractor Predicting: 87it [00:56,  1.56it/s]Extractor Predicting: 88it [00:57,  1.57it/s]Extractor Predicting: 89it [00:57,  1.53it/s]Extractor Predicting: 90it [00:58,  1.57it/s]Extractor Predicting: 91it [00:59,  1.61it/s]Extractor Predicting: 92it [01:00,  1.41it/s]Extractor Predicting: 93it [01:00,  1.43it/s]Extractor Predicting: 94it [01:01,  1.44it/s]Extractor Predicting: 95it [01:02,  1.48it/s]Extractor Predicting: 96it [01:02,  1.52it/s]Extractor Predicting: 97it [01:03,  1.48it/s]Extractor Predicting: 98it [01:04,  1.48it/s]Extractor Predicting: 99it [01:04,  1.51it/s]Extractor Predicting: 100it [01:05,  1.53it/s]Extractor Predicting: 101it [01:05,  1.56it/s]Extractor Predicting: 102it [01:06,  1.43it/s]Extractor Predicting: 103it [01:07,  1.48it/s]Extractor Predicting: 104it [01:08,  1.49it/s]Extractor Predicting: 105it [01:08,  1.49it/s]Extractor Predicting: 106it [01:09,  1.50it/s]Extractor Predicting: 107it [01:10,  1.41it/s]Extractor Predicting: 108it [01:10,  1.48it/s]Extractor Predicting: 109it [01:11,  1.51it/s]Extractor Predicting: 110it [01:12,  1.54it/s]Extractor Predicting: 111it [01:12,  1.59it/s]Extractor Predicting: 112it [01:13,  1.52it/s]Extractor Predicting: 113it [01:14,  1.53it/s]Extractor Predicting: 114it [01:14,  1.52it/s]Extractor Predicting: 115it [01:15,  1.53it/s]Extractor Predicting: 116it [01:15,  1.56it/s]Extractor Predicting: 117it [01:16,  1.58it/s]Extractor Predicting: 118it [01:17,  1.57it/s]Extractor Predicting: 119it [01:17,  1.51it/s]Extractor Predicting: 120it [01:18,  1.51it/s]Extractor Predicting: 121it [01:19,  1.53it/s]Extractor Predicting: 122it [01:19,  1.58it/s]Extractor Predicting: 123it [01:20,  1.59it/s]Extractor Predicting: 124it [01:21,  1.53it/s]Extractor Predicting: 125it [01:21,  1.52it/s]Extractor Predicting: 126it [01:22,  1.50it/s]Extractor Predicting: 127it [01:23,  1.51it/s]Extractor Predicting: 128it [01:23,  1.54it/s]Extractor Predicting: 129it [01:24,  1.52it/s]Extractor Predicting: 130it [01:25,  1.52it/s]Extractor Predicting: 131it [01:25,  1.57it/s]Extractor Predicting: 132it [01:26,  1.58it/s]Extractor Predicting: 133it [01:26,  1.57it/s]Extractor Predicting: 134it [01:27,  1.48it/s]Extractor Predicting: 135it [01:28,  1.49it/s]Extractor Predicting: 136it [01:29,  1.51it/s]Extractor Predicting: 137it [01:29,  1.55it/s]Extractor Predicting: 138it [01:30,  1.55it/s]Extractor Predicting: 139it [01:31,  1.48it/s]Extractor Predicting: 140it [01:31,  1.50it/s]Extractor Predicting: 141it [01:32,  1.55it/s]Extractor Predicting: 142it [01:32,  1.53it/s]Extractor Predicting: 143it [01:33,  1.54it/s]Extractor Predicting: 144it [01:34,  1.48it/s]Extractor Predicting: 145it [01:34,  1.87it/s]Extractor Predicting: 145it [01:34,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:34,552 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:34,618 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:34,618 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:34,618 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:34,618 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:43:35,777 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:43:35,778 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:43:36,491 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:43:37,616 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:43:37,671 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:41,030 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:41,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:41,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:41,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:43:41,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:43:42,204 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:43:42,206 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:43:42,949 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:43:43,360 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:43:43,360 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_single_is_eval_False.jsonl",
  "precision": 0.46921641791044777,
  "recall": 0.14541775079502747,
  "score": 0.2220260428161554,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.55it/s]Extractor Predicting: 1it [00:00,  1.55it/s]
[INFO|configuration_utils.py:515] 2023-08-28 12:43:51,684 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:43:51,782 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 12:43:51,972 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:43:51,973 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 12:43:52,059 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 12:44:33,433 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 12:44:33,506 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 12:44:34,106 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 12:44:34,107 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel/unseen_5_seed_4/generator/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 12:44:34,458 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:44:34,692 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:44:34,692 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:44:34,692 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:44:34,692 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:44:34,692 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 12:44:34,692 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.2,
  "recall": 0.024390243902439025,
  "score": 0.04347826086956522,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 12:44:35,518 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:36,107 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:36,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:37,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:37,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:38,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:39,110 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:39,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:40,246 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:40,807 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:41,404 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:41,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:42,626 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:43,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:43,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:44,456 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:45,165 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:45,726 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:46,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:47,099 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:47,723 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:12<01:54, 12.68s/it][WARNING|generation_utils.py:914] 2023-08-28 12:44:48,202 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:48,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:49,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:50,132 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:50,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:51,272 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:51,857 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:52,468 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:53,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:53,699 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:54,376 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:55,044 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:55,838 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:56,536 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:57,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:57,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:58,265 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:58,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:44:59,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:00,251 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:00,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:26<01:45, 13.13s/it][WARNING|generation_utils.py:914] 2023-08-28 12:45:01,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:02,287 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:03,076 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:03,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:04,340 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:04,946 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:05,589 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:06,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:06,940 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:07,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:08,116 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:08,828 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:09,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:10,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:10,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:11,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:12,303 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:12,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:13,447 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:14,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:14,847 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:15,593 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:16,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:41<01:38, 14.11s/it][WARNING|generation_utils.py:914] 2023-08-28 12:45:17,034 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:17,924 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:18,567 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:19,224 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:19,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:20,806 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:21,548 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:22,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:22,748 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:23,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:24,168 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:25,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:25,672 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:26,232 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:26,810 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:27,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:28,156 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:28,700 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:29,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:29,866 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:30,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:31,563 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:56<01:27, 14.56s/it][WARNING|generation_utils.py:914] 2023-08-28 12:45:32,172 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:32,800 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:33,588 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:34,187 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:34,926 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:35,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:36,308 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:36,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:37,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:38,361 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:39,088 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:39,733 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:40,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:41,148 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:42,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:42,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:43,356 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:44,000 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:44,545 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:45,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:45,960 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:46,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:47,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:12<01:14, 14.90s/it][WARNING|generation_utils.py:914] 2023-08-28 12:45:47,913 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:48,443 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:48,941 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:49,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:49,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:50,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:51,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:51,814 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:52,329 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:52,882 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:53,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:53,894 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:54,439 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:55,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:55,622 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:56,214 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:56,773 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:57,275 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:57,867 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:58,546 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:59,072 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:45:59,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:24<00:56, 14.07s/it][WARNING|generation_utils.py:914] 2023-08-28 12:46:00,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:00,837 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:01,343 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:02,151 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:02,710 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:03,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:03,804 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:04,259 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:04,868 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:05,566 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:06,127 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:06,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:07,237 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:07,785 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:08,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:09,085 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:09,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:10,173 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:10,729 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:11,284 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:36<00:39, 13.29s/it][WARNING|generation_utils.py:914] 2023-08-28 12:46:11,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:12,526 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:13,122 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:13,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:14,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:14,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:15,380 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:16,068 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:16,600 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:17,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:17,811 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:18,445 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:19,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:19,682 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:20,279 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:20,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:21,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:22,412 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:22,949 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:23,570 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:24,417 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:24,983 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:25,728 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:50<00:27, 13.68s/it][WARNING|generation_utils.py:914] 2023-08-28 12:46:26,321 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:26,988 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:27,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:28,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:29,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:29,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:30,426 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:31,007 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:31,581 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:32,395 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:32,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:33,582 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:34,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:34,727 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:35,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:36,075 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:37,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:37,734 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:38,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:39,053 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:39,704 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:40,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:05<00:13, 13.94s/it][WARNING|generation_utils.py:914] 2023-08-28 12:46:40,947 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:41,554 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:42,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:42,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:43,323 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:43,822 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:44,366 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:45,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:45,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:46,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:46,910 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:47,418 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:47,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:48,541 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:49,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:49,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:50,501 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:51,049 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:51,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:52,320 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:52,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:53,487 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:54,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:54,647 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:55,268 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:55,772 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:56,500 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:57,150 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:57,739 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:58,301 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 12:46:58,908 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:24<00:00, 15.42s/it]Generating: 100%|██████████| 10/10 [02:24<00:00, 14.40s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:15,055 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:15,057 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:15,057 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:15,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:15,058 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:47:16,694 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:47:16,695 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:47:17,513 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:47:19,000 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:47:19,001 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:22,763 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:22,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:22,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:22,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:47:22,926 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:47:24,392 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:47:24,394 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:47:25,226 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:47:25,663 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:47:25,663 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 114, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 204, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 292, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 344, 'raw': 384}
{'target': 600, 'success': 374, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 431, 'raw': 480}
{'target': 600, 'success': 461, 'raw': 512}
{'target': 600, 'success': 486, 'raw': 544}
{'target': 600, 'success': 513, 'raw': 576}
{'target': 600, 'success': 545, 'raw': 608}
{'target': 600, 'success': 575, 'raw': 640}
{'target': 600, 'success': 605, 'raw': 672}
{'prompt': 'Relation : director .', 'success_rate': 0.9002976190476191, 'errors': {''}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 232, 'raw': 256}
{'target': 600, 'success': 263, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 321, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 379, 'raw': 416}
{'target': 600, 'success': 409, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 467, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 529, 'raw': 576}
{'target': 600, 'success': 559, 'raw': 608}
{'target': 600, 'success': 590, 'raw': 640}
{'target': 600, 'success': 618, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9196428571428571, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 138, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 225, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 273, 'raw': 320}
{'target': 600, 'success': 298, 'raw': 352}
{'target': 600, 'success': 328, 'raw': 384}
{'target': 600, 'success': 356, 'raw': 416}
{'target': 600, 'success': 382, 'raw': 448}
{'target': 600, 'success': 409, 'raw': 480}
{'target': 600, 'success': 437, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 490, 'raw': 576}
{'target': 600, 'success': 513, 'raw': 608}
{'target': 600, 'success': 541, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : mother .', 'success_rate': 0.8519021739130435, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 136, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 215, 'raw': 256}
{'target': 600, 'success': 243, 'raw': 288}
{'target': 600, 'success': 271, 'raw': 320}
{'target': 600, 'success': 301, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 378, 'raw': 448}
{'target': 600, 'success': 408, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 464, 'raw': 544}
{'target': 600, 'success': 495, 'raw': 576}
{'target': 600, 'success': 523, 'raw': 608}
{'target': 600, 'success': 552, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 609, 'raw': 704}
{'prompt': 'Relation : part of .', 'success_rate': 0.8650568181818182, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 81, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 191, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 247, 'raw': 288}
{'target': 600, 'success': 275, 'raw': 320}
{'target': 600, 'success': 302, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 357, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 411, 'raw': 480}
{'target': 600, 'success': 438, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 491, 'raw': 576}
{'target': 600, 'success': 518, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 569, 'raw': 672}
{'target': 600, 'success': 598, 'raw': 704}
{'target': 600, 'success': 627, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8519021739130435, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 109, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 199, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 314, 'raw': 352}
{'target': 600, 'success': 340, 'raw': 384}
{'target': 600, 'success': 368, 'raw': 416}
{'target': 600, 'success': 393, 'raw': 448}
{'target': 600, 'success': 421, 'raw': 480}
{'target': 600, 'success': 449, 'raw': 512}
{'target': 600, 'success': 474, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 530, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 590, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 152, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 370, 'raw': 384}
{'target': 600, 'success': 401, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 554, 'raw': 576}
{'target': 600, 'success': 583, 'raw': 608}
{'target': 600, 'success': 615, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9609375, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 57, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 112, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 166, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 269, 'raw': 320}
{'target': 600, 'success': 299, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 460, 'raw': 544}
{'target': 600, 'success': 485, 'raw': 576}
{'target': 600, 'success': 510, 'raw': 608}
{'target': 600, 'success': 537, 'raw': 640}
{'target': 600, 'success': 565, 'raw': 672}
{'target': 600, 'success': 593, 'raw': 704}
{'target': 600, 'success': 622, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.845108695652174, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 162, 'raw': 192}
{'target': 600, 'success': 192, 'raw': 224}
{'target': 600, 'success': 216, 'raw': 256}
{'target': 600, 'success': 244, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 297, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 376, 'raw': 448}
{'target': 600, 'success': 406, 'raw': 480}
{'target': 600, 'success': 434, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 572, 'raw': 672}
{'target': 600, 'success': 601, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8536931818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
['Relation : position held . Context : On 31 occasions with the New York Yankees , he played shortstop and fifth outfielder respectively at shortstop and third baseman , respectively . Head Entity : first baseman , Tail Entity : shortstop .\n']
{'target': 600, 'success': 20, 'raw': 32}
{'target': 600, 'success': 41, 'raw': 64}
{'target': 600, 'success': 60, 'raw': 96}
{'target': 600, 'success': 77, 'raw': 128}
{'target': 600, 'success': 98, 'raw': 160}
{'target': 600, 'success': 115, 'raw': 192}
{'target': 600, 'success': 136, 'raw': 224}
{'target': 600, 'success': 156, 'raw': 256}
{'target': 600, 'success': 169, 'raw': 288}
{'target': 600, 'success': 191, 'raw': 320}
{'target': 600, 'success': 211, 'raw': 352}
{'target': 600, 'success': 229, 'raw': 384}
{'target': 600, 'success': 246, 'raw': 416}
{'target': 600, 'success': 263, 'raw': 448}
{'target': 600, 'success': 281, 'raw': 480}
{'target': 600, 'success': 303, 'raw': 512}
{'target': 600, 'success': 319, 'raw': 544}
{'target': 600, 'success': 345, 'raw': 576}
{'target': 600, 'success': 365, 'raw': 608}
{'target': 600, 'success': 389, 'raw': 640}
{'target': 600, 'success': 406, 'raw': 672}
{'target': 600, 'success': 428, 'raw': 704}
{'target': 600, 'success': 449, 'raw': 736}
{'target': 600, 'success': 472, 'raw': 768}
{'target': 600, 'success': 495, 'raw': 800}
{'target': 600, 'success': 518, 'raw': 832}
{'target': 600, 'success': 540, 'raw': 864}
{'target': 600, 'success': 561, 'raw': 896}
{'target': 600, 'success': 580, 'raw': 928}
{'target': 600, 'success': 599, 'raw': 960}
{'target': 600, 'success': 620, 'raw': 992}
{'prompt': 'Relation : position held .', 'success_rate': 0.625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/2.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/2_ext.jsonl'}}
estimate vocab size: 9787
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 9887, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.51it/s]Extractor Estimating: 2it [00:01,  1.35it/s]Extractor Estimating: 3it [00:02,  1.43it/s]Extractor Estimating: 4it [00:02,  1.46it/s]Extractor Estimating: 5it [00:03,  1.51it/s]Extractor Estimating: 6it [00:04,  1.44it/s]Extractor Estimating: 7it [00:04,  1.48it/s]Extractor Estimating: 8it [00:05,  1.54it/s]Extractor Estimating: 9it [00:06,  1.52it/s]Extractor Estimating: 10it [00:06,  1.54it/s]Extractor Estimating: 11it [00:07,  1.47it/s]Extractor Estimating: 12it [00:08,  1.48it/s]Extractor Estimating: 13it [00:08,  1.48it/s]Extractor Estimating: 14it [00:09,  1.49it/s]Extractor Estimating: 15it [00:10,  1.51it/s]Extractor Estimating: 16it [00:10,  1.52it/s]Extractor Estimating: 17it [00:11,  1.55it/s]Extractor Estimating: 18it [00:11,  1.60it/s]Extractor Estimating: 19it [00:12,  1.51it/s]Extractor Estimating: 20it [00:13,  1.51it/s]Extractor Estimating: 21it [00:14,  1.51it/s]Extractor Estimating: 22it [00:14,  1.50it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:16,  1.43it/s]Extractor Estimating: 25it [00:16,  1.51it/s]Extractor Estimating: 26it [00:17,  1.56it/s]Extractor Estimating: 27it [00:17,  1.59it/s]Extractor Estimating: 28it [00:18,  1.62it/s]Extractor Estimating: 29it [00:19,  1.56it/s]Extractor Estimating: 30it [00:19,  1.59it/s]Extractor Estimating: 31it [00:20,  1.66it/s]Extractor Estimating: 32it [00:20,  1.66it/s]Extractor Estimating: 33it [00:21,  1.64it/s]Extractor Estimating: 34it [00:22,  1.56it/s]Extractor Estimating: 35it [00:22,  1.61it/s]Extractor Estimating: 36it [00:23,  1.65it/s]Extractor Estimating: 37it [00:24,  1.63it/s]Extractor Estimating: 38it [00:24,  1.69it/s]Extractor Estimating: 39it [00:25,  1.53it/s]Extractor Estimating: 40it [00:25,  1.56it/s]Extractor Estimating: 41it [00:26,  1.59it/s]Extractor Estimating: 42it [00:27,  1.66it/s]Extractor Estimating: 43it [00:27,  1.72it/s]Extractor Estimating: 44it [00:28,  1.66it/s]Extractor Estimating: 45it [00:28,  1.65it/s]Extractor Estimating: 46it [00:29,  1.69it/s]Extractor Estimating: 47it [00:29,  1.78it/s]Extractor Estimating: 48it [00:30,  1.78it/s]Extractor Estimating: 49it [00:31,  1.79it/s]Extractor Estimating: 50it [00:31,  1.68it/s]Extractor Estimating: 51it [00:32,  1.64it/s]Extractor Estimating: 52it [00:33,  1.63it/s]Extractor Estimating: 53it [00:33,  1.59it/s]Extractor Estimating: 54it [00:34,  1.63it/s]Extractor Estimating: 55it [00:34,  1.60it/s]Extractor Estimating: 56it [00:35,  1.63it/s]Extractor Estimating: 57it [00:36,  1.64it/s]Extractor Estimating: 58it [00:36,  1.61it/s]Extractor Estimating: 59it [00:37,  1.58it/s]Extractor Estimating: 60it [00:38,  1.48it/s]Extractor Estimating: 61it [00:38,  1.57it/s]Extractor Estimating: 62it [00:39,  1.59it/s]Extractor Estimating: 63it [00:40,  1.52it/s]Extractor Estimating: 64it [00:40,  1.59it/s]Extractor Estimating: 65it [00:41,  1.60it/s]Extractor Estimating: 66it [00:41,  1.64it/s]Extractor Estimating: 67it [00:42,  1.63it/s]Extractor Estimating: 68it [00:43,  1.56it/s]Extractor Estimating: 69it [00:43,  1.59it/s]Extractor Estimating: 70it [00:44,  1.49it/s]Extractor Estimating: 71it [00:45,  1.55it/s]Extractor Estimating: 72it [00:45,  1.65it/s]Extractor Estimating: 73it [00:46,  1.54it/s]Extractor Estimating: 74it [00:47,  1.50it/s]Extractor Estimating: 75it [00:47,  1.50it/s]Extractor Estimating: 76it [00:48,  1.32it/s]Extractor Estimating: 77it [00:49,  1.36it/s]Extractor Estimating: 78it [00:50,  1.37it/s]Extractor Estimating: 79it [00:50,  1.44it/s]Extractor Estimating: 80it [00:51,  1.50it/s]Extractor Estimating: 81it [00:52,  1.48it/s]Extractor Estimating: 82it [00:52,  1.42it/s]Extractor Estimating: 83it [00:53,  1.44it/s]Extractor Estimating: 84it [00:54,  1.50it/s]Extractor Estimating: 85it [00:54,  1.48it/s]Extractor Estimating: 86it [00:55,  1.55it/s]Extractor Estimating: 87it [00:56,  1.49it/s]Extractor Estimating: 88it [00:56,  1.51it/s]Extractor Estimating: 89it [00:57,  1.54it/s]Extractor Estimating: 90it [00:57,  1.57it/s]Extractor Estimating: 91it [00:58,  1.58it/s]Extractor Estimating: 92it [00:59,  1.47it/s]Extractor Estimating: 93it [00:59,  1.55it/s]Extractor Estimating: 94it [01:00,  1.55it/s]Extractor Estimating: 95it [01:01,  1.60it/s]Extractor Estimating: 96it [01:01,  1.58it/s]Extractor Estimating: 97it [01:02,  1.55it/s]Extractor Estimating: 98it [01:03,  1.45it/s]Extractor Estimating: 99it [01:03,  1.50it/s]Extractor Estimating: 100it [01:04,  1.44it/s]Extractor Estimating: 101it [01:05,  1.52it/s]Extractor Estimating: 102it [01:05,  1.46it/s]Extractor Estimating: 103it [01:06,  1.49it/s]Extractor Estimating: 104it [01:07,  1.56it/s]Extractor Estimating: 105it [01:07,  1.59it/s]Extractor Estimating: 106it [01:08,  1.63it/s]Extractor Estimating: 107it [01:09,  1.45it/s]Extractor Estimating: 108it [01:09,  1.47it/s]Extractor Estimating: 109it [01:10,  1.48it/s]Extractor Estimating: 110it [01:11,  1.50it/s]Extractor Estimating: 111it [01:11,  1.53it/s]Extractor Estimating: 112it [01:12,  1.49it/s]Extractor Estimating: 113it [01:13,  1.48it/s]Extractor Estimating: 114it [01:14,  1.40it/s]Extractor Estimating: 115it [01:14,  1.48it/s]Extractor Estimating: 116it [01:15,  1.54it/s]Extractor Estimating: 117it [01:15,  1.57it/s]Extractor Estimating: 118it [01:16,  1.57it/s]Extractor Estimating: 119it [01:17,  1.46it/s]Extractor Estimating: 120it [01:17,  1.49it/s]Extractor Estimating: 121it [01:18,  1.51it/s]Extractor Estimating: 122it [01:19,  1.51it/s]Extractor Estimating: 123it [01:19,  1.54it/s]Extractor Estimating: 124it [01:20,  1.50it/s]Extractor Estimating: 125it [01:21,  1.55it/s]Extractor Estimating: 126it [01:21,  1.54it/s]Extractor Estimating: 127it [01:22,  1.55it/s]Extractor Estimating: 128it [01:23,  1.58it/s]Extractor Estimating: 129it [01:23,  1.52it/s]Extractor Estimating: 130it [01:24,  1.56it/s]Extractor Estimating: 131it [01:24,  1.56it/s]Extractor Estimating: 132it [01:25,  1.55it/s]Extractor Estimating: 133it [01:26,  1.62it/s]Extractor Estimating: 134it [01:26,  1.59it/s]Extractor Estimating: 135it [01:27,  1.56it/s]Extractor Estimating: 136it [01:28,  1.56it/s]Extractor Estimating: 137it [01:28,  1.58it/s]Extractor Estimating: 138it [01:29,  1.60it/s]Extractor Estimating: 139it [01:30,  1.54it/s]Extractor Estimating: 140it [01:30,  1.54it/s]Extractor Estimating: 141it [01:31,  1.57it/s]Extractor Estimating: 142it [01:31,  1.62it/s]Extractor Estimating: 143it [01:32,  1.60it/s]Extractor Estimating: 144it [01:33,  1.52it/s]Extractor Estimating: 145it [01:33,  1.57it/s]Extractor Estimating: 146it [01:34,  1.56it/s]Extractor Estimating: 147it [01:35,  1.46it/s]Extractor Estimating: 148it [01:35,  1.51it/s]Extractor Estimating: 149it [01:36,  1.41it/s]Extractor Estimating: 150it [01:37,  1.51it/s]Extractor Estimating: 151it [01:37,  1.58it/s]Extractor Estimating: 152it [01:38,  1.68it/s]Extractor Estimating: 153it [01:38,  1.76it/s]Extractor Estimating: 154it [01:39,  1.89it/s]Extractor Estimating: 155it [01:39,  1.80it/s]Extractor Estimating: 156it [01:40,  1.88it/s]Extractor Estimating: 157it [01:40,  1.92it/s]Extractor Estimating: 158it [01:41,  2.00it/s]Extractor Estimating: 159it [01:41,  2.02it/s]Extractor Estimating: 160it [01:42,  2.04it/s]Extractor Estimating: 161it [01:42,  2.06it/s]Extractor Estimating: 162it [01:43,  2.08it/s]Extractor Estimating: 163it [01:43,  2.16it/s]Extractor Estimating: 164it [01:44,  2.11it/s]Extractor Estimating: 165it [01:44,  1.82it/s]Extractor Estimating: 166it [01:45,  1.89it/s]Extractor Estimating: 167it [01:45,  1.93it/s]Extractor Estimating: 168it [01:46,  2.00it/s]Extractor Estimating: 169it [01:46,  2.00it/s]Extractor Estimating: 170it [01:47,  2.03it/s]Extractor Estimating: 171it [01:47,  1.84it/s]Extractor Estimating: 172it [01:48,  1.84it/s]Extractor Estimating: 173it [01:48,  1.91it/s]Extractor Estimating: 174it [01:49,  1.91it/s]Extractor Estimating: 175it [01:49,  1.97it/s]Extractor Estimating: 176it [01:50,  1.88it/s]Extractor Estimating: 177it [01:51,  1.58it/s]Extractor Estimating: 178it [01:51,  1.65it/s]Extractor Estimating: 179it [01:52,  1.67it/s]Extractor Estimating: 180it [01:53,  1.71it/s]Extractor Estimating: 181it [01:53,  1.69it/s]Extractor Estimating: 182it [01:54,  1.56it/s]Extractor Estimating: 183it [01:55,  1.62it/s]Extractor Estimating: 184it [01:55,  1.61it/s]Extractor Estimating: 185it [01:56,  1.64it/s]Extractor Estimating: 186it [01:56,  1.65it/s]Extractor Estimating: 187it [01:57,  1.44it/s]Extractor Estimating: 188it [01:58,  1.52it/s]Extractor Estimating: 189it [01:58,  1.57it/s]Extractor Estimating: 190it [01:59,  1.59it/s]Extractor Estimating: 191it [02:00,  1.57it/s]Extractor Estimating: 192it [02:00,  1.48it/s]Extractor Estimating: 193it [02:01,  1.50it/s]Extractor Estimating: 194it [02:02,  1.52it/s]Extractor Estimating: 195it [02:02,  1.58it/s]Extractor Estimating: 196it [02:03,  1.56it/s]Extractor Estimating: 197it [02:04,  1.46it/s]Extractor Estimating: 198it [02:05,  1.38it/s]Extractor Estimating: 199it [02:05,  1.50it/s]Extractor Estimating: 200it [02:06,  1.52it/s]Extractor Estimating: 201it [02:06,  1.53it/s]Extractor Estimating: 202it [02:07,  1.49it/s]Extractor Estimating: 203it [02:08,  1.55it/s]Extractor Estimating: 204it [02:08,  1.57it/s]Extractor Estimating: 205it [02:09,  1.53it/s]Extractor Estimating: 206it [02:10,  1.53it/s]Extractor Estimating: 207it [02:10,  1.46it/s]Extractor Estimating: 208it [02:11,  1.52it/s]Extractor Estimating: 209it [02:12,  1.55it/s]Extractor Estimating: 210it [02:12,  1.43it/s]Extractor Estimating: 211it [02:13,  1.47it/s]Extractor Estimating: 212it [02:14,  1.53it/s]Extractor Estimating: 213it [02:14,  1.55it/s]Extractor Estimating: 214it [02:15,  1.54it/s]Extractor Estimating: 215it [02:16,  1.51it/s]Extractor Estimating: 216it [02:16,  1.51it/s]Extractor Estimating: 217it [02:17,  1.58it/s]Extractor Estimating: 218it [02:17,  1.62it/s]Extractor Estimating: 219it [02:18,  1.61it/s]Extractor Estimating: 220it [02:19,  1.53it/s]Extractor Estimating: 221it [02:19,  1.57it/s]Extractor Estimating: 222it [02:20,  1.54it/s]Extractor Estimating: 223it [02:21,  1.59it/s]Extractor Estimating: 224it [02:21,  1.59it/s]Extractor Estimating: 225it [02:22,  1.51it/s]Extractor Estimating: 226it [02:23,  1.47it/s]Extractor Estimating: 227it [02:23,  1.52it/s]Extractor Estimating: 228it [02:24,  1.57it/s]Extractor Estimating: 229it [02:24,  1.65it/s]Extractor Estimating: 230it [02:25,  1.59it/s]Extractor Estimating: 231it [02:26,  1.59it/s]Extractor Estimating: 232it [02:26,  1.60it/s]Extractor Estimating: 233it [02:27,  1.68it/s]Extractor Estimating: 234it [02:27,  1.69it/s]Extractor Estimating: 235it [02:28,  1.62it/s]Extractor Estimating: 236it [02:29,  1.59it/s]Extractor Estimating: 237it [02:29,  1.62it/s]Extractor Estimating: 238it [02:30,  1.65it/s]Extractor Estimating: 239it [02:31,  1.66it/s]Extractor Estimating: 240it [02:31,  1.58it/s]Extractor Estimating: 241it [02:32,  1.60it/s]Extractor Estimating: 242it [02:32,  1.64it/s]Extractor Estimating: 243it [02:33,  1.72it/s]Extractor Estimating: 244it [02:34,  1.65it/s]Extractor Estimating: 245it [02:34,  1.56it/s]Extractor Estimating: 246it [02:35,  1.61it/s]Extractor Estimating: 247it [02:36,  1.58it/s]Extractor Estimating: 248it [02:36,  1.63it/s]Extractor Estimating: 249it [02:37,  1.61it/s]Extractor Estimating: 250it [02:38,  1.50it/s]Extractor Estimating: 250it [02:38,  1.58it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,083 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:45,190 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 12:50:46,982 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 12:50:46,983 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:50:47,779 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 12:50:49,049 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:50:49,156 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:52,825 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:52,956 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:52,956 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:52,956 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 12:50:52,956 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 12:50:54,347 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 12:50:54,348 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 12:50:55,122 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 12:50:55,417 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 12:50:55,417 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 14:15:24,930 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 14:15:26,457 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/2_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/2.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.6, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 3000, 'num_train': 2000}
num of filtered data: 4997 mean pseudo reward: 0.9461005022506831
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/2.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 21468
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 21568, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter2/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=21568, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.972, loss:687.1023
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.975, loss:656.8507
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.965, loss:633.3857
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.977, loss:616.0697
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.972, loss:613.1533
>> valid entity prec:0.6013, rec:0.6111, f1:0.6062
>> valid relation prec:0.4564, rec:0.1273, f1:0.1990
>> valid relation with NER prec:0.4564, rec:0.1273, f1:0.1990
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.296, loss:601.9938
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 0.961, loss:592.9693
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.976, loss:629.9476
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.971, loss:612.9368
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.966, loss:617.3209
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.6305, rec:0.5755, f1:0.6018
>> valid relation prec:0.5071, rec:0.1127, f1:0.1844
>> valid relation with NER prec:0.5071, rec:0.1127, f1:0.1844
g_step 1100, step 55, avg_time 2.244, loss:615.6793
g_step 1200, step 155, avg_time 0.979, loss:586.0066
g_step 1300, step 46, avg_time 0.966, loss:606.7849
g_step 1400, step 146, avg_time 0.967, loss:573.3119
g_step 1500, step 37, avg_time 0.976, loss:575.6185
>> valid entity prec:0.6252, rec:0.4845, f1:0.5459
>> valid relation prec:0.3766, rec:0.0821, f1:0.1348
>> valid relation with NER prec:0.3766, rec:0.0821, f1:0.1348
g_step 1600, step 137, avg_time 2.245, loss:546.9883
g_step 1700, step 28, avg_time 0.986, loss:544.1009
g_step 1800, step 128, avg_time 0.963, loss:514.1110
g_step 1900, step 19, avg_time 0.963, loss:504.6235
g_step 2000, step 119, avg_time 0.978, loss:505.2981
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.6384, rec:0.5372, f1:0.5834
>> valid relation prec:0.3803, rec:0.1072, f1:0.1673
>> valid relation with NER prec:0.3803, rec:0.1072, f1:0.1673
g_step 2100, step 10, avg_time 2.227, loss:496.7817
g_step 2200, step 110, avg_time 0.975, loss:474.2284
g_step 2300, step 1, avg_time 0.952, loss:480.8915
g_step 2400, step 101, avg_time 0.965, loss:447.7089
g_step 2500, step 201, avg_time 0.966, loss:457.5263
>> valid entity prec:0.6328, rec:0.5452, f1:0.5858
>> valid relation prec:0.3681, rec:0.1141, f1:0.1742
>> valid relation with NER prec:0.3681, rec:0.1141, f1:0.1742
g_step 2600, step 92, avg_time 2.231, loss:416.6598
g_step 2700, step 192, avg_time 0.980, loss:452.5913
g_step 2800, step 83, avg_time 0.962, loss:412.2226
g_step 2900, step 183, avg_time 0.964, loss:433.6028
g_step 3000, step 74, avg_time 0.950, loss:401.3884
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6085, rec:0.5409, f1:0.5727
>> valid relation prec:0.2978, rec:0.0935, f1:0.1423
>> valid relation with NER prec:0.2978, rec:0.0935, f1:0.1423
g_step 3100, step 174, avg_time 2.216, loss:411.5823
g_step 3200, step 65, avg_time 0.949, loss:389.7295
g_step 3300, step 165, avg_time 0.967, loss:381.4972
g_step 3400, step 56, avg_time 0.950, loss:368.5384
g_step 3500, step 156, avg_time 0.963, loss:378.8720
>> valid entity prec:0.6457, rec:0.5458, f1:0.5915
>> valid relation prec:0.3454, rec:0.1147, f1:0.1722
>> valid relation with NER prec:0.3454, rec:0.1147, f1:0.1722
g_step 3600, step 47, avg_time 2.210, loss:365.4007
g_step 3700, step 147, avg_time 0.947, loss:363.2166
g_step 3800, step 38, avg_time 0.947, loss:346.1791
g_step 3900, step 138, avg_time 0.949, loss:351.6312
g_step 4000, step 29, avg_time 0.950, loss:334.2713
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.6025, rec:0.5118, f1:0.5535
>> valid relation prec:0.2495, rec:0.0695, f1:0.1087
>> valid relation with NER prec:0.2495, rec:0.0695, f1:0.1087
g_step 4100, step 129, avg_time 2.213, loss:320.9885
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 14:15:26 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 14:15:26 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_14-15-24_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 14:15:27 - WARNING - datasets.builder -   Using custom data configuration default-ea5880a30b795f5e
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-ea5880a30b795f5e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 14:15:37,719 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:15:37,830 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:15:37,831 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:15:37,832 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:15:38,420 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:15:38,640 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:15:38,640 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:15:38,640 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:15:38,640 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:15:38,640 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:15:38,640 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 14:15:39,989 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:15:43,319 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 14:15:43,454 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-ea5880a30b795f5e/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:03,  1.21ba/s] 40%|████      | 2/5 [00:01<00:01,  2.19ba/s] 60%|██████    | 3/5 [00:01<00:00,  2.93ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.46ba/s]100%|██████████| 5/5 [00:01<00:00,  3.86ba/s]100%|██████████| 5/5 [00:01<00:00,  3.04ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.36ba/s] 50%|█████     | 2/4 [00:01<00:01,  1.91ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.60ba/s]100%|██████████| 4/4 [00:01<00:00,  3.60ba/s]100%|██████████| 4/4 [00:01<00:00,  2.77ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:01,  2.60ba/s] 60%|██████    | 3/5 [00:00<00:00,  5.98ba/s]100%|██████████| 5/5 [00:00<00:00,  7.89ba/s]100%|██████████| 5/5 [00:00<00:00,  6.72ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  2.37ba/s] 50%|█████     | 2/4 [00:00<00:00,  4.17ba/s]100%|██████████| 4/4 [00:00<00:00,  5.90ba/s]100%|██████████| 4/4 [00:00<00:00,  5.06ba/s]
[INFO|trainer.py:414] 2023-08-28 14:15:53,305 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 14:15:53,742 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 14:15:53,742 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 14:15:53,742 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 14:15:53,742 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 14:15:53,742 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 14:15:53,743 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 14:15:53,743 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:00<01:57,  3.30it/s]  1%|          | 2/390 [00:00<01:54,  3.39it/s]  1%|          | 3/390 [00:01<02:22,  2.72it/s]  1%|          | 4/390 [00:01<02:09,  2.98it/s]  1%|▏         | 5/390 [00:01<02:02,  3.15it/s]  2%|▏         | 6/390 [00:01<01:57,  3.27it/s]  2%|▏         | 7/390 [00:02<01:54,  3.34it/s]  2%|▏         | 8/390 [00:02<01:52,  3.40it/s]  2%|▏         | 9/390 [00:02<01:50,  3.43it/s]  3%|▎         | 10/390 [00:03<01:49,  3.45it/s]  3%|▎         | 11/390 [00:03<01:49,  3.47it/s]  3%|▎         | 12/390 [00:03<01:48,  3.48it/s]  3%|▎         | 13/390 [00:04<02:07,  2.97it/s]  4%|▎         | 14/390 [00:04<02:00,  3.11it/s]  4%|▍         | 15/390 [00:04<01:56,  3.22it/s]  4%|▍         | 16/390 [00:04<01:53,  3.30it/s]  4%|▍         | 17/390 [00:05<01:51,  3.34it/s]  5%|▍         | 18/390 [00:05<01:50,  3.38it/s]  5%|▍         | 19/390 [00:05<01:49,  3.40it/s]  5%|▌         | 20/390 [00:06<01:48,  3.42it/s]  5%|▌         | 21/390 [00:06<01:47,  3.43it/s]  6%|▌         | 22/390 [00:06<01:47,  3.44it/s]  6%|▌         | 23/390 [00:06<01:46,  3.45it/s]  6%|▌         | 24/390 [00:07<01:45,  3.46it/s]  6%|▋         | 25/390 [00:07<01:53,  3.21it/s]  7%|▋         | 26/390 [00:07<01:50,  3.29it/s]  7%|▋         | 27/390 [00:08<01:48,  3.35it/s]  7%|▋         | 28/390 [00:08<01:46,  3.40it/s]  7%|▋         | 29/390 [00:08<01:45,  3.43it/s]  8%|▊         | 30/390 [00:09<01:44,  3.46it/s]  8%|▊         | 31/390 [00:09<01:43,  3.47it/s]  8%|▊         | 32/390 [00:09<01:42,  3.48it/s]  8%|▊         | 33/390 [00:09<01:42,  3.49it/s]  9%|▊         | 34/390 [00:10<01:41,  3.50it/s]  9%|▉         | 35/390 [00:10<01:41,  3.50it/s]  9%|▉         | 36/390 [00:10<01:57,  3.01it/s]  9%|▉         | 37/390 [00:11<01:52,  3.14it/s] 10%|▉         | 38/390 [00:11<01:48,  3.24it/s] 10%|█         | 39/390 [00:11<01:45,  3.32it/s] 10%|█         | 40/390 [00:12<01:43,  3.37it/s] 11%|█         | 41/390 [00:12<01:42,  3.41it/s] 11%|█         | 42/390 [00:12<01:41,  3.44it/s] 11%|█         | 43/390 [00:12<01:40,  3.46it/s] 11%|█▏        | 44/390 [00:13<01:39,  3.47it/s] 12%|█▏        | 45/390 [00:13<01:39,  3.48it/s] 12%|█▏        | 46/390 [00:13<01:52,  3.07it/s] 12%|█▏        | 47/390 [00:14<01:47,  3.19it/s] 12%|█▏        | 48/390 [00:14<01:44,  3.27it/s] 13%|█▎        | 49/390 [00:14<01:42,  3.34it/s] 13%|█▎        | 50/390 [00:15<01:40,  3.39it/s] 13%|█▎        | 51/390 [00:15<01:39,  3.42it/s] 13%|█▎        | 52/390 [00:15<01:38,  3.44it/s] 14%|█▎        | 53/390 [00:15<01:37,  3.46it/s] 14%|█▍        | 54/390 [00:16<01:36,  3.47it/s] 14%|█▍        | 55/390 [00:16<01:36,  3.48it/s] 14%|█▍        | 56/390 [00:16<01:35,  3.49it/s] 15%|█▍        | 57/390 [00:17<01:46,  3.12it/s] 15%|█▍        | 58/390 [00:17<01:42,  3.23it/s] 15%|█▌        | 59/390 [00:17<01:40,  3.31it/s] 15%|█▌        | 60/390 [00:17<01:38,  3.36it/s] 16%|█▌        | 61/390 [00:18<01:36,  3.40it/s] 16%|█▌        | 62/390 [00:18<01:35,  3.43it/s] 16%|█▌        | 63/390 [00:18<01:34,  3.45it/s] 16%|█▋        | 64/390 [00:19<01:34,  3.46it/s] 17%|█▋        | 65/390 [00:19<01:33,  3.47it/s] 17%|█▋        | 66/390 [00:19<01:33,  3.48it/s] 17%|█▋        | 67/390 [00:19<01:32,  3.49it/s] 17%|█▋        | 68/390 [00:20<01:40,  3.20it/s] 18%|█▊        | 69/390 [00:20<01:37,  3.28it/s] 18%|█▊        | 70/390 [00:20<01:35,  3.35it/s] 18%|█▊        | 71/390 [00:21<01:34,  3.39it/s] 18%|█▊        | 72/390 [00:21<01:32,  3.42it/s] 19%|█▊        | 73/390 [00:21<01:32,  3.45it/s] 19%|█▉        | 74/390 [00:22<01:31,  3.46it/s] 19%|█▉        | 75/390 [00:22<01:30,  3.47it/s] 19%|█▉        | 76/390 [00:22<01:30,  3.48it/s] 20%|█▉        | 77/390 [00:22<01:29,  3.48it/s] 20%|██        | 78/390 [00:23<01:29,  3.49it/s][INFO|trainer.py:2140] 2023-08-28 14:16:17,085 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:16:17,085 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:16:17,085 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.63it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.74it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.91it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.17it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.74it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.44it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.12it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.38it/s][A
 11%|█         | 48/438 [00:01<00:08, 44.99it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 44.88it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 44.99it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.18it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.47it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.55it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.60it/s][A
 19%|█▉        | 83/438 [00:01<00:11, 30.54it/s][A
 20%|██        | 88/438 [00:02<00:10, 34.00it/s][A
 21%|██        | 93/438 [00:02<00:09, 36.89it/s][A
 22%|██▏       | 98/438 [00:02<00:08, 39.21it/s][A
 24%|██▎       | 103/438 [00:02<00:08, 41.02it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 42.35it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 43.38it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.04it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.00it/s][A
 29%|██▉       | 128/438 [00:03<00:09, 34.41it/s][A
 30%|███       | 133/438 [00:03<00:08, 37.15it/s][A
 32%|███▏      | 138/438 [00:03<00:07, 39.23it/s][A
 33%|███▎      | 143/438 [00:03<00:07, 40.97it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 42.23it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 43.23it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 43.90it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.44it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.30it/s][A
 39%|███▉      | 173/438 [00:04<00:05, 44.35it/s][A
 41%|████      | 178/438 [00:04<00:05, 44.68it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 44.87it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.05it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.11it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.29it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.32it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.33it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.01it/s][A
 50%|████▉     | 218/438 [00:05<00:04, 44.95it/s][A
 51%|█████     | 223/438 [00:05<00:04, 44.90it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 45.13it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.22it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 45.25it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 45.33it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 45.43it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 45.27it/s][A
 59%|█████▉    | 258/438 [00:06<00:03, 45.09it/s][A
 60%|██████    | 263/438 [00:06<00:05, 34.12it/s][A
 61%|██████    | 268/438 [00:06<00:04, 36.97it/s][A
 62%|██████▏   | 273/438 [00:06<00:04, 39.27it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 41.01it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 42.30it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 43.29it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 43.95it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.41it/s][A
 69%|██████▉   | 303/438 [00:07<00:03, 44.24it/s][A
 70%|███████   | 308/438 [00:07<00:02, 44.13it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 44.45it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 44.71it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.13it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.24it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.32it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 45.40it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 45.14it/s][A
 79%|███████▉  | 348/438 [00:08<00:02, 36.45it/s][A
 80%|████████  | 352/438 [00:09<00:08, 10.19it/s][A
 81%|████████  | 355/438 [00:09<00:07, 11.39it/s][A
 82%|████████▏ | 360/438 [00:09<00:05, 15.28it/s][A
 83%|████████▎ | 365/438 [00:09<00:03, 19.53it/s][A
 84%|████████▍ | 370/438 [00:09<00:02, 23.81it/s][A
 86%|████████▌ | 375/438 [00:09<00:02, 28.04it/s][A
 87%|████████▋ | 380/438 [00:10<00:01, 31.82it/s][A
 88%|████████▊ | 385/438 [00:10<00:01, 35.03it/s][A
 89%|████████▉ | 390/438 [00:10<00:01, 37.68it/s][A
 90%|█████████ | 395/438 [00:10<00:01, 39.46it/s][A
 91%|█████████▏| 400/438 [00:10<00:00, 40.73it/s][A
 92%|█████████▏| 405/438 [00:10<00:00, 41.92it/s][A
 94%|█████████▎| 410/438 [00:10<00:00, 42.99it/s][A
 95%|█████████▍| 415/438 [00:10<00:00, 43.70it/s][A
 96%|█████████▌| 420/438 [00:10<00:00, 44.35it/s][A
 97%|█████████▋| 425/438 [00:11<00:00, 44.77it/s][A
 98%|█████████▊| 430/438 [00:11<00:00, 44.98it/s][A
 99%|█████████▉| 435/438 [00:11<00:00, 45.01it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:11<00:00, 45.01it/s][A 20%|██        | 78/390 [00:34<01:29,  3.49it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:16:29,784 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 14:16:30,523 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:17:01,571 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:17:02,404 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:17:02,734 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [02:52<3:53:28, 45.04s/it] 21%|██        | 80/390 [02:53<2:43:31, 31.65s/it] 21%|██        | 81/390 [02:53<1:54:32, 22.24s/it] 21%|██        | 82/390 [02:53<1:20:21, 15.66s/it] 21%|██▏       | 83/390 [02:53<56:31, 11.05s/it]   22%|██▏       | 84/390 [02:54<39:52,  7.82s/it] 22%|██▏       | 85/390 [02:54<28:15,  5.56s/it] 22%|██▏       | 86/390 [02:54<20:08,  3.98s/it] 22%|██▏       | 87/390 [02:55<14:29,  2.87s/it] 23%|██▎       | 88/390 [02:55<10:32,  2.09s/it] 23%|██▎       | 89/390 [02:55<07:47,  1.55s/it] 23%|██▎       | 90/390 [02:55<05:51,  1.17s/it] 23%|██▎       | 91/390 [02:56<04:37,  1.08it/s] 24%|██▎       | 92/390 [02:56<03:39,  1.36it/s] 24%|██▍       | 93/390 [02:56<02:58,  1.66it/s] 24%|██▍       | 94/390 [02:57<02:30,  1.97it/s] 24%|██▍       | 95/390 [02:57<02:09,  2.27it/s] 25%|██▍       | 96/390 [02:57<01:55,  2.53it/s] 25%|██▍       | 97/390 [02:58<01:46,  2.76it/s] 25%|██▌       | 98/390 [02:58<01:38,  2.95it/s] 25%|██▌       | 99/390 [02:58<01:34,  3.09it/s] 26%|██▌       | 100/390 [02:58<01:30,  3.20it/s] 26%|██▌       | 101/390 [02:59<01:28,  3.28it/s] 26%|██▌       | 102/390 [02:59<01:33,  3.09it/s] 26%|██▋       | 103/390 [02:59<01:29,  3.20it/s] 27%|██▋       | 104/390 [03:00<01:26,  3.29it/s] 27%|██▋       | 105/390 [03:00<01:24,  3.35it/s] 27%|██▋       | 106/390 [03:00<01:23,  3.40it/s] 27%|██▋       | 107/390 [03:00<01:22,  3.43it/s] 28%|██▊       | 108/390 [03:01<01:21,  3.45it/s] 28%|██▊       | 109/390 [03:01<01:20,  3.47it/s] 28%|██▊       | 110/390 [03:01<01:20,  3.48it/s] 28%|██▊       | 111/390 [03:02<01:19,  3.49it/s] 29%|██▊       | 112/390 [03:02<01:19,  3.49it/s] 29%|██▉       | 113/390 [03:02<01:30,  3.05it/s] 29%|██▉       | 114/390 [03:03<01:26,  3.17it/s] 29%|██▉       | 115/390 [03:03<01:24,  3.27it/s] 30%|██▉       | 116/390 [03:03<01:22,  3.34it/s] 30%|███       | 117/390 [03:03<01:20,  3.39it/s] 30%|███       | 118/390 [03:04<01:19,  3.42it/s] 31%|███       | 119/390 [03:04<01:18,  3.45it/s] 31%|███       | 120/390 [03:04<01:17,  3.47it/s] 31%|███       | 121/390 [03:05<01:17,  3.48it/s] 31%|███▏      | 122/390 [03:05<01:16,  3.49it/s] 32%|███▏      | 123/390 [03:05<01:16,  3.50it/s] 32%|███▏      | 124/390 [03:06<01:26,  3.08it/s] 32%|███▏      | 125/390 [03:06<01:22,  3.20it/s] 32%|███▏      | 126/390 [03:06<01:20,  3.29it/s] 33%|███▎      | 127/390 [03:06<01:18,  3.35it/s] 33%|███▎      | 128/390 [03:07<01:17,  3.40it/s] 33%|███▎      | 129/390 [03:07<01:16,  3.43it/s] 33%|███▎      | 130/390 [03:07<01:15,  3.46it/s] 34%|███▎      | 131/390 [03:08<01:14,  3.47it/s] 34%|███▍      | 132/390 [03:08<01:14,  3.48it/s] 34%|███▍      | 133/390 [03:08<01:13,  3.49it/s] 34%|███▍      | 134/390 [03:08<01:13,  3.49it/s] 35%|███▍      | 135/390 [03:09<01:24,  3.01it/s] 35%|███▍      | 136/390 [03:09<01:20,  3.14it/s] 35%|███▌      | 137/390 [03:09<01:17,  3.25it/s] 35%|███▌      | 138/390 [03:10<01:15,  3.32it/s] 36%|███▌      | 139/390 [03:10<01:14,  3.38it/s] 36%|███▌      | 140/390 [03:10<01:13,  3.42it/s] 36%|███▌      | 141/390 [03:11<01:12,  3.45it/s] 36%|███▋      | 142/390 [03:11<01:11,  3.46it/s] 37%|███▋      | 143/390 [03:11<01:11,  3.48it/s] 37%|███▋      | 144/390 [03:11<01:10,  3.49it/s] 37%|███▋      | 145/390 [03:12<01:25,  2.88it/s] 37%|███▋      | 146/390 [03:12<01:20,  3.04it/s] 38%|███▊      | 147/390 [03:13<01:16,  3.17it/s] 38%|███▊      | 148/390 [03:13<01:14,  3.27it/s] 38%|███▊      | 149/390 [03:13<01:12,  3.34it/s] 38%|███▊      | 150/390 [03:13<01:10,  3.39it/s] 39%|███▊      | 151/390 [03:14<01:09,  3.42it/s] 39%|███▉      | 152/390 [03:14<01:09,  3.45it/s] 39%|███▉      | 153/390 [03:14<01:08,  3.46it/s] 39%|███▉      | 154/390 [03:14<01:07,  3.48it/s] 40%|███▉      | 155/390 [03:15<01:18,  2.98it/s] 40%|████      | 156/390 [03:15<01:14,  3.12it/s][INFO|trainer.py:2140] 2023-08-28 14:19:09,507 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:19:09,507 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:19:09,507 >>   Batch size = 8
{'eval_loss': 1.0286164283752441, 'eval_runtime': 11.3558, 'eval_samples_per_second': 307.948, 'eval_steps_per_second': 38.571, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.85it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.83it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.30it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.33it/s][A
  6%|▋         | 28/438 [00:00<00:08, 45.84it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.66it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.49it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.40it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.38it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.55it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.60it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.45it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.34it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.26it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.19it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.17it/s][A
 20%|██        | 88/438 [00:01<00:07, 45.10it/s][A
 21%|██        | 93/438 [00:02<00:07, 45.23it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.38it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.50it/s][A
 25%|██▍       | 108/438 [00:02<00:10, 31.20it/s][A
 26%|██▌       | 113/438 [00:02<00:09, 34.51it/s][A
 27%|██▋       | 118/438 [00:02<00:08, 37.28it/s][A
 28%|██▊       | 123/438 [00:02<00:08, 39.32it/s][A
 29%|██▉       | 128/438 [00:02<00:07, 41.20it/s][A
 30%|███       | 133/438 [00:03<00:07, 42.45it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 43.43it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.00it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.03it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 43.98it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.29it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.54it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.92it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 45.10it/s][A
 41%|████      | 178/438 [00:04<00:05, 45.36it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.50it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.45it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.14it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 44.89it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 44.88it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 44.93it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.13it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.29it/s][A
 51%|█████     | 223/438 [00:05<00:04, 45.49it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 45.56it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.45it/s][A
 54%|█████▍    | 238/438 [00:05<00:06, 28.87it/s][A
 55%|█████▌    | 243/438 [00:05<00:06, 32.48it/s][A
 57%|█████▋    | 248/438 [00:05<00:05, 35.62it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 38.15it/s][A
 59%|█████▉    | 258/438 [00:06<00:04, 40.15it/s][A
 60%|██████    | 263/438 [00:06<00:04, 41.73it/s][A
 61%|██████    | 268/438 [00:06<00:03, 42.83it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 43.54it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 43.65it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 43.82it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 44.00it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.48it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.93it/s][A
 69%|██████▉   | 303/438 [00:07<00:02, 45.14it/s][A
 70%|███████   | 308/438 [00:07<00:02, 45.35it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 45.50it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.35it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.03it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 44.77it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 44.82it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 44.91it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 45.08it/s][A
 79%|███████▉  | 348/438 [00:08<00:01, 45.23it/s][A
 81%|████████  | 353/438 [00:08<00:02, 29.42it/s][A
 82%|████████▏ | 358/438 [00:08<00:02, 32.95it/s][A
 83%|████████▎ | 363/438 [00:08<00:02, 35.97it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 38.46it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 40.41it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 41.90it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 42.97it/s][A
 89%|████████▊ | 388/438 [00:09<00:01, 43.68it/s][A
 90%|████████▉ | 393/438 [00:09<00:01, 43.77it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 43.82it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.07it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 44.41it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.75it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 45.04it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 45.32it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 45.45it/s][A
 99%|█████████▉| 433/438 [00:10<00:00, 45.36it/s][A
100%|██████████| 438/438 [00:10<00:00, 45.05it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 45.05it/s][A 40%|████      | 156/390 [03:26<01:14,  3.12it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:19:19,879 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 14:19:20,974 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:19:40,947 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:19:41,364 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:19:41,588 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [05:03<2:06:27, 32.56s/it] 41%|████      | 158/390 [05:03<1:28:35, 22.91s/it] 41%|████      | 159/390 [05:04<1:02:04, 16.13s/it] 41%|████      | 160/390 [05:04<43:36, 11.37s/it]   41%|████▏     | 161/390 [05:04<30:43,  8.05s/it] 42%|████▏     | 162/390 [05:05<21:44,  5.72s/it] 42%|████▏     | 163/390 [05:05<15:29,  4.09s/it] 42%|████▏     | 164/390 [05:05<11:07,  2.95s/it] 42%|████▏     | 165/390 [05:05<08:04,  2.15s/it] 43%|████▎     | 166/390 [05:06<05:57,  1.60s/it] 43%|████▎     | 167/390 [05:06<04:28,  1.20s/it] 43%|████▎     | 168/390 [05:06<03:32,  1.05it/s] 43%|████▎     | 169/390 [05:07<02:47,  1.32it/s] 44%|████▎     | 170/390 [05:07<02:15,  1.62it/s] 44%|████▍     | 171/390 [05:07<01:53,  1.93it/s] 44%|████▍     | 172/390 [05:08<01:38,  2.22it/s] 44%|████▍     | 173/390 [05:08<01:27,  2.49it/s] 45%|████▍     | 174/390 [05:08<01:19,  2.72it/s] 45%|████▍     | 175/390 [05:08<01:14,  2.90it/s] 45%|████▌     | 176/390 [05:09<01:10,  3.05it/s] 45%|████▌     | 177/390 [05:09<01:07,  3.16it/s] 46%|████▌     | 178/390 [05:09<01:05,  3.26it/s] 46%|████▌     | 179/390 [05:10<01:08,  3.07it/s] 46%|████▌     | 180/390 [05:10<01:05,  3.19it/s] 46%|████▋     | 181/390 [05:10<01:03,  3.27it/s] 47%|████▋     | 182/390 [05:11<01:02,  3.34it/s] 47%|████▋     | 183/390 [05:11<01:01,  3.39it/s] 47%|████▋     | 184/390 [05:11<01:00,  3.42it/s] 47%|████▋     | 185/390 [05:11<00:59,  3.44it/s] 48%|████▊     | 186/390 [05:12<00:59,  3.46it/s] 48%|████▊     | 187/390 [05:12<00:58,  3.47it/s] 48%|████▊     | 188/390 [05:12<00:58,  3.48it/s] 48%|████▊     | 189/390 [05:13<00:57,  3.49it/s] 49%|████▊     | 190/390 [05:13<01:08,  2.94it/s] 49%|████▉     | 191/390 [05:13<01:04,  3.08it/s] 49%|████▉     | 192/390 [05:14<01:01,  3.20it/s] 49%|████▉     | 193/390 [05:14<00:59,  3.29it/s] 50%|████▉     | 194/390 [05:14<00:58,  3.35it/s] 50%|█████     | 195/390 [05:14<00:57,  3.40it/s] 50%|█████     | 196/390 [05:15<00:56,  3.43it/s] 51%|█████     | 197/390 [05:15<00:55,  3.45it/s] 51%|█████     | 198/390 [05:15<00:55,  3.46it/s] 51%|█████     | 199/390 [05:16<00:55,  3.47it/s] 51%|█████▏    | 200/390 [05:16<01:02,  3.03it/s] 52%|█████▏    | 201/390 [05:16<00:59,  3.16it/s] 52%|█████▏    | 202/390 [05:17<00:57,  3.25it/s] 52%|█████▏    | 203/390 [05:17<00:56,  3.32it/s] 52%|█████▏    | 204/390 [05:17<00:55,  3.38it/s] 53%|█████▎    | 205/390 [05:17<00:54,  3.41it/s] 53%|█████▎    | 206/390 [05:18<00:53,  3.44it/s] 53%|█████▎    | 207/390 [05:18<00:52,  3.45it/s] 53%|█████▎    | 208/390 [05:18<00:52,  3.47it/s] 54%|█████▎    | 209/390 [05:19<00:52,  3.47it/s] 54%|█████▍    | 210/390 [05:19<01:00,  2.98it/s] 54%|█████▍    | 211/390 [05:19<00:57,  3.11it/s] 54%|█████▍    | 212/390 [05:20<00:55,  3.22it/s] 55%|█████▍    | 213/390 [05:20<00:53,  3.29it/s] 55%|█████▍    | 214/390 [05:20<00:52,  3.35it/s] 55%|█████▌    | 215/390 [05:20<00:51,  3.40it/s] 55%|█████▌    | 216/390 [05:21<00:50,  3.42it/s] 56%|█████▌    | 217/390 [05:21<00:50,  3.45it/s] 56%|█████▌    | 218/390 [05:21<00:49,  3.46it/s] 56%|█████▌    | 219/390 [05:22<00:49,  3.47it/s] 56%|█████▋    | 220/390 [05:22<00:58,  2.89it/s] 57%|█████▋    | 221/390 [05:22<00:55,  3.05it/s] 57%|█████▋    | 222/390 [05:23<00:53,  3.16it/s] 57%|█████▋    | 223/390 [05:23<00:51,  3.26it/s] 57%|█████▋    | 224/390 [05:23<00:49,  3.32it/s] 58%|█████▊    | 225/390 [05:24<00:48,  3.37it/s] 58%|█████▊    | 226/390 [05:24<00:48,  3.41it/s] 58%|█████▊    | 227/390 [05:24<00:47,  3.44it/s] 58%|█████▊    | 228/390 [05:24<00:46,  3.45it/s] 59%|█████▊    | 229/390 [05:25<00:46,  3.47it/s] 59%|█████▉    | 230/390 [05:25<00:55,  2.90it/s] 59%|█████▉    | 231/390 [05:25<00:51,  3.06it/s] 59%|█████▉    | 232/390 [05:26<00:49,  3.18it/s] 60%|█████▉    | 233/390 [05:26<00:48,  3.26it/s] 60%|██████    | 234/390 [05:26<00:46,  3.33it/s][INFO|trainer.py:2140] 2023-08-28 14:21:20,547 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:21:20,547 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:21:20,547 >>   Batch size = 8
{'eval_loss': 1.0388776063919067, 'eval_runtime': 10.2374, 'eval_samples_per_second': 341.591, 'eval_steps_per_second': 42.784, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.46it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.02it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.15it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.44it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.81it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.52it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.34it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.12it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.28it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.36it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.16it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.44it/s][A
 15%|█▌        | 67/438 [00:01<00:12, 30.64it/s][A
 16%|█▋        | 72/438 [00:01<00:10, 34.09it/s][A
 18%|█▊        | 77/438 [00:01<00:09, 36.89it/s][A
 19%|█▊        | 82/438 [00:01<00:09, 39.24it/s][A
 20%|█▉        | 87/438 [00:02<00:08, 41.02it/s][A
 21%|██        | 92/438 [00:02<00:08, 42.35it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.31it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 43.91it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 43.91it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 43.93it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.11it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.47it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.76it/s][A
 30%|███       | 132/438 [00:03<00:06, 44.97it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 45.20it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 45.37it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 45.40it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.13it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 44.85it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.76it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.87it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.00it/s][A
 40%|████      | 177/438 [00:04<00:05, 45.16it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 45.29it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 45.40it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.41it/s][A
 45%|████▍     | 197/438 [00:04<00:08, 29.88it/s][A
 46%|████▌     | 202/438 [00:04<00:07, 33.32it/s][A
 47%|████▋     | 207/438 [00:04<00:06, 36.28it/s][A
 48%|████▊     | 212/438 [00:05<00:05, 38.69it/s][A
 50%|████▉     | 217/438 [00:05<00:05, 40.54it/s][A
 51%|█████     | 222/438 [00:05<00:05, 42.04it/s][A
 52%|█████▏    | 227/438 [00:05<00:04, 42.99it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 43.75it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 43.79it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 43.83it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.04it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.36it/s][A
 59%|█████▊    | 257/438 [00:06<00:04, 44.68it/s][A
 60%|█████▉    | 262/438 [00:06<00:03, 44.99it/s][A
 61%|██████    | 267/438 [00:06<00:03, 45.14it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 45.30it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 45.39it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 45.19it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 44.87it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.75it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.71it/s][A
 69%|██████▉   | 302/438 [00:07<00:03, 44.92it/s][A
 70%|███████   | 307/438 [00:07<00:02, 45.17it/s][A
 71%|███████   | 312/438 [00:07<00:02, 45.29it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.37it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 45.39it/s][A
 75%|███████▍  | 327/438 [00:07<00:03, 31.81it/s][A
 76%|███████▌  | 332/438 [00:07<00:03, 34.99it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 37.63it/s][A
 78%|███████▊  | 342/438 [00:08<00:02, 39.68it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 41.27it/s][A
 80%|████████  | 352/438 [00:08<00:02, 42.48it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 43.41it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 43.96it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 43.95it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 43.93it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.18it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.55it/s][A
 88%|████████▊ | 387/438 [00:09<00:01, 44.81it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 45.03it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 45.20it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 45.24it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 45.25it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.92it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.81it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.69it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.81it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 45.00it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 45.22it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 45.22it/s][A 60%|██████    | 234/390 [05:37<00:46,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:21:31,160 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 14:21:32,013 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:21:52,705 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:21:53,570 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:21:53,910 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [07:34<1:39:17, 38.43s/it] 61%|██████    | 236/390 [07:34<1:09:22, 27.03s/it] 61%|██████    | 237/390 [07:34<48:27, 19.01s/it]   61%|██████    | 238/390 [07:35<33:55, 13.39s/it] 61%|██████▏   | 239/390 [07:35<23:48,  9.46s/it] 62%|██████▏   | 240/390 [07:35<16:46,  6.71s/it] 62%|██████▏   | 241/390 [07:36<11:52,  4.78s/it] 62%|██████▏   | 242/390 [07:36<08:28,  3.44s/it] 62%|██████▏   | 243/390 [07:36<06:06,  2.49s/it] 63%|██████▎   | 244/390 [07:36<04:27,  1.83s/it] 63%|██████▎   | 245/390 [07:37<03:18,  1.37s/it] 63%|██████▎   | 246/390 [07:37<02:39,  1.10s/it] 63%|██████▎   | 247/390 [07:37<02:02,  1.16it/s] 64%|██████▎   | 248/390 [07:38<01:37,  1.45it/s] 64%|██████▍   | 249/390 [07:38<01:20,  1.76it/s] 64%|██████▍   | 250/390 [07:38<01:08,  2.06it/s] 64%|██████▍   | 251/390 [07:39<00:59,  2.34it/s] 65%|██████▍   | 252/390 [07:39<00:53,  2.59it/s] 65%|██████▍   | 253/390 [07:39<00:49,  2.80it/s] 65%|██████▌   | 254/390 [07:40<00:45,  2.96it/s] 65%|██████▌   | 255/390 [07:40<00:43,  3.09it/s] 66%|██████▌   | 256/390 [07:40<00:49,  2.69it/s] 66%|██████▌   | 257/390 [07:41<00:46,  2.88it/s] 66%|██████▌   | 258/390 [07:41<00:43,  3.03it/s] 66%|██████▋   | 259/390 [07:41<00:41,  3.15it/s] 67%|██████▋   | 260/390 [07:41<00:40,  3.23it/s] 67%|██████▋   | 261/390 [07:42<00:39,  3.30it/s] 67%|██████▋   | 262/390 [07:42<00:38,  3.34it/s] 67%|██████▋   | 263/390 [07:42<00:37,  3.38it/s] 68%|██████▊   | 264/390 [07:43<00:37,  3.40it/s] 68%|██████▊   | 265/390 [07:43<00:36,  3.42it/s] 68%|██████▊   | 266/390 [07:43<00:42,  2.93it/s] 68%|██████▊   | 267/390 [07:44<00:40,  3.07it/s] 69%|██████▊   | 268/390 [07:44<00:43,  2.80it/s] 69%|██████▉   | 269/390 [07:44<00:40,  2.97it/s] 69%|██████▉   | 270/390 [07:45<00:38,  3.10it/s] 69%|██████▉   | 271/390 [07:45<00:37,  3.20it/s] 70%|██████▉   | 272/390 [07:45<00:36,  3.27it/s] 70%|███████   | 273/390 [07:46<00:35,  3.33it/s] 70%|███████   | 274/390 [07:46<00:34,  3.37it/s] 71%|███████   | 275/390 [07:46<00:33,  3.40it/s] 71%|███████   | 276/390 [07:47<00:37,  3.01it/s] 71%|███████   | 277/390 [07:47<00:36,  3.13it/s] 71%|███████▏  | 278/390 [07:47<00:34,  3.23it/s] 72%|███████▏  | 279/390 [07:47<00:33,  3.31it/s] 72%|███████▏  | 280/390 [07:48<00:32,  3.36it/s] 72%|███████▏  | 281/390 [07:48<00:32,  3.41it/s] 72%|███████▏  | 282/390 [07:48<00:31,  3.43it/s] 73%|███████▎  | 283/390 [07:49<00:30,  3.45it/s] 73%|███████▎  | 284/390 [07:49<00:30,  3.47it/s] 73%|███████▎  | 285/390 [07:49<00:30,  3.49it/s] 73%|███████▎  | 286/390 [07:49<00:29,  3.49it/s] 74%|███████▎  | 287/390 [07:50<00:34,  3.01it/s] 74%|███████▍  | 288/390 [07:50<00:32,  3.14it/s] 74%|███████▍  | 289/390 [07:50<00:31,  3.23it/s] 74%|███████▍  | 290/390 [07:51<00:30,  3.31it/s] 75%|███████▍  | 291/390 [07:51<00:29,  3.37it/s] 75%|███████▍  | 292/390 [07:51<00:28,  3.41it/s] 75%|███████▌  | 293/390 [07:52<00:28,  3.44it/s] 75%|███████▌  | 294/390 [07:52<00:27,  3.46it/s] 76%|███████▌  | 295/390 [07:52<00:27,  3.47it/s] 76%|███████▌  | 296/390 [07:52<00:27,  3.48it/s] 76%|███████▌  | 297/390 [07:53<00:29,  3.15it/s] 76%|███████▋  | 298/390 [07:53<00:28,  3.25it/s] 77%|███████▋  | 299/390 [07:53<00:27,  3.32it/s] 77%|███████▋  | 300/390 [07:54<00:26,  3.37it/s] 77%|███████▋  | 301/390 [07:54<00:26,  3.41it/s] 77%|███████▋  | 302/390 [07:54<00:25,  3.44it/s] 78%|███████▊  | 303/390 [07:54<00:25,  3.45it/s] 78%|███████▊  | 304/390 [07:55<00:24,  3.47it/s] 78%|███████▊  | 305/390 [07:55<00:24,  3.47it/s] 78%|███████▊  | 306/390 [07:55<00:24,  3.48it/s] 79%|███████▊  | 307/390 [07:56<00:23,  3.48it/s] 79%|███████▉  | 308/390 [07:56<00:28,  2.91it/s] 79%|███████▉  | 309/390 [07:56<00:26,  3.06it/s] 79%|███████▉  | 310/390 [07:57<00:25,  3.18it/s] 80%|███████▉  | 311/390 [07:57<00:24,  3.26it/s] 80%|████████  | 312/390 [07:57<00:23,  3.33it/s][INFO|trainer.py:2140] 2023-08-28 14:23:51,526 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:23:51,526 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:23:51,526 >>   Batch size = 8
{'eval_loss': 1.0466694831848145, 'eval_runtime': 10.2037, 'eval_samples_per_second': 342.718, 'eval_steps_per_second': 42.925, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.46it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.63it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.37it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.52it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.00it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.69it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.63it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.32it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.52it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.63it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.60it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.63it/s][A
 16%|█▌        | 68/438 [00:01<00:10, 34.66it/s][A
 17%|█▋        | 73/438 [00:01<00:09, 37.44it/s][A
 18%|█▊        | 78/438 [00:01<00:09, 39.59it/s][A
 19%|█▉        | 83/438 [00:01<00:08, 41.27it/s][A
 20%|██        | 88/438 [00:02<00:08, 42.51it/s][A
 21%|██        | 93/438 [00:02<00:07, 43.46it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 44.07it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 44.62it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 44.42it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.59it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.84it/s][A
 28%|██▊       | 123/438 [00:02<00:06, 45.19it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 45.33it/s][A
 30%|███       | 133/438 [00:03<00:06, 45.46it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 45.36it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 45.49it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 45.33it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.18it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 45.08it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 45.13it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 45.28it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 45.44it/s][A
 41%|████      | 178/438 [00:04<00:05, 45.49it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.42it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.34it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.18it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.07it/s][A
 46%|████▋     | 203/438 [00:04<00:06, 36.13it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 38.56it/s][A
 49%|████▊     | 213/438 [00:04<00:05, 40.49it/s][A
 50%|████▉     | 218/438 [00:04<00:05, 41.94it/s][A
 51%|█████     | 223/438 [00:05<00:04, 43.08it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 43.90it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 44.11it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 44.86it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 44.55it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 44.39it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.63it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.73it/s][A
 60%|██████    | 263/438 [00:05<00:03, 45.13it/s][A
 61%|██████    | 268/438 [00:06<00:03, 45.35it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 45.40it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 45.56it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 45.61it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 45.15it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.96it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.97it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 45.03it/s][A
 70%|███████   | 308/438 [00:06<00:02, 45.19it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 45.44it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.50it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.59it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.51it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.33it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 35.29it/s][A
 78%|███████▊  | 343/438 [00:08<00:02, 37.88it/s][A
 79%|███████▉  | 348/438 [00:08<00:03, 24.15it/s][A
 81%|████████  | 353/438 [00:08<00:03, 28.17it/s][A
 82%|████████▏ | 358/438 [00:08<00:02, 31.82it/s][A
 83%|████████▎ | 363/438 [00:08<00:02, 34.99it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 37.68it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 39.73it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 41.36it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 42.51it/s][A
 89%|████████▊ | 388/438 [00:09<00:01, 42.87it/s][A
 90%|████████▉ | 393/438 [00:09<00:01, 43.28it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 43.88it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.45it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 44.78it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 45.07it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 45.24it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 45.38it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 45.35it/s][A
 99%|█████████▉| 433/438 [00:10<00:00, 44.99it/s][A
100%|██████████| 438/438 [00:10<00:00, 44.92it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.92it/s][A 80%|████████  | 312/390 [08:08<00:23,  3.33it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:24:01,971 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 14:24:02,627 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:24:40,007 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:24:41,162 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:24:41,494 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [09:35<37:44, 29.41s/it] 81%|████████  | 314/390 [09:35<26:12, 20.69s/it] 81%|████████  | 315/390 [09:35<18:12, 14.57s/it] 81%|████████  | 316/390 [09:36<12:41, 10.29s/it] 81%|████████▏ | 317/390 [09:36<08:52,  7.29s/it] 82%|████████▏ | 318/390 [09:36<06:13,  5.19s/it] 82%|████████▏ | 319/390 [09:36<04:24,  3.72s/it] 82%|████████▏ | 320/390 [09:37<03:08,  2.69s/it] 82%|████████▏ | 321/390 [09:37<02:15,  1.97s/it] 83%|████████▎ | 322/390 [09:37<01:39,  1.46s/it] 83%|████████▎ | 323/390 [09:38<01:14,  1.11s/it] 83%|████████▎ | 324/390 [09:38<00:57,  1.16it/s] 83%|████████▎ | 325/390 [09:38<00:44,  1.45it/s] 84%|████████▎ | 326/390 [09:38<00:36,  1.76it/s] 84%|████████▍ | 327/390 [09:39<00:30,  2.06it/s] 84%|████████▍ | 328/390 [09:39<00:26,  2.35it/s] 84%|████████▍ | 329/390 [09:39<00:23,  2.61it/s] 85%|████████▍ | 330/390 [09:40<00:21,  2.82it/s] 85%|████████▍ | 331/390 [09:40<00:19,  2.99it/s] 85%|████████▌ | 332/390 [09:40<00:18,  3.12it/s] 85%|████████▌ | 333/390 [09:40<00:17,  3.22it/s] 86%|████████▌ | 334/390 [09:41<00:18,  3.05it/s] 86%|████████▌ | 335/390 [09:41<00:17,  3.16it/s] 86%|████████▌ | 336/390 [09:41<00:16,  3.25it/s] 86%|████████▋ | 337/390 [09:42<00:15,  3.32it/s] 87%|████████▋ | 338/390 [09:42<00:15,  3.36it/s] 87%|████████▋ | 339/390 [09:42<00:15,  3.40it/s] 87%|████████▋ | 340/390 [09:42<00:14,  3.42it/s] 87%|████████▋ | 341/390 [09:43<00:14,  3.44it/s] 88%|████████▊ | 342/390 [09:43<00:13,  3.45it/s] 88%|████████▊ | 343/390 [09:43<00:13,  3.45it/s] 88%|████████▊ | 344/390 [09:44<00:13,  3.47it/s] 88%|████████▊ | 345/390 [09:44<00:13,  3.26it/s] 89%|████████▊ | 346/390 [09:44<00:14,  3.02it/s] 89%|████████▉ | 347/390 [09:45<00:13,  3.15it/s] 89%|████████▉ | 348/390 [09:45<00:12,  3.25it/s] 89%|████████▉ | 349/390 [09:45<00:12,  3.31it/s] 90%|████████▉ | 350/390 [09:46<00:11,  3.37it/s] 90%|█████████ | 351/390 [09:46<00:11,  3.40it/s] 90%|█████████ | 352/390 [09:46<00:11,  3.43it/s] 91%|█████████ | 353/390 [09:46<00:10,  3.45it/s] 91%|█████████ | 354/390 [09:47<00:10,  3.46it/s] 91%|█████████ | 355/390 [09:47<00:10,  3.46it/s] 91%|█████████▏| 356/390 [09:47<00:09,  3.47it/s] 92%|█████████▏| 357/390 [09:48<00:09,  3.48it/s] 92%|█████████▏| 358/390 [09:48<00:09,  3.48it/s] 92%|█████████▏| 359/390 [09:48<00:08,  3.48it/s] 92%|█████████▏| 360/390 [09:48<00:08,  3.48it/s] 93%|█████████▎| 361/390 [09:49<00:08,  3.48it/s] 93%|█████████▎| 362/390 [09:49<00:08,  3.48it/s] 93%|█████████▎| 363/390 [09:49<00:07,  3.47it/s] 93%|█████████▎| 364/390 [09:50<00:08,  3.21it/s] 94%|█████████▎| 365/390 [09:51<00:12,  1.93it/s] 94%|█████████▍| 366/390 [09:51<00:12,  1.98it/s] 94%|█████████▍| 367/390 [09:51<00:10,  2.28it/s] 94%|█████████▍| 368/390 [09:52<00:08,  2.54it/s] 95%|█████████▍| 369/390 [09:52<00:07,  2.77it/s] 95%|█████████▍| 370/390 [09:52<00:06,  2.95it/s] 95%|█████████▌| 371/390 [09:53<00:06,  3.09it/s] 95%|█████████▌| 372/390 [09:53<00:05,  3.21it/s] 96%|█████████▌| 373/390 [09:53<00:05,  3.28it/s] 96%|█████████▌| 374/390 [09:53<00:04,  3.34it/s] 96%|█████████▌| 375/390 [09:54<00:04,  3.39it/s] 96%|█████████▋| 376/390 [09:54<00:04,  3.00it/s] 97%|█████████▋| 377/390 [09:54<00:04,  3.13it/s] 97%|█████████▋| 378/390 [09:55<00:03,  3.23it/s] 97%|█████████▋| 379/390 [09:55<00:03,  3.30it/s] 97%|█████████▋| 380/390 [09:55<00:02,  3.36it/s] 98%|█████████▊| 381/390 [09:56<00:02,  3.39it/s] 98%|█████████▊| 382/390 [09:56<00:02,  3.42it/s] 98%|█████████▊| 383/390 [09:56<00:02,  3.44it/s] 98%|█████████▊| 384/390 [09:56<00:01,  3.45it/s] 99%|█████████▊| 385/390 [09:57<00:01,  3.46it/s] 99%|█████████▉| 386/390 [09:57<00:01,  3.10it/s] 99%|█████████▉| 387/390 [09:57<00:00,  3.21it/s] 99%|█████████▉| 388/390 [09:58<00:00,  3.29it/s]100%|█████████▉| 389/390 [09:58<00:00,  3.34it/s]100%|██████████| 390/390 [09:58<00:00,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 14:25:52,480 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:25:52,480 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:25:52,480 >>   Batch size = 8
{'eval_loss': 1.0569549798965454, 'eval_runtime': 10.2317, 'eval_samples_per_second': 341.781, 'eval_steps_per_second': 42.808, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.04it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.02it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.18it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.36it/s][A
  6%|▌         | 27/438 [00:00<00:08, 45.88it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.51it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.41it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.21it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.26it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.36it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.35it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.25it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.18it/s][A
 16%|█▋        | 72/438 [00:01<00:10, 35.72it/s][A
 18%|█▊        | 77/438 [00:01<00:09, 38.25it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 40.17it/s][A
 20%|█▉        | 87/438 [00:01<00:08, 41.65it/s][A
 21%|██        | 92/438 [00:02<00:08, 42.83it/s][A
 22%|██▏       | 97/438 [00:02<00:07, 43.66it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 44.19it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 44.52it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 44.33it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.27it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.40it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.68it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.96it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 45.01it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 45.25it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 45.32it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.28it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.13it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 44.99it/s][A
 38%|███▊      | 167/438 [00:03<00:06, 44.99it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 44.99it/s][A
 40%|████      | 177/438 [00:03<00:05, 45.10it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 45.12it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 45.15it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.14it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.03it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 44.93it/s][A
 47%|████▋     | 207/438 [00:04<00:07, 31.86it/s][A
 48%|████▊     | 212/438 [00:04<00:06, 34.98it/s][A
 50%|████▉     | 217/438 [00:05<00:05, 37.59it/s][A
 51%|█████     | 222/438 [00:05<00:05, 39.64it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 41.12it/s][A
 53%|█████▎    | 232/438 [00:05<00:04, 42.44it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 43.31it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 43.91it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 43.98it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.14it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.41it/s][A
 60%|█████▉    | 262/438 [00:06<00:03, 44.78it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.86it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.96it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 45.04it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 45.19it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 45.10it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 44.91it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 44.89it/s][A
 69%|██████▉   | 302/438 [00:06<00:03, 44.94it/s][A
 70%|███████   | 307/438 [00:07<00:02, 44.96it/s][A
 71%|███████   | 312/438 [00:07<00:02, 44.96it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.19it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 45.26it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 45.36it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 45.09it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 35.64it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 38.17it/s][A
 79%|███████▉  | 347/438 [00:08<00:02, 40.19it/s][A
 80%|████████  | 352/438 [00:08<00:02, 41.61it/s][A
 82%|████████▏ | 357/438 [00:08<00:01, 42.74it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 43.56it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 44.13it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 44.39it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 44.27it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 44.08it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.32it/s][A
 89%|████████▉ | 392/438 [00:09<00:01, 44.71it/s][A
 91%|█████████ | 397/438 [00:09<00:00, 44.96it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 45.12it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 45.28it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 45.35it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 45.16it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 44.86it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 44.67it/s][A
 99%|█████████▊| 432/438 [00:10<00:00, 30.30it/s][A
100%|█████████▉| 437/438 [00:10<00:00, 33.72it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 33.72it/s][A100%|██████████| 390/390 [10:08<00:00,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 14:26:03,752 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 14:26:04,770 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:26:42,107 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:26:43,529 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:26:43,922 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 14:27:52,718 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 14:27:52,806 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78 (score: 1.0286164283752441).
                                                 100%|██████████| 390/390 [12:33<00:00,  3.38it/s]100%|██████████| 390/390 [12:33<00:00,  1.93s/it]
[INFO|trainer.py:1894] 2023-08-28 14:28:27,347 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model
[INFO|configuration_utils.py:351] 2023-08-28 14:28:27,676 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 14:28:41,657 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 14:28:42,416 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 14:28:42,794 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:28:45,549 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:45,677 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:45,678 >>   train_loss               =     0.6087
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:45,678 >>   train_runtime            = 0:12:33.54
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:45,678 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:45,678 >>   train_samples_per_second =     33.176
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:45,678 >>   train_steps_per_second   =      0.518
{'eval_loss': 1.0615936517715454, 'eval_runtime': 10.2147, 'eval_samples_per_second': 342.351, 'eval_steps_per_second': 42.88, 'epoch': 4.99}
{'train_runtime': 753.5487, 'train_samples_per_second': 33.176, 'train_steps_per_second': 0.518, 'train_loss': 0.6087127685546875, 'epoch': 4.99}
08/28/2023 14:28:46 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 14:28:46,950 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 14:28:46,950 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 14:28:46,950 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 56.94it/s]  3%|▎         | 12/438 [00:00<00:08, 49.92it/s]  4%|▍         | 18/438 [00:00<00:08, 48.14it/s]  5%|▌         | 23/438 [00:00<00:08, 47.30it/s]  6%|▋         | 28/438 [00:00<00:08, 46.91it/s]  8%|▊         | 33/438 [00:00<00:08, 46.67it/s]  9%|▊         | 38/438 [00:00<00:08, 46.46it/s] 10%|▉         | 43/438 [00:01<00:11, 33.04it/s] 11%|█         | 48/438 [00:01<00:10, 36.24it/s] 12%|█▏        | 53/438 [00:01<00:09, 38.70it/s] 13%|█▎        | 58/438 [00:01<00:09, 40.58it/s] 14%|█▍        | 63/438 [00:01<00:08, 41.91it/s] 16%|█▌        | 68/438 [00:01<00:08, 43.06it/s] 17%|█▋        | 73/438 [00:01<00:11, 31.20it/s] 18%|█▊        | 78/438 [00:01<00:10, 34.51it/s] 19%|█▉        | 83/438 [00:02<00:09, 37.24it/s] 20%|██        | 88/438 [00:02<00:08, 39.44it/s] 21%|██        | 93/438 [00:02<00:08, 41.13it/s] 22%|██▏       | 98/438 [00:02<00:08, 42.41it/s] 24%|██▎       | 103/438 [00:02<00:07, 43.35it/s] 25%|██▍       | 108/438 [00:02<00:07, 44.02it/s] 26%|██▌       | 113/438 [00:02<00:07, 44.03it/s] 27%|██▋       | 118/438 [00:02<00:07, 44.32it/s] 28%|██▊       | 123/438 [00:02<00:07, 44.56it/s] 29%|██▉       | 128/438 [00:03<00:06, 44.88it/s] 30%|███       | 133/438 [00:03<00:06, 45.12it/s] 32%|███▏      | 138/438 [00:03<00:06, 45.35it/s] 33%|███▎      | 143/438 [00:04<00:32,  9.07it/s] 34%|███▎      | 147/438 [00:05<00:27, 10.42it/s] 35%|███▍      | 152/438 [00:05<00:20, 13.77it/s] 36%|███▌      | 157/438 [00:05<00:15, 17.57it/s] 37%|███▋      | 162/438 [00:05<00:12, 21.64it/s] 38%|███▊      | 167/438 [00:05<00:10, 25.77it/s] 39%|███▉      | 172/438 [00:05<00:08, 29.70it/s] 40%|████      | 177/438 [00:05<00:07, 33.21it/s] 42%|████▏     | 182/438 [00:05<00:07, 36.21it/s] 43%|████▎     | 187/438 [00:05<00:06, 38.35it/s] 44%|████▍     | 192/438 [00:06<00:06, 40.00it/s] 45%|████▍     | 197/438 [00:06<00:05, 41.49it/s] 46%|████▌     | 202/438 [00:06<00:05, 42.57it/s] 47%|████▋     | 207/438 [00:06<00:05, 43.41it/s] 48%|████▊     | 212/438 [00:06<00:05, 44.10it/s] 50%|████▉     | 217/438 [00:06<00:04, 44.55it/s] 51%|█████     | 222/438 [00:06<00:04, 44.94it/s] 52%|█████▏    | 227/438 [00:06<00:04, 45.06it/s] 53%|█████▎    | 232/438 [00:06<00:04, 44.83it/s] 54%|█████▍    | 237/438 [00:07<00:04, 44.83it/s] 55%|█████▌    | 242/438 [00:07<00:04, 44.94it/s] 56%|█████▋    | 247/438 [00:07<00:04, 45.04it/s] 58%|█████▊    | 252/438 [00:07<00:04, 45.21it/s] 59%|█████▊    | 257/438 [00:07<00:03, 45.31it/s] 60%|█████▉    | 262/438 [00:07<00:03, 45.50it/s] 61%|██████    | 267/438 [00:07<00:03, 45.59it/s] 62%|██████▏   | 272/438 [00:07<00:03, 45.46it/s] 63%|██████▎   | 277/438 [00:07<00:03, 45.27it/s] 64%|██████▍   | 282/438 [00:08<00:04, 32.29it/s] 66%|██████▌   | 287/438 [00:08<00:04, 35.48it/s] 67%|██████▋   | 292/438 [00:08<00:03, 38.14it/s] 68%|██████▊   | 297/438 [00:08<00:03, 40.23it/s] 69%|██████▉   | 302/438 [00:08<00:03, 41.82it/s] 70%|███████   | 307/438 [00:08<00:03, 43.02it/s] 71%|███████   | 312/438 [00:08<00:02, 43.88it/s] 72%|███████▏  | 317/438 [00:08<00:02, 44.48it/s] 74%|███████▎  | 322/438 [00:09<00:02, 44.40it/s] 75%|███████▍  | 327/438 [00:09<00:02, 44.39it/s] 76%|███████▌  | 332/438 [00:09<00:02, 44.57it/s] 77%|███████▋  | 337/438 [00:09<00:02, 44.93it/s] 78%|███████▊  | 342/438 [00:09<00:03, 25.29it/s] 79%|███████▉  | 347/438 [00:09<00:03, 29.27it/s] 80%|████████  | 352/438 [00:10<00:02, 32.75it/s] 82%|████████▏ | 357/438 [00:10<00:02, 36.02it/s] 83%|████████▎ | 362/438 [00:10<00:01, 38.55it/s] 84%|████████▍ | 367/438 [00:10<00:01, 40.53it/s] 85%|████████▍ | 372/438 [00:10<00:01, 41.92it/s] 86%|████████▌ | 377/438 [00:10<00:01, 43.07it/s] 87%|████████▋ | 382/438 [00:10<00:01, 43.46it/s] 88%|████████▊ | 387/438 [00:10<00:01, 43.81it/s] 89%|████████▉ | 392/438 [00:10<00:01, 44.07it/s] 91%|█████████ | 397/438 [00:11<00:00, 44.52it/s] 92%|█████████▏| 402/438 [00:11<00:00, 44.85it/s] 93%|█████████▎| 407/438 [00:11<00:00, 45.15it/s] 94%|█████████▍| 412/438 [00:11<00:00, 45.39it/s] 95%|█████████▌| 417/438 [00:11<00:00, 45.59it/s] 96%|█████████▋| 422/438 [00:11<00:00, 45.59it/s] 97%|█████████▋| 427/438 [00:11<00:00, 45.51it/s] 99%|█████████▊| 432/438 [00:11<00:00, 45.18it/s]100%|█████████▉| 437/438 [00:11<00:00, 45.16it/s]100%|██████████| 438/438 [00:11<00:00, 36.76it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 14:28:58,903 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:58,903 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:58,903 >>   eval_loss               =     1.0286
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:58,903 >>   eval_runtime            = 0:00:11.95
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:58,903 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:58,903 >>   eval_samples_per_second =    292.565
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:58,904 >>   eval_steps_per_second   =     36.644
[INFO|trainer_pt_utils.py:913] 2023-08-28 14:28:58,904 >>   perplexity              =     2.7972
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:32,400 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:32,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:32,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:32,481 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:32,482 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:29:33,882 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:29:33,883 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:29:34,680 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:29:35,915 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:29:36,015 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:39,480 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:39,605 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:39,605 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:39,605 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:29:39,605 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:29:40,971 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:29:40,973 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:29:41,728 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:29:42,092 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:29:42,092 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/checkpoint-78
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.46it/s]Extractor Predicting: 2it [00:01,  1.51it/s]Extractor Predicting: 3it [00:01,  1.54it/s]Extractor Predicting: 4it [00:02,  1.55it/s]Extractor Predicting: 5it [00:03,  1.56it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.53it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.58it/s]Extractor Predicting: 10it [00:06,  1.56it/s]Extractor Predicting: 11it [00:07,  1.57it/s]Extractor Predicting: 12it [00:07,  1.55it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.55it/s]Extractor Predicting: 15it [00:09,  1.48it/s]Extractor Predicting: 16it [00:10,  1.49it/s]Extractor Predicting: 17it [00:11,  1.51it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.52it/s]Extractor Predicting: 21it [00:13,  1.56it/s]Extractor Predicting: 22it [00:14,  1.58it/s]Extractor Predicting: 23it [00:14,  1.58it/s]Extractor Predicting: 24it [00:15,  1.64it/s]Extractor Predicting: 25it [00:16,  1.58it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:17,  1.57it/s]Extractor Predicting: 28it [00:18,  1.57it/s]Extractor Predicting: 29it [00:18,  1.56it/s]Extractor Predicting: 30it [00:19,  1.49it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:20,  1.49it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:22,  1.48it/s]Extractor Predicting: 36it [00:23,  1.47it/s]Extractor Predicting: 37it [00:24,  1.41it/s]Extractor Predicting: 38it [00:24,  1.46it/s]Extractor Predicting: 39it [00:25,  1.44it/s]Extractor Predicting: 40it [00:26,  1.39it/s]Extractor Predicting: 41it [00:27,  1.42it/s]Extractor Predicting: 42it [00:27,  1.46it/s]Extractor Predicting: 43it [00:28,  1.49it/s]Extractor Predicting: 44it [00:28,  1.52it/s]Extractor Predicting: 45it [00:29,  1.42it/s]Extractor Predicting: 46it [00:30,  1.45it/s]Extractor Predicting: 47it [00:31,  1.44it/s]Extractor Predicting: 48it [00:31,  1.50it/s]Extractor Predicting: 49it [00:32,  1.49it/s]Extractor Predicting: 50it [00:33,  1.46it/s]Extractor Predicting: 51it [00:33,  1.50it/s]Extractor Predicting: 52it [00:34,  1.50it/s]Extractor Predicting: 53it [00:35,  1.49it/s]Extractor Predicting: 54it [00:35,  1.53it/s]Extractor Predicting: 55it [00:36,  1.43it/s]Extractor Predicting: 56it [00:37,  1.40it/s]Extractor Predicting: 57it [00:37,  1.46it/s]Extractor Predicting: 58it [00:38,  1.46it/s]Extractor Predicting: 59it [00:39,  1.45it/s]Extractor Predicting: 60it [00:39,  1.47it/s]Extractor Predicting: 61it [00:40,  1.39it/s]Extractor Predicting: 62it [00:41,  1.46it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.49it/s]Extractor Predicting: 65it [00:43,  1.54it/s]Extractor Predicting: 66it [00:43,  1.50it/s]Extractor Predicting: 67it [00:44,  1.52it/s]Extractor Predicting: 68it [00:45,  1.55it/s]Extractor Predicting: 69it [00:45,  1.56it/s]Extractor Predicting: 70it [00:46,  1.59it/s]Extractor Predicting: 71it [00:47,  1.51it/s]Extractor Predicting: 72it [00:47,  1.55it/s]Extractor Predicting: 73it [00:48,  1.54it/s]Extractor Predicting: 74it [00:49,  1.53it/s]Extractor Predicting: 75it [00:49,  1.56it/s]Extractor Predicting: 76it [00:50,  1.48it/s]Extractor Predicting: 77it [00:51,  1.54it/s]Extractor Predicting: 78it [00:51,  1.54it/s]Extractor Predicting: 79it [00:52,  1.60it/s]Extractor Predicting: 80it [00:52,  1.62it/s]Extractor Predicting: 81it [00:53,  1.53it/s]Extractor Predicting: 82it [00:54,  1.52it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:55,  1.53it/s]Extractor Predicting: 85it [00:56,  1.52it/s]Extractor Predicting: 86it [00:56,  1.46it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.52it/s]Extractor Predicting: 89it [00:58,  1.53it/s]Extractor Predicting: 90it [00:59,  1.56it/s]Extractor Predicting: 91it [01:00,  1.48it/s]Extractor Predicting: 92it [01:00,  1.51it/s]Extractor Predicting: 93it [01:01,  1.52it/s]Extractor Predicting: 94it [01:02,  1.56it/s]Extractor Predicting: 95it [01:02,  1.59it/s]Extractor Predicting: 96it [01:03,  1.49it/s]Extractor Predicting: 97it [01:04,  1.49it/s]Extractor Predicting: 98it [01:04,  1.50it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:06,  1.52it/s]Extractor Predicting: 101it [01:06,  1.49it/s]Extractor Predicting: 102it [01:07,  1.51it/s]Extractor Predicting: 103it [01:08,  1.28it/s]Extractor Predicting: 104it [01:09,  1.34it/s]Extractor Predicting: 105it [01:09,  1.39it/s]Extractor Predicting: 106it [01:10,  1.44it/s]Extractor Predicting: 107it [01:11,  1.34it/s]Extractor Predicting: 108it [01:11,  1.40it/s]Extractor Predicting: 109it [01:12,  1.42it/s]Extractor Predicting: 110it [01:13,  1.45it/s]Extractor Predicting: 111it [01:13,  1.49it/s]Extractor Predicting: 112it [01:14,  1.43it/s]Extractor Predicting: 113it [01:15,  1.44it/s]Extractor Predicting: 114it [01:16,  1.47it/s]Extractor Predicting: 115it [01:16,  1.48it/s]Extractor Predicting: 116it [01:17,  1.46it/s]Extractor Predicting: 117it [01:18,  1.39it/s]Extractor Predicting: 118it [01:18,  1.43it/s]Extractor Predicting: 119it [01:19,  1.46it/s]Extractor Predicting: 120it [01:20,  1.46it/s]Extractor Predicting: 121it [01:20,  1.49it/s]Extractor Predicting: 122it [01:21,  1.44it/s]Extractor Predicting: 123it [01:22,  1.47it/s]Extractor Predicting: 124it [01:22,  1.48it/s]Extractor Predicting: 125it [01:23,  1.47it/s]Extractor Predicting: 126it [01:24,  1.49it/s]Extractor Predicting: 127it [01:24,  1.48it/s]Extractor Predicting: 128it [01:25,  1.48it/s]Extractor Predicting: 129it [01:26,  1.49it/s]Extractor Predicting: 130it [01:26,  1.51it/s]Extractor Predicting: 131it [01:27,  1.48it/s]Extractor Predicting: 132it [01:28,  1.45it/s]Extractor Predicting: 133it [01:28,  1.49it/s]Extractor Predicting: 134it [01:29,  1.51it/s]Extractor Predicting: 135it [01:30,  1.48it/s]Extractor Predicting: 136it [01:31,  1.45it/s]Extractor Predicting: 137it [01:31,  1.43it/s]Extractor Predicting: 138it [01:32,  1.45it/s]Extractor Predicting: 139it [01:33,  1.46it/s]Extractor Predicting: 140it [01:33,  1.48it/s]Extractor Predicting: 141it [01:34,  1.49it/s]Extractor Predicting: 142it [01:35,  1.38it/s]Extractor Predicting: 143it [01:35,  1.41it/s]Extractor Predicting: 144it [01:36,  1.47it/s]Extractor Predicting: 145it [01:36,  1.66it/s]Extractor Predicting: 145it [01:36,  1.50it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:48,737 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:48,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:48,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:48,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:48,905 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:31:51,130 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:31:51,131 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:31:52,018 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:31:53,481 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:31:53,481 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:57,153 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:57,292 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:57,292 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:57,292 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:31:57,292 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:31:58,739 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:31:58,740 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:31:59,565 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:32:00,048 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:32:00,048 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.6413551401869159,
  "recall": 0.1569917071775808,
  "score": 0.25223983459682975,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.44it/s]Extractor Predicting: 7it [00:04,  1.40it/s]Extractor Predicting: 8it [00:05,  1.44it/s]Extractor Predicting: 9it [00:05,  1.50it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.49it/s]Extractor Predicting: 12it [00:07,  1.52it/s]Extractor Predicting: 13it [00:08,  1.53it/s]Extractor Predicting: 14it [00:09,  1.56it/s]Extractor Predicting: 15it [00:09,  1.52it/s]Extractor Predicting: 16it [00:10,  1.46it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.54it/s]Extractor Predicting: 19it [00:12,  1.53it/s]Extractor Predicting: 20it [00:13,  1.53it/s]Extractor Predicting: 21it [00:13,  1.46it/s]Extractor Predicting: 22it [00:14,  1.50it/s]Extractor Predicting: 23it [00:15,  1.53it/s]Extractor Predicting: 24it [00:15,  1.51it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:17,  1.46it/s]Extractor Predicting: 27it [00:17,  1.52it/s]Extractor Predicting: 28it [00:18,  1.52it/s]Extractor Predicting: 29it [00:19,  1.53it/s]Extractor Predicting: 30it [00:19,  1.56it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:21,  1.51it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.53it/s]Extractor Predicting: 35it [00:23,  1.55it/s]Extractor Predicting: 36it [00:23,  1.50it/s]Extractor Predicting: 37it [00:24,  1.55it/s]Extractor Predicting: 38it [00:25,  1.57it/s]Extractor Predicting: 39it [00:25,  1.59it/s]Extractor Predicting: 40it [00:26,  1.57it/s]Extractor Predicting: 41it [00:27,  1.48it/s]Extractor Predicting: 42it [00:27,  1.51it/s]Extractor Predicting: 43it [00:28,  1.51it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.53it/s]Extractor Predicting: 46it [00:30,  1.50it/s]Extractor Predicting: 47it [00:30,  1.53it/s]Extractor Predicting: 48it [00:31,  1.55it/s]Extractor Predicting: 49it [00:32,  1.58it/s]Extractor Predicting: 50it [00:32,  1.58it/s]Extractor Predicting: 51it [00:33,  1.48it/s]Extractor Predicting: 52it [00:34,  1.55it/s]Extractor Predicting: 53it [00:34,  1.56it/s]Extractor Predicting: 54it [00:35,  1.58it/s]Extractor Predicting: 55it [00:36,  1.58it/s]Extractor Predicting: 56it [00:36,  1.43it/s]Extractor Predicting: 57it [00:37,  1.45it/s]Extractor Predicting: 58it [00:38,  1.50it/s]Extractor Predicting: 59it [00:38,  1.49it/s]Extractor Predicting: 60it [00:39,  1.52it/s]Extractor Predicting: 61it [00:40,  1.42it/s]Extractor Predicting: 62it [00:40,  1.48it/s]Extractor Predicting: 63it [00:41,  1.50it/s]Extractor Predicting: 64it [00:42,  1.54it/s]Extractor Predicting: 65it [00:42,  1.56it/s]Extractor Predicting: 66it [00:43,  1.58it/s]Extractor Predicting: 67it [00:44,  1.59it/s]Extractor Predicting: 68it [00:44,  1.62it/s]Extractor Predicting: 69it [00:45,  1.54it/s]Extractor Predicting: 70it [00:45,  1.55it/s]Extractor Predicting: 71it [00:46,  1.58it/s]Extractor Predicting: 72it [00:47,  1.59it/s]Extractor Predicting: 73it [00:47,  1.60it/s]Extractor Predicting: 74it [00:48,  1.50it/s]Extractor Predicting: 75it [00:49,  1.50it/s]Extractor Predicting: 76it [00:49,  1.54it/s]Extractor Predicting: 77it [00:50,  1.60it/s]Extractor Predicting: 78it [00:51,  1.47it/s]Extractor Predicting: 79it [00:51,  1.43it/s]Extractor Predicting: 80it [00:52,  1.49it/s]Extractor Predicting: 81it [00:53,  1.52it/s]Extractor Predicting: 82it [00:53,  1.54it/s]Extractor Predicting: 83it [00:54,  1.53it/s]Extractor Predicting: 84it [00:55,  1.47it/s]Extractor Predicting: 85it [00:55,  1.51it/s]Extractor Predicting: 86it [00:56,  1.56it/s]Extractor Predicting: 87it [00:57,  1.59it/s]Extractor Predicting: 88it [00:57,  1.57it/s]Extractor Predicting: 89it [00:58,  1.47it/s]Extractor Predicting: 90it [00:59,  1.52it/s]Extractor Predicting: 91it [00:59,  1.57it/s]Extractor Predicting: 92it [01:00,  1.55it/s]Extractor Predicting: 93it [01:01,  1.53it/s]Extractor Predicting: 94it [01:01,  1.43it/s]Extractor Predicting: 95it [01:02,  1.47it/s]Extractor Predicting: 96it [01:03,  1.51it/s]Extractor Predicting: 97it [01:03,  1.53it/s]Extractor Predicting: 98it [01:04,  1.52it/s]Extractor Predicting: 99it [01:05,  1.45it/s]Extractor Predicting: 100it [01:05,  1.49it/s]Extractor Predicting: 101it [01:06,  1.54it/s]Extractor Predicting: 102it [01:07,  1.50it/s]Extractor Predicting: 103it [01:07,  1.53it/s]Extractor Predicting: 104it [01:08,  1.45it/s]Extractor Predicting: 105it [01:09,  1.46it/s]Extractor Predicting: 106it [01:09,  1.49it/s]Extractor Predicting: 107it [01:10,  1.50it/s]Extractor Predicting: 108it [01:11,  1.54it/s]Extractor Predicting: 109it [01:11,  1.48it/s]Extractor Predicting: 110it [01:12,  1.54it/s]Extractor Predicting: 111it [01:13,  1.54it/s]Extractor Predicting: 112it [01:13,  1.54it/s]Extractor Predicting: 113it [01:14,  1.55it/s]Extractor Predicting: 114it [01:14,  1.54it/s]Extractor Predicting: 115it [01:15,  1.55it/s]Extractor Predicting: 116it [01:16,  1.48it/s]Extractor Predicting: 117it [01:16,  1.53it/s]Extractor Predicting: 118it [01:17,  1.53it/s]Extractor Predicting: 119it [01:18,  1.56it/s]Extractor Predicting: 120it [01:18,  1.55it/s]Extractor Predicting: 121it [01:19,  1.48it/s]Extractor Predicting: 122it [01:20,  1.54it/s]Extractor Predicting: 123it [01:20,  1.57it/s]Extractor Predicting: 124it [01:21,  1.57it/s]Extractor Predicting: 125it [01:22,  1.55it/s]Extractor Predicting: 126it [01:22,  1.45it/s]Extractor Predicting: 127it [01:23,  1.48it/s]Extractor Predicting: 128it [01:24,  1.52it/s]Extractor Predicting: 129it [01:24,  1.57it/s]Extractor Predicting: 130it [01:25,  1.56it/s]Extractor Predicting: 131it [01:26,  1.50it/s]Extractor Predicting: 132it [01:26,  1.54it/s]Extractor Predicting: 133it [01:27,  1.54it/s]Extractor Predicting: 134it [01:28,  1.53it/s]Extractor Predicting: 135it [01:28,  1.54it/s]Extractor Predicting: 136it [01:29,  1.46it/s]Extractor Predicting: 137it [01:30,  1.51it/s]Extractor Predicting: 138it [01:30,  1.53it/s]Extractor Predicting: 139it [01:31,  1.54it/s]Extractor Predicting: 140it [01:31,  1.54it/s]Extractor Predicting: 141it [01:32,  1.49it/s]Extractor Predicting: 142it [01:33,  1.49it/s]Extractor Predicting: 143it [01:34,  1.51it/s]Extractor Predicting: 144it [01:34,  1.54it/s]Extractor Predicting: 145it [01:34,  1.93it/s]Extractor Predicting: 145it [01:34,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:32,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:32,781 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:32,782 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:32,782 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:32,782 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:35:34,671 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:35:34,673 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:35:35,537 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:35:36,908 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:35:37,103 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:40,852 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:40,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:40,938 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:40,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:35:40,939 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:35:42,209 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:35:42,210 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:35:42,993 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:35:43,382 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:35:43,382 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_single_is_eval_False.jsonl",
  "precision": 0.425748884639898,
  "recall": 0.1931193986701359,
  "score": 0.2657120127287192,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 1it [00:00,  1.53it/s]
[INFO|configuration_utils.py:515] 2023-08-28 14:35:52,311 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:35:52,434 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 14:35:52,653 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:35:52,654 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 14:35:52,739 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 14:36:33,510 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 14:36:33,640 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 14:36:34,415 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 14:36:34,416 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter1/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 14:36:34,742 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:36:34,986 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:36:34,986 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:36:34,986 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:36:34,986 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:36:34,986 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 14:36:34,986 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.8333333333333334,
  "recall": 0.12195121951219512,
  "score": 0.21276595744680848,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 14:36:35,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:36,535 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:37,357 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:37,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:38,675 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:39,231 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:39,843 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:40,488 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:41,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:41,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:42,618 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:43,223 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:43,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:44,460 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:45,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:45,692 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:46,261 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:46,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:47,508 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:48,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:48,753 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:13<02:01, 13.51s/it][WARNING|generation_utils.py:914] 2023-08-28 14:36:49,479 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:50,169 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:50,770 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:51,455 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:52,011 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:52,583 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:53,106 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:53,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:54,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:54,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:55,490 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:56,171 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:56,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:57,390 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:58,219 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:58,816 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:59,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:36:59,973 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:00,510 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:01,355 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:02,066 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:26<01:46, 13.33s/it][WARNING|generation_utils.py:914] 2023-08-28 14:37:02,691 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:03,283 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:03,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:04,624 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:05,211 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:05,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:06,602 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:07,189 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:08,014 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:08,808 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:09,378 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:09,915 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:10,542 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:11,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:12,002 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:13,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:13,756 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:14,434 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:15,091 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:15,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:16,437 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:17,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:41<01:39, 14.14s/it][WARNING|generation_utils.py:914] 2023-08-28 14:37:17,801 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:18,387 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:19,118 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:19,698 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:20,332 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:20,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:21,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:22,281 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:22,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:23,552 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:24,312 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:24,861 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:25,522 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:26,108 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:26,716 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:27,317 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:27,912 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:28,689 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:29,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:29,945 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:30,462 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:55<01:22, 13.80s/it][WARNING|generation_utils.py:914] 2023-08-28 14:37:31,201 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:31,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:32,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:33,161 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:33,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:34,480 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:35,153 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:35,740 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:36,388 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:36,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:37,575 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:38,384 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:39,046 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:39,629 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:40,337 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:40,895 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:41,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:42,218 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:42,916 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:43,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:44,216 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:44,965 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:45,561 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:10<01:11, 14.27s/it][WARNING|generation_utils.py:914] 2023-08-28 14:37:46,257 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:46,856 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:47,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:48,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:48,635 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:49,152 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:49,771 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:50,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:51,021 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:51,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:52,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:52,870 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:53,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:54,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:54,702 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:55,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:55,858 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:56,425 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:57,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:57,754 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:58,309 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:37:58,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:23<00:55, 13.91s/it][WARNING|generation_utils.py:914] 2023-08-28 14:37:59,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:00,047 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:00,669 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:01,204 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:01,765 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:02,307 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:02,787 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:03,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:04,022 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:04,512 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:05,146 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:05,653 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:06,144 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:06,655 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:07,339 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:07,872 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:08,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:08,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:09,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:09,963 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:34<00:39, 13.13s/it][WARNING|generation_utils.py:914] 2023-08-28 14:38:10,897 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:11,414 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:11,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:12,514 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:13,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:13,778 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:14,377 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:14,932 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:15,430 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:15,975 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:16,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:17,200 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:17,760 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:18,311 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:18,880 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:19,436 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:19,997 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:20,638 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:21,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:21,784 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:22,498 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:23,089 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:23,717 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:48<00:26, 13.22s/it][WARNING|generation_utils.py:914] 2023-08-28 14:38:24,313 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:24,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:25,450 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:26,005 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:26,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:27,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:28,004 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:28,578 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:29,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:29,667 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:30,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:30,982 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:31,743 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:32,432 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:33,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:33,736 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:34,245 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:34,904 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:35,502 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:36,020 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:36,769 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:37,503 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:38,159 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:02<00:13, 13.59s/it][WARNING|generation_utils.py:914] 2023-08-28 14:38:38,720 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:39,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:39,955 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:40,637 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:41,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:41,886 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:42,396 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:43,015 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:43,758 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:44,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:44,930 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:45,469 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:45,967 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:46,569 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:47,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:47,707 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:48,206 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:48,766 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:49,262 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:49,746 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:50,250 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:50,875 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:51,358 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:51,929 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:52,496 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 14:38:53,045 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:17<00:00, 13.99s/it]Generating: 100%|██████████| 10/10 [02:17<00:00, 13.78s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:12,209 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:12,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:12,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:12,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:12,344 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:39:14,037 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:39:14,038 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:39:14,884 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:39:16,205 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:39:16,334 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:20,062 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:20,208 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:20,208 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:20,208 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:39:20,208 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:39:21,711 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:39:21,712 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:39:22,567 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:39:23,021 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:39:23,021 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
['Relation : director . Context : Later in the year , the film was shot with a cast consisting of Michael Douglas at the helm and Tom Cruise in cinematographer . Head Entity : " The Fifth Estate " , Tail Entity : Tom Cruise .\n']
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 118, 'raw': 128}
{'target': 600, 'success': 147, 'raw': 160}
{'target': 600, 'success': 174, 'raw': 192}
{'target': 600, 'success': 201, 'raw': 224}
{'target': 600, 'success': 229, 'raw': 256}
{'target': 600, 'success': 260, 'raw': 288}
{'target': 600, 'success': 291, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 352, 'raw': 384}
{'target': 600, 'success': 381, 'raw': 416}
{'target': 600, 'success': 411, 'raw': 448}
{'target': 600, 'success': 440, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 496, 'raw': 544}
{'target': 600, 'success': 525, 'raw': 576}
{'target': 600, 'success': 554, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 612, 'raw': 672}
{'prompt': 'Relation : director .', 'success_rate': 0.9107142857142857, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 91, 'raw': 96}
{'target': 600, 'success': 120, 'raw': 128}
{'target': 600, 'success': 149, 'raw': 160}
{'target': 600, 'success': 179, 'raw': 192}
{'target': 600, 'success': 208, 'raw': 224}
{'target': 600, 'success': 238, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 294, 'raw': 320}
{'target': 600, 'success': 324, 'raw': 352}
{'target': 600, 'success': 353, 'raw': 384}
{'target': 600, 'success': 383, 'raw': 416}
{'target': 600, 'success': 412, 'raw': 448}
{'target': 600, 'success': 441, 'raw': 480}
{'target': 600, 'success': 470, 'raw': 512}
{'target': 600, 'success': 498, 'raw': 544}
{'target': 600, 'success': 528, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 584, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.9122023809523809, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 58, 'raw': 64}
{'target': 600, 'success': 87, 'raw': 96}
{'target': 600, 'success': 113, 'raw': 128}
{'target': 600, 'success': 142, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 195, 'raw': 224}
{'target': 600, 'success': 223, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 304, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 359, 'raw': 416}
{'target': 600, 'success': 386, 'raw': 448}
{'target': 600, 'success': 415, 'raw': 480}
{'target': 600, 'success': 445, 'raw': 512}
{'target': 600, 'success': 471, 'raw': 544}
{'target': 600, 'success': 499, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 553, 'raw': 640}
{'target': 600, 'success': 581, 'raw': 672}
{'target': 600, 'success': 610, 'raw': 704}
{'prompt': 'Relation : mother .', 'success_rate': 0.8664772727272727, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 54, 'raw': 64}
{'target': 600, 'success': 84, 'raw': 96}
{'target': 600, 'success': 116, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 170, 'raw': 192}
{'target': 600, 'success': 198, 'raw': 224}
{'target': 600, 'success': 228, 'raw': 256}
{'target': 600, 'success': 257, 'raw': 288}
{'target': 600, 'success': 285, 'raw': 320}
{'target': 600, 'success': 315, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 376, 'raw': 416}
{'target': 600, 'success': 406, 'raw': 448}
{'target': 600, 'success': 436, 'raw': 480}
{'target': 600, 'success': 465, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 527, 'raw': 576}
{'target': 600, 'success': 556, 'raw': 608}
{'target': 600, 'success': 581, 'raw': 640}
{'target': 600, 'success': 609, 'raw': 672}
{'prompt': 'Relation : part of .', 'success_rate': 0.90625, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 26, 'raw': 32}
{'target': 600, 'success': 53, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 106, 'raw': 128}
{'target': 600, 'success': 132, 'raw': 160}
{'target': 600, 'success': 157, 'raw': 192}
{'target': 600, 'success': 187, 'raw': 224}
{'target': 600, 'success': 214, 'raw': 256}
{'target': 600, 'success': 238, 'raw': 288}
{'target': 600, 'success': 267, 'raw': 320}
{'target': 600, 'success': 296, 'raw': 352}
{'target': 600, 'success': 323, 'raw': 384}
{'target': 600, 'success': 351, 'raw': 416}
{'target': 600, 'success': 380, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 596, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : residence .', 'success_rate': 0.8505434782608695, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 59, 'raw': 64}
{'target': 600, 'success': 88, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 169, 'raw': 192}
{'target': 600, 'success': 197, 'raw': 224}
{'target': 600, 'success': 226, 'raw': 256}
{'target': 600, 'success': 251, 'raw': 288}
{'target': 600, 'success': 279, 'raw': 320}
{'target': 600, 'success': 309, 'raw': 352}
{'target': 600, 'success': 337, 'raw': 384}
{'target': 600, 'success': 367, 'raw': 416}
{'target': 600, 'success': 390, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 447, 'raw': 512}
{'target': 600, 'success': 475, 'raw': 544}
{'target': 600, 'success': 503, 'raw': 576}
{'target': 600, 'success': 531, 'raw': 608}
{'target': 600, 'success': 560, 'raw': 640}
{'target': 600, 'success': 589, 'raw': 672}
{'target': 600, 'success': 618, 'raw': 704}
{'prompt': 'Relation : developer .', 'success_rate': 0.8778409090909091, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 30, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 90, 'raw': 96}
{'target': 600, 'success': 122, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 216, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 278, 'raw': 288}
{'target': 600, 'success': 309, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 372, 'raw': 384}
{'target': 600, 'success': 403, 'raw': 416}
{'target': 600, 'success': 434, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 526, 'raw': 544}
{'target': 600, 'success': 557, 'raw': 576}
{'target': 600, 'success': 588, 'raw': 608}
{'target': 600, 'success': 618, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.965625, 'errors': {''}}
['Relation : member of political party . Context : Following his leadership election victory , the government appointed him a Minister of Transport and Transport Reform . Head Entity : Minister of Transport and Transport Reform , Tail Entity : Liberal .\n']
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 50, 'raw': 64}
{'target': 600, 'success': 75, 'raw': 96}
{'target': 600, 'success': 103, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 154, 'raw': 192}
{'target': 600, 'success': 179, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 231, 'raw': 288}
{'target': 600, 'success': 257, 'raw': 320}
{'target': 600, 'success': 283, 'raw': 352}
{'target': 600, 'success': 309, 'raw': 384}
{'target': 600, 'success': 335, 'raw': 416}
{'target': 600, 'success': 362, 'raw': 448}
{'target': 600, 'success': 386, 'raw': 480}
{'target': 600, 'success': 414, 'raw': 512}
{'target': 600, 'success': 442, 'raw': 544}
{'target': 600, 'success': 471, 'raw': 576}
{'target': 600, 'success': 499, 'raw': 608}
{'target': 600, 'success': 529, 'raw': 640}
{'target': 600, 'success': 559, 'raw': 672}
{'target': 600, 'success': 586, 'raw': 704}
{'target': 600, 'success': 614, 'raw': 736}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.8342391304347826, 'errors': {''}}
{'target': 600, 'success': 27, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 111, 'raw': 128}
{'target': 600, 'success': 141, 'raw': 160}
{'target': 600, 'success': 165, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 218, 'raw': 256}
{'target': 600, 'success': 245, 'raw': 288}
{'target': 600, 'success': 272, 'raw': 320}
{'target': 600, 'success': 300, 'raw': 352}
{'target': 600, 'success': 325, 'raw': 384}
{'target': 600, 'success': 350, 'raw': 416}
{'target': 600, 'success': 379, 'raw': 448}
{'target': 600, 'success': 404, 'raw': 480}
{'target': 600, 'success': 433, 'raw': 512}
{'target': 600, 'success': 463, 'raw': 544}
{'target': 600, 'success': 492, 'raw': 576}
{'target': 600, 'success': 516, 'raw': 608}
{'target': 600, 'success': 544, 'raw': 640}
{'target': 600, 'success': 571, 'raw': 672}
{'target': 600, 'success': 599, 'raw': 704}
{'target': 600, 'success': 626, 'raw': 736}
{'prompt': 'Relation : operator .', 'success_rate': 0.8505434782608695, 'errors': {''}}
{'target': 600, 'success': 24, 'raw': 32}
{'target': 600, 'success': 43, 'raw': 64}
{'target': 600, 'success': 66, 'raw': 96}
{'target': 600, 'success': 87, 'raw': 128}
{'target': 600, 'success': 113, 'raw': 160}
{'target': 600, 'success': 136, 'raw': 192}
{'target': 600, 'success': 161, 'raw': 224}
{'target': 600, 'success': 186, 'raw': 256}
{'target': 600, 'success': 208, 'raw': 288}
{'target': 600, 'success': 232, 'raw': 320}
{'target': 600, 'success': 252, 'raw': 352}
{'target': 600, 'success': 274, 'raw': 384}
{'target': 600, 'success': 300, 'raw': 416}
{'target': 600, 'success': 324, 'raw': 448}
{'target': 600, 'success': 348, 'raw': 480}
{'target': 600, 'success': 371, 'raw': 512}
{'target': 600, 'success': 394, 'raw': 544}
{'target': 600, 'success': 416, 'raw': 576}
{'target': 600, 'success': 437, 'raw': 608}
{'target': 600, 'success': 461, 'raw': 640}
{'target': 600, 'success': 482, 'raw': 672}
{'target': 600, 'success': 505, 'raw': 704}
{'target': 600, 'success': 529, 'raw': 736}
{'target': 600, 'success': 551, 'raw': 768}
{'target': 600, 'success': 575, 'raw': 800}
{'target': 600, 'success': 600, 'raw': 832}
{'prompt': 'Relation : position held .', 'success_rate': 0.7211538461538461, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/3.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/3_ext.jsonl'}}
estimate vocab size: 8806
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 8906, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.40it/s]Extractor Estimating: 2it [00:01,  1.32it/s]Extractor Estimating: 3it [00:02,  1.26it/s]Extractor Estimating: 4it [00:03,  1.32it/s]Extractor Estimating: 5it [00:03,  1.39it/s]Extractor Estimating: 6it [00:04,  1.48it/s]Extractor Estimating: 7it [00:04,  1.53it/s]Extractor Estimating: 8it [00:05,  1.39it/s]Extractor Estimating: 9it [00:06,  1.41it/s]Extractor Estimating: 10it [00:07,  1.45it/s]Extractor Estimating: 11it [00:07,  1.48it/s]Extractor Estimating: 12it [00:08,  1.51it/s]Extractor Estimating: 13it [00:09,  1.46it/s]Extractor Estimating: 14it [00:09,  1.47it/s]Extractor Estimating: 15it [00:10,  1.48it/s]Extractor Estimating: 16it [00:11,  1.52it/s]Extractor Estimating: 17it [00:11,  1.56it/s]Extractor Estimating: 18it [00:12,  1.53it/s]Extractor Estimating: 19it [00:12,  1.57it/s]Extractor Estimating: 20it [00:13,  1.54it/s]Extractor Estimating: 21it [00:14,  1.52it/s]Extractor Estimating: 22it [00:14,  1.54it/s]Extractor Estimating: 23it [00:15,  1.49it/s]Extractor Estimating: 24it [00:16,  1.50it/s]Extractor Estimating: 25it [00:16,  1.50it/s]Extractor Estimating: 26it [00:17,  1.53it/s]Extractor Estimating: 27it [00:18,  1.61it/s]Extractor Estimating: 28it [00:18,  1.59it/s]Extractor Estimating: 29it [00:19,  1.69it/s]Extractor Estimating: 30it [00:19,  1.73it/s]Extractor Estimating: 31it [00:20,  1.76it/s]Extractor Estimating: 32it [00:20,  1.78it/s]Extractor Estimating: 33it [00:21,  1.85it/s]Extractor Estimating: 34it [00:22,  1.67it/s]Extractor Estimating: 35it [00:22,  1.73it/s]Extractor Estimating: 36it [00:23,  1.81it/s]Extractor Estimating: 37it [00:23,  1.81it/s]Extractor Estimating: 38it [00:24,  1.87it/s]Extractor Estimating: 39it [00:24,  1.93it/s]Extractor Estimating: 40it [00:25,  1.70it/s]Extractor Estimating: 41it [00:25,  1.80it/s]Extractor Estimating: 42it [00:26,  1.77it/s]Extractor Estimating: 43it [00:27,  1.77it/s]Extractor Estimating: 44it [00:27,  1.84it/s]Extractor Estimating: 45it [00:28,  1.86it/s]Extractor Estimating: 46it [00:28,  1.56it/s]Extractor Estimating: 47it [00:29,  1.62it/s]Extractor Estimating: 48it [00:30,  1.62it/s]Extractor Estimating: 49it [00:30,  1.66it/s]Extractor Estimating: 50it [00:31,  1.71it/s]Extractor Estimating: 51it [00:31,  1.60it/s]Extractor Estimating: 52it [00:32,  1.65it/s]Extractor Estimating: 53it [00:33,  1.49it/s]Extractor Estimating: 54it [00:33,  1.55it/s]Extractor Estimating: 55it [00:34,  1.61it/s]Extractor Estimating: 56it [00:35,  1.62it/s]Extractor Estimating: 57it [00:35,  1.66it/s]Extractor Estimating: 58it [00:36,  1.66it/s]Extractor Estimating: 59it [00:36,  1.69it/s]Extractor Estimating: 60it [00:37,  1.64it/s]Extractor Estimating: 61it [00:38,  1.65it/s]Extractor Estimating: 62it [00:38,  1.67it/s]Extractor Estimating: 63it [00:39,  1.70it/s]Extractor Estimating: 64it [00:39,  1.59it/s]Extractor Estimating: 65it [00:40,  1.58it/s]Extractor Estimating: 66it [00:41,  1.63it/s]Extractor Estimating: 67it [00:41,  1.64it/s]Extractor Estimating: 68it [00:42,  1.63it/s]Extractor Estimating: 69it [00:43,  1.55it/s]Extractor Estimating: 70it [00:43,  1.63it/s]Extractor Estimating: 71it [00:44,  1.65it/s]Extractor Estimating: 72it [00:44,  1.69it/s]Extractor Estimating: 73it [00:45,  1.71it/s]Extractor Estimating: 74it [00:45,  1.70it/s]Extractor Estimating: 75it [00:46,  1.59it/s]Extractor Estimating: 76it [00:47,  1.58it/s]Extractor Estimating: 77it [00:48,  1.54it/s]Extractor Estimating: 78it [00:48,  1.57it/s]Extractor Estimating: 79it [00:49,  1.58it/s]Extractor Estimating: 80it [00:50,  1.42it/s]Extractor Estimating: 81it [00:50,  1.45it/s]Extractor Estimating: 82it [00:51,  1.51it/s]Extractor Estimating: 83it [00:52,  1.51it/s]Extractor Estimating: 84it [00:52,  1.52it/s]Extractor Estimating: 85it [00:53,  1.39it/s]Extractor Estimating: 86it [00:54,  1.40it/s]Extractor Estimating: 87it [00:54,  1.42it/s]Extractor Estimating: 88it [00:55,  1.47it/s]Extractor Estimating: 89it [00:56,  1.53it/s]Extractor Estimating: 90it [00:56,  1.47it/s]Extractor Estimating: 91it [00:57,  1.50it/s]Extractor Estimating: 92it [00:58,  1.54it/s]Extractor Estimating: 93it [00:58,  1.57it/s]Extractor Estimating: 94it [00:59,  1.52it/s]Extractor Estimating: 95it [01:00,  1.42it/s]Extractor Estimating: 96it [01:00,  1.45it/s]Extractor Estimating: 97it [01:01,  1.47it/s]Extractor Estimating: 98it [01:02,  1.49it/s]Extractor Estimating: 99it [01:02,  1.58it/s]Extractor Estimating: 100it [01:03,  1.54it/s]Extractor Estimating: 101it [01:04,  1.49it/s]Extractor Estimating: 102it [01:04,  1.57it/s]Extractor Estimating: 103it [01:05,  1.64it/s]Extractor Estimating: 104it [01:05,  1.59it/s]Extractor Estimating: 105it [01:06,  1.60it/s]Extractor Estimating: 106it [01:07,  1.47it/s]Extractor Estimating: 107it [01:07,  1.55it/s]Extractor Estimating: 108it [01:08,  1.54it/s]Extractor Estimating: 109it [01:09,  1.55it/s]Extractor Estimating: 110it [01:09,  1.61it/s]Extractor Estimating: 111it [01:10,  1.53it/s]Extractor Estimating: 112it [01:11,  1.57it/s]Extractor Estimating: 113it [01:11,  1.53it/s]Extractor Estimating: 114it [01:12,  1.55it/s]Extractor Estimating: 115it [01:13,  1.59it/s]Extractor Estimating: 116it [01:14,  1.34it/s]Extractor Estimating: 117it [01:14,  1.42it/s]Extractor Estimating: 118it [01:15,  1.50it/s]Extractor Estimating: 119it [01:15,  1.50it/s]Extractor Estimating: 120it [01:16,  1.54it/s]Extractor Estimating: 121it [01:17,  1.45it/s]Extractor Estimating: 122it [01:17,  1.49it/s]Extractor Estimating: 123it [01:18,  1.54it/s]Extractor Estimating: 124it [01:19,  1.55it/s]Extractor Estimating: 125it [01:19,  1.56it/s]Extractor Estimating: 126it [01:20,  1.45it/s]Extractor Estimating: 127it [01:21,  1.46it/s]Extractor Estimating: 128it [01:21,  1.46it/s]Extractor Estimating: 129it [01:22,  1.50it/s]Extractor Estimating: 130it [01:23,  1.52it/s]Extractor Estimating: 131it [01:24,  1.39it/s]Extractor Estimating: 132it [01:24,  1.40it/s]Extractor Estimating: 133it [01:25,  1.46it/s]Extractor Estimating: 134it [01:26,  1.51it/s]Extractor Estimating: 135it [01:26,  1.44it/s]Extractor Estimating: 136it [01:27,  1.38it/s]Extractor Estimating: 137it [01:28,  1.41it/s]Extractor Estimating: 138it [01:28,  1.44it/s]Extractor Estimating: 139it [01:29,  1.48it/s]Extractor Estimating: 140it [01:30,  1.51it/s]Extractor Estimating: 141it [01:30,  1.46it/s]Extractor Estimating: 142it [01:31,  1.49it/s]Extractor Estimating: 143it [01:32,  1.51it/s]Extractor Estimating: 144it [01:32,  1.51it/s]Extractor Estimating: 145it [01:33,  1.53it/s]Extractor Estimating: 146it [01:34,  1.47it/s]Extractor Estimating: 147it [01:35,  1.33it/s]Extractor Estimating: 148it [01:35,  1.41it/s]Extractor Estimating: 149it [01:36,  1.42it/s]Extractor Estimating: 150it [01:37,  1.46it/s]Extractor Estimating: 151it [01:37,  1.48it/s]Extractor Estimating: 152it [01:38,  1.59it/s]Extractor Estimating: 153it [01:38,  1.73it/s]Extractor Estimating: 154it [01:39,  1.84it/s]Extractor Estimating: 155it [01:39,  1.87it/s]Extractor Estimating: 156it [01:40,  1.98it/s]Extractor Estimating: 157it [01:40,  1.85it/s]Extractor Estimating: 158it [01:41,  1.95it/s]Extractor Estimating: 159it [01:41,  1.99it/s]Extractor Estimating: 160it [01:42,  2.02it/s]Extractor Estimating: 161it [01:42,  2.07it/s]Extractor Estimating: 162it [01:43,  2.15it/s]Extractor Estimating: 163it [01:43,  2.08it/s]Extractor Estimating: 164it [01:44,  1.96it/s]Extractor Estimating: 165it [01:44,  2.05it/s]Extractor Estimating: 166it [01:45,  2.09it/s]Extractor Estimating: 167it [01:45,  2.06it/s]Extractor Estimating: 168it [01:45,  2.12it/s]Extractor Estimating: 169it [01:46,  2.06it/s]Extractor Estimating: 170it [01:46,  2.13it/s]Extractor Estimating: 171it [01:47,  1.90it/s]Extractor Estimating: 172it [01:48,  1.97it/s]Extractor Estimating: 173it [01:48,  2.03it/s]Extractor Estimating: 174it [01:48,  2.08it/s]Extractor Estimating: 175it [01:49,  2.12it/s]Extractor Estimating: 176it [01:50,  1.98it/s]Extractor Estimating: 177it [01:50,  1.78it/s]Extractor Estimating: 178it [01:51,  1.78it/s]Extractor Estimating: 179it [01:51,  1.79it/s]Extractor Estimating: 180it [01:52,  1.77it/s]Extractor Estimating: 181it [01:52,  1.74it/s]Extractor Estimating: 182it [01:53,  1.73it/s]Extractor Estimating: 183it [01:54,  1.62it/s]Extractor Estimating: 184it [01:54,  1.71it/s]Extractor Estimating: 185it [01:55,  1.67it/s]Extractor Estimating: 186it [01:56,  1.68it/s]Extractor Estimating: 187it [01:56,  1.72it/s]Extractor Estimating: 188it [01:57,  1.71it/s]Extractor Estimating: 189it [01:57,  1.55it/s]Extractor Estimating: 190it [01:58,  1.63it/s]Extractor Estimating: 191it [01:59,  1.66it/s]Extractor Estimating: 192it [01:59,  1.66it/s]Extractor Estimating: 193it [02:00,  1.70it/s]Extractor Estimating: 194it [02:01,  1.52it/s]Extractor Estimating: 195it [02:01,  1.63it/s]Extractor Estimating: 196it [02:02,  1.67it/s]Extractor Estimating: 197it [02:02,  1.61it/s]Extractor Estimating: 198it [02:03,  1.64it/s]Extractor Estimating: 199it [02:04,  1.54it/s]Extractor Estimating: 200it [02:04,  1.54it/s]Extractor Estimating: 201it [02:05,  1.56it/s]Extractor Estimating: 202it [02:05,  1.59it/s]Extractor Estimating: 203it [02:06,  1.62it/s]Extractor Estimating: 204it [02:07,  1.63it/s]Extractor Estimating: 205it [02:07,  1.62it/s]Extractor Estimating: 206it [02:08,  1.58it/s]Extractor Estimating: 207it [02:09,  1.54it/s]Extractor Estimating: 208it [02:09,  1.56it/s]Extractor Estimating: 209it [02:10,  1.60it/s]Extractor Estimating: 210it [02:11,  1.50it/s]Extractor Estimating: 211it [02:11,  1.61it/s]Extractor Estimating: 212it [02:12,  1.55it/s]Extractor Estimating: 213it [02:13,  1.52it/s]Extractor Estimating: 214it [02:13,  1.51it/s]Extractor Estimating: 215it [02:14,  1.54it/s]Extractor Estimating: 216it [02:14,  1.57it/s]Extractor Estimating: 217it [02:15,  1.52it/s]Extractor Estimating: 218it [02:16,  1.61it/s]Extractor Estimating: 219it [02:16,  1.62it/s]Extractor Estimating: 220it [02:17,  1.61it/s]Extractor Estimating: 221it [02:18,  1.63it/s]Extractor Estimating: 222it [02:18,  1.49it/s]Extractor Estimating: 223it [02:19,  1.53it/s]Extractor Estimating: 224it [02:20,  1.56it/s]Extractor Estimating: 225it [02:20,  1.53it/s]Extractor Estimating: 226it [02:21,  1.59it/s]Extractor Estimating: 227it [02:21,  1.56it/s]Extractor Estimating: 228it [02:22,  1.65it/s]Extractor Estimating: 229it [02:23,  1.70it/s]Extractor Estimating: 230it [02:23,  1.70it/s]Extractor Estimating: 231it [02:24,  1.72it/s]Extractor Estimating: 232it [02:24,  1.72it/s]Extractor Estimating: 233it [02:25,  1.61it/s]Extractor Estimating: 234it [02:26,  1.70it/s]Extractor Estimating: 235it [02:26,  1.73it/s]Extractor Estimating: 236it [02:27,  1.72it/s]Extractor Estimating: 237it [02:27,  1.75it/s]Extractor Estimating: 238it [02:28,  1.71it/s]Extractor Estimating: 239it [02:29,  1.58it/s]Extractor Estimating: 240it [02:29,  1.63it/s]Extractor Estimating: 241it [02:30,  1.70it/s]Extractor Estimating: 242it [02:30,  1.71it/s]Extractor Estimating: 243it [02:31,  1.73it/s]Extractor Estimating: 244it [02:31,  1.70it/s]Extractor Estimating: 245it [02:32,  1.59it/s]Extractor Estimating: 246it [02:33,  1.68it/s]Extractor Estimating: 247it [02:33,  1.71it/s]Extractor Estimating: 248it [02:34,  1.70it/s]Extractor Estimating: 249it [02:34,  1.71it/s]Extractor Estimating: 250it [02:35,  1.66it/s]Extractor Estimating: 250it [02:36,  1.60it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,651 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:35,760 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 14:42:37,020 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 14:42:37,021 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:42:37,882 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 14:42:39,216 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:42:39,216 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:42,897 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:43,023 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:43,024 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:43,024 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 14:42:43,024 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 14:42:44,437 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 14:42:44,438 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 14:42:45,255 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 14:42:45,630 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 14:42:45,630 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 16:07:53,763 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 16:07:55,639 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/3_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/3.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 0.8, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 4000, 'num_train': 999}
num of filtered data: 4998 mean pseudo reward: 0.9449039621200827
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/3.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 19546
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 19646, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter3/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=19646, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.985, loss:564.2839
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.976, loss:548.8664
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.967, loss:533.6650
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.988, loss:531.2828
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.971, loss:511.7116
>> valid entity prec:0.5514, rec:0.6242, f1:0.5856
>> valid relation prec:0.4800, rec:0.1404, f1:0.2173
>> valid relation with NER prec:0.4800, rec:0.1404, f1:0.2173
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.304, loss:527.7209
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 0.973, loss:514.1561
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.960, loss:524.0177
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.972, loss:497.5263
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.960, loss:534.5347
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5831, rec:0.5454, f1:0.5636
>> valid relation prec:0.4921, rec:0.1158, f1:0.1875
>> valid relation with NER prec:0.4921, rec:0.1158, f1:0.1875
g_step 1100, step 55, avg_time 2.243, loss:516.7049
g_step 1200, step 155, avg_time 0.974, loss:501.9610
g_step 1300, step 46, avg_time 0.965, loss:494.3919
g_step 1400, step 146, avg_time 0.962, loss:489.2441
g_step 1500, step 37, avg_time 0.957, loss:478.2108
>> valid entity prec:0.5589, rec:0.6175, f1:0.5868
>> valid relation prec:0.3523, rec:0.1078, f1:0.1651
>> valid relation with NER prec:0.3523, rec:0.1078, f1:0.1651
new max entity f1 on valid!
g_step 1600, step 137, avg_time 2.237, loss:463.7231
g_step 1700, step 28, avg_time 0.971, loss:469.0906
g_step 1800, step 128, avg_time 0.973, loss:448.0800
g_step 1900, step 19, avg_time 0.973, loss:430.4522
g_step 2000, step 119, avg_time 0.977, loss:419.7207
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5721, rec:0.5471, f1:0.5593
>> valid relation prec:0.3512, rec:0.0824, f1:0.1334
>> valid relation with NER prec:0.3512, rec:0.0824, f1:0.1334
g_step 2100, step 10, avg_time 2.213, loss:414.1554
g_step 2200, step 110, avg_time 0.983, loss:400.3430
g_step 2300, step 1, avg_time 0.953, loss:400.4531
g_step 2400, step 101, avg_time 0.978, loss:379.8074
g_step 2500, step 201, avg_time 0.968, loss:383.1998
>> valid entity prec:0.5965, rec:0.5905, f1:0.5934
>> valid relation prec:0.3337, rec:0.0975, f1:0.1509
>> valid relation with NER prec:0.3337, rec:0.0975, f1:0.1509
new max entity f1 on valid!
g_step 2600, step 92, avg_time 2.239, loss:361.8708
g_step 2700, step 192, avg_time 0.986, loss:359.5890
g_step 2800, step 83, avg_time 0.960, loss:352.4210
g_step 2900, step 183, avg_time 0.981, loss:352.9736
g_step 3000, step 74, avg_time 0.974, loss:330.6445
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.6104, rec:0.5472, f1:0.5771
>> valid relation prec:0.3187, rec:0.1015, f1:0.1540
>> valid relation with NER prec:0.3187, rec:0.1015, f1:0.1540
g_step 3100, step 174, avg_time 2.243, loss:341.6110
g_step 3200, step 65, avg_time 0.958, loss:320.4216
g_step 3300, step 165, avg_time 0.972, loss:325.8317
g_step 3400, step 56, avg_time 0.972, loss:322.7547
g_step 3500, step 156, avg_time 0.967, loss:315.0223
>> valid entity prec:0.6036, rec:0.5392, f1:0.5696
>> valid relation prec:0.3185, rec:0.1047, f1:0.1576
>> valid relation with NER prec:0.3185, rec:0.1047, f1:0.1576
g_step 3600, step 47, avg_time 2.240, loss:307.4575
g_step 3700, step 147, avg_time 0.965, loss:301.1534
g_step 3800, step 38, avg_time 0.971, loss:302.8638
g_step 3900, step 138, avg_time 0.972, loss:285.9577
g_step 4000, step 29, avg_time 0.961, loss:284.1143
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5818, rec:0.6305, f1:0.6052
>> valid relation prec:0.3281, rec:0.1421, f1:0.1983
>> valid relation with NER prec:0.3281, rec:0.1421, f1:0.1983
new max entity f1 on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 4100, step 129, avg_time 2.333, loss:271.1983
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 16:07:55 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 16:07:55 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_16-07-53_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 16:07:58 - WARNING - datasets.builder -   Using custom data configuration default-76e8ed1ca76b4d78
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-76e8ed1ca76b4d78/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 16:08:11,461 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:08:11,660 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:08:11,660 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:08:11,661 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:08:12,371 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:08:12,698 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:08:12,698 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:08:12,698 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:08:12,698 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:08:12,698 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:08:12,698 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 16:08:14,631 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:08:18,395 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 16:08:18,508 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-76e8ed1ca76b4d78/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.46ba/s] 40%|████      | 2/5 [00:00<00:01,  2.51ba/s] 60%|██████    | 3/5 [00:01<00:00,  3.27ba/s] 80%|████████  | 4/5 [00:01<00:00,  3.78ba/s]100%|██████████| 5/5 [00:01<00:00,  4.13ba/s]100%|██████████| 5/5 [00:01<00:00,  3.37ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:02,  1.10ba/s] 50%|█████     | 2/4 [00:01<00:01,  1.98ba/s] 75%|███████▌  | 3/4 [00:01<00:00,  2.67ba/s]100%|██████████| 4/4 [00:01<00:00,  3.69ba/s]100%|██████████| 4/4 [00:01<00:00,  2.73ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:02,  1.82ba/s] 60%|██████    | 3/5 [00:00<00:00,  4.83ba/s]100%|██████████| 5/5 [00:00<00:00,  6.86ba/s]100%|██████████| 5/5 [00:00<00:00,  5.56ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:01,  1.67ba/s] 50%|█████     | 2/4 [00:00<00:00,  3.20ba/s]100%|██████████| 4/4 [00:00<00:00,  6.32ba/s]100%|██████████| 4/4 [00:00<00:00,  4.74ba/s]
[INFO|trainer.py:414] 2023-08-28 16:08:32,948 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 16:08:33,711 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 16:08:33,712 >>   Num examples = 4999
[INFO|trainer.py:1149] 2023-08-28 16:08:33,712 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 16:08:33,712 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 16:08:33,712 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 16:08:33,712 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 16:08:33,712 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:03<21:37,  3.33s/it]  1%|          | 2/390 [00:05<16:59,  2.63s/it]  1%|          | 3/390 [00:06<12:14,  1.90s/it]  1%|          | 4/390 [00:07<09:56,  1.54s/it]  1%|▏         | 5/390 [00:08<07:51,  1.23s/it]  2%|▏         | 6/390 [00:08<06:05,  1.05it/s]  2%|▏         | 7/390 [00:09<05:00,  1.27it/s]  2%|▏         | 8/390 [00:09<04:24,  1.45it/s]  2%|▏         | 9/390 [00:10<04:07,  1.54it/s]  3%|▎         | 10/390 [00:10<03:23,  1.86it/s]  3%|▎         | 11/390 [00:10<02:54,  2.18it/s]  3%|▎         | 12/390 [00:10<02:33,  2.46it/s]  3%|▎         | 13/390 [00:11<02:19,  2.70it/s]  4%|▎         | 14/390 [00:11<02:09,  2.91it/s]  4%|▍         | 15/390 [00:11<02:02,  3.06it/s]  4%|▍         | 16/390 [00:12<01:57,  3.18it/s]  4%|▍         | 17/390 [00:12<01:53,  3.27it/s]  5%|▍         | 18/390 [00:12<01:51,  3.33it/s]  5%|▍         | 19/390 [00:13<02:07,  2.91it/s]  5%|▌         | 20/390 [00:13<02:00,  3.07it/s]  5%|▌         | 21/390 [00:13<01:55,  3.19it/s]  6%|▌         | 22/390 [00:13<01:52,  3.27it/s]  6%|▌         | 23/390 [00:14<01:49,  3.34it/s]  6%|▌         | 24/390 [00:14<01:48,  3.38it/s]  6%|▋         | 25/390 [00:14<01:46,  3.41it/s]  7%|▋         | 26/390 [00:15<02:07,  2.85it/s]  7%|▋         | 27/390 [00:15<02:00,  3.01it/s]  7%|▋         | 28/390 [00:15<01:55,  3.14it/s]  7%|▋         | 29/390 [00:16<02:06,  2.86it/s]  8%|▊         | 30/390 [00:16<01:59,  3.02it/s]  8%|▊         | 31/390 [00:16<01:54,  3.15it/s]  8%|▊         | 32/390 [00:17<01:50,  3.25it/s]  8%|▊         | 33/390 [00:17<01:47,  3.31it/s]  9%|▊         | 34/390 [00:17<01:45,  3.37it/s]  9%|▉         | 35/390 [00:18<01:44,  3.40it/s]  9%|▉         | 36/390 [00:18<01:43,  3.43it/s]  9%|▉         | 37/390 [00:18<01:42,  3.45it/s] 10%|▉         | 38/390 [00:18<01:41,  3.46it/s] 10%|█         | 39/390 [00:19<02:01,  2.88it/s] 10%|█         | 40/390 [00:19<01:55,  3.04it/s] 11%|█         | 41/390 [00:19<01:50,  3.17it/s] 11%|█         | 42/390 [00:20<01:46,  3.25it/s] 11%|█         | 43/390 [00:20<02:26,  2.36it/s] 11%|█▏        | 44/390 [00:22<03:47,  1.52it/s] 12%|█▏        | 45/390 [00:22<03:23,  1.70it/s] 12%|█▏        | 46/390 [00:22<02:52,  2.00it/s] 12%|█▏        | 47/390 [00:23<02:29,  2.29it/s] 12%|█▏        | 48/390 [00:23<02:14,  2.54it/s] 13%|█▎        | 49/390 [00:23<02:03,  2.76it/s] 13%|█▎        | 50/390 [00:23<01:55,  2.93it/s] 13%|█▎        | 51/390 [00:24<01:50,  3.07it/s] 13%|█▎        | 52/390 [00:24<01:46,  3.18it/s] 14%|█▎        | 53/390 [00:24<01:43,  3.27it/s] 14%|█▍        | 54/390 [00:25<01:40,  3.33it/s] 14%|█▍        | 55/390 [00:25<01:52,  2.97it/s] 14%|█▍        | 56/390 [00:25<01:47,  3.11it/s] 15%|█▍        | 57/390 [00:26<02:07,  2.60it/s] 15%|█▍        | 58/390 [00:26<01:57,  2.82it/s] 15%|█▌        | 59/390 [00:26<01:50,  2.99it/s] 15%|█▌        | 60/390 [00:27<01:45,  3.13it/s] 16%|█▌        | 61/390 [00:27<01:41,  3.23it/s] 16%|█▌        | 62/390 [00:27<01:39,  3.30it/s] 16%|█▌        | 63/390 [00:28<01:37,  3.36it/s] 16%|█▋        | 64/390 [00:28<01:35,  3.40it/s] 17%|█▋        | 65/390 [00:28<01:34,  3.42it/s] 17%|█▋        | 66/390 [00:28<01:34,  3.44it/s] 17%|█▋        | 67/390 [00:29<01:51,  2.89it/s] 17%|█▋        | 68/390 [00:29<01:45,  3.05it/s] 18%|█▊        | 69/390 [00:30<01:41,  3.17it/s] 18%|█▊        | 70/390 [00:30<01:38,  3.26it/s] 18%|█▊        | 71/390 [00:30<01:36,  3.32it/s] 18%|█▊        | 72/390 [00:30<01:34,  3.37it/s] 19%|█▊        | 73/390 [00:31<01:33,  3.41it/s] 19%|█▉        | 74/390 [00:31<01:32,  3.43it/s] 19%|█▉        | 75/390 [00:31<01:31,  3.45it/s] 19%|█▉        | 76/390 [00:32<01:30,  3.46it/s] 20%|█▉        | 77/390 [00:32<01:48,  2.88it/s] 20%|██        | 78/390 [00:32<01:42,  3.04it/s][INFO|trainer.py:2140] 2023-08-28 16:09:06,644 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:09:06,644 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 16:09:06,644 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.64it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.73it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.82it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.17it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.61it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.35it/s][A
  9%|▊         | 38/438 [00:00<00:08, 46.01it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.37it/s][A
 11%|█         | 48/438 [00:01<00:08, 44.94it/s][A
 12%|█▏        | 53/438 [00:01<00:11, 32.16it/s][A
 13%|█▎        | 57/438 [00:01<00:11, 31.88it/s][A
 14%|█▍        | 62/438 [00:01<00:10, 35.73it/s][A
 15%|█▌        | 67/438 [00:01<00:09, 38.28it/s][A
 16%|█▋        | 72/438 [00:01<00:09, 40.25it/s][A
 18%|█▊        | 77/438 [00:01<00:08, 41.76it/s][A
 19%|█▊        | 82/438 [00:01<00:08, 42.88it/s][A
 20%|█▉        | 87/438 [00:02<00:08, 43.40it/s][A
 21%|██        | 92/438 [00:02<00:10, 33.12it/s][A
 22%|██▏       | 97/438 [00:02<00:09, 36.11it/s][A
 23%|██▎       | 102/438 [00:02<00:08, 38.58it/s][A
 24%|██▍       | 107/438 [00:02<00:08, 40.43it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 41.90it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 42.85it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 43.66it/s][A
 29%|██▉       | 127/438 [00:03<00:07, 44.07it/s][A
 30%|███       | 132/438 [00:03<00:06, 43.97it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.01it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.40it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 44.72it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 44.95it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.07it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.29it/s][A
 38%|███▊      | 167/438 [00:03<00:05, 45.24it/s][A
 39%|███▉      | 172/438 [00:04<00:05, 45.04it/s][A
 40%|████      | 177/438 [00:04<00:07, 35.00it/s][A
 42%|████▏     | 182/438 [00:04<00:07, 34.40it/s][A
 43%|████▎     | 187/438 [00:04<00:06, 37.60it/s][A
 44%|████▍     | 192/438 [00:04<00:06, 39.77it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 41.32it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 42.59it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 43.36it/s][A
 48%|████▊     | 212/438 [00:05<00:05, 43.92it/s][A
 50%|████▉     | 217/438 [00:05<00:06, 33.70it/s][A
 51%|█████     | 222/438 [00:05<00:05, 36.59it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 38.93it/s][A
 53%|█████▎    | 232/438 [00:05<00:05, 40.67it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 41.96it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 42.97it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 43.66it/s][A
 58%|█████▊    | 252/438 [00:06<00:04, 44.11it/s][A
 59%|█████▊    | 257/438 [00:06<00:04, 44.15it/s][A
 60%|█████▉    | 262/438 [00:06<00:03, 44.07it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.20it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.55it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.83it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 45.09it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 45.17it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.32it/s][A
 68%|██████▊   | 297/438 [00:07<00:03, 45.12it/s][A
 69%|██████▉   | 302/438 [00:07<00:03, 34.91it/s][A
 70%|██████▉   | 306/438 [00:07<00:03, 35.05it/s][A
 71%|███████   | 311/438 [00:07<00:03, 38.26it/s][A
 72%|███████▏  | 316/438 [00:07<00:03, 40.31it/s][A
 73%|███████▎  | 321/438 [00:07<00:02, 41.79it/s][A
 74%|███████▍  | 326/438 [00:07<00:02, 42.96it/s][A
 76%|███████▌  | 331/438 [00:07<00:02, 43.75it/s][A
 77%|███████▋  | 336/438 [00:08<00:02, 44.32it/s][A
 78%|███████▊  | 341/438 [00:08<00:02, 44.20it/s][A
 79%|███████▉  | 346/438 [00:08<00:02, 33.75it/s][A
 80%|████████  | 351/438 [00:08<00:02, 36.66it/s][A
 81%|████████▏ | 356/438 [00:08<00:02, 39.02it/s][A
 82%|████████▏ | 361/438 [00:08<00:01, 40.86it/s][A
 84%|████████▎ | 366/438 [00:08<00:01, 42.19it/s][A
 85%|████████▍ | 371/438 [00:08<00:01, 43.22it/s][A
 86%|████████▌ | 376/438 [00:09<00:01, 44.02it/s][A
 87%|████████▋ | 381/438 [00:09<00:01, 44.34it/s][A
 88%|████████▊ | 386/438 [00:09<00:01, 44.27it/s][A
 89%|████████▉ | 391/438 [00:09<00:01, 44.15it/s][A
 90%|█████████ | 396/438 [00:09<00:00, 44.25it/s][A
 92%|█████████▏| 401/438 [00:09<00:00, 44.62it/s][A
 93%|█████████▎| 406/438 [00:09<00:00, 45.02it/s][A
 94%|█████████▍| 411/438 [00:09<00:00, 45.25it/s][A
 95%|█████████▍| 416/438 [00:09<00:00, 45.38it/s][A
 96%|█████████▌| 421/438 [00:10<00:00, 45.42it/s][A
 97%|█████████▋| 426/438 [00:10<00:00, 45.22it/s][A
 98%|█████████▊| 431/438 [00:10<00:00, 44.97it/s][A
100%|█████████▉| 436/438 [00:10<00:00, 44.59it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:10<00:00, 44.59it/s][A 20%|██        | 78/390 [00:43<01:42,  3.04it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:09:18,713 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 16:09:19,616 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:10:00,205 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:10:01,640 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:10:02,087 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [03:04<3:56:31, 45.63s/it] 21%|██        | 80/390 [03:04<2:45:47, 32.09s/it] 21%|██        | 81/390 [03:04<1:56:07, 22.55s/it] 21%|██        | 82/390 [03:05<1:21:28, 15.87s/it] 21%|██▏       | 83/390 [03:05<57:17, 11.20s/it]   22%|██▏       | 84/390 [03:05<40:25,  7.93s/it] 22%|██▏       | 85/390 [03:06<28:38,  5.63s/it] 22%|██▏       | 86/390 [03:06<20:25,  4.03s/it] 22%|██▏       | 87/390 [03:06<14:41,  2.91s/it] 23%|██▎       | 88/390 [03:06<10:41,  2.12s/it] 23%|██▎       | 89/390 [03:07<07:53,  1.57s/it] 23%|██▎       | 90/390 [03:07<06:12,  1.24s/it] 23%|██▎       | 91/390 [03:07<04:45,  1.05it/s] 24%|██▎       | 92/390 [03:08<03:45,  1.32it/s] 24%|██▍       | 93/390 [03:08<03:03,  1.62it/s] 24%|██▍       | 94/390 [03:08<02:33,  1.93it/s] 24%|██▍       | 95/390 [03:09<02:12,  2.22it/s] 25%|██▍       | 96/390 [03:09<01:58,  2.49it/s] 25%|██▍       | 97/390 [03:09<01:47,  2.71it/s] 25%|██▌       | 98/390 [03:10<01:40,  2.90it/s] 25%|██▌       | 99/390 [03:10<01:35,  3.05it/s] 26%|██▌       | 100/390 [03:10<01:51,  2.61it/s] 26%|██▌       | 101/390 [03:11<01:42,  2.81it/s] 26%|██▌       | 102/390 [03:11<01:36,  2.98it/s] 26%|██▋       | 103/390 [03:11<01:32,  3.10it/s] 27%|██▋       | 104/390 [03:11<01:29,  3.20it/s] 27%|██▋       | 105/390 [03:12<01:26,  3.29it/s] 27%|██▋       | 106/390 [03:12<01:24,  3.36it/s] 27%|██▋       | 107/390 [03:12<01:23,  3.40it/s] 28%|██▊       | 108/390 [03:13<01:22,  3.44it/s] 28%|██▊       | 109/390 [03:13<01:21,  3.46it/s] 28%|██▊       | 110/390 [03:13<01:31,  3.06it/s] 28%|██▊       | 111/390 [03:14<01:27,  3.18it/s] 29%|██▊       | 112/390 [03:14<01:24,  3.28it/s] 29%|██▉       | 113/390 [03:14<01:22,  3.34it/s] 29%|██▉       | 114/390 [03:14<01:21,  3.39it/s] 29%|██▉       | 115/390 [03:15<01:20,  3.43it/s] 30%|██▉       | 116/390 [03:15<01:19,  3.45it/s] 30%|███       | 117/390 [03:15<01:18,  3.47it/s] 30%|███       | 118/390 [03:16<01:18,  3.48it/s] 31%|███       | 119/390 [03:16<01:17,  3.49it/s] 31%|███       | 120/390 [03:16<01:17,  3.49it/s] 31%|███       | 121/390 [03:17<01:28,  3.02it/s] 31%|███▏      | 122/390 [03:17<01:25,  3.15it/s] 32%|███▏      | 123/390 [03:17<01:22,  3.25it/s] 32%|███▏      | 124/390 [03:17<01:19,  3.33it/s] 32%|███▏      | 125/390 [03:18<01:18,  3.38it/s] 32%|███▏      | 126/390 [03:18<01:17,  3.42it/s] 33%|███▎      | 127/390 [03:18<01:16,  3.44it/s] 33%|███▎      | 128/390 [03:19<01:15,  3.46it/s] 33%|███▎      | 129/390 [03:19<01:15,  3.48it/s] 33%|███▎      | 130/390 [03:19<01:14,  3.49it/s] 34%|███▎      | 131/390 [03:20<01:21,  3.19it/s] 34%|███▍      | 132/390 [03:20<01:18,  3.28it/s] 34%|███▍      | 133/390 [03:20<01:16,  3.34it/s] 34%|███▍      | 134/390 [03:20<01:15,  3.39it/s] 35%|███▍      | 135/390 [03:21<01:14,  3.42it/s] 35%|███▍      | 136/390 [03:21<01:13,  3.45it/s] 35%|███▌      | 137/390 [03:21<01:12,  3.47it/s] 35%|███▌      | 138/390 [03:22<01:12,  3.48it/s] 36%|███▌      | 139/390 [03:22<01:11,  3.49it/s] 36%|███▌      | 140/390 [03:22<01:11,  3.49it/s] 36%|███▌      | 141/390 [03:22<01:11,  3.49it/s] 36%|███▋      | 142/390 [03:23<01:18,  3.17it/s] 37%|███▋      | 143/390 [03:23<01:15,  3.27it/s] 37%|███▋      | 144/390 [03:23<01:13,  3.33it/s] 37%|███▋      | 145/390 [03:24<01:12,  3.38it/s] 37%|███▋      | 146/390 [03:24<01:11,  3.42it/s] 38%|███▊      | 147/390 [03:24<01:10,  3.44it/s] 38%|███▊      | 148/390 [03:24<01:09,  3.46it/s] 38%|███▊      | 149/390 [03:25<01:09,  3.47it/s] 38%|███▊      | 150/390 [03:25<01:08,  3.48it/s] 39%|███▊      | 151/390 [03:25<01:08,  3.49it/s] 39%|███▉      | 152/390 [03:26<01:08,  3.49it/s] 39%|███▉      | 153/390 [03:26<01:16,  3.11it/s] 39%|███▉      | 154/390 [03:26<01:13,  3.22it/s] 40%|███▉      | 155/390 [03:27<01:11,  3.30it/s] 40%|████      | 156/390 [03:27<01:09,  3.36it/s][INFO|trainer.py:2140] 2023-08-28 16:12:01,128 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:12:01,128 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 16:12:01,128 >>   Batch size = 8
{'eval_loss': 1.0499707460403442, 'eval_runtime': 10.4679, 'eval_samples_per_second': 334.068, 'eval_steps_per_second': 41.842, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.06it/s][A
  3%|▎         | 12/438 [00:00<00:08, 50.12it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.48it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.83it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.19it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.80it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.60it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.28it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.39it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.41it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.53it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.67it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.67it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.56it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.39it/s][A
 19%|█▉        | 83/438 [00:01<00:09, 37.63it/s][A
 20%|██        | 88/438 [00:01<00:08, 39.71it/s][A
 21%|██        | 93/438 [00:02<00:08, 41.44it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 42.67it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 43.63it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 44.19it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.71it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.91it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.76it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.64it/s][A
 30%|███       | 133/438 [00:02<00:06, 44.84it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.74it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 45.27it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 45.34it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.59it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 45.66it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 45.58it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 45.36it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 45.15it/s][A
 41%|████      | 178/438 [00:03<00:05, 45.03it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.11it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.27it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.31it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.49it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.62it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.55it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.38it/s][A
 50%|████▉     | 218/438 [00:05<00:07, 30.32it/s][A
 51%|█████     | 223/438 [00:05<00:06, 33.75it/s][A
 52%|█████▏    | 228/438 [00:05<00:05, 36.68it/s][A
 53%|█████▎    | 233/438 [00:05<00:05, 38.99it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 40.86it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 42.25it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 43.21it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 43.88it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.04it/s][A
 60%|██████    | 263/438 [00:06<00:03, 43.93it/s][A
 61%|██████    | 268/438 [00:06<00:03, 44.34it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 44.63it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 44.92it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 45.08it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 45.36it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 45.48it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 45.51it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 45.20it/s][A
 70%|███████   | 308/438 [00:07<00:02, 45.07it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 44.93it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.03it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.17it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.41it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.52it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 45.60it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 45.59it/s][A
 79%|███████▉  | 348/438 [00:08<00:02, 34.39it/s][A
 81%|████████  | 353/438 [00:08<00:02, 37.18it/s][A
 82%|████████▏ | 358/438 [00:08<00:02, 27.33it/s][A
 83%|████████▎ | 363/438 [00:08<00:02, 31.15it/s][A
 84%|████████▍ | 368/438 [00:08<00:02, 34.46it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 37.21it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 39.46it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 41.15it/s][A
 89%|████████▊ | 388/438 [00:09<00:01, 42.47it/s][A
 90%|████████▉ | 393/438 [00:09<00:01, 43.27it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 43.58it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 43.75it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 44.06it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 44.49it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 44.80it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 45.18it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 45.33it/s][A
 99%|█████████▉| 433/438 [00:10<00:00, 45.49it/s][A
100%|██████████| 438/438 [00:10<00:00, 45.41it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 45.41it/s][A 40%|████      | 156/390 [03:37<01:09,  3.36it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:12:11,549 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 16:12:13,085 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:13:03,374 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:13:05,627 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:13:06,178 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [05:24<2:17:19, 35.36s/it] 41%|████      | 158/390 [05:24<1:36:07, 24.86s/it] 41%|████      | 159/390 [05:25<1:07:19, 17.49s/it] 41%|████      | 160/390 [05:25<47:15, 12.33s/it]   41%|████▏     | 161/390 [05:25<33:16,  8.72s/it] 42%|████▏     | 162/390 [05:26<23:31,  6.19s/it] 42%|████▏     | 163/390 [05:26<16:43,  4.42s/it] 42%|████▏     | 164/390 [05:26<11:58,  3.18s/it] 42%|████▏     | 165/390 [05:26<08:40,  2.31s/it] 43%|████▎     | 166/390 [05:27<06:22,  1.71s/it] 43%|████▎     | 167/390 [05:27<04:45,  1.28s/it] 43%|████▎     | 168/390 [05:27<03:38,  1.02it/s] 43%|████▎     | 169/390 [05:28<02:53,  1.27it/s] 44%|████▎     | 170/390 [05:28<02:20,  1.57it/s] 44%|████▍     | 171/390 [05:28<01:56,  1.88it/s] 44%|████▍     | 172/390 [05:28<01:39,  2.18it/s] 44%|████▍     | 173/390 [05:29<01:28,  2.45it/s] 45%|████▍     | 174/390 [05:29<01:20,  2.69it/s] 45%|████▍     | 175/390 [05:29<01:16,  2.80it/s] 45%|████▌     | 176/390 [05:30<01:11,  2.97it/s] 45%|████▌     | 177/390 [05:30<01:08,  3.11it/s] 46%|████▌     | 178/390 [05:30<01:06,  3.21it/s] 46%|████▌     | 179/390 [05:31<01:04,  3.29it/s] 46%|████▌     | 180/390 [05:31<01:02,  3.34it/s] 46%|████▋     | 181/390 [05:31<01:01,  3.39it/s] 47%|████▋     | 182/390 [05:31<01:00,  3.42it/s] 47%|████▋     | 183/390 [05:32<00:59,  3.45it/s] 47%|████▋     | 184/390 [05:32<00:59,  3.46it/s] 47%|████▋     | 185/390 [05:32<00:58,  3.48it/s] 48%|████▊     | 186/390 [05:33<00:58,  3.49it/s] 48%|████▊     | 187/390 [05:33<00:58,  3.49it/s] 48%|████▊     | 188/390 [05:33<00:57,  3.50it/s] 48%|████▊     | 189/390 [05:33<00:57,  3.50it/s] 49%|████▊     | 190/390 [05:34<00:57,  3.50it/s] 49%|████▉     | 191/390 [05:34<01:02,  3.19it/s] 49%|████▉     | 192/390 [05:34<01:00,  3.28it/s] 49%|████▉     | 193/390 [05:35<00:58,  3.34it/s] 50%|████▉     | 194/390 [05:35<00:57,  3.39it/s] 50%|█████     | 195/390 [05:36<01:23,  2.33it/s] 50%|█████     | 196/390 [05:37<01:48,  1.78it/s] 51%|█████     | 197/390 [05:37<01:32,  2.09it/s] 51%|█████     | 198/390 [05:37<01:26,  2.21it/s] 51%|█████     | 199/390 [05:37<01:16,  2.48it/s] 51%|█████▏    | 200/390 [05:38<01:09,  2.72it/s] 52%|█████▏    | 201/390 [05:38<01:04,  2.92it/s] 52%|█████▏    | 202/390 [05:38<01:01,  3.07it/s] 52%|█████▏    | 203/390 [05:39<00:58,  3.19it/s] 52%|█████▏    | 204/390 [05:39<00:56,  3.28it/s] 53%|█████▎    | 205/390 [05:39<00:55,  3.35it/s] 53%|█████▎    | 206/390 [05:39<00:54,  3.39it/s] 53%|█████▎    | 207/390 [05:40<00:53,  3.42it/s] 53%|█████▎    | 208/390 [05:40<00:52,  3.45it/s] 54%|█████▎    | 209/390 [05:40<00:55,  3.24it/s] 54%|█████▍    | 210/390 [05:41<00:54,  3.32it/s] 54%|█████▍    | 211/390 [05:41<00:53,  3.37it/s] 54%|█████▍    | 212/390 [05:41<00:52,  3.41it/s] 55%|█████▍    | 213/390 [05:42<00:51,  3.44it/s] 55%|█████▍    | 214/390 [05:42<00:50,  3.46it/s] 55%|█████▌    | 215/390 [05:42<00:50,  3.47it/s] 55%|█████▌    | 216/390 [05:42<00:50,  3.48it/s] 56%|█████▌    | 217/390 [05:43<00:49,  3.48it/s] 56%|█████▌    | 218/390 [05:43<00:49,  3.49it/s] 56%|█████▌    | 219/390 [05:43<00:48,  3.49it/s] 56%|█████▋    | 220/390 [05:44<00:54,  3.14it/s] 57%|█████▋    | 221/390 [05:44<00:52,  3.24it/s] 57%|█████▋    | 222/390 [05:44<00:50,  3.32it/s] 57%|█████▋    | 223/390 [05:45<00:49,  3.37it/s] 57%|█████▋    | 224/390 [05:45<00:48,  3.41it/s] 58%|█████▊    | 225/390 [05:45<00:47,  3.44it/s] 58%|█████▊    | 226/390 [05:45<00:47,  3.45it/s] 58%|█████▊    | 227/390 [05:46<00:47,  3.47it/s] 58%|█████▊    | 228/390 [05:46<00:46,  3.48it/s] 59%|█████▊    | 229/390 [05:46<00:46,  3.47it/s] 59%|█████▉    | 230/390 [05:47<00:45,  3.48it/s] 59%|█████▉    | 231/390 [05:47<00:52,  3.01it/s] 59%|█████▉    | 232/390 [05:47<00:50,  3.14it/s] 60%|█████▉    | 233/390 [05:48<00:48,  3.24it/s] 60%|██████    | 234/390 [05:48<00:47,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 16:14:22,061 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:14:22,062 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 16:14:22,062 >>   Batch size = 8
{'eval_loss': 1.0591996908187866, 'eval_runtime': 10.1934, 'eval_samples_per_second': 343.065, 'eval_steps_per_second': 42.969, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.75it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.57it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.27it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.52it/s][A
  6%|▋         | 28/438 [00:00<00:08, 45.98it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.65it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.51it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.34it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.34it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.49it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.48it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.54it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.57it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.47it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.33it/s][A
 19%|█▉        | 83/438 [00:01<00:10, 34.47it/s][A
 20%|██        | 88/438 [00:02<00:09, 37.29it/s][A
 21%|██        | 93/438 [00:02<00:08, 39.49it/s][A
 22%|██▏       | 98/438 [00:02<00:08, 41.20it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 42.52it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 43.43it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.16it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.60it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.46it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.30it/s][A
 30%|███       | 133/438 [00:03<00:06, 44.48it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 44.79it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 44.99it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 45.27it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.42it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 45.56it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 45.49it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 45.17it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 45.00it/s][A
 41%|████      | 178/438 [00:04<00:05, 45.02it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.03it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 44.95it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.35it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.56it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.67it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.54it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.29it/s][A
 50%|████▉     | 218/438 [00:05<00:06, 34.33it/s][A
 51%|█████     | 223/438 [00:05<00:05, 37.09it/s][A
 52%|█████▏    | 228/438 [00:05<00:05, 39.38it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 41.08it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 42.40it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 43.31it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 44.04it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.44it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.35it/s][A
 60%|██████    | 263/438 [00:06<00:03, 44.28it/s][A
 61%|██████    | 268/438 [00:06<00:03, 44.53it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 44.72it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 44.93it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 45.18it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 45.40it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 45.51it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 45.35it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 45.19it/s][A
 70%|███████   | 308/438 [00:07<00:02, 45.07it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 44.95it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 44.97it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.18it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.27it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.49it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 45.50it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 45.46it/s][A
 79%|███████▉  | 348/438 [00:07<00:01, 45.16it/s][A
 81%|████████  | 353/438 [00:08<00:02, 40.01it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 41.63it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 42.71it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 43.58it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.28it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 44.69it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 44.95it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 45.14it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 44.76it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 44.66it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.80it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 45.04it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 45.22it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 45.40it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 45.45it/s][A
 98%|█████████▊| 428/438 [00:09<00:00, 45.46it/s][A
 99%|█████████▉| 433/438 [00:09<00:00, 45.36it/s][A
100%|██████████| 438/438 [00:09<00:00, 45.10it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 45.10it/s][A 60%|██████    | 234/390 [05:58<00:47,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:14:32,802 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 16:14:33,482 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:15:06,048 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:15:06,999 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:15:07,384 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [08:04<1:46:15, 41.13s/it] 61%|██████    | 236/390 [08:05<1:14:14, 28.93s/it] 61%|██████    | 237/390 [08:05<51:51, 20.34s/it]   61%|██████    | 238/390 [08:05<36:17, 14.32s/it] 61%|██████▏   | 239/390 [08:06<25:27, 10.11s/it] 62%|██████▏   | 240/390 [08:06<17:54,  7.17s/it] 62%|██████▏   | 241/390 [08:06<12:40,  5.10s/it] 62%|██████▏   | 242/390 [08:06<09:01,  3.66s/it] 62%|██████▏   | 243/390 [08:07<06:29,  2.65s/it] 63%|██████▎   | 244/390 [08:07<04:43,  1.94s/it] 63%|██████▎   | 245/390 [08:07<03:29,  1.45s/it] 63%|██████▎   | 246/390 [08:08<02:43,  1.14s/it] 63%|██████▎   | 247/390 [08:08<02:06,  1.13it/s] 64%|██████▎   | 248/390 [08:08<01:40,  1.42it/s] 64%|██████▍   | 249/390 [08:09<01:22,  1.72it/s] 64%|██████▍   | 250/390 [08:09<01:09,  2.02it/s] 64%|██████▍   | 251/390 [08:09<01:00,  2.31it/s] 65%|██████▍   | 252/390 [08:09<00:53,  2.56it/s] 65%|██████▍   | 253/390 [08:10<00:49,  2.78it/s] 65%|██████▌   | 254/390 [08:10<00:46,  2.95it/s] 65%|██████▌   | 255/390 [08:10<00:43,  3.09it/s] 66%|██████▌   | 256/390 [08:11<00:47,  2.85it/s] 66%|██████▌   | 257/390 [08:11<00:44,  3.01it/s] 66%|██████▌   | 258/390 [08:11<00:42,  3.14it/s] 66%|██████▋   | 259/390 [08:12<00:40,  3.24it/s] 67%|██████▋   | 260/390 [08:12<00:39,  3.31it/s] 67%|██████▋   | 261/390 [08:12<00:38,  3.37it/s] 67%|██████▋   | 262/390 [08:12<00:37,  3.41it/s] 67%|██████▋   | 263/390 [08:13<00:36,  3.44it/s] 68%|██████▊   | 264/390 [08:13<00:36,  3.46it/s] 68%|██████▊   | 265/390 [08:13<00:35,  3.47it/s] 68%|██████▊   | 266/390 [08:14<00:35,  3.48it/s] 68%|██████▊   | 267/390 [08:14<00:39,  3.10it/s] 69%|██████▊   | 268/390 [08:14<00:37,  3.21it/s] 69%|██████▉   | 269/390 [08:15<00:36,  3.30it/s] 69%|██████▉   | 270/390 [08:15<00:35,  3.35it/s] 69%|██████▉   | 271/390 [08:15<00:35,  3.40it/s] 70%|██████▉   | 272/390 [08:15<00:34,  3.43it/s] 70%|███████   | 273/390 [08:16<00:33,  3.45it/s] 70%|███████   | 274/390 [08:16<00:33,  3.47it/s] 71%|███████   | 275/390 [08:16<00:33,  3.48it/s] 71%|███████   | 276/390 [08:17<00:32,  3.49it/s] 71%|███████   | 277/390 [08:17<00:32,  3.50it/s] 71%|███████▏  | 278/390 [08:17<00:37,  3.00it/s] 72%|███████▏  | 279/390 [08:18<00:35,  3.14it/s] 72%|███████▏  | 280/390 [08:18<00:33,  3.24it/s] 72%|███████▏  | 281/390 [08:18<00:32,  3.31it/s] 72%|███████▏  | 282/390 [08:18<00:32,  3.37it/s] 73%|███████▎  | 283/390 [08:19<00:31,  3.41it/s] 73%|███████▎  | 284/390 [08:19<00:30,  3.44it/s] 73%|███████▎  | 285/390 [08:19<00:30,  3.46it/s] 73%|███████▎  | 286/390 [08:20<00:29,  3.47it/s] 74%|███████▎  | 287/390 [08:20<00:29,  3.48it/s] 74%|███████▍  | 288/390 [08:20<00:34,  2.98it/s] 74%|███████▍  | 289/390 [08:21<00:32,  3.12it/s] 74%|███████▍  | 290/390 [08:21<00:30,  3.23it/s] 75%|███████▍  | 291/390 [08:21<00:29,  3.31it/s] 75%|███████▍  | 292/390 [08:21<00:29,  3.36it/s] 75%|███████▌  | 293/390 [08:22<00:32,  2.98it/s] 75%|███████▌  | 294/390 [08:22<00:30,  3.12it/s] 76%|███████▌  | 295/390 [08:22<00:29,  3.23it/s] 76%|███████▌  | 296/390 [08:23<00:28,  3.30it/s] 76%|███████▌  | 297/390 [08:23<00:27,  3.36it/s] 76%|███████▋  | 298/390 [08:23<00:31,  2.96it/s] 77%|███████▋  | 299/390 [08:24<00:29,  3.10it/s] 77%|███████▋  | 300/390 [08:24<00:28,  3.21it/s] 77%|███████▋  | 301/390 [08:24<00:27,  3.29it/s] 77%|███████▋  | 302/390 [08:25<00:26,  3.35it/s] 78%|███████▊  | 303/390 [08:25<00:25,  3.39it/s] 78%|███████▊  | 304/390 [08:25<00:25,  3.43it/s] 78%|███████▊  | 305/390 [08:25<00:24,  3.44it/s] 78%|███████▊  | 306/390 [08:26<00:24,  3.46it/s] 79%|███████▊  | 307/390 [08:26<00:23,  3.47it/s] 79%|███████▉  | 308/390 [08:26<00:26,  3.08it/s] 79%|███████▉  | 309/390 [08:27<00:25,  3.19it/s] 79%|███████▉  | 310/390 [08:27<00:24,  3.28it/s] 80%|███████▉  | 311/390 [08:27<00:23,  3.34it/s] 80%|████████  | 312/390 [08:28<00:23,  3.38it/s][INFO|trainer.py:2140] 2023-08-28 16:17:01,817 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:17:01,817 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 16:17:01,817 >>   Batch size = 8
{'eval_loss': 1.0751274824142456, 'eval_runtime': 9.9322, 'eval_samples_per_second': 352.087, 'eval_steps_per_second': 44.099, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.01it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.36it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.23it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.49it/s][A
  6%|▋         | 28/438 [00:00<00:08, 45.94it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.67it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.55it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.41it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.38it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.50it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.61it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.54it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.45it/s][A
 17%|█▋        | 73/438 [00:01<00:11, 33.07it/s][A
 18%|█▊        | 78/438 [00:01<00:09, 36.13it/s][A
 19%|█▉        | 83/438 [00:01<00:09, 38.62it/s][A
 20%|██        | 88/438 [00:02<00:08, 40.56it/s][A
 21%|██        | 93/438 [00:02<00:08, 42.01it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 43.13it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 43.96it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 44.38it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 44.28it/s][A
 27%|██▋       | 118/438 [00:02<00:07, 44.20it/s][A
 28%|██▊       | 123/438 [00:02<00:07, 44.35it/s][A
 29%|██▉       | 128/438 [00:02<00:06, 44.57it/s][A
 30%|███       | 133/438 [00:03<00:06, 44.89it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 45.17it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 45.29it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 45.52it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.62it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 45.33it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 45.11it/s][A
 38%|███▊      | 168/438 [00:03<00:05, 45.09it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 44.97it/s][A
 41%|████      | 178/438 [00:04<00:05, 44.99it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.25it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.35it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.42it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.57it/s][A
 46%|████▋     | 203/438 [00:04<00:06, 35.93it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 38.43it/s][A
 49%|████▊     | 213/438 [00:04<00:05, 40.36it/s][A
 50%|████▉     | 218/438 [00:05<00:05, 41.87it/s][A
 51%|█████     | 223/438 [00:05<00:05, 42.97it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 43.83it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 44.41it/s][A
 54%|█████▍    | 238/438 [00:05<00:04, 44.52it/s][A
 55%|█████▌    | 243/438 [00:05<00:04, 44.39it/s][A
 57%|█████▋    | 248/438 [00:05<00:04, 44.24it/s][A
 58%|█████▊    | 253/438 [00:05<00:04, 44.46it/s][A
 59%|█████▉    | 258/438 [00:05<00:04, 44.74it/s][A
 60%|██████    | 263/438 [00:06<00:03, 45.02it/s][A
 61%|██████    | 268/438 [00:06<00:03, 45.26it/s][A
 62%|██████▏   | 273/438 [00:06<00:03, 45.46it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 45.58it/s][A
 65%|██████▍   | 283/438 [00:06<00:03, 45.43it/s][A
 66%|██████▌   | 288/438 [00:06<00:03, 45.04it/s][A
 67%|██████▋   | 293/438 [00:06<00:03, 44.83it/s][A
 68%|██████▊   | 298/438 [00:06<00:03, 44.87it/s][A
 69%|██████▉   | 303/438 [00:06<00:02, 45.03it/s][A
 70%|███████   | 308/438 [00:07<00:02, 45.17it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 45.29it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.42it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.59it/s][A
 75%|███████▍  | 328/438 [00:07<00:02, 45.42it/s][A
 76%|███████▌  | 333/438 [00:07<00:02, 45.19it/s][A
 77%|███████▋  | 338/438 [00:07<00:02, 34.33it/s][A
 78%|███████▊  | 343/438 [00:07<00:02, 37.11it/s][A
 79%|███████▉  | 348/438 [00:07<00:02, 39.30it/s][A
 81%|████████  | 353/438 [00:08<00:02, 41.08it/s][A
 82%|████████▏ | 358/438 [00:08<00:01, 42.35it/s][A
 83%|████████▎ | 363/438 [00:08<00:01, 43.36it/s][A
 84%|████████▍ | 368/438 [00:08<00:01, 43.89it/s][A
 85%|████████▌ | 373/438 [00:08<00:01, 44.38it/s][A
 86%|████████▋ | 378/438 [00:08<00:01, 44.25it/s][A
 87%|████████▋ | 383/438 [00:08<00:01, 44.39it/s][A
 89%|████████▊ | 388/438 [00:08<00:01, 44.70it/s][A
 90%|████████▉ | 393/438 [00:08<00:01, 44.99it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 45.17it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 45.32it/s][A
 93%|█████████▎| 408/438 [00:09<00:00, 45.31it/s][A
 94%|█████████▍| 413/438 [00:09<00:00, 45.44it/s][A
 95%|█████████▌| 418/438 [00:09<00:00, 45.28it/s][A
 97%|█████████▋| 423/438 [00:09<00:00, 45.01it/s][A
 98%|█████████▊| 428/438 [00:10<00:00, 44.90it/s][A
 99%|█████████▉| 433/438 [00:10<00:00, 27.87it/s][A
100%|██████████| 438/438 [00:10<00:00, 31.61it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 31.61it/s][A 80%|████████  | 312/390 [08:38<00:23,  3.38it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:17:12,191 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 16:17:13,377 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:17:43,526 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:17:45,284 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:17:45,796 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [10:41<51:49, 40.38s/it] 81%|████████  | 314/390 [10:42<36:00, 28.43s/it] 81%|████████  | 315/390 [10:42<24:58, 19.99s/it] 81%|████████  | 316/390 [10:43<17:21, 14.08s/it] 81%|████████▏ | 317/390 [10:43<12:05,  9.94s/it] 82%|████████▏ | 318/390 [10:43<08:27,  7.05s/it] 82%|████████▏ | 319/390 [10:43<05:56,  5.02s/it] 82%|████████▏ | 320/390 [10:44<04:12,  3.60s/it] 82%|████████▏ | 321/390 [10:44<02:59,  2.61s/it] 83%|████████▎ | 322/390 [10:44<02:10,  1.91s/it] 83%|████████▎ | 323/390 [10:45<01:35,  1.43s/it] 83%|████████▎ | 324/390 [10:45<01:15,  1.15s/it] 83%|████████▎ | 325/390 [10:45<00:57,  1.12it/s] 84%|████████▎ | 326/390 [10:46<00:45,  1.41it/s] 84%|████████▍ | 327/390 [10:46<00:36,  1.71it/s] 84%|████████▍ | 328/390 [10:46<00:30,  2.02it/s] 84%|████████▍ | 329/390 [10:47<00:26,  2.30it/s] 85%|████████▍ | 330/390 [10:47<00:23,  2.56it/s] 85%|████████▍ | 331/390 [10:47<00:21,  2.77it/s] 85%|████████▌ | 332/390 [10:47<00:19,  2.94it/s] 85%|████████▌ | 333/390 [10:48<00:18,  3.08it/s] 86%|████████▌ | 334/390 [10:48<00:20,  2.72it/s] 86%|████████▌ | 335/390 [10:48<00:18,  2.90it/s] 86%|████████▌ | 336/390 [10:49<00:17,  3.05it/s] 86%|████████▋ | 337/390 [10:49<00:16,  3.16it/s] 87%|████████▋ | 338/390 [10:49<00:16,  3.24it/s] 87%|████████▋ | 339/390 [10:50<00:15,  3.29it/s] 87%|████████▋ | 340/390 [10:50<00:14,  3.34it/s] 87%|████████▋ | 341/390 [10:50<00:14,  3.37it/s] 88%|████████▊ | 342/390 [10:51<00:14,  3.39it/s] 88%|████████▊ | 343/390 [10:51<00:13,  3.41it/s] 88%|████████▊ | 344/390 [10:51<00:15,  2.98it/s] 88%|████████▊ | 345/390 [10:52<00:14,  3.11it/s] 89%|████████▊ | 346/390 [10:52<00:13,  3.21it/s] 89%|████████▉ | 347/390 [10:52<00:13,  3.29it/s] 89%|████████▉ | 348/390 [10:52<00:12,  3.35it/s] 89%|████████▉ | 349/390 [10:53<00:12,  3.39it/s] 90%|████████▉ | 350/390 [10:53<00:11,  3.42it/s] 90%|█████████ | 351/390 [10:53<00:11,  3.44it/s] 90%|█████████ | 352/390 [10:54<00:10,  3.46it/s] 91%|█████████ | 353/390 [10:54<00:10,  3.47it/s] 91%|█████████ | 354/390 [10:54<00:11,  3.13it/s] 91%|█████████ | 355/390 [10:55<00:10,  3.23it/s] 91%|█████████▏| 356/390 [10:55<00:10,  3.30it/s] 92%|█████████▏| 357/390 [10:55<00:09,  3.36it/s] 92%|█████████▏| 358/390 [10:55<00:09,  3.40it/s] 92%|█████████▏| 359/390 [10:56<00:09,  3.43it/s] 92%|█████████▏| 360/390 [10:56<00:08,  3.44it/s] 93%|█████████▎| 361/390 [10:56<00:08,  3.46it/s] 93%|█████████▎| 362/390 [10:57<00:08,  3.47it/s] 93%|█████████▎| 363/390 [10:57<00:07,  3.47it/s] 93%|█████████▎| 364/390 [10:57<00:07,  3.48it/s] 94%|█████████▎| 365/390 [10:57<00:07,  3.20it/s] 94%|█████████▍| 366/390 [10:58<00:07,  3.28it/s] 94%|█████████▍| 367/390 [10:58<00:06,  3.34it/s] 94%|█████████▍| 368/390 [10:58<00:06,  3.38it/s] 95%|█████████▍| 369/390 [10:59<00:06,  3.41it/s] 95%|█████████▍| 370/390 [10:59<00:05,  3.44it/s] 95%|█████████▌| 371/390 [10:59<00:05,  3.45it/s] 95%|█████████▌| 372/390 [10:59<00:05,  3.47it/s] 96%|█████████▌| 373/390 [11:00<00:04,  3.47it/s] 96%|█████████▌| 374/390 [11:00<00:04,  3.48it/s] 96%|█████████▌| 375/390 [11:00<00:04,  3.49it/s] 96%|█████████▋| 376/390 [11:01<00:04,  3.18it/s] 97%|█████████▋| 377/390 [11:01<00:03,  3.27it/s] 97%|█████████▋| 378/390 [11:01<00:03,  3.33it/s] 97%|█████████▋| 379/390 [11:02<00:03,  3.38it/s] 97%|█████████▋| 380/390 [11:02<00:02,  3.41it/s] 98%|█████████▊| 381/390 [11:02<00:02,  3.44it/s] 98%|█████████▊| 382/390 [11:02<00:02,  3.45it/s] 98%|█████████▊| 383/390 [11:03<00:02,  3.47it/s] 98%|█████████▊| 384/390 [11:03<00:01,  3.48it/s] 99%|█████████▊| 385/390 [11:03<00:01,  3.48it/s] 99%|█████████▉| 386/390 [11:04<00:01,  3.49it/s] 99%|█████████▉| 387/390 [11:04<00:00,  3.10it/s] 99%|█████████▉| 388/390 [11:04<00:00,  3.21it/s]100%|█████████▉| 389/390 [11:05<00:00,  3.29it/s]100%|██████████| 390/390 [11:05<00:00,  3.35it/s][INFO|trainer.py:2140] 2023-08-28 16:19:39,040 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:19:39,040 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 16:19:39,040 >>   Batch size = 8
{'eval_loss': 1.0849398374557495, 'eval_runtime': 10.2304, 'eval_samples_per_second': 341.825, 'eval_steps_per_second': 42.814, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.12it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.24it/s][A
  4%|▍         | 17/438 [00:00<00:08, 47.35it/s][A
  5%|▌         | 22/438 [00:00<00:08, 46.57it/s][A
  6%|▌         | 27/438 [00:00<00:08, 46.03it/s][A
  7%|▋         | 32/438 [00:00<00:08, 45.67it/s][A
  8%|▊         | 37/438 [00:00<00:08, 45.43it/s][A
 10%|▉         | 42/438 [00:00<00:08, 45.39it/s][A
 11%|█         | 47/438 [00:01<00:08, 45.52it/s][A
 12%|█▏        | 52/438 [00:01<00:08, 45.51it/s][A
 13%|█▎        | 57/438 [00:01<00:08, 45.63it/s][A
 14%|█▍        | 62/438 [00:01<00:08, 45.58it/s][A
 15%|█▌        | 67/438 [00:01<00:08, 45.41it/s][A
 16%|█▋        | 72/438 [00:01<00:08, 45.33it/s][A
 18%|█▊        | 77/438 [00:01<00:07, 45.20it/s][A
 19%|█▊        | 82/438 [00:01<00:07, 45.21it/s][A
 20%|█▉        | 87/438 [00:01<00:09, 36.34it/s][A
 21%|██        | 92/438 [00:02<00:08, 38.82it/s][A
 22%|██▏       | 97/438 [00:02<00:08, 40.64it/s][A
 23%|██▎       | 102/438 [00:02<00:07, 42.07it/s][A
 24%|██▍       | 107/438 [00:02<00:07, 43.16it/s][A
 26%|██▌       | 112/438 [00:02<00:07, 43.94it/s][A
 27%|██▋       | 117/438 [00:02<00:07, 44.51it/s][A
 28%|██▊       | 122/438 [00:02<00:07, 44.80it/s][A
 29%|██▉       | 127/438 [00:02<00:06, 44.53it/s][A
 30%|███       | 132/438 [00:02<00:06, 44.48it/s][A
 31%|███▏      | 137/438 [00:03<00:06, 44.59it/s][A
 32%|███▏      | 142/438 [00:03<00:06, 44.92it/s][A
 34%|███▎      | 147/438 [00:03<00:06, 45.20it/s][A
 35%|███▍      | 152/438 [00:03<00:06, 45.32it/s][A
 36%|███▌      | 157/438 [00:03<00:06, 45.47it/s][A
 37%|███▋      | 162/438 [00:03<00:06, 45.55it/s][A
 38%|███▊      | 167/438 [00:03<00:05, 45.39it/s][A
 39%|███▉      | 172/438 [00:03<00:05, 45.18it/s][A
 40%|████      | 177/438 [00:03<00:05, 44.99it/s][A
 42%|████▏     | 182/438 [00:04<00:05, 44.94it/s][A
 43%|████▎     | 187/438 [00:04<00:05, 45.14it/s][A
 44%|████▍     | 192/438 [00:04<00:05, 45.33it/s][A
 45%|████▍     | 197/438 [00:04<00:05, 45.45it/s][A
 46%|████▌     | 202/438 [00:04<00:05, 45.59it/s][A
 47%|████▋     | 207/438 [00:04<00:05, 45.57it/s][A
 48%|████▊     | 212/438 [00:04<00:04, 45.49it/s][A
 50%|████▉     | 217/438 [00:04<00:04, 45.21it/s][A
 51%|█████     | 222/438 [00:05<00:05, 36.96it/s][A
 52%|█████▏    | 227/438 [00:05<00:05, 39.29it/s][A
 53%|█████▎    | 232/438 [00:05<00:05, 41.08it/s][A
 54%|█████▍    | 237/438 [00:05<00:04, 42.36it/s][A
 55%|█████▌    | 242/438 [00:05<00:04, 43.36it/s][A
 56%|█████▋    | 247/438 [00:05<00:04, 44.03it/s][A
 58%|█████▊    | 252/438 [00:05<00:04, 44.61it/s][A
 59%|█████▊    | 257/438 [00:05<00:04, 44.85it/s][A
 60%|█████▉    | 262/438 [00:05<00:03, 44.61it/s][A
 61%|██████    | 267/438 [00:06<00:03, 44.45it/s][A
 62%|██████▏   | 272/438 [00:06<00:03, 44.63it/s][A
 63%|██████▎   | 277/438 [00:06<00:03, 44.85it/s][A
 64%|██████▍   | 282/438 [00:06<00:03, 44.92it/s][A
 66%|██████▌   | 287/438 [00:06<00:03, 45.15it/s][A
 67%|██████▋   | 292/438 [00:06<00:03, 45.34it/s][A
 68%|██████▊   | 297/438 [00:06<00:03, 45.45it/s][A
 69%|██████▉   | 302/438 [00:06<00:02, 45.47it/s][A
 70%|███████   | 307/438 [00:06<00:02, 45.31it/s][A
 71%|███████   | 312/438 [00:07<00:02, 45.14it/s][A
 72%|███████▏  | 317/438 [00:07<00:02, 45.13it/s][A
 74%|███████▎  | 322/438 [00:07<00:02, 45.11it/s][A
 75%|███████▍  | 327/438 [00:07<00:02, 45.18it/s][A
 76%|███████▌  | 332/438 [00:07<00:02, 45.26it/s][A
 77%|███████▋  | 337/438 [00:07<00:02, 45.47it/s][A
 78%|███████▊  | 342/438 [00:07<00:02, 45.41it/s][A
 79%|███████▉  | 347/438 [00:07<00:02, 45.34it/s][A
 80%|████████  | 352/438 [00:08<00:01, 45.29it/s][A
 82%|████████▏ | 357/438 [00:08<00:02, 36.23it/s][A
 83%|████████▎ | 362/438 [00:08<00:01, 38.64it/s][A
 84%|████████▍ | 367/438 [00:08<00:01, 40.58it/s][A
 85%|████████▍ | 372/438 [00:08<00:01, 42.03it/s][A
 86%|████████▌ | 377/438 [00:08<00:01, 43.17it/s][A
 87%|████████▋ | 382/438 [00:08<00:01, 43.84it/s][A
 88%|████████▊ | 387/438 [00:08<00:01, 44.45it/s][A
 89%|████████▉ | 392/438 [00:08<00:01, 44.71it/s][A
 91%|█████████ | 397/438 [00:08<00:00, 44.51it/s][A
 92%|█████████▏| 402/438 [00:09<00:00, 44.32it/s][A
 93%|█████████▎| 407/438 [00:09<00:00, 44.48it/s][A
 94%|█████████▍| 412/438 [00:09<00:00, 44.72it/s][A
 95%|█████████▌| 417/438 [00:09<00:00, 44.95it/s][A
 96%|█████████▋| 422/438 [00:09<00:00, 45.19it/s][A
 97%|█████████▋| 427/438 [00:09<00:00, 45.37it/s][A
 99%|█████████▊| 432/438 [00:09<00:00, 45.51it/s][A
100%|█████████▉| 437/438 [00:09<00:00, 45.44it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:09<00:00, 45.44it/s][A100%|██████████| 390/390 [11:15<00:00,  3.35it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 16:19:49,079 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 16:19:50,339 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:20:38,190 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:20:40,281 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:20:40,767 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 16:22:16,624 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 16:22:16,754 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78 (score: 1.0499707460403442).
                                                 100%|██████████| 390/390 [14:40<00:00,  3.35it/s]100%|██████████| 390/390 [14:40<00:00,  2.26s/it]
[INFO|trainer.py:1894] 2023-08-28 16:23:15,120 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model
[INFO|configuration_utils.py:351] 2023-08-28 16:23:15,902 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 16:24:01,841 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 16:24:02,663 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 16:24:03,107 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:24:07,064 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:07,256 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:07,256 >>   train_loss               =     0.5151
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:07,256 >>   train_runtime            = 0:14:39.95
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:07,257 >>   train_samples            =       4999
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:07,257 >>   train_samples_per_second =     28.405
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:07,257 >>   train_steps_per_second   =      0.443
{'eval_loss': 1.0880460739135742, 'eval_runtime': 9.9087, 'eval_samples_per_second': 352.922, 'eval_steps_per_second': 44.204, 'epoch': 4.99}
{'train_runtime': 879.958, 'train_samples_per_second': 28.405, 'train_steps_per_second': 0.443, 'train_loss': 0.5151102310571916, 'epoch': 4.99}
08/28/2023 16:24:08 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 16:24:08,863 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 16:24:08,863 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 16:24:08,863 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 57.31it/s]  3%|▎         | 12/438 [00:00<00:08, 49.96it/s]  4%|▍         | 18/438 [00:00<00:08, 48.26it/s]  5%|▌         | 23/438 [00:00<00:08, 47.54it/s]  6%|▋         | 28/438 [00:00<00:08, 47.03it/s]  8%|▊         | 33/438 [00:00<00:08, 46.69it/s]  9%|▊         | 38/438 [00:00<00:08, 46.51it/s] 10%|▉         | 43/438 [00:00<00:08, 46.20it/s] 11%|█         | 48/438 [00:01<00:08, 45.81it/s] 12%|█▏        | 53/438 [00:01<00:08, 45.67it/s] 13%|█▎        | 58/438 [00:01<00:08, 45.79it/s] 14%|█▍        | 63/438 [00:01<00:11, 32.56it/s] 16%|█▌        | 68/438 [00:01<00:10, 35.74it/s] 17%|█▋        | 73/438 [00:01<00:09, 38.36it/s] 18%|█▊        | 78/438 [00:01<00:08, 40.39it/s] 19%|█▉        | 83/438 [00:01<00:08, 41.97it/s] 20%|██        | 88/438 [00:02<00:08, 43.19it/s] 21%|██        | 93/438 [00:02<00:07, 43.98it/s] 22%|██▏       | 98/438 [00:02<00:07, 44.58it/s] 24%|██▎       | 103/438 [00:02<00:07, 44.63it/s] 25%|██▍       | 108/438 [00:02<00:07, 44.62it/s] 26%|██▌       | 113/438 [00:02<00:07, 44.88it/s] 27%|██▋       | 118/438 [00:02<00:07, 45.11it/s] 28%|██▊       | 123/438 [00:02<00:06, 45.44it/s] 29%|██▉       | 128/438 [00:02<00:06, 45.61it/s] 30%|███       | 133/438 [00:03<00:06, 45.82it/s] 32%|███▏      | 138/438 [00:03<00:06, 45.89it/s] 33%|███▎      | 143/438 [00:03<00:06, 45.89it/s] 34%|███▍      | 148/438 [00:03<00:06, 45.61it/s] 35%|███▍      | 153/438 [00:03<00:06, 45.41it/s] 36%|███▌      | 158/438 [00:03<00:06, 45.21it/s] 37%|███▋      | 163/438 [00:03<00:06, 45.41it/s] 38%|███▊      | 168/438 [00:03<00:05, 45.65it/s] 39%|███▉      | 173/438 [00:03<00:05, 45.71it/s] 41%|████      | 178/438 [00:04<00:05, 45.87it/s] 42%|████▏     | 183/438 [00:04<00:05, 45.88it/s] 43%|████▎     | 188/438 [00:04<00:05, 45.95it/s] 44%|████▍     | 193/438 [00:04<00:05, 45.71it/s] 45%|████▌     | 198/438 [00:04<00:07, 31.89it/s] 46%|████▋     | 203/438 [00:04<00:06, 35.16it/s] 47%|████▋     | 208/438 [00:04<00:06, 37.84it/s] 49%|████▊     | 213/438 [00:05<00:18, 12.49it/s] 50%|█████     | 219/438 [00:05<00:13, 16.80it/s] 51%|█████     | 224/438 [00:06<00:10, 20.56it/s] 52%|█████▏    | 229/438 [00:06<00:08, 24.53it/s] 53%|█████▎    | 234/438 [00:06<00:07, 28.45it/s] 55%|█████▍    | 239/438 [00:06<00:06, 32.02it/s] 56%|█████▌    | 244/438 [00:06<00:05, 35.30it/s] 57%|█████▋    | 249/438 [00:06<00:04, 37.95it/s] 58%|█████▊    | 254/438 [00:06<00:04, 39.92it/s] 59%|█████▉    | 259/438 [00:06<00:04, 41.17it/s] 60%|██████    | 264/438 [00:06<00:04, 42.14it/s] 61%|██████▏   | 269/438 [00:07<00:03, 42.97it/s] 63%|██████▎   | 274/438 [00:07<00:03, 43.73it/s] 64%|██████▎   | 279/438 [00:07<00:03, 44.36it/s] 65%|██████▍   | 284/438 [00:07<00:03, 44.89it/s] 66%|██████▌   | 289/438 [00:07<00:05, 29.04it/s] 67%|██████▋   | 294/438 [00:07<00:04, 32.72it/s] 68%|██████▊   | 299/438 [00:07<00:03, 35.80it/s] 69%|██████▉   | 304/438 [00:08<00:03, 38.41it/s] 71%|███████   | 309/438 [00:08<00:03, 40.38it/s] 72%|███████▏  | 314/438 [00:08<00:02, 41.92it/s] 73%|███████▎  | 319/438 [00:08<00:02, 43.17it/s] 74%|███████▍  | 324/438 [00:08<00:02, 43.93it/s] 75%|███████▌  | 329/438 [00:08<00:02, 44.09it/s] 76%|███████▋  | 334/438 [00:08<00:02, 44.15it/s] 77%|███████▋  | 339/438 [00:08<00:02, 44.46it/s] 79%|███████▊  | 344/438 [00:08<00:02, 44.83it/s] 80%|███████▉  | 349/438 [00:09<00:01, 44.80it/s] 81%|████████  | 354/438 [00:09<00:01, 45.17it/s] 82%|████████▏ | 359/438 [00:09<00:01, 45.51it/s] 83%|████████▎ | 364/438 [00:09<00:01, 45.69it/s] 84%|████████▍ | 369/438 [00:09<00:01, 45.66it/s] 85%|████████▌ | 374/438 [00:09<00:01, 45.38it/s] 87%|████████▋ | 379/438 [00:09<00:01, 45.14it/s] 88%|████████▊ | 384/438 [00:09<00:01, 44.98it/s] 89%|████████▉ | 389/438 [00:09<00:01, 45.18it/s] 90%|████████▉ | 394/438 [00:10<00:00, 45.33it/s] 91%|█████████ | 399/438 [00:10<00:00, 45.45it/s] 92%|█████████▏| 404/438 [00:10<00:00, 45.68it/s] 93%|█████████▎| 409/438 [00:10<00:00, 45.80it/s] 95%|█████████▍| 414/438 [00:10<00:00, 45.91it/s] 96%|█████████▌| 419/438 [00:10<00:00, 36.46it/s] 97%|█████████▋| 424/438 [00:10<00:00, 38.87it/s] 98%|█████████▊| 429/438 [00:10<00:00, 40.78it/s] 99%|█████████▉| 434/438 [00:10<00:00, 42.24it/s]100%|██████████| 438/438 [00:11<00:00, 39.62it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 16:24:19,956 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:19,957 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:19,957 >>   eval_loss               =       1.05
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:19,957 >>   eval_runtime            = 0:00:11.09
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:19,957 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:19,957 >>   eval_samples_per_second =     315.24
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:19,957 >>   eval_steps_per_second   =     39.484
[INFO|trainer_pt_utils.py:913] 2023-08-28 16:24:19,957 >>   perplexity              =     2.8576
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:24:58,425 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:24:58,615 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:24:58,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:24:58,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:24:58,616 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:25:00,447 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:25:00,448 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:25:00,993 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:25:02,343 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:25:02,495 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:04,684 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:04,686 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:04,687 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:04,687 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:25:04,687 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:25:06,078 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:25:06,253 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:25:07,979 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:25:08,408 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:25:08,408 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-156
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/checkpoint-234
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.51it/s]Extractor Predicting: 2it [00:01,  1.53it/s]Extractor Predicting: 3it [00:01,  1.56it/s]Extractor Predicting: 4it [00:02,  1.57it/s]Extractor Predicting: 5it [00:03,  1.57it/s]Extractor Predicting: 6it [00:03,  1.50it/s]Extractor Predicting: 7it [00:04,  1.54it/s]Extractor Predicting: 8it [00:05,  1.60it/s]Extractor Predicting: 9it [00:05,  1.59it/s]Extractor Predicting: 10it [00:06,  1.57it/s]Extractor Predicting: 11it [00:07,  1.48it/s]Extractor Predicting: 12it [00:07,  1.49it/s]Extractor Predicting: 13it [00:08,  1.49it/s]Extractor Predicting: 14it [00:09,  1.52it/s]Extractor Predicting: 15it [00:09,  1.44it/s]Extractor Predicting: 16it [00:10,  1.47it/s]Extractor Predicting: 17it [00:11,  1.50it/s]Extractor Predicting: 18it [00:11,  1.53it/s]Extractor Predicting: 19it [00:12,  1.55it/s]Extractor Predicting: 20it [00:13,  1.49it/s]Extractor Predicting: 21it [00:13,  1.54it/s]Extractor Predicting: 22it [00:14,  1.57it/s]Extractor Predicting: 23it [00:15,  1.58it/s]Extractor Predicting: 24it [00:15,  1.64it/s]Extractor Predicting: 25it [00:16,  1.58it/s]Extractor Predicting: 26it [00:16,  1.59it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:18,  1.58it/s]Extractor Predicting: 29it [00:18,  1.57it/s]Extractor Predicting: 30it [00:19,  1.47it/s]Extractor Predicting: 31it [00:20,  1.49it/s]Extractor Predicting: 32it [00:20,  1.48it/s]Extractor Predicting: 33it [00:21,  1.51it/s]Extractor Predicting: 34it [00:22,  1.55it/s]Extractor Predicting: 35it [00:22,  1.47it/s]Extractor Predicting: 36it [00:23,  1.47it/s]Extractor Predicting: 37it [00:24,  1.51it/s]Extractor Predicting: 38it [00:24,  1.53it/s]Extractor Predicting: 39it [00:25,  1.49it/s]Extractor Predicting: 40it [00:26,  1.45it/s]Extractor Predicting: 41it [00:26,  1.46it/s]Extractor Predicting: 42it [00:27,  1.49it/s]Extractor Predicting: 43it [00:28,  1.52it/s]Extractor Predicting: 44it [00:28,  1.53it/s]Extractor Predicting: 45it [00:29,  1.46it/s]Extractor Predicting: 46it [00:30,  1.40it/s]Extractor Predicting: 47it [00:31,  1.41it/s]Extractor Predicting: 48it [00:31,  1.47it/s]Extractor Predicting: 49it [00:32,  1.46it/s]Extractor Predicting: 50it [00:33,  1.42it/s]Extractor Predicting: 51it [00:33,  1.47it/s]Extractor Predicting: 52it [00:34,  1.47it/s]Extractor Predicting: 53it [00:35,  1.47it/s]Extractor Predicting: 54it [00:35,  1.51it/s]Extractor Predicting: 55it [00:36,  1.41it/s]Extractor Predicting: 56it [00:37,  1.44it/s]Extractor Predicting: 57it [00:37,  1.50it/s]Extractor Predicting: 58it [00:38,  1.49it/s]Extractor Predicting: 59it [00:39,  1.47it/s]Extractor Predicting: 60it [00:40,  1.39it/s]Extractor Predicting: 61it [00:40,  1.34it/s]Extractor Predicting: 62it [00:41,  1.41it/s]Extractor Predicting: 63it [00:42,  1.47it/s]Extractor Predicting: 64it [00:42,  1.46it/s]Extractor Predicting: 65it [00:43,  1.52it/s]Extractor Predicting: 66it [00:44,  1.46it/s]Extractor Predicting: 67it [00:44,  1.49it/s]Extractor Predicting: 68it [00:45,  1.53it/s]Extractor Predicting: 69it [00:46,  1.54it/s]Extractor Predicting: 70it [00:46,  1.58it/s]Extractor Predicting: 71it [00:47,  1.51it/s]Extractor Predicting: 72it [00:47,  1.55it/s]Extractor Predicting: 73it [00:48,  1.55it/s]Extractor Predicting: 74it [00:49,  1.53it/s]Extractor Predicting: 75it [00:49,  1.56it/s]Extractor Predicting: 76it [00:50,  1.50it/s]Extractor Predicting: 77it [00:51,  1.56it/s]Extractor Predicting: 78it [00:51,  1.55it/s]Extractor Predicting: 79it [00:52,  1.61it/s]Extractor Predicting: 80it [00:53,  1.63it/s]Extractor Predicting: 81it [00:53,  1.54it/s]Extractor Predicting: 82it [00:54,  1.54it/s]Extractor Predicting: 83it [00:55,  1.54it/s]Extractor Predicting: 84it [00:55,  1.54it/s]Extractor Predicting: 85it [00:56,  1.53it/s]Extractor Predicting: 86it [00:57,  1.46it/s]Extractor Predicting: 87it [00:57,  1.50it/s]Extractor Predicting: 88it [00:58,  1.52it/s]Extractor Predicting: 89it [00:59,  1.53it/s]Extractor Predicting: 90it [00:59,  1.55it/s]Extractor Predicting: 91it [01:00,  1.45it/s]Extractor Predicting: 92it [01:01,  1.49it/s]Extractor Predicting: 93it [01:01,  1.50it/s]Extractor Predicting: 94it [01:02,  1.54it/s]Extractor Predicting: 95it [01:02,  1.58it/s]Extractor Predicting: 96it [01:03,  1.47it/s]Extractor Predicting: 97it [01:04,  1.48it/s]Extractor Predicting: 98it [01:05,  1.49it/s]Extractor Predicting: 99it [01:05,  1.50it/s]Extractor Predicting: 100it [01:06,  1.51it/s]Extractor Predicting: 101it [01:07,  1.48it/s]Extractor Predicting: 102it [01:07,  1.50it/s]Extractor Predicting: 103it [01:08,  1.52it/s]Extractor Predicting: 104it [01:09,  1.51it/s]Extractor Predicting: 105it [01:09,  1.53it/s]Extractor Predicting: 106it [01:10,  1.44it/s]Extractor Predicting: 107it [01:11,  1.45it/s]Extractor Predicting: 108it [01:11,  1.49it/s]Extractor Predicting: 109it [01:12,  1.49it/s]Extractor Predicting: 110it [01:13,  1.50it/s]Extractor Predicting: 111it [01:13,  1.53it/s]Extractor Predicting: 112it [01:14,  1.46it/s]Extractor Predicting: 113it [01:15,  1.46it/s]Extractor Predicting: 114it [01:15,  1.49it/s]Extractor Predicting: 115it [01:16,  1.50it/s]Extractor Predicting: 116it [01:17,  1.48it/s]Extractor Predicting: 117it [01:17,  1.40it/s]Extractor Predicting: 118it [01:18,  1.45it/s]Extractor Predicting: 119it [01:19,  1.47it/s]Extractor Predicting: 120it [01:19,  1.48it/s]Extractor Predicting: 121it [01:20,  1.50it/s]Extractor Predicting: 122it [01:21,  1.42it/s]Extractor Predicting: 123it [01:21,  1.45it/s]Extractor Predicting: 124it [01:22,  1.35it/s]Extractor Predicting: 125it [01:23,  1.38it/s]Extractor Predicting: 126it [01:24,  1.35it/s]Extractor Predicting: 127it [01:24,  1.42it/s]Extractor Predicting: 128it [01:25,  1.44it/s]Extractor Predicting: 129it [01:26,  1.46it/s]Extractor Predicting: 130it [01:26,  1.48it/s]Extractor Predicting: 131it [01:27,  1.39it/s]Extractor Predicting: 132it [01:28,  1.43it/s]Extractor Predicting: 133it [01:29,  1.47it/s]Extractor Predicting: 134it [01:29,  1.49it/s]Extractor Predicting: 135it [01:30,  1.46it/s]Extractor Predicting: 136it [01:31,  1.36it/s]Extractor Predicting: 137it [01:31,  1.42it/s]Extractor Predicting: 138it [01:32,  1.44it/s]Extractor Predicting: 139it [01:33,  1.45it/s]Extractor Predicting: 140it [01:33,  1.47it/s]Extractor Predicting: 141it [01:34,  1.39it/s]Extractor Predicting: 142it [01:35,  1.38it/s]Extractor Predicting: 143it [01:36,  1.41it/s]Extractor Predicting: 144it [01:36,  1.47it/s]Extractor Predicting: 145it [01:37,  1.66it/s]Extractor Predicting: 145it [01:37,  1.49it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:20,886 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:21,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:21,003 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:21,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:21,004 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:27:22,731 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:27:22,732 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:27:23,603 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:27:24,934 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:27:25,104 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:30,142 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:30,288 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:30,288 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:30,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:27:30,289 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:27:32,210 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:27:32,211 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:27:33,303 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:27:34,148 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:27:34,148 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.4596949891067538,
  "recall": 0.18101229625393195,
  "score": 0.25974558883873616,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.50it/s]Extractor Predicting: 2it [00:01,  1.57it/s]Extractor Predicting: 3it [00:01,  1.58it/s]Extractor Predicting: 4it [00:02,  1.59it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.44it/s]Extractor Predicting: 7it [00:04,  1.48it/s]Extractor Predicting: 8it [00:05,  1.48it/s]Extractor Predicting: 9it [00:05,  1.54it/s]Extractor Predicting: 10it [00:06,  1.61it/s]Extractor Predicting: 11it [00:07,  1.61it/s]Extractor Predicting: 12it [00:07,  1.61it/s]Extractor Predicting: 13it [00:08,  1.54it/s]Extractor Predicting: 14it [00:09,  1.57it/s]Extractor Predicting: 15it [00:09,  1.53it/s]Extractor Predicting: 16it [00:10,  1.55it/s]Extractor Predicting: 17it [00:10,  1.57it/s]Extractor Predicting: 18it [00:11,  1.52it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:12,  1.52it/s]Extractor Predicting: 21it [00:13,  1.55it/s]Extractor Predicting: 22it [00:14,  1.56it/s]Extractor Predicting: 23it [00:14,  1.50it/s]Extractor Predicting: 24it [00:15,  1.50it/s]Extractor Predicting: 25it [00:16,  1.53it/s]Extractor Predicting: 26it [00:16,  1.54it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:18,  1.47it/s]Extractor Predicting: 29it [00:18,  1.50it/s]Extractor Predicting: 30it [00:19,  1.45it/s]Extractor Predicting: 31it [00:20,  1.50it/s]Extractor Predicting: 32it [00:20,  1.52it/s]Extractor Predicting: 33it [00:21,  1.43it/s]Extractor Predicting: 34it [00:22,  1.46it/s]Extractor Predicting: 35it [00:22,  1.50it/s]Extractor Predicting: 36it [00:23,  1.53it/s]Extractor Predicting: 37it [00:24,  1.57it/s]Extractor Predicting: 38it [00:24,  1.49it/s]Extractor Predicting: 39it [00:25,  1.52it/s]Extractor Predicting: 40it [00:26,  1.51it/s]Extractor Predicting: 41it [00:26,  1.52it/s]Extractor Predicting: 42it [00:27,  1.53it/s]Extractor Predicting: 43it [00:28,  1.44it/s]Extractor Predicting: 44it [00:28,  1.47it/s]Extractor Predicting: 45it [00:29,  1.48it/s]Extractor Predicting: 46it [00:30,  1.54it/s]Extractor Predicting: 47it [00:30,  1.56it/s]Extractor Predicting: 48it [00:31,  1.49it/s]Extractor Predicting: 49it [00:32,  1.53it/s]Extractor Predicting: 50it [00:32,  1.54it/s]Extractor Predicting: 51it [00:33,  1.55it/s]Extractor Predicting: 52it [00:34,  1.58it/s]Extractor Predicting: 53it [00:34,  1.49it/s]Extractor Predicting: 54it [00:35,  1.52it/s]Extractor Predicting: 55it [00:36,  1.53it/s]Extractor Predicting: 56it [00:36,  1.52it/s]Extractor Predicting: 57it [00:37,  1.51it/s]Extractor Predicting: 58it [00:38,  1.54it/s]Extractor Predicting: 59it [00:38,  1.51it/s]Extractor Predicting: 60it [00:39,  1.43it/s]Extractor Predicting: 61it [00:40,  1.47it/s]Extractor Predicting: 62it [00:40,  1.52it/s]Extractor Predicting: 63it [00:41,  1.52it/s]Extractor Predicting: 64it [00:42,  1.56it/s]Extractor Predicting: 65it [00:42,  1.50it/s]Extractor Predicting: 66it [00:43,  1.53it/s]Extractor Predicting: 67it [00:43,  1.56it/s]Extractor Predicting: 68it [00:44,  1.59it/s]Extractor Predicting: 69it [00:45,  1.62it/s]Extractor Predicting: 70it [00:45,  1.52it/s]Extractor Predicting: 71it [00:46,  1.55it/s]Extractor Predicting: 72it [00:47,  1.58it/s]Extractor Predicting: 73it [00:47,  1.59it/s]Extractor Predicting: 74it [00:48,  1.57it/s]Extractor Predicting: 75it [00:49,  1.46it/s]Extractor Predicting: 76it [00:49,  1.51it/s]Extractor Predicting: 77it [00:50,  1.57it/s]Extractor Predicting: 78it [00:51,  1.58it/s]Extractor Predicting: 79it [00:51,  1.60it/s]Extractor Predicting: 80it [00:52,  1.53it/s]Extractor Predicting: 81it [00:52,  1.55it/s]Extractor Predicting: 82it [00:53,  1.57it/s]Extractor Predicting: 83it [00:54,  1.56it/s]Extractor Predicting: 84it [00:54,  1.58it/s]Extractor Predicting: 85it [00:55,  1.52it/s]Extractor Predicting: 86it [00:56,  1.57it/s]Extractor Predicting: 87it [00:56,  1.60it/s]Extractor Predicting: 88it [00:57,  1.59it/s]Extractor Predicting: 89it [00:58,  1.56it/s]Extractor Predicting: 90it [00:58,  1.50it/s]Extractor Predicting: 91it [00:59,  1.56it/s]Extractor Predicting: 92it [01:00,  1.55it/s]Extractor Predicting: 93it [01:00,  1.53it/s]Extractor Predicting: 94it [01:01,  1.53it/s]Extractor Predicting: 95it [01:02,  1.46it/s]Extractor Predicting: 96it [01:02,  1.51it/s]Extractor Predicting: 97it [01:03,  1.53it/s]Extractor Predicting: 98it [01:04,  1.52it/s]Extractor Predicting: 99it [01:04,  1.55it/s]Extractor Predicting: 100it [01:05,  1.48it/s]Extractor Predicting: 101it [01:06,  1.53it/s]Extractor Predicting: 102it [01:06,  1.49it/s]Extractor Predicting: 103it [01:07,  1.53it/s]Extractor Predicting: 104it [01:07,  1.53it/s]Extractor Predicting: 105it [01:08,  1.51it/s]Extractor Predicting: 106it [01:09,  1.53it/s]Extractor Predicting: 107it [01:10,  1.46it/s]Extractor Predicting: 108it [01:10,  1.51it/s]Extractor Predicting: 109it [01:11,  1.53it/s]Extractor Predicting: 110it [01:11,  1.58it/s]Extractor Predicting: 111it [01:12,  1.48it/s]Extractor Predicting: 112it [01:13,  1.42it/s]Extractor Predicting: 113it [01:14,  1.46it/s]Extractor Predicting: 114it [01:14,  1.47it/s]Extractor Predicting: 115it [01:15,  1.50it/s]Extractor Predicting: 116it [01:16,  1.52it/s]Extractor Predicting: 117it [01:16,  1.47it/s]Extractor Predicting: 118it [01:17,  1.49it/s]Extractor Predicting: 119it [01:18,  1.52it/s]Extractor Predicting: 120it [01:18,  1.52it/s]Extractor Predicting: 121it [01:19,  1.53it/s]Extractor Predicting: 122it [01:20,  1.49it/s]Extractor Predicting: 123it [01:20,  1.52it/s]Extractor Predicting: 124it [01:21,  1.54it/s]Extractor Predicting: 125it [01:21,  1.53it/s]Extractor Predicting: 126it [01:22,  1.51it/s]Extractor Predicting: 127it [01:23,  1.45it/s]Extractor Predicting: 128it [01:24,  1.49it/s]Extractor Predicting: 129it [01:24,  1.54it/s]Extractor Predicting: 130it [01:25,  1.53it/s]Extractor Predicting: 131it [01:25,  1.57it/s]Extractor Predicting: 132it [01:26,  1.48it/s]Extractor Predicting: 133it [01:27,  1.50it/s]Extractor Predicting: 134it [01:27,  1.50it/s]Extractor Predicting: 135it [01:28,  1.51it/s]Extractor Predicting: 136it [01:29,  1.53it/s]Extractor Predicting: 137it [01:29,  1.49it/s]Extractor Predicting: 138it [01:30,  1.52it/s]Extractor Predicting: 139it [01:31,  1.54it/s]Extractor Predicting: 140it [01:31,  1.54it/s]Extractor Predicting: 141it [01:32,  1.57it/s]Extractor Predicting: 142it [01:33,  1.45it/s]Extractor Predicting: 143it [01:33,  1.49it/s]Extractor Predicting: 144it [01:34,  1.51it/s]Extractor Predicting: 145it [01:34,  1.90it/s]Extractor Predicting: 145it [01:34,  1.53it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:04,594 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:04,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:04,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:04,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:04,759 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:30:06,384 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:30:06,385 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:30:07,260 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:30:08,801 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:30:09,084 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:14,038 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:14,680 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:14,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:14,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:30:14,681 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:30:17,619 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:30:17,816 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:30:18,945 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:30:19,469 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:30:19,469 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_single_is_eval_False.jsonl",
  "precision": 0.442916915720263,
  "recall": 0.2142237640936687,
  "score": 0.28877630553390493,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.52it/s]Extractor Predicting: 1it [00:01,  1.46s/it]
[INFO|configuration_utils.py:515] 2023-08-28 16:30:46,664 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:30:46,848 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 16:30:47,833 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:30:47,834 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:1150] 2023-08-28 16:30:48,130 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 16:33:08,636 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:1345] 2023-08-28 16:33:09,212 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:515] 2023-08-28 16:33:12,796 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 16:33:13,499 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter2/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 16:33:15,059 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:33:16,482 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:33:17,011 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:33:17,011 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:33:17,011 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:33:17,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 16:33:17,012 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model/tokenizer_config.json
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.75,
  "recall": 0.14634146341463414,
  "score": 0.24489795918367344,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/results_multi_is_eval_False.json"
}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
Generating:   0%|          | 0/10 [00:00<?, ?it/s][WARNING|generation_utils.py:914] 2023-08-28 16:33:19,407 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:19,992 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:21,616 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:22,160 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:22,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:23,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:24,641 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:25,220 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:25,749 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:26,276 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:26,998 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:28,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:28,610 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:29,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:29,679 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:30,247 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:31,096 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:31,683 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:32,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:33,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  10%|█         | 1/10 [00:14<02:08, 14.27s/it][WARNING|generation_utils.py:914] 2023-08-28 16:33:33,846 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:34,509 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:35,043 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:35,596 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:36,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:36,599 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:37,113 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:38,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:38,966 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:39,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:40,184 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:40,994 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:41,477 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:41,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:42,525 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:43,059 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:43,531 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:44,660 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:45,203 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:45,780 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  20%|██        | 2/10 [00:26<01:46, 13.31s/it][WARNING|generation_utils.py:914] 2023-08-28 16:33:46,310 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:46,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:47,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:48,350 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:48,970 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:49,608 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:50,207 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:51,397 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:52,074 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:52,636 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:53,266 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:54,905 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:55,532 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:56,240 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:56,823 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:58,513 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:59,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:33:59,898 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:01,533 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:02,164 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:02,850 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  30%|███       | 3/10 [00:43<01:45, 15.02s/it][WARNING|generation_utils.py:914] 2023-08-28 16:34:04,322 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:04,891 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:05,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:06,081 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:06,615 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:07,516 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:08,138 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:08,701 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:09,274 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:09,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:11,140 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:11,671 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:12,270 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:12,952 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:13,956 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:14,484 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:15,976 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:16,579 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:17,123 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:18,347 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  40%|████      | 4/10 [00:59<01:31, 15.24s/it][WARNING|generation_utils.py:914] 2023-08-28 16:34:18,951 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:19,521 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:20,359 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:21,305 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:21,902 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:22,400 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:23,016 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:23,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:24,577 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:25,197 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:25,776 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:26,263 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:26,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:27,849 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:28,663 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:29,293 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:29,876 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:31,405 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:31,986 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:32,633 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:33,277 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:34,942 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  50%|█████     | 5/10 [01:16<01:18, 15.75s/it][WARNING|generation_utils.py:914] 2023-08-28 16:34:35,592 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:36,112 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:36,708 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:37,907 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:38,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:38,936 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:39,504 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:40,031 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:41,060 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:41,842 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:42,385 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:43,010 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:43,719 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:45,298 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:45,782 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:46,360 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:46,845 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:47,903 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:48,420 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:48,969 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:49,713 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  60%|██████    | 6/10 [01:30<01:01, 15.37s/it][WARNING|generation_utils.py:914] 2023-08-28 16:34:50,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:50,979 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:51,474 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:51,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:52,441 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:53,006 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:54,137 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:54,621 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:55,094 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:55,515 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:55,980 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:56,499 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:58,038 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:58,565 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:59,054 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:34:59,553 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:00,678 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:01,155 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:01,693 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:02,234 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  70%|███████   | 7/10 [01:43<00:43, 14.42s/it][WARNING|generation_utils.py:914] 2023-08-28 16:35:03,037 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:03,603 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:04,181 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:04,818 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:05,411 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:06,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:07,369 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:07,927 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:08,440 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:09,017 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:10,179 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:10,759 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:11,249 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:11,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:12,297 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:13,198 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:13,744 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:14,282 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:14,834 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:15,349 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:16,136 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:16,685 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  80%|████████  | 8/10 [01:57<00:28, 14.48s/it][WARNING|generation_utils.py:914] 2023-08-28 16:35:17,291 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:17,855 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:18,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:19,260 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:19,836 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:20,431 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:21,051 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:21,885 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:22,467 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:23,080 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:23,634 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:25,018 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:25,630 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:26,183 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:26,694 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:27,590 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:28,133 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:28,659 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:29,225 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:29,820 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:30,476 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:30,978 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating:  90%|█████████ | 9/10 [02:12<00:14, 14.39s/it][WARNING|generation_utils.py:914] 2023-08-28 16:35:31,491 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:32,048 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:32,550 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:33,093 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:33,745 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:34,258 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:34,841 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:35,444 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:35,993 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:36,534 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:37,383 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:37,887 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:38,389 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:38,879 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:39,352 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:39,809 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:40,620 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:41,162 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:41,644 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:42,131 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:42,705 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:43,185 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:44,145 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
[WARNING|generation_utils.py:914] 2023-08-28 16:35:44,623 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Generating: 100%|██████████| 10/10 [02:25<00:00, 14.14s/it]Generating: 100%|██████████| 10/10 [02:25<00:00, 14.57s/it]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:37,459 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:37,804 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:37,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:37,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:37,805 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:36:43,106 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:36:43,655 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:36:46,250 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:36:48,226 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:36:48,226 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:55,414 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:55,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:55,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:55,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:36:55,643 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:36:58,328 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:36:58,329 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:37:00,131 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:37:01,280 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:37:01,280 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 62, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 117, 'raw': 128}
{'target': 600, 'success': 146, 'raw': 160}
{'target': 600, 'success': 176, 'raw': 192}
{'target': 600, 'success': 207, 'raw': 224}
{'target': 600, 'success': 236, 'raw': 256}
{'target': 600, 'success': 266, 'raw': 288}
{'target': 600, 'success': 296, 'raw': 320}
{'target': 600, 'success': 327, 'raw': 352}
{'target': 600, 'success': 358, 'raw': 384}
{'target': 600, 'success': 387, 'raw': 416}
{'target': 600, 'success': 417, 'raw': 448}
{'target': 600, 'success': 449, 'raw': 480}
{'target': 600, 'success': 481, 'raw': 512}
{'target': 600, 'success': 512, 'raw': 544}
{'target': 600, 'success': 543, 'raw': 576}
{'target': 600, 'success': 572, 'raw': 608}
{'target': 600, 'success': 600, 'raw': 640}
{'prompt': 'Relation : director .', 'success_rate': 0.9375, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 94, 'raw': 96}
{'target': 600, 'success': 125, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 185, 'raw': 192}
{'target': 600, 'success': 217, 'raw': 224}
{'target': 600, 'success': 247, 'raw': 256}
{'target': 600, 'success': 276, 'raw': 288}
{'target': 600, 'success': 308, 'raw': 320}
{'target': 600, 'success': 340, 'raw': 352}
{'target': 600, 'success': 369, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 462, 'raw': 480}
{'target': 600, 'success': 494, 'raw': 512}
{'target': 600, 'success': 525, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 584, 'raw': 608}
{'target': 600, 'success': 612, 'raw': 640}
{'prompt': 'Relation : located on terrain feature .', 'success_rate': 0.95625, 'errors': {''}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 145, 'raw': 160}
{'target': 600, 'success': 175, 'raw': 192}
{'target': 600, 'success': 205, 'raw': 224}
{'target': 600, 'success': 233, 'raw': 256}
{'target': 600, 'success': 262, 'raw': 288}
{'target': 600, 'success': 293, 'raw': 320}
{'target': 600, 'success': 322, 'raw': 352}
{'target': 600, 'success': 348, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 402, 'raw': 448}
{'target': 600, 'success': 429, 'raw': 480}
{'target': 600, 'success': 457, 'raw': 512}
{'target': 600, 'success': 487, 'raw': 544}
{'target': 600, 'success': 516, 'raw': 576}
{'target': 600, 'success': 547, 'raw': 608}
{'target': 600, 'success': 572, 'raw': 640}
{'target': 600, 'success': 601, 'raw': 672}
{'prompt': 'Relation : mother .', 'success_rate': 0.8943452380952381, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 31, 'raw': 32}
{'target': 600, 'success': 61, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 123, 'raw': 128}
{'target': 600, 'success': 154, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 215, 'raw': 224}
{'target': 600, 'success': 246, 'raw': 256}
{'target': 600, 'success': 275, 'raw': 288}
{'target': 600, 'success': 307, 'raw': 320}
{'target': 600, 'success': 338, 'raw': 352}
{'target': 600, 'success': 368, 'raw': 384}
{'target': 600, 'success': 400, 'raw': 416}
{'target': 600, 'success': 430, 'raw': 448}
{'target': 600, 'success': 461, 'raw': 480}
{'target': 600, 'success': 492, 'raw': 512}
{'target': 600, 'success': 523, 'raw': 544}
{'target': 600, 'success': 552, 'raw': 576}
{'target': 600, 'success': 582, 'raw': 608}
{'target': 600, 'success': 613, 'raw': 640}
{'prompt': 'Relation : part of .', 'success_rate': 0.9578125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 86, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 140, 'raw': 160}
{'target': 600, 'success': 168, 'raw': 192}
{'target': 600, 'success': 194, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 330, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 419, 'raw': 480}
{'target': 600, 'success': 448, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 501, 'raw': 576}
{'target': 600, 'success': 529, 'raw': 608}
{'target': 600, 'success': 559, 'raw': 640}
{'target': 600, 'success': 584, 'raw': 672}
{'target': 600, 'success': 612, 'raw': 704}
{'prompt': 'Relation : residence .', 'success_rate': 0.8693181818181818, 'errors': {'', 'too many values to unpack (expected 2)', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 60, 'raw': 64}
{'target': 600, 'success': 89, 'raw': 96}
{'target': 600, 'success': 115, 'raw': 128}
{'target': 600, 'success': 144, 'raw': 160}
{'target': 600, 'success': 172, 'raw': 192}
{'target': 600, 'success': 202, 'raw': 224}
{'target': 600, 'success': 231, 'raw': 256}
{'target': 600, 'success': 259, 'raw': 288}
{'target': 600, 'success': 290, 'raw': 320}
{'target': 600, 'success': 319, 'raw': 352}
{'target': 600, 'success': 347, 'raw': 384}
{'target': 600, 'success': 377, 'raw': 416}
{'target': 600, 'success': 408, 'raw': 448}
{'target': 600, 'success': 438, 'raw': 480}
{'target': 600, 'success': 466, 'raw': 512}
{'target': 600, 'success': 495, 'raw': 544}
{'target': 600, 'success': 523, 'raw': 576}
{'target': 600, 'success': 553, 'raw': 608}
{'target': 600, 'success': 583, 'raw': 640}
{'target': 600, 'success': 613, 'raw': 672}
{'prompt': 'Relation : developer .', 'success_rate': 0.9122023809523809, 'errors': {'', 'not enough values to unpack (expected 2, got 1)'}}
{'target': 600, 'success': 32, 'raw': 32}
{'target': 600, 'success': 63, 'raw': 64}
{'target': 600, 'success': 93, 'raw': 96}
{'target': 600, 'success': 121, 'raw': 128}
{'target': 600, 'success': 153, 'raw': 160}
{'target': 600, 'success': 184, 'raw': 192}
{'target': 600, 'success': 214, 'raw': 224}
{'target': 600, 'success': 243, 'raw': 256}
{'target': 600, 'success': 273, 'raw': 288}
{'target': 600, 'success': 304, 'raw': 320}
{'target': 600, 'success': 336, 'raw': 352}
{'target': 600, 'success': 367, 'raw': 384}
{'target': 600, 'success': 399, 'raw': 416}
{'target': 600, 'success': 431, 'raw': 448}
{'target': 600, 'success': 463, 'raw': 480}
{'target': 600, 'success': 495, 'raw': 512}
{'target': 600, 'success': 527, 'raw': 544}
{'target': 600, 'success': 559, 'raw': 576}
{'target': 600, 'success': 590, 'raw': 608}
{'target': 600, 'success': 621, 'raw': 640}
{'prompt': 'Relation : located in or next to body of water .', 'success_rate': 0.9703125, 'errors': {''}}
{'target': 600, 'success': 29, 'raw': 32}
{'target': 600, 'success': 55, 'raw': 64}
{'target': 600, 'success': 82, 'raw': 96}
{'target': 600, 'success': 107, 'raw': 128}
{'target': 600, 'success': 135, 'raw': 160}
{'target': 600, 'success': 163, 'raw': 192}
{'target': 600, 'success': 193, 'raw': 224}
{'target': 600, 'success': 220, 'raw': 256}
{'target': 600, 'success': 249, 'raw': 288}
{'target': 600, 'success': 277, 'raw': 320}
{'target': 600, 'success': 305, 'raw': 352}
{'target': 600, 'success': 332, 'raw': 384}
{'target': 600, 'success': 358, 'raw': 416}
{'target': 600, 'success': 387, 'raw': 448}
{'target': 600, 'success': 414, 'raw': 480}
{'target': 600, 'success': 443, 'raw': 512}
{'target': 600, 'success': 473, 'raw': 544}
{'target': 600, 'success': 500, 'raw': 576}
{'target': 600, 'success': 526, 'raw': 608}
{'target': 600, 'success': 555, 'raw': 640}
{'target': 600, 'success': 585, 'raw': 672}
{'target': 600, 'success': 616, 'raw': 704}
{'prompt': 'Relation : member of political party .', 'success_rate': 0.875, 'errors': {'', 'too many values to unpack (expected 2)'}}
{'target': 600, 'success': 28, 'raw': 32}
{'target': 600, 'success': 56, 'raw': 64}
{'target': 600, 'success': 83, 'raw': 96}
{'target': 600, 'success': 108, 'raw': 128}
{'target': 600, 'success': 133, 'raw': 160}
{'target': 600, 'success': 160, 'raw': 192}
{'target': 600, 'success': 186, 'raw': 224}
{'target': 600, 'success': 212, 'raw': 256}
{'target': 600, 'success': 239, 'raw': 288}
{'target': 600, 'success': 265, 'raw': 320}
{'target': 600, 'success': 290, 'raw': 352}
{'target': 600, 'success': 320, 'raw': 384}
{'target': 600, 'success': 348, 'raw': 416}
{'target': 600, 'success': 377, 'raw': 448}
{'target': 600, 'success': 405, 'raw': 480}
{'target': 600, 'success': 435, 'raw': 512}
{'target': 600, 'success': 462, 'raw': 544}
{'target': 600, 'success': 489, 'raw': 576}
{'target': 600, 'success': 517, 'raw': 608}
{'target': 600, 'success': 549, 'raw': 640}
{'target': 600, 'success': 578, 'raw': 672}
{'target': 600, 'success': 607, 'raw': 704}
{'prompt': 'Relation : operator .', 'success_rate': 0.8622159090909091, 'errors': {''}}
{'target': 600, 'success': 23, 'raw': 32}
{'target': 600, 'success': 48, 'raw': 64}
{'target': 600, 'success': 78, 'raw': 96}
{'target': 600, 'success': 104, 'raw': 128}
{'target': 600, 'success': 126, 'raw': 160}
{'target': 600, 'success': 151, 'raw': 192}
{'target': 600, 'success': 181, 'raw': 224}
{'target': 600, 'success': 204, 'raw': 256}
{'target': 600, 'success': 229, 'raw': 288}
{'target': 600, 'success': 255, 'raw': 320}
{'target': 600, 'success': 280, 'raw': 352}
{'target': 600, 'success': 303, 'raw': 384}
{'target': 600, 'success': 330, 'raw': 416}
{'target': 600, 'success': 353, 'raw': 448}
{'target': 600, 'success': 380, 'raw': 480}
{'target': 600, 'success': 404, 'raw': 512}
{'target': 600, 'success': 431, 'raw': 544}
{'target': 600, 'success': 452, 'raw': 576}
{'target': 600, 'success': 476, 'raw': 608}
{'target': 600, 'success': 504, 'raw': 640}
{'target': 600, 'success': 529, 'raw': 672}
{'target': 600, 'success': 550, 'raw': 704}
{'target': 600, 'success': 580, 'raw': 736}
{'target': 600, 'success': 603, 'raw': 768}
{'prompt': 'Relation : position held .', 'success_rate': 0.78515625, 'errors': {''}}
{'estimate': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/4.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/4_ext.jsonl'}}
estimate vocab size: 7763
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 7863, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/data/estimate.txt
Extractor Estimating: 0it [00:00, ?it/s]Extractor Estimating: 1it [00:00,  1.32it/s]Extractor Estimating: 2it [00:01,  1.41it/s]Extractor Estimating: 3it [00:02,  1.45it/s]Extractor Estimating: 4it [00:02,  1.50it/s]Extractor Estimating: 5it [00:04,  1.04s/it]Extractor Estimating: 6it [00:05,  1.04it/s]Extractor Estimating: 7it [00:05,  1.18it/s]Extractor Estimating: 8it [00:09,  1.58s/it]Extractor Estimating: 9it [00:09,  1.28s/it]Extractor Estimating: 10it [00:10,  1.07s/it]Extractor Estimating: 11it [00:11,  1.08s/it]Extractor Estimating: 12it [00:12,  1.04it/s]Extractor Estimating: 13it [00:12,  1.13it/s]Extractor Estimating: 14it [00:13,  1.24it/s]Extractor Estimating: 15it [00:14,  1.07it/s]Extractor Estimating: 16it [00:15,  1.22it/s]Extractor Estimating: 17it [00:15,  1.33it/s]Extractor Estimating: 18it [00:16,  1.40it/s]Extractor Estimating: 19it [00:17,  1.20it/s]Extractor Estimating: 20it [00:18,  1.28it/s]Extractor Estimating: 21it [00:18,  1.38it/s]Extractor Estimating: 22it [00:19,  1.45it/s]Extractor Estimating: 23it [00:20,  1.35it/s]Extractor Estimating: 24it [00:21,  1.23it/s]Extractor Estimating: 25it [00:21,  1.28it/s]Extractor Estimating: 26it [00:22,  1.42it/s]Extractor Estimating: 27it [00:22,  1.53it/s]Extractor Estimating: 28it [00:23,  1.66it/s]Extractor Estimating: 29it [00:24,  1.13it/s]Extractor Estimating: 30it [00:25,  1.30it/s]Extractor Estimating: 31it [00:25,  1.47it/s]Extractor Estimating: 32it [00:26,  1.58it/s]Extractor Estimating: 33it [00:27,  1.52it/s]Extractor Estimating: 34it [00:27,  1.61it/s]Extractor Estimating: 35it [00:28,  1.70it/s]Extractor Estimating: 36it [00:28,  1.81it/s]Extractor Estimating: 37it [00:29,  1.85it/s]Extractor Estimating: 38it [00:29,  1.83it/s]Extractor Estimating: 39it [00:30,  1.44it/s]Extractor Estimating: 40it [00:31,  1.56it/s]Extractor Estimating: 41it [00:31,  1.65it/s]Extractor Estimating: 42it [00:32,  1.72it/s]Extractor Estimating: 43it [00:32,  1.72it/s]Extractor Estimating: 44it [00:33,  1.62it/s]Extractor Estimating: 45it [00:34,  1.68it/s]Extractor Estimating: 46it [00:34,  1.74it/s]Extractor Estimating: 47it [00:35,  1.77it/s]Extractor Estimating: 48it [00:35,  1.82it/s]Extractor Estimating: 49it [00:36,  1.80it/s]Extractor Estimating: 50it [00:37,  1.69it/s]Extractor Estimating: 51it [00:37,  1.72it/s]Extractor Estimating: 52it [00:38,  1.75it/s]Extractor Estimating: 53it [00:38,  1.76it/s]Extractor Estimating: 54it [00:39,  1.80it/s]Extractor Estimating: 55it [00:39,  1.86it/s]Extractor Estimating: 56it [00:40,  1.38it/s]Extractor Estimating: 57it [00:41,  1.52it/s]Extractor Estimating: 58it [00:41,  1.58it/s]Extractor Estimating: 59it [00:42,  1.62it/s]Extractor Estimating: 60it [00:43,  1.62it/s]Extractor Estimating: 61it [00:44,  1.09it/s]Extractor Estimating: 62it [00:45,  1.23it/s]Extractor Estimating: 63it [00:45,  1.39it/s]Extractor Estimating: 64it [00:46,  1.47it/s]Extractor Estimating: 65it [00:47,  1.45it/s]Extractor Estimating: 66it [00:47,  1.50it/s]Extractor Estimating: 67it [00:48,  1.58it/s]Extractor Estimating: 68it [00:48,  1.64it/s]Extractor Estimating: 69it [00:49,  1.66it/s]Extractor Estimating: 70it [00:50,  1.53it/s]Extractor Estimating: 71it [00:50,  1.54it/s]Extractor Estimating: 72it [00:51,  1.62it/s]Extractor Estimating: 73it [00:51,  1.71it/s]Extractor Estimating: 74it [00:52,  1.77it/s]Extractor Estimating: 75it [00:52,  1.86it/s]Extractor Estimating: 76it [00:54,  1.06it/s]Extractor Estimating: 77it [00:55,  1.15it/s]Extractor Estimating: 78it [00:56,  1.24it/s]Extractor Estimating: 79it [00:57,  1.00s/it]Extractor Estimating: 80it [00:58,  1.14it/s]Extractor Estimating: 81it [00:58,  1.26it/s]Extractor Estimating: 82it [00:59,  1.33it/s]Extractor Estimating: 83it [01:00,  1.18it/s]Extractor Estimating: 84it [01:01,  1.26it/s]Extractor Estimating: 85it [01:01,  1.33it/s]Extractor Estimating: 86it [01:02,  1.40it/s]Extractor Estimating: 87it [01:03,  1.21it/s]Extractor Estimating: 88it [01:04,  1.32it/s]Extractor Estimating: 89it [01:04,  1.42it/s]Extractor Estimating: 90it [01:05,  1.49it/s]Extractor Estimating: 91it [01:06,  1.44it/s]Extractor Estimating: 92it [01:07,  1.30it/s]Extractor Estimating: 93it [01:07,  1.35it/s]Extractor Estimating: 94it [01:08,  1.45it/s]Extractor Estimating: 95it [01:08,  1.53it/s]Extractor Estimating: 96it [01:09,  1.55it/s]Extractor Estimating: 97it [01:10,  1.36it/s]Extractor Estimating: 98it [01:11,  1.38it/s]Extractor Estimating: 99it [01:11,  1.40it/s]Extractor Estimating: 100it [01:12,  1.44it/s]Extractor Estimating: 101it [01:13,  1.48it/s]Extractor Estimating: 102it [01:14,  1.02s/it]Extractor Estimating: 103it [01:15,  1.09it/s]Extractor Estimating: 104it [01:16,  1.21it/s]Extractor Estimating: 105it [01:16,  1.22it/s]Extractor Estimating: 106it [01:17,  1.35it/s]Extractor Estimating: 107it [01:18,  1.39it/s]Extractor Estimating: 108it [01:18,  1.47it/s]Extractor Estimating: 109it [01:19,  1.31it/s]Extractor Estimating: 110it [01:20,  1.39it/s]Extractor Estimating: 111it [01:20,  1.50it/s]Extractor Estimating: 112it [01:21,  1.56it/s]Extractor Estimating: 113it [01:22,  1.65it/s]Extractor Estimating: 114it [01:22,  1.53it/s]Extractor Estimating: 115it [01:23,  1.59it/s]Extractor Estimating: 116it [01:23,  1.59it/s]Extractor Estimating: 117it [01:24,  1.57it/s]Extractor Estimating: 118it [01:25,  1.61it/s]Extractor Estimating: 119it [01:26,  1.22it/s]Extractor Estimating: 120it [01:27,  1.30it/s]Extractor Estimating: 121it [01:27,  1.38it/s]Extractor Estimating: 122it [01:28,  1.48it/s]Extractor Estimating: 123it [01:29,  1.37it/s]Extractor Estimating: 124it [01:29,  1.43it/s]Extractor Estimating: 125it [01:30,  1.49it/s]Extractor Estimating: 126it [01:31,  1.51it/s]Extractor Estimating: 127it [01:31,  1.50it/s]Extractor Estimating: 128it [01:32,  1.34it/s]Extractor Estimating: 129it [01:33,  1.40it/s]Extractor Estimating: 130it [01:33,  1.45it/s]Extractor Estimating: 131it [01:34,  1.48it/s]Extractor Estimating: 132it [01:35,  1.52it/s]Extractor Estimating: 133it [01:36,  1.02it/s]Extractor Estimating: 134it [01:37,  1.14it/s]Extractor Estimating: 135it [01:38,  1.21it/s]Extractor Estimating: 136it [01:40,  1.33s/it]Extractor Estimating: 137it [01:41,  1.13s/it]Extractor Estimating: 138it [01:42,  1.23s/it]Extractor Estimating: 139it [01:43,  1.07s/it]Extractor Estimating: 140it [01:44,  1.06it/s]Extractor Estimating: 141it [01:44,  1.15it/s]Extractor Estimating: 142it [01:46,  1.13s/it]Extractor Estimating: 143it [01:47,  1.00it/s]Extractor Estimating: 144it [01:47,  1.14it/s]Extractor Estimating: 145it [01:49,  1.06it/s]Extractor Estimating: 146it [01:49,  1.20it/s]Extractor Estimating: 147it [01:51,  1.07s/it]Extractor Estimating: 148it [01:51,  1.07it/s]Extractor Estimating: 149it [01:52,  1.19it/s]Extractor Estimating: 150it [01:53,  1.08it/s]Extractor Estimating: 151it [01:54,  1.25it/s]Extractor Estimating: 152it [01:54,  1.43it/s]Extractor Estimating: 153it [01:55,  1.59it/s]Extractor Estimating: 154it [01:55,  1.73it/s]Extractor Estimating: 155it [01:56,  1.53it/s]Extractor Estimating: 156it [01:56,  1.67it/s]Extractor Estimating: 157it [01:57,  1.82it/s]Extractor Estimating: 158it [01:57,  1.98it/s]Extractor Estimating: 159it [01:58,  2.10it/s]Extractor Estimating: 160it [01:58,  2.13it/s]Extractor Estimating: 161it [01:59,  1.36it/s]Extractor Estimating: 162it [02:00,  1.56it/s]Extractor Estimating: 163it [02:00,  1.75it/s]Extractor Estimating: 164it [02:01,  1.81it/s]Extractor Estimating: 165it [02:01,  1.75it/s]Extractor Estimating: 166it [02:02,  1.46it/s]Extractor Estimating: 167it [02:03,  1.66it/s]Extractor Estimating: 168it [02:03,  1.83it/s]Extractor Estimating: 169it [02:04,  1.93it/s]Extractor Estimating: 170it [02:04,  2.04it/s]Extractor Estimating: 171it [02:04,  2.10it/s]Extractor Estimating: 172it [02:05,  1.56it/s]Extractor Estimating: 173it [02:06,  1.67it/s]Extractor Estimating: 174it [02:06,  1.82it/s]Extractor Estimating: 175it [02:07,  1.90it/s]Extractor Estimating: 176it [02:07,  1.88it/s]Extractor Estimating: 177it [02:08,  1.73it/s]Extractor Estimating: 178it [02:09,  1.59it/s]Extractor Estimating: 179it [02:09,  1.59it/s]Extractor Estimating: 180it [02:10,  1.59it/s]Extractor Estimating: 181it [02:11,  1.60it/s]Extractor Estimating: 182it [02:11,  1.61it/s]Extractor Estimating: 183it [02:12,  1.40it/s]Extractor Estimating: 184it [02:13,  1.49it/s]Extractor Estimating: 185it [02:13,  1.57it/s]Extractor Estimating: 186it [02:14,  1.64it/s]Extractor Estimating: 187it [02:15,  1.67it/s]Extractor Estimating: 188it [02:16,  1.32it/s]Extractor Estimating: 189it [02:16,  1.44it/s]Extractor Estimating: 190it [02:17,  1.52it/s]Extractor Estimating: 191it [02:17,  1.57it/s]Extractor Estimating: 192it [02:18,  1.63it/s]Extractor Estimating: 193it [02:19,  1.20it/s]Extractor Estimating: 194it [02:20,  1.31it/s]Extractor Estimating: 195it [02:21,  1.17it/s]Extractor Estimating: 196it [02:21,  1.30it/s]Extractor Estimating: 197it [02:22,  1.38it/s]Extractor Estimating: 198it [02:23,  1.50it/s]Extractor Estimating: 199it [02:24,  1.20it/s]Extractor Estimating: 200it [02:25,  1.28it/s]Extractor Estimating: 201it [02:25,  1.36it/s]Extractor Estimating: 202it [02:26,  1.43it/s]Extractor Estimating: 203it [02:27,  1.07it/s]Extractor Estimating: 204it [02:28,  1.21it/s]Extractor Estimating: 205it [02:28,  1.32it/s]Extractor Estimating: 206it [02:29,  1.41it/s]Extractor Estimating: 207it [02:31,  1.05it/s]Extractor Estimating: 208it [02:31,  1.18it/s]Extractor Estimating: 209it [02:32,  1.28it/s]Extractor Estimating: 210it [02:32,  1.34it/s]Extractor Estimating: 211it [02:34,  1.11s/it]Extractor Estimating: 212it [02:35,  1.04it/s]Extractor Estimating: 213it [02:36,  1.16it/s]Extractor Estimating: 214it [02:38,  1.23s/it]Extractor Estimating: 215it [02:38,  1.04s/it]Extractor Estimating: 216it [02:39,  1.12it/s]Extractor Estimating: 217it [02:40,  1.14it/s]Extractor Estimating: 218it [02:40,  1.28it/s]Extractor Estimating: 219it [02:41,  1.41it/s]Extractor Estimating: 220it [02:41,  1.48it/s]Extractor Estimating: 221it [02:42,  1.53it/s]Extractor Estimating: 222it [02:43,  1.35it/s]Extractor Estimating: 223it [02:44,  1.43it/s]Extractor Estimating: 224it [02:44,  1.51it/s]Extractor Estimating: 225it [02:45,  1.58it/s]Extractor Estimating: 226it [02:45,  1.61it/s]Extractor Estimating: 227it [02:46,  1.51it/s]Extractor Estimating: 228it [02:47,  1.56it/s]Extractor Estimating: 229it [02:47,  1.66it/s]Extractor Estimating: 230it [02:48,  1.69it/s]Extractor Estimating: 231it [02:48,  1.67it/s]Extractor Estimating: 232it [02:49,  1.53it/s]Extractor Estimating: 233it [02:50,  1.51it/s]Extractor Estimating: 234it [02:50,  1.54it/s]Extractor Estimating: 235it [02:52,  1.12it/s]Extractor Estimating: 236it [02:52,  1.27it/s]Extractor Estimating: 237it [02:53,  1.39it/s]Extractor Estimating: 238it [02:54,  1.50it/s]Extractor Estimating: 239it [02:54,  1.32it/s]Extractor Estimating: 240it [02:55,  1.45it/s]Extractor Estimating: 241it [02:56,  1.55it/s]Extractor Estimating: 242it [02:56,  1.68it/s]Extractor Estimating: 243it [02:57,  1.71it/s]Extractor Estimating: 244it [02:57,  1.49it/s]Extractor Estimating: 245it [02:58,  1.56it/s]Extractor Estimating: 246it [02:59,  1.59it/s]Extractor Estimating: 247it [02:59,  1.62it/s]Extractor Estimating: 248it [03:00,  1.69it/s]Extractor Estimating: 249it [03:01,  1.23it/s]Extractor Estimating: 250it [03:02,  1.33it/s]Extractor Estimating: 250it [03:02,  1.37it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:52,578 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:52,738 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:52,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:52,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:41:52,739 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 16:41:57,194 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 16:41:57,381 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:41:58,983 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 16:42:00,469 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:42:01,438 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:07,534 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:07,723 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:07,723 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:07,723 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 16:42:07,723 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 16:42:11,104 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 16:42:12,582 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 16:42:15,770 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 16:42:16,683 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 16:42:16,683 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/module.py:1113: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
[INFO|training_args.py:725] 2023-08-28 18:09:13,643 >> PyTorch: setting up devices
[INFO|training_args.py:625] 2023-08-28 18:09:18,264 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
{'filter_data_nb_rel': {'path_pseudo': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/synthetic/4_ext.jsonl', 'path_train': 'zero_rte/fewrel/unseen_5_seed_4/train.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/4.jsonl', 'total_pseudo_per_label': 500, 'pseudo_ratio': 1.0, 'with_train': True, 'by_rel': False, 'version': 'all', 'rescale_train': False}}
{'num_pseudo': 5000, 'num_train': 0}
num of filtered data: 5000 mean pseudo reward: 0.94053732612794
fit {'path_train': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/filtered/4.jsonl', 'path_dev': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl'}
train vocab size: 17502
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 17602, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/train.txt
{"train_model: args: ModelArguments(model_class='JointModel', model_read_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter4/model', model_write_ckpt='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/model', pretrained_wv='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/train.txt', label_config=None, batch_size=24, evaluate_interval=500, max_steps=10000, max_epoches=20, decay_rate=0.05, token_emb_dim=100, char_encoder='lstm', char_emb_dim=30, cased=False, hidden_dim=200, num_layers=3, crf=None, loss_reduction='sum', maxlen=None, dropout=0.5, optimizer='adam', lr=0.001, vocab_size=17602, vocab_file=None, ner_tag_vocab_size=64, re_tag_vocab_size=250, lm_emb_dim=1024, lm_emb_path='albert-large-v2', head_emb_dim=384, tag_form='iob2', warm_steps=1000, grad_period=1, device='cuda', seed=42)"}
warm up: learning rate was adjusted to 1e-06
warm up: learning rate was adjusted to 1.1e-05
warm up: learning rate was adjusted to 2.1000000000000002e-05
warm up: learning rate was adjusted to 3.1e-05
warm up: learning rate was adjusted to 4.1e-05
warm up: learning rate was adjusted to 5.1000000000000006e-05
warm up: learning rate was adjusted to 6.1e-05
warm up: learning rate was adjusted to 7.1e-05
warm up: learning rate was adjusted to 8.1e-05
warm up: learning rate was adjusted to 9.1e-05
g_step 100, step 100, avg_time 0.981, loss:616.3112
warm up: learning rate was adjusted to 0.000101
warm up: learning rate was adjusted to 0.000111
warm up: learning rate was adjusted to 0.000121
warm up: learning rate was adjusted to 0.000131
warm up: learning rate was adjusted to 0.000141
warm up: learning rate was adjusted to 0.00015099999999999998
warm up: learning rate was adjusted to 0.000161
warm up: learning rate was adjusted to 0.000171
warm up: learning rate was adjusted to 0.00018099999999999998
warm up: learning rate was adjusted to 0.000191
g_step 200, step 200, avg_time 0.973, loss:529.6258
warm up: learning rate was adjusted to 0.000201
warm up: learning rate was adjusted to 0.000211
warm up: learning rate was adjusted to 0.000221
warm up: learning rate was adjusted to 0.000231
warm up: learning rate was adjusted to 0.000241
warm up: learning rate was adjusted to 0.000251
warm up: learning rate was adjusted to 0.000261
warm up: learning rate was adjusted to 0.00027100000000000003
warm up: learning rate was adjusted to 0.00028100000000000005
warm up: learning rate was adjusted to 0.00029099999999999997
g_step 300, step 91, avg_time 0.981, loss:480.9195
warm up: learning rate was adjusted to 0.000301
warm up: learning rate was adjusted to 0.000311
warm up: learning rate was adjusted to 0.000321
warm up: learning rate was adjusted to 0.000331
warm up: learning rate was adjusted to 0.00034100000000000005
warm up: learning rate was adjusted to 0.000351
warm up: learning rate was adjusted to 0.000361
warm up: learning rate was adjusted to 0.000371
warm up: learning rate was adjusted to 0.000381
warm up: learning rate was adjusted to 0.000391
g_step 400, step 191, avg_time 0.986, loss:483.6490
warm up: learning rate was adjusted to 0.00040100000000000004
warm up: learning rate was adjusted to 0.000411
warm up: learning rate was adjusted to 0.000421
warm up: learning rate was adjusted to 0.000431
warm up: learning rate was adjusted to 0.000441
warm up: learning rate was adjusted to 0.000451
warm up: learning rate was adjusted to 0.00046100000000000004
warm up: learning rate was adjusted to 0.000471
warm up: learning rate was adjusted to 0.000481
warm up: learning rate was adjusted to 0.000491
g_step 500, step 82, avg_time 0.957, loss:428.1739
>> valid entity prec:0.5815, rec:0.6373, f1:0.6081
>> valid relation prec:0.4207, rec:0.1304, f1:0.1991
>> valid relation with NER prec:0.4207, rec:0.1304, f1:0.1991
new max entity f1 on valid!
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
warm up: learning rate was adjusted to 0.000501
warm up: learning rate was adjusted to 0.0005110000000000001
warm up: learning rate was adjusted to 0.000521
warm up: learning rate was adjusted to 0.000531
warm up: learning rate was adjusted to 0.000541
warm up: learning rate was adjusted to 0.0005510000000000001
warm up: learning rate was adjusted to 0.0005610000000000001
warm up: learning rate was adjusted to 0.0005710000000000001
warm up: learning rate was adjusted to 0.0005809999999999999
warm up: learning rate was adjusted to 0.0005909999999999999
g_step 600, step 182, avg_time 2.382, loss:444.7286
warm up: learning rate was adjusted to 0.000601
warm up: learning rate was adjusted to 0.000611
warm up: learning rate was adjusted to 0.000621
warm up: learning rate was adjusted to 0.000631
warm up: learning rate was adjusted to 0.000641
warm up: learning rate was adjusted to 0.000651
warm up: learning rate was adjusted to 0.000661
warm up: learning rate was adjusted to 0.000671
warm up: learning rate was adjusted to 0.0006810000000000001
warm up: learning rate was adjusted to 0.0006910000000000001
g_step 700, step 73, avg_time 0.982, loss:417.4914
warm up: learning rate was adjusted to 0.000701
warm up: learning rate was adjusted to 0.0007109999999999999
warm up: learning rate was adjusted to 0.000721
warm up: learning rate was adjusted to 0.000731
warm up: learning rate was adjusted to 0.000741
warm up: learning rate was adjusted to 0.000751
warm up: learning rate was adjusted to 0.000761
warm up: learning rate was adjusted to 0.000771
warm up: learning rate was adjusted to 0.000781
warm up: learning rate was adjusted to 0.000791
g_step 800, step 173, avg_time 0.990, loss:430.7949
warm up: learning rate was adjusted to 0.0008010000000000001
warm up: learning rate was adjusted to 0.0008110000000000001
warm up: learning rate was adjusted to 0.0008210000000000001
warm up: learning rate was adjusted to 0.000831
warm up: learning rate was adjusted to 0.000841
warm up: learning rate was adjusted to 0.000851
warm up: learning rate was adjusted to 0.000861
warm up: learning rate was adjusted to 0.000871
warm up: learning rate was adjusted to 0.0008810000000000001
warm up: learning rate was adjusted to 0.000891
g_step 900, step 64, avg_time 0.972, loss:409.9480
warm up: learning rate was adjusted to 0.000901
warm up: learning rate was adjusted to 0.000911
warm up: learning rate was adjusted to 0.000921
warm up: learning rate was adjusted to 0.0009310000000000001
warm up: learning rate was adjusted to 0.0009410000000000001
warm up: learning rate was adjusted to 0.000951
warm up: learning rate was adjusted to 0.0009609999999999999
warm up: learning rate was adjusted to 0.000971
warm up: learning rate was adjusted to 0.0009809999999999999
warm up: learning rate was adjusted to 0.000991
g_step 1000, step 164, avg_time 0.989, loss:399.9550
learning rate was adjusted to 0.0009523809523809524
>> valid entity prec:0.5837, rec:0.5590, f1:0.5711
>> valid relation prec:0.4092, rec:0.1804, f1:0.2504
>> valid relation with NER prec:0.4092, rec:0.1804, f1:0.2504
new max relation f1 on valid!
new max relation f1 with NER on valid!
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1100, step 55, avg_time 2.393, loss:394.4309
g_step 1200, step 155, avg_time 0.990, loss:408.7651
g_step 1300, step 46, avg_time 0.961, loss:381.2564
g_step 1400, step 146, avg_time 0.978, loss:372.6816
g_step 1500, step 37, avg_time 0.974, loss:361.8537
>> valid entity prec:0.5914, rec:0.5887, f1:0.5901
>> valid relation prec:0.4290, rec:0.1701, f1:0.2437
>> valid relation with NER prec:0.4290, rec:0.1701, f1:0.2437
new max averaged entity f1 and relation f1 on valid!
new max averaged entity f1 and relation f1 with NER on valid!
g_step 1600, step 137, avg_time 2.466, loss:368.0896
g_step 1700, step 28, avg_time 0.987, loss:344.3079
g_step 1800, step 128, avg_time 0.975, loss:338.6544
g_step 1900, step 19, avg_time 0.994, loss:340.7558
g_step 2000, step 119, avg_time 0.984, loss:318.6208
learning rate was adjusted to 0.0009090909090909091
>> valid entity prec:0.5937, rec:0.5821, f1:0.5879
>> valid relation prec:0.3659, rec:0.1510, f1:0.2138
>> valid relation with NER prec:0.3659, rec:0.1510, f1:0.2138
g_step 2100, step 10, avg_time 2.220, loss:320.9888
g_step 2200, step 110, avg_time 0.983, loss:301.3186
g_step 2300, step 1, avg_time 0.999, loss:319.2354
g_step 2400, step 101, avg_time 0.973, loss:285.6711
g_step 2500, step 201, avg_time 0.991, loss:309.7509
>> valid entity prec:0.5599, rec:0.6463, f1:0.6000
>> valid relation prec:0.3237, rec:0.1607, f1:0.2148
>> valid relation with NER prec:0.3237, rec:0.1607, f1:0.2148
g_step 2600, step 92, avg_time 2.247, loss:278.0244
g_step 2700, step 192, avg_time 0.972, loss:289.1750
g_step 2800, step 83, avg_time 0.969, loss:274.3781
g_step 2900, step 183, avg_time 0.998, loss:278.4699
g_step 3000, step 74, avg_time 0.995, loss:258.3513
learning rate was adjusted to 0.0008695652173913044
>> valid entity prec:0.5925, rec:0.5748, f1:0.5835
>> valid relation prec:0.3398, rec:0.1507, f1:0.2088
>> valid relation with NER prec:0.3398, rec:0.1507, f1:0.2088
g_step 3100, step 174, avg_time 2.253, loss:274.3014
g_step 3200, step 65, avg_time 0.974, loss:259.7965
g_step 3300, step 165, avg_time 1.000, loss:266.1168
g_step 3400, step 56, avg_time 0.981, loss:269.0659
g_step 3500, step 156, avg_time 0.990, loss:250.6139
>> valid entity prec:0.5664, rec:0.6071, f1:0.5860
>> valid relation prec:0.3144, rec:0.1664, f1:0.2177
>> valid relation with NER prec:0.3144, rec:0.1664, f1:0.2177
g_step 3600, step 47, avg_time 2.237, loss:234.8448
g_step 3700, step 147, avg_time 0.988, loss:233.1350
g_step 3800, step 38, avg_time 0.993, loss:231.7070
g_step 3900, step 138, avg_time 0.995, loss:237.9676
g_step 4000, step 29, avg_time 1.003, loss:225.2772
learning rate was adjusted to 0.0008333333333333334
>> valid entity prec:0.5660, rec:0.5569, f1:0.5614
>> valid relation prec:0.3432, rec:0.1653, f1:0.2231
>> valid relation with NER prec:0.3432, rec:0.1653, f1:0.2231
g_step 4100, step 129, avg_time 2.231, loss:221.6434
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
{'select_model': RelationGenerator(model_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model', data_dir='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/data', model_name='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', do_pretrain=False, encoder_name='generate', pipe_name='text-generation', batch_size=32, grad_accumulation=2, random_seed=42, warmup_ratio=0.2, lr_pretrain=0.0003, lr_finetune=3e-05, epochs_pretrain=3, epochs_finetune=5, train_fp16=True, block_size=128)}
08/28/2023 18:09:18 - WARNING - transformer_base.run_clm_rl -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
08/28/2023 18:09:18 - INFO - transformer_base.run_clm_rl -   Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=2,
greater_is_better=False,
group_by_length=False,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_on_each_node=True,
logging_dir=runs/Aug28_18-09-13_ctolab08.scc.idea,
logging_first_step=False,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=5,
output_dir=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.2,
warmup_steps=0,
weight_decay=0.0,
)
08/28/2023 18:09:24 - WARNING - datasets.builder -   Using custom data configuration default-2558b11cc6fdeed7
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/xuting/.cache/huggingface/datasets/json/default-2558b11cc6fdeed7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...
0 tables [00:00, ? tables/s]                            0 tables [00:00, ? tables/s]                            [INFO|configuration_utils.py:515] 2023-08-28 18:09:54,637 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:09:54,849 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:515] 2023-08-28 18:09:54,849 >> loading configuration file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/config.json
[INFO|configuration_utils.py:553] 2023-08-28 18:09:54,850 >> Model config GPT2Config {
  "_name_or_path": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter3/model",
  "activation_function": "gelu_new",
  "architectures": [
    "Model"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.7.0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1651] 2023-08-28 18:09:55,656 >> Didn't find file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/added_tokens.json. We won't load it.
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:09:56,370 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/vocab.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:09:56,370 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/merges.txt
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:09:56,370 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/tokenizer.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:09:56,370 >> loading file None
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:09:56,370 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/special_tokens_map.json
[INFO|tokenization_utils_base.py:1715] 2023-08-28 18:09:56,370 >> loading file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/tokenizer_config.json
[INFO|modeling_utils.py:1150] 2023-08-28 18:10:02,340 >> loading weights file outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model/pytorch_model.bin
[INFO|modeling_utils.py:1336] 2023-08-28 18:10:06,613 >> All model checkpoint weights were used when initializing Model.

[INFO|modeling_utils.py:1345] 2023-08-28 18:10:07,121 >> All the weights of Model were initialized from the model checkpoint at outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Model for predictions without further training.
Dataset json downloaded and prepared to /home/xuting/.cache/huggingface/datasets/json/default-2558b11cc6fdeed7/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.
PreTrainedTokenizerFast(name_or_path='outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter4/model', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:01<00:07,  1.92s/ba] 40%|████      | 2/5 [00:02<00:03,  1.02s/ba] 60%|██████    | 3/5 [00:02<00:01,  1.55ba/s] 80%|████████  | 4/5 [00:02<00:00,  2.14ba/s]100%|██████████| 5/5 [00:02<00:00,  2.72ba/s]100%|██████████| 5/5 [00:02<00:00,  1.73ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:02<00:06,  2.12s/ba] 50%|█████     | 2/4 [00:02<00:02,  1.02s/ba] 75%|███████▌  | 3/4 [00:02<00:00,  1.54ba/s]100%|██████████| 4/4 [00:02<00:00,  2.28ba/s]100%|██████████| 4/4 [00:02<00:00,  1.40ba/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:02<00:11,  2.82s/ba] 40%|████      | 2/5 [00:03<00:04,  1.58s/ba] 80%|████████  | 4/5 [00:03<00:00,  1.54ba/s]100%|██████████| 5/5 [00:03<00:00,  1.31ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:01<00:05,  1.81s/ba] 50%|█████     | 2/4 [00:01<00:01,  1.24ba/s]100%|██████████| 4/4 [00:02<00:00,  2.91ba/s]100%|██████████| 4/4 [00:02<00:00,  1.95ba/s]
[INFO|trainer.py:414] 2023-08-28 18:10:47,692 >> Using amp fp16 backend
[INFO|trainer.py:1147] 2023-08-28 18:10:48,064 >> ***** Running training *****
[INFO|trainer.py:1148] 2023-08-28 18:10:48,064 >>   Num examples = 5000
[INFO|trainer.py:1149] 2023-08-28 18:10:48,065 >>   Num Epochs = 5
[INFO|trainer.py:1150] 2023-08-28 18:10:48,065 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1151] 2023-08-28 18:10:48,065 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1152] 2023-08-28 18:10:48,065 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1153] 2023-08-28 18:10:48,065 >>   Total optimization steps = 390
  0%|          | 0/390 [00:00<?, ?it/s]  0%|          | 1/390 [00:05<37:28,  5.78s/it]  1%|          | 2/390 [00:11<36:41,  5.67s/it]  1%|          | 3/390 [00:13<26:33,  4.12s/it]  1%|          | 4/390 [00:14<17:20,  2.70s/it]  1%|▏         | 5/390 [00:14<11:43,  1.83s/it]  2%|▏         | 6/390 [00:16<11:43,  1.83s/it]  2%|▏         | 7/390 [00:16<08:28,  1.33s/it]  2%|▏         | 8/390 [00:16<06:21,  1.00it/s]  2%|▏         | 9/390 [00:17<04:55,  1.29it/s]  3%|▎         | 10/390 [00:18<05:54,  1.07it/s]  3%|▎         | 11/390 [00:18<05:00,  1.26it/s]  3%|▎         | 12/390 [00:19<05:10,  1.22it/s]  3%|▎         | 13/390 [00:20<04:09,  1.51it/s]  4%|▎         | 14/390 [00:20<03:26,  1.82it/s]  4%|▍         | 15/390 [00:20<02:56,  2.12it/s]  4%|▍         | 16/390 [00:20<02:35,  2.40it/s]  4%|▍         | 17/390 [00:21<02:20,  2.65it/s]  5%|▍         | 18/390 [00:21<02:10,  2.86it/s]  5%|▍         | 19/390 [00:22<02:59,  2.07it/s]  5%|▌         | 20/390 [00:22<02:36,  2.36it/s]  5%|▌         | 21/390 [00:22<02:20,  2.62it/s]  6%|▌         | 22/390 [00:23<02:09,  2.83it/s]  6%|▌         | 23/390 [00:23<02:01,  3.01it/s]  6%|▌         | 24/390 [00:23<01:56,  3.15it/s]  6%|▋         | 25/390 [00:24<01:52,  3.25it/s]  7%|▋         | 26/390 [00:24<01:49,  3.32it/s]  7%|▋         | 27/390 [00:24<01:47,  3.37it/s]  7%|▋         | 28/390 [00:25<02:18,  2.62it/s]  7%|▋         | 29/390 [00:25<02:07,  2.84it/s]  8%|▊         | 30/390 [00:25<01:59,  3.01it/s]  8%|▊         | 31/390 [00:26<01:54,  3.14it/s]  8%|▊         | 32/390 [00:26<01:50,  3.24it/s]  8%|▊         | 33/390 [00:26<01:47,  3.32it/s]  9%|▊         | 34/390 [00:26<01:45,  3.38it/s]  9%|▉         | 35/390 [00:27<01:44,  3.41it/s]  9%|▉         | 36/390 [00:27<01:42,  3.44it/s]  9%|▉         | 37/390 [00:27<01:41,  3.47it/s] 10%|▉         | 38/390 [00:28<02:35,  2.27it/s] 10%|█         | 39/390 [00:28<02:18,  2.54it/s] 10%|█         | 40/390 [00:29<02:06,  2.76it/s] 11%|█         | 41/390 [00:29<01:58,  2.94it/s] 11%|█         | 42/390 [00:29<01:53,  3.08it/s] 11%|█         | 43/390 [00:29<01:49,  3.18it/s] 11%|█▏        | 44/390 [00:30<01:46,  3.26it/s] 12%|█▏        | 45/390 [00:31<03:24,  1.69it/s] 12%|█▏        | 46/390 [00:31<02:52,  1.99it/s] 12%|█▏        | 47/390 [00:32<02:30,  2.29it/s] 12%|█▏        | 48/390 [00:32<02:13,  2.55it/s] 13%|█▎        | 49/390 [00:32<02:02,  2.78it/s] 13%|█▎        | 50/390 [00:32<01:54,  2.97it/s] 13%|█▎        | 51/390 [00:33<01:48,  3.11it/s] 13%|█▎        | 52/390 [00:33<02:22,  2.38it/s] 14%|█▎        | 53/390 [00:34<02:08,  2.63it/s] 14%|█▍        | 54/390 [00:34<01:58,  2.84it/s] 14%|█▍        | 55/390 [00:34<01:51,  3.01it/s] 14%|█▍        | 56/390 [00:35<01:46,  3.14it/s] 15%|█▍        | 57/390 [00:35<01:42,  3.25it/s] 15%|█▍        | 58/390 [00:35<01:40,  3.32it/s] 15%|█▌        | 59/390 [00:35<01:38,  3.37it/s] 15%|█▌        | 60/390 [00:36<01:36,  3.41it/s] 16%|█▌        | 61/390 [00:36<01:35,  3.44it/s] 16%|█▌        | 62/390 [00:37<02:09,  2.54it/s] 16%|█▌        | 63/390 [00:37<01:58,  2.77it/s] 16%|█▋        | 64/390 [00:37<01:50,  2.95it/s] 17%|█▋        | 65/390 [00:37<01:44,  3.10it/s] 17%|█▋        | 66/390 [00:38<01:40,  3.21it/s] 17%|█▋        | 67/390 [00:38<01:38,  3.29it/s] 17%|█▋        | 68/390 [00:38<01:36,  3.35it/s] 18%|█▊        | 69/390 [00:39<01:34,  3.39it/s] 18%|█▊        | 70/390 [00:39<01:33,  3.42it/s] 18%|█▊        | 71/390 [00:39<01:32,  3.45it/s] 18%|█▊        | 72/390 [00:40<02:53,  1.83it/s] 19%|█▊        | 73/390 [00:41<02:28,  2.13it/s] 19%|█▉        | 74/390 [00:41<02:10,  2.42it/s] 19%|█▉        | 75/390 [00:41<01:58,  2.66it/s] 19%|█▉        | 76/390 [00:41<01:49,  2.87it/s] 20%|█▉        | 77/390 [00:42<01:43,  3.03it/s] 20%|██        | 78/390 [00:42<01:38,  3.16it/s][INFO|trainer.py:2140] 2023-08-28 18:11:30,649 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:11:30,649 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 18:11:30,649 >>   Batch size = 8

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.82it/s][A
  3%|▎         | 12/438 [00:00<00:23, 18.10it/s][A
  4%|▍         | 17/438 [00:00<00:17, 24.15it/s][A
  5%|▌         | 22/438 [00:00<00:14, 29.26it/s][A
  6%|▌         | 27/438 [00:00<00:12, 33.46it/s][A
  7%|▋         | 32/438 [00:01<00:11, 36.71it/s][A
  8%|▊         | 37/438 [00:01<00:10, 39.24it/s][A
 10%|▉         | 42/438 [00:01<00:09, 41.11it/s][A
 11%|█         | 47/438 [00:01<00:09, 42.33it/s][A
 12%|█▏        | 52/438 [00:02<00:23, 16.43it/s][A
 13%|█▎        | 57/438 [00:02<00:23, 16.51it/s][A
 14%|█▍        | 62/438 [00:02<00:18, 20.72it/s][A
 15%|█▌        | 67/438 [00:02<00:14, 24.83it/s][A
 16%|█▋        | 72/438 [00:02<00:12, 28.75it/s][A
 18%|█▊        | 77/438 [00:02<00:11, 32.42it/s][A
 19%|█▊        | 82/438 [00:02<00:10, 35.55it/s][A
 20%|█▉        | 87/438 [00:03<00:09, 38.15it/s][A
 21%|██        | 92/438 [00:03<00:08, 40.12it/s][A
 22%|██▏       | 97/438 [00:03<00:10, 31.58it/s][A
 23%|██▎       | 102/438 [00:03<00:09, 34.89it/s][A
 24%|██▍       | 107/438 [00:03<00:08, 37.49it/s][A
 26%|██▌       | 112/438 [00:03<00:08, 39.72it/s][A
 27%|██▋       | 117/438 [00:03<00:07, 41.39it/s][A
 28%|██▊       | 122/438 [00:03<00:07, 42.60it/s][A
 29%|██▉       | 127/438 [00:04<00:07, 43.55it/s][A
 30%|███       | 132/438 [00:04<00:06, 44.05it/s][A
 31%|███▏      | 137/438 [00:04<00:06, 44.02it/s][A
 32%|███▏      | 142/438 [00:04<00:06, 44.05it/s][A
 34%|███▎      | 147/438 [00:04<00:06, 44.23it/s][A
 35%|███▍      | 152/438 [00:04<00:06, 44.51it/s][A
 36%|███▌      | 157/438 [00:04<00:06, 44.86it/s][A
 37%|███▋      | 162/438 [00:04<00:06, 45.11it/s][A
 38%|███▊      | 167/438 [00:04<00:05, 45.25it/s][A
 39%|███▉      | 172/438 [00:05<00:05, 45.42it/s][A
 40%|████      | 177/438 [00:05<00:16, 15.62it/s][A
 42%|████▏     | 182/438 [00:05<00:13, 18.35it/s][A
 43%|████▎     | 187/438 [00:06<00:11, 22.55it/s][A
 44%|████▍     | 192/438 [00:06<00:09, 26.59it/s][A
 45%|████▍     | 196/438 [00:06<00:10, 22.44it/s][A
 46%|████▌     | 201/438 [00:06<00:08, 26.73it/s][A
 47%|████▋     | 206/438 [00:06<00:07, 30.67it/s][A
 48%|████▊     | 211/438 [00:06<00:06, 34.18it/s][A
 49%|████▉     | 216/438 [00:06<00:05, 37.06it/s][A
 50%|█████     | 221/438 [00:07<00:05, 39.29it/s][A
 52%|█████▏    | 226/438 [00:07<00:05, 41.12it/s][A
 53%|█████▎    | 231/438 [00:07<00:08, 23.59it/s][A
 54%|█████▍    | 236/438 [00:07<00:07, 27.71it/s][A
 55%|█████▌    | 241/438 [00:07<00:06, 31.48it/s][A
 56%|█████▌    | 246/438 [00:07<00:05, 34.72it/s][A
 57%|█████▋    | 251/438 [00:07<00:04, 37.49it/s][A
 58%|█████▊    | 256/438 [00:08<00:04, 39.65it/s][A
 60%|█████▉    | 261/438 [00:08<00:04, 41.29it/s][A
 61%|██████    | 266/438 [00:08<00:04, 42.53it/s][A
 62%|██████▏   | 271/438 [00:08<00:03, 43.13it/s][A
 63%|██████▎   | 276/438 [00:08<00:03, 43.31it/s][A
 64%|██████▍   | 281/438 [00:08<00:03, 43.55it/s][A
 65%|██████▌   | 286/438 [00:08<00:03, 43.95it/s][A
 66%|██████▋   | 291/438 [00:08<00:03, 44.32it/s][A
 68%|██████▊   | 296/438 [00:08<00:03, 44.77it/s][A
 69%|██████▊   | 301/438 [00:09<00:04, 31.14it/s][A
 70%|██████▉   | 305/438 [00:09<00:05, 24.28it/s][A
 71%|███████   | 309/438 [00:09<00:06, 19.70it/s][A
 72%|███████▏  | 314/438 [00:09<00:05, 24.17it/s][A
 73%|███████▎  | 319/438 [00:10<00:04, 28.41it/s][A
 74%|███████▍  | 324/438 [00:10<00:03, 32.23it/s][A
 75%|███████▌  | 329/438 [00:10<00:03, 35.46it/s][A
 76%|███████▋  | 334/438 [00:10<00:02, 38.09it/s][A
 77%|███████▋  | 339/438 [00:10<00:05, 19.77it/s][A
 79%|███████▊  | 344/438 [00:11<00:03, 23.94it/s][A
 80%|███████▉  | 349/438 [00:11<00:03, 27.98it/s][A
 81%|████████  | 354/438 [00:11<00:02, 31.66it/s][A
 82%|████████▏ | 359/438 [00:11<00:02, 34.90it/s][A
 83%|████████▎ | 364/438 [00:11<00:01, 37.58it/s][A
 84%|████████▍ | 369/438 [00:11<00:01, 39.67it/s][A
 85%|████████▌ | 374/438 [00:11<00:01, 41.28it/s][A
 87%|████████▋ | 379/438 [00:11<00:01, 42.15it/s][A
 88%|████████▊ | 384/438 [00:11<00:01, 42.65it/s][A
 89%|████████▉ | 389/438 [00:12<00:01, 43.14it/s][A
 90%|████████▉ | 394/438 [00:12<00:01, 43.64it/s][A
 91%|█████████ | 399/438 [00:12<00:00, 44.23it/s][A
 92%|█████████▏| 404/438 [00:12<00:00, 44.62it/s][A
 93%|█████████▎| 409/438 [00:12<00:01, 28.55it/s][A
 94%|█████████▍| 413/438 [00:16<00:05,  4.50it/s][A
 95%|█████████▍| 416/438 [00:17<00:07,  3.03it/s][A
 96%|█████████▌| 421/438 [00:18<00:03,  4.43it/s][A
 97%|█████████▋| 426/438 [00:18<00:01,  6.28it/s][A
 98%|█████████▊| 431/438 [00:18<00:00,  8.65it/s][A
100%|█████████▉| 436/438 [00:18<00:00, 11.58it/s][A
                                                 [A                                                
100%|██████████| 438/438 [00:18<00:00, 11.58it/s][A 20%|██        | 78/390 [01:00<01:38,  3.16it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:11:51,597 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78
[INFO|configuration_utils.py:351] 2023-08-28 18:11:52,882 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:12:49,584 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:12:53,602 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:12:54,078 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78/special_tokens_map.json
 20%|██        | 79/390 [05:51<8:02:20, 93.06s/it] 21%|██        | 80/390 [05:52<5:37:16, 65.28s/it] 21%|██        | 81/390 [05:52<3:55:46, 45.78s/it] 21%|██        | 82/390 [05:53<2:44:57, 32.13s/it] 21%|██▏       | 83/390 [05:53<1:55:32, 22.58s/it] 22%|██▏       | 84/390 [05:53<1:21:03, 15.89s/it] 22%|██▏       | 85/390 [05:53<56:59, 11.21s/it]   22%|██▏       | 86/390 [05:54<40:12,  7.93s/it] 22%|██▏       | 87/390 [05:54<28:29,  5.64s/it] 23%|██▎       | 88/390 [05:54<20:18,  4.04s/it] 23%|██▎       | 89/390 [05:55<14:36,  2.91s/it] 23%|██▎       | 90/390 [05:55<10:54,  2.18s/it] 23%|██▎       | 91/390 [05:55<08:02,  1.61s/it] 24%|██▎       | 92/390 [05:56<06:02,  1.22s/it] 24%|██▍       | 93/390 [05:56<04:38,  1.06it/s] 24%|██▍       | 94/390 [05:56<03:40,  1.34it/s] 24%|██▍       | 95/390 [05:56<02:59,  1.64it/s] 25%|██▍       | 96/390 [05:57<02:30,  1.95it/s] 25%|██▍       | 97/390 [05:57<02:10,  2.24it/s] 25%|██▌       | 98/390 [05:57<01:56,  2.51it/s] 25%|██▌       | 99/390 [05:58<01:46,  2.73it/s] 26%|██▌       | 100/390 [05:59<02:30,  1.93it/s] 26%|██▌       | 101/390 [05:59<02:09,  2.23it/s] 26%|██▌       | 102/390 [05:59<01:55,  2.50it/s] 26%|██▋       | 103/390 [06:00<01:59,  2.40it/s] 27%|██▋       | 104/390 [06:00<01:47,  2.65it/s] 27%|██▋       | 105/390 [06:00<01:39,  2.86it/s] 27%|██▋       | 106/390 [06:00<01:33,  3.03it/s] 27%|██▋       | 107/390 [06:01<01:29,  3.16it/s] 28%|██▊       | 108/390 [06:01<01:50,  2.56it/s] 28%|██▊       | 109/390 [06:02<01:40,  2.78it/s] 28%|██▊       | 110/390 [06:02<01:34,  2.97it/s] 28%|██▊       | 111/390 [06:02<01:29,  3.11it/s] 29%|██▊       | 112/390 [06:02<01:26,  3.21it/s] 29%|██▉       | 113/390 [06:03<01:24,  3.29it/s] 29%|██▉       | 114/390 [06:03<01:22,  3.35it/s] 29%|██▉       | 115/390 [06:03<01:21,  3.39it/s] 30%|██▉       | 116/390 [06:04<01:20,  3.42it/s] 30%|███       | 117/390 [06:04<01:19,  3.44it/s] 30%|███       | 118/390 [06:05<02:46,  1.63it/s] 31%|███       | 119/390 [06:05<02:19,  1.95it/s] 31%|███       | 120/390 [06:06<02:00,  2.24it/s] 31%|███       | 121/390 [06:06<01:47,  2.51it/s] 31%|███▏      | 122/390 [06:06<01:37,  2.74it/s] 32%|███▏      | 123/390 [06:07<01:31,  2.93it/s] 32%|███▏      | 124/390 [06:07<01:26,  3.08it/s] 32%|███▏      | 125/390 [06:07<01:38,  2.69it/s] 32%|███▏      | 126/390 [06:08<01:31,  2.89it/s] 33%|███▎      | 127/390 [06:08<01:26,  3.05it/s] 33%|███▎      | 128/390 [06:08<01:22,  3.17it/s] 33%|███▎      | 129/390 [06:09<01:20,  3.26it/s] 33%|███▎      | 130/390 [06:09<01:18,  3.33it/s] 34%|███▎      | 131/390 [06:09<01:16,  3.37it/s] 34%|███▍      | 132/390 [06:09<01:15,  3.40it/s] 34%|███▍      | 133/390 [06:10<01:14,  3.43it/s] 34%|███▍      | 134/390 [06:10<01:14,  3.45it/s] 35%|███▍      | 135/390 [06:10<01:25,  3.00it/s] 35%|███▍      | 136/390 [06:11<01:21,  3.13it/s] 35%|███▌      | 137/390 [06:11<01:18,  3.22it/s] 35%|███▌      | 138/390 [06:11<01:16,  3.30it/s] 36%|███▌      | 139/390 [06:12<01:14,  3.35it/s] 36%|███▌      | 140/390 [06:12<01:13,  3.39it/s] 36%|███▌      | 141/390 [06:12<01:12,  3.42it/s] 36%|███▋      | 142/390 [06:12<01:12,  3.44it/s] 37%|███▋      | 143/390 [06:13<01:11,  3.45it/s] 37%|███▋      | 144/390 [06:13<01:11,  3.46it/s] 37%|███▋      | 145/390 [06:14<01:26,  2.83it/s] 37%|███▋      | 146/390 [06:14<01:21,  3.00it/s] 38%|███▊      | 147/390 [06:14<01:17,  3.13it/s] 38%|███▊      | 148/390 [06:14<01:14,  3.23it/s] 38%|███▊      | 149/390 [06:15<01:12,  3.30it/s] 38%|███▊      | 150/390 [06:15<01:11,  3.36it/s] 39%|███▊      | 151/390 [06:15<01:10,  3.40it/s] 39%|███▉      | 152/390 [06:16<01:09,  3.43it/s] 39%|███▉      | 153/390 [06:16<01:08,  3.44it/s] 39%|███▉      | 154/390 [06:16<01:08,  3.45it/s] 40%|███▉      | 155/390 [06:17<01:22,  2.84it/s] 40%|████      | 156/390 [06:17<01:17,  3.01it/s][INFO|trainer.py:2140] 2023-08-28 18:17:05,465 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:17:05,465 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 18:17:05,465 >>   Batch size = 8
{'eval_loss': 1.0977325439453125, 'eval_runtime': 18.4104, 'eval_samples_per_second': 189.947, 'eval_steps_per_second': 23.791, 'epoch': 0.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.36it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.66it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.81it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.09it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.44it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.83it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.40it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.27it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.36it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.53it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.52it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.75it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.73it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.62it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.42it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.21it/s][A
 20%|██        | 88/438 [00:01<00:07, 45.13it/s][A
 21%|██        | 93/438 [00:02<00:07, 45.08it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.29it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.41it/s][A
 25%|██▍       | 108/438 [00:02<00:15, 21.33it/s][A
 26%|██▌       | 113/438 [00:02<00:12, 25.43it/s][A
 27%|██▋       | 118/438 [00:02<00:10, 29.39it/s][A
 28%|██▊       | 123/438 [00:03<00:09, 32.98it/s][A
 29%|██▉       | 128/438 [00:03<00:08, 36.03it/s][A
 30%|███       | 133/438 [00:03<00:07, 38.54it/s][A
 32%|███▏      | 138/438 [00:03<00:07, 40.50it/s][A
 33%|███▎      | 143/438 [00:03<00:07, 41.94it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 42.73it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 43.29it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.13it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.59it/s][A
 38%|███▊      | 168/438 [00:04<00:05, 45.05it/s][A
 39%|███▉      | 173/438 [00:04<00:05, 45.30it/s][A
 41%|████      | 178/438 [00:04<00:05, 45.44it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.50it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.46it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.16it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.10it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.26it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.42it/s][A
 49%|████▊     | 213/438 [00:05<00:04, 45.54it/s][A
 50%|████▉     | 218/438 [00:05<00:04, 45.29it/s][A
 51%|█████     | 223/438 [00:05<00:04, 45.98it/s][A
 52%|█████▏    | 228/438 [00:06<00:12, 16.28it/s][A
 53%|█████▎    | 233/438 [00:06<00:10, 20.21it/s][A
 54%|█████▍    | 238/438 [00:06<00:08, 24.30it/s][A
 55%|█████▌    | 243/438 [00:06<00:06, 28.27it/s][A
 57%|█████▋    | 248/438 [00:06<00:05, 31.97it/s][A
 58%|█████▊    | 253/438 [00:06<00:05, 35.22it/s][A
 59%|█████▉    | 258/438 [00:06<00:04, 37.89it/s][A
 60%|██████    | 263/438 [00:06<00:04, 40.01it/s][A
 61%|██████    | 268/438 [00:06<00:04, 41.11it/s][A
 62%|██████▏   | 273/438 [00:07<00:03, 42.27it/s][A
 63%|██████▎   | 278/438 [00:08<00:03, 43.18it/s][A
 65%|██████▍   | 283/438 [00:09<00:20,  7.69it/s][A
 66%|██████▌   | 288/438 [00:09<00:14, 10.26it/s][A
 67%|██████▋   | 293/438 [00:09<00:10, 13.38it/s][A
 68%|██████▊   | 298/438 [00:09<00:08, 16.99it/s][A
 69%|██████▉   | 303/438 [00:09<00:06, 20.96it/s][A
 70%|███████   | 308/438 [00:09<00:05, 25.05it/s][A
 71%|███████▏  | 313/438 [00:09<00:04, 29.02it/s][A
 73%|███████▎  | 318/438 [00:09<00:03, 32.62it/s][A
 74%|███████▎  | 323/438 [00:09<00:03, 35.41it/s][A
 75%|███████▍  | 328/438 [00:10<00:02, 37.84it/s][A
 76%|███████▌  | 333/438 [00:10<00:02, 39.88it/s][A
 77%|███████▋  | 338/438 [00:10<00:03, 30.57it/s][A
 78%|███████▊  | 343/438 [00:10<00:02, 33.99it/s][A
 79%|███████▉  | 348/438 [00:10<00:02, 36.89it/s][A
 81%|████████  | 353/438 [00:10<00:02, 39.21it/s][A
 82%|████████▏ | 358/438 [00:10<00:01, 40.90it/s][A
 83%|████████▎ | 363/438 [00:10<00:01, 42.34it/s][A
 84%|████████▍ | 368/438 [00:11<00:01, 43.30it/s][A
 85%|████████▌ | 373/438 [00:11<00:01, 44.05it/s][A
 86%|████████▋ | 378/438 [00:11<00:01, 44.09it/s][A
 87%|████████▋ | 383/438 [00:11<00:01, 44.45it/s][A
 89%|████████▊ | 388/438 [00:11<00:01, 44.72it/s][A
 90%|████████▉ | 393/438 [00:11<00:00, 45.11it/s][A
 91%|█████████ | 398/438 [00:11<00:00, 45.31it/s][A
 92%|█████████▏| 403/438 [00:11<00:00, 45.51it/s][A
 93%|█████████▎| 408/438 [00:11<00:00, 45.65it/s][A
 94%|█████████▍| 413/438 [00:12<00:00, 45.65it/s][A
 95%|█████████▌| 418/438 [00:12<00:00, 45.49it/s][A
 97%|█████████▋| 423/438 [00:12<00:00, 45.31it/s][A
 98%|█████████▊| 428/438 [00:12<00:00, 45.21it/s][A
 99%|█████████▉| 433/438 [00:12<00:00, 45.33it/s][A
100%|██████████| 438/438 [00:12<00:00, 45.47it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:12<00:00, 45.47it/s][A 40%|████      | 156/390 [06:30<01:17,  3.01it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:17:18,399 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156
[INFO|configuration_utils.py:351] 2023-08-28 18:17:19,295 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:18:49,320 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:18:56,592 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:18:58,850 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156/special_tokens_map.json
 40%|████      | 157/390 [12:10<6:52:38, 106.26s/it] 41%|████      | 158/390 [12:11<4:48:10, 74.53s/it]  41%|████      | 159/390 [12:11<3:21:11, 52.26s/it] 41%|████      | 160/390 [12:11<2:20:33, 36.67s/it] 41%|████▏     | 161/390 [12:12<1:38:17, 25.75s/it] 42%|████▏     | 162/390 [12:12<1:08:49, 18.11s/it] 42%|████▏     | 163/390 [12:12<48:18, 12.77s/it]   42%|████▏     | 164/390 [12:13<33:59,  9.02s/it] 42%|████▏     | 165/390 [12:13<24:00,  6.40s/it] 43%|████▎     | 166/390 [12:13<17:03,  4.57s/it] 43%|████▎     | 167/390 [12:13<12:12,  3.29s/it] 43%|████▎     | 168/390 [12:14<09:25,  2.55s/it] 43%|████▎     | 169/390 [12:14<06:53,  1.87s/it] 44%|████▎     | 170/390 [12:15<05:06,  1.40s/it] 44%|████▍     | 171/390 [12:15<03:52,  1.06s/it] 44%|████▍     | 172/390 [12:15<03:01,  1.20it/s] 44%|████▍     | 173/390 [12:16<02:25,  1.49it/s] 45%|████▍     | 174/390 [12:16<01:59,  1.80it/s] 45%|████▍     | 175/390 [12:16<01:42,  2.10it/s] 45%|████▌     | 176/390 [12:17<01:29,  2.38it/s] 45%|████▌     | 177/390 [12:17<01:45,  2.02it/s] 46%|████▌     | 178/390 [12:17<01:31,  2.31it/s] 46%|████▌     | 179/390 [12:18<01:22,  2.56it/s] 46%|████▌     | 180/390 [12:18<01:15,  2.78it/s] 46%|████▋     | 181/390 [12:18<01:10,  2.95it/s] 47%|████▋     | 182/390 [12:19<01:07,  3.08it/s] 47%|████▋     | 183/390 [12:19<01:05,  3.18it/s] 47%|████▋     | 184/390 [12:19<01:03,  3.26it/s] 47%|████▋     | 185/390 [12:20<01:01,  3.31it/s] 48%|████▊     | 186/390 [12:20<01:00,  3.35it/s] 48%|████▊     | 187/390 [12:21<01:29,  2.28it/s] 48%|████▊     | 188/390 [12:21<01:19,  2.53it/s] 48%|████▊     | 189/390 [12:21<01:13,  2.75it/s] 49%|████▊     | 190/390 [12:21<01:08,  2.93it/s] 49%|████▉     | 191/390 [12:22<01:04,  3.07it/s] 49%|████▉     | 192/390 [12:22<01:02,  3.17it/s] 49%|████▉     | 193/390 [12:22<01:00,  3.25it/s] 50%|████▉     | 194/390 [12:23<00:59,  3.30it/s] 50%|█████     | 195/390 [12:23<00:58,  3.34it/s] 50%|█████     | 196/390 [12:23<01:09,  2.78it/s] 51%|█████     | 197/390 [12:24<01:05,  2.96it/s] 51%|█████     | 198/390 [12:24<01:02,  3.08it/s] 51%|█████     | 199/390 [12:24<01:00,  3.18it/s] 51%|█████▏    | 200/390 [12:25<00:58,  3.26it/s] 52%|█████▏    | 201/390 [12:25<00:57,  3.31it/s] 52%|█████▏    | 202/390 [12:25<00:56,  3.35it/s] 52%|█████▏    | 203/390 [12:25<00:55,  3.38it/s] 52%|█████▏    | 204/390 [12:26<00:54,  3.40it/s] 53%|█████▎    | 205/390 [12:26<00:54,  3.41it/s] 53%|█████▎    | 206/390 [12:27<01:25,  2.15it/s] 53%|█████▎    | 207/390 [12:27<01:15,  2.42it/s] 53%|█████▎    | 208/390 [12:27<01:08,  2.66it/s] 54%|█████▎    | 209/390 [12:28<01:03,  2.85it/s] 54%|█████▍    | 210/390 [12:28<00:59,  3.01it/s] 54%|█████▍    | 211/390 [12:28<00:57,  3.13it/s] 54%|█████▍    | 212/390 [12:29<00:55,  3.22it/s] 55%|█████▍    | 213/390 [12:29<00:53,  3.29it/s] 55%|█████▍    | 214/390 [12:29<00:52,  3.35it/s] 55%|█████▌    | 215/390 [12:30<01:14,  2.36it/s] 55%|█████▌    | 216/390 [12:30<01:06,  2.61it/s] 56%|█████▌    | 217/390 [12:30<01:01,  2.83it/s] 56%|█████▌    | 218/390 [12:31<00:57,  3.00it/s] 56%|█████▌    | 219/390 [12:31<00:54,  3.13it/s] 56%|█████▋    | 220/390 [12:31<00:52,  3.24it/s] 57%|█████▋    | 221/390 [12:32<00:51,  3.31it/s] 57%|█████▋    | 222/390 [12:32<00:49,  3.36it/s] 57%|█████▋    | 223/390 [12:32<00:49,  3.41it/s] 57%|█████▋    | 224/390 [12:33<01:20,  2.07it/s] 58%|█████▊    | 225/390 [12:33<01:09,  2.36it/s] 58%|█████▊    | 226/390 [12:34<01:02,  2.61it/s] 58%|█████▊    | 227/390 [12:34<00:57,  2.83it/s] 58%|█████▊    | 228/390 [12:34<00:53,  3.00it/s] 59%|█████▊    | 229/390 [12:35<00:51,  3.14it/s] 59%|█████▉    | 230/390 [12:35<00:49,  3.24it/s] 59%|█████▉    | 231/390 [12:35<00:48,  3.31it/s] 59%|█████▉    | 232/390 [12:35<00:46,  3.36it/s] 60%|█████▉    | 233/390 [12:36<01:16,  2.06it/s] 60%|██████    | 234/390 [12:37<01:06,  2.35it/s][INFO|trainer.py:2140] 2023-08-28 18:23:25,205 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:23:25,205 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 18:23:25,205 >>   Batch size = 8
{'eval_loss': 1.1224645376205444, 'eval_runtime': 12.6059, 'eval_samples_per_second': 277.409, 'eval_steps_per_second': 34.746, 'epoch': 1.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.30it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.99it/s][A
  4%|▍         | 18/438 [00:00<00:08, 47.69it/s][A
  5%|▌         | 23/438 [00:00<00:08, 46.94it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.45it/s][A
  8%|▊         | 33/438 [00:00<00:08, 46.08it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.73it/s][A
 10%|▉         | 43/438 [00:00<00:08, 45.58it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.47it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.75it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.89it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.90it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.88it/s][A
 17%|█▋        | 73/438 [00:01<00:07, 45.79it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.69it/s][A
 19%|█▉        | 83/438 [00:02<00:07, 45.54it/s][A
 20%|██        | 88/438 [00:02<00:20, 16.94it/s][A
 21%|██        | 93/438 [00:02<00:16, 20.92it/s][A
 22%|██▏       | 98/438 [00:02<00:13, 25.06it/s][A
 24%|██▎       | 103/438 [00:02<00:11, 28.97it/s][A
 25%|██▍       | 108/438 [00:02<00:10, 32.66it/s][A
 26%|██▌       | 113/438 [00:03<00:09, 35.81it/s][A
 27%|██▋       | 118/438 [00:03<00:08, 38.41it/s][A
 28%|██▊       | 123/438 [00:03<00:07, 40.34it/s][A
 29%|██▉       | 128/438 [00:03<00:07, 41.52it/s][A
 30%|███       | 133/438 [00:03<00:07, 42.38it/s][A
 32%|███▏      | 138/438 [00:03<00:06, 43.18it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 43.81it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 44.58it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 45.10it/s][A
 36%|███▌      | 158/438 [00:04<00:06, 45.34it/s][A
 37%|███▋      | 163/438 [00:04<00:06, 45.54it/s][A
 38%|███▊      | 168/438 [00:04<00:05, 45.46it/s][A
 39%|███▉      | 173/438 [00:04<00:05, 45.22it/s][A
 41%|████      | 178/438 [00:04<00:05, 45.07it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.06it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 45.23it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.41it/s][A
 45%|████▌     | 198/438 [00:05<00:07, 33.37it/s][A
 46%|████▋     | 203/438 [00:05<00:06, 36.44it/s][A
 47%|████▋     | 208/438 [00:06<00:05, 38.88it/s][A
 49%|████▊     | 213/438 [00:06<00:20, 10.98it/s][A
 50%|████▉     | 218/438 [00:06<00:15, 14.21it/s][A
 51%|█████     | 223/438 [00:06<00:11, 17.95it/s][A
 52%|█████▏    | 228/438 [00:06<00:09, 21.97it/s][A
 53%|█████▎    | 233/438 [00:06<00:07, 26.08it/s][A
 54%|█████▍    | 238/438 [00:07<00:06, 29.97it/s][A
 55%|█████▌    | 243/438 [00:07<00:05, 33.54it/s][A
 57%|█████▋    | 248/438 [00:07<00:05, 36.44it/s][A
 58%|█████▊    | 253/438 [00:07<00:04, 38.52it/s][A
 59%|█████▉    | 258/438 [00:07<00:04, 40.18it/s][A
 60%|██████    | 263/438 [00:07<00:04, 41.47it/s][A
 61%|██████    | 268/438 [00:07<00:03, 42.68it/s][A
 62%|██████▏   | 273/438 [00:07<00:03, 43.63it/s][A
 63%|██████▎   | 278/438 [00:07<00:03, 44.34it/s][A
 65%|██████▍   | 283/438 [00:08<00:03, 44.80it/s][A
 66%|██████▌   | 288/438 [00:08<00:03, 45.12it/s][A
 67%|██████▋   | 293/438 [00:08<00:03, 45.30it/s][A
 68%|██████▊   | 298/438 [00:08<00:08, 16.89it/s][A
 69%|██████▉   | 303/438 [00:09<00:06, 20.87it/s][A
 70%|███████   | 308/438 [00:09<00:05, 24.99it/s][A
 71%|███████▏  | 313/438 [00:09<00:04, 28.96it/s][A
 73%|███████▎  | 318/438 [00:09<00:03, 32.56it/s][A
 74%|███████▎  | 323/438 [00:09<00:03, 35.75it/s][A
 75%|███████▍  | 328/438 [00:09<00:02, 38.36it/s][A
 76%|███████▌  | 333/438 [00:09<00:02, 40.34it/s][A
 77%|███████▋  | 338/438 [00:09<00:02, 41.46it/s][A
 78%|███████▊  | 343/438 [00:09<00:02, 42.22it/s][A
 79%|███████▉  | 348/438 [00:10<00:02, 43.09it/s][A
 81%|████████  | 353/438 [00:10<00:01, 43.76it/s][A
 82%|████████▏ | 358/438 [00:10<00:01, 44.18it/s][A
 83%|████████▎ | 363/438 [00:10<00:01, 44.78it/s][A
 84%|████████▍ | 368/438 [00:10<00:01, 45.19it/s][A
 85%|████████▌ | 373/438 [00:10<00:01, 45.42it/s][A
 86%|████████▋ | 378/438 [00:10<00:01, 45.57it/s][A
 87%|████████▋ | 383/438 [00:10<00:01, 45.42it/s][A
 89%|████████▊ | 388/438 [00:10<00:01, 45.18it/s][A
 90%|████████▉ | 393/438 [00:11<00:00, 45.13it/s][A
 91%|█████████ | 398/438 [00:11<00:00, 45.26it/s][A
 92%|█████████▏| 403/438 [00:11<00:00, 45.08it/s][A
 93%|█████████▎| 408/438 [00:11<00:01, 28.93it/s][A
 94%|█████████▍| 413/438 [00:11<00:00, 32.60it/s][A
 95%|█████████▌| 418/438 [00:11<00:00, 35.72it/s][A
 97%|█████████▋| 423/438 [00:11<00:00, 38.31it/s][A
 98%|█████████▊| 428/438 [00:12<00:00, 40.41it/s][A
 99%|█████████▉| 433/438 [00:12<00:00, 41.95it/s][A
100%|██████████| 438/438 [00:12<00:00, 43.17it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:12<00:00, 43.17it/s][A 60%|██████    | 234/390 [12:49<01:06,  2.35it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:23:38,415 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234
[INFO|configuration_utils.py:351] 2023-08-28 18:23:40,203 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:26:56,020 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:27:01,828 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:27:02,557 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234/special_tokens_map.json
 60%|██████    | 235/390 [19:35<5:25:21, 125.95s/it] 61%|██████    | 236/390 [19:36<3:47:00, 88.45s/it]  61%|██████    | 237/390 [19:37<2:38:05, 62.00s/it] 61%|██████    | 238/390 [19:37<1:50:09, 43.49s/it] 61%|██████▏   | 239/390 [19:37<1:16:49, 30.53s/it] 62%|██████▏   | 240/390 [19:38<53:38, 21.46s/it]   62%|██████▏   | 241/390 [19:38<37:30, 15.11s/it] 62%|██████▏   | 242/390 [19:38<26:17, 10.66s/it] 62%|██████▏   | 243/390 [19:38<18:29,  7.55s/it] 63%|██████▎   | 244/390 [19:39<13:04,  5.37s/it] 63%|██████▎   | 245/390 [19:39<09:33,  3.95s/it] 63%|██████▎   | 246/390 [19:40<06:50,  2.85s/it] 63%|██████▎   | 247/390 [19:40<04:58,  2.08s/it] 64%|██████▎   | 248/390 [19:40<03:39,  1.55s/it] 64%|██████▍   | 249/390 [19:40<02:44,  1.17s/it] 64%|██████▍   | 250/390 [19:41<02:06,  1.11it/s] 64%|██████▍   | 251/390 [19:41<01:40,  1.39it/s] 65%|██████▍   | 252/390 [19:41<01:21,  1.69it/s] 65%|██████▍   | 253/390 [19:42<01:08,  2.00it/s] 65%|██████▌   | 254/390 [19:42<00:59,  2.29it/s] 65%|██████▌   | 255/390 [19:42<01:00,  2.24it/s] 66%|██████▌   | 256/390 [19:43<00:53,  2.50it/s] 66%|██████▌   | 257/390 [19:43<00:48,  2.73it/s] 66%|██████▌   | 258/390 [19:43<00:45,  2.91it/s] 66%|██████▋   | 259/390 [19:44<00:42,  3.05it/s] 67%|██████▋   | 260/390 [19:44<00:41,  3.16it/s] 67%|██████▋   | 261/390 [19:44<00:39,  3.24it/s] 67%|██████▋   | 262/390 [19:44<00:38,  3.30it/s] 67%|██████▋   | 263/390 [19:45<00:37,  3.34it/s] 68%|██████▊   | 264/390 [19:45<00:37,  3.38it/s] 68%|██████▊   | 265/390 [19:45<00:42,  2.96it/s] 68%|██████▊   | 266/390 [19:46<00:40,  3.09it/s] 68%|██████▊   | 267/390 [19:46<00:38,  3.19it/s] 69%|██████▊   | 268/390 [19:46<00:37,  3.26it/s] 69%|██████▉   | 269/390 [19:47<00:36,  3.32it/s] 69%|██████▉   | 270/390 [19:47<00:35,  3.36it/s] 69%|██████▉   | 271/390 [19:47<00:35,  3.38it/s] 70%|██████▉   | 272/390 [19:47<00:34,  3.41it/s] 70%|███████   | 273/390 [19:48<00:34,  3.42it/s] 70%|███████   | 274/390 [19:48<00:33,  3.42it/s] 71%|███████   | 275/390 [19:48<00:37,  3.10it/s] 71%|███████   | 276/390 [19:49<00:35,  3.20it/s] 71%|███████   | 277/390 [19:49<00:34,  3.27it/s] 71%|███████▏  | 278/390 [19:49<00:33,  3.32it/s] 72%|███████▏  | 279/390 [19:50<00:32,  3.37it/s] 72%|███████▏  | 280/390 [19:50<00:32,  3.39it/s] 72%|███████▏  | 281/390 [19:50<00:31,  3.41it/s] 72%|███████▏  | 282/390 [19:50<00:31,  3.42it/s] 73%|███████▎  | 283/390 [19:51<00:31,  3.43it/s] 73%|███████▎  | 284/390 [19:51<00:30,  3.44it/s] 73%|███████▎  | 285/390 [19:51<00:30,  3.44it/s] 73%|███████▎  | 286/390 [19:52<00:42,  2.45it/s] 74%|███████▎  | 287/390 [19:52<00:38,  2.69it/s] 74%|███████▍  | 288/390 [19:53<00:35,  2.88it/s] 74%|███████▍  | 289/390 [19:53<00:33,  3.03it/s] 74%|███████▍  | 290/390 [19:53<00:31,  3.14it/s] 75%|███████▍  | 291/390 [19:53<00:30,  3.23it/s] 75%|███████▍  | 292/390 [19:54<00:29,  3.29it/s] 75%|███████▌  | 293/390 [19:54<00:29,  3.34it/s] 75%|███████▌  | 294/390 [19:54<00:28,  3.37it/s] 76%|███████▌  | 295/390 [19:55<00:32,  2.89it/s] 76%|███████▌  | 296/390 [19:55<00:30,  3.04it/s] 76%|███████▌  | 297/390 [19:55<00:29,  3.15it/s] 76%|███████▋  | 298/390 [19:56<00:28,  3.23it/s] 77%|███████▋  | 299/390 [19:56<00:27,  3.29it/s] 77%|███████▋  | 300/390 [19:56<00:27,  3.33it/s] 77%|███████▋  | 301/390 [19:57<00:26,  3.37it/s] 77%|███████▋  | 302/390 [19:57<00:25,  3.39it/s] 78%|███████▊  | 303/390 [19:57<00:25,  3.41it/s] 78%|███████▊  | 304/390 [19:57<00:25,  3.42it/s] 78%|███████▊  | 305/390 [19:58<00:36,  2.30it/s] 78%|███████▊  | 306/390 [19:58<00:32,  2.55it/s] 79%|███████▊  | 307/390 [19:59<00:29,  2.77it/s] 79%|███████▉  | 308/390 [19:59<00:27,  2.94it/s] 79%|███████▉  | 309/390 [19:59<00:26,  3.08it/s] 79%|███████▉  | 310/390 [20:00<00:25,  3.18it/s] 80%|███████▉  | 311/390 [20:00<00:24,  3.26it/s] 80%|████████  | 312/390 [20:00<00:23,  3.31it/s][INFO|trainer.py:2140] 2023-08-28 18:30:48,841 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:30:48,841 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 18:30:48,841 >>   Batch size = 8
{'eval_loss': 1.1306686401367188, 'eval_runtime': 12.2715, 'eval_samples_per_second': 284.968, 'eval_steps_per_second': 35.692, 'epoch': 2.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 56.73it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.55it/s][A
  4%|▍         | 18/438 [00:00<00:26, 15.83it/s][A
  5%|▌         | 23/438 [00:01<00:20, 20.60it/s][A
  6%|▋         | 28/438 [00:01<00:16, 25.24it/s][A
  8%|▊         | 33/438 [00:01<00:13, 29.52it/s][A
  9%|▊         | 38/438 [00:01<00:12, 33.29it/s][A
 10%|▉         | 43/438 [00:01<00:10, 36.40it/s][A
 11%|█         | 48/438 [00:01<00:10, 38.87it/s][A
 12%|█▏        | 53/438 [00:01<00:09, 40.62it/s][A
 13%|█▎        | 58/438 [00:01<00:09, 41.55it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 42.41it/s][A
 16%|█▌        | 68/438 [00:02<00:08, 43.27it/s][A
 17%|█▋        | 73/438 [00:02<00:08, 43.92it/s][A
 18%|█▊        | 78/438 [00:02<00:08, 44.40it/s][A
 19%|█▉        | 83/438 [00:02<00:07, 44.72it/s][A
 20%|██        | 88/438 [00:02<00:07, 45.00it/s][A
 21%|██        | 93/438 [00:02<00:07, 45.18it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 45.25it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.12it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 45.13it/s][A
 26%|██▌       | 113/438 [00:03<00:07, 45.10it/s][A
 27%|██▋       | 118/438 [00:03<00:07, 45.17it/s][A
 28%|██▊       | 123/438 [00:03<00:06, 45.30it/s][A
 29%|██▉       | 128/438 [00:03<00:14, 21.06it/s][A
 30%|███       | 133/438 [00:03<00:12, 25.15it/s][A
 32%|███▏      | 138/438 [00:03<00:10, 29.11it/s][A
 33%|███▎      | 143/438 [00:04<00:09, 32.66it/s][A
 34%|███▍      | 148/438 [00:04<00:08, 35.71it/s][A
 35%|███▍      | 153/438 [00:04<00:07, 38.22it/s][A
 36%|███▌      | 158/438 [00:04<00:06, 40.16it/s][A
 37%|███▋      | 163/438 [00:04<00:06, 41.73it/s][A
 38%|███▊      | 168/438 [00:04<00:06, 42.33it/s][A
 39%|███▉      | 173/438 [00:04<00:06, 42.98it/s][A
 41%|████      | 178/438 [00:04<00:05, 43.63it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 44.20it/s][A
 43%|████▎     | 188/438 [00:05<00:05, 44.59it/s][A
 44%|████▍     | 193/438 [00:05<00:05, 44.90it/s][A
 45%|████▌     | 198/438 [00:05<00:05, 44.96it/s][A
 46%|████▋     | 203/438 [00:05<00:05, 45.31it/s][A
 47%|████▋     | 208/438 [00:05<00:05, 45.26it/s][A
 49%|████▊     | 213/438 [00:05<00:04, 45.17it/s][A
 50%|████▉     | 218/438 [00:05<00:04, 44.94it/s][A
 51%|█████     | 223/438 [00:05<00:04, 45.02it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 45.11it/s][A
 53%|█████▎    | 233/438 [00:06<00:04, 45.28it/s][A
 54%|█████▍    | 238/438 [00:06<00:04, 45.25it/s][A
 55%|█████▌    | 243/438 [00:07<00:04, 45.39it/s][A
 57%|█████▋    | 248/438 [00:07<00:21,  8.69it/s][A
 58%|█████▊    | 253/438 [00:08<00:16, 11.50it/s][A
 59%|█████▉    | 258/438 [00:08<00:12, 14.83it/s][A
 60%|██████    | 263/438 [00:08<00:09, 18.60it/s][A
 61%|██████    | 268/438 [00:08<00:07, 22.65it/s][A
 62%|██████▏   | 273/438 [00:08<00:06, 26.69it/s][A
 63%|██████▎   | 278/438 [00:08<00:05, 30.50it/s][A
 65%|██████▍   | 283/438 [00:08<00:04, 33.90it/s][A
 66%|██████▌   | 288/438 [00:08<00:04, 36.44it/s][A
 67%|██████▋   | 293/438 [00:08<00:03, 38.37it/s][A
 68%|██████▊   | 298/438 [00:10<00:03, 40.03it/s][A
 69%|██████▉   | 303/438 [00:10<00:13,  9.83it/s][A
 70%|███████   | 308/438 [00:10<00:10, 12.87it/s][A
 71%|███████▏  | 313/438 [00:10<00:07, 16.40it/s][A
 73%|███████▎  | 318/438 [00:10<00:05, 20.30it/s][A
 74%|███████▎  | 323/438 [00:10<00:04, 24.41it/s][A
 75%|███████▍  | 328/438 [00:11<00:03, 28.36it/s][A
 76%|███████▌  | 333/438 [00:11<00:03, 31.98it/s][A
 77%|███████▋  | 338/438 [00:11<00:02, 35.12it/s][A
 78%|███████▊  | 343/438 [00:11<00:02, 37.33it/s][A
 79%|███████▉  | 348/438 [00:11<00:02, 39.23it/s][A
 81%|████████  | 353/438 [00:11<00:02, 40.83it/s][A
 82%|████████▏ | 358/438 [00:11<00:01, 42.13it/s][A
 83%|████████▎ | 363/438 [00:11<00:01, 43.12it/s][A
 84%|████████▍ | 368/438 [00:11<00:01, 43.89it/s][A
 85%|████████▌ | 373/438 [00:12<00:01, 44.40it/s][A
 86%|████████▋ | 378/438 [00:12<00:01, 44.75it/s][A
 87%|████████▋ | 383/438 [00:12<00:02, 22.15it/s][A
 89%|████████▊ | 388/438 [00:12<00:01, 26.27it/s][A
 90%|████████▉ | 393/438 [00:12<00:01, 30.13it/s][A
 91%|█████████ | 398/438 [00:12<00:01, 33.56it/s][A
 92%|█████████▏| 403/438 [00:13<00:00, 36.55it/s][A
 93%|█████████▎| 408/438 [00:13<00:00, 38.91it/s][A
 94%|█████████▍| 413/438 [00:13<00:00, 40.76it/s][A
 95%|█████████▌| 418/438 [00:13<00:00, 41.98it/s][A
 97%|█████████▋| 423/438 [00:13<00:00, 42.57it/s][A
 98%|█████████▊| 428/438 [00:13<00:00, 42.95it/s][A
 99%|█████████▉| 433/438 [00:13<00:00, 43.28it/s][A
100%|██████████| 438/438 [00:13<00:00, 43.93it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:13<00:00, 43.93it/s][A 80%|████████  | 312/390 [20:14<00:23,  3.31it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:31:03,530 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312
[INFO|configuration_utils.py:351] 2023-08-28 18:31:05,969 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:32:43,385 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:32:47,942 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:32:49,789 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312/special_tokens_map.json
 80%|████████  | 313/390 [25:48<2:14:07, 104.51s/it] 81%|████████  | 314/390 [25:49<1:32:56, 73.37s/it]  81%|████████  | 315/390 [25:49<1:04:18, 51.45s/it] 81%|████████  | 316/390 [25:49<44:31, 36.10s/it]   81%|████████▏ | 317/390 [25:49<30:51, 25.36s/it] 82%|████████▏ | 318/390 [25:50<21:24, 17.84s/it] 82%|████████▏ | 319/390 [25:50<14:52, 12.57s/it] 82%|████████▏ | 320/390 [25:50<10:22,  8.89s/it] 82%|████████▏ | 321/390 [25:51<07:15,  6.31s/it] 83%|████████▎ | 322/390 [25:51<05:06,  4.50s/it] 83%|████████▎ | 323/390 [25:52<03:51,  3.45s/it] 83%|████████▎ | 324/390 [25:52<02:45,  2.50s/it] 83%|████████▎ | 325/390 [25:53<01:59,  1.84s/it] 84%|████████▎ | 326/390 [25:53<01:27,  1.37s/it] 84%|████████▍ | 327/390 [25:53<01:06,  1.05s/it] 84%|████████▍ | 328/390 [25:53<00:50,  1.22it/s] 84%|████████▍ | 329/390 [25:54<00:40,  1.51it/s] 85%|████████▍ | 330/390 [25:54<00:32,  1.82it/s] 85%|████████▍ | 331/390 [25:54<00:30,  1.92it/s] 85%|████████▌ | 332/390 [25:55<00:26,  2.21it/s] 85%|████████▌ | 333/390 [25:55<00:22,  2.48it/s] 86%|████████▌ | 334/390 [25:55<00:20,  2.72it/s] 86%|████████▌ | 335/390 [25:56<00:18,  2.91it/s] 86%|████████▌ | 336/390 [25:56<00:17,  3.07it/s] 86%|████████▋ | 337/390 [25:56<00:16,  3.19it/s] 87%|████████▋ | 338/390 [25:56<00:15,  3.28it/s] 87%|████████▋ | 339/390 [25:57<00:15,  3.34it/s] 87%|████████▋ | 340/390 [25:57<00:14,  3.39it/s] 87%|████████▋ | 341/390 [25:58<00:18,  2.72it/s] 88%|████████▊ | 342/390 [25:58<00:16,  2.92it/s] 88%|████████▊ | 343/390 [25:58<00:15,  3.07it/s] 88%|████████▊ | 344/390 [25:58<00:14,  3.19it/s] 88%|████████▊ | 345/390 [25:59<00:13,  3.28it/s] 89%|████████▊ | 346/390 [25:59<00:13,  3.35it/s] 89%|████████▉ | 347/390 [25:59<00:12,  3.40it/s] 89%|████████▉ | 348/390 [26:00<00:12,  3.43it/s] 89%|████████▉ | 349/390 [26:00<00:11,  3.45it/s] 90%|████████▉ | 350/390 [26:00<00:11,  3.47it/s] 90%|█████████ | 351/390 [26:01<00:12,  3.02it/s] 90%|█████████ | 352/390 [26:01<00:12,  3.15it/s] 91%|█████████ | 353/390 [26:01<00:11,  3.25it/s] 91%|█████████ | 354/390 [26:01<00:10,  3.33it/s] 91%|█████████ | 355/390 [26:02<00:10,  3.38it/s] 91%|█████████▏| 356/390 [26:02<00:09,  3.42it/s] 92%|█████████▏| 357/390 [26:02<00:09,  3.44it/s] 92%|█████████▏| 358/390 [26:03<00:09,  3.46it/s] 92%|█████████▏| 359/390 [26:03<00:08,  3.48it/s] 92%|█████████▏| 360/390 [26:03<00:08,  3.49it/s] 93%|█████████▎| 361/390 [26:03<00:08,  3.50it/s] 93%|█████████▎| 362/390 [26:04<00:12,  2.30it/s] 93%|█████████▎| 363/390 [26:04<00:10,  2.56it/s] 93%|█████████▎| 364/390 [26:05<00:09,  2.79it/s] 94%|█████████▎| 365/390 [26:05<00:08,  2.97it/s] 94%|█████████▍| 366/390 [26:05<00:07,  3.11it/s] 94%|█████████▍| 367/390 [26:06<00:07,  3.22it/s] 94%|█████████▍| 368/390 [26:06<00:06,  3.30it/s] 95%|█████████▍| 369/390 [26:06<00:06,  3.36it/s] 95%|█████████▍| 370/390 [26:06<00:05,  3.40it/s] 95%|█████████▌| 371/390 [26:07<00:07,  2.49it/s] 95%|█████████▌| 372/390 [26:07<00:06,  2.73it/s] 96%|█████████▌| 373/390 [26:08<00:05,  2.92it/s] 96%|█████████▌| 374/390 [26:08<00:05,  3.07it/s] 96%|█████████▌| 375/390 [26:08<00:04,  3.19it/s] 96%|█████████▋| 376/390 [26:09<00:04,  3.28it/s] 97%|█████████▋| 377/390 [26:09<00:03,  3.34it/s] 97%|█████████▋| 378/390 [26:09<00:03,  3.39it/s] 97%|█████████▋| 379/390 [26:10<00:05,  2.03it/s] 97%|█████████▋| 380/390 [26:11<00:05,  1.86it/s] 98%|█████████▊| 381/390 [26:11<00:04,  2.16it/s] 98%|█████████▊| 382/390 [26:11<00:03,  2.44it/s] 98%|█████████▊| 383/390 [26:12<00:02,  2.68it/s] 98%|█████████▊| 384/390 [26:12<00:02,  2.89it/s] 99%|█████████▊| 385/390 [26:12<00:01,  3.04it/s] 99%|█████████▉| 386/390 [26:12<00:01,  3.17it/s] 99%|█████████▉| 387/390 [26:13<00:00,  3.26it/s] 99%|█████████▉| 388/390 [26:13<00:00,  3.33it/s]100%|█████████▉| 389/390 [26:13<00:00,  3.38it/s]100%|██████████| 390/390 [26:14<00:00,  2.75it/s][INFO|trainer.py:2140] 2023-08-28 18:37:02,345 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:37:02,345 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 18:37:02,345 >>   Batch size = 8
{'eval_loss': 1.1420729160308838, 'eval_runtime': 13.8455, 'eval_samples_per_second': 252.573, 'eval_steps_per_second': 31.635, 'epoch': 3.99}

  0%|          | 0/438 [00:00<?, ?it/s][A
  1%|▏         | 6/438 [00:00<00:07, 57.17it/s][A
  3%|▎         | 12/438 [00:00<00:08, 49.87it/s][A
  4%|▍         | 18/438 [00:00<00:08, 48.05it/s][A
  5%|▌         | 23/438 [00:00<00:08, 47.21it/s][A
  6%|▋         | 28/438 [00:00<00:08, 46.58it/s][A
  8%|▊         | 33/438 [00:00<00:08, 45.79it/s][A
  9%|▊         | 38/438 [00:00<00:08, 45.20it/s][A
 10%|▉         | 43/438 [00:00<00:08, 44.94it/s][A
 11%|█         | 48/438 [00:01<00:08, 45.03it/s][A
 12%|█▏        | 53/438 [00:01<00:08, 45.24it/s][A
 13%|█▎        | 58/438 [00:01<00:08, 45.36it/s][A
 14%|█▍        | 63/438 [00:01<00:08, 45.50it/s][A
 16%|█▌        | 68/438 [00:01<00:08, 45.65it/s][A
 17%|█▋        | 73/438 [00:01<00:08, 45.62it/s][A
 18%|█▊        | 78/438 [00:01<00:07, 45.34it/s][A
 19%|█▉        | 83/438 [00:01<00:07, 45.07it/s][A
 20%|██        | 88/438 [00:01<00:07, 44.86it/s][A
 21%|██        | 93/438 [00:02<00:07, 44.83it/s][A
 22%|██▏       | 98/438 [00:02<00:07, 44.97it/s][A
 24%|██▎       | 103/438 [00:02<00:07, 45.14it/s][A
 25%|██▍       | 108/438 [00:02<00:07, 45.37it/s][A
 26%|██▌       | 113/438 [00:02<00:07, 45.45it/s][A
 27%|██▋       | 118/438 [00:02<00:09, 32.33it/s][A
 28%|██▊       | 123/438 [00:02<00:08, 35.41it/s][A
 29%|██▉       | 128/438 [00:02<00:08, 38.04it/s][A
 30%|███       | 133/438 [00:03<00:07, 40.08it/s][A
 32%|███▏      | 138/438 [00:03<00:07, 41.64it/s][A
 33%|███▎      | 143/438 [00:03<00:06, 42.77it/s][A
 34%|███▍      | 148/438 [00:03<00:06, 43.57it/s][A
 35%|███▍      | 153/438 [00:03<00:06, 44.07it/s][A
 36%|███▌      | 158/438 [00:03<00:06, 44.09it/s][A
 37%|███▋      | 163/438 [00:03<00:06, 44.25it/s][A
 38%|███▊      | 168/438 [00:03<00:06, 44.43it/s][A
 39%|███▉      | 173/438 [00:03<00:05, 44.74it/s][A
 41%|████      | 178/438 [00:04<00:05, 45.02it/s][A
 42%|████▏     | 183/438 [00:04<00:05, 45.18it/s][A
 43%|████▎     | 188/438 [00:04<00:05, 44.94it/s][A
 44%|████▍     | 193/438 [00:04<00:05, 45.31it/s][A
 45%|████▌     | 198/438 [00:04<00:05, 45.22it/s][A
 46%|████▋     | 203/438 [00:04<00:05, 45.16it/s][A
 47%|████▋     | 208/438 [00:04<00:05, 45.02it/s][A
 49%|████▊     | 213/438 [00:04<00:04, 45.01it/s][A
 50%|████▉     | 218/438 [00:04<00:04, 45.09it/s][A
 51%|█████     | 223/438 [00:05<00:04, 45.23it/s][A
 52%|█████▏    | 228/438 [00:05<00:04, 45.27it/s][A
 53%|█████▎    | 233/438 [00:05<00:04, 45.38it/s][A
 54%|█████▍    | 238/438 [00:06<00:12, 15.92it/s][A
 55%|█████▌    | 243/438 [00:06<00:09, 19.79it/s][A
 57%|█████▋    | 248/438 [00:06<00:07, 23.87it/s][A
 58%|█████▊    | 253/438 [00:06<00:06, 27.91it/s][A
 59%|█████▉    | 258/438 [00:06<00:05, 31.58it/s][A
 60%|██████    | 263/438 [00:06<00:05, 34.87it/s][A
 61%|██████    | 268/438 [00:06<00:04, 37.59it/s][A
 62%|██████▏   | 273/438 [00:06<00:04, 39.60it/s][A
 63%|██████▎   | 278/438 [00:06<00:03, 40.82it/s][A
 65%|██████▍   | 283/438 [00:07<00:03, 41.71it/s][A
 66%|██████▌   | 288/438 [00:07<00:03, 42.50it/s][A
 67%|██████▋   | 293/438 [00:07<00:03, 43.13it/s][A
 68%|██████▊   | 298/438 [00:07<00:03, 43.97it/s][A
 69%|██████▉   | 303/438 [00:07<00:03, 44.55it/s][A
 70%|███████   | 308/438 [00:07<00:02, 44.78it/s][A
 71%|███████▏  | 313/438 [00:07<00:02, 45.07it/s][A
 73%|███████▎  | 318/438 [00:07<00:02, 45.21it/s][A
 74%|███████▎  | 323/438 [00:07<00:02, 45.10it/s][A
 75%|███████▍  | 328/438 [00:08<00:02, 44.99it/s][A
 76%|███████▌  | 333/438 [00:08<00:02, 44.80it/s][A
 77%|███████▋  | 338/438 [00:08<00:02, 44.80it/s][A
 78%|███████▊  | 343/438 [00:08<00:03, 27.32it/s][A
 79%|███████▉  | 348/438 [00:08<00:02, 31.08it/s][A
 81%|████████  | 353/438 [00:08<00:02, 34.37it/s][A
 82%|████████▏ | 358/438 [00:08<00:02, 37.21it/s][A
 83%|████████▎ | 363/438 [00:09<00:01, 39.45it/s][A
 84%|████████▍ | 368/438 [00:09<00:01, 41.19it/s][A
 85%|████████▌ | 373/438 [00:09<00:01, 42.46it/s][A
 86%|████████▋ | 378/438 [00:09<00:01, 43.17it/s][A
 87%|████████▋ | 383/438 [00:09<00:01, 43.46it/s][A
 89%|████████▊ | 388/438 [00:09<00:01, 43.57it/s][A
 90%|████████▉ | 393/438 [00:09<00:01, 43.86it/s][A
 91%|█████████ | 398/438 [00:09<00:00, 44.17it/s][A
 92%|█████████▏| 403/438 [00:09<00:00, 44.59it/s][A
 93%|█████████▎| 408/438 [00:10<00:00, 44.83it/s][A
 94%|█████████▍| 413/438 [00:10<00:00, 45.14it/s][A
 95%|█████████▌| 418/438 [00:10<00:00, 45.27it/s][A
 97%|█████████▋| 423/438 [00:10<00:00, 45.36it/s][A
 98%|█████████▊| 428/438 [00:10<00:00, 45.17it/s][A
 99%|█████████▉| 433/438 [00:10<00:00, 45.07it/s][A
100%|██████████| 438/438 [00:10<00:00, 44.95it/s][A
                                                 [A                                                 
100%|██████████| 438/438 [00:10<00:00, 44.95it/s][A100%|██████████| 390/390 [26:25<00:00,  2.75it/s]
                                                 [A[INFO|trainer.py:1894] 2023-08-28 18:37:13,285 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390
[INFO|configuration_utils.py:351] 2023-08-28 18:37:16,136 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:47:38,903 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:47:41,539 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:47:42,048 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390/special_tokens_map.json
[INFO|trainer.py:1343] 2023-08-28 18:50:25,191 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1352] 2023-08-28 18:50:25,527 >> Loading best model from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78 (score: 1.0977325439453125).
                                                 100%|██████████| 390/390 [41:23<00:00,  2.75it/s]100%|██████████| 390/390 [41:23<00:00,  6.37s/it]
[INFO|trainer.py:1894] 2023-08-28 18:52:18,165 >> Saving model checkpoint to outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model
[INFO|configuration_utils.py:351] 2023-08-28 18:52:20,673 >> Configuration saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/config.json
[INFO|modeling_utils.py:886] 2023-08-28 18:53:30,836 >> Model weights saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/pytorch_model.bin
[INFO|tokenization_utils_base.py:1925] 2023-08-28 18:53:34,536 >> tokenizer config file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/tokenizer_config.json
[INFO|tokenization_utils_base.py:1931] 2023-08-28 18:53:34,975 >> Special tokens file saved in outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/special_tokens_map.json
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:53:42,343 >> ***** train metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:43,205 >>   epoch                    =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:43,205 >>   train_loss               =     0.4241
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:43,205 >>   train_runtime            = 0:41:23.16
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:43,205 >>   train_samples            =       5000
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:43,205 >>   train_samples_per_second =     10.068
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:43,205 >>   train_steps_per_second   =      0.157
{'eval_loss': 1.1491498947143555, 'eval_runtime': 10.7288, 'eval_samples_per_second': 325.944, 'eval_steps_per_second': 40.825, 'epoch': 4.99}
{'train_runtime': 2483.1603, 'train_samples_per_second': 10.068, 'train_steps_per_second': 0.157, 'train_loss': 0.4241264930138221, 'epoch': 4.99}
08/28/2023 18:53:46 - INFO - transformer_base.run_clm_rl -   *** Evaluate ***
[INFO|trainer.py:2140] 2023-08-28 18:53:47,269 >> ***** Running Evaluation *****
[INFO|trainer.py:2142] 2023-08-28 18:53:47,269 >>   Num examples = 3497
[INFO|trainer.py:2145] 2023-08-28 18:53:47,269 >>   Batch size = 8
  0%|          | 0/438 [00:00<?, ?it/s]  1%|▏         | 6/438 [00:00<00:07, 56.75it/s]  3%|▎         | 12/438 [00:00<00:08, 50.17it/s]  4%|▍         | 18/438 [00:00<00:08, 48.34it/s]  5%|▌         | 23/438 [00:00<00:08, 47.56it/s]  6%|▋         | 28/438 [00:00<00:08, 47.11it/s]  8%|▊         | 33/438 [00:00<00:08, 46.85it/s]  9%|▊         | 38/438 [00:00<00:08, 46.66it/s] 10%|▉         | 43/438 [00:00<00:08, 46.36it/s] 11%|█         | 48/438 [00:01<00:08, 45.74it/s] 12%|█▏        | 53/438 [00:01<00:08, 45.58it/s] 13%|█▎        | 58/438 [00:01<00:08, 45.75it/s] 14%|█▍        | 63/438 [00:01<00:08, 45.85it/s] 16%|█▌        | 68/438 [00:01<00:08, 45.93it/s] 17%|█▋        | 73/438 [00:01<00:07, 46.04it/s] 18%|█▊        | 78/438 [00:01<00:07, 46.14it/s] 19%|█▉        | 83/438 [00:01<00:07, 46.10it/s] 20%|██        | 88/438 [00:01<00:07, 45.99it/s] 21%|██        | 93/438 [00:01<00:07, 45.74it/s] 22%|██▏       | 98/438 [00:02<00:07, 45.67it/s] 24%|██▎       | 103/438 [00:02<00:07, 45.68it/s] 25%|██▍       | 108/438 [00:03<00:24, 13.27it/s] 26%|██▌       | 113/438 [00:03<00:19, 16.89it/s] 27%|██▋       | 118/438 [00:03<00:15, 20.84it/s] 28%|██▊       | 123/438 [00:03<00:12, 24.95it/s] 29%|██▉       | 128/438 [00:03<00:10, 28.94it/s] 30%|███       | 133/438 [00:03<00:09, 32.53it/s] 32%|███▏      | 138/438 [00:03<00:08, 35.68it/s] 33%|███▎      | 143/438 [00:03<00:07, 38.25it/s] 34%|███▍      | 148/438 [00:04<00:07, 40.00it/s] 35%|███▍      | 153/438 [00:04<00:06, 41.49it/s] 36%|███▌      | 158/438 [00:04<00:06, 42.77it/s] 37%|███▋      | 163/438 [00:04<00:06, 43.75it/s] 38%|███▊      | 168/438 [00:04<00:06, 44.35it/s] 39%|███▉      | 173/438 [00:04<00:05, 44.80it/s] 41%|████      | 178/438 [00:04<00:05, 45.20it/s] 42%|████▏     | 183/438 [00:04<00:05, 45.38it/s] 43%|████▎     | 188/438 [00:04<00:05, 45.47it/s] 44%|████▍     | 193/438 [00:05<00:05, 45.47it/s] 45%|████▌     | 198/438 [00:05<00:05, 45.45it/s] 46%|████▋     | 203/438 [00:05<00:05, 45.50it/s] 47%|████▋     | 208/438 [00:06<00:14, 15.61it/s] 49%|████▊     | 213/438 [00:06<00:11, 19.48it/s] 50%|████▉     | 218/438 [00:06<00:09, 23.57it/s] 51%|█████     | 223/438 [00:06<00:07, 27.62it/s] 52%|█████▏    | 228/438 [00:06<00:06, 31.35it/s] 53%|█████▎    | 233/438 [00:06<00:05, 34.68it/s] 54%|█████▍    | 238/438 [00:06<00:05, 37.50it/s] 55%|█████▌    | 243/438 [00:06<00:04, 39.68it/s] 57%|█████▋    | 248/438 [00:06<00:04, 41.04it/s] 58%|█████▊    | 253/438 [00:07<00:04, 42.26it/s] 59%|█████▉    | 258/438 [00:07<00:04, 43.35it/s] 60%|██████    | 263/438 [00:07<00:03, 44.09it/s] 61%|██████    | 268/438 [00:07<00:03, 44.64it/s] 62%|██████▏   | 273/438 [00:07<00:03, 45.03it/s] 63%|██████▎   | 278/438 [00:07<00:03, 45.22it/s] 65%|██████▍   | 283/438 [00:07<00:03, 45.46it/s] 66%|██████▌   | 288/438 [00:07<00:03, 45.41it/s] 67%|██████▋   | 293/438 [00:07<00:03, 45.19it/s] 68%|██████▊   | 298/438 [00:08<00:03, 45.29it/s] 69%|██████▉   | 303/438 [00:08<00:02, 45.34it/s] 70%|███████   | 308/438 [00:08<00:02, 45.50it/s] 71%|███████▏  | 313/438 [00:08<00:04, 26.42it/s] 73%|███████▎  | 318/438 [00:08<00:03, 30.34it/s] 74%|███████▎  | 323/438 [00:08<00:03, 33.78it/s] 75%|███████▍  | 328/438 [00:08<00:02, 36.74it/s] 76%|███████▌  | 333/438 [00:09<00:02, 39.11it/s] 77%|███████▋  | 338/438 [00:09<00:02, 40.94it/s] 78%|███████▊  | 343/438 [00:09<00:02, 42.30it/s] 79%|███████▉  | 348/438 [00:09<00:02, 43.31it/s] 81%|████████  | 353/438 [00:09<00:01, 43.70it/s] 82%|████████▏ | 358/438 [00:09<00:01, 44.22it/s] 83%|████████▎ | 363/438 [00:09<00:01, 44.67it/s] 84%|████████▍ | 368/438 [00:09<00:01, 45.09it/s] 85%|████████▌ | 373/438 [00:09<00:01, 45.32it/s] 86%|████████▋ | 378/438 [00:10<00:01, 45.15it/s] 87%|████████▋ | 383/438 [00:10<00:01, 45.46it/s] 89%|████████▊ | 388/438 [00:10<00:01, 45.55it/s] 90%|████████▉ | 393/438 [00:10<00:00, 45.45it/s] 91%|█████████ | 398/438 [00:10<00:00, 45.28it/s] 92%|█████████▏| 403/438 [00:10<00:00, 45.24it/s] 93%|█████████▎| 408/438 [00:10<00:00, 45.34it/s] 94%|█████████▍| 413/438 [00:10<00:00, 45.59it/s] 95%|█████████▌| 418/438 [00:10<00:00, 45.61it/s] 97%|█████████▋| 423/438 [00:11<00:00, 45.78it/s] 98%|█████████▊| 428/438 [00:11<00:00, 45.80it/s] 99%|█████████▉| 433/438 [00:11<00:00, 45.80it/s]100%|██████████| 438/438 [00:11<00:00, 45.75it/s]100%|██████████| 438/438 [00:11<00:00, 37.74it/s]
[INFO|trainer_pt_utils.py:908] 2023-08-28 18:53:58,913 >> ***** eval metrics *****
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:58,913 >>   epoch                   =       4.99
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:58,913 >>   eval_loss               =     1.0977
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:58,913 >>   eval_runtime            = 0:00:11.64
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:58,913 >>   eval_samples            =       3497
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:58,914 >>   eval_samples_per_second =    300.331
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:58,914 >>   eval_steps_per_second   =     37.617
[INFO|trainer_pt_utils.py:913] 2023-08-28 18:53:58,914 >>   perplexity              =     2.9974
/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:26,463 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:26,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:26,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:26,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:26,953 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:55:32,114 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:55:32,279 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:55:33,958 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:55:35,405 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:55:35,667 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:42,809 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:42,957 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:42,957 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:42,957 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:55:42,957 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:55:45,683 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:55:45,684 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:55:47,090 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:55:48,204 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:55:48,205 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-78
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-312
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-234
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-390
outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/generator/iter5/model/checkpoint-156
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/dev.jsonl', 'labels': ['director', 'located on terrain feature', 'mother', 'part of', 'residence'], 'mode': 'all_single', 'model_size': 'large', 'is_eval': True, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl'}}
predict vocab size: 14271
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 14371, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.53it/s]Extractor Predicting: 2it [00:01,  1.56it/s]Extractor Predicting: 3it [00:01,  1.59it/s]Extractor Predicting: 4it [00:02,  1.60it/s]Extractor Predicting: 5it [00:03,  1.60it/s]Extractor Predicting: 6it [00:03,  1.52it/s]Extractor Predicting: 7it [00:04,  1.56it/s]Extractor Predicting: 8it [00:05,  1.61it/s]Extractor Predicting: 9it [00:05,  1.60it/s]Extractor Predicting: 10it [00:06,  1.58it/s]Extractor Predicting: 11it [00:07,  1.27it/s]Extractor Predicting: 12it [00:08,  1.34it/s]Extractor Predicting: 13it [00:08,  1.38it/s]Extractor Predicting: 14it [00:09,  1.45it/s]Extractor Predicting: 15it [00:10,  1.32it/s]Extractor Predicting: 16it [00:10,  1.38it/s]Extractor Predicting: 17it [00:11,  1.44it/s]Extractor Predicting: 18it [00:12,  1.49it/s]Extractor Predicting: 19it [00:12,  1.52it/s]Extractor Predicting: 20it [00:13,  1.40it/s]Extractor Predicting: 21it [00:14,  1.48it/s]Extractor Predicting: 22it [00:14,  1.54it/s]Extractor Predicting: 23it [00:15,  1.56it/s]Extractor Predicting: 24it [00:16,  1.63it/s]Extractor Predicting: 25it [00:16,  1.57it/s]Extractor Predicting: 26it [00:17,  1.58it/s]Extractor Predicting: 27it [00:17,  1.58it/s]Extractor Predicting: 28it [00:18,  1.59it/s]Extractor Predicting: 29it [00:19,  1.59it/s]Extractor Predicting: 30it [00:20,  1.25it/s]Extractor Predicting: 31it [00:21,  1.33it/s]Extractor Predicting: 32it [00:21,  1.37it/s]Extractor Predicting: 33it [00:22,  1.43it/s]Extractor Predicting: 34it [00:23,  1.40it/s]Extractor Predicting: 35it [00:23,  1.46it/s]Extractor Predicting: 36it [00:24,  1.46it/s]Extractor Predicting: 37it [00:25,  1.50it/s]Extractor Predicting: 38it [00:25,  1.53it/s]Extractor Predicting: 39it [00:26,  1.38it/s]Extractor Predicting: 40it [00:27,  1.44it/s]Extractor Predicting: 41it [00:27,  1.46it/s]Extractor Predicting: 42it [00:28,  1.50it/s]Extractor Predicting: 43it [00:29,  1.54it/s]Extractor Predicting: 44it [00:30,  1.18it/s]Extractor Predicting: 45it [00:31,  1.27it/s]Extractor Predicting: 46it [00:32,  1.17it/s]Extractor Predicting: 47it [00:32,  1.25it/s]Extractor Predicting: 48it [00:33,  1.36it/s]Extractor Predicting: 49it [00:33,  1.39it/s]Extractor Predicting: 50it [00:34,  1.39it/s]Extractor Predicting: 51it [00:35,  1.46it/s]Extractor Predicting: 52it [00:35,  1.48it/s]Extractor Predicting: 53it [00:36,  1.50it/s]Extractor Predicting: 54it [00:37,  1.54it/s]Extractor Predicting: 55it [00:37,  1.46it/s]Extractor Predicting: 56it [00:38,  1.49it/s]Extractor Predicting: 57it [00:39,  1.54it/s]Extractor Predicting: 58it [00:39,  1.53it/s]Extractor Predicting: 59it [00:40,  1.51it/s]Extractor Predicting: 60it [00:41,  1.30it/s]Extractor Predicting: 61it [00:42,  1.34it/s]Extractor Predicting: 62it [00:42,  1.43it/s]Extractor Predicting: 63it [00:43,  1.50it/s]Extractor Predicting: 64it [00:44,  1.50it/s]Extractor Predicting: 65it [00:44,  1.46it/s]Extractor Predicting: 66it [00:45,  1.52it/s]Extractor Predicting: 67it [00:46,  1.55it/s]Extractor Predicting: 68it [00:46,  1.59it/s]Extractor Predicting: 69it [00:47,  1.59it/s]Extractor Predicting: 70it [00:48,  1.37it/s]Extractor Predicting: 71it [00:48,  1.44it/s]Extractor Predicting: 72it [00:49,  1.41it/s]Extractor Predicting: 73it [00:50,  1.46it/s]Extractor Predicting: 74it [00:50,  1.48it/s]Extractor Predicting: 75it [00:51,  1.42it/s]Extractor Predicting: 76it [00:52,  1.48it/s]Extractor Predicting: 77it [00:52,  1.56it/s]Extractor Predicting: 78it [00:53,  1.55it/s]Extractor Predicting: 79it [00:54,  1.61it/s]Extractor Predicting: 80it [00:55,  1.18it/s]Extractor Predicting: 81it [00:56,  1.29it/s]Extractor Predicting: 82it [00:56,  1.35it/s]Extractor Predicting: 83it [00:57,  1.41it/s]Extractor Predicting: 84it [00:58,  1.11it/s]Extractor Predicting: 85it [00:59,  1.20it/s]Extractor Predicting: 86it [01:00,  1.28it/s]Extractor Predicting: 87it [01:00,  1.37it/s]Extractor Predicting: 88it [01:03,  1.26s/it]Extractor Predicting: 89it [01:03,  1.08s/it]Extractor Predicting: 90it [01:04,  1.07it/s]Extractor Predicting: 91it [01:05,  1.18it/s]Extractor Predicting: 92it [01:06,  1.07it/s]Extractor Predicting: 93it [01:06,  1.17it/s]Extractor Predicting: 94it [01:07,  1.28it/s]Extractor Predicting: 95it [01:08,  1.38it/s]Extractor Predicting: 96it [01:09,  1.03s/it]Extractor Predicting: 97it [01:10,  1.09it/s]Extractor Predicting: 98it [01:11,  1.19it/s]Extractor Predicting: 99it [01:12,  1.04s/it]Extractor Predicting: 100it [01:13,  1.09it/s]Extractor Predicting: 101it [01:13,  1.22it/s]Extractor Predicting: 102it [01:14,  1.31it/s]Extractor Predicting: 103it [01:15,  1.07it/s]Extractor Predicting: 104it [01:16,  1.17it/s]Extractor Predicting: 105it [01:17,  1.26it/s]Extractor Predicting: 106it [01:17,  1.35it/s]Extractor Predicting: 107it [01:19,  1.09it/s]Extractor Predicting: 108it [01:19,  1.21it/s]Extractor Predicting: 109it [01:20,  1.28it/s]Extractor Predicting: 110it [01:20,  1.35it/s]Extractor Predicting: 111it [01:22,  1.19it/s]Extractor Predicting: 112it [01:22,  1.30it/s]Extractor Predicting: 113it [01:23,  1.35it/s]Extractor Predicting: 114it [01:23,  1.40it/s]Extractor Predicting: 115it [01:25,  1.07it/s]Extractor Predicting: 116it [01:26,  1.17it/s]Extractor Predicting: 117it [01:26,  1.26it/s]Extractor Predicting: 118it [01:27,  1.34it/s]Extractor Predicting: 119it [01:28,  1.11it/s]Extractor Predicting: 120it [01:29,  1.20it/s]Extractor Predicting: 121it [01:29,  1.30it/s]Extractor Predicting: 122it [01:30,  1.38it/s]Extractor Predicting: 123it [01:31,  1.19it/s]Extractor Predicting: 124it [01:32,  1.28it/s]Extractor Predicting: 125it [01:32,  1.34it/s]Extractor Predicting: 126it [01:33,  1.40it/s]Extractor Predicting: 127it [01:34,  1.27it/s]Extractor Predicting: 128it [01:35,  1.34it/s]Extractor Predicting: 129it [01:35,  1.40it/s]Extractor Predicting: 130it [01:36,  1.45it/s]Extractor Predicting: 131it [01:37,  1.46it/s]Extractor Predicting: 132it [01:39,  1.06s/it]Extractor Predicting: 133it [01:39,  1.08it/s]Extractor Predicting: 134it [01:40,  1.19it/s]Extractor Predicting: 135it [01:41,  1.20it/s]Extractor Predicting: 136it [01:41,  1.25it/s]Extractor Predicting: 137it [01:42,  1.35it/s]Extractor Predicting: 138it [01:43,  1.40it/s]Extractor Predicting: 139it [01:43,  1.43it/s]Extractor Predicting: 140it [01:44,  1.40it/s]Extractor Predicting: 141it [01:45,  1.44it/s]Extractor Predicting: 142it [01:45,  1.44it/s]Extractor Predicting: 143it [01:46,  1.46it/s]Extractor Predicting: 144it [01:47,  1.51it/s]Extractor Predicting: 145it [01:47,  1.62it/s]Extractor Predicting: 145it [01:47,  1.35it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:58:56,434 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:58:56,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:58:56,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:58:56,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:58:56,701 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 18:58:59,240 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 18:58:59,241 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:58:59,845 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 18:59:01,469 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:59:01,469 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:05,075 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:05,202 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:05,203 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:05,203 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 18:59:05,203 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 18:59:07,499 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 18:59:07,500 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 18:59:08,405 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 18:59:10,067 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 18:59:10,067 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_all_single_is_eval_True.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_all_single_is_eval_True.jsonl",
  "precision": 0.48344880677444185,
  "recall": 0.1795824992851015,
  "score": 0.2618849040867389,
  "mode": "all_single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/results_all_single_is_eval_True.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'single', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl'}}
predict vocab size: 13198
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 13298, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.58it/s]Extractor Predicting: 2it [00:01,  1.63it/s]Extractor Predicting: 3it [00:01,  1.64it/s]Extractor Predicting: 4it [00:02,  1.64it/s]Extractor Predicting: 5it [00:03,  1.62it/s]Extractor Predicting: 6it [00:04,  1.10it/s]Extractor Predicting: 7it [00:05,  1.23it/s]Extractor Predicting: 8it [00:05,  1.31it/s]Extractor Predicting: 9it [00:06,  1.41it/s]Extractor Predicting: 10it [00:07,  1.18it/s]Extractor Predicting: 11it [00:08,  1.28it/s]Extractor Predicting: 12it [00:08,  1.37it/s]Extractor Predicting: 13it [00:09,  1.43it/s]Extractor Predicting: 14it [00:10,  1.08it/s]Extractor Predicting: 15it [00:11,  1.18it/s]Extractor Predicting: 16it [00:12,  1.28it/s]Extractor Predicting: 17it [00:12,  1.37it/s]Extractor Predicting: 18it [00:13,  1.22it/s]Extractor Predicting: 19it [00:14,  1.31it/s]Extractor Predicting: 20it [00:15,  1.37it/s]Extractor Predicting: 21it [00:15,  1.43it/s]Extractor Predicting: 22it [00:16,  1.49it/s]Extractor Predicting: 23it [00:17,  1.36it/s]Extractor Predicting: 24it [00:17,  1.40it/s]Extractor Predicting: 25it [00:18,  1.46it/s]Extractor Predicting: 26it [00:19,  1.49it/s]Extractor Predicting: 27it [00:19,  1.55it/s]Extractor Predicting: 28it [00:20,  1.25it/s]Extractor Predicting: 29it [00:21,  1.33it/s]Extractor Predicting: 30it [00:22,  1.41it/s]Extractor Predicting: 31it [00:22,  1.47it/s]Extractor Predicting: 32it [00:23,  1.40it/s]Extractor Predicting: 33it [00:24,  1.43it/s]Extractor Predicting: 34it [00:24,  1.47it/s]Extractor Predicting: 35it [00:25,  1.52it/s]Extractor Predicting: 36it [00:26,  1.57it/s]Extractor Predicting: 37it [00:27,  1.18it/s]Extractor Predicting: 38it [00:27,  1.28it/s]Extractor Predicting: 39it [00:28,  1.37it/s]Extractor Predicting: 40it [00:29,  1.41it/s]Extractor Predicting: 41it [00:30,  1.38it/s]Extractor Predicting: 42it [00:30,  1.44it/s]Extractor Predicting: 43it [00:31,  1.47it/s]Extractor Predicting: 44it [00:32,  1.40it/s]Extractor Predicting: 45it [00:32,  1.44it/s]Extractor Predicting: 46it [00:33,  1.28it/s]Extractor Predicting: 47it [00:34,  1.37it/s]Extractor Predicting: 48it [00:35,  1.02it/s]Extractor Predicting: 49it [00:36,  1.15it/s]Extractor Predicting: 50it [00:37,  1.25it/s]Extractor Predicting: 51it [00:37,  1.34it/s]Extractor Predicting: 52it [00:38,  1.27it/s]Extractor Predicting: 53it [00:39,  1.35it/s]Extractor Predicting: 54it [00:39,  1.42it/s]Extractor Predicting: 55it [00:40,  1.46it/s]Extractor Predicting: 56it [00:41,  1.48it/s]Extractor Predicting: 57it [00:42,  1.27it/s]Extractor Predicting: 58it [00:42,  1.35it/s]Extractor Predicting: 59it [00:43,  1.39it/s]Extractor Predicting: 60it [00:44,  1.45it/s]Extractor Predicting: 61it [00:44,  1.49it/s]Extractor Predicting: 62it [00:45,  1.41it/s]Extractor Predicting: 63it [00:46,  1.45it/s]Extractor Predicting: 64it [00:46,  1.51it/s]Extractor Predicting: 65it [00:47,  1.54it/s]Extractor Predicting: 66it [00:48,  1.57it/s]Extractor Predicting: 67it [00:49,  1.28it/s]Extractor Predicting: 68it [00:49,  1.38it/s]Extractor Predicting: 69it [00:50,  1.47it/s]Extractor Predicting: 70it [00:50,  1.52it/s]Extractor Predicting: 71it [00:51,  1.57it/s]Extractor Predicting: 72it [00:52,  1.22it/s]Extractor Predicting: 73it [00:53,  1.32it/s]Extractor Predicting: 74it [00:54,  1.38it/s]Extractor Predicting: 75it [00:54,  1.42it/s]Extractor Predicting: 76it [00:56,  1.09it/s]Extractor Predicting: 77it [00:56,  1.23it/s]Extractor Predicting: 78it [00:57,  1.32it/s]Extractor Predicting: 79it [00:57,  1.41it/s]Extractor Predicting: 80it [00:59,  1.04it/s]Extractor Predicting: 81it [01:00,  1.17it/s]Extractor Predicting: 82it [01:00,  1.27it/s]Extractor Predicting: 83it [01:01,  1.35it/s]Extractor Predicting: 84it [01:02,  1.06it/s]Extractor Predicting: 85it [01:03,  1.19it/s]Extractor Predicting: 86it [01:03,  1.31it/s]Extractor Predicting: 87it [01:04,  1.40it/s]Extractor Predicting: 88it [01:05,  1.20it/s]Extractor Predicting: 89it [01:06,  1.28it/s]Extractor Predicting: 90it [01:06,  1.35it/s]Extractor Predicting: 91it [01:07,  1.45it/s]Extractor Predicting: 92it [01:08,  1.48it/s]Extractor Predicting: 93it [01:08,  1.50it/s]Extractor Predicting: 94it [01:09,  1.52it/s]Extractor Predicting: 95it [01:10,  1.08it/s]Extractor Predicting: 96it [01:11,  1.21it/s]Extractor Predicting: 97it [01:12,  1.31it/s]Extractor Predicting: 98it [01:12,  1.37it/s]Extractor Predicting: 99it [01:13,  1.20it/s]Extractor Predicting: 100it [01:14,  1.30it/s]Extractor Predicting: 101it [01:15,  1.40it/s]Extractor Predicting: 102it [01:15,  1.41it/s]Extractor Predicting: 103it [01:16,  1.47it/s]Extractor Predicting: 104it [01:17,  1.41it/s]Extractor Predicting: 105it [01:17,  1.44it/s]Extractor Predicting: 106it [01:18,  1.49it/s]Extractor Predicting: 107it [01:19,  1.51it/s]Extractor Predicting: 108it [01:19,  1.56it/s]Extractor Predicting: 109it [01:20,  1.25it/s]Extractor Predicting: 110it [01:21,  1.36it/s]Extractor Predicting: 111it [01:22,  1.46it/s]Extractor Predicting: 112it [01:22,  1.50it/s]Extractor Predicting: 113it [01:23,  1.53it/s]Extractor Predicting: 114it [01:24,  1.32it/s]Extractor Predicting: 115it [01:24,  1.40it/s]Extractor Predicting: 116it [01:25,  1.47it/s]Extractor Predicting: 117it [01:26,  1.53it/s]Extractor Predicting: 118it [01:26,  1.54it/s]Extractor Predicting: 119it [01:27,  1.48it/s]Extractor Predicting: 120it [01:28,  1.50it/s]Extractor Predicting: 121it [01:28,  1.53it/s]Extractor Predicting: 122it [01:29,  1.58it/s]Extractor Predicting: 123it [01:29,  1.60it/s]Extractor Predicting: 124it [01:30,  1.41it/s]Extractor Predicting: 125it [01:31,  1.45it/s]Extractor Predicting: 126it [01:32,  1.47it/s]Extractor Predicting: 127it [01:32,  1.50it/s]Extractor Predicting: 128it [01:33,  1.53it/s]Extractor Predicting: 129it [01:34,  1.52it/s]Extractor Predicting: 130it [01:34,  1.53it/s]Extractor Predicting: 131it [01:35,  1.59it/s]Extractor Predicting: 132it [01:35,  1.60it/s]Extractor Predicting: 133it [01:36,  1.59it/s]Extractor Predicting: 134it [01:39,  1.19s/it]Extractor Predicting: 135it [01:39,  1.03s/it]Extractor Predicting: 136it [01:40,  1.10it/s]Extractor Predicting: 137it [01:42,  1.16s/it]Extractor Predicting: 138it [01:42,  1.00it/s]Extractor Predicting: 139it [01:43,  1.13it/s]Extractor Predicting: 140it [01:44,  1.18it/s]Extractor Predicting: 141it [01:44,  1.30it/s]Extractor Predicting: 142it [01:45,  1.36it/s]Extractor Predicting: 143it [01:45,  1.43it/s]Extractor Predicting: 144it [01:46,  1.47it/s]Extractor Predicting: 145it [01:46,  1.76it/s]Extractor Predicting: 145it [01:46,  1.36it/s]
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:51,768 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/xuting/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:51,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:51,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:51,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/xuting/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:01:51,770 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|configuration_utils.py:517] 2023-08-28 19:01:54,627 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:553] 2023-08-28 19:01:54,698 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:01:55,182 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1328] 2023-08-28 19:01:57,989 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:01:58,471 >> All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:02:01,883 >> loading file https://huggingface.co/albert-large-v2/resolve/main/spiece.model from cache at /home/xuting/.cache/huggingface/transformers/b4bd5194827ca5bc0342e0421aace72462c676f37679a440862cf3ee46f95f48.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:02:02,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:02:02,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:02:02,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|tokenization_utils_base.py:1717] 2023-08-28 19:02:02,513 >> loading file https://huggingface.co/albert-large-v2/resolve/main/tokenizer.json from cache at /home/xuting/.cache/huggingface/transformers/8f1144987c0a5fcedc8808300dc830a4a00787ceaccb85e9f913ef047103bd89.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|configuration_utils.py:517] 2023-08-28 19:02:04,348 >> loading configuration file https://huggingface.co/albert-large-v2/resolve/main/config.json from cache at /home/xuting/.cache/huggingface/transformers/b2da41a68a8020e0d5923bb74adfb48c33df97683e143ca33ad6e52a3e05d70d.06fa0ad0d486db01b65880587686cbf167e4c2d52e242d574fac416eda16c32d
[INFO|configuration_utils.py:553] 2023-08-28 19:02:04,349 >> Model config AlbertConfig {
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 16,
  "num_hidden_groups": 1,
  "num_hidden_layers": 24,
  "num_memory_blocks": 0,
  "output_attentions": true,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.7.0",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|modeling_utils.py:1152] 2023-08-28 19:02:04,959 >> loading weights file https://huggingface.co/albert-large-v2/resolve/main/pytorch_model.bin from cache at /home/xuting/.cache/huggingface/transformers/4552fda677d63af6d9acdc968e0a4bfb09ef6994bbb37c065ea6533cf8dc0977.4ffe1a3c3f6feb9b16e8d8811a495b5ca957bb71b1215ae0960c2b06f2e7d9bd
[WARNING|modeling_utils.py:1328] 2023-08-28 19:02:06,443 >> Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.weight']
- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1345] 2023-08-28 19:02:06,443 >> All the weights of AlbertModel were initialized from the model checkpoint at albert-large-v2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertModel for predictions without further training.
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_single_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_single_is_eval_False.jsonl",
  "precision": 0.4519119351100811,
  "recall": 0.22549869904596703,
  "score": 0.30086788813886206,
  "mode": "single",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/results_single_is_eval_False.json"
}
{'run_eval': {'path_model': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5', 'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'mode': 'multi', 'model_size': 'large', 'is_eval': False, 'limit': 0}}
{'predict': {'path_in': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl', 'path_out': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl'}}
predict vocab size: 301
{'get_model': {'config': {'cased': False, 'token_emb_dim': 100, 'char_encoder': 'lstm', 'char_emb_dim': 30, 'tag_emb_dim': 50, 'hidden_dim': 200, 'num_layers': 3, 'max_depth': None, 'crf': None, 'loss_reduction': 'sum', 'dropout': 0.5, 'lr': 0.001, 'optimizer': 'adam', 'maxlen': None, 'vocab_size': 401, 'vocab_file': None, 'ner_tag_vocab_size': 64, 're_tag_vocab_size': 250, 'tag_form': 'iob2', 'device': 'cuda', 'lm_emb_dim': 1024, 'lm_emb_path': 'albert-large-v2', 'pos_emb_dim': 0, 'head_emb_dim': 384, 'warm_steps': 1000, 'grad_period': 1}}}
albert-large-v2 is not file, try load as bert model.
albert-large-v2 loaded successfully.
Note it only supports default options now, i.e.: 
  layers='-1,-2,-3,-4', use_scalar_mix=True, pooling_operation="mean"
reading pretrained wv from outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/data/predict.txt
Extractor Predicting: 0it [00:00, ?it/s]Extractor Predicting: 1it [00:00,  1.48it/s]Extractor Predicting: 1it [00:00,  1.48it/s]
{
  "path_pred": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_out_filter_multi_is_eval_False.jsonl",
  "path_gold": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/pred_in_multi_is_eval_False.jsonl",
  "precision": 0.6666666666666666,
  "recall": 0.1951219512195122,
  "score": 0.3018867924528301,
  "mode": "multi",
  "limit": 0,
  "path_results": "outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter5/results_multi_is_eval_False.json"
}
{'eval_best': {'path_test': 'zero_rte/fewrel/unseen_5_seed_4/test.jsonl', 'save_dir': 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/', 'labels': ['developer', 'located in or next to body of water', 'member of political party', 'operator', 'position held'], 'num_iter': 5, 'limit': 5000}}
Traceback (most recent call last):
  File "wrapper.py", line 811, in <module>
    Fire()
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 471, in _Fire
    target=component.__name__)
  File "/cto_studio/xuting/miniconda3/envs/table_v1/lib/python3.7/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "wrapper.py", line 654, in main_dual
    eval_best(path_test=path_test, save_dir=save_dir, labels=labels_test, num_iter=num_iter, limit=limit)
  File "wrapper.py", line 711, in eval_best
    with open(path_results) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'outputs/wrapper/fewrel_rl_all_rsFalse_nbrel_filtered_large/unseen_5_seed_4/extractor/iter1/results_single_is_eval_True_limit5000.json'
